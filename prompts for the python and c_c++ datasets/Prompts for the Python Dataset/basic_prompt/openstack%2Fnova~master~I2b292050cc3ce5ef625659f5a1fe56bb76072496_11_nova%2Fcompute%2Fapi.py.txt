Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import copy
24 import functools
25 import re
26 import string
27 
28 from castellan import key_manager
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_utils import excutils
33 from oslo_utils import strutils
34 from oslo_utils import timeutils
35 from oslo_utils import units
36 from oslo_utils import uuidutils
37 import six
38 from six.moves import range
39 
40 from nova.api.openstack import common
41 from nova import availability_zones
42 from nova import block_device
43 from nova.compute import flavors
44 from nova.compute import instance_actions
45 from nova.compute import instance_list
46 from nova.compute import migration_list
47 from nova.compute import power_state
48 from nova.compute import rpcapi as compute_rpcapi
49 from nova.compute import task_states
50 from nova.compute import utils as compute_utils
51 from nova.compute.utils import wrap_instance_event
52 from nova.compute import vm_states
53 from nova import conductor
54 import nova.conf
55 from nova import context as nova_context
56 from nova import crypto
57 from nova.db import base
58 from nova.db.sqlalchemy import api as db_api
59 from nova import exception
60 from nova import exception_wrapper
61 from nova import hooks
62 from nova.i18n import _
63 from nova import image
64 from nova import network
65 from nova.network import model as network_model
66 from nova.network.neutronv2 import constants
67 from nova.network.security_group import openstack_driver
68 from nova.network.security_group import security_group_base
69 from nova import objects
70 from nova.objects import base as obj_base
71 from nova.objects import block_device as block_device_obj
72 from nova.objects import fields as fields_obj
73 from nova.objects import keypair as keypair_obj
74 from nova.objects import quotas as quotas_obj
75 from nova.pci import request as pci_request
76 from nova.policies import servers as servers_policies
77 import nova.policy
78 from nova import profiler
79 from nova import rpc
80 from nova.scheduler.client import query
81 from nova.scheduler.client import report
82 from nova.scheduler import utils as scheduler_utils
83 from nova import servicegroup
84 from nova import utils
85 from nova.virt import hardware
86 from nova.volume import cinder
87 
88 LOG = logging.getLogger(__name__)
89 
90 get_notifier = functools.partial(rpc.get_notifier, service='compute')
91 # NOTE(gibi): legacy notification used compute as a service but these
92 # calls still run on the client side of the compute service which is
93 # nova-api. By setting the binary to nova-api below, we can make sure
94 # that the new versioned notifications has the right publisher_id but the
95 # legacy notifications does not change.
96 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
97                                    get_notifier=get_notifier,
98                                    binary='nova-api')
99 CONF = nova.conf.CONF
100 
101 RO_SECURITY_GROUPS = ['default']
102 
103 AGGREGATE_ACTION_UPDATE = 'Update'
104 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
105 AGGREGATE_ACTION_DELETE = 'Delete'
106 AGGREGATE_ACTION_ADD = 'Add'
107 MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION = 34
108 MIN_COMPUTE_VOLUME_TYPE = 36
109 MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED = 38
110 
111 # FIXME(danms): Keep a global cache of the cells we find the
112 # first time we look. This needs to be refreshed on a timer or
113 # trigger.
114 CELLS = []
115 
116 
117 def check_instance_state(vm_state=None, task_state=(None,),
118                          must_have_launched=True):
119     """Decorator to check VM and/or task state before entry to API functions.
120 
121     If the instance is in the wrong state, or has not been successfully
122     started at least once the wrapper will raise an exception.
123     """
124 
125     if vm_state is not None and not isinstance(vm_state, set):
126         vm_state = set(vm_state)
127     if task_state is not None and not isinstance(task_state, set):
128         task_state = set(task_state)
129 
130     def outer(f):
131         @six.wraps(f)
132         def inner(self, context, instance, *args, **kw):
133             if vm_state is not None and instance.vm_state not in vm_state:
134                 raise exception.InstanceInvalidState(
135                     attr='vm_state',
136                     instance_uuid=instance.uuid,
137                     state=instance.vm_state,
138                     method=f.__name__)
139             if (task_state is not None and
140                     instance.task_state not in task_state):
141                 raise exception.InstanceInvalidState(
142                     attr='task_state',
143                     instance_uuid=instance.uuid,
144                     state=instance.task_state,
145                     method=f.__name__)
146             if must_have_launched and not instance.launched_at:
147                 raise exception.InstanceInvalidState(
148                     attr='launched_at',
149                     instance_uuid=instance.uuid,
150                     state=instance.launched_at,
151                     method=f.__name__)
152 
153             return f(self, context, instance, *args, **kw)
154         return inner
155     return outer
156 
157 
158 def _set_or_none(q):
159     return q if q is None or isinstance(q, set) else set(q)
160 
161 
162 def reject_instance_state(vm_state=None, task_state=None):
163     """Decorator.  Raise InstanceInvalidState if instance is in any of the
164     given states.
165     """
166 
167     vm_state = _set_or_none(vm_state)
168     task_state = _set_or_none(task_state)
169 
170     def outer(f):
171         @six.wraps(f)
172         def inner(self, context, instance, *args, **kw):
173             _InstanceInvalidState = functools.partial(
174                 exception.InstanceInvalidState,
175                 instance_uuid=instance.uuid,
176                 method=f.__name__)
177 
178             if vm_state is not None and instance.vm_state in vm_state:
179                 raise _InstanceInvalidState(
180                     attr='vm_state', state=instance.vm_state)
181 
182             if task_state is not None and instance.task_state in task_state:
183                 raise _InstanceInvalidState(
184                     attr='task_state', state=instance.task_state)
185 
186             return f(self, context, instance, *args, **kw)
187         return inner
188     return outer
189 
190 
191 def check_instance_host(function):
192     @six.wraps(function)
193     def wrapped(self, context, instance, *args, **kwargs):
194         if not instance.host:
195             raise exception.InstanceNotReady(instance_id=instance.uuid)
196         return function(self, context, instance, *args, **kwargs)
197     return wrapped
198 
199 
200 def check_instance_lock(function):
201     @six.wraps(function)
202     def inner(self, context, instance, *args, **kwargs):
203         if instance.locked and not context.is_admin:
204             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
205         return function(self, context, instance, *args, **kwargs)
206     return inner
207 
208 
209 def _diff_dict(orig, new):
210     """Return a dict describing how to change orig to new.  The keys
211     correspond to values that have changed; the value will be a list
212     of one or two elements.  The first element of the list will be
213     either '+' or '-', indicating whether the key was updated or
214     deleted; if the key was updated, the list will contain a second
215     element, giving the updated value.
216     """
217     # Figure out what keys went away
218     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
219     # Compute the updates
220     for key, value in new.items():
221         if key not in orig or value != orig[key]:
222             result[key] = ['+', value]
223     return result
224 
225 
226 def load_cells():
227     global CELLS
228     if not CELLS:
229         CELLS = objects.CellMappingList.get_all(
230             nova_context.get_admin_context())
231         LOG.debug('Found %(count)i cells: %(cells)s',
232                   dict(count=len(CELLS),
233                        cells=','.join([c.identity for c in CELLS])))
234 
235     if not CELLS:
236         LOG.error('No cells are configured, unable to continue')
237 
238 
239 def _get_image_meta_obj(image_meta_dict):
240     try:
241         image_meta = objects.ImageMeta.from_dict(image_meta_dict)
242     except ValueError as e:
243         # there must be invalid values in the image meta properties so
244         # consider this an invalid request
245         msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
246         raise exception.InvalidRequest(msg)
247     return image_meta
248 
249 
250 def _is_valid_vm_task_power_states(instance, tag):
251     """Check if the current state of the instance allows it to be
252     a candidate for the power-update event.
253 
254     :param instance: The nova instance object.
255     :param tag: The desired target power state.
256     :returns Boolean: True if the instance can be subjected to the
257                       power-update event.
258     """
259     if ((tag == common.POWER_ON and
260             instance.task_state is None and
261             instance.vm_state == vm_states.STOPPED and
262             instance.power_state == power_state.SHUTDOWN) or
263         (tag == common.POWER_OFF and
264             instance.task_state is None and
265             instance.vm_state == vm_states.ACTIVE and
266             instance.power_state == power_state.RUNNING)):
267         return True
268     return False
269 
270 
271 @profiler.trace_cls("compute_api")
272 class API(base.Base):
273     """API for interacting with the compute manager."""
274 
275     def __init__(self, image_api=None, network_api=None, volume_api=None,
276                  security_group_api=None, **kwargs):
277         self.image_api = image_api or image.API()
278         self.network_api = network_api or network.API()
279         self.volume_api = volume_api or cinder.API()
280         self._placementclient = None  # Lazy-load on first access.
281         self.security_group_api = (security_group_api or
282             openstack_driver.get_openstack_security_group_driver())
283         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
284         self.compute_task_api = conductor.ComputeTaskAPI()
285         self.servicegroup_api = servicegroup.API()
286         self.host_api = HostAPI(self.compute_rpcapi, self.servicegroup_api)
287         self.notifier = rpc.get_notifier('compute', CONF.host)
288         if CONF.ephemeral_storage_encryption.enabled:
289             self.key_manager = key_manager.API()
290         # Help us to record host in EventReporter
291         self.host = CONF.host
292         super(API, self).__init__(**kwargs)
293 
294     def _record_action_start(self, context, instance, action):
295         objects.InstanceAction.action_start(context, instance.uuid,
296                                             action, want_result=False)
297 
298     def _check_injected_file_quota(self, context, injected_files):
299         """Enforce quota limits on injected files.
300 
301         Raises a QuotaError if any limit is exceeded.
302         """
303         if injected_files is None:
304             return
305 
306         # Check number of files first
307         try:
308             objects.Quotas.limit_check(context,
309                                        injected_files=len(injected_files))
310         except exception.OverQuota:
311             raise exception.OnsetFileLimitExceeded()
312 
313         # OK, now count path and content lengths; we're looking for
314         # the max...
315         max_path = 0
316         max_content = 0
317         for path, content in injected_files:
318             max_path = max(max_path, len(path))
319             max_content = max(max_content, len(content))
320 
321         try:
322             objects.Quotas.limit_check(context,
323                                        injected_file_path_bytes=max_path,
324                                        injected_file_content_bytes=max_content)
325         except exception.OverQuota as exc:
326             # Favor path limit over content limit for reporting
327             # purposes
328             if 'injected_file_path_bytes' in exc.kwargs['overs']:
329                 raise exception.OnsetFilePathLimitExceeded(
330                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
331             else:
332                 raise exception.OnsetFileContentLimitExceeded(
333                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
334 
335     def _check_metadata_properties_quota(self, context, metadata=None):
336         """Enforce quota limits on metadata properties."""
337         if not metadata:
338             metadata = {}
339         if not isinstance(metadata, dict):
340             msg = (_("Metadata type should be dict."))
341             raise exception.InvalidMetadata(reason=msg)
342         num_metadata = len(metadata)
343         try:
344             objects.Quotas.limit_check(context, metadata_items=num_metadata)
345         except exception.OverQuota as exc:
346             quota_metadata = exc.kwargs['quotas']['metadata_items']
347             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
348 
349         # Because metadata is stored in the DB, we hard-code the size limits
350         # In future, we may support more variable length strings, so we act
351         #  as if this is quota-controlled for forwards compatibility.
352         # Those are only used in V2 API, from V2.1 API, those checks are
353         # validated at API layer schema validation.
354         for k, v in metadata.items():
355             try:
356                 utils.check_string_length(v)
357                 utils.check_string_length(k, min_length=1)
358             except exception.InvalidInput as e:
359                 raise exception.InvalidMetadata(reason=e.format_message())
360 
361             if len(k) > 255:
362                 msg = _("Metadata property key greater than 255 characters")
363                 raise exception.InvalidMetadataSize(reason=msg)
364             if len(v) > 255:
365                 msg = _("Metadata property value greater than 255 characters")
366                 raise exception.InvalidMetadataSize(reason=msg)
367 
368     def _check_requested_secgroups(self, context, secgroups):
369         """Check if the security group requested exists and belongs to
370         the project.
371 
372         :param context: The nova request context.
373         :type context: nova.context.RequestContext
374         :param secgroups: list of requested security group names, or uuids in
375             the case of Neutron.
376         :type secgroups: list
377         :returns: list of requested security group names unmodified if using
378             nova-network. If using Neutron, the list returned is all uuids.
379             Note that 'default' is a special case and will be unmodified if
380             it's requested.
381         """
382         security_groups = []
383         for secgroup in secgroups:
384             # NOTE(sdague): default is handled special
385             if secgroup == "default":
386                 security_groups.append(secgroup)
387                 continue
388             secgroup_dict = self.security_group_api.get(context, secgroup)
389             if not secgroup_dict:
390                 raise exception.SecurityGroupNotFoundForProject(
391                     project_id=context.project_id, security_group_id=secgroup)
392 
393             # Check to see if it's a nova-network or neutron type.
394             if isinstance(secgroup_dict['id'], int):
395                 # This is nova-network so just return the requested name.
396                 security_groups.append(secgroup)
397             else:
398                 # The id for neutron is a uuid, so we return the id (uuid).
399                 security_groups.append(secgroup_dict['id'])
400 
401         return security_groups
402 
403     def _check_requested_networks(self, context, requested_networks,
404                                   max_count):
405         """Check if the networks requested belongs to the project
406         and the fixed IP address for each network provided is within
407         same the network block
408         """
409         if requested_networks is not None:
410             if requested_networks.no_allocate:
411                 # If the network request was specifically 'none' meaning don't
412                 # allocate any networks, we just return the number of requested
413                 # instances since quotas don't change at all.
414                 return max_count
415 
416             # NOTE(danms): Temporary transition
417             requested_networks = requested_networks.as_tuples()
418 
419         return self.network_api.validate_networks(context, requested_networks,
420                                                   max_count)
421 
422     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
423                                    image):
424         """Choose kernel and ramdisk appropriate for the instance.
425 
426         The kernel and ramdisk can be chosen in one of two ways:
427 
428             1. Passed in with create-instance request.
429 
430             2. Inherited from image metadata.
431 
432         If inherited from image metadata, and if that image metadata value is
433         set to 'nokernel', both kernel and ramdisk will default to None.
434         """
435         # Inherit from image if not specified
436         image_properties = image.get('properties', {})
437 
438         if kernel_id is None:
439             kernel_id = image_properties.get('kernel_id')
440 
441         if ramdisk_id is None:
442             ramdisk_id = image_properties.get('ramdisk_id')
443 
444         # Force to None if kernel_id indicates that a kernel is not to be used
445         if kernel_id == 'nokernel':
446             kernel_id = None
447             ramdisk_id = None
448 
449         # Verify kernel and ramdisk exist (fail-fast)
450         if kernel_id is not None:
451             kernel_image = self.image_api.get(context, kernel_id)
452             # kernel_id could have been a URI, not a UUID, so to keep behaviour
453             # from before, which leaked that implementation detail out to the
454             # caller, we return the image UUID of the kernel image and ramdisk
455             # image (below) and not any image URIs that might have been
456             # supplied.
457             # TODO(jaypipes): Get rid of this silliness once we move to a real
458             # Image object and hide all of that stuff within nova.image.api.
459             kernel_id = kernel_image['id']
460 
461         if ramdisk_id is not None:
462             ramdisk_image = self.image_api.get(context, ramdisk_id)
463             ramdisk_id = ramdisk_image['id']
464 
465         return kernel_id, ramdisk_id
466 
467     @staticmethod
468     def parse_availability_zone(context, availability_zone):
469         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
470         #             via az using az:host:node. It might be nice to expose an
471         #             api to specify specific hosts to force onto, but for
472         #             now it just supports this legacy hack.
473         # NOTE(deva): It is also possible to specify az::node, in which case
474         #             the host manager will determine the correct host.
475         forced_host = None
476         forced_node = None
477         if availability_zone and ':' in availability_zone:
478             c = availability_zone.count(':')
479             if c == 1:
480                 availability_zone, forced_host = availability_zone.split(':')
481             elif c == 2:
482                 if '::' in availability_zone:
483                     availability_zone, forced_node = \
484                             availability_zone.split('::')
485                 else:
486                     availability_zone, forced_host, forced_node = \
487                             availability_zone.split(':')
488             else:
489                 raise exception.InvalidInput(
490                         reason="Unable to parse availability_zone")
491 
492         if not availability_zone:
493             availability_zone = CONF.default_schedule_zone
494 
495         return availability_zone, forced_host, forced_node
496 
497     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
498                                           auto_disk_config, image):
499         auto_disk_config_disabled = \
500                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
501         if auto_disk_config_disabled and auto_disk_config:
502             raise exception.AutoDiskConfigDisabledByImage(image=image)
503 
504     def _inherit_properties_from_image(self, image, auto_disk_config):
505         image_properties = image.get('properties', {})
506         auto_disk_config_img = \
507                 utils.get_auto_disk_config_from_image_props(image_properties)
508         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
509                                                auto_disk_config,
510                                                image.get("id"))
511         if auto_disk_config is None:
512             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
513 
514         return {
515             'os_type': image_properties.get('os_type'),
516             'architecture': image_properties.get('architecture'),
517             'vm_mode': image_properties.get('vm_mode'),
518             'auto_disk_config': auto_disk_config
519         }
520 
521     def _check_config_drive(self, config_drive):
522         if config_drive:
523             try:
524                 bool_val = strutils.bool_from_string(config_drive,
525                                                      strict=True)
526             except ValueError:
527                 raise exception.ConfigDriveInvalidValue(option=config_drive)
528         else:
529             bool_val = False
530         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
531         # but this is because the config drive column is a String.  False
532         # is represented by using an empty string.  And for whatever
533         # reason, we rely on the DB to cast True to a String.
534         return True if bool_val else ''
535 
536     def _validate_flavor_image(self, context, image_id, image,
537                                instance_type, root_bdm, validate_numa=True):
538         """Validate the flavor and image.
539 
540         This is called from the API service to ensure that the flavor
541         extra-specs and image properties are self-consistent and compatible
542         with each other.
543 
544         :param context: A context.RequestContext
545         :param image_id: UUID of the image
546         :param image: a dict representation of the image including properties,
547                       enforces the image status is active.
548         :param instance_type: Flavor object
549         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
550                the resize case.
551         :param validate_numa: Flag to indicate whether or not to validate
552                the NUMA-related metadata.
553         :raises: Many different possible exceptions.  See
554                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
555                  for the full list.
556         """
557         if image and image['status'] != 'active':
558             raise exception.ImageNotActive(image_id=image_id)
559         self._validate_flavor_image_nostatus(context, image, instance_type,
560                                              root_bdm, validate_numa)
561 
562     @staticmethod
563     def _validate_flavor_image_nostatus(context, image, instance_type,
564                                         root_bdm, validate_numa=True,
565                                         validate_pci=False):
566         """Validate the flavor and image.
567 
568         This is called from the API service to ensure that the flavor
569         extra-specs and image properties are self-consistent and compatible
570         with each other.
571 
572         :param context: A context.RequestContext
573         :param image: a dict representation of the image including properties
574         :param instance_type: Flavor object
575         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
576                the resize case.
577         :param validate_numa: Flag to indicate whether or not to validate
578                the NUMA-related metadata.
579         :param validate_pci: Flag to indicate whether or not to validate
580                the PCI-related metadata.
581         :raises: Many different possible exceptions.  See
582                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
583                  for the full list.
584         """
585         if not image:
586             return
587 
588         image_properties = image.get('properties', {})
589         config_drive_option = image_properties.get(
590             'img_config_drive', 'optional')
591         if config_drive_option not in ['optional', 'mandatory']:
592             raise exception.InvalidImageConfigDrive(
593                 config_drive=config_drive_option)
594 
595         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
596             raise exception.FlavorMemoryTooSmall()
597 
598         # Image min_disk is in gb, size is in bytes. For sanity, have them both
599         # in bytes.
600         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
601         image_size = int(image.get('size') or 0)
602 
603         # Target disk is a volume. Don't check flavor disk size because it
604         # doesn't make sense, and check min_disk against the volume size.
605         if root_bdm is not None and root_bdm.is_volume:
606             # There are 2 possibilities here:
607             #
608             # 1. The target volume already exists but bdm.volume_size is not
609             #    yet set because this method is called before
610             #    _bdm_validate_set_size_and_instance during server create.
611             # 2. The target volume doesn't exist, in which case the bdm will
612             #    contain the intended volume size
613             #
614             # Note that rebuild also calls this method with potentially a new
615             # image but you can't rebuild a volume-backed server with a new
616             # image (yet).
617             #
618             # Cinder does its own check against min_disk, so if the target
619             # volume already exists this has already been done and we don't
620             # need to check it again here. In this case, volume_size may not be
621             # set on the bdm.
622             #
623             # If we're going to create the volume, the bdm will contain
624             # volume_size. Therefore we should check it if it exists. This will
625             # still be checked again by cinder when the volume is created, but
626             # that will not happen until the request reaches a host. By
627             # checking it here, the user gets an immediate and useful failure
628             # indication.
629             #
630             # The third possibility is that we have failed to consider
631             # something, and there are actually more than 2 possibilities. In
632             # this case cinder will still do the check at volume creation time.
633             # The behaviour will still be correct, but the user will not get an
634             # immediate failure from the api, and will instead have to
635             # determine why the instance is in an error state with a task of
636             # block_device_mapping.
637             #
638             # We could reasonably refactor this check into _validate_bdm at
639             # some future date, as the various size logic is already split out
640             # in there.
641             dest_size = root_bdm.volume_size
642             if dest_size is not None:
643                 dest_size *= units.Gi
644 
645                 if image_min_disk > dest_size:
646                     raise exception.VolumeSmallerThanMinDisk(
647                         volume_size=dest_size, image_min_disk=image_min_disk)
648 
649         # Target disk is a local disk whose size is taken from the flavor
650         else:
651             dest_size = instance_type['root_gb'] * units.Gi
652 
653             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
654             # since libvirt interpreted the value differently than other
655             # drivers. A value of 0 means don't check size.
656             if dest_size != 0:
657                 if image_size > dest_size:
658                     raise exception.FlavorDiskSmallerThanImage(
659                         flavor_size=dest_size, image_size=image_size)
660 
661                 if image_min_disk > dest_size:
662                     raise exception.FlavorDiskSmallerThanMinDisk(
663                         flavor_size=dest_size, image_min_disk=image_min_disk)
664             else:
665                 # The user is attempting to create a server with a 0-disk
666                 # image-backed flavor, which can lead to issues with a large
667                 # image consuming an unexpectedly large amount of local disk
668                 # on the compute host. Check to see if the deployment will
669                 # allow that.
670                 if not context.can(
671                         servers_policies.ZERO_DISK_FLAVOR, fatal=False):
672                     raise exception.BootFromVolumeRequiredForZeroDiskFlavor()
673 
674         API._validate_flavor_image_numa_pci(
675             image, instance_type, validate_numa=validate_numa,
676             validate_pci=validate_pci)
677 
678     @staticmethod
679     def _validate_flavor_image_numa_pci(image, instance_type,
680                                         validate_numa=True,
681                                         validate_pci=False):
682         """Validate the flavor and image NUMA/PCI values.
683 
684         This is called from the API service to ensure that the flavor
685         extra-specs and image properties are self-consistent and compatible
686         with each other.
687 
688         :param image: a dict representation of the image including properties
689         :param instance_type: Flavor object
690         :param validate_numa: Flag to indicate whether or not to validate
691                the NUMA-related metadata.
692         :param validate_pci: Flag to indicate whether or not to validate
693                the PCI-related metadata.
694         :raises: Many different possible exceptions.  See
695                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
696                  for the full list.
697         """
698         image_meta = _get_image_meta_obj(image)
699 
700         # Only validate values of flavor/image so the return results of
701         # following 'get' functions are not used.
702         hardware.get_number_of_serial_ports(instance_type, image_meta)
703         if hardware.is_realtime_enabled(instance_type):
704             hardware.vcpus_realtime_topology(instance_type, image_meta)
705         hardware.get_cpu_topology_constraints(instance_type, image_meta)
706         if validate_numa:
707             hardware.numa_get_constraints(instance_type, image_meta)
708         if validate_pci:
709             pci_request.get_pci_requests_from_flavor(instance_type)
710 
711     def _get_image_defined_bdms(self, instance_type, image_meta,
712                                 root_device_name):
713         image_properties = image_meta.get('properties', {})
714 
715         # Get the block device mappings defined by the image.
716         image_defined_bdms = image_properties.get('block_device_mapping', [])
717         legacy_image_defined = not image_properties.get('bdm_v2', False)
718 
719         image_mapping = image_properties.get('mappings', [])
720 
721         if legacy_image_defined:
722             image_defined_bdms = block_device.from_legacy_mapping(
723                 image_defined_bdms, None, root_device_name)
724         else:
725             image_defined_bdms = list(map(block_device.BlockDeviceDict,
726                                           image_defined_bdms))
727 
728         if image_mapping:
729             image_mapping = self._prepare_image_mapping(instance_type,
730                                                         image_mapping)
731             image_defined_bdms = self._merge_bdms_lists(
732                 image_mapping, image_defined_bdms)
733 
734         return image_defined_bdms
735 
736     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
737         flavor_defined_bdms = []
738 
739         have_ephemeral_bdms = any(filter(
740             block_device.new_format_is_ephemeral, block_device_mapping))
741         have_swap_bdms = any(filter(
742             block_device.new_format_is_swap, block_device_mapping))
743 
744         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
745             flavor_defined_bdms.append(
746                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
747         if instance_type.get('swap') and not have_swap_bdms:
748             flavor_defined_bdms.append(
749                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
750 
751         return flavor_defined_bdms
752 
753     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
754         """Override any block devices from the first list by device name
755 
756         :param overridable_mappings: list which items are overridden
757         :param overrider_mappings: list which items override
758 
759         :returns: A merged list of bdms
760         """
761         device_names = set(bdm['device_name'] for bdm in overrider_mappings
762                            if bdm['device_name'])
763         return (overrider_mappings +
764                 [bdm for bdm in overridable_mappings
765                  if bdm['device_name'] not in device_names])
766 
767     def _check_and_transform_bdm(self, context, base_options, instance_type,
768                                  image_meta, min_count, max_count,
769                                  block_device_mapping, legacy_bdm):
770         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
771         #                  It's needed for legacy conversion to work.
772         root_device_name = (base_options.get('root_device_name') or 'vda')
773         image_ref = base_options.get('image_ref', '')
774         # If the instance is booted by image and has a volume attached,
775         # the volume cannot have the same device name as root_device_name
776         if image_ref:
777             for bdm in block_device_mapping:
778                 if (bdm.get('destination_type') == 'volume' and
779                     block_device.strip_dev(bdm.get(
780                     'device_name')) == root_device_name):
781                     msg = _('The volume cannot be assigned the same device'
782                             ' name as the root device %s') % root_device_name
783                     raise exception.InvalidRequest(msg)
784 
785         image_defined_bdms = self._get_image_defined_bdms(
786             instance_type, image_meta, root_device_name)
787         root_in_image_bdms = (
788             block_device.get_root_bdm(image_defined_bdms) is not None)
789 
790         if legacy_bdm:
791             block_device_mapping = block_device.from_legacy_mapping(
792                 block_device_mapping, image_ref, root_device_name,
793                 no_root=root_in_image_bdms)
794         elif root_in_image_bdms:
795             # NOTE (ndipanov): client will insert an image mapping into the v2
796             # block_device_mapping, but if there is a bootable device in image
797             # mappings - we need to get rid of the inserted image
798             # NOTE (gibi): another case is when a server is booted with an
799             # image to bdm mapping where the image only contains a bdm to a
800             # snapshot. In this case the other image to bdm mapping
801             # contains an unnecessary device with boot_index == 0.
802             # Also in this case the image_ref is None as we are booting from
803             # an image to volume bdm.
804             def not_image_and_root_bdm(bdm):
805                 return not (bdm.get('boot_index') == 0 and
806                             bdm.get('source_type') == 'image')
807 
808             block_device_mapping = list(
809                 filter(not_image_and_root_bdm, block_device_mapping))
810 
811         block_device_mapping = self._merge_bdms_lists(
812             image_defined_bdms, block_device_mapping)
813 
814         if min_count > 1 or max_count > 1:
815             if any(map(lambda bdm: bdm['source_type'] == 'volume',
816                        block_device_mapping)):
817                 msg = _('Cannot attach one or more volumes to multiple'
818                         ' instances')
819                 raise exception.InvalidRequest(msg)
820 
821         block_device_mapping += self._get_flavor_defined_bdms(
822             instance_type, block_device_mapping)
823 
824         return block_device_obj.block_device_make_list_from_dicts(
825                 context, block_device_mapping)
826 
827     def _get_image(self, context, image_href):
828         if not image_href:
829             return None, {}
830 
831         image = self.image_api.get(context, image_href)
832         return image['id'], image
833 
834     def _checks_for_create_and_rebuild(self, context, image_id, image,
835                                        instance_type, metadata,
836                                        files_to_inject, root_bdm,
837                                        validate_numa=True):
838         self._check_metadata_properties_quota(context, metadata)
839         self._check_injected_file_quota(context, files_to_inject)
840         self._validate_flavor_image(context, image_id, image,
841                                     instance_type, root_bdm,
842                                     validate_numa=validate_numa)
843 
844     def _validate_and_build_base_options(self, context, instance_type,
845                                          boot_meta, image_href, image_id,
846                                          kernel_id, ramdisk_id, display_name,
847                                          display_description, key_name,
848                                          key_data, security_groups,
849                                          availability_zone, user_data,
850                                          metadata, access_ip_v4, access_ip_v6,
851                                          requested_networks, config_drive,
852                                          auto_disk_config, reservation_id,
853                                          max_count,
854                                          supports_port_resource_request):
855         """Verify all the input parameters regardless of the provisioning
856         strategy being performed.
857         """
858         if instance_type['disabled']:
859             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
860 
861         if user_data:
862             try:
863                 base64utils.decode_as_bytes(user_data)
864             except TypeError:
865                 raise exception.InstanceUserDataMalformed()
866 
867         # When using Neutron, _check_requested_secgroups will translate and
868         # return any requested security group names to uuids.
869         security_groups = (
870             self._check_requested_secgroups(context, security_groups))
871 
872         # Note:  max_count is the number of instances requested by the user,
873         # max_network_count is the maximum number of instances taking into
874         # account any network quotas
875         max_network_count = self._check_requested_networks(context,
876                                      requested_networks, max_count)
877 
878         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
879                 context, kernel_id, ramdisk_id, boot_meta)
880 
881         config_drive = self._check_config_drive(config_drive)
882 
883         if key_data is None and key_name is not None:
884             key_pair = objects.KeyPair.get_by_name(context,
885                                                    context.user_id,
886                                                    key_name)
887             key_data = key_pair.public_key
888         else:
889             key_pair = None
890 
891         root_device_name = block_device.prepend_dev(
892                 block_device.properties_root_device_name(
893                     boot_meta.get('properties', {})))
894 
895         image_meta = _get_image_meta_obj(boot_meta)
896         numa_topology = hardware.numa_get_constraints(
897                 instance_type, image_meta)
898 
899         system_metadata = {}
900 
901         # PCI requests come from two sources: instance flavor and
902         # requested_networks. The first call in below returns an
903         # InstancePCIRequests object which is a list of InstancePCIRequest
904         # objects. The second call in below creates an InstancePCIRequest
905         # object for each SR-IOV port, and append it to the list in the
906         # InstancePCIRequests object
907         pci_request_info = pci_request.get_pci_requests_from_flavor(
908             instance_type)
909         result = self.network_api.create_resource_requests(
910             context, requested_networks, pci_request_info)
911         network_metadata, port_resource_requests = result
912 
913         # Creating servers with ports that have resource requests, like QoS
914         # minimum bandwidth rules, is only supported in a requested minimum
915         # microversion.
916         if port_resource_requests and not supports_port_resource_request:
917             raise exception.CreateWithPortResourceRequestOldVersion()
918 
919         base_options = {
920             'reservation_id': reservation_id,
921             'image_ref': image_href,
922             'kernel_id': kernel_id or '',
923             'ramdisk_id': ramdisk_id or '',
924             'power_state': power_state.NOSTATE,
925             'vm_state': vm_states.BUILDING,
926             'config_drive': config_drive,
927             'user_id': context.user_id,
928             'project_id': context.project_id,
929             'instance_type_id': instance_type['id'],
930             'memory_mb': instance_type['memory_mb'],
931             'vcpus': instance_type['vcpus'],
932             'root_gb': instance_type['root_gb'],
933             'ephemeral_gb': instance_type['ephemeral_gb'],
934             'display_name': display_name,
935             'display_description': display_description,
936             'user_data': user_data,
937             'key_name': key_name,
938             'key_data': key_data,
939             'locked': False,
940             'metadata': metadata or {},
941             'access_ip_v4': access_ip_v4,
942             'access_ip_v6': access_ip_v6,
943             'availability_zone': availability_zone,
944             'root_device_name': root_device_name,
945             'progress': 0,
946             'pci_requests': pci_request_info,
947             'numa_topology': numa_topology,
948             'system_metadata': system_metadata,
949             'port_resource_requests': port_resource_requests}
950 
951         options_from_image = self._inherit_properties_from_image(
952                 boot_meta, auto_disk_config)
953 
954         base_options.update(options_from_image)
955 
956         # return the validated options and maximum number of instances allowed
957         # by the network quotas
958         return (base_options, max_network_count, key_pair, security_groups,
959                 network_metadata)
960 
961     @staticmethod
962     @db_api.api_context_manager.writer
963     def _create_reqspec_buildreq_instmapping(context, rs, br, im):
964         """Create the request spec, build request, and instance mapping in a
965         single database transaction.
966 
967         The RequestContext must be passed in to this method so that the
968         database transaction context manager decorator will nest properly and
969         include each create() into the same transaction context.
970         """
971         rs.create()
972         br.create()
973         im.create()
974 
975     def _validate_host_or_node(self, context, host, hypervisor_hostname):
976         """Check whether compute nodes exist by validating the host
977         and/or the hypervisor_hostname. There are three cases:
978         1. If only host is supplied, we can lookup the HostMapping in
979         the API DB.
980         2. If only node is supplied, we can query a resource provider
981         with that name in placement.
982         3. If both host and node are supplied, we can get the cell from
983         HostMapping and from that lookup the ComputeNode with the
984         given cell.
985 
986         :param context: The API request context.
987         :param host: Target host.
988         :param hypervisor_hostname: Target node.
989         :raises: ComputeHostNotFound if we find no compute nodes with host
990                  and/or hypervisor_hostname.
991         """
992 
993         if host:
994             # When host is specified.
995             try:
996                 host_mapping = objects.HostMapping.get_by_host(context, host)
997             except exception.HostMappingNotFound:
998                 LOG.warning('No host-to-cell mapping found for host '
999                             '%(host)s.', {'host': host})
1000                 raise exception.ComputeHostNotFound(host=host)
1001             # When both host and node are specified.
1002             if hypervisor_hostname:
1003                 cell = host_mapping.cell_mapping
1004                 with nova_context.target_cell(context, cell) as cctxt:
1005                     # Here we only do an existence check, so we don't
1006                     # need to store the return value into a variable.
1007                     objects.ComputeNode.get_by_host_and_nodename(
1008                         cctxt, host, hypervisor_hostname)
1009         elif hypervisor_hostname:
1010             # When only node is specified.
1011             try:
1012                 self.placementclient.get_provider_by_name(
1013                     context, hypervisor_hostname)
1014             except exception.ResourceProviderNotFound:
1015                 raise exception.ComputeHostNotFound(host=hypervisor_hostname)
1016 
1017     def _provision_instances(self, context, instance_type, min_count,
1018             max_count, base_options, boot_meta, security_groups,
1019             block_device_mapping, shutdown_terminate,
1020             instance_group, check_server_group_quota, filter_properties,
1021             key_pair, tags, trusted_certs, supports_multiattach,
1022             network_metadata=None, requested_host=None,
1023             requested_hypervisor_hostname=None):
1024         # NOTE(boxiang): Check whether compute nodes exist by validating
1025         # the host and/or the hypervisor_hostname. Pass the destination
1026         # to the scheduler with host and/or hypervisor_hostname(node).
1027         destination = None
1028         if requested_host or requested_hypervisor_hostname:
1029             self._validate_host_or_node(context, requested_host,
1030                                         requested_hypervisor_hostname)
1031             destination = objects.Destination()
1032             if requested_host:
1033                 destination.host = requested_host
1034             destination.node = requested_hypervisor_hostname
1035         # Check quotas
1036         num_instances = compute_utils.check_num_instances_quota(
1037                 context, instance_type, min_count, max_count)
1038         security_groups = self.security_group_api.populate_security_groups(
1039                 security_groups)
1040         self.security_group_api.ensure_default(context)
1041         port_resource_requests = base_options.pop('port_resource_requests')
1042         LOG.debug("Going to run %s instances...", num_instances)
1043         instances_to_build = []
1044         try:
1045             for i in range(num_instances):
1046                 # Create a uuid for the instance so we can store the
1047                 # RequestSpec before the instance is created.
1048                 instance_uuid = uuidutils.generate_uuid()
1049                 # Store the RequestSpec that will be used for scheduling.
1050                 req_spec = objects.RequestSpec.from_components(context,
1051                         instance_uuid, boot_meta, instance_type,
1052                         base_options['numa_topology'],
1053                         base_options['pci_requests'], filter_properties,
1054                         instance_group, base_options['availability_zone'],
1055                         security_groups=security_groups,
1056                         port_resource_requests=port_resource_requests)
1057 
1058                 if block_device_mapping:
1059                     # Record whether or not we are a BFV instance
1060                     root = block_device_mapping.root_bdm()
1061                     req_spec.is_bfv = bool(root and root.is_volume)
1062                 else:
1063                     # If we have no BDMs, we're clearly not BFV
1064                     req_spec.is_bfv = False
1065 
1066                 # NOTE(danms): We need to record num_instances on the request
1067                 # spec as this is how the conductor knows how many were in this
1068                 # batch.
1069                 req_spec.num_instances = num_instances
1070 
1071                 # NOTE(stephenfin): The network_metadata field is not persisted
1072                 # inside RequestSpec object.
1073                 if network_metadata:
1074                     req_spec.network_metadata = network_metadata
1075 
1076                 if destination:
1077                     req_spec.requested_destination = destination
1078 
1079                 # Create an instance object, but do not store in db yet.
1080                 instance = objects.Instance(context=context)
1081                 instance.uuid = instance_uuid
1082                 instance.update(base_options)
1083                 instance.keypairs = objects.KeyPairList(objects=[])
1084                 if key_pair:
1085                     instance.keypairs.objects.append(key_pair)
1086 
1087                 instance.trusted_certs = self._retrieve_trusted_certs_object(
1088                     context, trusted_certs)
1089 
1090                 instance = self.create_db_entry_for_new_instance(context,
1091                         instance_type, boot_meta, instance, security_groups,
1092                         block_device_mapping, num_instances, i,
1093                         shutdown_terminate, create_instance=False)
1094                 block_device_mapping = (
1095                     self._bdm_validate_set_size_and_instance(context,
1096                         instance, instance_type, block_device_mapping,
1097                         supports_multiattach))
1098                 instance_tags = self._transform_tags(tags, instance.uuid)
1099 
1100                 build_request = objects.BuildRequest(context,
1101                         instance=instance, instance_uuid=instance.uuid,
1102                         project_id=instance.project_id,
1103                         block_device_mappings=block_device_mapping,
1104                         tags=instance_tags)
1105 
1106                 # Create an instance_mapping.  The null cell_mapping indicates
1107                 # that the instance doesn't yet exist in a cell, and lookups
1108                 # for it need to instead look for the RequestSpec.
1109                 # cell_mapping will be populated after scheduling, with a
1110                 # scheduling failure using the cell_mapping for the special
1111                 # cell0.
1112                 inst_mapping = objects.InstanceMapping(context=context)
1113                 inst_mapping.instance_uuid = instance_uuid
1114                 inst_mapping.project_id = context.project_id
1115                 inst_mapping.user_id = context.user_id
1116                 inst_mapping.cell_mapping = None
1117 
1118                 # Create the request spec, build request, and instance mapping
1119                 # records in a single transaction so that if a DBError is
1120                 # raised from any of them, all INSERTs will be rolled back and
1121                 # no orphaned records will be left behind.
1122                 self._create_reqspec_buildreq_instmapping(context, req_spec,
1123                                                           build_request,
1124                                                           inst_mapping)
1125 
1126                 instances_to_build.append(
1127                     (req_spec, build_request, inst_mapping))
1128 
1129                 if instance_group:
1130                     if check_server_group_quota:
1131                         try:
1132                             objects.Quotas.check_deltas(
1133                                 context, {'server_group_members': 1},
1134                                 instance_group, context.user_id)
1135                         except exception.OverQuota:
1136                             msg = _("Quota exceeded, too many servers in "
1137                                     "group")
1138                             raise exception.QuotaError(msg)
1139 
1140                     members = objects.InstanceGroup.add_members(
1141                         context, instance_group.uuid, [instance.uuid])
1142 
1143                     # NOTE(melwitt): We recheck the quota after creating the
1144                     # object to prevent users from allocating more resources
1145                     # than their allowed quota in the event of a race. This is
1146                     # configurable because it can be expensive if strict quota
1147                     # limits are not required in a deployment.
1148                     if CONF.quota.recheck_quota and check_server_group_quota:
1149                         try:
1150                             objects.Quotas.check_deltas(
1151                                 context, {'server_group_members': 0},
1152                                 instance_group, context.user_id)
1153                         except exception.OverQuota:
1154                             objects.InstanceGroup._remove_members_in_db(
1155                                 context, instance_group.id, [instance.uuid])
1156                             msg = _("Quota exceeded, too many servers in "
1157                                     "group")
1158                             raise exception.QuotaError(msg)
1159                     # list of members added to servers group in this iteration
1160                     # is needed to check quota of server group during add next
1161                     # instance
1162                     instance_group.members.extend(members)
1163 
1164         # In the case of any exceptions, attempt DB cleanup
1165         except Exception:
1166             with excutils.save_and_reraise_exception():
1167                 self._cleanup_build_artifacts(None, instances_to_build)
1168 
1169         return instances_to_build
1170 
1171     @staticmethod
1172     def _retrieve_trusted_certs_object(context, trusted_certs, rebuild=False):
1173         """Convert user-requested trusted cert IDs to TrustedCerts object
1174 
1175         Also validates that the deployment is new enough to support trusted
1176         image certification validation.
1177 
1178         :param context: The user request auth context
1179         :param trusted_certs: list of user-specified trusted cert string IDs,
1180             may be None
1181         :param rebuild: True if rebuilding the server, False if creating a
1182             new server
1183         :returns: nova.objects.TrustedCerts object or None if no user-specified
1184             trusted cert IDs were given and nova is not configured with
1185             default trusted cert IDs
1186         """
1187         # Retrieve trusted_certs parameter, or use CONF value if certificate
1188         # validation is enabled
1189         if trusted_certs:
1190             certs_to_return = objects.TrustedCerts(ids=trusted_certs)
1191         elif (CONF.glance.verify_glance_signatures and
1192               CONF.glance.enable_certificate_validation and
1193               CONF.glance.default_trusted_certificate_ids):
1194             certs_to_return = objects.TrustedCerts(
1195                 ids=CONF.glance.default_trusted_certificate_ids)
1196         else:
1197             return None
1198 
1199         return certs_to_return
1200 
1201     def _get_bdm_image_metadata(self, context, block_device_mapping,
1202                                 legacy_bdm=True):
1203         """If we are booting from a volume, we need to get the
1204         volume details from Cinder and make sure we pass the
1205         metadata back accordingly.
1206         """
1207         if not block_device_mapping:
1208             return {}
1209 
1210         for bdm in block_device_mapping:
1211             if (legacy_bdm and
1212                     block_device.get_device_letter(
1213                        bdm.get('device_name', '')) != 'a'):
1214                 continue
1215             elif not legacy_bdm and bdm.get('boot_index') != 0:
1216                 continue
1217 
1218             volume_id = bdm.get('volume_id')
1219             snapshot_id = bdm.get('snapshot_id')
1220             if snapshot_id:
1221                 # NOTE(alaski): A volume snapshot inherits metadata from the
1222                 # originating volume, but the API does not expose metadata
1223                 # on the snapshot itself.  So we query the volume for it below.
1224                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1225                 volume_id = snapshot['volume_id']
1226 
1227             if bdm.get('image_id'):
1228                 try:
1229                     image_id = bdm['image_id']
1230                     image_meta = self.image_api.get(context, image_id)
1231                     return image_meta
1232                 except Exception:
1233                     raise exception.InvalidBDMImage(id=image_id)
1234             elif volume_id:
1235                 try:
1236                     volume = self.volume_api.get(context, volume_id)
1237                 except exception.CinderConnectionFailed:
1238                     raise
1239                 except Exception:
1240                     raise exception.InvalidBDMVolume(id=volume_id)
1241 
1242                 if not volume.get('bootable', True):
1243                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1244 
1245                 return utils.get_image_metadata_from_volume(volume)
1246         return {}
1247 
1248     @staticmethod
1249     def _get_requested_instance_group(context, filter_properties):
1250         if (not filter_properties or
1251                 not filter_properties.get('scheduler_hints')):
1252             return
1253 
1254         group_hint = filter_properties.get('scheduler_hints').get('group')
1255         if not group_hint:
1256             return
1257 
1258         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1259 
1260     def _create_instance(self, context, instance_type,
1261                image_href, kernel_id, ramdisk_id,
1262                min_count, max_count,
1263                display_name, display_description,
1264                key_name, key_data, security_groups,
1265                availability_zone, user_data, metadata, injected_files,
1266                admin_password, access_ip_v4, access_ip_v6,
1267                requested_networks, config_drive,
1268                block_device_mapping, auto_disk_config, filter_properties,
1269                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1270                check_server_group_quota=False, tags=None,
1271                supports_multiattach=False, trusted_certs=None,
1272                supports_port_resource_request=False,
1273                requested_host=None, requested_hypervisor_hostname=None):
1274         """Verify all the input parameters regardless of the provisioning
1275         strategy being performed and schedule the instance(s) for
1276         creation.
1277         """
1278 
1279         # Normalize and setup some parameters
1280         if reservation_id is None:
1281             reservation_id = utils.generate_uid('r')
1282         security_groups = security_groups or ['default']
1283         min_count = min_count or 1
1284         max_count = max_count or min_count
1285         block_device_mapping = block_device_mapping or []
1286         tags = tags or []
1287 
1288         if image_href:
1289             image_id, boot_meta = self._get_image(context, image_href)
1290         else:
1291             # This is similar to the logic in _retrieve_trusted_certs_object.
1292             if (trusted_certs or
1293                 (CONF.glance.verify_glance_signatures and
1294                  CONF.glance.enable_certificate_validation and
1295                  CONF.glance.default_trusted_certificate_ids)):
1296                 msg = _("Image certificate validation is not supported "
1297                         "when booting from volume")
1298                 raise exception.CertificateValidationFailed(message=msg)
1299             image_id = None
1300             boot_meta = self._get_bdm_image_metadata(
1301                 context, block_device_mapping, legacy_bdm)
1302 
1303         self._check_auto_disk_config(image=boot_meta,
1304                                      auto_disk_config=auto_disk_config)
1305 
1306         base_options, max_net_count, key_pair, security_groups, \
1307             network_metadata = self._validate_and_build_base_options(
1308                     context, instance_type, boot_meta, image_href, image_id,
1309                     kernel_id, ramdisk_id, display_name, display_description,
1310                     key_name, key_data, security_groups, availability_zone,
1311                     user_data, metadata, access_ip_v4, access_ip_v6,
1312                     requested_networks, config_drive, auto_disk_config,
1313                     reservation_id, max_count, supports_port_resource_request)
1314 
1315         # max_net_count is the maximum number of instances requested by the
1316         # user adjusted for any network quota constraints, including
1317         # consideration of connections to each requested network
1318         if max_net_count < min_count:
1319             raise exception.PortLimitExceeded()
1320         elif max_net_count < max_count:
1321             LOG.info("max count reduced from %(max_count)d to "
1322                      "%(max_net_count)d due to network port quota",
1323                      {'max_count': max_count,
1324                       'max_net_count': max_net_count})
1325             max_count = max_net_count
1326 
1327         block_device_mapping = self._check_and_transform_bdm(context,
1328             base_options, instance_type, boot_meta, min_count, max_count,
1329             block_device_mapping, legacy_bdm)
1330 
1331         # We can't do this check earlier because we need bdms from all sources
1332         # to have been merged in order to get the root bdm.
1333         # Set validate_numa=False since numa validation is already done by
1334         # _validate_and_build_base_options().
1335         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1336                 instance_type, metadata, injected_files,
1337                 block_device_mapping.root_bdm(), validate_numa=False)
1338 
1339         instance_group = self._get_requested_instance_group(context,
1340                                    filter_properties)
1341 
1342         tags = self._create_tag_list_obj(context, tags)
1343 
1344         instances_to_build = self._provision_instances(
1345             context, instance_type, min_count, max_count, base_options,
1346             boot_meta, security_groups, block_device_mapping,
1347             shutdown_terminate, instance_group, check_server_group_quota,
1348             filter_properties, key_pair, tags, trusted_certs,
1349             supports_multiattach, network_metadata,
1350             requested_host, requested_hypervisor_hostname)
1351 
1352         instances = []
1353         request_specs = []
1354         build_requests = []
1355         for rs, build_request, im in instances_to_build:
1356             build_requests.append(build_request)
1357             instance = build_request.get_new_instance(context)
1358             instances.append(instance)
1359             request_specs.append(rs)
1360 
1361         self.compute_task_api.schedule_and_build_instances(
1362             context,
1363             build_requests=build_requests,
1364             request_spec=request_specs,
1365             image=boot_meta,
1366             admin_password=admin_password,
1367             injected_files=injected_files,
1368             requested_networks=requested_networks,
1369             block_device_mapping=block_device_mapping,
1370             tags=tags)
1371 
1372         return instances, reservation_id
1373 
1374     @staticmethod
1375     def _cleanup_build_artifacts(instances, instances_to_build):
1376         # instances_to_build is a list of tuples:
1377         # (RequestSpec, BuildRequest, InstanceMapping)
1378 
1379         # Be paranoid about artifacts being deleted underneath us.
1380         for instance in instances or []:
1381             try:
1382                 instance.destroy()
1383             except exception.InstanceNotFound:
1384                 pass
1385         for rs, build_request, im in instances_to_build or []:
1386             try:
1387                 rs.destroy()
1388             except exception.RequestSpecNotFound:
1389                 pass
1390             try:
1391                 build_request.destroy()
1392             except exception.BuildRequestNotFound:
1393                 pass
1394             try:
1395                 im.destroy()
1396             except exception.InstanceMappingNotFound:
1397                 pass
1398 
1399     @staticmethod
1400     def _volume_size(instance_type, bdm):
1401         size = bdm.get('volume_size')
1402         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1403         if (size is None and bdm.get('source_type') == 'blank' and
1404                 bdm.get('destination_type') == 'local'):
1405             if bdm.get('guest_format') == 'swap':
1406                 size = instance_type.get('swap', 0)
1407             else:
1408                 size = instance_type.get('ephemeral_gb', 0)
1409         return size
1410 
1411     def _prepare_image_mapping(self, instance_type, mappings):
1412         """Extract and format blank devices from image mappings."""
1413 
1414         prepared_mappings = []
1415 
1416         for bdm in block_device.mappings_prepend_dev(mappings):
1417             LOG.debug("Image bdm %s", bdm)
1418 
1419             virtual_name = bdm['virtual']
1420             if virtual_name == 'ami' or virtual_name == 'root':
1421                 continue
1422 
1423             if not block_device.is_swap_or_ephemeral(virtual_name):
1424                 continue
1425 
1426             guest_format = bdm.get('guest_format')
1427             if virtual_name == 'swap':
1428                 guest_format = 'swap'
1429             if not guest_format:
1430                 guest_format = CONF.default_ephemeral_format
1431 
1432             values = block_device.BlockDeviceDict({
1433                 'device_name': bdm['device'],
1434                 'source_type': 'blank',
1435                 'destination_type': 'local',
1436                 'device_type': 'disk',
1437                 'guest_format': guest_format,
1438                 'delete_on_termination': True,
1439                 'boot_index': -1})
1440 
1441             values['volume_size'] = self._volume_size(
1442                 instance_type, values)
1443             if values['volume_size'] == 0:
1444                 continue
1445 
1446             prepared_mappings.append(values)
1447 
1448         return prepared_mappings
1449 
1450     def _bdm_validate_set_size_and_instance(self, context, instance,
1451                                             instance_type,
1452                                             block_device_mapping,
1453                                             supports_multiattach=False):
1454         """Ensure the bdms are valid, then set size and associate with instance
1455 
1456         Because this method can be called multiple times when more than one
1457         instance is booted in a single request it makes a copy of the bdm list.
1458         """
1459         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1460                   instance_uuid=instance.uuid)
1461         self._validate_bdm(
1462             context, instance, instance_type, block_device_mapping,
1463             supports_multiattach)
1464         instance_block_device_mapping = block_device_mapping.obj_clone()
1465         for bdm in instance_block_device_mapping:
1466             bdm.volume_size = self._volume_size(instance_type, bdm)
1467             bdm.instance_uuid = instance.uuid
1468         return instance_block_device_mapping
1469 
1470     def _create_block_device_mapping(self, block_device_mapping):
1471         # Copy the block_device_mapping because this method can be called
1472         # multiple times when more than one instance is booted in a single
1473         # request. This avoids 'id' being set and triggering the object dupe
1474         # detection
1475         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1476         # Create the BlockDeviceMapping objects in the db.
1477         for bdm in db_block_device_mapping:
1478             # TODO(alaski): Why is this done?
1479             if bdm.volume_size == 0:
1480                 continue
1481 
1482             bdm.update_or_create()
1483 
1484     @staticmethod
1485     def _check_requested_volume_type(bdm, volume_type_id_or_name,
1486                                      volume_types):
1487         """If we are specifying a volume type, we need to get the
1488         volume type details from Cinder and make sure the ``volume_type``
1489         is available.
1490         """
1491 
1492         # NOTE(brinzhang): Verify that the specified volume type exists.
1493         # And save the volume type name internally for consistency in the
1494         # BlockDeviceMapping object.
1495         for vol_type in volume_types:
1496             if (volume_type_id_or_name == vol_type['id'] or
1497                         volume_type_id_or_name == vol_type['name']):
1498                 bdm.volume_type = vol_type['name']
1499                 break
1500         else:
1501             raise exception.VolumeTypeNotFound(
1502                 id_or_name=volume_type_id_or_name)
1503 
1504     @staticmethod
1505     def _check_compute_supports_volume_type(context):
1506         # NOTE(brinzhang): Checking the minimum nova-compute service
1507         # version across the deployment. Just make sure the volume
1508         # type can be supported when the bdm.volume_type is requested.
1509         min_compute_version = objects.service.get_minimum_version_all_cells(
1510             context, ['nova-compute'])
1511         if min_compute_version < MIN_COMPUTE_VOLUME_TYPE:
1512             raise exception.VolumeTypeSupportNotYetAvailable()
1513 
1514     def _validate_bdm(self, context, instance, instance_type,
1515                       block_device_mappings, supports_multiattach=False):
1516         # Make sure that the boot indexes make sense.
1517         # Setting a negative value or None indicates that the device should not
1518         # be used for booting.
1519         boot_indexes = sorted([bdm.boot_index
1520                                for bdm in block_device_mappings
1521                                if bdm.boot_index is not None and
1522                                bdm.boot_index >= 0])
1523 
1524         # Each device which is capable of being used as boot device should
1525         # be given a unique boot index, starting from 0 in ascending order, and
1526         # there needs to be at least one boot device.
1527         if not boot_indexes or any(i != v for i, v in enumerate(boot_indexes)):
1528             # Convert the BlockDeviceMappingList to a list for repr details.
1529             LOG.debug('Invalid block device mapping boot sequence for '
1530                       'instance: %s', list(block_device_mappings),
1531                       instance=instance)
1532             raise exception.InvalidBDMBootSequence()
1533 
1534         volume_types = None
1535         volume_type_is_supported = False
1536         for bdm in block_device_mappings:
1537             volume_type = bdm.volume_type
1538             if volume_type:
1539                 if not volume_type_is_supported:
1540                     # The following method raises
1541                     # VolumeTypeSupportNotYetAvailable if the minimum
1542                     # nova-compute service version across the deployment is
1543                     # not new enough to support creating volumes with a
1544                     # specific type.
1545                     self._check_compute_supports_volume_type(context)
1546                     # Set the flag to avoid calling
1547                     # _check_compute_supports_volume_type more than once in
1548                     # this for loop.
1549                     volume_type_is_supported = True
1550 
1551                 if not volume_types:
1552                     # In order to reduce the number of hit cinder APIs,
1553                     # initialize our cache of volume types.
1554                     volume_types = self.volume_api.get_all_volume_types(
1555                         context)
1556                 # NOTE(brinzhang): Ensure the validity of volume_type.
1557                 self._check_requested_volume_type(bdm, volume_type,
1558                                                   volume_types)
1559 
1560             # NOTE(vish): For now, just make sure the volumes are accessible.
1561             # Additionally, check that the volume can be attached to this
1562             # instance.
1563             snapshot_id = bdm.snapshot_id
1564             volume_id = bdm.volume_id
1565             image_id = bdm.image_id
1566             if image_id is not None:
1567                 if image_id != instance.get('image_ref'):
1568                     try:
1569                         self._get_image(context, image_id)
1570                     except Exception:
1571                         raise exception.InvalidBDMImage(id=image_id)
1572                 if (bdm.source_type == 'image' and
1573                         bdm.destination_type == 'volume' and
1574                         not bdm.volume_size):
1575                     raise exception.InvalidBDM(message=_("Images with "
1576                         "destination_type 'volume' need to have a non-zero "
1577                         "size specified"))
1578             elif volume_id is not None:
1579                 try:
1580                     volume = self.volume_api.get(context, volume_id)
1581                     self._check_attach_and_reserve_volume(
1582                         context, volume, instance, bdm, supports_multiattach)
1583                     bdm.volume_size = volume.get('size')
1584 
1585                     # NOTE(mnaser): If we end up reserving the volume, it will
1586                     #               not have an attachment_id which is needed
1587                     #               for cleanups.  This can be removed once
1588                     #               all calls to reserve_volume are gone.
1589                     if 'attachment_id' not in bdm:
1590                         bdm.attachment_id = None
1591                 except (exception.CinderConnectionFailed,
1592                         exception.InvalidVolume,
1593                         exception.MultiattachNotSupportedOldMicroversion):
1594                     raise
1595                 except exception.InvalidInput as exc:
1596                     raise exception.InvalidVolume(reason=exc.format_message())
1597                 except Exception:
1598                     raise exception.InvalidBDMVolume(id=volume_id)
1599             elif snapshot_id is not None:
1600                 try:
1601                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1602                     bdm.volume_size = bdm.volume_size or snap.get('size')
1603                 except exception.CinderConnectionFailed:
1604                     raise
1605                 except Exception:
1606                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1607             elif (bdm.source_type == 'blank' and
1608                     bdm.destination_type == 'volume' and
1609                     not bdm.volume_size):
1610                 raise exception.InvalidBDM(message=_("Blank volumes "
1611                     "(source: 'blank', dest: 'volume') need to have non-zero "
1612                     "size"))
1613 
1614         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1615                 for bdm in block_device_mappings
1616                 if block_device.new_format_is_ephemeral(bdm))
1617         if ephemeral_size > instance_type['ephemeral_gb']:
1618             raise exception.InvalidBDMEphemeralSize()
1619 
1620         # There should be only one swap
1621         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1622         if len(swap_list) > 1:
1623             msg = _("More than one swap drive requested.")
1624             raise exception.InvalidBDMFormat(details=msg)
1625 
1626         if swap_list:
1627             swap_size = swap_list[0].volume_size or 0
1628             if swap_size > instance_type['swap']:
1629                 raise exception.InvalidBDMSwapSize()
1630 
1631         max_local = CONF.max_local_block_devices
1632         if max_local >= 0:
1633             num_local = len([bdm for bdm in block_device_mappings
1634                              if bdm.destination_type == 'local'])
1635             if num_local > max_local:
1636                 raise exception.InvalidBDMLocalsLimit()
1637 
1638     def _populate_instance_names(self, instance, num_instances, index):
1639         """Populate instance display_name and hostname.
1640 
1641         :param instance: The instance to set the display_name, hostname for
1642         :type instance: nova.objects.Instance
1643         :param num_instances: Total number of instances being created in this
1644             request
1645         :param index: The 0-based index of this particular instance
1646         """
1647         # NOTE(mriedem): This is only here for test simplicity since a server
1648         # name is required in the REST API.
1649         if 'display_name' not in instance or instance.display_name is None:
1650             instance.display_name = 'Server %s' % instance.uuid
1651 
1652         # if we're booting multiple instances, we need to add an indexing
1653         # suffix to both instance.hostname and instance.display_name. This is
1654         # not necessary for a single instance.
1655         if num_instances == 1:
1656             default_hostname = 'Server-%s' % instance.uuid
1657             instance.hostname = utils.sanitize_hostname(
1658                 instance.display_name, default_hostname)
1659         elif num_instances > 1:
1660             old_display_name = instance.display_name
1661             new_display_name = '%s-%d' % (old_display_name, index + 1)
1662 
1663             if utils.sanitize_hostname(old_display_name) == "":
1664                 instance.hostname = 'Server-%s' % instance.uuid
1665             else:
1666                 instance.hostname = utils.sanitize_hostname(
1667                     new_display_name)
1668 
1669             instance.display_name = new_display_name
1670 
1671     def _populate_instance_for_create(self, context, instance, image,
1672                                       index, security_groups, instance_type,
1673                                       num_instances, shutdown_terminate):
1674         """Build the beginning of a new instance."""
1675 
1676         instance.launch_index = index
1677         instance.vm_state = vm_states.BUILDING
1678         instance.task_state = task_states.SCHEDULING
1679         info_cache = objects.InstanceInfoCache()
1680         info_cache.instance_uuid = instance.uuid
1681         info_cache.network_info = network_model.NetworkInfo()
1682         instance.info_cache = info_cache
1683         instance.flavor = instance_type
1684         instance.old_flavor = None
1685         instance.new_flavor = None
1686         if CONF.ephemeral_storage_encryption.enabled:
1687             # NOTE(kfarr): dm-crypt expects the cipher in a
1688             # hyphenated format: cipher-chainmode-ivmode
1689             # (ex: aes-xts-plain64). The algorithm needs
1690             # to be parsed out to pass to the key manager (ex: aes).
1691             cipher = CONF.ephemeral_storage_encryption.cipher
1692             algorithm = cipher.split('-')[0] if cipher else None
1693             instance.ephemeral_key_uuid = self.key_manager.create_key(
1694                 context,
1695                 algorithm=algorithm,
1696                 length=CONF.ephemeral_storage_encryption.key_size)
1697         else:
1698             instance.ephemeral_key_uuid = None
1699 
1700         # Store image properties so we can use them later
1701         # (for notifications, etc).  Only store what we can.
1702         if not instance.obj_attr_is_set('system_metadata'):
1703             instance.system_metadata = {}
1704         # Make sure we have the dict form that we need for instance_update.
1705         instance.system_metadata = utils.instance_sys_meta(instance)
1706 
1707         system_meta = utils.get_system_metadata_from_image(
1708             image, instance_type)
1709 
1710         # In case we couldn't find any suitable base_image
1711         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1712 
1713         system_meta['owner_user_name'] = context.user_name
1714         system_meta['owner_project_name'] = context.project_name
1715 
1716         instance.system_metadata.update(system_meta)
1717 
1718         if CONF.use_neutron:
1719             # For Neutron we don't actually store anything in the database, we
1720             # proxy the security groups on the instance from the ports
1721             # attached to the instance.
1722             instance.security_groups = objects.SecurityGroupList()
1723         else:
1724             instance.security_groups = security_groups
1725 
1726         self._populate_instance_names(instance, num_instances, index)
1727         instance.shutdown_terminate = shutdown_terminate
1728 
1729         return instance
1730 
1731     def _create_tag_list_obj(self, context, tags):
1732         """Create TagList objects from simple string tags.
1733 
1734         :param context: security context.
1735         :param tags: simple string tags from API request.
1736         :returns: TagList object.
1737         """
1738         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1739         tag_list_obj = objects.TagList(objects=tag_list)
1740         return tag_list_obj
1741 
1742     def _transform_tags(self, tags, resource_id):
1743         """Change the resource_id of the tags according to the input param.
1744 
1745         Because this method can be called multiple times when more than one
1746         instance is booted in a single request it makes a copy of the tags
1747         list.
1748 
1749         :param tags: TagList object.
1750         :param resource_id: string.
1751         :returns: TagList object.
1752         """
1753         instance_tags = tags.obj_clone()
1754         for tag in instance_tags:
1755             tag.resource_id = resource_id
1756         return instance_tags
1757 
1758     # This method remains because cellsv1 uses it in the scheduler
1759     def create_db_entry_for_new_instance(self, context, instance_type, image,
1760             instance, security_group, block_device_mapping, num_instances,
1761             index, shutdown_terminate=False, create_instance=True):
1762         """Create an entry in the DB for this new instance,
1763         including any related table updates (such as security group,
1764         etc).
1765 
1766         This is called by the scheduler after a location for the
1767         instance has been determined.
1768 
1769         :param create_instance: Determines if the instance is created here or
1770             just populated for later creation. This is done so that this code
1771             can be shared with cellsv1 which needs the instance creation to
1772             happen here. It should be removed and this method cleaned up when
1773             cellsv1 is a distant memory.
1774         """
1775         self._populate_instance_for_create(context, instance, image, index,
1776                                            security_group, instance_type,
1777                                            num_instances, shutdown_terminate)
1778 
1779         if create_instance:
1780             instance.create()
1781 
1782         return instance
1783 
1784     def _check_multiple_instances_with_neutron_ports(self,
1785                                                      requested_networks):
1786         """Check whether multiple instances are created from port id(s)."""
1787         for requested_net in requested_networks:
1788             if requested_net.port_id:
1789                 msg = _("Unable to launch multiple instances with"
1790                         " a single configured port ID. Please launch your"
1791                         " instance one by one with different ports.")
1792                 raise exception.MultiplePortsNotApplicable(reason=msg)
1793 
1794     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1795         """Check whether multiple instances are created with specified ip."""
1796 
1797         for requested_net in requested_networks:
1798             if requested_net.network_id and requested_net.address:
1799                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1800                         "is specified.")
1801                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1802 
1803     @hooks.add_hook("create_instance")
1804     def create(self, context, instance_type,
1805                image_href, kernel_id=None, ramdisk_id=None,
1806                min_count=None, max_count=None,
1807                display_name=None, display_description=None,
1808                key_name=None, key_data=None, security_groups=None,
1809                availability_zone=None, forced_host=None, forced_node=None,
1810                user_data=None, metadata=None, injected_files=None,
1811                admin_password=None, block_device_mapping=None,
1812                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1813                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1814                legacy_bdm=True, shutdown_terminate=False,
1815                check_server_group_quota=False, tags=None,
1816                supports_multiattach=False, trusted_certs=None,
1817                supports_port_resource_request=False,
1818                requested_host=None, requested_hypervisor_hostname=None):
1819         """Provision instances, sending instance information to the
1820         scheduler.  The scheduler will determine where the instance(s)
1821         go and will handle creating the DB entries.
1822 
1823         Returns a tuple of (instances, reservation_id)
1824         """
1825         if requested_networks and max_count is not None and max_count > 1:
1826             self._check_multiple_instances_with_specified_ip(
1827                 requested_networks)
1828             if utils.is_neutron():
1829                 self._check_multiple_instances_with_neutron_ports(
1830                     requested_networks)
1831 
1832         if availability_zone:
1833             available_zones = availability_zones.\
1834                 get_availability_zones(context.elevated(), self.host_api,
1835                                        get_only_available=True)
1836             if forced_host is None and availability_zone not in \
1837                     available_zones:
1838                 msg = _('The requested availability zone is not available')
1839                 raise exception.InvalidRequest(msg)
1840 
1841         filter_properties = scheduler_utils.build_filter_properties(
1842                 scheduler_hints, forced_host, forced_node, instance_type)
1843 
1844         return self._create_instance(
1845             context, instance_type,
1846             image_href, kernel_id, ramdisk_id,
1847             min_count, max_count,
1848             display_name, display_description,
1849             key_name, key_data, security_groups,
1850             availability_zone, user_data, metadata,
1851             injected_files, admin_password,
1852             access_ip_v4, access_ip_v6,
1853             requested_networks, config_drive,
1854             block_device_mapping, auto_disk_config,
1855             filter_properties=filter_properties,
1856             legacy_bdm=legacy_bdm,
1857             shutdown_terminate=shutdown_terminate,
1858             check_server_group_quota=check_server_group_quota,
1859             tags=tags, supports_multiattach=supports_multiattach,
1860             trusted_certs=trusted_certs,
1861             supports_port_resource_request=supports_port_resource_request,
1862             requested_host=requested_host,
1863             requested_hypervisor_hostname=requested_hypervisor_hostname)
1864 
1865     def _check_auto_disk_config(self, instance=None, image=None,
1866                                 **extra_instance_updates):
1867         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1868         if auto_disk_config is None:
1869             return
1870         if not image and not instance:
1871             return
1872 
1873         if image:
1874             image_props = image.get("properties", {})
1875             auto_disk_config_img = \
1876                 utils.get_auto_disk_config_from_image_props(image_props)
1877             image_ref = image.get("id")
1878         else:
1879             sys_meta = utils.instance_sys_meta(instance)
1880             image_ref = sys_meta.get('image_base_image_ref')
1881             auto_disk_config_img = \
1882                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1883 
1884         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1885                                                auto_disk_config,
1886                                                image_ref)
1887 
1888     def _lookup_instance(self, context, uuid):
1889         '''Helper method for pulling an instance object from a database.
1890 
1891         During the transition to cellsv2 there is some complexity around
1892         retrieving an instance from the database which this method hides. If
1893         there is an instance mapping then query the cell for the instance, if
1894         no mapping exists then query the configured nova database.
1895 
1896         Once we are past the point that all deployments can be assumed to be
1897         migrated to cellsv2 this method can go away.
1898         '''
1899         inst_map = None
1900         try:
1901             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1902                 context, uuid)
1903         except exception.InstanceMappingNotFound:
1904             # TODO(alaski): This exception block can be removed once we're
1905             # guaranteed everyone is using cellsv2.
1906             pass
1907 
1908         if inst_map is None or inst_map.cell_mapping is None:
1909             # If inst_map is None then the deployment has not migrated to
1910             # cellsv2 yet.
1911             # If inst_map.cell_mapping is None then the instance is not in a
1912             # cell yet. Until instance creation moves to the conductor the
1913             # instance can be found in the configured database, so attempt
1914             # to look it up.
1915             cell = None
1916             try:
1917                 instance = objects.Instance.get_by_uuid(context, uuid)
1918             except exception.InstanceNotFound:
1919                 # If we get here then the conductor is in charge of writing the
1920                 # instance to the database and hasn't done that yet. It's up to
1921                 # the caller of this method to determine what to do with that
1922                 # information.
1923                 return None, None
1924         else:
1925             cell = inst_map.cell_mapping
1926             with nova_context.target_cell(context, cell) as cctxt:
1927                 try:
1928                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1929                 except exception.InstanceNotFound:
1930                     # Since the cell_mapping exists we know the instance is in
1931                     # the cell, however InstanceNotFound means it's already
1932                     # deleted.
1933                     return None, None
1934         return cell, instance
1935 
1936     def _delete_while_booting(self, context, instance):
1937         """Handle deletion if the instance has not reached a cell yet
1938 
1939         Deletion before an instance reaches a cell needs to be handled
1940         differently. What we're attempting to do is delete the BuildRequest
1941         before the api level conductor does.  If we succeed here then the boot
1942         request stops before reaching a cell.  If not then the instance will
1943         need to be looked up in a cell db and the normal delete path taken.
1944         """
1945         deleted = self._attempt_delete_of_buildrequest(context, instance)
1946         if deleted:
1947             # If we've reached this block the successful deletion of the
1948             # buildrequest indicates that the build process should be halted by
1949             # the conductor.
1950 
1951             # NOTE(alaski): Though the conductor halts the build process it
1952             # does not currently delete the instance record. This is
1953             # because in the near future the instance record will not be
1954             # created if the buildrequest has been deleted here. For now we
1955             # ensure the instance has been set to deleted at this point.
1956             # Yes this directly contradicts the comment earlier in this
1957             # method, but this is a temporary measure.
1958             # Look up the instance because the current instance object was
1959             # stashed on the buildrequest and therefore not complete enough
1960             # to run .destroy().
1961             try:
1962                 instance_uuid = instance.uuid
1963                 cell, instance = self._lookup_instance(context, instance_uuid)
1964                 if instance is not None:
1965                     # If instance is None it has already been deleted.
1966                     if cell:
1967                         with nova_context.target_cell(context, cell) as cctxt:
1968                             # FIXME: When the instance context is targeted,
1969                             # we can remove this
1970                             with compute_utils.notify_about_instance_delete(
1971                                     self.notifier, cctxt, instance):
1972                                 instance.destroy()
1973                     else:
1974                         instance.destroy()
1975             except exception.InstanceNotFound:
1976                 pass
1977 
1978             return True
1979         return False
1980 
1981     def _attempt_delete_of_buildrequest(self, context, instance):
1982         # If there is a BuildRequest then the instance may not have been
1983         # written to a cell db yet. Delete the BuildRequest here, which
1984         # will indicate that the Instance build should not proceed.
1985         try:
1986             build_req = objects.BuildRequest.get_by_instance_uuid(
1987                 context, instance.uuid)
1988             build_req.destroy()
1989         except exception.BuildRequestNotFound:
1990             # This means that conductor has deleted the BuildRequest so the
1991             # instance is now in a cell and the delete needs to proceed
1992             # normally.
1993             return False
1994 
1995         # We need to detach from any volumes so they aren't orphaned.
1996         self._local_cleanup_bdm_volumes(
1997             build_req.block_device_mappings, instance, context)
1998 
1999         return True
2000 
2001     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
2002         if instance.disable_terminate:
2003             LOG.info('instance termination disabled', instance=instance)
2004             return
2005 
2006         cell = None
2007         # If there is an instance.host (or the instance is shelved-offloaded or
2008         # in error state), the instance has been scheduled and sent to a
2009         # cell/compute which means it was pulled from the cell db.
2010         # Normal delete should be attempted.
2011         may_have_ports_or_volumes = compute_utils.may_have_ports_or_volumes(
2012             instance)
2013         if not instance.host and not may_have_ports_or_volumes:
2014             try:
2015                 if self._delete_while_booting(context, instance):
2016                     return
2017                 # If instance.host was not set it's possible that the Instance
2018                 # object here was pulled from a BuildRequest object and is not
2019                 # fully populated. Notably it will be missing an 'id' field
2020                 # which will prevent instance.destroy from functioning
2021                 # properly. A lookup is attempted which will either return a
2022                 # full Instance or None if not found. If not found then it's
2023                 # acceptable to skip the rest of the delete processing.
2024                 cell, instance = self._lookup_instance(context, instance.uuid)
2025                 if cell and instance:
2026                     try:
2027                         # Now destroy the instance from the cell it lives in.
2028                         with compute_utils.notify_about_instance_delete(
2029                                 self.notifier, context, instance):
2030                             instance.destroy()
2031                     except exception.InstanceNotFound:
2032                         pass
2033                     # The instance was deleted or is already gone.
2034                     return
2035                 if not instance:
2036                     # Instance is already deleted.
2037                     return
2038             except exception.ObjectActionError:
2039                 # NOTE(melwitt): This means the instance.host changed
2040                 # under us indicating the instance became scheduled
2041                 # during the destroy(). Refresh the instance from the DB and
2042                 # continue on with the delete logic for a scheduled instance.
2043                 # NOTE(danms): If instance.host is set, we should be able to
2044                 # do the following lookup. If not, there's not much we can
2045                 # do to recover.
2046                 cell, instance = self._lookup_instance(context, instance.uuid)
2047                 if not instance:
2048                     # Instance is already deleted
2049                     return
2050 
2051         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2052                 context, instance.uuid)
2053 
2054         # At these states an instance has a snapshot associate.
2055         if instance.vm_state in (vm_states.SHELVED,
2056                                  vm_states.SHELVED_OFFLOADED):
2057             snapshot_id = instance.system_metadata.get('shelved_image_id')
2058             LOG.info("Working on deleting snapshot %s "
2059                      "from shelved instance...",
2060                      snapshot_id, instance=instance)
2061             try:
2062                 self.image_api.delete(context, snapshot_id)
2063             except (exception.ImageNotFound,
2064                     exception.ImageNotAuthorized) as exc:
2065                 LOG.warning("Failed to delete snapshot "
2066                             "from shelved instance (%s).",
2067                             exc.format_message(), instance=instance)
2068             except Exception:
2069                 LOG.exception("Something wrong happened when trying to "
2070                               "delete snapshot from shelved instance.",
2071                               instance=instance)
2072 
2073         original_task_state = instance.task_state
2074         try:
2075             # NOTE(maoy): no expected_task_state needs to be set
2076             instance.update(instance_attrs)
2077             instance.progress = 0
2078             instance.save()
2079 
2080             if not instance.host and not may_have_ports_or_volumes:
2081                 try:
2082                     with compute_utils.notify_about_instance_delete(
2083                             self.notifier, context, instance,
2084                             delete_type
2085                             if delete_type != 'soft_delete'
2086                             else 'delete'):
2087                         instance.destroy()
2088                     LOG.info('Instance deleted and does not have host '
2089                              'field, its vm_state is %(state)s.',
2090                              {'state': instance.vm_state},
2091                               instance=instance)
2092                     return
2093                 except exception.ObjectActionError as ex:
2094                     # The instance's host likely changed under us as
2095                     # this instance could be building and has since been
2096                     # scheduled. Continue with attempts to delete it.
2097                     LOG.debug('Refreshing instance because: %s', ex,
2098                               instance=instance)
2099                     instance.refresh()
2100 
2101             if instance.vm_state == vm_states.RESIZED:
2102                 self._confirm_resize_on_deleting(context, instance)
2103                 # NOTE(neha_alhat): After confirm resize vm_state will become
2104                 # 'active' and task_state will be set to 'None'. But for soft
2105                 # deleting a vm, the _do_soft_delete callback requires
2106                 # task_state in 'SOFT_DELETING' status. So, we need to set
2107                 # task_state as 'SOFT_DELETING' again for soft_delete case.
2108                 # After confirm resize and before saving the task_state to
2109                 # "SOFT_DELETING", during the short window, user can submit
2110                 # soft delete vm request again and system will accept and
2111                 # process it without any errors.
2112                 if delete_type == 'soft_delete':
2113                     instance.task_state = instance_attrs['task_state']
2114                     instance.save()
2115 
2116             is_local_delete = True
2117             try:
2118                 # instance.host must be set in order to look up the service.
2119                 if instance.host is not None:
2120                     service = objects.Service.get_by_compute_host(
2121                         context.elevated(), instance.host)
2122                     is_local_delete = not self.servicegroup_api.service_is_up(
2123                         service)
2124                 if not is_local_delete:
2125                     if original_task_state in (task_states.DELETING,
2126                                                   task_states.SOFT_DELETING):
2127                         LOG.info('Instance is already in deleting state, '
2128                                  'ignoring this request',
2129                                  instance=instance)
2130                         return
2131                     self._record_action_start(context, instance,
2132                                               instance_actions.DELETE)
2133 
2134                     cb(context, instance, bdms)
2135             except exception.ComputeHostNotFound:
2136                 LOG.debug('Compute host %s not found during service up check, '
2137                           'going to local delete instance', instance.host,
2138                           instance=instance)
2139 
2140             if is_local_delete:
2141                 # If instance is in shelved_offloaded state or compute node
2142                 # isn't up, delete instance from db and clean bdms info and
2143                 # network info
2144                 if cell is None:
2145                     # NOTE(danms): If we didn't get our cell from one of the
2146                     # paths above, look it up now.
2147                     try:
2148                         im = objects.InstanceMapping.get_by_instance_uuid(
2149                             context, instance.uuid)
2150                         cell = im.cell_mapping
2151                     except exception.InstanceMappingNotFound:
2152                         LOG.warning('During local delete, failed to find '
2153                                     'instance mapping', instance=instance)
2154                         return
2155 
2156                 LOG.debug('Doing local delete in cell %s', cell.identity,
2157                           instance=instance)
2158                 with nova_context.target_cell(context, cell) as cctxt:
2159                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
2160 
2161         except exception.InstanceNotFound:
2162             # NOTE(comstud): Race condition. Instance already gone.
2163             pass
2164 
2165     def _confirm_resize_on_deleting(self, context, instance):
2166         # If in the middle of a resize, use confirm_resize to
2167         # ensure the original instance is cleaned up too along
2168         # with its allocations (and migration-based allocations)
2169         # in placement.
2170         migration = None
2171         for status in ('finished', 'confirming'):
2172             try:
2173                 migration = objects.Migration.get_by_instance_and_status(
2174                         context.elevated(), instance.uuid, status)
2175                 LOG.info('Found an unconfirmed migration during delete, '
2176                          'id: %(id)s, status: %(status)s',
2177                          {'id': migration.id,
2178                           'status': migration.status},
2179                          instance=instance)
2180                 break
2181             except exception.MigrationNotFoundByStatus:
2182                 pass
2183 
2184         if not migration:
2185             LOG.info('Instance may have been confirmed during delete',
2186                      instance=instance)
2187             return
2188 
2189         src_host = migration.source_compute
2190 
2191         self._record_action_start(context, instance,
2192                                   instance_actions.CONFIRM_RESIZE)
2193 
2194         self.compute_rpcapi.confirm_resize(context,
2195                 instance, migration, src_host, cast=False)
2196 
2197     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
2198         """The method deletes the bdm records and, if a bdm is a volume, call
2199         the terminate connection and the detach volume via the Volume API.
2200         """
2201         elevated = context.elevated()
2202         for bdm in bdms:
2203             if bdm.is_volume:
2204                 try:
2205                     if bdm.attachment_id:
2206                         self.volume_api.attachment_delete(context,
2207                                                           bdm.attachment_id)
2208                     else:
2209                         connector = compute_utils.get_stashed_volume_connector(
2210                             bdm, instance)
2211                         if connector:
2212                             self.volume_api.terminate_connection(context,
2213                                                                  bdm.volume_id,
2214                                                                  connector)
2215                         else:
2216                             LOG.debug('Unable to find connector for volume %s,'
2217                                       ' not attempting terminate_connection.',
2218                                       bdm.volume_id, instance=instance)
2219                         # Attempt to detach the volume. If there was no
2220                         # connection made in the first place this is just
2221                         # cleaning up the volume state in the Cinder DB.
2222                         self.volume_api.detach(elevated, bdm.volume_id,
2223                                                instance.uuid)
2224 
2225                     if bdm.delete_on_termination:
2226                         self.volume_api.delete(context, bdm.volume_id)
2227                 except Exception as exc:
2228                     LOG.warning("Ignoring volume cleanup failure due to %s",
2229                                 exc, instance=instance)
2230             # If we're cleaning up volumes from an instance that wasn't yet
2231             # created in a cell, i.e. the user deleted the server while
2232             # the BuildRequest still existed, then the BDM doesn't actually
2233             # exist in the DB to destroy it.
2234             if 'id' in bdm:
2235                 bdm.destroy()
2236 
2237     @property
2238     def placementclient(self):
2239         if self._placementclient is None:
2240             self._placementclient = report.SchedulerReportClient()
2241         return self._placementclient
2242 
2243     def _local_delete(self, context, instance, bdms, delete_type, cb):
2244         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2245             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2246                      " the instance's info from database.",
2247                      instance=instance)
2248         else:
2249             LOG.warning("instance's host %s is down, deleting from "
2250                         "database", instance.host, instance=instance)
2251         with compute_utils.notify_about_instance_delete(
2252                 self.notifier, context, instance,
2253                 delete_type if delete_type != 'soft_delete' else 'delete'):
2254 
2255             elevated = context.elevated()
2256             # NOTE(liusheng): In nova-network multi_host scenario,deleting
2257             # network info of the instance may need instance['host'] as
2258             # destination host of RPC call. If instance in
2259             # SHELVED_OFFLOADED state, instance['host'] is None, here, use
2260             # shelved_host as host to deallocate network info and reset
2261             # instance['host'] after that. Here we shouldn't use
2262             # instance.save(), because this will mislead user who may think
2263             # the instance's host has been changed, and actually, the
2264             # instance.host is always None.
2265             orig_host = instance.host
2266             try:
2267                 if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2268                     sysmeta = getattr(instance,
2269                                       obj_base.get_attrname(
2270                                           'system_metadata'))
2271                     instance.host = sysmeta.get('shelved_host')
2272                 self.network_api.deallocate_for_instance(elevated,
2273                                                          instance)
2274             finally:
2275                 instance.host = orig_host
2276 
2277             # cleanup volumes
2278             self._local_cleanup_bdm_volumes(bdms, instance, context)
2279             # Cleanup allocations in Placement since we can't do it from the
2280             # compute service.
2281             self.placementclient.delete_allocation_for_instance(
2282                 context, instance.uuid)
2283             cb(context, instance, bdms, local=True)
2284             instance.destroy()
2285 
2286     @staticmethod
2287     def _update_queued_for_deletion(context, instance, qfd):
2288         # NOTE(tssurya): We query the instance_mapping record of this instance
2289         # and update the queued_for_delete flag to True (or False according to
2290         # the state of the instance). This just means that the instance is
2291         # queued for deletion (or is no longer queued for deletion). It does
2292         # not guarantee its successful deletion (or restoration). Hence the
2293         # value could be stale which is fine, considering its use is only
2294         # during down cell (desperate) situation.
2295         im = objects.InstanceMapping.get_by_instance_uuid(context,
2296                                                           instance.uuid)
2297         im.queued_for_delete = qfd
2298         im.save()
2299 
2300     def _do_delete(self, context, instance, bdms, local=False):
2301         if local:
2302             instance.vm_state = vm_states.DELETED
2303             instance.task_state = None
2304             instance.terminated_at = timeutils.utcnow()
2305             instance.save()
2306         else:
2307             self.compute_rpcapi.terminate_instance(context, instance, bdms)
2308         self._update_queued_for_deletion(context, instance, True)
2309 
2310     def _do_soft_delete(self, context, instance, bdms, local=False):
2311         if local:
2312             instance.vm_state = vm_states.SOFT_DELETED
2313             instance.task_state = None
2314             instance.terminated_at = timeutils.utcnow()
2315             instance.save()
2316         else:
2317             self.compute_rpcapi.soft_delete_instance(context, instance)
2318         self._update_queued_for_deletion(context, instance, True)
2319 
2320     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2321     @check_instance_lock
2322     @check_instance_state(vm_state=None, task_state=None,
2323                           must_have_launched=True)
2324     def soft_delete(self, context, instance):
2325         """Terminate an instance."""
2326         LOG.debug('Going to try to soft delete instance',
2327                   instance=instance)
2328 
2329         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2330                      task_state=task_states.SOFT_DELETING,
2331                      deleted_at=timeutils.utcnow())
2332 
2333     def _delete_instance(self, context, instance):
2334         self._delete(context, instance, 'delete', self._do_delete,
2335                      task_state=task_states.DELETING)
2336 
2337     @check_instance_lock
2338     @check_instance_state(vm_state=None, task_state=None,
2339                           must_have_launched=False)
2340     def delete(self, context, instance):
2341         """Terminate an instance."""
2342         LOG.debug("Going to try to terminate instance", instance=instance)
2343         self._delete_instance(context, instance)
2344 
2345     @check_instance_lock
2346     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2347     def restore(self, context, instance):
2348         """Restore a previously deleted (but not reclaimed) instance."""
2349         # Check quotas
2350         flavor = instance.get_flavor()
2351         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2352         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2353                 project_id=project_id, user_id=user_id)
2354 
2355         self._record_action_start(context, instance, instance_actions.RESTORE)
2356 
2357         if instance.host:
2358             instance.task_state = task_states.RESTORING
2359             instance.deleted_at = None
2360             instance.save(expected_task_state=[None])
2361             # TODO(melwitt): We're not rechecking for strict quota here to
2362             # guard against going over quota during a race at this time because
2363             # the resource consumption for this operation is written to the
2364             # database by compute.
2365             self.compute_rpcapi.restore_instance(context, instance)
2366         else:
2367             instance.vm_state = vm_states.ACTIVE
2368             instance.task_state = None
2369             instance.deleted_at = None
2370             instance.save(expected_task_state=[None])
2371         self._update_queued_for_deletion(context, instance, False)
2372 
2373     @check_instance_lock
2374     @check_instance_state(task_state=None,
2375                           must_have_launched=False)
2376     def force_delete(self, context, instance):
2377         """Force delete an instance in any vm_state/task_state."""
2378         self._delete(context, instance, 'force_delete', self._do_delete,
2379                      task_state=task_states.DELETING)
2380 
2381     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2382         LOG.debug("Going to try to stop instance", instance=instance)
2383 
2384         instance.task_state = task_states.POWERING_OFF
2385         instance.progress = 0
2386         instance.save(expected_task_state=[None])
2387 
2388         self._record_action_start(context, instance, instance_actions.STOP)
2389 
2390         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2391                                           clean_shutdown=clean_shutdown)
2392 
2393     @check_instance_lock
2394     @check_instance_host
2395     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2396     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2397         """Stop an instance."""
2398         self.force_stop(context, instance, do_cast, clean_shutdown)
2399 
2400     @check_instance_lock
2401     @check_instance_host
2402     @check_instance_state(vm_state=[vm_states.STOPPED])
2403     def start(self, context, instance):
2404         """Start an instance."""
2405         LOG.debug("Going to try to start instance", instance=instance)
2406 
2407         instance.task_state = task_states.POWERING_ON
2408         instance.save(expected_task_state=[None])
2409 
2410         self._record_action_start(context, instance, instance_actions.START)
2411         self.compute_rpcapi.start_instance(context, instance)
2412 
2413     @check_instance_lock
2414     @check_instance_host
2415     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2416     def trigger_crash_dump(self, context, instance):
2417         """Trigger crash dump in an instance."""
2418         LOG.debug("Try to trigger crash dump", instance=instance)
2419 
2420         self._record_action_start(context, instance,
2421                                   instance_actions.TRIGGER_CRASH_DUMP)
2422 
2423         self.compute_rpcapi.trigger_crash_dump(context, instance)
2424 
2425     def _generate_minimal_construct_for_down_cells(self, context,
2426                                                    down_cell_uuids,
2427                                                    project, limit):
2428         """Generate a list of minimal instance constructs for a given list of
2429         cells that did not respond to a list operation. This will list
2430         every instance mapping in the affected cells and return a minimal
2431         objects.Instance for each (non-queued-for-delete) mapping.
2432 
2433         :param context: RequestContext
2434         :param down_cell_uuids: A list of cell UUIDs that did not respond
2435         :param project: A project ID to filter mappings, or None
2436         :param limit: A numeric limit on the number of results, or None
2437         :returns: An InstanceList() of partial Instance() objects
2438         """
2439         unavailable_servers = objects.InstanceList()
2440         for cell_uuid in down_cell_uuids:
2441             LOG.warning("Cell %s is not responding and hence only "
2442                         "partial results are available from this "
2443                         "cell if any.", cell_uuid)
2444             instance_mappings = (objects.InstanceMappingList.
2445                 get_not_deleted_by_cell_and_project(context, cell_uuid,
2446                                                     project, limit=limit))
2447             for im in instance_mappings:
2448                 unavailable_servers.objects.append(
2449                     objects.Instance(
2450                         context=context,
2451                         uuid=im.instance_uuid,
2452                         project_id=im.project_id,
2453                         created_at=im.created_at
2454                     )
2455                 )
2456             if limit is not None:
2457                 limit -= len(instance_mappings)
2458                 if limit <= 0:
2459                     break
2460         return unavailable_servers
2461 
2462     def _get_instance_map_or_none(self, context, instance_uuid):
2463         try:
2464             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2465                     context, instance_uuid)
2466         except exception.InstanceMappingNotFound:
2467             # InstanceMapping should always be found generally. This exception
2468             # may be raised if a deployment has partially migrated the nova-api
2469             # services.
2470             inst_map = None
2471         return inst_map
2472 
2473     @staticmethod
2474     def _save_user_id_in_instance_mapping(mapping, instance):
2475         # TODO(melwitt): We take the opportunity to migrate user_id on the
2476         # instance mapping if it's not yet been migrated. This can be removed
2477         # in a future release, when all migrations are complete.
2478         # If the instance came from a RequestSpec because of a down cell, its
2479         # user_id could be None and the InstanceMapping.user_id field is
2480         # non-nullable. Avoid trying to set/save the user_id in that case.
2481         if 'user_id' not in mapping and instance.user_id is not None:
2482             mapping.user_id = instance.user_id
2483             mapping.save()
2484 
2485     def _get_instance_from_cell(self, context, im, expected_attrs,
2486                                 cell_down_support):
2487         # NOTE(danms): Even though we're going to scatter/gather to the
2488         # right cell, other code depends on this being force targeted when
2489         # the get call returns.
2490         nova_context.set_target_cell(context, im.cell_mapping)
2491 
2492         uuid = im.instance_uuid
2493         result = nova_context.scatter_gather_single_cell(context,
2494             im.cell_mapping, objects.Instance.get_by_uuid, uuid,
2495             expected_attrs=expected_attrs)
2496         cell_uuid = im.cell_mapping.uuid
2497         if not nova_context.is_cell_failure_sentinel(result[cell_uuid]):
2498             inst = result[cell_uuid]
2499             self._save_user_id_in_instance_mapping(im, inst)
2500             return inst
2501         elif isinstance(result[cell_uuid], exception.InstanceNotFound):
2502             raise exception.InstanceNotFound(instance_id=uuid)
2503         elif cell_down_support:
2504             if im.queued_for_delete:
2505                 # should be treated like deleted instance.
2506                 raise exception.InstanceNotFound(instance_id=uuid)
2507 
2508             # instance in down cell, return a minimal construct
2509             LOG.warning("Cell %s is not responding and hence only "
2510                         "partial results are available from this "
2511                         "cell.", cell_uuid)
2512             try:
2513                 rs = objects.RequestSpec.get_by_instance_uuid(context,
2514                                                               uuid)
2515                 # For BFV case, we could have rs.image but rs.image.id might
2516                 # still not be set. So we check the existence of both image
2517                 # and its id.
2518                 image_ref = (rs.image.id if rs.image and
2519                              'id' in rs.image else None)
2520                 inst = objects.Instance(context=context, power_state=0,
2521                                         uuid=uuid,
2522                                         project_id=im.project_id,
2523                                         created_at=im.created_at,
2524                                         user_id=rs.user_id,
2525                                         flavor=rs.flavor,
2526                                         image_ref=image_ref,
2527                                         availability_zone=rs.availability_zone)
2528                 self._save_user_id_in_instance_mapping(im, inst)
2529                 return inst
2530             except exception.RequestSpecNotFound:
2531                 # could be that a deleted instance whose request
2532                 # spec has been archived is being queried.
2533                 raise exception.InstanceNotFound(instance_id=uuid)
2534         else:
2535             raise exception.NovaException(
2536                 _("Cell %s is not responding and hence instance "
2537                   "info is not available.") % cell_uuid)
2538 
2539     def _get_instance(self, context, instance_uuid, expected_attrs,
2540                       cell_down_support=False):
2541         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2542         if inst_map and (inst_map.cell_mapping is not None):
2543             instance = self._get_instance_from_cell(context, inst_map,
2544                 expected_attrs, cell_down_support)
2545         elif inst_map and (inst_map.cell_mapping is None):
2546             # This means the instance has not been scheduled and put in
2547             # a cell yet. For now it also may mean that the deployer
2548             # has not created their cell(s) yet.
2549             try:
2550                 build_req = objects.BuildRequest.get_by_instance_uuid(
2551                         context, instance_uuid)
2552                 instance = build_req.instance
2553             except exception.BuildRequestNotFound:
2554                 # Instance was mapped and the BuildRequest was deleted
2555                 # while fetching. Try again.
2556                 inst_map = self._get_instance_map_or_none(context,
2557                                                           instance_uuid)
2558                 if inst_map and (inst_map.cell_mapping is not None):
2559                     instance = self._get_instance_from_cell(context, inst_map,
2560                         expected_attrs, cell_down_support)
2561                 else:
2562                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2563         else:
2564             # If we got here, we don't have an instance mapping, but we aren't
2565             # sure why. The instance mapping might be missing because the
2566             # upgrade is incomplete (map_instances wasn't run). Or because the
2567             # instance was deleted and the DB was archived at which point the
2568             # mapping is deleted. The former case is bad, but because of the
2569             # latter case we can't really log any kind of warning/error here
2570             # since it might be normal.
2571             raise exception.InstanceNotFound(instance_id=instance_uuid)
2572 
2573         return instance
2574 
2575     def get(self, context, instance_id, expected_attrs=None,
2576             cell_down_support=False):
2577         """Get a single instance with the given instance_id.
2578 
2579         :param cell_down_support: True if the API (and caller) support
2580                                   returning a minimal instance
2581                                   construct if the relevant cell is
2582                                   down. If False, an error is raised
2583                                   since the instance cannot be retrieved
2584                                   due to the cell being down.
2585         """
2586         if not expected_attrs:
2587             expected_attrs = []
2588         expected_attrs.extend(['metadata', 'system_metadata',
2589                                'security_groups', 'info_cache'])
2590         # NOTE(ameade): we still need to support integer ids for ec2
2591         try:
2592             if uuidutils.is_uuid_like(instance_id):
2593                 LOG.debug("Fetching instance by UUID",
2594                            instance_uuid=instance_id)
2595 
2596                 instance = self._get_instance(context, instance_id,
2597                     expected_attrs, cell_down_support=cell_down_support)
2598             else:
2599                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2600                 raise exception.InstanceNotFound(instance_id=instance_id)
2601         except exception.InvalidID:
2602             LOG.debug("Invalid instance id %s", instance_id)
2603             raise exception.InstanceNotFound(instance_id=instance_id)
2604 
2605         return instance
2606 
2607     def get_all(self, context, search_opts=None, limit=None, marker=None,
2608                 expected_attrs=None, sort_keys=None, sort_dirs=None,
2609                 cell_down_support=False, all_tenants=False):
2610         """Get all instances filtered by one of the given parameters.
2611 
2612         If there is no filter and the context is an admin, it will retrieve
2613         all instances in the system.
2614 
2615         Deleted instances will be returned by default, unless there is a
2616         search option that says otherwise.
2617 
2618         The results will be sorted based on the list of sort keys in the
2619         'sort_keys' parameter (first value is primary sort key, second value is
2620         secondary sort ket, etc.). For each sort key, the associated sort
2621         direction is based on the list of sort directions in the 'sort_dirs'
2622         parameter.
2623 
2624         :param cell_down_support: True if the API (and caller) support
2625                                   returning a minimal instance
2626                                   construct if the relevant cell is
2627                                   down. If False, instances from
2628                                   unreachable cells will be omitted.
2629         :param all_tenants: True if the "all_tenants" filter was passed.
2630 
2631         """
2632         if search_opts is None:
2633             search_opts = {}
2634 
2635         LOG.debug("Searching by: %s", str(search_opts))
2636 
2637         # Fixups for the DB call
2638         filters = {}
2639 
2640         def _remap_flavor_filter(flavor_id):
2641             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2642             filters['instance_type_id'] = flavor.id
2643 
2644         def _remap_fixed_ip_filter(fixed_ip):
2645             # Turn fixed_ip into a regexp match. Since '.' matches
2646             # any character, we need to use regexp escaping for it.
2647             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2648 
2649         # search_option to filter_name mapping.
2650         filter_mapping = {
2651                 'image': 'image_ref',
2652                 'name': 'display_name',
2653                 'tenant_id': 'project_id',
2654                 'flavor': _remap_flavor_filter,
2655                 'fixed_ip': _remap_fixed_ip_filter}
2656 
2657         # copy from search_opts, doing various remappings as necessary
2658         for opt, value in search_opts.items():
2659             # Do remappings.
2660             # Values not in the filter_mapping table are copied as-is.
2661             # If remapping is None, option is not copied
2662             # If the remapping is a string, it is the filter_name to use
2663             try:
2664                 remap_object = filter_mapping[opt]
2665             except KeyError:
2666                 filters[opt] = value
2667             else:
2668                 # Remaps are strings to translate to, or functions to call
2669                 # to do the translating as defined by the table above.
2670                 if isinstance(remap_object, six.string_types):
2671                     filters[remap_object] = value
2672                 else:
2673                     try:
2674                         remap_object(value)
2675 
2676                     # We already know we can't match the filter, so
2677                     # return an empty list
2678                     except ValueError:
2679                         return objects.InstanceList()
2680 
2681         # IP address filtering cannot be applied at the DB layer, remove any DB
2682         # limit so that it can be applied after the IP filter.
2683         filter_ip = 'ip6' in filters or 'ip' in filters
2684         skip_build_request = False
2685         orig_limit = limit
2686         if filter_ip:
2687             # We cannot skip build requests if there is a marker since the
2688             # the marker could be a build request.
2689             skip_build_request = marker is None
2690             if self.network_api.has_substr_port_filtering_extension(context):
2691                 # We're going to filter by IP using Neutron so set filter_ip
2692                 # to False so we don't attempt post-DB query filtering in
2693                 # memory below.
2694                 filter_ip = False
2695                 instance_uuids = self._ip_filter_using_neutron(context,
2696                                                                filters)
2697                 if instance_uuids:
2698                     # Note that 'uuid' is not in the 2.1 GET /servers query
2699                     # parameter schema, however, we allow additionalProperties
2700                     # so someone could filter instances by uuid, which doesn't
2701                     # make a lot of sense but we have to account for it.
2702                     if 'uuid' in filters and filters['uuid']:
2703                         filter_uuids = filters['uuid']
2704                         if isinstance(filter_uuids, list):
2705                             instance_uuids.extend(filter_uuids)
2706                         else:
2707                             # Assume a string. If it's a dict or tuple or
2708                             # something, well...that's too bad. This is why
2709                             # we have query parameter schema definitions.
2710                             if filter_uuids not in instance_uuids:
2711                                 instance_uuids.append(filter_uuids)
2712                     filters['uuid'] = instance_uuids
2713                 else:
2714                     # No matches on the ip filter(s), return an empty list.
2715                     return objects.InstanceList()
2716             elif limit:
2717                 LOG.debug('Removing limit for DB query due to IP filter')
2718                 limit = None
2719 
2720         # Skip get BuildRequest if filtering by IP address, as building
2721         # instances will not have IP addresses.
2722         if skip_build_request:
2723             build_requests = objects.BuildRequestList()
2724         else:
2725             # The ordering of instances will be
2726             # [sorted instances with no host] + [sorted instances with host].
2727             # This means BuildRequest and cell0 instances first, then cell
2728             # instances
2729             try:
2730                 build_requests = objects.BuildRequestList.get_by_filters(
2731                     context, filters, limit=limit, marker=marker,
2732                     sort_keys=sort_keys, sort_dirs=sort_dirs)
2733                 # If we found the marker in we need to set it to None
2734                 # so we don't expect to find it in the cells below.
2735                 marker = None
2736             except exception.MarkerNotFound:
2737                 # If we didn't find the marker in the build requests then keep
2738                 # looking for it in the cells.
2739                 build_requests = objects.BuildRequestList()
2740 
2741         build_req_instances = objects.InstanceList(
2742             objects=[build_req.instance for build_req in build_requests])
2743         # Only subtract from limit if it is not None
2744         limit = (limit - len(build_req_instances)) if limit else limit
2745 
2746         # We could arguably avoid joining on security_groups if we're using
2747         # neutron (which is the default) but if you're using neutron then the
2748         # security_group_instance_association table should be empty anyway
2749         # and the DB should optimize out that join, making it insignificant.
2750         fields = ['metadata', 'info_cache', 'security_groups']
2751         if expected_attrs:
2752             fields.extend(expected_attrs)
2753 
2754         insts, down_cell_uuids = instance_list.get_instance_objects_sorted(
2755             context, filters, limit, marker, fields, sort_keys, sort_dirs,
2756             cell_down_support=cell_down_support)
2757 
2758         def _get_unique_filter_method():
2759             seen_uuids = set()
2760 
2761             def _filter(instance):
2762                 if instance.uuid in seen_uuids:
2763                     return False
2764                 seen_uuids.add(instance.uuid)
2765                 return True
2766 
2767             return _filter
2768 
2769         filter_method = _get_unique_filter_method()
2770         # Only subtract from limit if it is not None
2771         limit = (limit - len(insts)) if limit else limit
2772         # TODO(alaski): Clean up the objects concatenation when List objects
2773         # support it natively.
2774         instances = objects.InstanceList(
2775             objects=list(filter(filter_method,
2776                            build_req_instances.objects +
2777                            insts.objects)))
2778 
2779         if filter_ip:
2780             instances = self._ip_filter(instances, filters, orig_limit)
2781 
2782         if cell_down_support:
2783             # API and client want minimal construct instances for any cells
2784             # that didn't return, so generate and prefix those to the actual
2785             # results.
2786             project = search_opts.get('project_id', context.project_id)
2787             if all_tenants:
2788                 # NOTE(tssurya): The only scenario where project has to be None
2789                 # is when using "all_tenants" in which case we do not want
2790                 # the query to be restricted based on the project_id.
2791                 project = None
2792             limit = (orig_limit - len(instances)) if limit else limit
2793             return (self._generate_minimal_construct_for_down_cells(context,
2794                 down_cell_uuids, project, limit) + instances)
2795 
2796         return instances
2797 
2798     @staticmethod
2799     def _ip_filter(inst_models, filters, limit):
2800         ipv4_f = re.compile(str(filters.get('ip')))
2801         ipv6_f = re.compile(str(filters.get('ip6')))
2802 
2803         def _match_instance(instance):
2804             nw_info = instance.get_network_info()
2805             for vif in nw_info:
2806                 for fixed_ip in vif.fixed_ips():
2807                     address = fixed_ip.get('address')
2808                     if not address:
2809                         continue
2810                     version = fixed_ip.get('version')
2811                     if ((version == 4 and ipv4_f.match(address)) or
2812                         (version == 6 and ipv6_f.match(address))):
2813                         return True
2814             return False
2815 
2816         result_objs = []
2817         for instance in inst_models:
2818             if _match_instance(instance):
2819                 result_objs.append(instance)
2820                 if limit and len(result_objs) == limit:
2821                     break
2822         return objects.InstanceList(objects=result_objs)
2823 
2824     def _ip_filter_using_neutron(self, context, filters):
2825         ip4_address = filters.get('ip')
2826         ip6_address = filters.get('ip6')
2827         addresses = [ip4_address, ip6_address]
2828         uuids = []
2829         for address in addresses:
2830             if address:
2831                 try:
2832                     ports = self.network_api.list_ports(
2833                         context, fixed_ips='ip_address_substr=' + address,
2834                         fields=['device_id'])['ports']
2835                     for port in ports:
2836                         uuids.append(port['device_id'])
2837                 except Exception as e:
2838                     LOG.error('An error occurred while listing ports '
2839                               'with an ip_address filter value of "%s". '
2840                               'Error: %s',
2841                               address, six.text_type(e))
2842         return uuids
2843 
2844     def _get_instances_by_filters(self, context, filters,
2845                                   limit=None, marker=None, fields=None,
2846                                   sort_keys=None, sort_dirs=None):
2847         return objects.InstanceList.get_by_filters(
2848             context, filters=filters, limit=limit, marker=marker,
2849             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2850 
2851     def update_instance(self, context, instance, updates):
2852         """Updates a single Instance object with some updates dict.
2853 
2854         Returns the updated instance.
2855         """
2856 
2857         # NOTE(sbauza): Given we only persist the Instance object after we
2858         # create the BuildRequest, we are sure that if the Instance object
2859         # has an ID field set, then it was persisted in the right Cell DB.
2860         if instance.obj_attr_is_set('id'):
2861             instance.update(updates)
2862             instance.save()
2863         else:
2864             # Instance is not yet mapped to a cell, so we need to update
2865             # BuildRequest instead
2866             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2867             # could be deleted because of either a concurrent instance delete
2868             # or because the scheduler just returned a destination right
2869             # after we called the instance in the API.
2870             try:
2871                 build_req = objects.BuildRequest.get_by_instance_uuid(
2872                     context, instance.uuid)
2873                 instance = build_req.instance
2874                 instance.update(updates)
2875                 # FIXME(sbauza): Here we are updating the current
2876                 # thread-related BuildRequest object. Given that another worker
2877                 # could have looking up at that BuildRequest in the API, it
2878                 # means that it could pass it down to the conductor without
2879                 # making sure that it's not updated, we could have some race
2880                 # condition where it would missing the updated fields, but
2881                 # that's something we could discuss once the instance record
2882                 # is persisted by the conductor.
2883                 build_req.save()
2884             except exception.BuildRequestNotFound:
2885                 # Instance was mapped and the BuildRequest was deleted
2886                 # while fetching (and possibly the instance could have been
2887                 # deleted as well). We need to lookup again the Instance object
2888                 # in order to correctly update it.
2889                 # TODO(sbauza): Figure out a good way to know the expected
2890                 # attributes by checking which fields are set or not.
2891                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2892                                   'tags', 'metadata', 'system_metadata',
2893                                   'security_groups', 'info_cache']
2894                 inst_map = self._get_instance_map_or_none(context,
2895                                                           instance.uuid)
2896                 if inst_map and (inst_map.cell_mapping is not None):
2897                     with nova_context.target_cell(
2898                             context,
2899                             inst_map.cell_mapping) as cctxt:
2900                         instance = objects.Instance.get_by_uuid(
2901                             cctxt, instance.uuid,
2902                             expected_attrs=expected_attrs)
2903                         instance.update(updates)
2904                         instance.save()
2905                 else:
2906                     # Conductor doesn't delete the BuildRequest until after the
2907                     # InstanceMapping record is created, so if we didn't get
2908                     # that and the BuildRequest doesn't exist, then the
2909                     # instance is already gone and we need to just error out.
2910                     raise exception.InstanceNotFound(instance_id=instance.uuid)
2911         return instance
2912 
2913     # NOTE(melwitt): We don't check instance lock for backup because lock is
2914     #                intended to prevent accidental change/delete of instances
2915     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2916                                     vm_states.PAUSED, vm_states.SUSPENDED])
2917     def backup(self, context, instance, name, backup_type, rotation,
2918                extra_properties=None):
2919         """Backup the given instance
2920 
2921         :param instance: nova.objects.instance.Instance object
2922         :param name: name of the backup
2923         :param backup_type: 'daily' or 'weekly'
2924         :param rotation: int representing how many backups to keep around;
2925             None if rotation shouldn't be used (as in the case of snapshots)
2926         :param extra_properties: dict of extra image properties to include
2927                                  when creating the image.
2928         :returns: A dict containing image metadata
2929         """
2930         props_copy = dict(extra_properties, backup_type=backup_type)
2931 
2932         if compute_utils.is_volume_backed_instance(context, instance):
2933             LOG.info("It's not supported to backup volume backed "
2934                      "instance.", instance=instance)
2935             raise exception.InvalidRequest(
2936                 _('Backup is not supported for volume-backed instances.'))
2937         else:
2938             image_meta = compute_utils.create_image(
2939                 context, instance, name, 'backup', self.image_api,
2940                 extra_properties=props_copy)
2941 
2942         instance.task_state = task_states.IMAGE_BACKUP
2943         instance.save(expected_task_state=[None])
2944 
2945         self._record_action_start(context, instance,
2946                                   instance_actions.BACKUP)
2947 
2948         self.compute_rpcapi.backup_instance(context, instance,
2949                                             image_meta['id'],
2950                                             backup_type,
2951                                             rotation)
2952         return image_meta
2953 
2954     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2955     #                intended to prevent accidental change/delete of instances
2956     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2957                                     vm_states.PAUSED, vm_states.SUSPENDED])
2958     def snapshot(self, context, instance, name, extra_properties=None):
2959         """Snapshot the given instance.
2960 
2961         :param instance: nova.objects.instance.Instance object
2962         :param name: name of the snapshot
2963         :param extra_properties: dict of extra image properties to include
2964                                  when creating the image.
2965         :returns: A dict containing image metadata
2966         """
2967         image_meta = compute_utils.create_image(
2968             context, instance, name, 'snapshot', self.image_api,
2969             extra_properties=extra_properties)
2970 
2971         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2972         try:
2973             instance.save(expected_task_state=[None])
2974         except (exception.InstanceNotFound,
2975                 exception.UnexpectedDeletingTaskStateError) as ex:
2976             # Changing the instance task state to use in raising the
2977             # InstanceInvalidException below
2978             LOG.debug('Instance disappeared during snapshot.',
2979                       instance=instance)
2980             try:
2981                 image_id = image_meta['id']
2982                 self.image_api.delete(context, image_id)
2983                 LOG.info('Image %s deleted because instance '
2984                          'deleted before snapshot started.',
2985                          image_id, instance=instance)
2986             except exception.ImageNotFound:
2987                 pass
2988             except Exception as exc:
2989                 LOG.warning("Error while trying to clean up image %(img_id)s: "
2990                             "%(error_msg)s",
2991                             {"img_id": image_meta['id'],
2992                              "error_msg": six.text_type(exc)})
2993             attr = 'task_state'
2994             state = task_states.DELETING
2995             if type(ex) == exception.InstanceNotFound:
2996                 attr = 'vm_state'
2997                 state = vm_states.DELETED
2998             raise exception.InstanceInvalidState(attr=attr,
2999                                            instance_uuid=instance.uuid,
3000                                            state=state,
3001                                            method='snapshot')
3002 
3003         self._record_action_start(context, instance,
3004                                   instance_actions.CREATE_IMAGE)
3005 
3006         self.compute_rpcapi.snapshot_instance(context, instance,
3007                                               image_meta['id'])
3008 
3009         return image_meta
3010 
3011     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3012     #                intended to prevent accidental change/delete of instances
3013     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3014                                     vm_states.SUSPENDED])
3015     def snapshot_volume_backed(self, context, instance, name,
3016                                extra_properties=None):
3017         """Snapshot the given volume-backed instance.
3018 
3019         :param instance: nova.objects.instance.Instance object
3020         :param name: name of the backup or snapshot
3021         :param extra_properties: dict of extra image properties to include
3022 
3023         :returns: the new image metadata
3024         """
3025         image_meta = compute_utils.initialize_instance_snapshot_metadata(
3026             context, instance, name, extra_properties)
3027         # the new image is simply a bucket of properties (particularly the
3028         # block device mapping, kernel and ramdisk IDs) with no image data,
3029         # hence the zero size
3030         image_meta['size'] = 0
3031         for attr in ('container_format', 'disk_format'):
3032             image_meta.pop(attr, None)
3033         properties = image_meta['properties']
3034         # clean properties before filling
3035         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
3036             properties.pop(key, None)
3037         if instance.root_device_name:
3038             properties['root_device_name'] = instance.root_device_name
3039 
3040         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3041                 context, instance.uuid)
3042 
3043         mapping = []  # list of BDM dicts that can go into the image properties
3044         # Do some up-front filtering of the list of BDMs from
3045         # which we are going to create snapshots.
3046         volume_bdms = []
3047         for bdm in bdms:
3048             if bdm.no_device:
3049                 continue
3050             if bdm.is_volume:
3051                 # These will be handled below.
3052                 volume_bdms.append(bdm)
3053             else:
3054                 mapping.append(bdm.get_image_mapping())
3055 
3056         # Check limits in Cinder before creating snapshots to avoid going over
3057         # quota in the middle of a list of volumes. This is a best-effort check
3058         # but concurrently running snapshot requests from the same project
3059         # could still fail to create volume snapshots if they go over limit.
3060         if volume_bdms:
3061             limits = self.volume_api.get_absolute_limits(context)
3062             total_snapshots_used = limits['totalSnapshotsUsed']
3063             max_snapshots = limits['maxTotalSnapshots']
3064             # -1 means there is unlimited quota for snapshots
3065             if (max_snapshots > -1 and
3066                     len(volume_bdms) + total_snapshots_used > max_snapshots):
3067                 LOG.debug('Unable to create volume snapshots for instance. '
3068                           'Currently has %s snapshots, requesting %s new '
3069                           'snapshots, with a limit of %s.',
3070                           total_snapshots_used, len(volume_bdms),
3071                           max_snapshots, instance=instance)
3072                 raise exception.OverQuota(overs='snapshots')
3073 
3074         quiesced = False
3075         if instance.vm_state == vm_states.ACTIVE:
3076             try:
3077                 LOG.info("Attempting to quiesce instance before volume "
3078                          "snapshot.", instance=instance)
3079                 self.compute_rpcapi.quiesce_instance(context, instance)
3080                 quiesced = True
3081             except (exception.InstanceQuiesceNotSupported,
3082                     exception.QemuGuestAgentNotEnabled,
3083                     exception.NovaException, NotImplementedError) as err:
3084                 if strutils.bool_from_string(instance.system_metadata.get(
3085                         'image_os_require_quiesce')):
3086                     raise
3087 
3088                 if isinstance(err, exception.NovaException):
3089                     LOG.info('Skipping quiescing instance: %(reason)s.',
3090                              {'reason': err.format_message()},
3091                              instance=instance)
3092                 else:
3093                     LOG.info('Skipping quiescing instance because the '
3094                              'operation is not supported by the underlying '
3095                              'compute driver.', instance=instance)
3096             # NOTE(tasker): discovered that an uncaught exception could occur
3097             #               after the instance has been frozen. catch and thaw.
3098             except Exception as ex:
3099                 with excutils.save_and_reraise_exception():
3100                     LOG.error("An error occurred during quiesce of instance. "
3101                               "Unquiescing to ensure instance is thawed. "
3102                               "Error: %s", six.text_type(ex),
3103                               instance=instance)
3104                     self.compute_rpcapi.unquiesce_instance(context, instance,
3105                                                            mapping=None)
3106 
3107         @wrap_instance_event(prefix='api')
3108         def snapshot_instance(self, context, instance, bdms):
3109             try:
3110                 for bdm in volume_bdms:
3111                     # create snapshot based on volume_id
3112                     volume = self.volume_api.get(context, bdm.volume_id)
3113                     # NOTE(yamahata): Should we wait for snapshot creation?
3114                     #                 Linux LVM snapshot creation completes in
3115                     #                 short time, it doesn't matter for now.
3116                     name = _('snapshot for %s') % image_meta['name']
3117                     LOG.debug('Creating snapshot from volume %s.',
3118                               volume['id'], instance=instance)
3119                     snapshot = self.volume_api.create_snapshot_force(
3120                         context, volume['id'],
3121                         name, volume['display_description'])
3122                     mapping_dict = block_device.snapshot_from_bdm(
3123                         snapshot['id'], bdm)
3124                     mapping_dict = mapping_dict.get_image_mapping()
3125                     mapping.append(mapping_dict)
3126                 return mapping
3127             # NOTE(tasker): No error handling is done in the above for loop.
3128             # This means that if the snapshot fails and throws an exception
3129             # the traceback will skip right over the unquiesce needed below.
3130             # Here, catch any exception, unquiesce the instance, and raise the
3131             # error so that the calling function can do what it needs to in
3132             # order to properly treat a failed snap.
3133             except Exception:
3134                 with excutils.save_and_reraise_exception():
3135                     if quiesced:
3136                         LOG.info("Unquiescing instance after volume snapshot "
3137                                  "failure.", instance=instance)
3138                         self.compute_rpcapi.unquiesce_instance(
3139                             context, instance, mapping)
3140 
3141         self._record_action_start(context, instance,
3142                                   instance_actions.CREATE_IMAGE)
3143         mapping = snapshot_instance(self, context, instance, bdms)
3144 
3145         if quiesced:
3146             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
3147 
3148         if mapping:
3149             properties['block_device_mapping'] = mapping
3150             properties['bdm_v2'] = True
3151 
3152         return self.image_api.create(context, image_meta)
3153 
3154     @check_instance_lock
3155     def reboot(self, context, instance, reboot_type):
3156         """Reboot the given instance."""
3157         if reboot_type == 'SOFT':
3158             self._soft_reboot(context, instance)
3159         else:
3160             self._hard_reboot(context, instance)
3161 
3162     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
3163                           task_state=[None])
3164     def _soft_reboot(self, context, instance):
3165         expected_task_state = [None]
3166         instance.task_state = task_states.REBOOTING
3167         instance.save(expected_task_state=expected_task_state)
3168 
3169         self._record_action_start(context, instance, instance_actions.REBOOT)
3170 
3171         self.compute_rpcapi.reboot_instance(context, instance=instance,
3172                                             block_device_info=None,
3173                                             reboot_type='SOFT')
3174 
3175     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
3176                           task_state=task_states.ALLOW_REBOOT)
3177     def _hard_reboot(self, context, instance):
3178         instance.task_state = task_states.REBOOTING_HARD
3179         instance.save(expected_task_state=task_states.ALLOW_REBOOT)
3180 
3181         self._record_action_start(context, instance, instance_actions.REBOOT)
3182 
3183         self.compute_rpcapi.reboot_instance(context, instance=instance,
3184                                             block_device_info=None,
3185                                             reboot_type='HARD')
3186 
3187     # TODO(stephenfin): We should expand kwargs out to named args
3188     @check_instance_lock
3189     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3190                                     vm_states.ERROR])
3191     def rebuild(self, context, instance, image_href, admin_password,
3192                 files_to_inject=None, **kwargs):
3193         """Rebuild the given instance with the provided attributes."""
3194         files_to_inject = files_to_inject or []
3195         metadata = kwargs.get('metadata', {})
3196         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
3197         auto_disk_config = kwargs.get('auto_disk_config')
3198 
3199         if 'key_name' in kwargs:
3200             key_name = kwargs.pop('key_name')
3201             if key_name:
3202                 # NOTE(liuyulong): we are intentionally using the user_id from
3203                 # the request context rather than the instance.user_id because
3204                 # users own keys but instances are owned by projects, and
3205                 # another user in the same project can rebuild an instance
3206                 # even if they didn't create it.
3207                 key_pair = objects.KeyPair.get_by_name(context,
3208                                                        context.user_id,
3209                                                        key_name)
3210                 instance.key_name = key_pair.name
3211                 instance.key_data = key_pair.public_key
3212                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
3213             else:
3214                 instance.key_name = None
3215                 instance.key_data = None
3216                 instance.keypairs = objects.KeyPairList(objects=[])
3217 
3218         # Use trusted_certs value from kwargs to create TrustedCerts object
3219         trusted_certs = None
3220         if 'trusted_certs' in kwargs:
3221             # Note that the user can set, change, or unset / reset trusted
3222             # certs. If they are explicitly specifying
3223             # trusted_image_certificates=None, that means we'll either unset
3224             # them on the instance *or* reset to use the defaults (if defaults
3225             # are configured).
3226             trusted_certs = kwargs.pop('trusted_certs')
3227             instance.trusted_certs = self._retrieve_trusted_certs_object(
3228                 context, trusted_certs, rebuild=True)
3229 
3230         image_id, image = self._get_image(context, image_href)
3231         self._check_auto_disk_config(image=image, **kwargs)
3232 
3233         flavor = instance.get_flavor()
3234         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3235             context, instance.uuid)
3236         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
3237 
3238         # Check to see if the image is changing and we have a volume-backed
3239         # server. The compute doesn't support changing the image in the
3240         # root disk of a volume-backed server, so we need to just fail fast.
3241         is_volume_backed = compute_utils.is_volume_backed_instance(
3242             context, instance, bdms)
3243         if is_volume_backed:
3244             if trusted_certs:
3245                 # The only way we can get here is if the user tried to set
3246                 # trusted certs or specified trusted_image_certificates=None
3247                 # and default_trusted_certificate_ids is configured.
3248                 msg = _("Image certificate validation is not supported "
3249                         "for volume-backed servers.")
3250                 raise exception.CertificateValidationFailed(message=msg)
3251 
3252             # For boot from volume, instance.image_ref is empty, so we need to
3253             # query the image from the volume.
3254             if root_bdm is None:
3255                 # This shouldn't happen and is an error, we need to fail. This
3256                 # is not the users fault, it's an internal error. Without a
3257                 # root BDM we have no way of knowing the backing volume (or
3258                 # image in that volume) for this instance.
3259                 raise exception.NovaException(
3260                     _('Unable to find root block device mapping for '
3261                       'volume-backed instance.'))
3262 
3263             volume = self.volume_api.get(context, root_bdm.volume_id)
3264             volume_image_metadata = volume.get('volume_image_metadata', {})
3265             orig_image_ref = volume_image_metadata.get('image_id')
3266 
3267             if orig_image_ref != image_href:
3268                 # Leave a breadcrumb.
3269                 LOG.debug('Requested to rebuild instance with a new image %s '
3270                           'for a volume-backed server with image %s in its '
3271                           'root volume which is not supported.', image_href,
3272                           orig_image_ref, instance=instance)
3273                 msg = _('Unable to rebuild with a different image for a '
3274                         'volume-backed server.')
3275                 raise exception.ImageUnacceptable(
3276                     image_id=image_href, reason=msg)
3277         else:
3278             orig_image_ref = instance.image_ref
3279 
3280         request_spec = objects.RequestSpec.get_by_instance_uuid(
3281             context, instance.uuid)
3282 
3283         self._checks_for_create_and_rebuild(context, image_id, image,
3284                 flavor, metadata, files_to_inject, root_bdm)
3285 
3286         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
3287                 context, None, None, image)
3288 
3289         def _reset_image_metadata():
3290             """Remove old image properties that we're storing as instance
3291             system metadata.  These properties start with 'image_'.
3292             Then add the properties for the new image.
3293             """
3294             # FIXME(comstud): There's a race condition here in that if
3295             # the system_metadata for this instance is updated after
3296             # we do the previous save() and before we update.. those
3297             # other updates will be lost. Since this problem exists in
3298             # a lot of other places, I think it should be addressed in
3299             # a DB layer overhaul.
3300 
3301             orig_sys_metadata = dict(instance.system_metadata)
3302             # Remove the old keys
3303             for key in list(instance.system_metadata.keys()):
3304                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
3305                     del instance.system_metadata[key]
3306 
3307             # Add the new ones
3308             new_sys_metadata = utils.get_system_metadata_from_image(
3309                 image, flavor)
3310 
3311             instance.system_metadata.update(new_sys_metadata)
3312             instance.save()
3313             return orig_sys_metadata
3314 
3315         # Since image might have changed, we may have new values for
3316         # os_type, vm_mode, etc
3317         options_from_image = self._inherit_properties_from_image(
3318                 image, auto_disk_config)
3319         instance.update(options_from_image)
3320 
3321         instance.task_state = task_states.REBUILDING
3322         # An empty instance.image_ref is currently used as an indication
3323         # of BFV.  Preserve that over a rebuild to not break users.
3324         if not is_volume_backed:
3325             instance.image_ref = image_href
3326         instance.kernel_id = kernel_id or ""
3327         instance.ramdisk_id = ramdisk_id or ""
3328         instance.progress = 0
3329         instance.update(kwargs)
3330         instance.save(expected_task_state=[None])
3331 
3332         # On a rebuild, since we're potentially changing images, we need to
3333         # wipe out the old image properties that we're storing as instance
3334         # system metadata... and copy in the properties for the new image.
3335         orig_sys_metadata = _reset_image_metadata()
3336 
3337         self._record_action_start(context, instance, instance_actions.REBUILD)
3338 
3339         # NOTE(sbauza): The migration script we provided in Newton should make
3340         # sure that all our instances are currently migrated to have an
3341         # attached RequestSpec object but let's consider that the operator only
3342         # half migrated all their instances in the meantime.
3343         host = instance.host
3344         # If a new image is provided on rebuild, we will need to run
3345         # through the scheduler again, but we want the instance to be
3346         # rebuilt on the same host it's already on.
3347         if orig_image_ref != image_href:
3348             # We have to modify the request spec that goes to the scheduler
3349             # to contain the new image. We persist this since we've already
3350             # changed the instance.image_ref above so we're being
3351             # consistent.
3352             request_spec.image = objects.ImageMeta.from_dict(image)
3353             request_spec.save()
3354             if 'scheduler_hints' not in request_spec:
3355                 request_spec.scheduler_hints = {}
3356             # Nuke the id on this so we can't accidentally save
3357             # this hint hack later
3358             del request_spec.id
3359 
3360             # NOTE(danms): Passing host=None tells conductor to
3361             # call the scheduler. The _nova_check_type hint
3362             # requires that the scheduler returns only the same
3363             # host that we are currently on and only checks
3364             # rebuild-related filters.
3365             request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3366             request_spec.force_hosts = [instance.host]
3367             request_spec.force_nodes = [instance.node]
3368             host = None
3369 
3370         self.compute_task_api.rebuild_instance(context, instance=instance,
3371                 new_pass=admin_password, injected_files=files_to_inject,
3372                 image_ref=image_href, orig_image_ref=orig_image_ref,
3373                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3374                 preserve_ephemeral=preserve_ephemeral, host=host,
3375                 request_spec=request_spec)
3376 
3377     @staticmethod
3378     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3379         project_id, user_id = quotas_obj.ids_from_instance(context,
3380                                                            instance)
3381         # Deltas will be empty if the resize is not an upsize.
3382         deltas = compute_utils.upsize_quota_delta(new_flavor,
3383                                                   current_flavor)
3384         if deltas:
3385             try:
3386                 res_deltas = {'cores': deltas.get('cores', 0),
3387                               'ram': deltas.get('ram', 0)}
3388                 objects.Quotas.check_deltas(context, res_deltas,
3389                                             project_id, user_id=user_id,
3390                                             check_project_id=project_id,
3391                                             check_user_id=user_id)
3392             except exception.OverQuota as exc:
3393                 quotas = exc.kwargs['quotas']
3394                 overs = exc.kwargs['overs']
3395                 usages = exc.kwargs['usages']
3396                 headroom = compute_utils.get_headroom(quotas, usages,
3397                                                       deltas)
3398                 (overs, reqs, total_alloweds,
3399                  useds) = compute_utils.get_over_quota_detail(headroom,
3400                                                               overs,
3401                                                               quotas,
3402                                                               deltas)
3403                 LOG.info("%(overs)s quota exceeded for %(pid)s,"
3404                          " tried to resize instance.",
3405                          {'overs': overs, 'pid': context.project_id})
3406                 raise exception.TooManyInstances(overs=overs,
3407                                                  req=reqs,
3408                                                  used=useds,
3409                                                  allowed=total_alloweds)
3410 
3411     @check_instance_lock
3412     @check_instance_state(vm_state=[vm_states.RESIZED])
3413     def revert_resize(self, context, instance):
3414         """Reverts a resize, deleting the 'new' instance in the process."""
3415         elevated = context.elevated()
3416         migration = objects.Migration.get_by_instance_and_status(
3417             elevated, instance.uuid, 'finished')
3418 
3419         # If this is a resize down, a revert might go over quota.
3420         self._check_quota_for_upsize(context, instance, instance.flavor,
3421                                      instance.old_flavor)
3422 
3423         # The AZ for the server may have changed when it was migrated so while
3424         # we are in the API and have access to the API DB, update the
3425         # instance.availability_zone before casting off to the compute service.
3426         # Note that we do this in the API to avoid an "up-call" from the
3427         # compute service to the API DB. This is not great in case something
3428         # fails during revert before the instance.host is updated to the
3429         # original source host, but it is good enough for now. Long-term we
3430         # could consider passing the AZ down to compute so it can set it when
3431         # the instance.host value is set in finish_revert_resize.
3432         instance.availability_zone = (
3433             availability_zones.get_host_availability_zone(
3434                 context, migration.source_compute))
3435 
3436         # Conductor updated the RequestSpec.flavor during the initial resize
3437         # operation to point at the new flavor, so we need to update the
3438         # RequestSpec to point back at the original flavor, otherwise
3439         # subsequent move operations through the scheduler will be using the
3440         # wrong flavor.
3441         reqspec = objects.RequestSpec.get_by_instance_uuid(
3442             context, instance.uuid)
3443         reqspec.flavor = instance.old_flavor
3444         reqspec.save()
3445 
3446         instance.task_state = task_states.RESIZE_REVERTING
3447         instance.save(expected_task_state=[None])
3448 
3449         migration.status = 'reverting'
3450         migration.save()
3451 
3452         self._record_action_start(context, instance,
3453                                   instance_actions.REVERT_RESIZE)
3454 
3455         # TODO(melwitt): We're not rechecking for strict quota here to guard
3456         # against going over quota during a race at this time because the
3457         # resource consumption for this operation is written to the database
3458         # by compute.
3459         self.compute_rpcapi.revert_resize(context, instance,
3460                                           migration,
3461                                           migration.dest_compute)
3462 
3463     @check_instance_lock
3464     @check_instance_state(vm_state=[vm_states.RESIZED])
3465     def confirm_resize(self, context, instance, migration=None):
3466         """Confirms a migration/resize and deletes the 'old' instance."""
3467         elevated = context.elevated()
3468         # NOTE(melwitt): We're not checking quota here because there isn't a
3469         # change in resource usage when confirming a resize. Resource
3470         # consumption for resizes are written to the database by compute, so
3471         # a confirm resize is just a clean up of the migration objects and a
3472         # state change in compute.
3473         if migration is None:
3474             migration = objects.Migration.get_by_instance_and_status(
3475                 elevated, instance.uuid, 'finished')
3476 
3477         migration.status = 'confirming'
3478         migration.save()
3479 
3480         self._record_action_start(context, instance,
3481                                   instance_actions.CONFIRM_RESIZE)
3482 
3483         self.compute_rpcapi.confirm_resize(context,
3484                                            instance,
3485                                            migration,
3486                                            migration.source_compute)
3487 
3488     # TODO(mriedem): It looks like for resize (not cold migrate) the only
3489     # possible kwarg here is auto_disk_config. Drop this dumb **kwargs and make
3490     # it explicitly an auto_disk_config param
3491     @check_instance_lock
3492     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3493     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3494                host_name=None, **extra_instance_updates):
3495         """Resize (ie, migrate) a running instance.
3496 
3497         If flavor_id is None, the process is considered a migration, keeping
3498         the original flavor_id. If flavor_id is not None, the instance should
3499         be migrated to a new host and resized to the new flavor_id.
3500         host_name is always None in the resize case.
3501         host_name can be set in the cold migration case only.
3502         """
3503         if host_name is not None:
3504             # Cannot migrate to the host where the instance exists
3505             # because it is useless.
3506             if host_name == instance.host:
3507                 raise exception.CannotMigrateToSameHost()
3508 
3509             # Check whether host exists or not.
3510             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3511                 context, host_name, use_slave=True)
3512 
3513         self._check_auto_disk_config(instance, **extra_instance_updates)
3514 
3515         current_instance_type = instance.get_flavor()
3516 
3517         # If flavor_id is not provided, only migrate the instance.
3518         volume_backed = None
3519         if not flavor_id:
3520             LOG.debug("flavor_id is None. Assuming migration.",
3521                       instance=instance)
3522             new_instance_type = current_instance_type
3523         else:
3524             new_instance_type = flavors.get_flavor_by_flavor_id(
3525                     flavor_id, read_deleted="no")
3526             # Check to see if we're resizing to a zero-disk flavor which is
3527             # only supported with volume-backed servers.
3528             if (new_instance_type.get('root_gb') == 0 and
3529                     current_instance_type.get('root_gb') != 0):
3530                 volume_backed = compute_utils.is_volume_backed_instance(
3531                         context, instance)
3532                 if not volume_backed:
3533                     reason = _('Resize to zero disk flavor is not allowed.')
3534                     raise exception.CannotResizeDisk(reason=reason)
3535 
3536         current_instance_type_name = current_instance_type['name']
3537         new_instance_type_name = new_instance_type['name']
3538         LOG.debug("Old instance type %(current_instance_type_name)s, "
3539                   "new instance type %(new_instance_type_name)s",
3540                   {'current_instance_type_name': current_instance_type_name,
3541                    'new_instance_type_name': new_instance_type_name},
3542                   instance=instance)
3543 
3544         same_instance_type = (current_instance_type['id'] ==
3545                               new_instance_type['id'])
3546 
3547         # NOTE(sirp): We don't want to force a customer to change their flavor
3548         # when Ops is migrating off of a failed host.
3549         if not same_instance_type and new_instance_type.get('disabled'):
3550             raise exception.FlavorNotFound(flavor_id=flavor_id)
3551 
3552         if same_instance_type and flavor_id:
3553             raise exception.CannotResizeToSameFlavor()
3554 
3555         # ensure there is sufficient headroom for upsizes
3556         if flavor_id:
3557             self._check_quota_for_upsize(context, instance,
3558                                          current_instance_type,
3559                                          new_instance_type)
3560 
3561         if not same_instance_type:
3562             image = utils.get_image_from_system_metadata(
3563                 instance.system_metadata)
3564             # Figure out if the instance is volume-backed but only if we didn't
3565             # already figure that out above (avoid the extra db hit).
3566             if volume_backed is None:
3567                 volume_backed = compute_utils.is_volume_backed_instance(
3568                     context, instance)
3569             # If the server is volume-backed, we still want to validate numa
3570             # and pci information in the new flavor, but we don't call
3571             # _validate_flavor_image_nostatus because how it handles checking
3572             # disk size validation was not intended for a volume-backed
3573             # resize case.
3574             if volume_backed:
3575                 self._validate_flavor_image_numa_pci(
3576                     image, new_instance_type, validate_pci=True)
3577             else:
3578                 self._validate_flavor_image_nostatus(
3579                     context, image, new_instance_type, root_bdm=None,
3580                     validate_pci=True)
3581 
3582         filter_properties = {'ignore_hosts': []}
3583 
3584         if not CONF.allow_resize_to_same_host:
3585             filter_properties['ignore_hosts'].append(instance.host)
3586 
3587         request_spec = objects.RequestSpec.get_by_instance_uuid(
3588             context, instance.uuid)
3589         request_spec.ignore_hosts = filter_properties['ignore_hosts']
3590 
3591         instance.task_state = task_states.RESIZE_PREP
3592         instance.progress = 0
3593         instance.update(extra_instance_updates)
3594         instance.save(expected_task_state=[None])
3595 
3596         if not flavor_id:
3597             self._record_action_start(context, instance,
3598                                       instance_actions.MIGRATE)
3599         else:
3600             self._record_action_start(context, instance,
3601                                       instance_actions.RESIZE)
3602 
3603         # TODO(melwitt): We're not rechecking for strict quota here to guard
3604         # against going over quota during a race at this time because the
3605         # resource consumption for this operation is written to the database
3606         # by compute.
3607         scheduler_hint = {'filter_properties': filter_properties}
3608 
3609         if host_name is None:
3610             # If 'host_name' is not specified,
3611             # clear the 'requested_destination' field of the RequestSpec.
3612             request_spec.requested_destination = None
3613         else:
3614             # Set the host and the node so that the scheduler will
3615             # validate them.
3616             request_spec.requested_destination = objects.Destination(
3617                 host=node.host, node=node.hypervisor_hostname)
3618 
3619         self.compute_task_api.resize_instance(context, instance,
3620             scheduler_hint=scheduler_hint,
3621             flavor=new_instance_type,
3622             clean_shutdown=clean_shutdown,
3623             request_spec=request_spec)
3624 
3625     @check_instance_lock
3626     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3627                                     vm_states.PAUSED, vm_states.SUSPENDED])
3628     def shelve(self, context, instance, clean_shutdown=True):
3629         """Shelve an instance.
3630 
3631         Shuts down an instance and frees it up to be removed from the
3632         hypervisor.
3633         """
3634         instance.task_state = task_states.SHELVING
3635         instance.save(expected_task_state=[None])
3636 
3637         self._record_action_start(context, instance, instance_actions.SHELVE)
3638 
3639         if not compute_utils.is_volume_backed_instance(context, instance):
3640             name = '%s-shelved' % instance.display_name
3641             image_meta = compute_utils.create_image(
3642                 context, instance, name, 'snapshot', self.image_api)
3643             image_id = image_meta['id']
3644             self.compute_rpcapi.shelve_instance(context, instance=instance,
3645                     image_id=image_id, clean_shutdown=clean_shutdown)
3646         else:
3647             self.compute_rpcapi.shelve_offload_instance(context,
3648                     instance=instance, clean_shutdown=clean_shutdown)
3649 
3650     @check_instance_lock
3651     @check_instance_state(vm_state=[vm_states.SHELVED])
3652     def shelve_offload(self, context, instance, clean_shutdown=True):
3653         """Remove a shelved instance from the hypervisor."""
3654         instance.task_state = task_states.SHELVING_OFFLOADING
3655         instance.save(expected_task_state=[None])
3656 
3657         self._record_action_start(context, instance,
3658                                   instance_actions.SHELVE_OFFLOAD)
3659 
3660         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3661             clean_shutdown=clean_shutdown)
3662 
3663     @check_instance_lock
3664     @check_instance_state(vm_state=[vm_states.SHELVED,
3665         vm_states.SHELVED_OFFLOADED])
3666     def unshelve(self, context, instance):
3667         """Restore a shelved instance."""
3668         request_spec = objects.RequestSpec.get_by_instance_uuid(
3669             context, instance.uuid)
3670 
3671         instance.task_state = task_states.UNSHELVING
3672         instance.save(expected_task_state=[None])
3673 
3674         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3675 
3676         self.compute_task_api.unshelve_instance(context, instance,
3677                                                 request_spec)
3678 
3679     @check_instance_lock
3680     def add_fixed_ip(self, context, instance, network_id):
3681         """Add fixed_ip from specified network to given instance."""
3682         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3683                 instance=instance, network_id=network_id)
3684 
3685     @check_instance_lock
3686     def remove_fixed_ip(self, context, instance, address):
3687         """Remove fixed_ip from specified network to given instance."""
3688         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3689                 instance=instance, address=address)
3690 
3691     @check_instance_lock
3692     @check_instance_state(vm_state=[vm_states.ACTIVE])
3693     def pause(self, context, instance):
3694         """Pause the given instance."""
3695         instance.task_state = task_states.PAUSING
3696         instance.save(expected_task_state=[None])
3697         self._record_action_start(context, instance, instance_actions.PAUSE)
3698         self.compute_rpcapi.pause_instance(context, instance)
3699 
3700     @check_instance_lock
3701     @check_instance_state(vm_state=[vm_states.PAUSED])
3702     def unpause(self, context, instance):
3703         """Unpause the given instance."""
3704         instance.task_state = task_states.UNPAUSING
3705         instance.save(expected_task_state=[None])
3706         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3707         self.compute_rpcapi.unpause_instance(context, instance)
3708 
3709     @check_instance_host
3710     def get_diagnostics(self, context, instance):
3711         """Retrieve diagnostics for the given instance."""
3712         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3713 
3714     @check_instance_host
3715     def get_instance_diagnostics(self, context, instance):
3716         """Retrieve diagnostics for the given instance."""
3717         return self.compute_rpcapi.get_instance_diagnostics(context,
3718                                                             instance=instance)
3719 
3720     @check_instance_lock
3721     @check_instance_state(vm_state=[vm_states.ACTIVE])
3722     def suspend(self, context, instance):
3723         """Suspend the given instance."""
3724         instance.task_state = task_states.SUSPENDING
3725         instance.save(expected_task_state=[None])
3726         self._record_action_start(context, instance, instance_actions.SUSPEND)
3727         self.compute_rpcapi.suspend_instance(context, instance)
3728 
3729     @check_instance_lock
3730     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3731     def resume(self, context, instance):
3732         """Resume the given instance."""
3733         instance.task_state = task_states.RESUMING
3734         instance.save(expected_task_state=[None])
3735         self._record_action_start(context, instance, instance_actions.RESUME)
3736         self.compute_rpcapi.resume_instance(context, instance)
3737 
3738     @check_instance_lock
3739     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3740                                     vm_states.ERROR])
3741     def rescue(self, context, instance, rescue_password=None,
3742                rescue_image_ref=None, clean_shutdown=True):
3743         """Rescue the given instance."""
3744 
3745         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3746                     context, instance.uuid)
3747         for bdm in bdms:
3748             if bdm.volume_id:
3749                 vol = self.volume_api.get(context, bdm.volume_id)
3750                 self.volume_api.check_attached(context, vol)
3751         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3752             reason = _("Cannot rescue a volume-backed instance")
3753             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3754                                                  reason=reason)
3755 
3756         instance.task_state = task_states.RESCUING
3757         instance.save(expected_task_state=[None])
3758 
3759         self._record_action_start(context, instance, instance_actions.RESCUE)
3760 
3761         self.compute_rpcapi.rescue_instance(context, instance=instance,
3762             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3763             clean_shutdown=clean_shutdown)
3764 
3765     @check_instance_lock
3766     @check_instance_state(vm_state=[vm_states.RESCUED])
3767     def unrescue(self, context, instance):
3768         """Unrescue the given instance."""
3769         instance.task_state = task_states.UNRESCUING
3770         instance.save(expected_task_state=[None])
3771 
3772         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3773 
3774         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3775 
3776     @check_instance_lock
3777     @check_instance_state(vm_state=[vm_states.ACTIVE])
3778     def set_admin_password(self, context, instance, password=None):
3779         """Set the root/admin password for the given instance.
3780 
3781         @param context: Nova auth context.
3782         @param instance: Nova instance object.
3783         @param password: The admin password for the instance.
3784         """
3785         instance.task_state = task_states.UPDATING_PASSWORD
3786         instance.save(expected_task_state=[None])
3787 
3788         self._record_action_start(context, instance,
3789                                   instance_actions.CHANGE_PASSWORD)
3790 
3791         self.compute_rpcapi.set_admin_password(context,
3792                                                instance=instance,
3793                                                new_pass=password)
3794 
3795     @check_instance_host
3796     @reject_instance_state(
3797         task_state=[task_states.DELETING, task_states.MIGRATING])
3798     def get_vnc_console(self, context, instance, console_type):
3799         """Get a url to an instance Console."""
3800         connect_info = self.compute_rpcapi.get_vnc_console(context,
3801                 instance=instance, console_type=console_type)
3802         return {'url': connect_info['access_url']}
3803 
3804     @check_instance_host
3805     @reject_instance_state(
3806         task_state=[task_states.DELETING, task_states.MIGRATING])
3807     def get_spice_console(self, context, instance, console_type):
3808         """Get a url to an instance Console."""
3809         connect_info = self.compute_rpcapi.get_spice_console(context,
3810                 instance=instance, console_type=console_type)
3811         return {'url': connect_info['access_url']}
3812 
3813     @check_instance_host
3814     @reject_instance_state(
3815         task_state=[task_states.DELETING, task_states.MIGRATING])
3816     def get_rdp_console(self, context, instance, console_type):
3817         """Get a url to an instance Console."""
3818         connect_info = self.compute_rpcapi.get_rdp_console(context,
3819                 instance=instance, console_type=console_type)
3820         return {'url': connect_info['access_url']}
3821 
3822     @check_instance_host
3823     @reject_instance_state(
3824         task_state=[task_states.DELETING, task_states.MIGRATING])
3825     def get_serial_console(self, context, instance, console_type):
3826         """Get a url to a serial console."""
3827         connect_info = self.compute_rpcapi.get_serial_console(context,
3828                 instance=instance, console_type=console_type)
3829         return {'url': connect_info['access_url']}
3830 
3831     @check_instance_host
3832     @reject_instance_state(
3833         task_state=[task_states.DELETING, task_states.MIGRATING])
3834     def get_mks_console(self, context, instance, console_type):
3835         """Get a url to a MKS console."""
3836         connect_info = self.compute_rpcapi.get_mks_console(context,
3837                 instance=instance, console_type=console_type)
3838         return {'url': connect_info['access_url']}
3839 
3840     @check_instance_host
3841     def get_console_output(self, context, instance, tail_length=None):
3842         """Get console output for an instance."""
3843         return self.compute_rpcapi.get_console_output(context,
3844                 instance=instance, tail_length=tail_length)
3845 
3846     def lock(self, context, instance, reason=None):
3847         """Lock the given instance."""
3848         # Only update the lock if we are an admin (non-owner)
3849         is_owner = instance.project_id == context.project_id
3850         if instance.locked and is_owner:
3851             return
3852 
3853         context = context.elevated()
3854         self._record_action_start(context, instance,
3855                                   instance_actions.LOCK)
3856 
3857         @wrap_instance_event(prefix='api')
3858         def lock(self, context, instance, reason=None):
3859             LOG.debug('Locking', instance=instance)
3860             instance.locked = True
3861             instance.locked_by = 'owner' if is_owner else 'admin'
3862             if reason:
3863                 instance.system_metadata['locked_reason'] = reason
3864             instance.save()
3865 
3866         lock(self, context, instance, reason=reason)
3867         compute_utils.notify_about_instance_action(
3868             context, instance, CONF.host,
3869             action=fields_obj.NotificationAction.LOCK,
3870             source=fields_obj.NotificationSource.API)
3871 
3872     def is_expected_locked_by(self, context, instance):
3873         is_owner = instance.project_id == context.project_id
3874         expect_locked_by = 'owner' if is_owner else 'admin'
3875         locked_by = instance.locked_by
3876         if locked_by and locked_by != expect_locked_by:
3877             return False
3878         return True
3879 
3880     def unlock(self, context, instance):
3881         """Unlock the given instance."""
3882         context = context.elevated()
3883         self._record_action_start(context, instance,
3884                                   instance_actions.UNLOCK)
3885 
3886         @wrap_instance_event(prefix='api')
3887         def unlock(self, context, instance):
3888             LOG.debug('Unlocking', instance=instance)
3889             instance.locked = False
3890             instance.locked_by = None
3891             instance.system_metadata.pop('locked_reason', None)
3892             instance.save()
3893 
3894         unlock(self, context, instance)
3895         compute_utils.notify_about_instance_action(
3896             context, instance, CONF.host,
3897             action=fields_obj.NotificationAction.UNLOCK,
3898             source=fields_obj.NotificationSource.API)
3899 
3900     @check_instance_lock
3901     def reset_network(self, context, instance):
3902         """Reset networking on the instance."""
3903         self.compute_rpcapi.reset_network(context, instance=instance)
3904 
3905     @check_instance_lock
3906     def inject_network_info(self, context, instance):
3907         """Inject network info for the instance."""
3908         self.compute_rpcapi.inject_network_info(context, instance=instance)
3909 
3910     def _create_volume_bdm(self, context, instance, device, volume,
3911                            disk_bus, device_type, is_local_creation=False,
3912                            tag=None):
3913         volume_id = volume['id']
3914         if is_local_creation:
3915             # when the creation is done locally we can't specify the device
3916             # name as we do not have a way to check that the name specified is
3917             # a valid one.
3918             # We leave the setting of that value when the actual attach
3919             # happens on the compute manager
3920             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
3921             # support device tagging because we have no way to call the compute
3922             # manager to check that it supports device tagging. In fact, we
3923             # don't even know which computer manager the instance will
3924             # eventually end up on when it's unshelved.
3925             volume_bdm = objects.BlockDeviceMapping(
3926                 context=context,
3927                 source_type='volume', destination_type='volume',
3928                 instance_uuid=instance.uuid, boot_index=None,
3929                 volume_id=volume_id,
3930                 device_name=None, guest_format=None,
3931                 disk_bus=disk_bus, device_type=device_type)
3932             volume_bdm.create()
3933         else:
3934             # NOTE(vish): This is done on the compute host because we want
3935             #             to avoid a race where two devices are requested at
3936             #             the same time. When db access is removed from
3937             #             compute, the bdm will be created here and we will
3938             #             have to make sure that they are assigned atomically.
3939             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3940                 context, instance, device, volume_id, disk_bus=disk_bus,
3941                 device_type=device_type, tag=tag,
3942                 multiattach=volume['multiattach'])
3943         return volume_bdm
3944 
3945     def _check_volume_already_attached_to_instance(self, context, instance,
3946                                                    volume_id):
3947         """Avoid attaching the same volume to the same instance twice.
3948 
3949            As the new Cinder flow (microversion 3.44) is handling the checks
3950            differently and allows to attach the same volume to the same
3951            instance twice to enable live_migrate we are checking whether the
3952            BDM already exists for this combination for the new flow and fail
3953            if it does.
3954         """
3955 
3956         try:
3957             objects.BlockDeviceMapping.get_by_volume_and_instance(
3958                 context, volume_id, instance.uuid)
3959 
3960             msg = _("volume %s already attached") % volume_id
3961             raise exception.InvalidVolume(reason=msg)
3962         except exception.VolumeBDMNotFound:
3963             pass
3964 
3965     def _check_attach_and_reserve_volume(self, context, volume, instance,
3966                                          bdm, supports_multiattach=False):
3967         volume_id = volume['id']
3968         self.volume_api.check_availability_zone(context, volume,
3969                                                 instance=instance)
3970         # If volume.multiattach=True and the microversion to
3971         # support multiattach is not used, fail the request.
3972         if volume['multiattach'] and not supports_multiattach:
3973             raise exception.MultiattachNotSupportedOldMicroversion()
3974 
3975         attachment_id = self.volume_api.attachment_create(
3976             context, volume_id, instance.uuid)['id']
3977         bdm.attachment_id = attachment_id
3978         # NOTE(ildikov): In case of boot from volume the BDM at this
3979         # point is not yet created in a cell database, so we can't
3980         # call save().  When attaching a volume to an existing
3981         # instance, the instance is already in a cell and the BDM has
3982         # been created in that same cell so updating here in that case
3983         # is "ok".
3984         if bdm.obj_attr_is_set('id'):
3985             bdm.save()
3986 
3987     # TODO(stephenfin): Fold this back in now that cells v1 no longer needs to
3988     # override it.
3989     def _attach_volume(self, context, instance, volume, device,
3990                        disk_bus, device_type, tag=None,
3991                        supports_multiattach=False):
3992         """Attach an existing volume to an existing instance.
3993 
3994         This method is separated to make it possible for cells version
3995         to override it.
3996         """
3997         volume_bdm = self._create_volume_bdm(
3998             context, instance, device, volume, disk_bus=disk_bus,
3999             device_type=device_type, tag=tag)
4000         try:
4001             self._check_attach_and_reserve_volume(context, volume, instance,
4002                                                   volume_bdm,
4003                                                   supports_multiattach)
4004             self._record_action_start(
4005                 context, instance, instance_actions.ATTACH_VOLUME)
4006             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
4007         except Exception:
4008             with excutils.save_and_reraise_exception():
4009                 volume_bdm.destroy()
4010 
4011         return volume_bdm.device_name
4012 
4013     def _attach_volume_shelved_offloaded(self, context, instance, volume,
4014                                          device, disk_bus, device_type):
4015         """Attach an existing volume to an instance in shelved offloaded state.
4016 
4017         Attaching a volume for an instance in shelved offloaded state requires
4018         to perform the regular check to see if we can attach and reserve the
4019         volume then we need to call the attach method on the volume API
4020         to mark the volume as 'in-use'.
4021         The instance at this stage is not managed by a compute manager
4022         therefore the actual attachment will be performed once the
4023         instance will be unshelved.
4024         """
4025         volume_id = volume['id']
4026 
4027         @wrap_instance_event(prefix='api')
4028         def attach_volume(self, context, v_id, instance, dev, attachment_id):
4029             if attachment_id:
4030                 # Normally we wouldn't complete an attachment without a host
4031                 # connector, but we do this to make the volume status change
4032                 # to "in-use" to maintain the API semantics with the old flow.
4033                 # When unshelving the instance, the compute service will deal
4034                 # with this disconnected attachment.
4035                 self.volume_api.attachment_complete(context, attachment_id)
4036             else:
4037                 self.volume_api.attach(context,
4038                                        v_id,
4039                                        instance.uuid,
4040                                        dev)
4041 
4042         volume_bdm = self._create_volume_bdm(
4043             context, instance, device, volume, disk_bus=disk_bus,
4044             device_type=device_type, is_local_creation=True)
4045         try:
4046             self._check_attach_and_reserve_volume(context, volume, instance,
4047                                                   volume_bdm)
4048             self._record_action_start(
4049                 context, instance,
4050                 instance_actions.ATTACH_VOLUME)
4051             attach_volume(self, context, volume_id, instance, device,
4052                           volume_bdm.attachment_id)
4053         except Exception:
4054             with excutils.save_and_reraise_exception():
4055                 volume_bdm.destroy()
4056 
4057         return volume_bdm.device_name
4058 
4059     @check_instance_lock
4060     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4061                                     vm_states.STOPPED, vm_states.RESIZED,
4062                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4063                                     vm_states.SHELVED_OFFLOADED])
4064     def attach_volume(self, context, instance, volume_id, device=None,
4065                       disk_bus=None, device_type=None, tag=None,
4066                       supports_multiattach=False):
4067         """Attach an existing volume to an existing instance."""
4068         # NOTE(vish): Fail fast if the device is not going to pass. This
4069         #             will need to be removed along with the test if we
4070         #             change the logic in the manager for what constitutes
4071         #             a valid device.
4072         if device and not block_device.match_device(device):
4073             raise exception.InvalidDevicePath(path=device)
4074 
4075         # Make sure the volume isn't already attached to this instance
4076         # because we'll use the v3.44 attachment flow in
4077         # _check_attach_and_reserve_volume and Cinder will allow multiple
4078         # attachments between the same volume and instance but the old flow
4079         # API semantics don't allow that so we enforce it here.
4080         self._check_volume_already_attached_to_instance(context,
4081                                                         instance,
4082                                                         volume_id)
4083 
4084         volume = self.volume_api.get(context, volume_id)
4085         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
4086         if is_shelved_offloaded:
4087             if tag:
4088                 # NOTE(artom) Local attach (to a shelved-offload instance)
4089                 # cannot support device tagging because we have no way to call
4090                 # the compute manager to check that it supports device tagging.
4091                 # In fact, we don't even know which computer manager the
4092                 # instance will eventually end up on when it's unshelved.
4093                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
4094             if volume['multiattach']:
4095                 # NOTE(mriedem): Similar to tagged attach, we don't support
4096                 # attaching a multiattach volume to shelved offloaded instances
4097                 # because we can't tell if the compute host (since there isn't
4098                 # one) supports it. This could possibly be supported in the
4099                 # future if the scheduler was made aware of which computes
4100                 # support multiattach volumes.
4101                 raise exception.MultiattachToShelvedNotSupported()
4102             return self._attach_volume_shelved_offloaded(context,
4103                                                          instance,
4104                                                          volume,
4105                                                          device,
4106                                                          disk_bus,
4107                                                          device_type)
4108 
4109         return self._attach_volume(context, instance, volume, device,
4110                                    disk_bus, device_type, tag=tag,
4111                                    supports_multiattach=supports_multiattach)
4112 
4113     # TODO(stephenfin): Fold this back in now that cells v1 no longer needs to
4114     # override it.
4115     def _detach_volume(self, context, instance, volume):
4116         """Detach volume from instance.
4117 
4118         This method is separated to make it easier for cells version
4119         to override.
4120         """
4121         try:
4122             self.volume_api.begin_detaching(context, volume['id'])
4123         except exception.InvalidInput as exc:
4124             raise exception.InvalidVolume(reason=exc.format_message())
4125         attachments = volume.get('attachments', {})
4126         attachment_id = None
4127         if attachments and instance.uuid in attachments:
4128             attachment_id = attachments[instance.uuid]['attachment_id']
4129         self._record_action_start(
4130             context, instance, instance_actions.DETACH_VOLUME)
4131         self.compute_rpcapi.detach_volume(context, instance=instance,
4132                 volume_id=volume['id'], attachment_id=attachment_id)
4133 
4134     def _detach_volume_shelved_offloaded(self, context, instance, volume):
4135         """Detach a volume from an instance in shelved offloaded state.
4136 
4137         If the instance is shelved offloaded we just need to cleanup volume
4138         calling the volume api detach, the volume api terminate_connection
4139         and delete the bdm record.
4140         If the volume has delete_on_termination option set then we call the
4141         volume api delete as well.
4142         """
4143         @wrap_instance_event(prefix='api')
4144         def detach_volume(self, context, instance, bdms):
4145             self._local_cleanup_bdm_volumes(bdms, instance, context)
4146 
4147         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
4148                 context, volume['id'], instance.uuid)]
4149         # The begin_detaching() call only works with in-use volumes,
4150         # which will not be the case for volumes attached to a shelved
4151         # offloaded server via the attachments API since those volumes
4152         # will have `reserved` status.
4153         if not bdms[0].attachment_id:
4154             try:
4155                 self.volume_api.begin_detaching(context, volume['id'])
4156             except exception.InvalidInput as exc:
4157                 raise exception.InvalidVolume(reason=exc.format_message())
4158         self._record_action_start(
4159             context, instance,
4160             instance_actions.DETACH_VOLUME)
4161         detach_volume(self, context, instance, bdms)
4162 
4163     @check_instance_lock
4164     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4165                                     vm_states.STOPPED, vm_states.RESIZED,
4166                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4167                                     vm_states.SHELVED_OFFLOADED])
4168     def detach_volume(self, context, instance, volume):
4169         """Detach a volume from an instance."""
4170         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4171             self._detach_volume_shelved_offloaded(context, instance, volume)
4172         else:
4173             self._detach_volume(context, instance, volume)
4174 
4175     def _count_attachments_for_swap(self, ctxt, volume):
4176         """Counts the number of attachments for a swap-related volume.
4177 
4178         Attempts to only count read/write attachments if the volume attachment
4179         records exist, otherwise simply just counts the number of attachments
4180         regardless of attach mode.
4181 
4182         :param ctxt: nova.context.RequestContext - user request context
4183         :param volume: nova-translated volume dict from nova.volume.cinder.
4184         :returns: count of attachments for the volume
4185         """
4186         # This is a dict, keyed by server ID, to a dict of attachment_id and
4187         # mountpoint.
4188         attachments = volume.get('attachments', {})
4189         # Multiattach volumes can have more than one attachment, so if there
4190         # is more than one attachment, attempt to count the read/write
4191         # attachments.
4192         if len(attachments) > 1:
4193             count = 0
4194             for attachment in attachments.values():
4195                 attachment_id = attachment['attachment_id']
4196                 # Get the attachment record for this attachment so we can
4197                 # get the attach_mode.
4198                 # TODO(mriedem): This could be optimized if we had
4199                 # GET /attachments/detail?volume_id=volume['id'] in Cinder.
4200                 try:
4201                     attachment_record = self.volume_api.attachment_get(
4202                         ctxt, attachment_id)
4203                     # Note that the attachment record from Cinder has
4204                     # attach_mode in the top-level of the resource but the
4205                     # nova.volume.cinder code translates it and puts the
4206                     # attach_mode in the connection_info for some legacy
4207                     # reason...
4208                     if attachment_record['attach_mode'] == 'rw':
4209                         count += 1
4210                 except exception.VolumeAttachmentNotFound:
4211                     # attachments are read/write by default so count it
4212                     count += 1
4213         else:
4214             count = len(attachments)
4215 
4216         return count
4217 
4218     @check_instance_lock
4219     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4220                                     vm_states.RESIZED])
4221     def swap_volume(self, context, instance, old_volume, new_volume):
4222         """Swap volume attached to an instance."""
4223         # The caller likely got the instance from volume['attachments']
4224         # in the first place, but let's sanity check.
4225         if not old_volume.get('attachments', {}).get(instance.uuid):
4226             msg = _("Old volume is attached to a different instance.")
4227             raise exception.InvalidVolume(reason=msg)
4228         if new_volume['attach_status'] == 'attached':
4229             msg = _("New volume must be detached in order to swap.")
4230             raise exception.InvalidVolume(reason=msg)
4231         if int(new_volume['size']) < int(old_volume['size']):
4232             msg = _("New volume must be the same size or larger.")
4233             raise exception.InvalidVolume(reason=msg)
4234         self.volume_api.check_availability_zone(context, new_volume,
4235                                                 instance=instance)
4236         try:
4237             self.volume_api.begin_detaching(context, old_volume['id'])
4238         except exception.InvalidInput as exc:
4239             raise exception.InvalidVolume(reason=exc.format_message())
4240 
4241         # Disallow swapping from multiattach volumes that have more than one
4242         # read/write attachment. We know the old_volume has at least one
4243         # attachment since it's attached to this server. The new_volume
4244         # can't have any attachments because of the attach_status check above.
4245         # We do this count after calling "begin_detaching" to lock against
4246         # concurrent attachments being made while we're counting.
4247         try:
4248             if self._count_attachments_for_swap(context, old_volume) > 1:
4249                 raise exception.MultiattachSwapVolumeNotSupported()
4250         except Exception:  # This is generic to handle failures while counting
4251             # We need to reset the detaching status before raising.
4252             with excutils.save_and_reraise_exception():
4253                 self.volume_api.roll_detaching(context, old_volume['id'])
4254 
4255         # Get the BDM for the attached (old) volume so we can tell if it was
4256         # attached with the new-style Cinder 3.44 API.
4257         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4258             context, old_volume['id'], instance.uuid)
4259         new_attachment_id = None
4260         if bdm.attachment_id is None:
4261             # This is an old-style attachment so reserve the new volume before
4262             # we cast to the compute host.
4263             self.volume_api.reserve_volume(context, new_volume['id'])
4264         else:
4265             try:
4266                 self._check_volume_already_attached_to_instance(
4267                     context, instance, new_volume['id'])
4268             except exception.InvalidVolume:
4269                 with excutils.save_and_reraise_exception():
4270                     self.volume_api.roll_detaching(context, old_volume['id'])
4271 
4272             # This is a new-style attachment so for the volume that we are
4273             # going to swap to, create a new volume attachment.
4274             new_attachment_id = self.volume_api.attachment_create(
4275                 context, new_volume['id'], instance.uuid)['id']
4276 
4277         self._record_action_start(
4278             context, instance, instance_actions.SWAP_VOLUME)
4279 
4280         try:
4281             self.compute_rpcapi.swap_volume(
4282                     context, instance=instance,
4283                     old_volume_id=old_volume['id'],
4284                     new_volume_id=new_volume['id'],
4285                     new_attachment_id=new_attachment_id)
4286         except Exception:
4287             with excutils.save_and_reraise_exception():
4288                 self.volume_api.roll_detaching(context, old_volume['id'])
4289                 if new_attachment_id is None:
4290                     self.volume_api.unreserve_volume(context, new_volume['id'])
4291                 else:
4292                     self.volume_api.attachment_delete(
4293                         context, new_attachment_id)
4294 
4295     @check_instance_lock
4296     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4297                                     vm_states.STOPPED],
4298                           task_state=[None])
4299     def attach_interface(self, context, instance, network_id, port_id,
4300                          requested_ip, tag=None):
4301         """Use hotplug to add an network adapter to an instance."""
4302         self._record_action_start(
4303             context, instance, instance_actions.ATTACH_INTERFACE)
4304 
4305         # NOTE(gibi): Checking if the requested port has resource request as
4306         # such ports are currently not supported as they would at least
4307         # need resource allocation manipulation in placement but might also
4308         # need a new scheduling if resource on this host is not available.
4309         if port_id:
4310             port = self.network_api.show_port(context, port_id)
4311             if port['port'].get(constants.RESOURCE_REQUEST):
4312                 raise exception.AttachInterfaceWithQoSPolicyNotSupported(
4313                     instance_uuid=instance.uuid)
4314 
4315         return self.compute_rpcapi.attach_interface(context,
4316             instance=instance, network_id=network_id, port_id=port_id,
4317             requested_ip=requested_ip, tag=tag)
4318 
4319     @check_instance_lock
4320     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4321                                     vm_states.STOPPED],
4322                           task_state=[None])
4323     def detach_interface(self, context, instance, port_id):
4324         """Detach an network adapter from an instance."""
4325         self._record_action_start(
4326             context, instance, instance_actions.DETACH_INTERFACE)
4327         self.compute_rpcapi.detach_interface(context, instance=instance,
4328             port_id=port_id)
4329 
4330     def get_instance_metadata(self, context, instance):
4331         """Get all metadata associated with an instance."""
4332         return self.db.instance_metadata_get(context, instance.uuid)
4333 
4334     @check_instance_lock
4335     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4336                                     vm_states.SUSPENDED, vm_states.STOPPED],
4337                           task_state=None)
4338     def delete_instance_metadata(self, context, instance, key):
4339         """Delete the given metadata item from an instance."""
4340         instance.delete_metadata_key(key)
4341         self.compute_rpcapi.change_instance_metadata(context,
4342                                                      instance=instance,
4343                                                      diff={key: ['-']})
4344 
4345     @check_instance_lock
4346     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4347                                     vm_states.SUSPENDED, vm_states.STOPPED],
4348                           task_state=None)
4349     def update_instance_metadata(self, context, instance,
4350                                  metadata, delete=False):
4351         """Updates or creates instance metadata.
4352 
4353         If delete is True, metadata items that are not specified in the
4354         `metadata` argument will be deleted.
4355 
4356         """
4357         orig = dict(instance.metadata)
4358         if delete:
4359             _metadata = metadata
4360         else:
4361             _metadata = dict(instance.metadata)
4362             _metadata.update(metadata)
4363 
4364         self._check_metadata_properties_quota(context, _metadata)
4365         instance.metadata = _metadata
4366         instance.save()
4367         diff = _diff_dict(orig, instance.metadata)
4368         self.compute_rpcapi.change_instance_metadata(context,
4369                                                      instance=instance,
4370                                                      diff=diff)
4371         return _metadata
4372 
4373     @check_instance_lock
4374     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
4375     def live_migrate(self, context, instance, block_migration,
4376                      disk_over_commit, host_name, force=None, async_=False):
4377         """Migrate a server lively to a new host."""
4378         LOG.debug("Going to try to live migrate instance to %s",
4379                   host_name or "another host", instance=instance)
4380 
4381         if host_name:
4382             # Validate the specified host before changing the instance task
4383             # state.
4384             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
4385 
4386         request_spec = objects.RequestSpec.get_by_instance_uuid(
4387             context, instance.uuid)
4388 
4389         instance.task_state = task_states.MIGRATING
4390         instance.save(expected_task_state=[None])
4391 
4392         self._record_action_start(context, instance,
4393                                   instance_actions.LIVE_MIGRATION)
4394 
4395         # NOTE(sbauza): Force is a boolean by the new related API version
4396         if force is False and host_name:
4397             # Unset the host to make sure we call the scheduler
4398             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
4399             # to behavior in conductor and not great.
4400             host_name = None
4401             # FIXME(sbauza): Since only Ironic driver uses more than one
4402             # compute per service but doesn't support live migrations,
4403             # let's provide the first one.
4404             target = nodes[0]
4405             destination = objects.Destination(
4406                 host=target.host,
4407                 node=target.hypervisor_hostname
4408             )
4409             # This is essentially a hint to the scheduler to only consider
4410             # the specified host but still run it through the filters.
4411             request_spec.requested_destination = destination
4412 
4413         try:
4414             self.compute_task_api.live_migrate_instance(context, instance,
4415                 host_name, block_migration=block_migration,
4416                 disk_over_commit=disk_over_commit,
4417                 request_spec=request_spec, async_=async_)
4418         except oslo_exceptions.MessagingTimeout as messaging_timeout:
4419             with excutils.save_and_reraise_exception():
4420                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
4421                 # occurs, but LM will still be in progress, so write
4422                 # instance fault to database
4423                 compute_utils.add_instance_fault_from_exc(context,
4424                                                           instance,
4425                                                           messaging_timeout)
4426 
4427     @check_instance_lock
4428     @check_instance_state(vm_state=[vm_states.ACTIVE],
4429                           task_state=[task_states.MIGRATING])
4430     def live_migrate_force_complete(self, context, instance, migration_id):
4431         """Force live migration to complete.
4432 
4433         :param context: Security context
4434         :param instance: The instance that is being migrated
4435         :param migration_id: ID of ongoing migration
4436 
4437         """
4438         LOG.debug("Going to try to force live migration to complete",
4439                   instance=instance)
4440 
4441         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
4442         # live migration for particular instance. Also pass migration id to
4443         # compute to double check and avoid possible race condition.
4444         migration = objects.Migration.get_by_id_and_instance(
4445             context, migration_id, instance.uuid)
4446         if migration.status != 'running':
4447             raise exception.InvalidMigrationState(migration_id=migration_id,
4448                                                   instance_uuid=instance.uuid,
4449                                                   state=migration.status,
4450                                                   method='force complete')
4451 
4452         self._record_action_start(
4453             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
4454 
4455         self.compute_rpcapi.live_migration_force_complete(
4456             context, instance, migration)
4457 
4458     @check_instance_lock
4459     @check_instance_state(task_state=[task_states.MIGRATING])
4460     def live_migrate_abort(self, context, instance, migration_id,
4461                            support_abort_in_queue=False):
4462         """Abort an in-progress live migration.
4463 
4464         :param context: Security context
4465         :param instance: The instance that is being migrated
4466         :param migration_id: ID of in-progress live migration
4467         :param support_abort_in_queue: Flag indicating whether we can support
4468             abort migrations in "queued" or "preparing" status.
4469 
4470         """
4471         migration = objects.Migration.get_by_id_and_instance(context,
4472                     migration_id, instance.uuid)
4473         LOG.debug("Going to cancel live migration %s",
4474                   migration.id, instance=instance)
4475 
4476         # If the microversion does not support abort migration in queue,
4477         # we are only be able to abort migrations with `running` status;
4478         # if it is supported, we are able to also abort migrations in
4479         # `queued` and `preparing` status.
4480         allowed_states = ['running']
4481         queued_states = ['queued', 'preparing']
4482         if support_abort_in_queue:
4483             # The user requested a microversion that supports aborting a queued
4484             # or preparing live migration. But we need to check that the
4485             # compute service hosting the instance is new enough to support
4486             # aborting a queued/preparing live migration, so we check the
4487             # service version here.
4488             # TODO(Kevin_Zheng): This service version check can be removed in
4489             # Stein (at the earliest) when the API only supports Rocky or
4490             # newer computes.
4491             if migration.status in queued_states:
4492                 service = objects.Service.get_by_compute_host(
4493                     context, instance.host)
4494                 if service.version < MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION:
4495                     raise exception.AbortQueuedLiveMigrationNotYetSupported(
4496                         migration_id=migration_id, status=migration.status)
4497             allowed_states.extend(queued_states)
4498 
4499         if migration.status not in allowed_states:
4500             raise exception.InvalidMigrationState(migration_id=migration_id,
4501                     instance_uuid=instance.uuid,
4502                     state=migration.status,
4503                     method='abort live migration')
4504         self._record_action_start(context, instance,
4505                                   instance_actions.LIVE_MIGRATION_CANCEL)
4506 
4507         self.compute_rpcapi.live_migration_abort(context,
4508                 instance, migration.id)
4509 
4510     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4511                                     vm_states.ERROR])
4512     def evacuate(self, context, instance, host, on_shared_storage,
4513                  admin_password=None, force=None):
4514         """Running evacuate to target host.
4515 
4516         Checking vm compute host state, if the host not in expected_state,
4517         raising an exception.
4518 
4519         :param instance: The instance to evacuate
4520         :param host: Target host. if not set, the scheduler will pick up one
4521         :param on_shared_storage: True if instance files on shared storage
4522         :param admin_password: password to set on rebuilt instance
4523         :param force: Force the evacuation to the specific host target
4524 
4525         """
4526         LOG.debug('vm evacuation scheduled', instance=instance)
4527         inst_host = instance.host
4528         service = objects.Service.get_by_compute_host(context, inst_host)
4529         if self.servicegroup_api.service_is_up(service):
4530             LOG.error('Instance compute service state on %s '
4531                       'expected to be down, but it was up.', inst_host)
4532             raise exception.ComputeServiceInUse(host=inst_host)
4533 
4534         request_spec = objects.RequestSpec.get_by_instance_uuid(
4535             context, instance.uuid)
4536 
4537         instance.task_state = task_states.REBUILDING
4538         instance.save(expected_task_state=[None])
4539         self._record_action_start(context, instance, instance_actions.EVACUATE)
4540 
4541         # NOTE(danms): Create this as a tombstone for the source compute
4542         # to find and cleanup. No need to pass it anywhere else.
4543         migration = objects.Migration(context,
4544                                       source_compute=instance.host,
4545                                       source_node=instance.node,
4546                                       instance_uuid=instance.uuid,
4547                                       status='accepted',
4548                                       migration_type='evacuation')
4549         if host:
4550             migration.dest_compute = host
4551         migration.create()
4552 
4553         compute_utils.notify_about_instance_usage(
4554             self.notifier, context, instance, "evacuate")
4555         compute_utils.notify_about_instance_action(
4556             context, instance, CONF.host,
4557             action=fields_obj.NotificationAction.EVACUATE,
4558             source=fields_obj.NotificationSource.API)
4559 
4560         # NOTE(sbauza): Force is a boolean by the new related API version
4561         # TODO(stephenfin): Any reason we can't use 'not force' here to handle
4562         # the pre-v2.29 API microversion, which wouldn't set force
4563         if force is False and host:
4564             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
4565             # NOTE(sbauza): Unset the host to make sure we call the scheduler
4566             host = None
4567             # FIXME(sbauza): Since only Ironic driver uses more than one
4568             # compute per service but doesn't support evacuations,
4569             # let's provide the first one.
4570             target = nodes[0]
4571             destination = objects.Destination(
4572                 host=target.host,
4573                 node=target.hypervisor_hostname
4574             )
4575             request_spec.requested_destination = destination
4576 
4577         return self.compute_task_api.rebuild_instance(context,
4578                        instance=instance,
4579                        new_pass=admin_password,
4580                        injected_files=None,
4581                        image_ref=None,
4582                        orig_image_ref=None,
4583                        orig_sys_metadata=None,
4584                        bdms=None,
4585                        recreate=True,
4586                        on_shared_storage=on_shared_storage,
4587                        host=host,
4588                        request_spec=request_spec,
4589                        )
4590 
4591     def get_migrations(self, context, filters):
4592         """Get all migrations for the given filters."""
4593         load_cells()
4594 
4595         migrations = []
4596         for cell in CELLS:
4597             if cell.uuid == objects.CellMapping.CELL0_UUID:
4598                 continue
4599             with nova_context.target_cell(context, cell) as cctxt:
4600                 migrations.extend(objects.MigrationList.get_by_filters(
4601                     cctxt, filters).objects)
4602         return objects.MigrationList(objects=migrations)
4603 
4604     def get_migrations_sorted(self, context, filters, sort_dirs=None,
4605                               sort_keys=None, limit=None, marker=None):
4606         """Get all migrations for the given parameters."""
4607         mig_objs = migration_list.get_migration_objects_sorted(
4608             context, filters, limit, marker, sort_keys, sort_dirs)
4609         return mig_objs
4610 
4611     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
4612                                                migration_type=None):
4613         """Get all migrations of an instance in progress."""
4614         return objects.MigrationList.get_in_progress_by_instance(
4615                 context, instance_uuid, migration_type)
4616 
4617     def get_migration_by_id_and_instance(self, context,
4618                                          migration_id, instance_uuid):
4619         """Get the migration of an instance by id."""
4620         return objects.Migration.get_by_id_and_instance(
4621                 context, migration_id, instance_uuid)
4622 
4623     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
4624         """Retrieve a BDM without knowing its cell.
4625 
4626         .. note:: The context will be targeted to the cell in which the
4627             BDM is found, if any.
4628 
4629         :param context: The API request context.
4630         :param volume_id: The ID of the volume.
4631         :param expected_attrs: list of any additional attributes that should
4632             be joined when the BDM is loaded from the database.
4633         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
4634         """
4635         load_cells()
4636         for cell in CELLS:
4637             nova_context.set_target_cell(context, cell)
4638             try:
4639                 return objects.BlockDeviceMapping.get_by_volume(
4640                     context, volume_id, expected_attrs=expected_attrs)
4641             except exception.NotFound:
4642                 continue
4643         raise exception.VolumeBDMNotFound(volume_id=volume_id)
4644 
4645     def volume_snapshot_create(self, context, volume_id, create_info):
4646         bdm = self._get_bdm_by_volume_id(
4647             context, volume_id, expected_attrs=['instance'])
4648 
4649         # We allow creating the snapshot in any vm_state as long as there is
4650         # no task being performed on the instance and it has a host.
4651         @check_instance_host
4652         @check_instance_state(vm_state=None)
4653         def do_volume_snapshot_create(self, context, instance):
4654             self.compute_rpcapi.volume_snapshot_create(context, instance,
4655                     volume_id, create_info)
4656             snapshot = {
4657                 'snapshot': {
4658                     'id': create_info.get('id'),
4659                     'volumeId': volume_id
4660                 }
4661             }
4662             return snapshot
4663 
4664         return do_volume_snapshot_create(self, context, bdm.instance)
4665 
4666     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4667                                delete_info):
4668         bdm = self._get_bdm_by_volume_id(
4669             context, volume_id, expected_attrs=['instance'])
4670 
4671         # We allow deleting the snapshot in any vm_state as long as there is
4672         # no task being performed on the instance and it has a host.
4673         @check_instance_host
4674         @check_instance_state(vm_state=None)
4675         def do_volume_snapshot_delete(self, context, instance):
4676             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4677                     volume_id, snapshot_id, delete_info)
4678 
4679         do_volume_snapshot_delete(self, context, bdm.instance)
4680 
4681     def _handle_power_update_external_event(self, context, instance, tag):
4682         """Power update of an instance prompted by an external event.
4683         :param context: The API request context.
4684         :param instance: The nova instance object.
4685         :param tag: The desired target power state.
4686         :returns Boolean: True if the power-update has been initialized on
4687                           the instance.
4688         """
4689         if not _is_valid_vm_task_power_states(instance, tag):
4690             LOG.debug('The power-update %(tag)s event for instance '
4691                       '%(uuid)s is a no-op since the instance is in '
4692                       'vm_state %(vm_state)s, task_state '
4693                       '%(task_state)s and power_state '
4694                       '%(power_state)s.',
4695                       {'tag': tag, 'uuid': instance.uuid,
4696                       'vm_state': instance.vm_state,
4697                       'task_state': instance.task_state,
4698                       'power_state': instance.power_state})
4699             return False
4700         else:
4701             LOG.debug("Trying to %(state)s instance",
4702                       {'state': tag}, instance=instance)
4703             if tag == common.POWER_ON:
4704                 instance.task_state = task_states.POWERING_ON
4705                 instance.save(expected_task_state=[None])
4706                 self._record_action_start(
4707                     context, instance, instance_actions.START)
4708             else:
4709                 # It's POWER_OFF
4710                 instance.task_state = task_states.POWERING_OFF
4711                 instance.progress = 0
4712                 instance.save(expected_task_state=[None])
4713                 self._record_action_start(
4714                     context, instance, instance_actions.STOP)
4715         return True
4716 
4717     def external_instance_event(self, api_context, instances, events):
4718         # NOTE(danms): The external API consumer just provides events,
4719         # but doesn't know where they go. We need to collate lists
4720         # by the host the affected instance is on and dispatch them
4721         # according to host
4722         instances_by_host = collections.defaultdict(list)
4723         events_by_host = collections.defaultdict(list)
4724         hosts_by_instance = collections.defaultdict(list)
4725         cell_contexts_by_host = {}
4726         for instance in instances:
4727             # instance._context is used here since it's already targeted to
4728             # the cell that the instance lives in, and we need to use that
4729             # cell context to lookup any migrations associated to the instance.
4730             for host in self._get_relevant_hosts(instance._context, instance):
4731                 # NOTE(danms): All instances on a host must have the same
4732                 # mapping, so just use that
4733                 # NOTE(mdbooth): We don't currently support migrations between
4734                 # cells, and given that the Migration record is hosted in the
4735                 # cell _get_relevant_hosts will likely have to change before we
4736                 # do. Consequently we can currently assume that the context for
4737                 # both the source and destination hosts of a migration is the
4738                 # same.
4739                 if host not in cell_contexts_by_host:
4740                     cell_contexts_by_host[host] = instance._context
4741 
4742                 instances_by_host[host].append(instance)
4743                 hosts_by_instance[instance.uuid].append(host)
4744 
4745         for event in events:
4746             if event.name == 'volume-extended':
4747                 # Volume extend is a user-initiated operation starting in the
4748                 # Block Storage service API. We record an instance action so
4749                 # the user can monitor the operation to completion.
4750                 host = hosts_by_instance[event.instance_uuid][0]
4751                 cell_context = cell_contexts_by_host[host]
4752                 objects.InstanceAction.action_start(
4753                     cell_context, event.instance_uuid,
4754                     instance_actions.EXTEND_VOLUME, want_result=False)
4755             elif event.name == 'power-update':
4756                 host = hosts_by_instance[event.instance_uuid][0]
4757                 cell_context = cell_contexts_by_host[host]
4758                 for inst in instances_by_host[host]:
4759                     if inst.uuid == event.instance_uuid:
4760                         # once we find it, we break for performance
4761                         instance = inst
4762                         break
4763                 if not self._handle_power_update_external_event(
4764                         cell_context, instance, event.tag):
4765                     continue
4766 
4767             for host in hosts_by_instance[event.instance_uuid]:
4768                 events_by_host[host].append(event)
4769 
4770         for host in instances_by_host:
4771             cell_context = cell_contexts_by_host[host]
4772 
4773             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4774             # in order to ensure that a failure in processing events on a host
4775             # will not prevent processing events on other hosts
4776             self.compute_rpcapi.external_instance_event(
4777                 cell_context, instances_by_host[host], events_by_host[host],
4778                 host=host)
4779 
4780     def _get_relevant_hosts(self, context, instance):
4781         hosts = set()
4782         hosts.add(instance.host)
4783         if instance.migration_context is not None:
4784             migration_id = instance.migration_context.migration_id
4785             migration = objects.Migration.get_by_id(context, migration_id)
4786             hosts.add(migration.dest_compute)
4787             hosts.add(migration.source_compute)
4788             LOG.debug('Instance %(instance)s is migrating, '
4789                       'copying events to all relevant hosts: '
4790                       '%(hosts)s', {'instance': instance.uuid,
4791                                     'hosts': hosts})
4792         return hosts
4793 
4794     def get_instance_host_status(self, instance):
4795         if instance.host:
4796             try:
4797                 service = [service for service in instance.services if
4798                            service.binary == 'nova-compute'][0]
4799                 if service.forced_down:
4800                     host_status = fields_obj.HostStatus.DOWN
4801                 elif service.disabled:
4802                     host_status = fields_obj.HostStatus.MAINTENANCE
4803                 else:
4804                     alive = self.servicegroup_api.service_is_up(service)
4805                     host_status = ((alive and fields_obj.HostStatus.UP) or
4806                                    fields_obj.HostStatus.UNKNOWN)
4807             except IndexError:
4808                 host_status = fields_obj.HostStatus.NONE
4809         else:
4810             host_status = fields_obj.HostStatus.NONE
4811         return host_status
4812 
4813     def get_instances_host_statuses(self, instance_list):
4814         host_status_dict = dict()
4815         host_statuses = dict()
4816         for instance in instance_list:
4817             if instance.host:
4818                 if instance.host not in host_status_dict:
4819                     host_status = self.get_instance_host_status(instance)
4820                     host_status_dict[instance.host] = host_status
4821                 else:
4822                     host_status = host_status_dict[instance.host]
4823             else:
4824                 host_status = fields_obj.HostStatus.NONE
4825             host_statuses[instance.uuid] = host_status
4826         return host_statuses
4827 
4828 
4829 def target_host_cell(fn):
4830     """Target a host-based function to a cell.
4831 
4832     Expects to wrap a function of signature:
4833 
4834        func(self, context, host, ...)
4835     """
4836 
4837     @functools.wraps(fn)
4838     def targeted(self, context, host, *args, **kwargs):
4839         mapping = objects.HostMapping.get_by_host(context, host)
4840         nova_context.set_target_cell(context, mapping.cell_mapping)
4841         return fn(self, context, host, *args, **kwargs)
4842     return targeted
4843 
4844 
4845 def _find_service_in_cell(context, service_id=None, service_host=None):
4846     """Find a service by id or hostname by searching all cells.
4847 
4848     If one matching service is found, return it. If none or multiple
4849     are found, raise an exception.
4850 
4851     :param context: A context.RequestContext
4852     :param service_id: If not none, the DB ID of the service to find
4853     :param service_host: If not None, the hostname of the service to find
4854     :returns: An objects.Service
4855     :raises: ServiceNotUnique if multiple matching IDs are found
4856     :raises: NotFound if no matches are found
4857     :raises: NovaException if called with neither search option
4858     """
4859 
4860     load_cells()
4861     service = None
4862     found_in_cell = None
4863 
4864     is_uuid = False
4865     if service_id is not None:
4866         is_uuid = uuidutils.is_uuid_like(service_id)
4867         if is_uuid:
4868             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
4869         else:
4870             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
4871     elif service_host is not None:
4872         lookup_fn = lambda c: (
4873             objects.Service.get_by_compute_host(c, service_host))
4874     else:
4875         LOG.exception('_find_service_in_cell called with no search parameters')
4876         # This is intentionally cryptic so we don't leak implementation details
4877         # out of the API.
4878         raise exception.NovaException()
4879 
4880     for cell in CELLS:
4881         # NOTE(danms): Services can be in cell0, so don't skip it here
4882         try:
4883             with nova_context.target_cell(context, cell) as cctxt:
4884                 cell_service = lookup_fn(cctxt)
4885         except exception.NotFound:
4886             # NOTE(danms): Keep looking in other cells
4887             continue
4888         if service and cell_service:
4889             raise exception.ServiceNotUnique()
4890         service = cell_service
4891         found_in_cell = cell
4892         if service and is_uuid:
4893             break
4894 
4895     if service:
4896         # NOTE(danms): Set the cell on the context so it remains
4897         # when we return to our caller
4898         nova_context.set_target_cell(context, found_in_cell)
4899         return service
4900     else:
4901         raise exception.NotFound()
4902 
4903 
4904 class HostAPI(base.Base):
4905     """Sub-set of the Compute Manager API for managing host operations."""
4906 
4907     def __init__(self, rpcapi=None, servicegroup_api=None):
4908         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
4909         self.servicegroup_api = servicegroup_api or servicegroup.API()
4910         super(HostAPI, self).__init__()
4911 
4912     def _assert_host_exists(self, context, host_name, must_be_up=False):
4913         """Raise HostNotFound if compute host doesn't exist."""
4914         service = objects.Service.get_by_compute_host(context, host_name)
4915         if not service:
4916             raise exception.HostNotFound(host=host_name)
4917         if must_be_up and not self.servicegroup_api.service_is_up(service):
4918             raise exception.ComputeServiceUnavailable(host=host_name)
4919         return service['host']
4920 
4921     @wrap_exception()
4922     @target_host_cell
4923     def set_host_enabled(self, context, host_name, enabled):
4924         """Sets the specified host's ability to accept new instances."""
4925         host_name = self._assert_host_exists(context, host_name)
4926         payload = {'host_name': host_name, 'enabled': enabled}
4927         compute_utils.notify_about_host_update(context,
4928                                                'set_enabled.start',
4929                                                payload)
4930         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
4931                 host=host_name)
4932         compute_utils.notify_about_host_update(context,
4933                                                'set_enabled.end',
4934                                                payload)
4935         return result
4936 
4937     @target_host_cell
4938     def get_host_uptime(self, context, host_name):
4939         """Returns the result of calling "uptime" on the target host."""
4940         host_name = self._assert_host_exists(context, host_name,
4941                          must_be_up=True)
4942         return self.rpcapi.get_host_uptime(context, host=host_name)
4943 
4944     @wrap_exception()
4945     @target_host_cell
4946     def host_power_action(self, context, host_name, action):
4947         """Reboots, shuts down or powers up the host."""
4948         host_name = self._assert_host_exists(context, host_name)
4949         payload = {'host_name': host_name, 'action': action}
4950         compute_utils.notify_about_host_update(context,
4951                                                'power_action.start',
4952                                                payload)
4953         result = self.rpcapi.host_power_action(context, action=action,
4954                 host=host_name)
4955         compute_utils.notify_about_host_update(context,
4956                                                'power_action.end',
4957                                                payload)
4958         return result
4959 
4960     @wrap_exception()
4961     @target_host_cell
4962     def set_host_maintenance(self, context, host_name, mode):
4963         """Start/Stop host maintenance window. On start, it triggers
4964         guest VMs evacuation.
4965         """
4966         host_name = self._assert_host_exists(context, host_name)
4967         payload = {'host_name': host_name, 'mode': mode}
4968         compute_utils.notify_about_host_update(context,
4969                                                'set_maintenance.start',
4970                                                payload)
4971         result = self.rpcapi.host_maintenance_mode(context,
4972                 host_param=host_name, mode=mode, host=host_name)
4973         compute_utils.notify_about_host_update(context,
4974                                                'set_maintenance.end',
4975                                                payload)
4976         return result
4977 
4978     def service_get_all(self, context, filters=None, set_zones=False,
4979                         all_cells=False, cell_down_support=False):
4980         """Returns a list of services, optionally filtering the results.
4981 
4982         If specified, 'filters' should be a dictionary containing services
4983         attributes and matching values.  Ie, to get a list of services for
4984         the 'compute' topic, use filters={'topic': 'compute'}.
4985 
4986         If all_cells=True, then scan all cells and merge the results.
4987 
4988         If cell_down_support=True then return minimal service records
4989         for cells that do not respond based on what we have in the
4990         host mappings. These will have only 'binary' and 'host' set.
4991         """
4992         if filters is None:
4993             filters = {}
4994         disabled = filters.pop('disabled', None)
4995         if 'availability_zone' in filters:
4996             set_zones = True
4997 
4998         # NOTE(danms): Eventually this all_cells nonsense should go away
4999         # and we should always iterate over the cells. However, certain
5000         # callers need the legacy behavior for now.
5001         if all_cells:
5002             services = []
5003             service_dict = nova_context.scatter_gather_all_cells(context,
5004                 objects.ServiceList.get_all, disabled, set_zones=set_zones)
5005             for cell_uuid, service in service_dict.items():
5006                 if not nova_context.is_cell_failure_sentinel(service):
5007                     services.extend(service)
5008                 elif cell_down_support:
5009                     unavailable_services = objects.ServiceList()
5010                     cid = [cm.id for cm in nova_context.CELLS
5011                            if cm.uuid == cell_uuid]
5012                     # We know cid[0] is in the list because we are using the
5013                     # same list that scatter_gather_all_cells used
5014                     hms = objects.HostMappingList.get_by_cell_id(context,
5015                                                                  cid[0])
5016                     for hm in hms:
5017                         unavailable_services.objects.append(objects.Service(
5018                             binary='nova-compute', host=hm.host))
5019                     LOG.warning("Cell %s is not responding and hence only "
5020                                 "partial results are available from this "
5021                                 "cell.", cell_uuid)
5022                     services.extend(unavailable_services)
5023                 else:
5024                     LOG.warning("Cell %s is not responding and hence skipped "
5025                                 "from the results.", cell_uuid)
5026         else:
5027             services = objects.ServiceList.get_all(context, disabled,
5028                                                    set_zones=set_zones)
5029         ret_services = []
5030         for service in services:
5031             for key, val in filters.items():
5032                 if service[key] != val:
5033                     break
5034             else:
5035                 # All filters matched.
5036                 ret_services.append(service)
5037         return ret_services
5038 
5039     def service_get_by_id(self, context, service_id):
5040         """Get service entry for the given service id or uuid."""
5041         try:
5042             return _find_service_in_cell(context, service_id=service_id)
5043         except exception.NotFound:
5044             raise exception.ServiceNotFound(service_id=service_id)
5045 
5046     @target_host_cell
5047     def service_get_by_compute_host(self, context, host_name):
5048         """Get service entry for the given compute hostname."""
5049         return objects.Service.get_by_compute_host(context, host_name)
5050 
5051     def _update_compute_provider_status(self, context, service):
5052         """Calls the compute service to sync the COMPUTE_STATUS_DISABLED trait.
5053 
5054         There are two cases where the API will not call the compute service:
5055 
5056         * The compute service is down. In this case the trait is synchronized
5057           when the compute service is restarted.
5058         * The compute service is old. In this case the trait is synchronized
5059           when the compute service is upgraded and restarted.
5060 
5061         :param context: nova auth RequestContext
5062         :param service: nova.objects.Service object which has been enabled
5063             or disabled (see ``service_update``).
5064         """
5065         # Make sure the service is up so we can make the RPC call.
5066         if not self.servicegroup_api.service_is_up(service):
5067             LOG.info('Compute service on host %s is down. The '
5068                      'COMPUTE_STATUS_DISABLED trait will be synchronized '
5069                      'when the service is restarted.', service.host)
5070             return
5071 
5072         # Make sure the compute service is new enough for the trait sync
5073         # behavior.
5074         # TODO(mriedem): Remove this compat check in the U release.
5075         if service.version < MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED:
5076             LOG.info('Compute service on host %s is too old to sync the '
5077                      'COMPUTE_STATUS_DISABLED trait in Placement. The '
5078                      'trait will be synchronized when the service is '
5079                      'upgraded and restarted.', service.host)
5080             return
5081 
5082         enabled = not service.disabled
5083         # Avoid leaking errors out of the API.
5084         try:
5085             LOG.debug('Calling the compute service on host %s to sync the '
5086                       'COMPUTE_STATUS_DISABLED trait.', service.host)
5087             self.rpcapi.set_host_enabled(context, service.host, enabled)
5088         except Exception:
5089             LOG.exception('An error occurred while updating the '
5090                           'COMPUTE_STATUS_DISABLED trait on compute node '
5091                           'resource providers managed by host %s. The trait '
5092                           'will be synchronized automatically by the compute '
5093                           'service when the update_available_resource '
5094                           'periodic task runs.', service.host)
5095 
5096     def service_update(self, context, service):
5097         """Performs the actual service update operation.
5098 
5099         If the "disabled" field is changed, potentially calls the compute
5100         service to sync the COMPUTE_STATUS_DISABLED trait on the compute node
5101         resource providers managed by this compute service.
5102 
5103         :param context: nova auth RequestContext
5104         :param service: nova.objects.Service object with changes already
5105             set on the object
5106         """
5107         # Before persisting changes and resetting the changed fields on the
5108         # Service object, determine if the disabled field changed.
5109         update_placement = 'disabled' in service.obj_what_changed()
5110         # Persist the Service object changes to the database.
5111         service.save()
5112         # If the disabled field changed, potentially call the compute service
5113         # to sync the COMPUTE_STATUS_DISABLED trait.
5114         if update_placement:
5115             self._update_compute_provider_status(context, service)
5116         return service
5117 
5118     @target_host_cell
5119     def service_update_by_host_and_binary(self, context, host_name, binary,
5120                                           params_to_update):
5121         """Enable / Disable a service.
5122 
5123         Determines the cell that the service is in using the HostMapping.
5124 
5125         For compute services, this stops new builds and migrations going to
5126         the host.
5127 
5128         See also ``service_update``.
5129 
5130         :param context: nova auth RequestContext
5131         :param host_name: hostname of the service
5132         :param binary: service binary (really only supports "nova-compute")
5133         :param params_to_update: dict of changes to make to the Service object
5134         :raises: HostMappingNotFound if the host is not mapped to a cell
5135         :raises: HostBinaryNotFound if a services table record is not found
5136             with the given host_name and binary
5137         """
5138         # TODO(mriedem): Service.get_by_args is deprecated; we should use
5139         # get_by_compute_host here (remember to update the "raises" docstring).
5140         service = objects.Service.get_by_args(context, host_name, binary)
5141         service.update(params_to_update)
5142         return self.service_update(context, service)
5143 
5144     def _service_delete(self, context, service_id):
5145         """Performs the actual Service deletion operation."""
5146         try:
5147             service = _find_service_in_cell(context, service_id=service_id)
5148         except exception.NotFound:
5149             raise exception.ServiceNotFound(service_id=service_id)
5150         service.destroy()
5151 
5152     # TODO(mriedem): Nothing outside of tests is using this now so we should
5153     # be able to remove it.
5154     def service_delete(self, context, service_id):
5155         """Deletes the specified service found via id or uuid."""
5156         self._service_delete(context, service_id)
5157 
5158     @target_host_cell
5159     def instance_get_all_by_host(self, context, host_name):
5160         """Return all instances on the given host."""
5161         return objects.InstanceList.get_by_host(context, host_name)
5162 
5163     def task_log_get_all(self, context, task_name, period_beginning,
5164                          period_ending, host=None, state=None):
5165         """Return the task logs within a given range, optionally
5166         filtering by host and/or state.
5167         """
5168         return self.db.task_log_get_all(context, task_name,
5169                                         period_beginning,
5170                                         period_ending,
5171                                         host=host,
5172                                         state=state)
5173 
5174     def compute_node_get(self, context, compute_id):
5175         """Return compute node entry for particular integer ID or UUID."""
5176         load_cells()
5177 
5178         # NOTE(danms): Unfortunately this API exposes database identifiers
5179         # which means we really can't do something efficient here
5180         is_uuid = uuidutils.is_uuid_like(compute_id)
5181         for cell in CELLS:
5182             if cell.uuid == objects.CellMapping.CELL0_UUID:
5183                 continue
5184             with nova_context.target_cell(context, cell) as cctxt:
5185                 try:
5186                     if is_uuid:
5187                         return objects.ComputeNode.get_by_uuid(cctxt,
5188                                                                compute_id)
5189                     return objects.ComputeNode.get_by_id(cctxt,
5190                                                          int(compute_id))
5191                 except exception.ComputeHostNotFound:
5192                     # NOTE(danms): Keep looking in other cells
5193                     continue
5194 
5195         raise exception.ComputeHostNotFound(host=compute_id)
5196 
5197     def compute_node_get_all(self, context, limit=None, marker=None):
5198         load_cells()
5199 
5200         computes = []
5201         uuid_marker = marker and uuidutils.is_uuid_like(marker)
5202         for cell in CELLS:
5203             if cell.uuid == objects.CellMapping.CELL0_UUID:
5204                 continue
5205             with nova_context.target_cell(context, cell) as cctxt:
5206 
5207                 # If we have a marker and it's a uuid, see if the compute node
5208                 # is in this cell.
5209                 if marker and uuid_marker:
5210                     try:
5211                         compute_marker = objects.ComputeNode.get_by_uuid(
5212                             cctxt, marker)
5213                         # we found the marker compute node, so use it's id
5214                         # for the actual marker for paging in this cell's db
5215                         marker = compute_marker.id
5216                     except exception.ComputeHostNotFound:
5217                         # The marker node isn't in this cell so keep looking.
5218                         continue
5219 
5220                 try:
5221                     cell_computes = objects.ComputeNodeList.get_by_pagination(
5222                         cctxt, limit=limit, marker=marker)
5223                 except exception.MarkerNotFound:
5224                     # NOTE(danms): Keep looking through cells
5225                     continue
5226                 computes.extend(cell_computes)
5227                 # NOTE(danms): We must have found the marker, so continue on
5228                 # without one
5229                 marker = None
5230                 if limit:
5231                     limit -= len(cell_computes)
5232                     if limit <= 0:
5233                         break
5234 
5235         if marker is not None and len(computes) == 0:
5236             # NOTE(danms): If we did not find the marker in any cell,
5237             # mimic the db_api behavior here.
5238             raise exception.MarkerNotFound(marker=marker)
5239 
5240         return objects.ComputeNodeList(objects=computes)
5241 
5242     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
5243         load_cells()
5244 
5245         computes = []
5246         for cell in CELLS:
5247             if cell.uuid == objects.CellMapping.CELL0_UUID:
5248                 continue
5249             with nova_context.target_cell(context, cell) as cctxt:
5250                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
5251                     cctxt, hypervisor_match)
5252             computes.extend(cell_computes)
5253         return objects.ComputeNodeList(objects=computes)
5254 
5255     def compute_node_statistics(self, context):
5256         load_cells()
5257 
5258         cell_stats = []
5259         for cell in CELLS:
5260             if cell.uuid == objects.CellMapping.CELL0_UUID:
5261                 continue
5262             with nova_context.target_cell(context, cell) as cctxt:
5263                 cell_stats.append(self.db.compute_node_statistics(cctxt))
5264 
5265         if cell_stats:
5266             keys = cell_stats[0].keys()
5267             return {k: sum(stats[k] for stats in cell_stats)
5268                     for k in keys}
5269         else:
5270             return {}
5271 
5272 
5273 class InstanceActionAPI(base.Base):
5274     """Sub-set of the Compute Manager API for managing instance actions."""
5275 
5276     def actions_get(self, context, instance, limit=None, marker=None,
5277                     filters=None):
5278         return objects.InstanceActionList.get_by_instance_uuid(
5279             context, instance.uuid, limit, marker, filters)
5280 
5281     def action_get_by_request_id(self, context, instance, request_id):
5282         return objects.InstanceAction.get_by_request_id(
5283             context, instance.uuid, request_id)
5284 
5285     def action_events_get(self, context, instance, action_id):
5286         return objects.InstanceActionEventList.get_by_action(
5287             context, action_id)
5288 
5289 
5290 class AggregateAPI(base.Base):
5291     """Sub-set of the Compute Manager API for managing host aggregates."""
5292     def __init__(self, **kwargs):
5293         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5294         self.query_client = query.SchedulerQueryClient()
5295         self._placement_client = None  # Lazy-load on first access.
5296         super(AggregateAPI, self).__init__(**kwargs)
5297 
5298     @property
5299     def placement_client(self):
5300         if self._placement_client is None:
5301             self._placement_client = report.SchedulerReportClient()
5302         return self._placement_client
5303 
5304     @wrap_exception()
5305     def create_aggregate(self, context, aggregate_name, availability_zone):
5306         """Creates the model for the aggregate."""
5307 
5308         aggregate = objects.Aggregate(context=context)
5309         aggregate.name = aggregate_name
5310         if availability_zone:
5311             aggregate.metadata = {'availability_zone': availability_zone}
5312         aggregate.create()
5313         self.query_client.update_aggregates(context, [aggregate])
5314         return aggregate
5315 
5316     def get_aggregate(self, context, aggregate_id):
5317         """Get an aggregate by id."""
5318         return objects.Aggregate.get_by_id(context, aggregate_id)
5319 
5320     def get_aggregate_list(self, context):
5321         """Get all the aggregates."""
5322         return objects.AggregateList.get_all(context)
5323 
5324     def get_aggregates_by_host(self, context, compute_host):
5325         """Get all the aggregates where the given host is presented."""
5326         return objects.AggregateList.get_by_host(context, compute_host)
5327 
5328     @wrap_exception()
5329     def update_aggregate(self, context, aggregate_id, values):
5330         """Update the properties of an aggregate."""
5331         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5332         if 'name' in values:
5333             aggregate.name = values.pop('name')
5334             aggregate.save()
5335         self.is_safe_to_update_az(context, values, aggregate=aggregate,
5336                                   action_name=AGGREGATE_ACTION_UPDATE,
5337                                   check_no_instances_in_az=True)
5338         if values:
5339             aggregate.update_metadata(values)
5340             aggregate.updated_at = timeutils.utcnow()
5341         self.query_client.update_aggregates(context, [aggregate])
5342         # If updated values include availability_zones, then the cache
5343         # which stored availability_zones and host need to be reset
5344         if values.get('availability_zone'):
5345             availability_zones.reset_cache()
5346         return aggregate
5347 
5348     @wrap_exception()
5349     def update_aggregate_metadata(self, context, aggregate_id, metadata):
5350         """Updates the aggregate metadata."""
5351         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5352         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
5353                                   action_name=AGGREGATE_ACTION_UPDATE_META,
5354                                   check_no_instances_in_az=True)
5355         aggregate.update_metadata(metadata)
5356         self.query_client.update_aggregates(context, [aggregate])
5357         # If updated metadata include availability_zones, then the cache
5358         # which stored availability_zones and host need to be reset
5359         if metadata and metadata.get('availability_zone'):
5360             availability_zones.reset_cache()
5361         aggregate.updated_at = timeutils.utcnow()
5362         return aggregate
5363 
5364     @wrap_exception()
5365     def delete_aggregate(self, context, aggregate_id):
5366         """Deletes the aggregate."""
5367         aggregate_payload = {'aggregate_id': aggregate_id}
5368         compute_utils.notify_about_aggregate_update(context,
5369                                                     "delete.start",
5370                                                     aggregate_payload)
5371         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5372 
5373         compute_utils.notify_about_aggregate_action(
5374             context=context,
5375             aggregate=aggregate,
5376             action=fields_obj.NotificationAction.DELETE,
5377             phase=fields_obj.NotificationPhase.START)
5378 
5379         if len(aggregate.hosts) > 0:
5380             msg = _("Host aggregate is not empty")
5381             raise exception.InvalidAggregateActionDelete(
5382                 aggregate_id=aggregate_id, reason=msg)
5383         aggregate.destroy()
5384         self.query_client.delete_aggregate(context, aggregate)
5385         compute_utils.notify_about_aggregate_update(context,
5386                                                     "delete.end",
5387                                                     aggregate_payload)
5388         compute_utils.notify_about_aggregate_action(
5389             context=context,
5390             aggregate=aggregate,
5391             action=fields_obj.NotificationAction.DELETE,
5392             phase=fields_obj.NotificationPhase.END)
5393 
5394     def is_safe_to_update_az(self, context, metadata, aggregate,
5395                              hosts=None,
5396                              action_name=AGGREGATE_ACTION_ADD,
5397                              check_no_instances_in_az=False):
5398         """Determine if updates alter an aggregate's availability zone.
5399 
5400             :param context: local context
5401             :param metadata: Target metadata for updating aggregate
5402             :param aggregate: Aggregate to update
5403             :param hosts: Hosts to check. If None, aggregate.hosts is used
5404             :type hosts: list
5405             :param action_name: Calling method for logging purposes
5406             :param check_no_instances_in_az: if True, it checks
5407                 there is no instances on any hosts of the aggregate
5408 
5409         """
5410         if 'availability_zone' in metadata:
5411             if not metadata['availability_zone']:
5412                 msg = _("Aggregate %s does not support empty named "
5413                         "availability zone") % aggregate.name
5414                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5415                                                   msg)
5416             _hosts = hosts or aggregate.hosts
5417             host_aggregates = objects.AggregateList.get_by_metadata_key(
5418                 context, 'availability_zone', hosts=_hosts)
5419             conflicting_azs = [
5420                 agg.availability_zone for agg in host_aggregates
5421                 if agg.availability_zone != metadata['availability_zone'] and
5422                 agg.id != aggregate.id]
5423             if conflicting_azs:
5424                 msg = _("One or more hosts already in availability zone(s) "
5425                         "%s") % conflicting_azs
5426                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5427                                                   msg)
5428             same_az_name = (aggregate.availability_zone ==
5429                             metadata['availability_zone'])
5430             if check_no_instances_in_az and not same_az_name:
5431                 instance_count_by_cell = (
5432                     nova_context.scatter_gather_skip_cell0(
5433                         context,
5434                         objects.InstanceList.get_count_by_hosts,
5435                         _hosts))
5436                 if any(cnt for cnt in instance_count_by_cell.values()):
5437                     msg = _("One or more hosts contain instances in this zone")
5438                     self._raise_invalid_aggregate_exc(
5439                         action_name, aggregate.id, msg)
5440 
5441     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
5442         if action_name == AGGREGATE_ACTION_ADD:
5443             raise exception.InvalidAggregateActionAdd(
5444                 aggregate_id=aggregate_id, reason=reason)
5445         elif action_name == AGGREGATE_ACTION_UPDATE:
5446             raise exception.InvalidAggregateActionUpdate(
5447                 aggregate_id=aggregate_id, reason=reason)
5448         elif action_name == AGGREGATE_ACTION_UPDATE_META:
5449             raise exception.InvalidAggregateActionUpdateMeta(
5450                 aggregate_id=aggregate_id, reason=reason)
5451         elif action_name == AGGREGATE_ACTION_DELETE:
5452             raise exception.InvalidAggregateActionDelete(
5453                 aggregate_id=aggregate_id, reason=reason)
5454 
5455         raise exception.NovaException(
5456             _("Unexpected aggregate action %s") % action_name)
5457 
5458     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
5459         # Update the availability_zone cache to avoid getting wrong
5460         # availability_zone in cache retention time when add/remove
5461         # host to/from aggregate.
5462         if aggregate_meta and aggregate_meta.get('availability_zone'):
5463             availability_zones.update_host_availability_zone_cache(context,
5464                                                                    host_name)
5465 
5466     @wrap_exception()
5467     def add_host_to_aggregate(self, context, aggregate_id, host_name):
5468         """Adds the host to an aggregate."""
5469         aggregate_payload = {'aggregate_id': aggregate_id,
5470                              'host_name': host_name}
5471         compute_utils.notify_about_aggregate_update(context,
5472                                                     "addhost.start",
5473                                                     aggregate_payload)
5474         # validates the host; HostMappingNotFound or ComputeHostNotFound
5475         # is raised if invalid
5476         try:
5477             mapping = objects.HostMapping.get_by_host(context, host_name)
5478             nova_context.set_target_cell(context, mapping.cell_mapping)
5479             service = objects.Service.get_by_compute_host(context, host_name)
5480         except exception.HostMappingNotFound:
5481             try:
5482                 # NOTE(danms): This targets our cell
5483                 service = _find_service_in_cell(context,
5484                                                 service_host=host_name)
5485             except exception.NotFound:
5486                 raise exception.ComputeHostNotFound(host=host_name)
5487 
5488         if service.host != host_name:
5489             # NOTE(danms): If we found a service but it is not an
5490             # exact match, we may have a case-insensitive backend
5491             # database (like mysql) which will end up with us
5492             # adding the host-aggregate mapping with a
5493             # non-matching hostname.
5494             raise exception.ComputeHostNotFound(host=host_name)
5495 
5496         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5497 
5498         compute_utils.notify_about_aggregate_action(
5499             context=context,
5500             aggregate=aggregate,
5501             action=fields_obj.NotificationAction.ADD_HOST,
5502             phase=fields_obj.NotificationPhase.START)
5503 
5504         self.is_safe_to_update_az(context, aggregate.metadata,
5505                                   hosts=[host_name], aggregate=aggregate)
5506 
5507         aggregate.add_host(host_name)
5508         self.query_client.update_aggregates(context, [aggregate])
5509         try:
5510             self.placement_client.aggregate_add_host(
5511                 context, aggregate.uuid, host_name=host_name)
5512         except exception.PlacementAPIConnectFailure:
5513             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5514             # service not communicating with the Placement API, so just log a
5515             # warning here.
5516             # TODO(jaypipes): Remove this in Stein, when placement must be able
5517             # to be contacted from the nova-api service.
5518             LOG.warning("Failed to associate %s with a placement "
5519                         "aggregate: %s. There was a failure to communicate "
5520                         "with the placement service.",
5521                         host_name, aggregate.uuid)
5522         except (exception.ResourceProviderNotFound,
5523                 exception.ResourceProviderAggregateRetrievalFailed,
5524                 exception.ResourceProviderUpdateFailed,
5525                 exception.ResourceProviderUpdateConflict) as err:
5526             # NOTE(jaypipes): We don't want a failure perform the mirroring
5527             # action in the placement service to be returned to the user (they
5528             # probably don't know anything about the placement service and
5529             # would just be confused). So, we just log a warning here, noting
5530             # that on the next run of nova-manage placement sync_aggregates
5531             # things will go back to normal
5532             LOG.warning("Failed to associate %s with a placement "
5533                         "aggregate: %s. This may be corrected after running "
5534                         "nova-manage placement sync_aggregates.",
5535                         host_name, err)
5536         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5537         # NOTE(jogo): Send message to host to support resource pools
5538         self.compute_rpcapi.add_aggregate_host(context,
5539                 aggregate=aggregate, host_param=host_name, host=host_name)
5540         aggregate_payload.update({'name': aggregate.name})
5541         compute_utils.notify_about_aggregate_update(context,
5542                                                     "addhost.end",
5543                                                     aggregate_payload)
5544         compute_utils.notify_about_aggregate_action(
5545             context=context,
5546             aggregate=aggregate,
5547             action=fields_obj.NotificationAction.ADD_HOST,
5548             phase=fields_obj.NotificationPhase.END)
5549 
5550         return aggregate
5551 
5552     @wrap_exception()
5553     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
5554         """Removes host from the aggregate."""
5555         aggregate_payload = {'aggregate_id': aggregate_id,
5556                              'host_name': host_name}
5557         compute_utils.notify_about_aggregate_update(context,
5558                                                     "removehost.start",
5559                                                     aggregate_payload)
5560         # validates the host; HostMappingNotFound or ComputeHostNotFound
5561         # is raised if invalid
5562         mapping = objects.HostMapping.get_by_host(context, host_name)
5563         nova_context.set_target_cell(context, mapping.cell_mapping)
5564         objects.Service.get_by_compute_host(context, host_name)
5565         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5566 
5567         compute_utils.notify_about_aggregate_action(
5568             context=context,
5569             aggregate=aggregate,
5570             action=fields_obj.NotificationAction.REMOVE_HOST,
5571             phase=fields_obj.NotificationPhase.START)
5572 
5573         aggregate.delete_host(host_name)
5574         self.query_client.update_aggregates(context, [aggregate])
5575         try:
5576             self.placement_client.aggregate_remove_host(
5577                 context, aggregate.uuid, host_name)
5578         except exception.PlacementAPIConnectFailure:
5579             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5580             # service not communicating with the Placement API, so just log a
5581             # warning here.
5582             # TODO(jaypipes): Remove this in Stein, when placement must be able
5583             # to be contacted from the nova-api service.
5584             LOG.warning("Failed to remove association of %s with a placement "
5585                         "aggregate: %s. There was a failure to communicate "
5586                         "with the placement service.",
5587                         host_name, aggregate.uuid)
5588         except (exception.ResourceProviderNotFound,
5589                 exception.ResourceProviderAggregateRetrievalFailed,
5590                 exception.ResourceProviderUpdateFailed,
5591                 exception.ResourceProviderUpdateConflict) as err:
5592             # NOTE(jaypipes): We don't want a failure perform the mirroring
5593             # action in the placement service to be returned to the user (they
5594             # probably don't know anything about the placement service and
5595             # would just be confused). So, we just log a warning here, noting
5596             # that on the next run of nova-manage placement sync_aggregates
5597             # things will go back to normal
5598             LOG.warning("Failed to remove association of %s with a placement "
5599                         "aggregate: %s. This may be corrected after running "
5600                         "nova-manage placement sync_aggregates.",
5601                         host_name, err)
5602         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5603         self.compute_rpcapi.remove_aggregate_host(context,
5604                 aggregate=aggregate, host_param=host_name, host=host_name)
5605         compute_utils.notify_about_aggregate_update(context,
5606                                                     "removehost.end",
5607                                                     aggregate_payload)
5608         compute_utils.notify_about_aggregate_action(
5609             context=context,
5610             aggregate=aggregate,
5611             action=fields_obj.NotificationAction.REMOVE_HOST,
5612             phase=fields_obj.NotificationPhase.END)
5613         return aggregate
5614 
5615 
5616 class KeypairAPI(base.Base):
5617     """Subset of the Compute Manager API for managing key pairs."""
5618 
5619     get_notifier = functools.partial(rpc.get_notifier, service='api')
5620     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
5621                                        get_notifier=get_notifier,
5622                                        binary='nova-api')
5623 
5624     def _notify(self, context, event_suffix, keypair_name):
5625         payload = {
5626             'tenant_id': context.project_id,
5627             'user_id': context.user_id,
5628             'key_name': keypair_name,
5629         }
5630         notify = self.get_notifier()
5631         notify.info(context, 'keypair.%s' % event_suffix, payload)
5632 
5633     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
5634         safe_chars = "_- " + string.digits + string.ascii_letters
5635         clean_value = "".join(x for x in key_name if x in safe_chars)
5636         if clean_value != key_name:
5637             raise exception.InvalidKeypair(
5638                 reason=_("Keypair name contains unsafe characters"))
5639 
5640         try:
5641             utils.check_string_length(key_name, min_length=1, max_length=255)
5642         except exception.InvalidInput:
5643             raise exception.InvalidKeypair(
5644                 reason=_('Keypair name must be string and between '
5645                          '1 and 255 characters long'))
5646         try:
5647             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
5648         except exception.OverQuota:
5649             raise exception.KeypairLimitExceeded()
5650 
5651     @wrap_exception()
5652     def import_key_pair(self, context, user_id, key_name, public_key,
5653                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5654         """Import a key pair using an existing public key."""
5655         self._validate_new_key_pair(context, user_id, key_name, key_type)
5656 
5657         self._notify(context, 'import.start', key_name)
5658 
5659         keypair = objects.KeyPair(context)
5660         keypair.user_id = user_id
5661         keypair.name = key_name
5662         keypair.type = key_type
5663         keypair.fingerprint = None
5664         keypair.public_key = public_key
5665 
5666         compute_utils.notify_about_keypair_action(
5667             context=context,
5668             keypair=keypair,
5669             action=fields_obj.NotificationAction.IMPORT,
5670             phase=fields_obj.NotificationPhase.START)
5671 
5672         fingerprint = self._generate_fingerprint(public_key, key_type)
5673 
5674         keypair.fingerprint = fingerprint
5675         keypair.create()
5676 
5677         compute_utils.notify_about_keypair_action(
5678             context=context,
5679             keypair=keypair,
5680             action=fields_obj.NotificationAction.IMPORT,
5681             phase=fields_obj.NotificationPhase.END)
5682         self._notify(context, 'import.end', key_name)
5683 
5684         return keypair
5685 
5686     @wrap_exception()
5687     def create_key_pair(self, context, user_id, key_name,
5688                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5689         """Create a new key pair."""
5690         self._validate_new_key_pair(context, user_id, key_name, key_type)
5691 
5692         keypair = objects.KeyPair(context)
5693         keypair.user_id = user_id
5694         keypair.name = key_name
5695         keypair.type = key_type
5696         keypair.fingerprint = None
5697         keypair.public_key = None
5698 
5699         self._notify(context, 'create.start', key_name)
5700         compute_utils.notify_about_keypair_action(
5701             context=context,
5702             keypair=keypair,
5703             action=fields_obj.NotificationAction.CREATE,
5704             phase=fields_obj.NotificationPhase.START)
5705 
5706         private_key, public_key, fingerprint = self._generate_key_pair(
5707             user_id, key_type)
5708 
5709         keypair.fingerprint = fingerprint
5710         keypair.public_key = public_key
5711         keypair.create()
5712 
5713         # NOTE(melwitt): We recheck the quota after creating the object to
5714         # prevent users from allocating more resources than their allowed quota
5715         # in the event of a race. This is configurable because it can be
5716         # expensive if strict quota limits are not required in a deployment.
5717         if CONF.quota.recheck_quota:
5718             try:
5719                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
5720             except exception.OverQuota:
5721                 keypair.destroy()
5722                 raise exception.KeypairLimitExceeded()
5723 
5724         compute_utils.notify_about_keypair_action(
5725             context=context,
5726             keypair=keypair,
5727             action=fields_obj.NotificationAction.CREATE,
5728             phase=fields_obj.NotificationPhase.END)
5729 
5730         self._notify(context, 'create.end', key_name)
5731 
5732         return keypair, private_key
5733 
5734     def _generate_fingerprint(self, public_key, key_type):
5735         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5736             return crypto.generate_fingerprint(public_key)
5737         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5738             return crypto.generate_x509_fingerprint(public_key)
5739 
5740     def _generate_key_pair(self, user_id, key_type):
5741         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5742             return crypto.generate_key_pair()
5743         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5744             return crypto.generate_winrm_x509_cert(user_id)
5745 
5746     @wrap_exception()
5747     def delete_key_pair(self, context, user_id, key_name):
5748         """Delete a keypair by name."""
5749         self._notify(context, 'delete.start', key_name)
5750         keypair = self.get_key_pair(context, user_id, key_name)
5751         compute_utils.notify_about_keypair_action(
5752             context=context,
5753             keypair=keypair,
5754             action=fields_obj.NotificationAction.DELETE,
5755             phase=fields_obj.NotificationPhase.START)
5756         objects.KeyPair.destroy_by_name(context, user_id, key_name)
5757         compute_utils.notify_about_keypair_action(
5758             context=context,
5759             keypair=keypair,
5760             action=fields_obj.NotificationAction.DELETE,
5761             phase=fields_obj.NotificationPhase.END)
5762         self._notify(context, 'delete.end', key_name)
5763 
5764     def get_key_pairs(self, context, user_id, limit=None, marker=None):
5765         """List key pairs."""
5766         return objects.KeyPairList.get_by_user(
5767             context, user_id, limit=limit, marker=marker)
5768 
5769     def get_key_pair(self, context, user_id, key_name):
5770         """Get a keypair by name."""
5771         return objects.KeyPair.get_by_name(context, user_id, key_name)
5772 
5773 
5774 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
5775     """Sub-set of the Compute API related to managing security groups
5776     and security group rules
5777     """
5778 
5779     # The nova security group api does not use a uuid for the id.
5780     id_is_uuid = False
5781 
5782     def __init__(self, **kwargs):
5783         super(SecurityGroupAPI, self).__init__(**kwargs)
5784         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5785 
5786     def validate_property(self, value, property, allowed):
5787         """Validate given security group property.
5788 
5789         :param value:          the value to validate, as a string or unicode
5790         :param property:       the property, either 'name' or 'description'
5791         :param allowed:        the range of characters allowed
5792         """
5793 
5794         try:
5795             val = value.strip()
5796         except AttributeError:
5797             msg = _("Security group %s is not a string or unicode") % property
5798             self.raise_invalid_property(msg)
5799         utils.check_string_length(val, name=property, min_length=1,
5800                                   max_length=255)
5801 
5802         if allowed and not re.match(allowed, val):
5803             # Some validation to ensure that values match API spec.
5804             # - Alphanumeric characters, spaces, dashes, and underscores.
5805             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
5806             #  probably create a param validator that can be used elsewhere.
5807             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
5808                      "invalid. Content limited to '%(allowed)s'.") %
5809                    {'value': value, 'allowed': allowed,
5810                     'property': property.capitalize()})
5811             self.raise_invalid_property(msg)
5812 
5813     def ensure_default(self, context):
5814         """Ensure that a context has a security group.
5815 
5816         Creates a security group for the security context if it does not
5817         already exist.
5818 
5819         :param context: the security context
5820         """
5821         self.db.security_group_ensure_default(context)
5822 
5823     def create_security_group(self, context, name, description):
5824         try:
5825             objects.Quotas.check_deltas(context, {'security_groups': 1},
5826                                         context.project_id,
5827                                         user_id=context.user_id)
5828         except exception.OverQuota:
5829             msg = _("Quota exceeded, too many security groups.")
5830             self.raise_over_quota(msg)
5831 
5832         LOG.info("Create Security Group %s", name)
5833 
5834         self.ensure_default(context)
5835 
5836         group = {'user_id': context.user_id,
5837                  'project_id': context.project_id,
5838                  'name': name,
5839                  'description': description}
5840         try:
5841             group_ref = self.db.security_group_create(context, group)
5842         except exception.SecurityGroupExists:
5843             msg = _('Security group %s already exists') % name
5844             self.raise_group_already_exists(msg)
5845 
5846         # NOTE(melwitt): We recheck the quota after creating the object to
5847         # prevent users from allocating more resources than their allowed quota
5848         # in the event of a race. This is configurable because it can be
5849         # expensive if strict quota limits are not required in a deployment.
5850         if CONF.quota.recheck_quota:
5851             try:
5852                 objects.Quotas.check_deltas(context, {'security_groups': 0},
5853                                             context.project_id,
5854                                             user_id=context.user_id)
5855             except exception.OverQuota:
5856                 self.db.security_group_destroy(context, group_ref['id'])
5857                 msg = _("Quota exceeded, too many security groups.")
5858                 self.raise_over_quota(msg)
5859 
5860         return group_ref
5861 
5862     def update_security_group(self, context, security_group,
5863                                 name, description):
5864         if security_group['name'] in RO_SECURITY_GROUPS:
5865             msg = (_("Unable to update system group '%s'") %
5866                     security_group['name'])
5867             self.raise_invalid_group(msg)
5868 
5869         group = {'name': name,
5870                  'description': description}
5871 
5872         columns_to_join = ['rules.grantee_group']
5873         group_ref = self.db.security_group_update(context,
5874                 security_group['id'],
5875                 group,
5876                 columns_to_join=columns_to_join)
5877         return group_ref
5878 
5879     def get(self, context, name=None, id=None, map_exception=False):
5880         self.ensure_default(context)
5881         cols = ['rules']
5882         try:
5883             if name:
5884                 return self.db.security_group_get_by_name(context,
5885                                                           context.project_id,
5886                                                           name,
5887                                                           columns_to_join=cols)
5888             elif id:
5889                 return self.db.security_group_get(context, id,
5890                                                   columns_to_join=cols)
5891         except exception.NotFound as exp:
5892             if map_exception:
5893                 msg = exp.format_message()
5894                 self.raise_not_found(msg)
5895             else:
5896                 raise
5897 
5898     def list(self, context, names=None, ids=None, project=None,
5899              search_opts=None):
5900         self.ensure_default(context)
5901 
5902         groups = []
5903         if names or ids:
5904             if names:
5905                 for name in names:
5906                     groups.append(self.db.security_group_get_by_name(context,
5907                                                                      project,
5908                                                                      name))
5909             if ids:
5910                 for id in ids:
5911                     groups.append(self.db.security_group_get(context, id))
5912 
5913         elif context.is_admin:
5914             # TODO(eglynn): support a wider set of search options than just
5915             # all_tenants, at least include the standard filters defined for
5916             # the EC2 DescribeSecurityGroups API for the non-admin case also
5917             if (search_opts and 'all_tenants' in search_opts):
5918                 groups = self.db.security_group_get_all(context)
5919             else:
5920                 groups = self.db.security_group_get_by_project(context,
5921                                                                project)
5922 
5923         elif project:
5924             groups = self.db.security_group_get_by_project(context, project)
5925 
5926         return groups
5927 
5928     def destroy(self, context, security_group):
5929         if security_group['name'] in RO_SECURITY_GROUPS:
5930             msg = _("Unable to delete system group '%s'") % \
5931                     security_group['name']
5932             self.raise_invalid_group(msg)
5933 
5934         if self.db.security_group_in_use(context, security_group['id']):
5935             msg = _("Security group is still in use")
5936             self.raise_invalid_group(msg)
5937 
5938         LOG.info("Delete security group %s", security_group['name'])
5939         self.db.security_group_destroy(context, security_group['id'])
5940 
5941     def is_associated_with_server(self, security_group, instance_uuid):
5942         """Check if the security group is already associated
5943            with the instance. If Yes, return True.
5944         """
5945 
5946         if not security_group:
5947             return False
5948 
5949         instances = security_group.get('instances')
5950         if not instances:
5951             return False
5952 
5953         for inst in instances:
5954             if (instance_uuid == inst['uuid']):
5955                 return True
5956 
5957         return False
5958 
5959     def add_to_instance(self, context, instance, security_group_name):
5960         """Add security group to the instance."""
5961         security_group = self.db.security_group_get_by_name(context,
5962                 context.project_id,
5963                 security_group_name)
5964 
5965         instance_uuid = instance.uuid
5966 
5967         # check if the security group is associated with the server
5968         if self.is_associated_with_server(security_group, instance_uuid):
5969             raise exception.SecurityGroupExistsForInstance(
5970                                         security_group_id=security_group['id'],
5971                                         instance_id=instance_uuid)
5972 
5973         self.db.instance_add_security_group(context.elevated(),
5974                                             instance_uuid,
5975                                             security_group['id'])
5976         if instance.host:
5977             self.compute_rpcapi.refresh_instance_security_rules(
5978                     context, instance, instance.host)
5979 
5980     def remove_from_instance(self, context, instance, security_group_name):
5981         """Remove the security group associated with the instance."""
5982         security_group = self.db.security_group_get_by_name(context,
5983                 context.project_id,
5984                 security_group_name)
5985 
5986         instance_uuid = instance.uuid
5987 
5988         # check if the security group is associated with the server
5989         if not self.is_associated_with_server(security_group, instance_uuid):
5990             raise exception.SecurityGroupNotExistsForInstance(
5991                                     security_group_id=security_group['id'],
5992                                     instance_id=instance_uuid)
5993 
5994         self.db.instance_remove_security_group(context.elevated(),
5995                                                instance_uuid,
5996                                                security_group['id'])
5997         if instance.host:
5998             self.compute_rpcapi.refresh_instance_security_rules(
5999                     context, instance, instance.host)
6000 
6001     def get_rule(self, context, id):
6002         self.ensure_default(context)
6003         try:
6004             return self.db.security_group_rule_get(context, id)
6005         except exception.NotFound:
6006             msg = _("Rule (%s) not found") % id
6007             self.raise_not_found(msg)
6008 
6009     def add_rules(self, context, id, name, vals):
6010         """Add security group rule(s) to security group.
6011 
6012         Note: the Nova security group API doesn't support adding multiple
6013         security group rules at once but the EC2 one does. Therefore,
6014         this function is written to support both.
6015         """
6016 
6017         try:
6018             objects.Quotas.check_deltas(context,
6019                                         {'security_group_rules': len(vals)},
6020                                         id)
6021         except exception.OverQuota:
6022             msg = _("Quota exceeded, too many security group rules.")
6023             self.raise_over_quota(msg)
6024 
6025         msg = ("Security group %(name)s added %(protocol)s ingress "
6026                "(%(from_port)s:%(to_port)s)")
6027         rules = []
6028         for v in vals:
6029             rule = self.db.security_group_rule_create(context, v)
6030 
6031             # NOTE(melwitt): We recheck the quota after creating the object to
6032             # prevent users from allocating more resources than their allowed
6033             # quota in the event of a race. This is configurable because it can
6034             # be expensive if strict quota limits are not required in a
6035             # deployment.
6036             if CONF.quota.recheck_quota:
6037                 try:
6038                     objects.Quotas.check_deltas(context,
6039                                                 {'security_group_rules': 0},
6040                                                 id)
6041                 except exception.OverQuota:
6042                     self.db.security_group_rule_destroy(context, rule['id'])
6043                     msg = _("Quota exceeded, too many security group rules.")
6044                     self.raise_over_quota(msg)
6045 
6046             rules.append(rule)
6047             LOG.info(msg, {'name': name,
6048                            'protocol': rule.protocol,
6049                            'from_port': rule.from_port,
6050                            'to_port': rule.to_port})
6051 
6052         self.trigger_rules_refresh(context, id=id)
6053         return rules
6054 
6055     def remove_rules(self, context, security_group, rule_ids):
6056         msg = ("Security group %(name)s removed %(protocol)s ingress "
6057                "(%(from_port)s:%(to_port)s)")
6058         for rule_id in rule_ids:
6059             rule = self.get_rule(context, rule_id)
6060             LOG.info(msg, {'name': security_group['name'],
6061                            'protocol': rule.protocol,
6062                            'from_port': rule.from_port,
6063                            'to_port': rule.to_port})
6064 
6065             self.db.security_group_rule_destroy(context, rule_id)
6066 
6067         # NOTE(vish): we removed some rules, so refresh
6068         self.trigger_rules_refresh(context, id=security_group['id'])
6069 
6070     def remove_default_rules(self, context, rule_ids):
6071         for rule_id in rule_ids:
6072             self.db.security_group_default_rule_destroy(context, rule_id)
6073 
6074     def add_default_rules(self, context, vals):
6075         rules = [self.db.security_group_default_rule_create(context, v)
6076                  for v in vals]
6077         return rules
6078 
6079     def default_rule_exists(self, context, values):
6080         """Indicates whether the specified rule values are already
6081            defined in the default security group rules.
6082         """
6083         for rule in self.db.security_group_default_rule_list(context):
6084             keys = ('cidr', 'from_port', 'to_port', 'protocol')
6085             for key in keys:
6086                 if rule.get(key) != values.get(key):
6087                     break
6088             else:
6089                 return rule.get('id') or True
6090         return False
6091 
6092     def get_all_default_rules(self, context):
6093         try:
6094             rules = self.db.security_group_default_rule_list(context)
6095         except Exception:
6096             msg = 'cannot get default security group rules'
6097             raise exception.SecurityGroupDefaultRuleNotFound(msg)
6098 
6099         return rules
6100 
6101     def get_default_rule(self, context, id):
6102         return self.db.security_group_default_rule_get(context, id)
6103 
6104     def validate_id(self, id):
6105         try:
6106             return int(id)
6107         except ValueError:
6108             msg = _("Security group id should be integer")
6109             self.raise_invalid_property(msg)
6110 
6111     def _refresh_instance_security_rules(self, context, instances):
6112         for instance in instances:
6113             if instance.host is not None:
6114                 self.compute_rpcapi.refresh_instance_security_rules(
6115                         context, instance, instance.host)
6116 
6117     def trigger_rules_refresh(self, context, id):
6118         """Called when a rule is added to or removed from a security_group."""
6119         instances = objects.InstanceList.get_by_security_group_id(context, id)
6120         self._refresh_instance_security_rules(context, instances)
6121 
6122     def trigger_members_refresh(self, context, group_ids):
6123         """Called when a security group gains a new or loses a member.
6124 
6125         Sends an update request to each compute node for each instance for
6126         which this is relevant.
6127         """
6128         instances = objects.InstanceList.get_by_grantee_security_group_ids(
6129             context, group_ids)
6130         self._refresh_instance_security_rules(context, instances)
6131 
6132     def get_instance_security_groups(self, context, instance, detailed=False):
6133         if detailed:
6134             return self.db.security_group_get_by_instance(context,
6135                                                           instance.uuid)
6136         return [{'name': group.name} for group in instance.security_groups]
