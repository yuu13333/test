Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import collections
29 from collections import deque
30 import contextlib
31 import errno
32 import functools
33 import glob
34 import itertools
35 import mmap
36 import operator
37 import os
38 import shutil
39 import tempfile
40 import time
41 import uuid
42 
43 import eventlet
44 from eventlet import greenthread
45 from eventlet import tpool
46 from lxml import etree
47 from os_brick import exception as brick_exception
48 from os_brick.initiator import connector
49 from oslo_concurrency import processutils
50 from oslo_log import log as logging
51 from oslo_serialization import jsonutils
52 from oslo_service import loopingcall
53 from oslo_utils import excutils
54 from oslo_utils import fileutils
55 from oslo_utils import importutils
56 from oslo_utils import strutils
57 from oslo_utils import timeutils
58 from oslo_utils import units
59 import six
60 from six.moves import range
61 
62 from nova.api.metadata import base as instance_metadata
63 from nova import block_device
64 from nova.compute import power_state
65 from nova.compute import task_states
66 from nova.compute import utils as compute_utils
67 import nova.conf
68 from nova.console import serial as serial_console
69 from nova.console import type as ctype
70 from nova import context as nova_context
71 from nova import exception
72 from nova.i18n import _
73 from nova.i18n import _LE
74 from nova.i18n import _LI
75 from nova.i18n import _LW
76 from nova import image
77 from nova.network import model as network_model
78 from nova import objects
79 from nova.objects import fields
80 from nova.objects import migrate_data as migrate_data_obj
81 from nova.pci import manager as pci_manager
82 from nova.pci import utils as pci_utils
83 from nova import utils
84 from nova import version
85 from nova.virt import block_device as driver_block_device
86 from nova.virt import configdrive
87 from nova.virt import diagnostics
88 from nova.virt.disk import api as disk_api
89 from nova.virt.disk.vfs import guestfs
90 from nova.virt import driver
91 from nova.virt import firewall
92 from nova.virt import hardware
93 from nova.virt.image import model as imgmodel
94 from nova.virt import images
95 from nova.virt.libvirt import blockinfo
96 from nova.virt.libvirt import config as vconfig
97 from nova.virt.libvirt import firewall as libvirt_firewall
98 from nova.virt.libvirt import guest as libvirt_guest
99 from nova.virt.libvirt import host
100 from nova.virt.libvirt import imagebackend
101 from nova.virt.libvirt import imagecache
102 from nova.virt.libvirt import instancejobtracker
103 from nova.virt.libvirt import migration as libvirt_migrate
104 from nova.virt.libvirt.storage import dmcrypt
105 from nova.virt.libvirt.storage import lvm
106 from nova.virt.libvirt.storage import rbd_utils
107 from nova.virt.libvirt.storage import sio_utils
108 from nova.virt.libvirt import utils as libvirt_utils
109 from nova.virt.libvirt import vif as libvirt_vif
110 from nova.virt.libvirt.volume import remotefs
111 from nova.virt import netutils
112 from nova.volume import cinder
113 from nova.volume import encryptors
114 
115 libvirt = None
116 
117 uefi_logged = False
118 
119 LOG = logging.getLogger(__name__)
120 
121 CONF = nova.conf.CONF
122 
123 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
124     libvirt_firewall.__name__,
125     libvirt_firewall.IptablesFirewallDriver.__name__)
126 
127 DEFAULT_UEFI_LOADER_PATH = {
128     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
129     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
130 }
131 
132 MAX_CONSOLE_BYTES = 100 * units.Ki
133 
134 # The libvirt driver will prefix any disable reason codes with this string.
135 DISABLE_PREFIX = 'AUTO: '
136 # Disable reason for the service which was enabled or disabled without reason
137 DISABLE_REASON_UNDEFINED = None
138 
139 # Guest config console string
140 CONSOLE = "console=tty0 console=ttyS0"
141 
142 GuestNumaConfig = collections.namedtuple(
143     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
144 
145 InjectionInfo = collections.namedtuple(
146     'InjectionInfo', ['network_info', 'files', 'admin_pass'])
147 
148 libvirt_volume_drivers = [
149     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
150     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
151     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
152     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
153     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
154     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
155     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
156     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
157     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
158     'glusterfs='
159         'nova.virt.libvirt.volume.glusterfs.LibvirtGlusterfsVolumeDriver',
160     'fibre_channel='
161         'nova.virt.libvirt.volume.fibrechannel.'
162         'LibvirtFibreChannelVolumeDriver',
163     'scality=nova.virt.libvirt.volume.scality.LibvirtScalityVolumeDriver',
164     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
165     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
166     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
167     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
168     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
169     'vzstorage='
170         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
171 ]
172 
173 
174 def patch_tpool_proxy():
175     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
176     or __repr__() calls. See bug #962840 for details.
177     We perform a monkey patch to replace those two instance methods.
178     """
179     def str_method(self):
180         return str(self._obj)
181 
182     def repr_method(self):
183         return repr(self._obj)
184 
185     tpool.Proxy.__str__ = str_method
186     tpool.Proxy.__repr__ = repr_method
187 
188 
189 patch_tpool_proxy()
190 
191 # For information about when MIN_LIBVIRT_VERSION and
192 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
193 #
194 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
195 #
196 # Currently this is effectively the min version for i686/x86_64
197 # + KVM/QEMU, as other architectures/hypervisors require newer
198 # versions. Over time, this will become a common min version
199 # for all architectures/hypervisors, as this value rises to
200 # meet them.
201 MIN_LIBVIRT_VERSION = (1, 2, 1)
202 MIN_QEMU_VERSION = (1, 5, 3)
203 # TODO(berrange): Re-evaluate this at start of each release cycle
204 # to decide if we want to plan a future min version bump.
205 # MIN_LIBVIRT_VERSION can be updated to match this after
206 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
207 # one cycle
208 NEXT_MIN_LIBVIRT_VERSION = (1, 2, 9)
209 NEXT_MIN_QEMU_VERSION = (2, 1, 0)
210 
211 # When the above version matches/exceeds this version
212 # delete it & corresponding code using it
213 # Relative block commit & rebase (feature is detected,
214 # this version is only used for messaging)
215 MIN_LIBVIRT_BLOCKJOB_RELATIVE_VERSION = (1, 2, 7)
216 # Libvirt version 1.2.17 is required for successful block live migration
217 # of vm booted from image with attached devices
218 MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION = (1, 2, 17)
219 # libvirt discard feature
220 MIN_QEMU_DISCARD_VERSION = (1, 6, 0)
221 # While earlier versions could support NUMA reporting and
222 # NUMA placement, not until 1.2.7 was there the ability
223 # to pin guest nodes to host nodes, so mandate that. Without
224 # this the scheduler cannot make guaranteed decisions, as the
225 # guest placement may not match what was requested
226 MIN_LIBVIRT_NUMA_VERSION = (1, 2, 7)
227 # PowerPC based hosts that support NUMA using libvirt
228 MIN_LIBVIRT_NUMA_VERSION_PPC = (1, 2, 19)
229 # Versions of libvirt with known NUMA topology issues
230 # See bug #1449028
231 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
232 # While earlier versions could support hugepage backed
233 # guests, not until 1.2.8 was there the ability to request
234 # a particular huge page size. Without this the scheduler
235 # cannot make guaranteed decisions, as the huge page size
236 # used by the guest may not match what was requested
237 MIN_LIBVIRT_HUGEPAGE_VERSION = (1, 2, 8)
238 # Versions of libvirt with broken cpu pinning support. This excludes
239 # versions of libvirt with broken NUMA support since pinning needs
240 # NUMA
241 # See bug #1438226
242 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
243 # qemu 2.1 introduces support for pinning memory on host
244 # NUMA nodes, along with the ability to specify hugepage
245 # sizes per guest NUMA node
246 MIN_QEMU_NUMA_HUGEPAGE_VERSION = (2, 1, 0)
247 # fsFreeze/fsThaw requirement
248 MIN_LIBVIRT_FSFREEZE_VERSION = (1, 2, 5)
249 
250 # UEFI booting support
251 MIN_LIBVIRT_UEFI_VERSION = (1, 2, 9)
252 
253 # Hyper-V paravirtualized time source
254 MIN_LIBVIRT_HYPERV_TIMER_VERSION = (1, 2, 2)
255 MIN_QEMU_HYPERV_TIMER_VERSION = (2, 0, 0)
256 
257 # Virtuozzo driver support
258 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
259 MIN_LIBVIRT_VIRTUOZZO_VERSION = (1, 2, 12)
260 
261 # Ability to set the user guest password with Qemu
262 MIN_LIBVIRT_SET_ADMIN_PASSWD = (1, 2, 16)
263 
264 # Ability to set the user guest password with parallels
265 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
266 
267 # s/390 & s/390x architectures with KVM
268 MIN_LIBVIRT_KVM_S390_VERSION = (1, 2, 13)
269 MIN_QEMU_S390_VERSION = (2, 3, 0)
270 
271 # libvirt < 1.3 reported virt_functions capability
272 # only when VFs are enabled.
273 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
274 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
275 
276 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
277 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
278 MIN_QEMU_VIRTLOGD = (2, 7, 0)
279 
280 # ppc64/ppc64le architectures with KVM
281 # NOTE(rfolco): Same levels for Libvirt/Qemu on Big Endian and Little
282 # Endian giving the nuance around guest vs host architectures
283 MIN_LIBVIRT_KVM_PPC64_VERSION = (1, 2, 12)
284 MIN_QEMU_PPC64_VERSION = (2, 1, 0)
285 
286 # Auto converge support
287 MIN_LIBVIRT_AUTO_CONVERGE_VERSION = (1, 2, 3)
288 MIN_QEMU_AUTO_CONVERGE = (1, 6, 0)
289 
290 # Names of the types that do not get compressed during migration
291 NO_COMPRESSION_TYPES = ('qcow2',)
292 
293 
294 # number of serial console limit
295 QEMU_MAX_SERIAL_PORTS = 4
296 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
297 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
298 
299 # realtime support
300 MIN_LIBVIRT_REALTIME_VERSION = (1, 2, 13)
301 
302 # libvirt postcopy support
303 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
304 
305 # qemu postcopy support
306 MIN_QEMU_POSTCOPY_VERSION = (2, 5, 0)
307 
308 MIN_LIBVIRT_OTHER_ARCH = {
309     fields.Architecture.S390: MIN_LIBVIRT_KVM_S390_VERSION,
310     fields.Architecture.S390X: MIN_LIBVIRT_KVM_S390_VERSION,
311     fields.Architecture.PPC: MIN_LIBVIRT_KVM_PPC64_VERSION,
312     fields.Architecture.PPC64: MIN_LIBVIRT_KVM_PPC64_VERSION,
313     fields.Architecture.PPC64LE: MIN_LIBVIRT_KVM_PPC64_VERSION,
314 }
315 
316 MIN_QEMU_OTHER_ARCH = {
317     fields.Architecture.S390: MIN_QEMU_S390_VERSION,
318     fields.Architecture.S390X: MIN_QEMU_S390_VERSION,
319     fields.Architecture.PPC: MIN_QEMU_PPC64_VERSION,
320     fields.Architecture.PPC64: MIN_QEMU_PPC64_VERSION,
321     fields.Architecture.PPC64LE: MIN_QEMU_PPC64_VERSION,
322 }
323 
324 # perf events support
325 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
326 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
327 
328 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
329                                 'mbml': 'mbm_local',
330                                 'mbmt': 'mbm_total',
331                                }
332 
333 
334 class LibvirtDriver(driver.ComputeDriver):
335     capabilities = {
336         "has_imagecache": True,
337         "supports_recreate": True,
338         "supports_migrate_to_same_host": False,
339         "supports_attach_interface": True,
340         "supports_device_tagging": True,
341     }
342 
343     def __init__(self, virtapi, read_only=False):
344         super(LibvirtDriver, self).__init__(virtapi)
345 
346         global libvirt
347         if libvirt is None:
348             libvirt = importutils.import_module('libvirt')
349             libvirt_migrate.libvirt = libvirt
350 
351         self._host = host.Host(self._uri(), read_only,
352                                lifecycle_event_handler=self.emit_event,
353                                conn_event_handler=self._handle_conn_event)
354         self._initiator = None
355         self._fc_wwnns = None
356         self._fc_wwpns = None
357         self._caps = None
358         self._supported_perf_events = []
359         self.firewall_driver = firewall.load_driver(
360             DEFAULT_FIREWALL_DRIVER,
361             host=self._host)
362 
363         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
364 
365         # TODO(mriedem): Long-term we should load up the volume drivers on
366         # demand as needed rather than doing this on startup, as there might
367         # be unsupported volume drivers in this list based on the underlying
368         # platform.
369         self.volume_drivers = self._get_volume_drivers()
370 
371         self._disk_cachemode = None
372         self.image_cache_manager = imagecache.ImageCacheManager()
373         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
374 
375         self.disk_cachemodes = {}
376 
377         self.valid_cachemodes = ["default",
378                                  "none",
379                                  "writethrough",
380                                  "writeback",
381                                  "directsync",
382                                  "unsafe",
383                                 ]
384         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
385                                                                       'qemu')
386 
387         for mode_str in CONF.libvirt.disk_cachemodes:
388             disk_type, sep, cache_mode = mode_str.partition('=')
389             if cache_mode not in self.valid_cachemodes:
390                 LOG.warning(_LW('Invalid cachemode %(cache_mode)s specified '
391                              'for disk type %(disk_type)s.'),
392                          {'cache_mode': cache_mode, 'disk_type': disk_type})
393                 continue
394             self.disk_cachemodes[disk_type] = cache_mode
395 
396         self._volume_api = cinder.API()
397         self._image_api = image.API()
398 
399         sysinfo_serial_funcs = {
400             'none': lambda: None,
401             'hardware': self._get_host_sysinfo_serial_hardware,
402             'os': self._get_host_sysinfo_serial_os,
403             'auto': self._get_host_sysinfo_serial_auto,
404         }
405 
406         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
407             CONF.libvirt.sysinfo_serial)
408 
409         self.job_tracker = instancejobtracker.InstanceJobTracker()
410         self._remotefs = remotefs.RemoteFilesystem()
411 
412         self._live_migration_flags = self._block_migration_flags = 0
413         self.active_migrations = {}
414 
415         # Compute reserved hugepages from conf file at the very
416         # beginning to ensure any syntax error will be reported and
417         # avoid any re-calculation when computing resources.
418         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
419 
420     def _get_volume_drivers(self):
421         driver_registry = dict()
422 
423         for driver_str in libvirt_volume_drivers:
424             driver_type, _sep, driver = driver_str.partition('=')
425             driver_class = importutils.import_class(driver)
426             try:
427                 driver_registry[driver_type] = driver_class(self._host)
428             except brick_exception.InvalidConnectorProtocol:
429                 LOG.debug('Unable to load volume driver %s. It is not '
430                           'supported on this host.', driver)
431 
432         return driver_registry
433 
434     @property
435     def disk_cachemode(self):
436         if self._disk_cachemode is None:
437             # We prefer 'none' for consistent performance, host crash
438             # safety & migration correctness by avoiding host page cache.
439             # Some filesystems (eg GlusterFS via FUSE) don't support
440             # O_DIRECT though. For those we fallback to 'writethrough'
441             # which gives host crash safety, and is safe for migration
442             # provided the filesystem is cache coherent (cluster filesystems
443             # typically are, but things like NFS are not).
444             self._disk_cachemode = "none"
445             if not self._supports_direct_io(CONF.instances_path):
446                 self._disk_cachemode = "writethrough"
447         return self._disk_cachemode
448 
449     def _set_cache_mode(self, conf):
450         """Set cache mode on LibvirtConfigGuestDisk object."""
451         try:
452             source_type = conf.source_type
453             driver_cache = conf.driver_cache
454         except AttributeError:
455             return
456 
457         cache_mode = self.disk_cachemodes.get(source_type,
458                                               driver_cache)
459         conf.driver_cache = cache_mode
460 
461     def _do_quality_warnings(self):
462         """Warn about untested driver configurations.
463 
464         This will log a warning message about untested driver or host arch
465         configurations to indicate to administrators that the quality is
466         unknown. Currently, only qemu or kvm on intel 32- or 64-bit systems
467         is tested upstream.
468         """
469         caps = self._host.get_capabilities()
470         hostarch = caps.host.cpu.arch
471         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
472             hostarch not in (fields.Architecture.I686,
473                              fields.Architecture.X86_64)):
474             LOG.warning(_LW('The libvirt driver is not tested on '
475                          '%(type)s/%(arch)s by the OpenStack project and '
476                          'thus its quality can not be ensured. For more '
477                          'information, see: http://docs.openstack.org/'
478                          'developer/nova/support-matrix.html'),
479                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
480 
481     def _handle_conn_event(self, enabled, reason):
482         LOG.info(_LI("Connection event '%(enabled)d' reason '%(reason)s'"),
483                  {'enabled': enabled, 'reason': reason})
484         self._set_host_enabled(enabled, reason)
485 
486     def _version_to_string(self, version):
487         return '.'.join([str(x) for x in version])
488 
489     def init_host(self, host):
490         self._host.initialize()
491 
492         self._do_quality_warnings()
493 
494         self._parse_migration_flags()
495 
496         self._supported_perf_events = self._get_supported_perf_events()
497 
498         if (CONF.libvirt.virt_type == 'lxc' and
499                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
500             LOG.warning(_LW("Running libvirt-lxc without user namespaces is "
501                          "dangerous. Containers spawned by Nova will be run "
502                          "as the host's root user. It is highly suggested "
503                          "that user namespaces be used in a public or "
504                          "multi-tenant environment."))
505 
506         # Stop libguestfs using KVM unless we're also configured
507         # to use this. This solves problem where people need to
508         # stop Nova use of KVM because nested-virt is broken
509         if CONF.libvirt.virt_type != "kvm":
510             guestfs.force_tcg()
511 
512         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
513             raise exception.NovaException(
514                 _('Nova requires libvirt version %s or greater.') %
515                 self._version_to_string(MIN_LIBVIRT_VERSION))
516 
517         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
518             not self._host.has_min_version(hv_ver=MIN_QEMU_VERSION)):
519             raise exception.NovaException(
520                 _('Nova requires QEMU version %s or greater.') %
521                 self._version_to_string(MIN_QEMU_VERSION))
522 
523         if CONF.libvirt.virt_type == 'parallels':
524             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
525                 raise exception.NovaException(
526                     _('Nova requires Virtuozzo version %s or greater.') %
527                     self._version_to_string(MIN_VIRTUOZZO_VERSION))
528             if not self._host.has_min_version(MIN_LIBVIRT_VIRTUOZZO_VERSION):
529                 raise exception.NovaException(
530                     _('Running Nova with parallels virt_type requires '
531                       'libvirt version %s') %
532                     self._version_to_string(MIN_LIBVIRT_VIRTUOZZO_VERSION))
533 
534         # Give the cloud admin a heads up if we are intending to
535         # change the MIN_LIBVIRT_VERSION in the next release.
536         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
537             LOG.warning(_LW('Running Nova with a libvirt version less than '
538                             '%(version)s is deprecated. The required minimum '
539                             'version of libvirt will be raised to %(version)s '
540                             'in the next release.'),
541                         {'version': self._version_to_string(
542                             NEXT_MIN_LIBVIRT_VERSION)})
543         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
544             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
545             LOG.warning(_LW('Running Nova with a QEMU version less than '
546                             '%(version)s is deprecated. The required minimum '
547                             'version of QEMU will be raised to %(version)s '
548                             'in the next release.'),
549                         {'version': self._version_to_string(
550                             NEXT_MIN_QEMU_VERSION)})
551 
552         kvm_arch = fields.Architecture.from_host()
553         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
554             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
555                 not self._host.has_min_version(
556                                         MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch),
557                                         MIN_QEMU_OTHER_ARCH.get(kvm_arch))):
558             raise exception.NovaException(
559                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
560                   'requires libvirt version %(libvirt_ver)s and '
561                   'qemu version %(qemu_ver)s, or greater') %
562                 {'arch': kvm_arch,
563                  'libvirt_ver': self._version_to_string(
564                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch)),
565                  'qemu_ver': self._version_to_string(
566                      MIN_QEMU_OTHER_ARCH.get(kvm_arch))})
567 
568     def _prepare_migration_flags(self):
569         migration_flags = 0
570 
571         migration_flags |= libvirt.VIR_MIGRATE_LIVE
572 
573         # Adding p2p flag only if xen is not in use, because xen does not
574         # support p2p migrations
575         if CONF.libvirt.virt_type != 'xen':
576             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
577 
578         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
579         # instance will remain defined on the source host
580         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
581 
582         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
583         # destination host
584         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
585 
586         live_migration_flags = block_migration_flags = migration_flags
587 
588         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
589         # will be live-migrations instead
590         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
591 
592         return (live_migration_flags, block_migration_flags)
593 
594     def _handle_live_migration_tunnelled(self, migration_flags):
595         if (CONF.libvirt.live_migration_tunnelled is None or
596                 CONF.libvirt.live_migration_tunnelled):
597             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
598         return migration_flags
599 
600     def _is_post_copy_available(self):
601         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION,
602                                       hv_ver=MIN_QEMU_POSTCOPY_VERSION):
603             return True
604         return False
605 
606     def _is_virtlogd_available(self):
607         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
608                                           MIN_QEMU_VIRTLOGD)
609 
610     def _handle_live_migration_post_copy(self, migration_flags):
611         if CONF.libvirt.live_migration_permit_post_copy:
612             if self._is_post_copy_available():
613                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
614             else:
615                 LOG.info(_LI('The live_migration_permit_post_copy is set '
616                              'to True, but it is not supported.'))
617         return migration_flags
618 
619     def _handle_live_migration_auto_converge(self, migration_flags):
620         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_AUTO_CONVERGE_VERSION,
621                                       hv_ver=MIN_QEMU_AUTO_CONVERGE):
622             if (self._is_post_copy_available() and
623                     (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
624                 LOG.info(_LI('The live_migration_permit_post_copy is set to '
625                              'True and post copy live migration is available '
626                              'so auto-converge will not be in use.'))
627             elif CONF.libvirt.live_migration_permit_auto_converge:
628                 migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
629         elif CONF.libvirt.live_migration_permit_auto_converge:
630             LOG.info(_LI('The live_migration_permit_auto_converge is set '
631                             'to True, but it is not supported.'))
632         return migration_flags
633 
634     def _parse_migration_flags(self):
635         (live_migration_flags,
636             block_migration_flags) = self._prepare_migration_flags()
637 
638         live_migration_flags = self._handle_live_migration_tunnelled(
639             live_migration_flags)
640         block_migration_flags = self._handle_live_migration_tunnelled(
641             block_migration_flags)
642 
643         live_migration_flags = self._handle_live_migration_post_copy(
644             live_migration_flags)
645         block_migration_flags = self._handle_live_migration_post_copy(
646             block_migration_flags)
647 
648         live_migration_flags = self._handle_live_migration_auto_converge(
649             live_migration_flags)
650         block_migration_flags = self._handle_live_migration_auto_converge(
651             block_migration_flags)
652 
653         self._live_migration_flags = live_migration_flags
654         self._block_migration_flags = block_migration_flags
655 
656     # TODO(sahid): This method is targeted for removal when the tests
657     # have been updated to avoid its use
658     #
659     # All libvirt API calls on the libvirt.Connect object should be
660     # encapsulated by methods on the nova.virt.libvirt.host.Host
661     # object, rather than directly invoking the libvirt APIs. The goal
662     # is to avoid a direct dependency on the libvirt API from the
663     # driver.py file.
664     def _get_connection(self):
665         return self._host.get_connection()
666 
667     _conn = property(_get_connection)
668 
669     @staticmethod
670     def _uri():
671         if CONF.libvirt.virt_type == 'uml':
672             uri = CONF.libvirt.connection_uri or 'uml:///system'
673         elif CONF.libvirt.virt_type == 'xen':
674             uri = CONF.libvirt.connection_uri or 'xen:///'
675         elif CONF.libvirt.virt_type == 'lxc':
676             uri = CONF.libvirt.connection_uri or 'lxc:///'
677         elif CONF.libvirt.virt_type == 'parallels':
678             uri = CONF.libvirt.connection_uri or 'parallels:///system'
679         else:
680             uri = CONF.libvirt.connection_uri or 'qemu:///system'
681         return uri
682 
683     @staticmethod
684     def _live_migration_uri(dest):
685         # Only Xen and QEMU support live migration, see
686         # https://libvirt.org/migration.html#scenarios for reference
687         uris = {
688             'kvm': 'qemu+tcp://%s/system',
689             'qemu': 'qemu+tcp://%s/system',
690             'xen': 'xenmigr://%s/system',
691         }
692         virt_type = CONF.libvirt.virt_type
693         uri = CONF.libvirt.live_migration_uri or uris.get(virt_type)
694         if uri is None:
695             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
696         return uri % dest
697 
698     @staticmethod
699     def _migrate_uri(dest):
700         uri = None
701         # Only QEMU live migrations supports migrate-uri parameter
702         virt_type = CONF.libvirt.virt_type
703         if virt_type in ('qemu', 'kvm'):
704             # QEMU accept two schemes: tcp and rdma.  By default
705             # libvirt build the URI using the remote hostname and the
706             # tcp schema.
707             uri = 'tcp://%s' % dest
708         # Because dest might be of type unicode, here we might return value of
709         # type unicode as well which is not acceptable by libvirt python
710         # binding when Python 2.7 is in use, so let's convert it explicitly
711         # back to string. When Python 3.x is in use, libvirt python binding
712         # accepts unicode type so it is completely fine to do a no-op str(uri)
713         # conversion which will return value of type unicode.
714         return uri and str(uri)
715 
716     def instance_exists(self, instance):
717         """Efficient override of base instance_exists method."""
718         try:
719             self._host.get_guest(instance)
720             return True
721         except exception.NovaException:
722             return False
723 
724     def list_instances(self):
725         names = []
726         for guest in self._host.list_guests(only_running=False):
727             names.append(guest.name)
728 
729         return names
730 
731     def list_instance_uuids(self):
732         uuids = []
733         for guest in self._host.list_guests(only_running=False):
734             uuids.append(guest.uuid)
735 
736         return uuids
737 
738     def plug_vifs(self, instance, network_info):
739         """Plug VIFs into networks."""
740         for vif in network_info:
741             self.vif_driver.plug(instance, vif)
742 
743     def _unplug_vifs(self, instance, network_info, ignore_errors):
744         """Unplug VIFs from networks."""
745         for vif in network_info:
746             try:
747                 self.vif_driver.unplug(instance, vif)
748             except exception.NovaException:
749                 if not ignore_errors:
750                     raise
751 
752     def unplug_vifs(self, instance, network_info):
753         self._unplug_vifs(instance, network_info, False)
754 
755     def _teardown_container(self, instance):
756         inst_path = libvirt_utils.get_instance_path(instance)
757         container_dir = os.path.join(inst_path, 'rootfs')
758         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
759         LOG.debug('Attempting to teardown container at path %(dir)s with '
760                   'root device: %(rootfs_dev)s',
761                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
762                   instance=instance)
763         disk_api.teardown_container(container_dir, rootfs_dev)
764 
765     def _destroy(self, instance, attempt=1):
766         try:
767             guest = self._host.get_guest(instance)
768             if CONF.serial_console.enabled:
769                 # This method is called for several events: destroy,
770                 # rebuild, hard-reboot, power-off - For all of these
771                 # events we want to release the serial ports acquired
772                 # for the guest before destroying it.
773                 serials = self._get_serial_ports_from_guest(guest)
774                 for hostname, port in serials:
775                     serial_console.release_port(host=hostname, port=port)
776         except exception.InstanceNotFound:
777             guest = None
778 
779         # If the instance is already terminated, we're still happy
780         # Otherwise, destroy it
781         old_domid = -1
782         if guest is not None:
783             try:
784                 old_domid = guest.id
785                 guest.poweroff()
786 
787             except libvirt.libvirtError as e:
788                 is_okay = False
789                 errcode = e.get_error_code()
790                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
791                     # Domain already gone. This can safely be ignored.
792                     is_okay = True
793                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
794                     # If the instance is already shut off, we get this:
795                     # Code=55 Error=Requested operation is not valid:
796                     # domain is not running
797 
798                     state = guest.get_power_state(self._host)
799                     if state == power_state.SHUTDOWN:
800                         is_okay = True
801                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
802                     errmsg = e.get_error_message()
803                     if (CONF.libvirt.virt_type == 'lxc' and
804                         errmsg == 'internal error: '
805                                   'Some processes refused to die'):
806                         # Some processes in the container didn't die
807                         # fast enough for libvirt. The container will
808                         # eventually die. For now, move on and let
809                         # the wait_for_destroy logic take over.
810                         is_okay = True
811                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
812                     LOG.warning(_LW("Cannot destroy instance, operation time "
813                                  "out"),
814                              instance=instance)
815                     reason = _("operation time out")
816                     raise exception.InstancePowerOffFailure(reason=reason)
817                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
818                     if e.get_int1() == errno.EBUSY:
819                         # NOTE(danpb): When libvirt kills a process it sends it
820                         # SIGTERM first and waits 10 seconds. If it hasn't gone
821                         # it sends SIGKILL and waits another 5 seconds. If it
822                         # still hasn't gone then you get this EBUSY error.
823                         # Usually when a QEMU process fails to go away upon
824                         # SIGKILL it is because it is stuck in an
825                         # uninterruptible kernel sleep waiting on I/O from
826                         # some non-responsive server.
827                         # Given the CPU load of the gate tests though, it is
828                         # conceivable that the 15 second timeout is too short,
829                         # particularly if the VM running tempest has a high
830                         # steal time from the cloud host. ie 15 wallclock
831                         # seconds may have passed, but the VM might have only
832                         # have a few seconds of scheduled run time.
833                         LOG.warning(_LW('Error from libvirt during destroy. '
834                                      'Code=%(errcode)s Error=%(e)s; '
835                                      'attempt %(attempt)d of 3'),
836                                  {'errcode': errcode, 'e': e,
837                                   'attempt': attempt},
838                                  instance=instance)
839                         with excutils.save_and_reraise_exception() as ctxt:
840                             # Try up to 3 times before giving up.
841                             if attempt < 3:
842                                 ctxt.reraise = False
843                                 self._destroy(instance, attempt + 1)
844                                 return
845 
846                 if not is_okay:
847                     with excutils.save_and_reraise_exception():
848                         LOG.error(_LE('Error from libvirt during destroy. '
849                                       'Code=%(errcode)s Error=%(e)s'),
850                                   {'errcode': errcode, 'e': e},
851                                   instance=instance)
852 
853         def _wait_for_destroy(expected_domid):
854             """Called at an interval until the VM is gone."""
855             # NOTE(vish): If the instance disappears during the destroy
856             #             we ignore it so the cleanup can still be
857             #             attempted because we would prefer destroy to
858             #             never fail.
859             try:
860                 dom_info = self.get_info(instance)
861                 state = dom_info.state
862                 new_domid = dom_info.id
863             except exception.InstanceNotFound:
864                 LOG.info(_LI("During wait destroy, instance disappeared."),
865                          instance=instance)
866                 raise loopingcall.LoopingCallDone()
867 
868             if state == power_state.SHUTDOWN:
869                 LOG.info(_LI("Instance destroyed successfully."),
870                          instance=instance)
871                 raise loopingcall.LoopingCallDone()
872 
873             # NOTE(wangpan): If the instance was booted again after destroy,
874             #                this may be an endless loop, so check the id of
875             #                domain here, if it changed and the instance is
876             #                still running, we should destroy it again.
877             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
878             if new_domid != expected_domid:
879                 LOG.info(_LI("Instance may be started again."),
880                          instance=instance)
881                 kwargs['is_running'] = True
882                 raise loopingcall.LoopingCallDone()
883 
884         kwargs = {'is_running': False}
885         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
886                                                      old_domid)
887         timer.start(interval=0.5).wait()
888         if kwargs['is_running']:
889             LOG.info(_LI("Going to destroy instance again."),
890                      instance=instance)
891             self._destroy(instance)
892         else:
893             # NOTE(GuanQiang): teardown container to avoid resource leak
894             if CONF.libvirt.virt_type == 'lxc':
895                 self._teardown_container(instance)
896 
897     def destroy(self, context, instance, network_info, block_device_info=None,
898                 destroy_disks=True, migrate_data=None):
899         self._destroy(instance)
900         self.cleanup(context, instance, network_info, block_device_info,
901                      destroy_disks, migrate_data)
902 
903     def _undefine_domain(self, instance):
904         try:
905             guest = self._host.get_guest(instance)
906             try:
907                 guest.delete_configuration()
908             except libvirt.libvirtError as e:
909                 with excutils.save_and_reraise_exception():
910                     errcode = e.get_error_code()
911                     LOG.error(_LE('Error from libvirt during undefine. '
912                                   'Code=%(errcode)s Error=%(e)s'),
913                               {'errcode': errcode, 'e': e}, instance=instance)
914         except exception.InstanceNotFound:
915             pass
916 
917     def cleanup(self, context, instance, network_info, block_device_info=None,
918                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
919         if destroy_vifs:
920             self._unplug_vifs(instance, network_info, True)
921 
922         retry = True
923         while retry:
924             try:
925                 self.unfilter_instance(instance, network_info)
926             except libvirt.libvirtError as e:
927                 try:
928                     state = self.get_info(instance).state
929                 except exception.InstanceNotFound:
930                     state = power_state.SHUTDOWN
931 
932                 if state != power_state.SHUTDOWN:
933                     LOG.warning(_LW("Instance may be still running, destroy "
934                                  "it again."), instance=instance)
935                     self._destroy(instance)
936                 else:
937                     retry = False
938                     errcode = e.get_error_code()
939                     LOG.exception(_LE('Error from libvirt during unfilter. '
940                                       'Code=%(errcode)s Error=%(e)s'),
941                                   {'errcode': errcode, 'e': e},
942                                   instance=instance)
943                     reason = "Error unfiltering instance."
944                     raise exception.InstanceTerminationFailure(reason=reason)
945             except Exception:
946                 retry = False
947                 raise
948             else:
949                 retry = False
950 
951         # FIXME(wangpan): if the instance is booted again here, such as the
952         #                 the soft reboot operation boot it here, it will
953         #                 become "running deleted", should we check and destroy
954         #                 it at the end of this method?
955 
956         # NOTE(vish): we disconnect from volumes regardless
957         block_device_mapping = driver.block_device_info_get_mapping(
958             block_device_info)
959         for vol in block_device_mapping:
960             connection_info = vol['connection_info']
961             disk_dev = vol['mount_device']
962             if disk_dev is not None:
963                 disk_dev = disk_dev.rpartition("/")[2]
964 
965             if ('data' in connection_info and
966                     'volume_id' in connection_info['data']):
967                 volume_id = connection_info['data']['volume_id']
968                 encryption = encryptors.get_encryption_metadata(
969                     context, self._volume_api, volume_id, connection_info)
970 
971                 if encryption:
972                     # The volume must be detached from the VM before
973                     # disconnecting it from its encryptor. Otherwise, the
974                     # encryptor may report that the volume is still in use.
975                     encryptor = self._get_volume_encryptor(connection_info,
976                                                            encryption)
977                     encryptor.detach_volume(**encryption)
978 
979             try:
980                 self._disconnect_volume(connection_info, disk_dev)
981             except Exception as exc:
982                 with excutils.save_and_reraise_exception() as ctxt:
983                     if destroy_disks:
984                         # Don't block on Volume errors if we're trying to
985                         # delete the instance as we may be partially created
986                         # or deleted
987                         ctxt.reraise = False
988                         LOG.warning(
989                             _LW("Ignoring Volume Error on vol %(vol_id)s "
990                                 "during delete %(exc)s"),
991                             {'vol_id': vol.get('volume_id'), 'exc': exc},
992                             instance=instance)
993 
994         if destroy_disks:
995             # NOTE(haomai): destroy volumes if needed
996             if CONF.libvirt.images_type == 'lvm':
997                 self._cleanup_lvm(instance, block_device_info)
998             if CONF.libvirt.images_type == 'rbd':
999                 self._cleanup_rbd(instance)
1000         if CONF.libvirt.images_type == 'sio':
1001             self._cleanup_sio(instance, destroy_disks)
1002 
1003         is_shared_block_storage = False
1004         if migrate_data and 'is_shared_block_storage' in migrate_data:
1005             is_shared_block_storage = migrate_data.is_shared_block_storage
1006         if destroy_disks or is_shared_block_storage:
1007             attempts = int(instance.system_metadata.get('clean_attempts',
1008                                                         '0'))
1009             success = self.delete_instance_files(instance)
1010             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1011             # task in the compute manager. The tight coupling is not great...
1012             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1013             if success:
1014                 instance.cleaned = True
1015             instance.save()
1016 
1017         self._undefine_domain(instance)
1018 
1019     def _detach_encrypted_volumes(self, instance, block_device_info):
1020         """Detaches encrypted volumes attached to instance."""
1021         disks = jsonutils.loads(self.get_instance_disk_info(instance,
1022                                                             block_device_info))
1023         encrypted_volumes = filter(dmcrypt.is_encrypted,
1024                                    [disk['path'] for disk in disks])
1025         for path in encrypted_volumes:
1026             dmcrypt.delete_volume(path)
1027 
1028     def _get_serial_ports_from_guest(self, guest, mode=None):
1029         """Returns an iterator over serial port(s) configured on guest.
1030 
1031         :param mode: Should be a value in (None, bind, connect)
1032         """
1033         xml = guest.get_xml_desc()
1034         tree = etree.fromstring(xml)
1035 
1036         # The 'serial' device is the base for x86 platforms. Other platforms
1037         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1038         xpath_mode = "[@mode='%s']" % mode if mode else ""
1039         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1040         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1041 
1042         tcp_devices = tree.findall(serial_tcp)
1043         if len(tcp_devices) == 0:
1044             tcp_devices = tree.findall(console_tcp)
1045         for source in tcp_devices:
1046             yield (source.get("host"), int(source.get("service")))
1047 
1048     @staticmethod
1049     def _get_rbd_driver():
1050         return rbd_utils.RBDDriver(
1051                 pool=CONF.libvirt.images_rbd_pool,
1052                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1053                 rbd_user=CONF.libvirt.rbd_user)
1054 
1055     @staticmethod
1056     def _get_sio_driver():
1057         return sio_utils.SIODriver()
1058 
1059     def _cleanup_rbd(self, instance):
1060         # NOTE(nic): On revert_resize, the cleanup steps for the root
1061         # volume are handled with an "rbd snap rollback" command,
1062         # and none of this is needed (and is, in fact, harmful) so
1063         # filter out non-ephemerals from the list
1064         if instance.task_state == task_states.RESIZE_REVERTING:
1065             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1066                                       disk.endswith('disk.local'))
1067         else:
1068             filter_fn = lambda disk: disk.startswith(instance.uuid)
1069         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1070 
1071     def _cleanup_lvm(self, instance, block_device_info):
1072         """Delete all LVM disks for given instance object."""
1073         if instance.get('ephemeral_key_uuid') is not None:
1074             self._detach_encrypted_volumes(instance, block_device_info)
1075 
1076         disks = self._lvm_disks(instance)
1077         if disks:
1078             lvm.remove_volumes(disks)
1079 
1080     def _lvm_disks(self, instance):
1081         """Returns all LVM disks for given instance object."""
1082         if CONF.libvirt.images_volume_group:
1083             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1084             if not os.path.exists(vg):
1085                 return []
1086             pattern = '%s_' % instance.uuid
1087 
1088             def belongs_to_instance(disk):
1089                 return disk.startswith(pattern)
1090 
1091             def fullpath(name):
1092                 return os.path.join(vg, name)
1093 
1094             logical_volumes = lvm.list_volumes(vg)
1095 
1096             disk_names = filter(belongs_to_instance, logical_volumes)
1097             disks = map(fullpath, disk_names)
1098             return disks
1099         return []
1100 
1101     def _cleanup_sio(self, instance, destroy_disks):
1102         LibvirtDriver._get_sio_driver().cleanup_volumes(
1103             instance, unmap_only=not destroy_disks)
1104 
1105     def get_volume_connector(self, instance):
1106         root_helper = utils.get_root_helper()
1107         return connector.get_connector_properties(
1108             root_helper, CONF.my_block_storage_ip,
1109             CONF.libvirt.volume_use_multipath,
1110             enforce_multipath=True,
1111             host=CONF.host)
1112 
1113     def _cleanup_resize(self, instance, network_info):
1114         target = libvirt_utils.get_instance_path(instance) + '_resize'
1115 
1116         if os.path.exists(target):
1117             # Deletion can fail over NFS, so retry the deletion as required.
1118             # Set maximum attempt as 5, most test can remove the directory
1119             # for the second time.
1120             utils.execute('rm', '-rf', target, delay_on_retry=True,
1121                           attempts=5)
1122 
1123         root_disk = self.image_backend.by_name(instance, 'disk')
1124         # TODO(nic): Set ignore_errors=False in a future release.
1125         # It is set to True here to avoid any upgrade issues surrounding
1126         # instances being in pending resize state when the software is updated;
1127         # in that case there will be no snapshot to remove.  Once it can be
1128         # reasonably assumed that no such instances exist in the wild
1129         # anymore, it should be set back to False (the default) so it will
1130         # throw errors, like it should.
1131         if root_disk.exists():
1132             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1133                                   ignore_errors=True)
1134 
1135         if instance.host != CONF.host:
1136             self._undefine_domain(instance)
1137             self.unplug_vifs(instance, network_info)
1138             self.unfilter_instance(instance, network_info)
1139             self.image_backend.backend().disconnect_disks(instance)
1140 
1141     def _get_volume_driver(self, connection_info):
1142         driver_type = connection_info.get('driver_volume_type')
1143         if driver_type not in self.volume_drivers:
1144             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1145         return self.volume_drivers[driver_type]
1146 
1147     def _connect_volume(self, connection_info, disk_info):
1148         vol_driver = self._get_volume_driver(connection_info)
1149         vol_driver.connect_volume(connection_info, disk_info)
1150 
1151     def _disconnect_volume(self, connection_info, disk_dev):
1152         vol_driver = self._get_volume_driver(connection_info)
1153         vol_driver.disconnect_volume(connection_info, disk_dev)
1154 
1155     def _get_volume_config(self, connection_info, disk_info):
1156         vol_driver = self._get_volume_driver(connection_info)
1157         return vol_driver.get_config(connection_info, disk_info)
1158 
1159     def _get_volume_encryptor(self, connection_info, encryption):
1160         encryptor = encryptors.get_volume_encryptor(connection_info,
1161                                                     **encryption)
1162         return encryptor
1163 
1164     def _check_discard_for_attach_volume(self, conf, instance):
1165         """Perform some checks for volumes configured for discard support.
1166 
1167         If discard is configured for the volume, and the guest is using a
1168         configuration known to not work, we will log a message explaining
1169         the reason why.
1170         """
1171         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1172             LOG.debug('Attempting to attach volume %(id)s with discard '
1173                       'support enabled to an instance using an '
1174                       'unsupported configuration. target_bus = '
1175                       '%(bus)s. Trim commands will not be issued to '
1176                       'the storage device.',
1177                       {'bus': conf.target_bus,
1178                        'id': conf.serial},
1179                       instance=instance)
1180 
1181     def attach_volume(self, context, connection_info, instance, mountpoint,
1182                       disk_bus=None, device_type=None, encryption=None):
1183         guest = self._host.get_guest(instance)
1184 
1185         disk_dev = mountpoint.rpartition("/")[2]
1186         bdm = {
1187             'device_name': disk_dev,
1188             'disk_bus': disk_bus,
1189             'device_type': device_type}
1190 
1191         # Note(cfb): If the volume has a custom block size, check that
1192         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1193         #            presence of a block size is considered mandatory by
1194         #            cinder so we fail if we can't honor the request.
1195         data = {}
1196         if ('data' in connection_info):
1197             data = connection_info['data']
1198         if ('logical_block_size' in data or 'physical_block_size' in data):
1199             if ((CONF.libvirt.virt_type != "kvm" and
1200                  CONF.libvirt.virt_type != "qemu")):
1201                 msg = _("Volume sets block size, but the current "
1202                         "libvirt hypervisor '%s' does not support custom "
1203                         "block size") % CONF.libvirt.virt_type
1204                 raise exception.InvalidHypervisorType(msg)
1205 
1206         disk_info = blockinfo.get_info_from_bdm(
1207             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1208         self._connect_volume(connection_info, disk_info)
1209         conf = self._get_volume_config(connection_info, disk_info)
1210         self._set_cache_mode(conf)
1211 
1212         self._check_discard_for_attach_volume(conf, instance)
1213 
1214         try:
1215             state = guest.get_power_state(self._host)
1216             live = state in (power_state.RUNNING, power_state.PAUSED)
1217 
1218             if encryption:
1219                 encryptor = self._get_volume_encryptor(connection_info,
1220                                                        encryption)
1221                 encryptor.attach_volume(context, **encryption)
1222 
1223             guest.attach_device(conf, persistent=True, live=live)
1224         except Exception as ex:
1225             LOG.exception(_LE('Failed to attach volume at mountpoint: %s'),
1226                           mountpoint, instance=instance)
1227             if isinstance(ex, libvirt.libvirtError):
1228                 errcode = ex.get_error_code()
1229                 if errcode == libvirt.VIR_ERR_OPERATION_FAILED:
1230                     self._disconnect_volume(connection_info, disk_dev)
1231                     raise exception.DeviceIsBusy(device=disk_dev)
1232 
1233             with excutils.save_and_reraise_exception():
1234                 self._disconnect_volume(connection_info, disk_dev)
1235 
1236     def _swap_volume(self, guest, disk_path, new_path, resize_to):
1237         """Swap existing disk with a new block device."""
1238         dev = guest.get_block_device(disk_path)
1239 
1240         # Save a copy of the domain's persistent XML file
1241         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1242 
1243         # Abort is an idempotent operation, so make sure any block
1244         # jobs which may have failed are ended.
1245         try:
1246             dev.abort_job()
1247         except Exception:
1248             pass
1249 
1250         try:
1251             # NOTE (rmk): blockRebase cannot be executed on persistent
1252             #             domains, so we need to temporarily undefine it.
1253             #             If any part of this block fails, the domain is
1254             #             re-defined regardless.
1255             if guest.has_persistent_configuration():
1256                 guest.delete_configuration()
1257 
1258             # Start copy with VIR_DOMAIN_REBASE_REUSE_EXT flag to
1259             # allow writing to existing external volume file
1260             dev.rebase(new_path, copy=True, reuse_ext=True)
1261 
1262             while not dev.is_job_complete():
1263                 time.sleep(0.5)
1264 
1265             dev.abort_job(pivot=True)
1266             if resize_to:
1267                 # NOTE(alex_xu): domain.blockJobAbort isn't sync call. This
1268                 # is bug in libvirt. So we need waiting for the pivot is
1269                 # finished. libvirt bug #1119173
1270                 while not dev.is_job_complete():
1271                     time.sleep(0.5)
1272                 dev.resize(resize_to * units.Gi / units.Ki)
1273         finally:
1274             self._host.write_instance_config(xml)
1275 
1276     def swap_volume(self, old_connection_info,
1277                     new_connection_info, instance, mountpoint, resize_to):
1278 
1279         guest = self._host.get_guest(instance)
1280 
1281         disk_dev = mountpoint.rpartition("/")[2]
1282         if not guest.get_disk(disk_dev):
1283             raise exception.DiskNotFound(location=disk_dev)
1284         disk_info = {
1285             'dev': disk_dev,
1286             'bus': blockinfo.get_disk_bus_for_disk_dev(
1287                 CONF.libvirt.virt_type, disk_dev),
1288             'type': 'disk',
1289             }
1290         self._connect_volume(new_connection_info, disk_info)
1291         conf = self._get_volume_config(new_connection_info, disk_info)
1292         if not conf.source_path:
1293             self._disconnect_volume(new_connection_info, disk_dev)
1294             raise NotImplementedError(_("Swap only supports host devices"))
1295 
1296         # Save updates made in connection_info when connect_volume was called
1297         volume_id = new_connection_info.get('serial')
1298         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
1299             nova_context.get_admin_context(), volume_id, instance.uuid)
1300         driver_bdm = driver_block_device.convert_volume(bdm)
1301         driver_bdm['connection_info'] = new_connection_info
1302         driver_bdm.save()
1303 
1304         self._swap_volume(guest, disk_dev, conf.source_path, resize_to)
1305         self._disconnect_volume(old_connection_info, disk_dev)
1306 
1307     def _get_existing_domain_xml(self, instance, network_info,
1308                                  block_device_info=None):
1309         try:
1310             guest = self._host.get_guest(instance)
1311             xml = guest.get_xml_desc()
1312         except exception.InstanceNotFound:
1313             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1314                                                 instance,
1315                                                 instance.image_meta,
1316                                                 block_device_info)
1317             xml = self._get_guest_xml(nova_context.get_admin_context(),
1318                                       instance, network_info, disk_info,
1319                                       instance.image_meta,
1320                                       block_device_info=block_device_info)
1321         return xml
1322 
1323     def detach_volume(self, connection_info, instance, mountpoint,
1324                       encryption=None):
1325         disk_dev = mountpoint.rpartition("/")[2]
1326         try:
1327             guest = self._host.get_guest(instance)
1328 
1329             state = guest.get_power_state(self._host)
1330             live = state in (power_state.RUNNING, power_state.PAUSED)
1331 
1332             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1333                                                              disk_dev,
1334                                                              persistent=True,
1335                                                              live=live)
1336 
1337             if encryption:
1338                 # The volume must be detached from the VM before
1339                 # disconnecting it from its encryptor. Otherwise, the
1340                 # encryptor may report that the volume is still in use.
1341                 encryptor = self._get_volume_encryptor(connection_info,
1342                                                        encryption)
1343                 encryptor.detach_volume(**encryption)
1344 
1345             wait_for_detach()
1346         except exception.InstanceNotFound:
1347             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1348             #                will throw InstanceNotFound exception. Need to
1349             #                disconnect volume under this circumstance.
1350             LOG.warning(_LW("During detach_volume, instance disappeared."),
1351                      instance=instance)
1352         except exception.DeviceNotFound:
1353             raise exception.DiskNotFound(location=disk_dev)
1354         except libvirt.libvirtError as ex:
1355             # NOTE(vish): This is called to cleanup volumes after live
1356             #             migration, so we should still disconnect even if
1357             #             the instance doesn't exist here anymore.
1358             error_code = ex.get_error_code()
1359             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1360                 # NOTE(vish):
1361                 LOG.warning(_LW("During detach_volume, instance disappeared."),
1362                          instance=instance)
1363             else:
1364                 raise
1365 
1366         self._disconnect_volume(connection_info, disk_dev)
1367 
1368     def attach_interface(self, context, instance, image_meta, vif):
1369         guest = self._host.get_guest(instance)
1370 
1371         self.vif_driver.plug(instance, vif)
1372         self.firewall_driver.setup_basic_filtering(instance, [vif])
1373         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1374                                          instance.flavor,
1375                                          CONF.libvirt.virt_type,
1376                                          self._host)
1377         try:
1378             state = guest.get_power_state(self._host)
1379             live = state in (power_state.RUNNING, power_state.PAUSED)
1380             guest.attach_device(cfg, persistent=True, live=live)
1381         except libvirt.libvirtError:
1382             LOG.error(_LE('attaching network adapter failed.'),
1383                      instance=instance, exc_info=True)
1384             self.vif_driver.unplug(instance, vif)
1385             raise exception.InterfaceAttachFailed(
1386                     instance_uuid=instance.uuid)
1387 
1388     def detach_interface(self, context, instance, vif):
1389         guest = self._host.get_guest(instance)
1390         cfg = self.vif_driver.get_config(instance, vif,
1391                                          instance.image_meta,
1392                                          instance.flavor,
1393                                          CONF.libvirt.virt_type, self._host)
1394         interface = guest.get_interface_by_cfg(cfg)
1395         try:
1396             self.vif_driver.unplug(instance, vif)
1397             # NOTE(mriedem): When deleting an instance and using Neutron,
1398             # we can be racing against Neutron deleting the port and
1399             # sending the vif-deleted event which then triggers a call to
1400             # detach the interface, so if the interface is not found then
1401             # we can just log it as a warning.
1402             if not interface:
1403                 mac = vif.get('address')
1404                 # The interface is gone so just log it as a warning.
1405                 LOG.warning(_LW('Detaching interface %(mac)s failed because '
1406                                 'the device is no longer found on the guest.'),
1407                             {'mac': mac}, instance=instance)
1408                 return
1409 
1410             state = guest.get_power_state(self._host)
1411             live = state in (power_state.RUNNING, power_state.PAUSED)
1412             guest.detach_device(interface, persistent=True, live=live)
1413         except libvirt.libvirtError as ex:
1414             error_code = ex.get_error_code()
1415             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1416                 LOG.warning(_LW("During detach_interface, "
1417                              "instance disappeared."),
1418                          instance=instance)
1419             else:
1420                 # NOTE(mriedem): When deleting an instance and using Neutron,
1421                 # we can be racing against Neutron deleting the port and
1422                 # sending the vif-deleted event which then triggers a call to
1423                 # detach the interface, so we might have failed because the
1424                 # network device no longer exists. Libvirt will fail with
1425                 # "operation failed: no matching network device was found"
1426                 # which unfortunately does not have a unique error code so we
1427                 # need to look up the interface by config and if it's not found
1428                 # then we can just log it as a warning rather than tracing an
1429                 # error.
1430                 mac = vif.get('address')
1431                 interface = guest.get_interface_by_cfg(cfg)
1432                 if interface:
1433                     LOG.error(_LE('detaching network adapter failed.'),
1434                              instance=instance, exc_info=True)
1435                     raise exception.InterfaceDetachFailed(
1436                             instance_uuid=instance.uuid)
1437 
1438                 # The interface is gone so just log it as a warning.
1439                 LOG.warning(_LW('Detaching interface %(mac)s failed  because '
1440                                 'the device is no longer found on the guest.'),
1441                             {'mac': mac}, instance=instance)
1442 
1443     def _create_snapshot_metadata(self, image_meta, instance,
1444                                   img_fmt, snp_name):
1445         metadata = {'is_public': False,
1446                     'status': 'active',
1447                     'name': snp_name,
1448                     'properties': {
1449                                    'kernel_id': instance.kernel_id,
1450                                    'image_location': 'snapshot',
1451                                    'image_state': 'available',
1452                                    'owner_id': instance.project_id,
1453                                    'ramdisk_id': instance.ramdisk_id,
1454                                    }
1455                     }
1456         if instance.os_type:
1457             metadata['properties']['os_type'] = instance.os_type
1458 
1459         # NOTE(vish): glance forces ami disk format to be ami
1460         if image_meta.disk_format == 'ami':
1461             metadata['disk_format'] = 'ami'
1462         else:
1463             metadata['disk_format'] = img_fmt
1464 
1465         if image_meta.obj_attr_is_set("container_format"):
1466             metadata['container_format'] = image_meta.container_format
1467         else:
1468             metadata['container_format'] = "bare"
1469 
1470         return metadata
1471 
1472     def snapshot(self, context, instance, image_id, update_task_state):
1473         """Create snapshot from a running VM instance.
1474 
1475         This command only works with qemu 0.14+
1476         """
1477         try:
1478             guest = self._host.get_guest(instance)
1479 
1480             # TODO(sahid): We are converting all calls from a
1481             # virDomain object to use nova.virt.libvirt.Guest.
1482             # We should be able to remove virt_dom at the end.
1483             virt_dom = guest._domain
1484         except exception.InstanceNotFound:
1485             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1486 
1487         snapshot = self._image_api.get(context, image_id)
1488 
1489         # source_format is an on-disk format
1490         # source_type is a backend type
1491         disk_path, source_format = libvirt_utils.find_disk(virt_dom)
1492         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1493 
1494         # We won't have source_type for raw or qcow2 disks, because we can't
1495         # determine that from the path. We should have it from the libvirt
1496         # xml, though.
1497         if source_type is None:
1498             source_type = source_format
1499         # For lxc instances we won't have it either from libvirt xml
1500         # (because we just gave libvirt the mounted filesystem), or the path,
1501         # so source_type is still going to be None. In this case,
1502         # root_disk is going to default to CONF.libvirt.images_type
1503         # below, which is still safe.
1504 
1505         image_format = CONF.libvirt.snapshot_image_format or source_type
1506 
1507         # NOTE(bfilippov): save lvm and rbd as raw
1508         if image_format in ('lvm', 'rbd', 'sio'):
1509             image_format = 'raw'
1510 
1511         metadata = self._create_snapshot_metadata(instance.image_meta,
1512                                                   instance,
1513                                                   image_format,
1514                                                   snapshot['name'])
1515 
1516         snapshot_name = uuid.uuid4().hex
1517 
1518         state = guest.get_power_state(self._host)
1519 
1520         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1521         #               cold snapshots. Currently, checking for encryption is
1522         #               redundant because LVM supports only cold snapshots.
1523         #               It is necessary in case this situation changes in the
1524         #               future.
1525         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1526              and source_type not in ('lvm', 'sio')
1527              and not CONF.ephemeral_storage_encryption.enabled
1528              and not CONF.workarounds.disable_libvirt_livesnapshot):
1529             live_snapshot = True
1530             # Abort is an idempotent operation, so make sure any block
1531             # jobs which may have failed are ended. This operation also
1532             # confirms the running instance, as opposed to the system as a
1533             # whole, has a new enough version of the hypervisor (bug 1193146).
1534             try:
1535                 guest.get_block_device(disk_path).abort_job()
1536             except libvirt.libvirtError as ex:
1537                 error_code = ex.get_error_code()
1538                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1539                     live_snapshot = False
1540                 else:
1541                     pass
1542         else:
1543             live_snapshot = False
1544 
1545         # NOTE(rmk): We cannot perform live snapshots when a managedSave
1546         #            file is present, so we will use the cold/legacy method
1547         #            for instances which are shutdown.
1548         if state == power_state.SHUTDOWN:
1549             live_snapshot = False
1550 
1551         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1552                                           instance)
1553 
1554         root_disk = self.image_backend.by_libvirt_path(
1555             instance, disk_path, image_type=source_type)
1556 
1557         if live_snapshot:
1558             LOG.info(_LI("Beginning live snapshot process"),
1559                      instance=instance)
1560         else:
1561             LOG.info(_LI("Beginning cold snapshot process"),
1562                      instance=instance)
1563 
1564         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1565 
1566         try:
1567             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1568                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1569             metadata['location'] = root_disk.direct_snapshot(
1570                 context, snapshot_name, image_format, image_id,
1571                 instance.image_ref)
1572             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1573                                   instance)
1574             self._image_api.update(context, image_id, metadata,
1575                                    purge_props=False)
1576         except (NotImplementedError, exception.ImageUnacceptable,
1577                 exception.Forbidden) as e:
1578             if type(e) != NotImplementedError:
1579                 LOG.warning(_LW('Performing standard snapshot because direct '
1580                                 'snapshot failed: %(error)s'), {'error': e})
1581             failed_snap = metadata.pop('location', None)
1582             if failed_snap:
1583                 failed_snap = {'url': str(failed_snap)}
1584             root_disk.cleanup_direct_snapshot(failed_snap,
1585                                                   also_destroy_volume=True,
1586                                                   ignore_errors=True)
1587             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1588                               expected_state=task_states.IMAGE_UPLOADING)
1589 
1590             # TODO(nic): possibly abstract this out to the root_disk
1591             if source_type == 'rbd' and live_snapshot:
1592                 # Standard snapshot uses qemu-img convert from RBD which is
1593                 # not safe to run with live_snapshot.
1594                 live_snapshot = False
1595                 # Suspend the guest, so this is no longer a live snapshot
1596                 self._prepare_domain_for_snapshot(context, live_snapshot,
1597                                                   state, instance)
1598 
1599             snapshot_directory = CONF.libvirt.snapshots_directory
1600             fileutils.ensure_tree(snapshot_directory)
1601             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1602                 try:
1603                     out_path = os.path.join(tmpdir, snapshot_name)
1604                     if live_snapshot:
1605                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1606                         os.chmod(tmpdir, 0o701)
1607                         self._live_snapshot(context, instance, guest,
1608                                             disk_path, out_path, source_format,
1609                                             image_format, instance.image_meta)
1610                     else:
1611                         root_disk.snapshot_extract(out_path, image_format)
1612                 finally:
1613                     self._snapshot_domain(context, live_snapshot, virt_dom,
1614                                           state, instance)
1615                     LOG.info(_LI("Snapshot extracted, beginning image upload"),
1616                              instance=instance)
1617 
1618                 # Upload that image to the image service
1619                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1620                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1621                 with libvirt_utils.file_open(out_path) as image_file:
1622                     self._image_api.update(context,
1623                                            image_id,
1624                                            metadata,
1625                                            image_file)
1626         except Exception:
1627             with excutils.save_and_reraise_exception():
1628                 LOG.exception(_LE("Failed to snapshot image"))
1629                 failed_snap = metadata.pop('location', None)
1630                 if failed_snap:
1631                     failed_snap = {'url': str(failed_snap)}
1632                 root_disk.cleanup_direct_snapshot(
1633                         failed_snap, also_destroy_volume=True,
1634                         ignore_errors=True)
1635 
1636         LOG.info(_LI("Snapshot image upload complete"), instance=instance)
1637 
1638     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1639                                      instance):
1640         # NOTE(dkang): managedSave does not work for LXC
1641         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1642             if state == power_state.RUNNING or state == power_state.PAUSED:
1643                 self.suspend(context, instance)
1644 
1645     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1646                          instance):
1647         guest = None
1648         # NOTE(dkang): because previous managedSave is not called
1649         #              for LXC, _create_domain must not be called.
1650         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1651             if state == power_state.RUNNING:
1652                 guest = self._create_domain(domain=virt_dom)
1653             elif state == power_state.PAUSED:
1654                 guest = self._create_domain(domain=virt_dom, pause=True)
1655 
1656             if guest is not None:
1657                 self._attach_pci_devices(
1658                     guest, pci_manager.get_instance_pci_devs(instance))
1659                 self._attach_sriov_ports(context, instance, guest)
1660 
1661     def _can_set_admin_password(self, image_meta):
1662 
1663         if CONF.libvirt.virt_type == 'parallels':
1664             if not self._host.has_min_version(
1665                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
1666                 raise exception.SetAdminPasswdNotSupported()
1667         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
1668             if not self._host.has_min_version(
1669                    MIN_LIBVIRT_SET_ADMIN_PASSWD):
1670                 raise exception.SetAdminPasswdNotSupported()
1671             hw_qga = image_meta.properties.get('hw_qemu_guest_agent', '')
1672             if not strutils.bool_from_string(hw_qga):
1673                 raise exception.QemuGuestAgentNotEnabled()
1674         else:
1675             raise exception.SetAdminPasswdNotSupported()
1676 
1677     def set_admin_password(self, instance, new_pass):
1678         self._can_set_admin_password(instance.image_meta)
1679 
1680         guest = self._host.get_guest(instance)
1681         user = instance.image_meta.properties.get("os_admin_user")
1682         if not user:
1683             if instance.os_type == "windows":
1684                 user = "Administrator"
1685             else:
1686                 user = "root"
1687         try:
1688             guest.set_user_password(user, new_pass)
1689         except libvirt.libvirtError as ex:
1690             error_code = ex.get_error_code()
1691             msg = (_('Error from libvirt while set password for username '
1692                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
1693                    % {'user': user, 'error_code': error_code, 'ex': ex})
1694             raise exception.NovaException(msg)
1695 
1696     def _can_quiesce(self, instance, image_meta):
1697         if (CONF.libvirt.virt_type not in ('kvm', 'qemu') or
1698             not self._host.has_min_version(MIN_LIBVIRT_FSFREEZE_VERSION)):
1699             raise exception.InstanceQuiesceNotSupported(
1700                 instance_id=instance.uuid)
1701 
1702         if not image_meta.properties.get('hw_qemu_guest_agent', False):
1703             raise exception.QemuGuestAgentNotEnabled()
1704 
1705     def _set_quiesced(self, context, instance, image_meta, quiesced):
1706         self._can_quiesce(instance, image_meta)
1707         try:
1708             guest = self._host.get_guest(instance)
1709             if quiesced:
1710                 guest.freeze_filesystems()
1711             else:
1712                 guest.thaw_filesystems()
1713         except libvirt.libvirtError as ex:
1714             error_code = ex.get_error_code()
1715             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
1716                      '[Error Code %(error_code)s] %(ex)s')
1717                    % {'instance_name': instance.name,
1718                       'error_code': error_code, 'ex': ex})
1719             raise exception.NovaException(msg)
1720 
1721     def quiesce(self, context, instance, image_meta):
1722         """Freeze the guest filesystems to prepare for snapshot.
1723 
1724         The qemu-guest-agent must be setup to execute fsfreeze.
1725         """
1726         self._set_quiesced(context, instance, image_meta, True)
1727 
1728     def unquiesce(self, context, instance, image_meta):
1729         """Thaw the guest filesystems after snapshot."""
1730         self._set_quiesced(context, instance, image_meta, False)
1731 
1732     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
1733                        source_format, image_format, image_meta):
1734         """Snapshot an instance without downtime."""
1735         dev = guest.get_block_device(disk_path)
1736 
1737         # Save a copy of the domain's persistent XML file
1738         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1739 
1740         # Abort is an idempotent operation, so make sure any block
1741         # jobs which may have failed are ended.
1742         try:
1743             dev.abort_job()
1744         except Exception:
1745             pass
1746 
1747         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
1748         #             in QEMU 1.3. In order to do this, we need to create
1749         #             a destination image with the original backing file
1750         #             and matching size of the instance root disk.
1751         src_disk_size = libvirt_utils.get_disk_size(disk_path,
1752                                                     format=source_format)
1753         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
1754                                                         format=source_format,
1755                                                         basename=False)
1756         disk_delta = out_path + '.delta'
1757         libvirt_utils.create_cow_image(src_back_path, disk_delta,
1758                                        src_disk_size)
1759 
1760         quiesced = False
1761         try:
1762             self._set_quiesced(context, instance, image_meta, True)
1763             quiesced = True
1764         except exception.NovaException as err:
1765             if image_meta.properties.get('os_require_quiesce', False):
1766                 raise
1767             LOG.info(_LI('Skipping quiescing instance: %(reason)s.'),
1768                      {'reason': err}, instance=instance)
1769 
1770         try:
1771             # NOTE (rmk): blockRebase cannot be executed on persistent
1772             #             domains, so we need to temporarily undefine it.
1773             #             If any part of this block fails, the domain is
1774             #             re-defined regardless.
1775             if guest.has_persistent_configuration():
1776                 guest.delete_configuration()
1777 
1778             # NOTE (rmk): Establish a temporary mirror of our root disk and
1779             #             issue an abort once we have a complete copy.
1780             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
1781 
1782             while not dev.is_job_complete():
1783                 time.sleep(0.5)
1784 
1785             dev.abort_job()
1786             libvirt_utils.chown(disk_delta, os.getuid())
1787         finally:
1788             self._host.write_instance_config(xml)
1789             if quiesced:
1790                 self._set_quiesced(context, instance, image_meta, False)
1791 
1792         # Convert the delta (CoW) image with a backing file to a flat
1793         # image with no backing file.
1794         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
1795                                        out_path, image_format)
1796 
1797     def _volume_snapshot_update_status(self, context, snapshot_id, status):
1798         """Send a snapshot status update to Cinder.
1799 
1800         This method captures and logs exceptions that occur
1801         since callers cannot do anything useful with these exceptions.
1802 
1803         Operations on the Cinder side waiting for this will time out if
1804         a failure occurs sending the update.
1805 
1806         :param context: security context
1807         :param snapshot_id: id of snapshot being updated
1808         :param status: new status value
1809 
1810         """
1811 
1812         try:
1813             self._volume_api.update_snapshot_status(context,
1814                                                     snapshot_id,
1815                                                     status)
1816         except Exception:
1817             LOG.exception(_LE('Failed to send updated snapshot status '
1818                               'to volume service.'))
1819 
1820     def _volume_snapshot_create(self, context, instance, guest,
1821                                 volume_id, new_file):
1822         """Perform volume snapshot.
1823 
1824            :param guest: VM that volume is attached to
1825            :param volume_id: volume UUID to snapshot
1826            :param new_file: relative path to new qcow2 file present on share
1827 
1828         """
1829         xml = guest.get_xml_desc()
1830         xml_doc = etree.fromstring(xml)
1831 
1832         device_info = vconfig.LibvirtConfigGuest()
1833         device_info.parse_dom(xml_doc)
1834 
1835         disks_to_snap = []          # to be snapshotted by libvirt
1836         network_disks_to_snap = []  # network disks (netfs, gluster, etc.)
1837         disks_to_skip = []          # local disks not snapshotted
1838 
1839         for guest_disk in device_info.devices:
1840             if (guest_disk.root_name != 'disk'):
1841                 continue
1842 
1843             if (guest_disk.target_dev is None):
1844                 continue
1845 
1846             if (guest_disk.serial is None or guest_disk.serial != volume_id):
1847                 disks_to_skip.append(guest_disk.target_dev)
1848                 continue
1849 
1850             # disk is a Cinder volume with the correct volume_id
1851 
1852             disk_info = {
1853                 'dev': guest_disk.target_dev,
1854                 'serial': guest_disk.serial,
1855                 'current_file': guest_disk.source_path,
1856                 'source_protocol': guest_disk.source_protocol,
1857                 'source_name': guest_disk.source_name,
1858                 'source_hosts': guest_disk.source_hosts,
1859                 'source_ports': guest_disk.source_ports
1860             }
1861 
1862             # Determine path for new_file based on current path
1863             if disk_info['current_file'] is not None:
1864                 current_file = disk_info['current_file']
1865                 new_file_path = os.path.join(os.path.dirname(current_file),
1866                                              new_file)
1867                 disks_to_snap.append((current_file, new_file_path))
1868             elif disk_info['source_protocol'] in ('gluster', 'netfs'):
1869                 network_disks_to_snap.append((disk_info, new_file))
1870 
1871         if not disks_to_snap and not network_disks_to_snap:
1872             msg = _('Found no disk to snapshot.')
1873             raise exception.NovaException(msg)
1874 
1875         snapshot = vconfig.LibvirtConfigGuestSnapshot()
1876 
1877         for current_name, new_filename in disks_to_snap:
1878             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
1879             snap_disk.name = current_name
1880             snap_disk.source_path = new_filename
1881             snap_disk.source_type = 'file'
1882             snap_disk.snapshot = 'external'
1883             snap_disk.driver_name = 'qcow2'
1884 
1885             snapshot.add_disk(snap_disk)
1886 
1887         for disk_info, new_filename in network_disks_to_snap:
1888             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
1889             snap_disk.name = disk_info['dev']
1890             snap_disk.source_type = 'network'
1891             snap_disk.source_protocol = disk_info['source_protocol']
1892             snap_disk.snapshot = 'external'
1893             snap_disk.source_path = new_filename
1894             old_dir = disk_info['source_name'].split('/')[0]
1895             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
1896             snap_disk.source_hosts = disk_info['source_hosts']
1897             snap_disk.source_ports = disk_info['source_ports']
1898 
1899             snapshot.add_disk(snap_disk)
1900 
1901         for dev in disks_to_skip:
1902             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
1903             snap_disk.name = dev
1904             snap_disk.snapshot = 'no'
1905 
1906             snapshot.add_disk(snap_disk)
1907 
1908         snapshot_xml = snapshot.to_xml()
1909         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
1910 
1911         try:
1912             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
1913                            reuse_ext=True, quiesce=True)
1914             return
1915         except libvirt.libvirtError:
1916             LOG.exception(_LE('Unable to create quiesced VM snapshot, '
1917                               'attempting again with quiescing disabled.'),
1918                           instance=instance)
1919 
1920         try:
1921             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
1922                            reuse_ext=True, quiesce=False)
1923         except libvirt.libvirtError:
1924             LOG.exception(_LE('Unable to create VM snapshot, '
1925                               'failing volume_snapshot operation.'),
1926                           instance=instance)
1927 
1928             raise
1929 
1930     def _volume_refresh_connection_info(self, context, instance, volume_id):
1931         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
1932                   context, volume_id, instance.uuid)
1933 
1934         driver_bdm = driver_block_device.convert_volume(bdm)
1935         if driver_bdm:
1936             driver_bdm.refresh_connection_info(context, instance,
1937                                                self._volume_api, self)
1938 
1939     def volume_snapshot_create(self, context, instance, volume_id,
1940                                create_info):
1941         """Create snapshots of a Cinder volume via libvirt.
1942 
1943         :param instance: VM instance object reference
1944         :param volume_id: id of volume being snapshotted
1945         :param create_info: dict of information used to create snapshots
1946                      - snapshot_id : ID of snapshot
1947                      - type : qcow2 / <other>
1948                      - new_file : qcow2 file created by Cinder which
1949                      becomes the VM's active image after
1950                      the snapshot is complete
1951         """
1952 
1953         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
1954                   {'c_info': create_info}, instance=instance)
1955 
1956         try:
1957             guest = self._host.get_guest(instance)
1958         except exception.InstanceNotFound:
1959             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1960 
1961         if create_info['type'] != 'qcow2':
1962             raise exception.NovaException(_('Unknown type: %s') %
1963                                           create_info['type'])
1964 
1965         snapshot_id = create_info.get('snapshot_id', None)
1966         if snapshot_id is None:
1967             raise exception.NovaException(_('snapshot_id required '
1968                                             'in create_info'))
1969 
1970         try:
1971             self._volume_snapshot_create(context, instance, guest,
1972                                          volume_id, create_info['new_file'])
1973         except Exception:
1974             with excutils.save_and_reraise_exception():
1975                 LOG.exception(_LE('Error occurred during '
1976                                   'volume_snapshot_create, '
1977                                   'sending error status to Cinder.'),
1978                               instance=instance)
1979                 self._volume_snapshot_update_status(
1980                     context, snapshot_id, 'error')
1981 
1982         self._volume_snapshot_update_status(
1983             context, snapshot_id, 'creating')
1984 
1985         def _wait_for_snapshot():
1986             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
1987 
1988             if snapshot.get('status') != 'creating':
1989                 self._volume_refresh_connection_info(context, instance,
1990                                                      volume_id)
1991                 raise loopingcall.LoopingCallDone()
1992 
1993         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
1994         timer.start(interval=0.5).wait()
1995 
1996     @staticmethod
1997     def _rebase_with_qemu_img(guest, device, active_disk_object,
1998                               rebase_base):
1999         """Rebase a device tied to a guest using qemu-img.
2000 
2001         :param guest:the Guest which owns the device being rebased
2002         :type guest: nova.virt.libvirt.guest.Guest
2003         :param device: the guest block device to rebase
2004         :type device: nova.virt.libvirt.guest.BlockDevice
2005         :param active_disk_object: the guest block device to rebase
2006         :type active_disk_object: nova.virt.libvirt.config.\
2007                                     LibvirtConfigGuestDisk
2008         :param rebase_base: the new parent in the backing chain
2009         :type rebase_base: None or string
2010         """
2011 
2012         # It's unsure how well qemu-img handles network disks for
2013         # every protocol. So let's be safe.
2014         active_protocol = active_disk_object.source_protocol
2015         if active_protocol is not None:
2016             msg = _("Something went wrong when deleting a volume snapshot: "
2017                     "rebasing a %(protocol)s network disk using qemu-img "
2018                     "has not been fully tested") % {'protocol':
2019                     active_protocol}
2020             LOG.error(msg)
2021             raise exception.NovaException(msg)
2022 
2023         if rebase_base is None:
2024             # If backing_file is specified as "" (the empty string), then
2025             # the image is rebased onto no backing file (i.e. it will exist
2026             # independently of any backing file).
2027             backing_file = ""
2028             qemu_img_extra_arg = []
2029         else:
2030             # If the rebased image is going to have a backing file then
2031             # explicitly set the backing file format to avoid any security
2032             # concerns related to file format auto detection.
2033             backing_file = rebase_base
2034             b_file_fmt = images.qemu_img_info(backing_file).file_format
2035             qemu_img_extra_arg = ['-F', b_file_fmt]
2036 
2037         qemu_img_extra_arg.append(active_disk_object.source_path)
2038         utils.execute("qemu-img", "rebase", "-b", backing_file,
2039                       *qemu_img_extra_arg)
2040 
2041     def _volume_snapshot_delete(self, context, instance, volume_id,
2042                                 snapshot_id, delete_info=None):
2043         """Note:
2044             if file being merged into == active image:
2045                 do a blockRebase (pull) operation
2046             else:
2047                 do a blockCommit operation
2048             Files must be adjacent in snap chain.
2049 
2050         :param instance: instance object reference
2051         :param volume_id: volume UUID
2052         :param snapshot_id: snapshot UUID (unused currently)
2053         :param delete_info: {
2054             'type':              'qcow2',
2055             'file_to_merge':     'a.img',
2056             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2057                                                   active image)
2058           }
2059         """
2060 
2061         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2062                   instance=instance)
2063 
2064         if delete_info['type'] != 'qcow2':
2065             msg = _('Unknown delete_info type %s') % delete_info['type']
2066             raise exception.NovaException(msg)
2067 
2068         try:
2069             guest = self._host.get_guest(instance)
2070         except exception.InstanceNotFound:
2071             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2072 
2073         # Find dev name
2074         my_dev = None
2075         active_disk = None
2076 
2077         xml = guest.get_xml_desc()
2078         xml_doc = etree.fromstring(xml)
2079 
2080         device_info = vconfig.LibvirtConfigGuest()
2081         device_info.parse_dom(xml_doc)
2082 
2083         active_disk_object = None
2084 
2085         for guest_disk in device_info.devices:
2086             if (guest_disk.root_name != 'disk'):
2087                 continue
2088 
2089             if (guest_disk.target_dev is None or guest_disk.serial is None):
2090                 continue
2091 
2092             if guest_disk.serial == volume_id:
2093                 my_dev = guest_disk.target_dev
2094 
2095                 active_disk = guest_disk.source_path
2096                 active_protocol = guest_disk.source_protocol
2097                 active_disk_object = guest_disk
2098                 break
2099 
2100         if my_dev is None or (active_disk is None and active_protocol is None):
2101             msg = _('Disk with id: %s '
2102                     'not found attached to instance.') % volume_id
2103             LOG.debug('Domain XML: %s', xml, instance=instance)
2104             raise exception.NovaException(msg)
2105 
2106         LOG.debug("found device at %s", my_dev, instance=instance)
2107 
2108         def _get_snap_dev(filename, backing_store):
2109             if filename is None:
2110                 msg = _('filename cannot be None')
2111                 raise exception.NovaException(msg)
2112 
2113             # libgfapi delete
2114             LOG.debug("XML: %s", xml)
2115 
2116             LOG.debug("active disk object: %s", active_disk_object)
2117 
2118             # determine reference within backing store for desired image
2119             filename_to_merge = filename
2120             matched_name = None
2121             b = backing_store
2122             index = None
2123 
2124             current_filename = active_disk_object.source_name.split('/')[1]
2125             if current_filename == filename_to_merge:
2126                 return my_dev + '[0]'
2127 
2128             while b is not None:
2129                 source_filename = b.source_name.split('/')[1]
2130                 if source_filename == filename_to_merge:
2131                     LOG.debug('found match: %s', b.source_name)
2132                     matched_name = b.source_name
2133                     index = b.index
2134                     break
2135 
2136                 b = b.backing_store
2137 
2138             if matched_name is None:
2139                 msg = _('no match found for %s') % (filename_to_merge)
2140                 raise exception.NovaException(msg)
2141 
2142             LOG.debug('index of match (%s) is %s', b.source_name, index)
2143 
2144             my_snap_dev = '%s[%s]' % (my_dev, index)
2145             return my_snap_dev
2146 
2147         if delete_info['merge_target_file'] is None:
2148             # pull via blockRebase()
2149 
2150             # Merge the most recent snapshot into the active image
2151 
2152             rebase_disk = my_dev
2153             rebase_base = delete_info['file_to_merge']  # often None
2154             if (active_protocol is not None) and (rebase_base is not None):
2155                 rebase_base = _get_snap_dev(rebase_base,
2156                                             active_disk_object.backing_store)
2157 
2158             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2159             # and when available this flag _must_ be used to ensure backing
2160             # paths are maintained relative by qemu.
2161             #
2162             # If _RELATIVE flag not found, continue with old behaviour
2163             # (relative backing path seems to work for this case)
2164             try:
2165                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2166                 relative = rebase_base is not None
2167             except AttributeError:
2168                 LOG.warning(_LW(
2169                     "Relative blockrebase support was not detected. "
2170                     "Continuing with old behaviour."))
2171                 relative = False
2172 
2173             LOG.debug(
2174                 'disk: %(disk)s, base: %(base)s, '
2175                 'bw: %(bw)s, relative: %(relative)s',
2176                 {'disk': rebase_disk,
2177                  'base': rebase_base,
2178                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2179                  'relative': str(relative)}, instance=instance)
2180 
2181             dev = guest.get_block_device(rebase_disk)
2182             if guest.is_active():
2183                 result = dev.rebase(rebase_base, relative=relative)
2184                 if result == 0:
2185                     LOG.debug('blockRebase started successfully',
2186                               instance=instance)
2187 
2188                 while not dev.is_job_complete():
2189                     LOG.debug('waiting for blockRebase job completion',
2190                               instance=instance)
2191                     time.sleep(0.5)
2192 
2193             # If the guest is not running libvirt won't do a blockRebase.
2194             # In that case, let's ask qemu-img to rebase the disk.
2195             else:
2196                 LOG.debug('Guest is not running so doing a block rebase '
2197                           'using "qemu-img rebase"', instance=instance)
2198                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2199                                            rebase_base)
2200 
2201         else:
2202             # commit with blockCommit()
2203             my_snap_base = None
2204             my_snap_top = None
2205             commit_disk = my_dev
2206 
2207             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2208             # and when available this flag _must_ be used to ensure backing
2209             # paths are maintained relative by qemu.
2210             #
2211             # If _RELATIVE flag not found, raise exception as relative backing
2212             # path may not be maintained and Cinder flow is broken if allowed
2213             # to continue.
2214             try:
2215                 libvirt.VIR_DOMAIN_BLOCK_COMMIT_RELATIVE
2216             except AttributeError:
2217                 ver = '.'.join(
2218                     [str(x) for x in
2219                      MIN_LIBVIRT_BLOCKJOB_RELATIVE_VERSION])
2220                 msg = _("Relative blockcommit support was not detected. "
2221                         "Libvirt '%s' or later is required for online "
2222                         "deletion of file/network storage-backed volume "
2223                         "snapshots.") % ver
2224                 raise exception.Invalid(msg)
2225 
2226             if active_protocol is not None:
2227                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2228                                              active_disk_object.backing_store)
2229                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2230                                             active_disk_object.backing_store)
2231 
2232             commit_base = my_snap_base or delete_info['merge_target_file']
2233             commit_top = my_snap_top or delete_info['file_to_merge']
2234 
2235             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2236                       'commit_base=%(commit_base)s '
2237                       'commit_top=%(commit_top)s ',
2238                       {'commit_disk': commit_disk,
2239                        'commit_base': commit_base,
2240                        'commit_top': commit_top}, instance=instance)
2241 
2242             dev = guest.get_block_device(commit_disk)
2243             result = dev.commit(commit_base, commit_top, relative=True)
2244 
2245             if result == 0:
2246                 LOG.debug('blockCommit started successfully',
2247                           instance=instance)
2248 
2249             while not dev.is_job_complete():
2250                 LOG.debug('waiting for blockCommit job completion',
2251                           instance=instance)
2252                 time.sleep(0.5)
2253 
2254     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2255                                delete_info):
2256         try:
2257             self._volume_snapshot_delete(context, instance, volume_id,
2258                                          snapshot_id, delete_info=delete_info)
2259         except Exception:
2260             with excutils.save_and_reraise_exception():
2261                 LOG.exception(_LE('Error occurred during '
2262                                   'volume_snapshot_delete, '
2263                                   'sending error status to Cinder.'),
2264                               instance=instance)
2265                 self._volume_snapshot_update_status(
2266                     context, snapshot_id, 'error_deleting')
2267 
2268         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2269         self._volume_refresh_connection_info(context, instance, volume_id)
2270 
2271     def reboot(self, context, instance, network_info, reboot_type,
2272                block_device_info=None, bad_volumes_callback=None):
2273         """Reboot a virtual machine, given an instance reference."""
2274         if reboot_type == 'SOFT':
2275             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2276             try:
2277                 soft_reboot_success = self._soft_reboot(instance)
2278             except libvirt.libvirtError as e:
2279                 LOG.debug("Instance soft reboot failed: %s", e,
2280                           instance=instance)
2281                 soft_reboot_success = False
2282 
2283             if soft_reboot_success:
2284                 LOG.info(_LI("Instance soft rebooted successfully."),
2285                          instance=instance)
2286                 return
2287             else:
2288                 LOG.warning(_LW("Failed to soft reboot instance. "
2289                              "Trying hard reboot."),
2290                          instance=instance)
2291         return self._hard_reboot(context, instance, network_info,
2292                                  block_device_info)
2293 
2294     def _soft_reboot(self, instance):
2295         """Attempt to shutdown and restart the instance gracefully.
2296 
2297         We use shutdown and create here so we can return if the guest
2298         responded and actually rebooted. Note that this method only
2299         succeeds if the guest responds to acpi. Therefore we return
2300         success or failure so we can fall back to a hard reboot if
2301         necessary.
2302 
2303         :returns: True if the reboot succeeded
2304         """
2305         guest = self._host.get_guest(instance)
2306 
2307         state = guest.get_power_state(self._host)
2308         old_domid = guest.id
2309         # NOTE(vish): This check allows us to reboot an instance that
2310         #             is already shutdown.
2311         if state == power_state.RUNNING:
2312             guest.shutdown()
2313         # NOTE(vish): This actually could take slightly longer than the
2314         #             FLAG defines depending on how long the get_info
2315         #             call takes to return.
2316         self._prepare_pci_devices_for_use(
2317             pci_manager.get_instance_pci_devs(instance, 'all'))
2318         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2319             guest = self._host.get_guest(instance)
2320 
2321             state = guest.get_power_state(self._host)
2322             new_domid = guest.id
2323 
2324             # NOTE(ivoks): By checking domain IDs, we make sure we are
2325             #              not recreating domain that's already running.
2326             if old_domid != new_domid:
2327                 if state in [power_state.SHUTDOWN,
2328                              power_state.CRASHED]:
2329                     LOG.info(_LI("Instance shutdown successfully."),
2330                              instance=instance)
2331                     self._create_domain(domain=guest._domain)
2332                     timer = loopingcall.FixedIntervalLoopingCall(
2333                         self._wait_for_running, instance)
2334                     timer.start(interval=0.5).wait()
2335                     return True
2336                 else:
2337                     LOG.info(_LI("Instance may have been rebooted during soft "
2338                                  "reboot, so return now."), instance=instance)
2339                     return True
2340             greenthread.sleep(1)
2341         return False
2342 
2343     def _hard_reboot(self, context, instance, network_info,
2344                      block_device_info=None):
2345         """Reboot a virtual machine, given an instance reference.
2346 
2347         Performs a Libvirt reset (if supported) on the domain.
2348 
2349         If Libvirt reset is unavailable this method actually destroys and
2350         re-creates the domain to ensure the reboot happens, as the guest
2351         OS cannot ignore this action.
2352         """
2353 
2354         self._destroy(instance)
2355         # Domain XML will be redefined so we can safely undefine it
2356         # from libvirt. This ensure that such process as create serial
2357         # console for guest will run smoothly.
2358         self._undefine_domain(instance)
2359 
2360         # Convert the system metadata to image metadata
2361         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2362         #                https://bugs.launchpad.net/nova/+bug/1349978
2363         instance_dir = libvirt_utils.get_instance_path(instance)
2364         fileutils.ensure_tree(instance_dir)
2365 
2366         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2367                                             instance,
2368                                             instance.image_meta,
2369                                             block_device_info)
2370         # NOTE(vish): This could generate the wrong device_format if we are
2371         #             using the raw backend and the images don't exist yet.
2372         #             The create_images_and_backing below doesn't properly
2373         #             regenerate raw backend images, however, so when it
2374         #             does we need to (re)generate the xml after the images
2375         #             are in place.
2376         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2377                                   instance.image_meta,
2378                                   block_device_info=block_device_info)
2379 
2380         # NOTE(mdbooth): context.auth_token will not be set when we call
2381         #                _hard_reboot from resume_state_on_host_boot()
2382         if context.auth_token is not None:
2383             # NOTE (rmk): Re-populate any missing backing files.
2384             backing_disk_info = self._get_instance_disk_info(instance.name,
2385                                                              xml,
2386                                                              block_device_info)
2387             self._create_images_and_backing(context, instance, instance_dir,
2388                                             backing_disk_info)
2389 
2390         # Initialize all the necessary networking, block devices and
2391         # start the instance.
2392         self._create_domain_and_network(context, xml, instance, network_info,
2393                                         disk_info,
2394                                         block_device_info=block_device_info,
2395                                         reboot=True,
2396                                         vifs_already_plugged=True)
2397         self._prepare_pci_devices_for_use(
2398             pci_manager.get_instance_pci_devs(instance, 'all'))
2399 
2400         def _wait_for_reboot():
2401             """Called at an interval until the VM is running again."""
2402             state = self.get_info(instance).state
2403 
2404             if state == power_state.RUNNING:
2405                 LOG.info(_LI("Instance rebooted successfully."),
2406                          instance=instance)
2407                 raise loopingcall.LoopingCallDone()
2408 
2409         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2410         timer.start(interval=0.5).wait()
2411 
2412     def pause(self, instance):
2413         """Pause VM instance."""
2414         self._host.get_guest(instance).pause()
2415 
2416     def unpause(self, instance):
2417         """Unpause paused VM instance."""
2418         guest = self._host.get_guest(instance)
2419         guest.resume()
2420         guest.sync_guest_time()
2421 
2422     def _clean_shutdown(self, instance, timeout, retry_interval):
2423         """Attempt to shutdown the instance gracefully.
2424 
2425         :param instance: The instance to be shutdown
2426         :param timeout: How long to wait in seconds for the instance to
2427                         shutdown
2428         :param retry_interval: How often in seconds to signal the instance
2429                                to shutdown while waiting
2430 
2431         :returns: True if the shutdown succeeded
2432         """
2433 
2434         # List of states that represent a shutdown instance
2435         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2436                            power_state.CRASHED]
2437 
2438         try:
2439             guest = self._host.get_guest(instance)
2440         except exception.InstanceNotFound:
2441             # If the instance has gone then we don't need to
2442             # wait for it to shutdown
2443             return True
2444 
2445         state = guest.get_power_state(self._host)
2446         if state in SHUTDOWN_STATES:
2447             LOG.info(_LI("Instance already shutdown."),
2448                      instance=instance)
2449             return True
2450 
2451         LOG.debug("Shutting down instance from state %s", state,
2452                   instance=instance)
2453         guest.shutdown()
2454         retry_countdown = retry_interval
2455 
2456         for sec in six.moves.range(timeout):
2457 
2458             guest = self._host.get_guest(instance)
2459             state = guest.get_power_state(self._host)
2460 
2461             if state in SHUTDOWN_STATES:
2462                 LOG.info(_LI("Instance shutdown successfully after %d "
2463                               "seconds."), sec, instance=instance)
2464                 return True
2465 
2466             # Note(PhilD): We can't assume that the Guest was able to process
2467             #              any previous shutdown signal (for example it may
2468             #              have still been startingup, so within the overall
2469             #              timeout we re-trigger the shutdown every
2470             #              retry_interval
2471             if retry_countdown == 0:
2472                 retry_countdown = retry_interval
2473                 # Instance could shutdown at any time, in which case we
2474                 # will get an exception when we call shutdown
2475                 try:
2476                     LOG.debug("Instance in state %s after %d seconds - "
2477                               "resending shutdown", state, sec,
2478                               instance=instance)
2479                     guest.shutdown()
2480                 except libvirt.libvirtError:
2481                     # Assume this is because its now shutdown, so loop
2482                     # one more time to clean up.
2483                     LOG.debug("Ignoring libvirt exception from shutdown "
2484                               "request.", instance=instance)
2485                     continue
2486             else:
2487                 retry_countdown -= 1
2488 
2489             time.sleep(1)
2490 
2491         LOG.info(_LI("Instance failed to shutdown in %d seconds."),
2492                  timeout, instance=instance)
2493         return False
2494 
2495     def power_off(self, instance, timeout=0, retry_interval=0):
2496         """Power off the specified instance."""
2497         if timeout:
2498             self._clean_shutdown(instance, timeout, retry_interval)
2499         self._destroy(instance)
2500 
2501     def power_on(self, context, instance, network_info,
2502                  block_device_info=None):
2503         """Power on the specified instance."""
2504         # We use _hard_reboot here to ensure that all backing files,
2505         # network, and block device connections, etc. are established
2506         # and available before we attempt to start the instance.
2507         self._hard_reboot(context, instance, network_info, block_device_info)
2508 
2509     def trigger_crash_dump(self, instance):
2510 
2511         """Trigger crash dump by injecting an NMI to the specified instance."""
2512         try:
2513             self._host.get_guest(instance).inject_nmi()
2514         except libvirt.libvirtError as ex:
2515             error_code = ex.get_error_code()
2516 
2517             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2518                 raise exception.TriggerCrashDumpNotSupported()
2519             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2520                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2521 
2522             LOG.exception(_LE('Error from libvirt while injecting an NMI to '
2523                               '%(instance_uuid)s: '
2524                               '[Error Code %(error_code)s] %(ex)s'),
2525                           {'instance_uuid': instance.uuid,
2526                            'error_code': error_code, 'ex': ex})
2527             raise
2528 
2529     def suspend(self, context, instance):
2530         """Suspend the specified instance."""
2531         guest = self._host.get_guest(instance)
2532 
2533         self._detach_pci_devices(guest,
2534             pci_manager.get_instance_pci_devs(instance))
2535         self._detach_sriov_ports(context, instance, guest)
2536         guest.save_memory_state()
2537 
2538     def resume(self, context, instance, network_info, block_device_info=None):
2539         """resume the specified instance."""
2540         disk_info = blockinfo.get_disk_info(
2541                 CONF.libvirt.virt_type, instance, instance.image_meta,
2542                 block_device_info=block_device_info)
2543 
2544         xml = self._get_existing_domain_xml(instance, network_info,
2545                                             block_device_info)
2546         guest = self._create_domain_and_network(context, xml, instance,
2547                            network_info, disk_info,
2548                            block_device_info=block_device_info,
2549                            vifs_already_plugged=True)
2550         self._attach_pci_devices(guest,
2551             pci_manager.get_instance_pci_devs(instance))
2552         self._attach_sriov_ports(context, instance, guest, network_info)
2553         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2554                                                      instance)
2555         timer.start(interval=0.5).wait()
2556         guest.sync_guest_time()
2557 
2558     def resume_state_on_host_boot(self, context, instance, network_info,
2559                                   block_device_info=None):
2560         """resume guest state when a host is booted."""
2561         # Check if the instance is running already and avoid doing
2562         # anything if it is.
2563         try:
2564             guest = self._host.get_guest(instance)
2565             state = guest.get_power_state(self._host)
2566 
2567             ignored_states = (power_state.RUNNING,
2568                               power_state.SUSPENDED,
2569                               power_state.NOSTATE,
2570                               power_state.PAUSED)
2571 
2572             if state in ignored_states:
2573                 return
2574         except exception.NovaException:
2575             pass
2576 
2577         # Instance is not up and could be in an unknown state.
2578         # Be as absolute as possible about getting it back into
2579         # a known and running state.
2580         self._hard_reboot(context, instance, network_info, block_device_info)
2581 
2582     def rescue(self, context, instance, network_info, image_meta,
2583                rescue_password):
2584         """Loads a VM using rescue images.
2585 
2586         A rescue is normally performed when something goes wrong with the
2587         primary images and data needs to be corrected/recovered. Rescuing
2588         should not edit or over-ride the original image, only allow for
2589         data recovery.
2590 
2591         """
2592         instance_dir = libvirt_utils.get_instance_path(instance)
2593         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2594         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2595         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2596 
2597         rescue_image_id = None
2598         if image_meta.obj_attr_is_set("id"):
2599             rescue_image_id = image_meta.id
2600 
2601         rescue_images = {
2602             'image_id': (rescue_image_id or
2603                         CONF.libvirt.rescue_image_id or instance.image_ref),
2604             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2605                           instance.kernel_id),
2606             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2607                            instance.ramdisk_id),
2608         }
2609         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2610                                             instance,
2611                                             image_meta,
2612                                             rescue=True)
2613         injection_info = InjectionInfo(network_info=network_info,
2614                                        admin_pass=rescue_password,
2615                                        files=None)
2616         gen_confdrive = functools.partial(self._create_configdrive,
2617                                           context, instance, injection_info,
2618                                           rescue=True)
2619         self._create_image(context, instance, disk_info['mapping'],
2620                            injection_info=injection_info, suffix='.rescue',
2621                            disk_images=rescue_images)
2622         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2623                                   image_meta, rescue=rescue_images)
2624         self._destroy(instance)
2625         self._create_domain(xml, post_xml_callback=gen_confdrive)
2626 
2627     def unrescue(self, instance, network_info):
2628         """Reboot the VM which is being rescued back into primary images.
2629         """
2630         instance_dir = libvirt_utils.get_instance_path(instance)
2631         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2632         xml = libvirt_utils.load_file(unrescue_xml_path)
2633         guest = self._host.get_guest(instance)
2634 
2635         # TODO(sahid): We are converting all calls from a
2636         # virDomain object to use nova.virt.libvirt.Guest.
2637         # We should be able to remove virt_dom at the end.
2638         virt_dom = guest._domain
2639         self._destroy(instance)
2640         self._create_domain(xml, virt_dom)
2641         libvirt_utils.file_delete(unrescue_xml_path)
2642         rescue_files = os.path.join(instance_dir, "*.rescue")
2643         for rescue_file in glob.iglob(rescue_files):
2644             if os.path.isdir(rescue_file):
2645                 shutil.rmtree(rescue_file)
2646             else:
2647                 libvirt_utils.file_delete(rescue_file)
2648         # cleanup rescue volume
2649         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2650                                 if lvmdisk.endswith('.rescue')])
2651         if CONF.libvirt.images_type == 'rbd':
2652             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
2653                                       disk.endswith('.rescue'))
2654             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
2655         if CONF.libvirt.images_type == 'sio':
2656             LibvirtDriver._get_sio_driver().cleanup_rescue_volumes(instance)
2657 
2658     def poll_rebooting_instances(self, timeout, instances):
2659         pass
2660 
2661     # NOTE(ilyaalekseyev): Implementation like in multinics
2662     # for xenapi(tr3buchet)
2663     def spawn(self, context, instance, image_meta, injected_files,
2664               admin_password, network_info=None, block_device_info=None):
2665         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2666                                             instance,
2667                                             image_meta,
2668                                             block_device_info)
2669         injection_info = InjectionInfo(network_info=network_info,
2670                                        files=injected_files,
2671                                        admin_pass=admin_password)
2672         gen_confdrive = functools.partial(self._create_configdrive,
2673                                           context, instance,
2674                                           injection_info)
2675         self._create_image(context, instance, disk_info['mapping'],
2676                            injection_info=injection_info,
2677                            block_device_info=block_device_info)
2678 
2679         # Required by Quobyte CI
2680         self._ensure_console_log_for_instance(instance)
2681 
2682         xml = self._get_guest_xml(context, instance, network_info,
2683                                   disk_info, image_meta,
2684                                   block_device_info=block_device_info)
2685         self._create_domain_and_network(
2686             context, xml, instance, network_info, disk_info,
2687             block_device_info=block_device_info,
2688             post_xml_callback=gen_confdrive)
2689         LOG.debug("Instance is running", instance=instance)
2690 
2691         def _wait_for_boot():
2692             """Called at an interval until the VM is running."""
2693             state = self.get_info(instance).state
2694 
2695             if state == power_state.RUNNING:
2696                 LOG.info(_LI("Instance spawned successfully."),
2697                          instance=instance)
2698                 raise loopingcall.LoopingCallDone()
2699 
2700         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
2701         timer.start(interval=0.5).wait()
2702 
2703     def _flush_libvirt_console(self, pty):
2704         out, err = utils.execute('dd',
2705                                  'if=%s' % pty,
2706                                  'iflag=nonblock',
2707                                  run_as_root=True,
2708                                  check_exit_code=False)
2709         return out
2710 
2711     def _append_to_file(self, data, fpath):
2712         LOG.info(_LI('data: %(data)r, fpath: %(fpath)r'),
2713                  {'data': data, 'fpath': fpath})
2714         with open(fpath, 'a+') as fp:
2715             fp.write(data)
2716 
2717         return fpath
2718 
2719     def _get_console_output_file(self, instance, console_log):
2720         bytes_to_read = MAX_CONSOLE_BYTES
2721         log_data = ""  # The last N read bytes
2722         i = 0  # in case there is a log rotation (like "virtlogd")
2723         path = console_log
2724         while bytes_to_read > 0 and os.path.exists(path):
2725             libvirt_utils.chown(path, os.getuid())
2726             with libvirt_utils.file_open(path, 'rb') as fp:
2727                 read_log_data, remaining = utils.last_bytes(fp, bytes_to_read)
2728                 # We need the log file content in chronological order,
2729                 # that's why we *prepend* the log data.
2730                 log_data = read_log_data + log_data
2731                 bytes_to_read -= len(read_log_data)
2732                 path = console_log + "." + str(i)
2733                 i += 1
2734             if remaining > 0:
2735                 LOG.info(_LI('Truncated console log returned, '
2736                              '%d bytes ignored'), remaining,
2737                          instance=instance)
2738         return log_data
2739 
2740     def get_console_output(self, context, instance):
2741         guest = self._host.get_guest(instance)
2742 
2743         xml = guest.get_xml_desc()
2744         tree = etree.fromstring(xml)
2745 
2746         # If the guest has a console logging to a file prefer to use that
2747         file_consoles = tree.findall("./devices/console[@type='file']")
2748         if file_consoles:
2749             for file_console in file_consoles:
2750                 source_node = file_console.find('./source')
2751                 if source_node is None:
2752                     continue
2753                 path = source_node.get("path")
2754                 if not path:
2755                     continue
2756 
2757                 if not os.path.exists(path):
2758                     LOG.info(_LI('Instance is configured with a file console, '
2759                                  'but the backing file is not (yet?) present'),
2760                              instance=instance)
2761                     return ""
2762 
2763                 return self._get_console_output_file(instance, path)
2764 
2765         # Try 'pty' types
2766         pty_consoles = tree.findall("./devices/console[@type='pty']")
2767         if pty_consoles:
2768             for pty_console in pty_consoles:
2769                 source_node = pty_console.find('./source')
2770                 if source_node is None:
2771                     continue
2772                 pty = source_node.get("path")
2773                 if not pty:
2774                     continue
2775                 break
2776         else:
2777             raise exception.ConsoleNotAvailable()
2778 
2779         console_log = self._get_console_log_path(instance)
2780         # By default libvirt chowns the console log when it starts a domain.
2781         # We need to chown it back before attempting to read from or write
2782         # to it.
2783         if os.path.exists(console_log):
2784             libvirt_utils.chown(console_log, os.getuid())
2785 
2786         data = self._flush_libvirt_console(pty)
2787         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
2788         # which create a dedicated file device for the console logging.
2789         # Other virt_types like xen, lxc, uml, parallels depend on the
2790         # flush of that pty device into the "console.log" file to ensure
2791         # that a series of "get_console_output" calls return the complete
2792         # content even after rebooting a guest.
2793         fpath = self._append_to_file(data, console_log)
2794 
2795         return self._get_console_output_file(instance, fpath)
2796 
2797     def get_host_ip_addr(self):
2798         ips = compute_utils.get_machine_ips()
2799         if CONF.my_ip not in ips:
2800             LOG.warning(_LW('my_ip address (%(my_ip)s) was not found on '
2801                          'any of the interfaces: %(ifaces)s'),
2802                      {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
2803         return CONF.my_ip
2804 
2805     def get_vnc_console(self, context, instance):
2806         def get_vnc_port_for_instance(instance_name):
2807             guest = self._host.get_guest(instance)
2808 
2809             xml = guest.get_xml_desc()
2810             xml_dom = etree.fromstring(xml)
2811 
2812             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
2813             if graphic is not None:
2814                 return graphic.get('port')
2815             # NOTE(rmk): We had VNC consoles enabled but the instance in
2816             # question is not actually listening for connections.
2817             raise exception.ConsoleTypeUnavailable(console_type='vnc')
2818 
2819         port = get_vnc_port_for_instance(instance.name)
2820         host = CONF.vnc.vncserver_proxyclient_address
2821 
2822         return ctype.ConsoleVNC(host=host, port=port)
2823 
2824     def get_spice_console(self, context, instance):
2825         def get_spice_ports_for_instance(instance_name):
2826             guest = self._host.get_guest(instance)
2827 
2828             xml = guest.get_xml_desc()
2829             xml_dom = etree.fromstring(xml)
2830 
2831             graphic = xml_dom.find("./devices/graphics[@type='spice']")
2832             if graphic is not None:
2833                 return (graphic.get('port'), graphic.get('tlsPort'))
2834             # NOTE(rmk): We had Spice consoles enabled but the instance in
2835             # question is not actually listening for connections.
2836             raise exception.ConsoleTypeUnavailable(console_type='spice')
2837 
2838         ports = get_spice_ports_for_instance(instance.name)
2839         host = CONF.spice.server_proxyclient_address
2840 
2841         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
2842 
2843     def get_serial_console(self, context, instance):
2844         guest = self._host.get_guest(instance)
2845         for hostname, port in self._get_serial_ports_from_guest(
2846                 guest, mode='bind'):
2847             return ctype.ConsoleSerial(host=hostname, port=port)
2848         raise exception.ConsoleTypeUnavailable(console_type='serial')
2849 
2850     @staticmethod
2851     def _supports_direct_io(dirpath):
2852 
2853         if not hasattr(os, 'O_DIRECT'):
2854             LOG.debug("This python runtime does not support direct I/O")
2855             return False
2856 
2857         testfile = os.path.join(dirpath, ".directio.test")
2858 
2859         hasDirectIO = True
2860         fd = None
2861         try:
2862             fd = os.open(testfile, os.O_CREAT | os.O_WRONLY | os.O_DIRECT)
2863             # Check is the write allowed with 512 byte alignment
2864             align_size = 512
2865             m = mmap.mmap(-1, align_size)
2866             m.write(b"x" * align_size)
2867             os.write(fd, m)
2868             LOG.debug("Path '%(path)s' supports direct I/O",
2869                       {'path': dirpath})
2870         except OSError as e:
2871             if e.errno == errno.EINVAL:
2872                 LOG.debug("Path '%(path)s' does not support direct I/O: "
2873                           "'%(ex)s'", {'path': dirpath, 'ex': e})
2874                 hasDirectIO = False
2875             else:
2876                 with excutils.save_and_reraise_exception():
2877                     LOG.error(_LE("Error on '%(path)s' while checking "
2878                                   "direct I/O: '%(ex)s'"),
2879                               {'path': dirpath, 'ex': e})
2880         except Exception as e:
2881             with excutils.save_and_reraise_exception():
2882                 LOG.error(_LE("Error on '%(path)s' while checking direct I/O: "
2883                               "'%(ex)s'"), {'path': dirpath, 'ex': e})
2884         finally:
2885             # ensure unlink(filepath) will actually remove the file by deleting
2886             # the remaining link to it in close(fd)
2887             if fd is not None:
2888                 os.close(fd)
2889 
2890             try:
2891                 os.unlink(testfile)
2892             except Exception:
2893                 pass
2894 
2895         return hasDirectIO
2896 
2897     @staticmethod
2898     def _create_ephemeral(target, ephemeral_size,
2899                           fs_label, os_type, is_block_dev=False,
2900                           context=None, specified_fs=None):
2901         if not is_block_dev:
2902             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
2903 
2904         # Run as root only for block devices.
2905         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
2906                       specified_fs=specified_fs)
2907 
2908     @staticmethod
2909     def _create_swap(target, swap_mb, is_block_dev=False, context=None):
2910         """Create a swap file of specified size."""
2911         if not is_block_dev:
2912             libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
2913         utils.mkfs('swap', target, run_as_root=is_block_dev)
2914 
2915     @staticmethod
2916     def _get_console_log_path(instance):
2917         return os.path.join(libvirt_utils.get_instance_path(instance),
2918                             'console.log')
2919 
2920     def _ensure_console_log_for_instance(self, instance):
2921         # NOTE(mdbooth): Although libvirt will create this file for us
2922         # automatically when it starts, it will initially create it with
2923         # root ownership and then chown it depending on the configuration of
2924         # the domain it is launching. Quobyte CI explicitly disables the
2925         # chown by setting dynamic_ownership=0 in libvirt's config.
2926         # Consequently when the domain starts it is unable to write to its
2927         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
2928         #
2929         # To work around this, we create the file manually before starting
2930         # the domain so it has the same ownership as Nova. This works
2931         # for Quobyte CI because it is also configured to run qemu as the same
2932         # user as the Nova service. Installations which don't set
2933         # dynamic_ownership=0 are not affected because libvirt will always
2934         # correctly configure permissions regardless of initial ownership.
2935         #
2936         # Setting dynamic_ownership=0 is dubious and potentially broken in
2937         # more ways than console.log (see comment #22 on the above bug), so
2938         # Future Maintainer who finds this code problematic should check to see
2939         # if we still support it.
2940         console_file = self._get_console_log_path(instance)
2941         LOG.debug('Ensure instance console log exists: %s', console_file,
2942                   instance=instance)
2943         libvirt_utils.file_open(console_file, 'a').close()
2944 
2945     @staticmethod
2946     def _get_disk_config_image_type():
2947         # TODO(mikal): there is a bug here if images_type has
2948         # changed since creation of the instance, but I am pretty
2949         # sure that this bug already exists.
2950         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
2951 
2952     @staticmethod
2953     def _is_booted_from_volume(block_device_info):
2954         """Determines whether the VM is booting from volume
2955 
2956         Determines whether the block device info indicates that the VM
2957         is booting from a volume.
2958         """
2959         block_device_mapping = driver.block_device_info_get_mapping(
2960             block_device_info)
2961         return bool(block_device.get_root_bdm(block_device_mapping))
2962 
2963     def _inject_data(self, disk, instance, injection_info):
2964         """Injects data in a disk image
2965 
2966         Helper used for injecting data in a disk image file system.
2967 
2968         :param disk: The disk we're injecting into (an Image object)
2969         :param instance: The instance we're injecting into
2970         :param injection_info: Injection info
2971         """
2972         # Handles the partition need to be used.
2973         LOG.debug('Checking root disk injection %(info)s',
2974                   info=str(injection_info), instance=instance)
2975         target_partition = None
2976         if not instance.kernel_id:
2977             target_partition = CONF.libvirt.inject_partition
2978             if target_partition == 0:
2979                 target_partition = None
2980         if CONF.libvirt.virt_type == 'lxc':
2981             target_partition = None
2982 
2983         # Handles the key injection.
2984         if CONF.libvirt.inject_key and instance.get('key_data'):
2985             key = str(instance.key_data)
2986         else:
2987             key = None
2988 
2989         # Handles the admin password injection.
2990         if not CONF.libvirt.inject_password:
2991             admin_pass = None
2992         else:
2993             admin_pass = injection_info.admin_pass
2994 
2995         # Handles the network injection.
2996         net = netutils.get_injected_network_template(
2997             injection_info.network_info,
2998             libvirt_virt_type=CONF.libvirt.virt_type)
2999 
3000         # Handles the metadata injection
3001         metadata = instance.get('metadata')
3002 
3003         if any((key, net, metadata, admin_pass, injection_info.files)):
3004             LOG.debug('Injecting %(info)s', info=str(injection_info),
3005                       instance=instance)
3006             img_id = instance.image_ref
3007             try:
3008                 disk_api.inject_data(disk.get_model(self._conn),
3009                                      key, net, metadata, admin_pass,
3010                                      injection_info.files,
3011                                      partition=target_partition,
3012                                      mandatory=('files',))
3013             except Exception as e:
3014                 with excutils.save_and_reraise_exception():
3015                     LOG.error(_LE('Error injecting data into image '
3016                                   '%(img_id)s (%(e)s)'),
3017                               {'img_id': img_id, 'e': e},
3018                               instance=instance)
3019 
3020     # NOTE(sileht): many callers of this method assume that this
3021     # method doesn't fail if an image already exists but instead
3022     # think that it will be reused (ie: (live)-migration/resize)
3023     def _create_image(self, context, instance,
3024                       disk_mapping, injection_info=None, suffix='',
3025                       disk_images=None, block_device_info=None,
3026                       fallback_from_host=None,
3027                       ignore_bdi_for_swap=False):
3028         booted_from_volume = self._is_booted_from_volume(block_device_info)
3029 
3030         def image(fname, image_type=CONF.libvirt.images_type):
3031             return self.image_backend.by_name(instance,
3032                                               fname + suffix, image_type)
3033 
3034         def raw(fname):
3035             return image(fname, image_type='raw')
3036 
3037         # ensure directories exist and are writable
3038         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3039 
3040         LOG.info(_LI('Creating image'), instance=instance)
3041 
3042         if not disk_images:
3043             disk_images = {'image_id': instance.image_ref,
3044                            'kernel_id': instance.kernel_id,
3045                            'ramdisk_id': instance.ramdisk_id}
3046 
3047         if disk_images['kernel_id']:
3048             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3049             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3050                                 context=context,
3051                                 filename=fname,
3052                                 image_id=disk_images['kernel_id'])
3053             if disk_images['ramdisk_id']:
3054                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3055                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3056                                      context=context,
3057                                      filename=fname,
3058                                      image_id=disk_images['ramdisk_id'])
3059 
3060         inst_type = instance.get_flavor()
3061         if CONF.libvirt.virt_type == 'uml':
3062             libvirt_utils.chown(image('disk').path, 'root')
3063 
3064         self._create_and_inject_local_root(context, instance,
3065                                            booted_from_volume, suffix,
3066                                            disk_images, injection_info,
3067                                            fallback_from_host)
3068 
3069         # Lookup the filesystem type if required
3070         os_type_with_default = disk_api.get_fs_type_for_os_type(
3071             instance.os_type)
3072         # Generate a file extension based on the file system
3073         # type and the mkfs commands configured if any
3074         file_extension = disk_api.get_file_extension_for_os_type(
3075                                                           os_type_with_default)
3076 
3077         ephemeral_gb = instance.flavor.ephemeral_gb
3078         if 'disk.local' in disk_mapping:
3079             disk_image = image('disk.local')
3080             fn = functools.partial(self._create_ephemeral,
3081                                    fs_label='ephemeral0',
3082                                    os_type=instance.os_type,
3083                                    is_block_dev=disk_image.is_block_dev)
3084             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3085             size = ephemeral_gb * units.Gi
3086             disk_image.cache(fetch_func=fn,
3087                              context=context,
3088                              filename=fname,
3089                              size=size,
3090                              ephemeral_size=ephemeral_gb)
3091 
3092         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3093                 block_device_info)):
3094             disk_image = image(blockinfo.get_eph_disk(idx))
3095 
3096             specified_fs = eph.get('guest_format')
3097             if specified_fs and not self.is_supported_fs_format(specified_fs):
3098                 msg = _("%s format is not supported") % specified_fs
3099                 raise exception.InvalidBDMFormat(details=msg)
3100 
3101             fn = functools.partial(self._create_ephemeral,
3102                                    fs_label='ephemeral%d' % idx,
3103                                    os_type=instance.os_type,
3104                                    is_block_dev=disk_image.is_block_dev)
3105             size = eph['size'] * units.Gi
3106             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3107             disk_image.cache(fetch_func=fn,
3108                              context=context,
3109                              filename=fname,
3110                              size=size,
3111                              ephemeral_size=eph['size'],
3112                              specified_fs=specified_fs)
3113 
3114         if 'disk.swap' in disk_mapping:
3115             mapping = disk_mapping['disk.swap']
3116             swap_mb = 0
3117 
3118             if ignore_bdi_for_swap:
3119                 # This is a workaround to support legacy swap resizing,
3120                 # which does not touch swap size specified in bdm,
3121                 # but works with flavor specified size only.
3122                 # In this case we follow the legacy logic and ignore block
3123                 # device info completely.
3124                 # NOTE(ft): This workaround must be removed when a correct
3125                 # implementation of resize operation changing sizes in bdms is
3126                 # developed. Also at that stage we probably may get rid of
3127                 # the direct usage of flavor swap size here,
3128                 # leaving the work with bdm only.
3129                 swap_mb = inst_type['swap']
3130             else:
3131                 swap = driver.block_device_info_get_swap(block_device_info)
3132                 if driver.swap_is_usable(swap):
3133                     swap_mb = swap['swap_size']
3134                 elif (inst_type['swap'] > 0 and
3135                       not block_device.volume_in_mapping(
3136                         mapping['dev'], block_device_info)):
3137                     swap_mb = inst_type['swap']
3138 
3139             if swap_mb > 0:
3140                 size = swap_mb * units.Mi
3141                 image('disk.swap').cache(fetch_func=self._create_swap,
3142                                          context=context,
3143                                          filename="swap_%s" % swap_mb,
3144                                          size=size,
3145                                          swap_mb=swap_mb)
3146 
3147     def _create_and_inject_local_root(self, context, instance,
3148                                       booted_from_volume, suffix, disk_images,
3149                                       injection_info, fallback_from_host):
3150         # File injection only if needed
3151         need_inject = (not configdrive.required_by(instance) and
3152                        injection_info is not None and
3153                        CONF.libvirt.inject_partition != -2)
3154 
3155         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3156         # currently happens only on rescue - we still don't want to
3157         # create a base image.
3158         if not booted_from_volume:
3159             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3160             size = instance.flavor.root_gb * units.Gi
3161 
3162             if size == 0 or suffix == '.rescue':
3163                 size = None
3164 
3165             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3166                                                  CONF.libvirt.images_type)
3167             if instance.task_state == task_states.RESIZE_FINISH:
3168                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3169             if backend.SUPPORTS_CLONE:
3170                 def clone_fallback_to_fetch(*args, **kwargs):
3171                     try:
3172                         backend.clone(context, disk_images['image_id'])
3173                     except exception.ImageUnacceptable:
3174                         libvirt_utils.fetch_image(*args, **kwargs)
3175                 fetch_func = clone_fallback_to_fetch
3176             else:
3177                 fetch_func = libvirt_utils.fetch_image
3178             self._try_fetch_image_cache(backend, fetch_func, context,
3179                                         root_fname, disk_images['image_id'],
3180                                         instance, size, fallback_from_host)
3181 
3182             if need_inject:
3183                 self._inject_data(backend, instance, injection_info)
3184 
3185         elif need_inject:
3186             LOG.warning(_LW('File injection into a boot from volume '
3187                             'instance is not supported'), instance=instance)
3188 
3189     def _create_configdrive(self, context, instance, injection_info,
3190                             rescue=False):
3191         # As this method being called right after the definition of a
3192         # domain, but before its actual launch, device metadata will be built
3193         # and saved in the instance for it to be used by the config drive and
3194         # the metadata service.
3195         instance.device_metadata = self._build_device_metadata(context,
3196                                                                instance)
3197         if configdrive.required_by(instance):
3198             LOG.info(_LI('Using config drive'), instance=instance)
3199 
3200             name = 'disk.config'
3201             if rescue:
3202                 name += '.rescue'
3203 
3204             config_disk = self.image_backend.by_name(
3205                 instance, name, self._get_disk_config_image_type())
3206 
3207             # Don't overwrite an existing config drive
3208             if not config_disk.exists():
3209                 extra_md = {}
3210                 if injection_info.admin_pass:
3211                     extra_md['admin_pass'] = injection_info.admin_pass
3212 
3213                 inst_md = instance_metadata.InstanceMetadata(
3214                     instance, content=injection_info.files, extra_md=extra_md,
3215                     network_info=injection_info.network_info,
3216                     request_context=context)
3217 
3218                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3219                 with cdb:
3220                     # NOTE(mdbooth): We're hardcoding here the path of the
3221                     # config disk when using the flat backend. This isn't
3222                     # good, but it's required because we need a local path we
3223                     # know we can write to in case we're subsequently
3224                     # importing into rbd. This will be cleaned up when we
3225                     # replace this with a call to create_from_func, but that
3226                     # can't happen until we've updated the backends and we
3227                     # teach them not to cache config disks. This isn't
3228                     # possible while we're still using cache() under the hood.
3229                     config_disk_local_path = os.path.join(
3230                         libvirt_utils.get_instance_path(instance), name)
3231                     LOG.info(_LI('Creating config drive at %(path)s'),
3232                              {'path': config_disk_local_path},
3233                              instance=instance)
3234 
3235                     try:
3236                         cdb.make_drive(config_disk_local_path)
3237                     except processutils.ProcessExecutionError as e:
3238                         with excutils.save_and_reraise_exception():
3239                             LOG.error(_LE('Creating config drive failed '
3240                                           'with error: %s'),
3241                                       e, instance=instance)
3242 
3243                 try:
3244                     config_disk.import_file(
3245                         instance, config_disk_local_path, name)
3246                 finally:
3247                     # NOTE(mikal): if the config drive was imported into RBD,
3248                     # then we no longer need the local copy
3249                     if CONF.libvirt.images_type == 'rbd':
3250                         os.unlink(config_disk_local_path)
3251 
3252     def _prepare_pci_devices_for_use(self, pci_devices):
3253         # kvm , qemu support managed mode
3254         # In managed mode, the configured device will be automatically
3255         # detached from the host OS drivers when the guest is started,
3256         # and then re-attached when the guest shuts down.
3257         if CONF.libvirt.virt_type != 'xen':
3258             # we do manual detach only for xen
3259             return
3260         try:
3261             for dev in pci_devices:
3262                 libvirt_dev_addr = dev['hypervisor_name']
3263                 libvirt_dev = \
3264                         self._host.device_lookup_by_name(libvirt_dev_addr)
3265                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3266                 # http://libvirt.org/html/libvirt-libvirt.html.
3267                 libvirt_dev.dettach()
3268 
3269             # Note(yjiang5): A reset of one PCI device may impact other
3270             # devices on the same bus, thus we need two separated loops
3271             # to detach and then reset it.
3272             for dev in pci_devices:
3273                 libvirt_dev_addr = dev['hypervisor_name']
3274                 libvirt_dev = \
3275                         self._host.device_lookup_by_name(libvirt_dev_addr)
3276                 libvirt_dev.reset()
3277 
3278         except libvirt.libvirtError as exc:
3279             raise exception.PciDevicePrepareFailed(id=dev['id'],
3280                                                    instance_uuid=
3281                                                    dev['instance_uuid'],
3282                                                    reason=six.text_type(exc))
3283 
3284     def _detach_pci_devices(self, guest, pci_devs):
3285         try:
3286             for dev in pci_devs:
3287                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3288                 # after detachDeviceFlags returned, we should check the dom to
3289                 # ensure the detaching is finished
3290                 xml = guest.get_xml_desc()
3291                 xml_doc = etree.fromstring(xml)
3292                 guest_config = vconfig.LibvirtConfigGuest()
3293                 guest_config.parse_dom(xml_doc)
3294 
3295                 for hdev in [d for d in guest_config.devices
3296                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3297                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3298                     dbsf = pci_utils.parse_address(dev.address)
3299                     if [int(x, 16) for x in hdbsf] ==\
3300                             [int(x, 16) for x in dbsf]:
3301                         raise exception.PciDeviceDetachFailed(reason=
3302                                                               "timeout",
3303                                                               dev=dev)
3304 
3305         except libvirt.libvirtError as ex:
3306             error_code = ex.get_error_code()
3307             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3308                 LOG.warning(_LW("Instance disappeared while detaching "
3309                              "a PCI device from it."))
3310             else:
3311                 raise
3312 
3313     def _attach_pci_devices(self, guest, pci_devs):
3314         try:
3315             for dev in pci_devs:
3316                 guest.attach_device(self._get_guest_pci_device(dev))
3317 
3318         except libvirt.libvirtError:
3319             LOG.error(_LE('Attaching PCI devices %(dev)s to %(dom)s failed.'),
3320                       {'dev': pci_devs, 'dom': guest.id})
3321             raise
3322 
3323     @staticmethod
3324     def _has_sriov_port(network_info):
3325         for vif in network_info:
3326             if vif['vnic_type'] in [network_model.VNIC_TYPE_DIRECT,
3327                                     network_model.VNIC_TYPE_DIRECT_PHYSICAL]:
3328                 return True
3329         return False
3330 
3331     def _attach_sriov_ports(self, context, instance, guest, network_info=None):
3332         if network_info is None:
3333             network_info = instance.info_cache.network_info
3334         if network_info is None:
3335             return
3336 
3337         if self._has_sriov_port(network_info):
3338             for vif in network_info:
3339                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
3340                     cfg = self.vif_driver.get_config(instance,
3341                                                      vif,
3342                                                      instance.image_meta,
3343                                                      instance.flavor,
3344                                                      CONF.libvirt.virt_type,
3345                                                      self._host)
3346                     LOG.debug('Attaching SR-IOV port %(port)s to %(dom)s',
3347                               {'port': vif, 'dom': guest.id},
3348                               instance=instance)
3349                     guest.attach_device(cfg)
3350 
3351     def _detach_sriov_ports(self, context, instance, guest):
3352         network_info = instance.info_cache.network_info
3353         if network_info is None:
3354             return
3355 
3356         if self._has_sriov_port(network_info):
3357             # In case of SR-IOV vif types we create pci request per SR-IOV port
3358             # Therefore we can trust that pci_slot value in the vif is correct.
3359             sriov_pci_addresses = [
3360                 vif['profile']['pci_slot']
3361                 for vif in network_info
3362                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV and
3363                    vif['profile'].get('pci_slot') is not None
3364             ]
3365 
3366             # use detach_pci_devices to avoid failure in case of
3367             # multiple guest SRIOV ports with the same MAC
3368             # (protection use-case, ports are on different physical
3369             # interfaces)
3370             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3371             sriov_devs = [pci_dev for pci_dev in pci_devs
3372                           if pci_dev.address in sriov_pci_addresses]
3373             self._detach_pci_devices(guest, sriov_devs)
3374 
3375     def _set_host_enabled(self, enabled,
3376                           disable_reason=DISABLE_REASON_UNDEFINED):
3377         """Enables / Disables the compute service on this host.
3378 
3379            This doesn't override non-automatic disablement with an automatic
3380            setting; thereby permitting operators to keep otherwise
3381            healthy hosts out of rotation.
3382         """
3383 
3384         status_name = {True: 'disabled',
3385                        False: 'enabled'}
3386 
3387         disable_service = not enabled
3388 
3389         ctx = nova_context.get_admin_context()
3390         try:
3391             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3392 
3393             if service.disabled != disable_service:
3394                 # Note(jang): this is a quick fix to stop operator-
3395                 # disabled compute hosts from re-enabling themselves
3396                 # automatically. We prefix any automatic reason code
3397                 # with a fixed string. We only re-enable a host
3398                 # automatically if we find that string in place.
3399                 # This should probably be replaced with a separate flag.
3400                 if not service.disabled or (
3401                         service.disabled_reason and
3402                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3403                     service.disabled = disable_service
3404                     service.disabled_reason = (
3405                        DISABLE_PREFIX + disable_reason
3406                        if disable_service and disable_reason else
3407                            DISABLE_REASON_UNDEFINED)
3408                     service.save()
3409                     LOG.debug('Updating compute service status to %s',
3410                               status_name[disable_service])
3411                 else:
3412                     LOG.debug('Not overriding manual compute service '
3413                               'status with: %s',
3414                               status_name[disable_service])
3415         except exception.ComputeHostNotFound:
3416             LOG.warning(_LW('Cannot update service status on host "%s" '
3417                          'since it is not registered.'), CONF.host)
3418         except Exception:
3419             LOG.warning(_LW('Cannot update service status on host "%s" '
3420                          'due to an unexpected exception.'), CONF.host,
3421                      exc_info=True)
3422 
3423     def _get_guest_cpu_model_config(self):
3424         mode = CONF.libvirt.cpu_mode
3425         model = CONF.libvirt.cpu_model
3426 
3427         if (CONF.libvirt.virt_type == "kvm" or
3428             CONF.libvirt.virt_type == "qemu"):
3429             if mode is None:
3430                 mode = "host-model"
3431             if mode == "none":
3432                 return vconfig.LibvirtConfigGuestCPU()
3433         else:
3434             if mode is None or mode == "none":
3435                 return None
3436 
3437         if ((CONF.libvirt.virt_type != "kvm" and
3438              CONF.libvirt.virt_type != "qemu")):
3439             msg = _("Config requested an explicit CPU model, but "
3440                     "the current libvirt hypervisor '%s' does not "
3441                     "support selecting CPU models") % CONF.libvirt.virt_type
3442             raise exception.Invalid(msg)
3443 
3444         if mode == "custom" and model is None:
3445             msg = _("Config requested a custom CPU model, but no "
3446                     "model name was provided")
3447             raise exception.Invalid(msg)
3448         elif mode != "custom" and model is not None:
3449             msg = _("A CPU model name should not be set when a "
3450                     "host CPU model is requested")
3451             raise exception.Invalid(msg)
3452 
3453         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3454                   {'mode': mode, 'model': (model or "")})
3455 
3456         cpu = vconfig.LibvirtConfigGuestCPU()
3457         cpu.mode = mode
3458         cpu.model = model
3459 
3460         return cpu
3461 
3462     def _get_guest_cpu_config(self, flavor, image_meta,
3463                               guest_cpu_numa_config, instance_numa_topology):
3464         cpu = self._get_guest_cpu_model_config()
3465 
3466         if cpu is None:
3467             return None
3468 
3469         topology = hardware.get_best_cpu_topology(
3470                 flavor, image_meta, numa_topology=instance_numa_topology)
3471 
3472         cpu.sockets = topology.sockets
3473         cpu.cores = topology.cores
3474         cpu.threads = topology.threads
3475         cpu.numa = guest_cpu_numa_config
3476 
3477         return cpu
3478 
3479     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3480                                image_type=None):
3481         if CONF.libvirt.hw_disk_discard:
3482             if not self._host.has_min_version(hv_ver=MIN_QEMU_DISCARD_VERSION,
3483                                               hv_type=host.HV_DRIVER_QEMU):
3484                 msg = (_('Volume sets discard option, qemu %(qemu)s'
3485                          ' or later is required.') %
3486                       {'qemu': MIN_QEMU_DISCARD_VERSION})
3487                 raise exception.Invalid(msg)
3488 
3489         disk = self.image_backend.by_name(instance, name, image_type)
3490         if (name == 'disk.config' and image_type == 'rbd' and
3491                 not disk.exists()):
3492             # This is likely an older config drive that has not been migrated
3493             # to rbd yet. Try to fall back on 'flat' image type.
3494             # TODO(melwitt): Add online migration of some sort so we can
3495             # remove this fall back once we know all config drives are in rbd.
3496             # NOTE(vladikr): make sure that the flat image exist, otherwise
3497             # the image will be created after the domain definition.
3498             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3499             if flat_disk.exists():
3500                 disk = flat_disk
3501                 LOG.debug('Config drive not found in RBD, falling back to the '
3502                           'instance directory', instance=instance)
3503         disk_info = disk_mapping[name]
3504         return disk.libvirt_info(disk_info['bus'],
3505                                  disk_info['dev'],
3506                                  disk_info['type'],
3507                                  self.disk_cachemode,
3508                                  inst_type['extra_specs'],
3509                                  self._host.get_version())
3510 
3511     def _get_guest_fs_config(self, instance, name, image_type=None):
3512         disk = self.image_backend.by_name(instance, name, image_type)
3513         return disk.libvirt_fs_info("/", "ploop")
3514 
3515     def _get_guest_storage_config(self, instance, image_meta,
3516                                   disk_info,
3517                                   rescue, block_device_info,
3518                                   inst_type, os_type):
3519         devices = []
3520         disk_mapping = disk_info['mapping']
3521 
3522         block_device_mapping = driver.block_device_info_get_mapping(
3523             block_device_info)
3524         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3525         if mount_rootfs:
3526             fs = vconfig.LibvirtConfigGuestFilesys()
3527             fs.source_type = "mount"
3528             fs.source_dir = os.path.join(
3529                 libvirt_utils.get_instance_path(instance), 'rootfs')
3530             devices.append(fs)
3531         elif (os_type == fields.VMMode.EXE and
3532               CONF.libvirt.virt_type == "parallels"):
3533             if rescue:
3534                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3535                 devices.append(fsrescue)
3536 
3537                 fsos = self._get_guest_fs_config(instance, "disk")
3538                 fsos.target_dir = "/mnt/rescue"
3539                 devices.append(fsos)
3540             else:
3541                 if 'disk' in disk_mapping:
3542                     fs = self._get_guest_fs_config(instance, "disk")
3543                     devices.append(fs)
3544         else:
3545 
3546             if rescue:
3547                 diskrescue = self._get_guest_disk_config(instance,
3548                                                          'disk.rescue',
3549                                                          disk_mapping,
3550                                                          inst_type)
3551                 devices.append(diskrescue)
3552 
3553                 diskos = self._get_guest_disk_config(instance,
3554                                                      'disk',
3555                                                      disk_mapping,
3556                                                      inst_type)
3557                 devices.append(diskos)
3558             else:
3559                 if 'disk' in disk_mapping:
3560                     diskos = self._get_guest_disk_config(instance,
3561                                                          'disk',
3562                                                          disk_mapping,
3563                                                          inst_type)
3564                     devices.append(diskos)
3565 
3566                 if 'disk.local' in disk_mapping:
3567                     disklocal = self._get_guest_disk_config(instance,
3568                                                             'disk.local',
3569                                                             disk_mapping,
3570                                                             inst_type)
3571                     devices.append(disklocal)
3572                     instance.default_ephemeral_device = (
3573                         block_device.prepend_dev(disklocal.target_dev))
3574 
3575                 for idx, eph in enumerate(
3576                     driver.block_device_info_get_ephemerals(
3577                         block_device_info)):
3578                     diskeph = self._get_guest_disk_config(
3579                         instance,
3580                         blockinfo.get_eph_disk(idx),
3581                         disk_mapping, inst_type)
3582                     devices.append(diskeph)
3583 
3584                 if 'disk.swap' in disk_mapping:
3585                     diskswap = self._get_guest_disk_config(instance,
3586                                                            'disk.swap',
3587                                                            disk_mapping,
3588                                                            inst_type)
3589                     devices.append(diskswap)
3590                     instance.default_swap_device = (
3591                         block_device.prepend_dev(diskswap.target_dev))
3592 
3593             config_name = 'disk.config.rescue' if rescue else 'disk.config'
3594             if config_name in disk_mapping:
3595                 diskconfig = self._get_guest_disk_config(
3596                     instance, config_name, disk_mapping, inst_type,
3597                     self._get_disk_config_image_type())
3598                 devices.append(diskconfig)
3599 
3600         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3601                                                    mount_rootfs):
3602             connection_info = vol['connection_info']
3603             vol_dev = block_device.prepend_dev(vol['mount_device'])
3604             info = disk_mapping[vol_dev]
3605             self._connect_volume(connection_info, info)
3606             cfg = self._get_volume_config(connection_info, info)
3607             devices.append(cfg)
3608             vol['connection_info'] = connection_info
3609             vol.save()
3610 
3611         for d in devices:
3612             self._set_cache_mode(d)
3613 
3614         if image_meta.properties.get('hw_scsi_model'):
3615             hw_scsi_model = image_meta.properties.hw_scsi_model
3616             scsi_controller = vconfig.LibvirtConfigGuestController()
3617             scsi_controller.type = 'scsi'
3618             scsi_controller.model = hw_scsi_model
3619             devices.append(scsi_controller)
3620 
3621         return devices
3622 
3623     def _get_host_sysinfo_serial_hardware(self):
3624         """Get a UUID from the host hardware
3625 
3626         Get a UUID for the host hardware reported by libvirt.
3627         This is typically from the SMBIOS data, unless it has
3628         been overridden in /etc/libvirt/libvirtd.conf
3629         """
3630         caps = self._host.get_capabilities()
3631         return caps.host.uuid
3632 
3633     def _get_host_sysinfo_serial_os(self):
3634         """Get a UUID from the host operating system
3635 
3636         Get a UUID for the host operating system. Modern Linux
3637         distros based on systemd provide a /etc/machine-id
3638         file containing a UUID. This is also provided inside
3639         systemd based containers and can be provided by other
3640         init systems too, since it is just a plain text file.
3641         """
3642         if not os.path.exists("/etc/machine-id"):
3643             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
3644             raise exception.NovaException(msg)
3645 
3646         with open("/etc/machine-id") as f:
3647             # We want to have '-' in the right place
3648             # so we parse & reformat the value
3649             lines = f.read().split()
3650             if not lines:
3651                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
3652                 raise exception.NovaException(msg)
3653 
3654             return str(uuid.UUID(lines[0]))
3655 
3656     def _get_host_sysinfo_serial_auto(self):
3657         if os.path.exists("/etc/machine-id"):
3658             return self._get_host_sysinfo_serial_os()
3659         else:
3660             return self._get_host_sysinfo_serial_hardware()
3661 
3662     def _get_guest_config_sysinfo(self, instance):
3663         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
3664 
3665         sysinfo.system_manufacturer = version.vendor_string()
3666         sysinfo.system_product = version.product_string()
3667         sysinfo.system_version = version.version_string_with_package()
3668 
3669         sysinfo.system_serial = self._sysinfo_serial_func()
3670         sysinfo.system_uuid = instance.uuid
3671 
3672         sysinfo.system_family = "Virtual Machine"
3673 
3674         return sysinfo
3675 
3676     def _get_guest_pci_device(self, pci_device):
3677 
3678         dbsf = pci_utils.parse_address(pci_device.address)
3679         dev = vconfig.LibvirtConfigGuestHostdevPCI()
3680         dev.domain, dev.bus, dev.slot, dev.function = dbsf
3681 
3682         # only kvm support managed mode
3683         if CONF.libvirt.virt_type in ('xen', 'parallels',):
3684             dev.managed = 'no'
3685         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3686             dev.managed = 'yes'
3687 
3688         return dev
3689 
3690     def _get_guest_config_meta(self, context, instance):
3691         """Get metadata config for guest."""
3692 
3693         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
3694         meta.package = version.version_string_with_package()
3695         meta.name = instance.display_name
3696         meta.creationTime = time.time()
3697 
3698         if instance.image_ref not in ("", None):
3699             meta.roottype = "image"
3700             meta.rootid = instance.image_ref
3701 
3702         if context is not None:
3703             ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
3704             ometa.userid = context.user_id
3705             ometa.username = context.user_name
3706             ometa.projectid = context.project_id
3707             ometa.projectname = context.project_name
3708             meta.owner = ometa
3709 
3710         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
3711         flavor = instance.flavor
3712         fmeta.name = flavor.name
3713         fmeta.memory = flavor.memory_mb
3714         fmeta.vcpus = flavor.vcpus
3715         fmeta.ephemeral = flavor.ephemeral_gb
3716         fmeta.disk = flavor.root_gb
3717         fmeta.swap = flavor.swap
3718 
3719         meta.flavor = fmeta
3720 
3721         return meta
3722 
3723     def _machine_type_mappings(self):
3724         mappings = {}
3725         for mapping in CONF.libvirt.hw_machine_type:
3726             host_arch, _, machine_type = mapping.partition('=')
3727             mappings[host_arch] = machine_type
3728         return mappings
3729 
3730     def _get_machine_type(self, image_meta, caps):
3731         # The underlying machine type can be set as an image attribute,
3732         # or otherwise based on some architecture specific defaults
3733 
3734         mach_type = None
3735 
3736         if image_meta.properties.get('hw_machine_type') is not None:
3737             mach_type = image_meta.properties.hw_machine_type
3738         else:
3739             # For ARM systems we will default to vexpress-a15 for armv7
3740             # and virt for aarch64
3741             if caps.host.cpu.arch == fields.Architecture.ARMV7:
3742                 mach_type = "vexpress-a15"
3743 
3744             if caps.host.cpu.arch == fields.Architecture.AARCH64:
3745                 mach_type = "virt"
3746 
3747             if caps.host.cpu.arch in (fields.Architecture.S390,
3748                                       fields.Architecture.S390X):
3749                 mach_type = 's390-ccw-virtio'
3750 
3751             # If set in the config, use that as the default.
3752             if CONF.libvirt.hw_machine_type:
3753                 mappings = self._machine_type_mappings()
3754                 mach_type = mappings.get(caps.host.cpu.arch)
3755 
3756         return mach_type
3757 
3758     @staticmethod
3759     def _create_idmaps(klass, map_strings):
3760         idmaps = []
3761         if len(map_strings) > 5:
3762             map_strings = map_strings[0:5]
3763             LOG.warning(_LW("Too many id maps, only included first five."))
3764         for map_string in map_strings:
3765             try:
3766                 idmap = klass()
3767                 values = [int(i) for i in map_string.split(":")]
3768                 idmap.start = values[0]
3769                 idmap.target = values[1]
3770                 idmap.count = values[2]
3771                 idmaps.append(idmap)
3772             except (ValueError, IndexError):
3773                 LOG.warning(_LW("Invalid value for id mapping %s"), map_string)
3774         return idmaps
3775 
3776     def _get_guest_idmaps(self):
3777         id_maps = []
3778         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
3779             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
3780                                            CONF.libvirt.uid_maps)
3781             id_maps.extend(uid_maps)
3782         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
3783             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
3784                                            CONF.libvirt.gid_maps)
3785             id_maps.extend(gid_maps)
3786         return id_maps
3787 
3788     def _update_guest_cputune(self, guest, flavor, virt_type):
3789         is_able = self._host.is_cpu_control_policy_capable()
3790 
3791         cputuning = ['shares', 'period', 'quota']
3792         wants_cputune = any([k for k in cputuning
3793             if "quota:cpu_" + k in flavor.extra_specs.keys()])
3794 
3795         if wants_cputune and not is_able:
3796             raise exception.UnsupportedHostCPUControlPolicy()
3797 
3798         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
3799             return
3800 
3801         if guest.cputune is None:
3802             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
3803             # Setting the default cpu.shares value to be a value
3804             # dependent on the number of vcpus
3805         guest.cputune.shares = 1024 * guest.vcpus
3806 
3807         for name in cputuning:
3808             key = "quota:cpu_" + name
3809             if key in flavor.extra_specs:
3810                 setattr(guest.cputune, name,
3811                         int(flavor.extra_specs[key]))
3812 
3813     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
3814                                            wants_hugepages):
3815         if instance_numa_topology:
3816             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
3817             for instance_cell in instance_numa_topology.cells:
3818                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
3819                 guest_cell.id = instance_cell.id
3820                 guest_cell.cpus = instance_cell.cpuset
3821                 guest_cell.memory = instance_cell.memory * units.Ki
3822 
3823                 # The vhost-user network backend requires file backed
3824                 # guest memory (ie huge pages) to be marked as shared
3825                 # access, not private, so an external process can read
3826                 # and write the pages.
3827                 #
3828                 # You can't change the shared vs private flag for an
3829                 # already running guest, and since we can't predict what
3830                 # types of NIC may be hotplugged, we have no choice but
3831                 # to unconditionally turn on the shared flag. This has
3832                 # no real negative functional effect on the guest, so
3833                 # is a reasonable approach to take
3834                 if wants_hugepages:
3835                     guest_cell.memAccess = "shared"
3836                 guest_cpu_numa.cells.append(guest_cell)
3837             return guest_cpu_numa
3838 
3839     def _has_cpu_policy_support(self):
3840         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
3841             if self._host.has_version(ver):
3842                 ver_ = self._version_to_string(ver)
3843                 raise exception.CPUPinningNotSupported(reason=_(
3844                     'Invalid libvirt version %(version)s') % {'version': ver_})
3845         return True
3846 
3847     def _wants_hugepages(self, host_topology, instance_topology):
3848         """Determine if the guest / host topology implies the
3849            use of huge pages for guest RAM backing
3850         """
3851 
3852         if host_topology is None or instance_topology is None:
3853             return False
3854 
3855         avail_pagesize = [page.size_kb
3856                           for page in host_topology.cells[0].mempages]
3857         avail_pagesize.sort()
3858         # Remove smallest page size as that's not classed as a largepage
3859         avail_pagesize = avail_pagesize[1:]
3860 
3861         # See if we have page size set
3862         for cell in instance_topology.cells:
3863             if (cell.pagesize is not None and
3864                 cell.pagesize in avail_pagesize):
3865                 return True
3866 
3867         return False
3868 
3869     def _get_guest_numa_config(self, instance_numa_topology, flavor,
3870                                allowed_cpus=None, image_meta=None):
3871         """Returns the config objects for the guest NUMA specs.
3872 
3873         Determines the CPUs that the guest can be pinned to if the guest
3874         specifies a cell topology and the host supports it. Constructs the
3875         libvirt XML config object representing the NUMA topology selected
3876         for the guest. Returns a tuple of:
3877 
3878             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
3879 
3880         With the following caveats:
3881 
3882             a) If there is no specified guest NUMA topology, then
3883                all tuple elements except cpu_set shall be None. cpu_set
3884                will be populated with the chosen CPUs that the guest
3885                allowed CPUs fit within, which could be the supplied
3886                allowed_cpus value if the host doesn't support NUMA
3887                topologies.
3888 
3889             b) If there is a specified guest NUMA topology, then
3890                cpu_set will be None and guest_cpu_numa will be the
3891                LibvirtConfigGuestCPUNUMA object representing the guest's
3892                NUMA topology. If the host supports NUMA, then guest_cpu_tune
3893                will contain a LibvirtConfigGuestCPUTune object representing
3894                the optimized chosen cells that match the host capabilities
3895                with the instance's requested topology. If the host does
3896                not support NUMA, then guest_cpu_tune and guest_numa_tune
3897                will be None.
3898         """
3899 
3900         if (not self._has_numa_support() and
3901                 instance_numa_topology is not None):
3902             # We should not get here, since we should have avoided
3903             # reporting NUMA topology from _get_host_numa_topology
3904             # in the first place. Just in case of a scheduler
3905             # mess up though, raise an exception
3906             raise exception.NUMATopologyUnsupported()
3907 
3908         topology = self._get_host_numa_topology()
3909 
3910         # We have instance NUMA so translate it to the config class
3911         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
3912                 instance_numa_topology,
3913                 self._wants_hugepages(topology, instance_numa_topology))
3914 
3915         if not guest_cpu_numa_config:
3916             # No NUMA topology defined for instance - let the host kernel deal
3917             # with the NUMA effects.
3918             # TODO(ndipanov): Attempt to spread the instance
3919             # across NUMA nodes and expose the topology to the
3920             # instance as an optimisation
3921             return GuestNumaConfig(allowed_cpus, None, None, None)
3922         else:
3923             if topology:
3924                 # Now get the CpuTune configuration from the numa_topology
3925                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
3926                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
3927                 emupcpus = []
3928 
3929                 numa_mem = vconfig.LibvirtConfigGuestNUMATuneMemory()
3930                 numa_memnodes = [vconfig.LibvirtConfigGuestNUMATuneMemNode()
3931                                  for _ in guest_cpu_numa_config.cells]
3932 
3933                 vcpus_rt = set([])
3934                 wants_realtime = hardware.is_realtime_enabled(flavor)
3935                 if wants_realtime:
3936                     if not self._host.has_min_version(
3937                             MIN_LIBVIRT_REALTIME_VERSION):
3938                         raise exception.RealtimePolicyNotSupported()
3939                     # Prepare realtime config for libvirt
3940                     vcpus_rt = hardware.vcpus_realtime_topology(
3941                         flavor, image_meta)
3942                     vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
3943                     vcpusched.vcpus = vcpus_rt
3944                     vcpusched.scheduler = "fifo"
3945                     vcpusched.priority = (
3946                         CONF.libvirt.realtime_scheduler_priority)
3947                     guest_cpu_tune.vcpusched.append(vcpusched)
3948 
3949                 for host_cell in topology.cells:
3950                     for guest_node_id, guest_config_cell in enumerate(
3951                             guest_cpu_numa_config.cells):
3952                         if guest_config_cell.id == host_cell.id:
3953                             node = numa_memnodes[guest_node_id]
3954                             node.cellid = guest_node_id
3955                             node.nodeset = [host_cell.id]
3956                             node.mode = "strict"
3957 
3958                             numa_mem.nodeset.append(host_cell.id)
3959 
3960                             object_numa_cell = (
3961                                     instance_numa_topology.cells[guest_node_id]
3962                                 )
3963                             for cpu in guest_config_cell.cpus:
3964                                 pin_cpuset = (
3965                                     vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
3966                                 pin_cpuset.id = cpu
3967                                 # If there is pinning information in the cell
3968                                 # we pin to individual CPUs, otherwise we float
3969                                 # over the whole host NUMA node
3970 
3971                                 if (object_numa_cell.cpu_pinning and
3972                                         self._has_cpu_policy_support()):
3973                                     pcpu = object_numa_cell.cpu_pinning[cpu]
3974                                     pin_cpuset.cpuset = set([pcpu])
3975                                 else:
3976                                     pin_cpuset.cpuset = host_cell.cpuset
3977                                 if not wants_realtime or cpu not in vcpus_rt:
3978                                     # - If realtime IS NOT enabled, the
3979                                     #   emulator threads are allowed to float
3980                                     #   across all the pCPUs associated with
3981                                     #   the guest vCPUs ("not wants_realtime"
3982                                     #   is true, so we add all pcpus)
3983                                     # - If realtime IS enabled, then at least
3984                                     #   1 vCPU is required to be set aside for
3985                                     #   non-realtime usage. The emulator
3986                                     #   threads are allowed to float acros the
3987                                     #   pCPUs that are associated with the
3988                                     #   non-realtime VCPUs (the "cpu not in
3989                                     #   vcpu_rt" check deals with this
3990                                     #   filtering)
3991                                     emupcpus.extend(pin_cpuset.cpuset)
3992                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
3993 
3994                 # TODO(berrange) When the guest has >1 NUMA node, it will
3995                 # span multiple host NUMA nodes. By pinning emulator threads
3996                 # to the union of all nodes, we guarantee there will be
3997                 # cross-node memory access by the emulator threads when
3998                 # responding to guest I/O operations. The only way to avoid
3999                 # this would be to pin emulator threads to a single node and
4000                 # tell the guest OS to only do I/O from one of its virtual
4001                 # NUMA nodes. This is not even remotely practical.
4002                 #
4003                 # The long term solution is to make use of a new QEMU feature
4004                 # called "I/O Threads" which will let us configure an explicit
4005                 # I/O thread for each guest vCPU or guest NUMA node. It is
4006                 # still TBD how to make use of this feature though, especially
4007                 # how to associate IO threads with guest devices to eliminiate
4008                 # cross NUMA node traffic. This is an area of investigation
4009                 # for QEMU community devs.
4010                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
4011                 emulatorpin.cpuset = set(emupcpus)
4012                 guest_cpu_tune.emulatorpin = emulatorpin
4013                 # Sort the vcpupin list per vCPU id for human-friendlier XML
4014                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4015 
4016                 guest_numa_tune.memory = numa_mem
4017                 guest_numa_tune.memnodes = numa_memnodes
4018 
4019                 # normalize cell.id
4020                 for i, (cell, memnode) in enumerate(
4021                                             zip(guest_cpu_numa_config.cells,
4022                                                 guest_numa_tune.memnodes)):
4023                     cell.id = i
4024                     memnode.cellid = i
4025 
4026                 return GuestNumaConfig(None, guest_cpu_tune,
4027                                        guest_cpu_numa_config,
4028                                        guest_numa_tune)
4029             else:
4030                 return GuestNumaConfig(allowed_cpus, None,
4031                                        guest_cpu_numa_config, None)
4032 
4033     def _get_guest_os_type(self, virt_type):
4034         """Returns the guest OS type based on virt type."""
4035         if virt_type == "lxc":
4036             ret = fields.VMMode.EXE
4037         elif virt_type == "uml":
4038             ret = fields.VMMode.UML
4039         elif virt_type == "xen":
4040             ret = fields.VMMode.XEN
4041         else:
4042             ret = fields.VMMode.HVM
4043         return ret
4044 
4045     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4046                               root_device_name):
4047         if rescue.get('kernel_id'):
4048             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4049             if virt_type == "xen":
4050                 guest.os_cmdline = "ro root=%s" % root_device_name
4051             else:
4052                 guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4053                 if virt_type == "qemu":
4054                     guest.os_cmdline += " no_timer_check"
4055         if rescue.get('ramdisk_id'):
4056             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4057 
4058     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4059                                 root_device_name, image_meta):
4060         guest.os_kernel = os.path.join(inst_path, "kernel")
4061         if virt_type == "xen":
4062             guest.os_cmdline = "ro root=%s" % root_device_name
4063         else:
4064             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4065             if virt_type == "qemu":
4066                 guest.os_cmdline += " no_timer_check"
4067         if instance.ramdisk_id:
4068             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4069         # we only support os_command_line with images with an explicit
4070         # kernel set and don't want to break nova if there's an
4071         # os_command_line property without a specified kernel_id param
4072         if image_meta.properties.get("os_command_line"):
4073             guest.os_cmdline = image_meta.properties.os_command_line
4074 
4075     def _set_clock(self, guest, os_type, image_meta, virt_type):
4076         # NOTE(mikal): Microsoft Windows expects the clock to be in
4077         # "localtime". If the clock is set to UTC, then you can use a
4078         # registry key to let windows know, but Microsoft says this is
4079         # buggy in http://support.microsoft.com/kb/2687252
4080         clk = vconfig.LibvirtConfigGuestClock()
4081         if os_type == 'windows':
4082             LOG.info(_LI('Configuring timezone for windows instance to '
4083                          'localtime'))
4084             clk.offset = 'localtime'
4085         else:
4086             clk.offset = 'utc'
4087         guest.set_clock(clk)
4088 
4089         if virt_type == "kvm":
4090             self._set_kvm_timers(clk, os_type, image_meta)
4091 
4092     def _set_kvm_timers(self, clk, os_type, image_meta):
4093         # TODO(berrange) One day this should be per-guest
4094         # OS type configurable
4095         tmpit = vconfig.LibvirtConfigGuestTimer()
4096         tmpit.name = "pit"
4097         tmpit.tickpolicy = "delay"
4098 
4099         tmrtc = vconfig.LibvirtConfigGuestTimer()
4100         tmrtc.name = "rtc"
4101         tmrtc.tickpolicy = "catchup"
4102 
4103         clk.add_timer(tmpit)
4104         clk.add_timer(tmrtc)
4105 
4106         guestarch = libvirt_utils.get_arch(image_meta)
4107         if guestarch in (fields.Architecture.I686,
4108                          fields.Architecture.X86_64):
4109             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4110             # qemu -no-hpet is not supported on non-x86 targets.
4111             tmhpet = vconfig.LibvirtConfigGuestTimer()
4112             tmhpet.name = "hpet"
4113             tmhpet.present = False
4114             clk.add_timer(tmhpet)
4115 
4116         # With new enough QEMU we can provide Windows guests
4117         # with the paravirtualized hyperv timer source. This
4118         # is the windows equiv of kvm-clock, allowing Windows
4119         # guests to accurately keep time.
4120         if (os_type == 'windows' and
4121             self._host.has_min_version(MIN_LIBVIRT_HYPERV_TIMER_VERSION,
4122                                        MIN_QEMU_HYPERV_TIMER_VERSION)):
4123             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4124             tmhyperv.name = "hypervclock"
4125             tmhyperv.present = True
4126             clk.add_timer(tmhyperv)
4127 
4128     def _set_features(self, guest, os_type, caps, virt_type):
4129         if virt_type == "xen":
4130             # PAE only makes sense in X86
4131             if caps.host.cpu.arch in (fields.Architecture.I686,
4132                                       fields.Architecture.X86_64):
4133                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4134 
4135         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4136                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4137             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4138             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4139 
4140         if (virt_type in ("qemu", "kvm") and
4141                 os_type == 'windows'):
4142             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4143             hv.relaxed = True
4144 
4145             hv.spinlocks = True
4146             # Increase spinlock retries - value recommended by
4147             # KVM maintainers who certify Windows guests
4148             # with Microsoft
4149             hv.spinlock_retries = 8191
4150             hv.vapic = True
4151             guest.features.append(hv)
4152 
4153     def _check_number_of_serial_console(self, num_ports):
4154         virt_type = CONF.libvirt.virt_type
4155         if (virt_type in ("kvm", "qemu") and
4156             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4157             raise exception.SerialPortNumberLimitExceeded(
4158                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4159 
4160     def _add_video_driver(self, guest, image_meta, flavor):
4161         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga", "xen", "qxl")
4162         video = vconfig.LibvirtConfigGuestVideo()
4163         # NOTE(ldbragst): The following logic sets the video.type
4164         # depending on supported defaults given the architecture,
4165         # virtualization type, and features. The video.type attribute can
4166         # be overridden by the user with image_meta.properties, which
4167         # is carried out in the next if statement below this one.
4168         guestarch = libvirt_utils.get_arch(image_meta)
4169         if guest.os_type == fields.VMMode.XEN:
4170             video.type = 'xen'
4171         elif CONF.libvirt.virt_type == 'parallels':
4172             video.type = 'vga'
4173         elif guestarch in (fields.Architecture.PPC,
4174                            fields.Architecture.PPC64,
4175                            fields.Architecture.PPC64LE):
4176             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4177             # so use 'vga' instead when running on Power hardware.
4178             video.type = 'vga'
4179         elif CONF.spice.enabled:
4180             video.type = 'qxl'
4181         if image_meta.properties.get('hw_video_model'):
4182             video.type = image_meta.properties.hw_video_model
4183             if (video.type not in VALID_VIDEO_DEVICES):
4184                 raise exception.InvalidVideoMode(model=video.type)
4185 
4186         # Set video memory, only if the flavor's limit is set
4187         video_ram = image_meta.properties.get('hw_video_ram', 0)
4188         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4189         if video_ram > max_vram:
4190             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4191                                                  max_vram=max_vram)
4192         if max_vram and video_ram:
4193             video.vram = video_ram * units.Mi / units.Ki
4194         guest.add_device(video)
4195 
4196     def _add_qga_device(self, guest, instance):
4197         qga = vconfig.LibvirtConfigGuestChannel()
4198         qga.type = "unix"
4199         qga.target_name = "org.qemu.guest_agent.0"
4200         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4201                           ("org.qemu.guest_agent.0", instance.name))
4202         guest.add_device(qga)
4203 
4204     def _add_rng_device(self, guest, flavor):
4205         rng_device = vconfig.LibvirtConfigGuestRng()
4206         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4207         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4208         if rate_bytes:
4209             rng_device.rate_bytes = int(rate_bytes)
4210             rng_device.rate_period = int(period)
4211         rng_path = CONF.libvirt.rng_dev_path
4212         if (rng_path and not os.path.exists(rng_path)):
4213             raise exception.RngDeviceNotExist(path=rng_path)
4214         rng_device.backend = rng_path
4215         guest.add_device(rng_device)
4216 
4217     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4218         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4219         if image_meta.properties.get('hw_qemu_guest_agent', False):
4220             LOG.debug("Qemu guest agent is enabled through image "
4221                       "metadata", instance=instance)
4222             self._add_qga_device(guest, instance)
4223         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4224         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4225         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4226         if rng_is_virtio and rng_allowed:
4227             self._add_rng_device(guest, flavor)
4228 
4229     def _get_guest_memory_backing_config(
4230             self, inst_topology, numatune, flavor):
4231         wantsmempages = False
4232         if inst_topology:
4233             for cell in inst_topology.cells:
4234                 if cell.pagesize:
4235                     wantsmempages = True
4236                     break
4237 
4238         wantsrealtime = hardware.is_realtime_enabled(flavor)
4239 
4240         membacking = None
4241         if wantsmempages:
4242             pages = self._get_memory_backing_hugepages_support(
4243                 inst_topology, numatune)
4244             if pages:
4245                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4246                 membacking.hugepages = pages
4247         if wantsrealtime:
4248             if not membacking:
4249                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4250             membacking.locked = True
4251             membacking.sharedpages = False
4252 
4253         return membacking
4254 
4255     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4256         if not self._has_hugepage_support():
4257             # We should not get here, since we should have avoided
4258             # reporting NUMA topology from _get_host_numa_topology
4259             # in the first place. Just in case of a scheduler
4260             # mess up though, raise an exception
4261             raise exception.MemoryPagesUnsupported()
4262 
4263         host_topology = self._get_host_numa_topology()
4264 
4265         if host_topology is None:
4266             # As above, we should not get here but just in case...
4267             raise exception.MemoryPagesUnsupported()
4268 
4269         # Currently libvirt does not support the smallest
4270         # pagesize set as a backend memory.
4271         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4272         avail_pagesize = [page.size_kb
4273                           for page in host_topology.cells[0].mempages]
4274         avail_pagesize.sort()
4275         smallest = avail_pagesize[0]
4276 
4277         pages = []
4278         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4279             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4280                 for memnode in numatune.memnodes:
4281                     if guest_cellid == memnode.cellid:
4282                         page = (
4283                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4284                         page.nodeset = [guest_cellid]
4285                         page.size_kb = inst_cell.pagesize
4286                         pages.append(page)
4287                         break  # Quit early...
4288         return pages
4289 
4290     def _get_flavor(self, ctxt, instance, flavor):
4291         if flavor is not None:
4292             return flavor
4293         return instance.flavor
4294 
4295     def _has_uefi_support(self):
4296         # This means that the host can support uefi booting for guests
4297         supported_archs = [fields.Architecture.X86_64,
4298                            fields.Architecture.AARCH64]
4299         caps = self._host.get_capabilities()
4300         return ((caps.host.cpu.arch in supported_archs) and
4301                 self._host.has_min_version(MIN_LIBVIRT_UEFI_VERSION) and
4302                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4303 
4304     def _get_supported_perf_events(self):
4305 
4306         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4307              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4308             return []
4309 
4310         supported_events = []
4311         host_cpu_info = self._get_cpu_info()
4312         for event in CONF.libvirt.enabled_perf_events:
4313             if self._supported_perf_event(event, host_cpu_info['features']):
4314                 supported_events.append(event)
4315         return supported_events
4316 
4317     def _supported_perf_event(self, event, cpu_features):
4318 
4319         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4320 
4321         if not hasattr(libvirt, libvirt_perf_event_name):
4322             LOG.warning(_LW("Libvirt doesn't support event type %s."),
4323                         event)
4324             return False
4325 
4326         if (event in PERF_EVENTS_CPU_FLAG_MAPPING
4327             and PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features):
4328             LOG.warning(_LW("Host does not support event type %s."), event)
4329             return False
4330 
4331         return True
4332 
4333     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4334                                       image_meta, flavor, root_device_name):
4335         if virt_type == "xen":
4336             if guest.os_type == fields.VMMode.HVM:
4337                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4338         elif virt_type in ("kvm", "qemu"):
4339             if caps.host.cpu.arch in (fields.Architecture.I686,
4340                                       fields.Architecture.X86_64):
4341                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4342                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4343             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4344             if hw_firmware_type == fields.FirmwareType.UEFI:
4345                 if self._has_uefi_support():
4346                     global uefi_logged
4347                     if not uefi_logged:
4348                         LOG.warning(_LW("uefi support is without some kind of "
4349                                         "functional testing and therefore "
4350                                         "considered experimental."))
4351                         uefi_logged = True
4352                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4353                         caps.host.cpu.arch]
4354                     guest.os_loader_type = "pflash"
4355                 else:
4356                     raise exception.UEFINotSupported()
4357             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4358             if image_meta.properties.get('hw_boot_menu') is None:
4359                 guest.os_bootmenu = strutils.bool_from_string(
4360                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4361             else:
4362                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4363 
4364         elif virt_type == "lxc":
4365             guest.os_init_path = "/sbin/init"
4366             guest.os_cmdline = CONSOLE
4367         elif virt_type == "uml":
4368             guest.os_kernel = "/usr/bin/linux"
4369             guest.os_root = root_device_name
4370         elif virt_type == "parallels":
4371             if guest.os_type == fields.VMMode.EXE:
4372                 guest.os_init_path = "/sbin/init"
4373 
4374     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4375                     instance, inst_path, image_meta, disk_info):
4376         if rescue:
4377             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4378                                        root_device_name)
4379         elif instance.kernel_id:
4380             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4381                                             virt_type, root_device_name,
4382                                             image_meta)
4383         else:
4384             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4385 
4386     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4387                          image_meta):
4388         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4389         # easy to lose track. Use this chart to figure out your case:
4390         #
4391         # case | is serial | has       | is qemu | resulting
4392         #      | enabled?  | virtlogd? | or kvm? | devices
4393         # --------------------------------------------------
4394         #    1 |        no |        no |     no  | pty*
4395         #    2 |        no |        no |     yes | file + pty
4396         #    3 |        no |       yes |      no | see case 1
4397         #    4 |        no |       yes |     yes | pty with logd
4398         #    5 |       yes |        no |      no | see case 1
4399         #    6 |       yes |        no |     yes | tcp + pty
4400         #    7 |       yes |       yes |      no | see case 1
4401         #    8 |       yes |       yes |     yes | tcp with logd
4402         #    * exception: virt_type "parallels" doesn't create a device
4403         if virt_type == 'parallels':
4404             pass
4405         elif virt_type not in ("qemu", "kvm"):
4406             self._create_pty_device(guest_cfg,
4407                                     vconfig.LibvirtConfigGuestConsole)
4408         elif (virt_type in ("qemu", "kvm") and
4409                   self._is_s390x_guest(image_meta)):
4410             self._create_consoles_s390x(guest_cfg, instance,
4411                                         flavor, image_meta)
4412         elif virt_type in ("qemu", "kvm"):
4413             self._create_consoles_qemu_kvm(guest_cfg, instance,
4414                                         flavor, image_meta)
4415 
4416     def _is_s390x_guest(self, image_meta):
4417         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4418         return libvirt_utils.get_arch(image_meta) in s390x_archs
4419 
4420     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4421                                   image_meta):
4422         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4423         log_path = self._get_console_log_path(instance)
4424         if CONF.serial_console.enabled:
4425             if not self._serial_ports_already_defined(instance):
4426                 num_ports = hardware.get_number_of_serial_ports(flavor,
4427                                                                 image_meta)
4428                 self._check_number_of_serial_console(num_ports)
4429                 self._create_serial_consoles(guest_cfg, num_ports,
4430                                              char_dev_cls, log_path)
4431         else:
4432             self._create_file_device(guest_cfg, instance, char_dev_cls)
4433         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4434 
4435     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4436         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4437         log_path = self._get_console_log_path(instance)
4438         if CONF.serial_console.enabled:
4439             if not self._serial_ports_already_defined(instance):
4440                 num_ports = hardware.get_number_of_serial_ports(flavor,
4441                                                                 image_meta)
4442                 self._create_serial_consoles(guest_cfg, num_ports,
4443                                              char_dev_cls, log_path)
4444         else:
4445             self._create_file_device(guest_cfg, instance, char_dev_cls,
4446                                      "sclplm")
4447         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4448 
4449     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4450                            log_path=None):
4451         def _create_base_dev():
4452             consolepty = char_dev_cls()
4453             consolepty.target_type = target_type
4454             consolepty.type = "pty"
4455             return consolepty
4456 
4457         def _create_logd_dev():
4458             consolepty = _create_base_dev()
4459             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4460             log.file = log_path
4461             consolepty.log = log
4462             return consolepty
4463 
4464         if CONF.serial_console.enabled:
4465             if self._is_virtlogd_available():
4466                 return
4467             else:
4468                 # NOTE(markus_z): You may wonder why this is necessary and
4469                 # so do I. I'm certain that this is *not* needed in any
4470                 # real use case. It is, however, useful if you want to
4471                 # pypass the Nova API and use "virsh console <guest>" on
4472                 # an hypervisor, as this CLI command doesn't work with TCP
4473                 # devices (like the serial console is).
4474                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4475                 # Pypassing the Nova API however is a thing we don't want.
4476                 # Future changes should remove this and fix the unit tests
4477                 # which ask for the existence.
4478                 guest_cfg.add_device(_create_base_dev())
4479         else:
4480             if self._is_virtlogd_available():
4481                 guest_cfg.add_device(_create_logd_dev())
4482             else:
4483                 guest_cfg.add_device(_create_base_dev())
4484 
4485     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4486                             target_type=None):
4487         if self._is_virtlogd_available():
4488             return
4489 
4490         consolelog = char_dev_cls()
4491         consolelog.target_type = target_type
4492         consolelog.type = "file"
4493         consolelog.source_path = self._get_console_log_path(instance)
4494         guest_cfg.add_device(consolelog)
4495 
4496     def _serial_ports_already_defined(self, instance):
4497         try:
4498             guest = self._host.get_guest(instance)
4499             if list(self._get_serial_ports_from_guest(guest)):
4500                 # Serial port are already configured for instance that
4501                 # means we are in a context of migration.
4502                 return True
4503         except exception.InstanceNotFound:
4504             LOG.debug(
4505                 "Instance does not exist yet on libvirt, we can "
4506                 "safely pass on looking for already defined serial "
4507                 "ports in its domain XML", instance=instance)
4508         return False
4509 
4510     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4511                                 log_path):
4512         for port in six.moves.range(num_ports):
4513             console = char_dev_cls()
4514             console.port = port
4515             console.type = "tcp"
4516             console.listen_host = CONF.serial_console.proxyclient_address
4517             listen_port = serial_console.acquire_port(console.listen_host)
4518             console.listen_port = listen_port
4519             # NOTE: only the first serial console gets the boot messages,
4520             # that's why we attach the logd subdevice only to that.
4521             if port == 0 and self._is_virtlogd_available():
4522                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
4523                 log.file = log_path
4524                 console.log = log
4525             guest_cfg.add_device(console)
4526 
4527     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
4528         """Update VirtCPUModel object according to libvirt CPU config.
4529 
4530         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
4531                            instance's virtual cpu configuration.
4532         :param:vcpu_model: VirtCPUModel object. A new object will be created
4533                            if None.
4534 
4535         :return: Updated VirtCPUModel object, or None if cpu_config is None
4536 
4537         """
4538 
4539         if not cpu_config:
4540             return
4541         if not vcpu_model:
4542             vcpu_model = objects.VirtCPUModel()
4543 
4544         vcpu_model.arch = cpu_config.arch
4545         vcpu_model.vendor = cpu_config.vendor
4546         vcpu_model.model = cpu_config.model
4547         vcpu_model.mode = cpu_config.mode
4548         vcpu_model.match = cpu_config.match
4549 
4550         if cpu_config.sockets:
4551             vcpu_model.topology = objects.VirtCPUTopology(
4552                 sockets=cpu_config.sockets,
4553                 cores=cpu_config.cores,
4554                 threads=cpu_config.threads)
4555         else:
4556             vcpu_model.topology = None
4557 
4558         features = [objects.VirtCPUFeature(
4559             name=f.name,
4560             policy=f.policy) for f in cpu_config.features]
4561         vcpu_model.features = features
4562 
4563         return vcpu_model
4564 
4565     def _vcpu_model_to_cpu_config(self, vcpu_model):
4566         """Create libvirt CPU config according to VirtCPUModel object.
4567 
4568         :param:vcpu_model: VirtCPUModel object.
4569 
4570         :return: vconfig.LibvirtConfigGuestCPU.
4571 
4572         """
4573 
4574         cpu_config = vconfig.LibvirtConfigGuestCPU()
4575         cpu_config.arch = vcpu_model.arch
4576         cpu_config.model = vcpu_model.model
4577         cpu_config.mode = vcpu_model.mode
4578         cpu_config.match = vcpu_model.match
4579         cpu_config.vendor = vcpu_model.vendor
4580         if vcpu_model.topology:
4581             cpu_config.sockets = vcpu_model.topology.sockets
4582             cpu_config.cores = vcpu_model.topology.cores
4583             cpu_config.threads = vcpu_model.topology.threads
4584         if vcpu_model.features:
4585             for f in vcpu_model.features:
4586                 xf = vconfig.LibvirtConfigGuestCPUFeature()
4587                 xf.name = f.name
4588                 xf.policy = f.policy
4589                 cpu_config.features.add(xf)
4590         return cpu_config
4591 
4592     def _get_guest_config(self, instance, network_info, image_meta,
4593                           disk_info, rescue=None, block_device_info=None,
4594                           context=None):
4595         """Get config data for parameters.
4596 
4597         :param rescue: optional dictionary that should contain the key
4598             'ramdisk_id' if a ramdisk is needed for the rescue image and
4599             'kernel_id' if a kernel is needed for the rescue image.
4600         """
4601         flavor = instance.flavor
4602         inst_path = libvirt_utils.get_instance_path(instance)
4603         disk_mapping = disk_info['mapping']
4604 
4605         virt_type = CONF.libvirt.virt_type
4606         guest = vconfig.LibvirtConfigGuest()
4607         guest.virt_type = virt_type
4608         guest.name = instance.name
4609         guest.uuid = instance.uuid
4610         # We are using default unit for memory: KiB
4611         guest.memory = flavor.memory_mb * units.Ki
4612         guest.vcpus = flavor.vcpus
4613         allowed_cpus = hardware.get_vcpu_pin_set()
4614 
4615         guest_numa_config = self._get_guest_numa_config(
4616             instance.numa_topology, flavor, allowed_cpus, image_meta)
4617 
4618         guest.cpuset = guest_numa_config.cpuset
4619         guest.cputune = guest_numa_config.cputune
4620         guest.numatune = guest_numa_config.numatune
4621 
4622         guest.membacking = self._get_guest_memory_backing_config(
4623             instance.numa_topology,
4624             guest_numa_config.numatune,
4625             flavor)
4626 
4627         guest.metadata.append(self._get_guest_config_meta(context,
4628                                                           instance))
4629         guest.idmaps = self._get_guest_idmaps()
4630 
4631         for event in self._supported_perf_events:
4632             guest.add_perf_event(event)
4633 
4634         self._update_guest_cputune(guest, flavor, virt_type)
4635 
4636         guest.cpu = self._get_guest_cpu_config(
4637             flavor, image_meta, guest_numa_config.numaconfig,
4638             instance.numa_topology)
4639 
4640         # Notes(yjiang5): we always sync the instance's vcpu model with
4641         # the corresponding config file.
4642         instance.vcpu_model = self._cpu_config_to_vcpu_model(
4643             guest.cpu, instance.vcpu_model)
4644 
4645         if 'root' in disk_mapping:
4646             root_device_name = block_device.prepend_dev(
4647                 disk_mapping['root']['dev'])
4648         else:
4649             root_device_name = None
4650 
4651         if root_device_name:
4652             # NOTE(yamahata):
4653             # for nova.api.ec2.cloud.CloudController.get_metadata()
4654             instance.root_device_name = root_device_name
4655 
4656         guest.os_type = (fields.VMMode.get_from_instance(instance) or
4657                 self._get_guest_os_type(virt_type))
4658         caps = self._host.get_capabilities()
4659 
4660         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
4661                                            image_meta, flavor,
4662                                            root_device_name)
4663         if virt_type not in ('lxc', 'uml'):
4664             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
4665                     instance, inst_path, image_meta, disk_info)
4666 
4667         self._set_features(guest, instance.os_type, caps, virt_type)
4668         self._set_clock(guest, instance.os_type, image_meta, virt_type)
4669 
4670         storage_configs = self._get_guest_storage_config(
4671                 instance, image_meta, disk_info, rescue, block_device_info,
4672                 flavor, guest.os_type)
4673         for config in storage_configs:
4674             guest.add_device(config)
4675 
4676         for vif in network_info:
4677             config = self.vif_driver.get_config(
4678                 instance, vif, image_meta,
4679                 flavor, virt_type, self._host)
4680             guest.add_device(config)
4681 
4682         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
4683 
4684         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
4685         if pointer:
4686             guest.add_device(pointer)
4687 
4688         if (CONF.spice.enabled and CONF.spice.agent_enabled and
4689                 virt_type not in ('lxc', 'uml', 'xen')):
4690             channel = vconfig.LibvirtConfigGuestChannel()
4691             channel.target_name = "com.redhat.spice.0"
4692             guest.add_device(channel)
4693 
4694         # NB some versions of libvirt support both SPICE and VNC
4695         # at the same time. We're not trying to second guess which
4696         # those versions are. We'll just let libvirt report the
4697         # errors appropriately if the user enables both.
4698         add_video_driver = False
4699         if ((CONF.vnc.enabled and
4700              virt_type not in ('lxc', 'uml'))):
4701             graphics = vconfig.LibvirtConfigGuestGraphics()
4702             graphics.type = "vnc"
4703             graphics.keymap = CONF.vnc.keymap
4704             graphics.listen = CONF.vnc.vncserver_listen
4705             guest.add_device(graphics)
4706             add_video_driver = True
4707 
4708         if (CONF.spice.enabled and
4709                 virt_type not in ('lxc', 'uml', 'xen')):
4710             graphics = vconfig.LibvirtConfigGuestGraphics()
4711             graphics.type = "spice"
4712             graphics.keymap = CONF.spice.keymap
4713             graphics.listen = CONF.spice.server_listen
4714             guest.add_device(graphics)
4715             add_video_driver = True
4716 
4717         if add_video_driver:
4718             self._add_video_driver(guest, image_meta, flavor)
4719 
4720         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
4721         if virt_type in ('qemu', 'kvm'):
4722             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
4723 
4724         if virt_type in ('xen', 'qemu', 'kvm'):
4725             # Get all generic PCI devices (non-SR-IOV).
4726             for pci_dev in pci_manager.get_instance_pci_devs(instance):
4727                 guest.add_device(self._get_guest_pci_device(pci_dev))
4728         else:
4729             # PCI devices is only supported for hypervisor 'xen', 'qemu' and
4730             # 'kvm'.
4731             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
4732             if len(pci_devs) > 0:
4733                 raise exception.PciDeviceUnsupportedHypervisor(
4734                     type=virt_type)
4735 
4736         # image meta takes precedence over flavor extra specs; disable the
4737         # watchdog action by default
4738         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
4739                            or 'disabled')
4740         watchdog_action = image_meta.properties.get('hw_watchdog_action',
4741                                                     watchdog_action)
4742 
4743         # NB(sross): currently only actually supported by KVM/QEmu
4744         if watchdog_action != 'disabled':
4745             if watchdog_action in fields.WatchdogAction.ALL:
4746                 bark = vconfig.LibvirtConfigGuestWatchdog()
4747                 bark.action = watchdog_action
4748                 guest.add_device(bark)
4749             else:
4750                 raise exception.InvalidWatchdogAction(action=watchdog_action)
4751 
4752         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
4753         if (virt_type in ('xen', 'qemu', 'kvm') and
4754                 CONF.libvirt.mem_stats_period_seconds > 0):
4755             balloon = vconfig.LibvirtConfigMemoryBalloon()
4756             if virt_type in ('qemu', 'kvm'):
4757                 balloon.model = 'virtio'
4758             else:
4759                 balloon.model = 'xen'
4760             balloon.period = CONF.libvirt.mem_stats_period_seconds
4761             guest.add_device(balloon)
4762 
4763         return guest
4764 
4765     def _get_guest_pointer_model(self, os_type, image_meta):
4766         pointer_model = image_meta.properties.get(
4767             'hw_pointer_model', CONF.pointer_model)
4768         if pointer_model is None and CONF.libvirt.use_usb_tablet:
4769             # TODO(sahid): We set pointer_model to keep compatibility
4770             # until the next release O*. It means operators can continue
4771             # to use the deprecated option "use_usb_tablet" or set a
4772             # specific device to use
4773             pointer_model = "usbtablet"
4774             LOG.warning(_LW('The option "use_usb_tablet" has been '
4775                             'deprecated for Newton in favor of the more '
4776                             'generic "pointer_model". Please update '
4777                             'nova.conf to address this change.'))
4778 
4779         if pointer_model == "usbtablet":
4780             # We want a tablet if VNC is enabled, or SPICE is enabled and
4781             # the SPICE agent is disabled. If the SPICE agent is enabled
4782             # it provides a paravirt mouse which drastically reduces
4783             # overhead (by eliminating USB polling).
4784             if CONF.vnc.enabled or (
4785                     CONF.spice.enabled and not CONF.spice.agent_enabled):
4786                 return self._get_guest_usb_tablet(os_type)
4787             else:
4788                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
4789                     # For backward compatibility We don't want to break
4790                     # process of booting an instance if host is configured
4791                     # to use USB tablet without VNC or SPICE and SPICE
4792                     # agent disable.
4793                     LOG.warning(_LW('USB tablet requested for guests by host '
4794                                     'configuration. In order to accept this '
4795                                     'request VNC should be enabled or SPICE '
4796                                     'and SPICE agent disabled on host.'))
4797                 else:
4798                     raise exception.UnsupportedPointerModelRequested(
4799                         model="usbtablet")
4800 
4801     def _get_guest_usb_tablet(self, os_type):
4802         tablet = None
4803         if os_type == fields.VMMode.HVM:
4804             tablet = vconfig.LibvirtConfigGuestInput()
4805             tablet.type = "tablet"
4806             tablet.bus = "usb"
4807         else:
4808             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
4809                 # For backward compatibility We don't want to break
4810                 # process of booting an instance if virtual machine mode
4811                 # is not configured as HVM.
4812                 LOG.warning(_LW('USB tablet requested for guests by host '
4813                                 'configuration. In order to accept this '
4814                                 'request the machine mode should be '
4815                                 'configured as HVM.'))
4816             else:
4817                 raise exception.UnsupportedPointerModelRequested(
4818                     model="usbtablet")
4819         return tablet
4820 
4821     def _get_guest_xml(self, context, instance, network_info, disk_info,
4822                        image_meta, rescue=None,
4823                        block_device_info=None):
4824         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
4825         # this ahead of time so that we don't acquire it while also
4826         # holding the logging lock.
4827         network_info_str = str(network_info)
4828         msg = ('Start _get_guest_xml '
4829                'network_info=%(network_info)s '
4830                'disk_info=%(disk_info)s '
4831                'image_meta=%(image_meta)s rescue=%(rescue)s '
4832                'block_device_info=%(block_device_info)s' %
4833                {'network_info': network_info_str, 'disk_info': disk_info,
4834                 'image_meta': image_meta, 'rescue': rescue,
4835                 'block_device_info': block_device_info})
4836         # NOTE(mriedem): block_device_info can contain auth_password so we
4837         # need to sanitize the password in the message.
4838         LOG.debug(strutils.mask_password(msg), instance=instance)
4839         conf = self._get_guest_config(instance, network_info, image_meta,
4840                                       disk_info, rescue, block_device_info,
4841                                       context)
4842         xml = conf.to_xml()
4843 
4844         LOG.debug('End _get_guest_xml xml=%(xml)s',
4845                   {'xml': xml}, instance=instance)
4846         return xml
4847 
4848     def get_info(self, instance):
4849         """Retrieve information from libvirt for a specific instance name.
4850 
4851         If a libvirt error is encountered during lookup, we might raise a
4852         NotFound exception or Error exception depending on how severe the
4853         libvirt error is.
4854 
4855         """
4856         guest = self._host.get_guest(instance)
4857         # Kind of ugly but we need to pass host to get_info as for a
4858         # workaround, see libvirt/compat.py
4859         return guest.get_info(self._host)
4860 
4861     def _create_domain_setup_lxc(self, instance, image_meta,
4862                                  block_device_info, disk_info):
4863         inst_path = libvirt_utils.get_instance_path(instance)
4864         block_device_mapping = driver.block_device_info_get_mapping(
4865             block_device_info)
4866         root_disk = block_device.get_root_bdm(block_device_mapping)
4867         if root_disk:
4868             disk_info = blockinfo.get_info_from_bdm(
4869                 instance, CONF.libvirt.virt_type, image_meta, root_disk)
4870             self._connect_volume(root_disk['connection_info'], disk_info)
4871             disk_path = root_disk['connection_info']['data']['device_path']
4872 
4873             # NOTE(apmelton) - Even though the instance is being booted from a
4874             # cinder volume, it is still presented as a local block device.
4875             # LocalBlockImage is used here to indicate that the instance's
4876             # disk is backed by a local block device.
4877             image_model = imgmodel.LocalBlockImage(disk_path)
4878         else:
4879             root_disk = self.image_backend.by_name(instance, 'disk')
4880             image_model = root_disk.get_model(self._conn)
4881 
4882         container_dir = os.path.join(inst_path, 'rootfs')
4883         fileutils.ensure_tree(container_dir)
4884         rootfs_dev = disk_api.setup_container(image_model,
4885                                               container_dir=container_dir)
4886 
4887         try:
4888             # Save rootfs device to disconnect it when deleting the instance
4889             if rootfs_dev:
4890                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
4891             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
4892                 id_maps = self._get_guest_idmaps()
4893                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
4894         except Exception:
4895             with excutils.save_and_reraise_exception():
4896                 self._create_domain_cleanup_lxc(instance)
4897 
4898     def _create_domain_cleanup_lxc(self, instance):
4899         inst_path = libvirt_utils.get_instance_path(instance)
4900         container_dir = os.path.join(inst_path, 'rootfs')
4901 
4902         try:
4903             state = self.get_info(instance).state
4904         except exception.InstanceNotFound:
4905             # The domain may not be present if the instance failed to start
4906             state = None
4907 
4908         if state == power_state.RUNNING:
4909             # NOTE(uni): Now the container is running with its own private
4910             # mount namespace and so there is no need to keep the container
4911             # rootfs mounted in the host namespace
4912             LOG.debug('Attempting to unmount container filesystem: %s',
4913                       container_dir, instance=instance)
4914             disk_api.clean_lxc_namespace(container_dir=container_dir)
4915         else:
4916             disk_api.teardown_container(container_dir=container_dir)
4917 
4918     @contextlib.contextmanager
4919     def _lxc_disk_handler(self, instance, image_meta,
4920                           block_device_info, disk_info):
4921         """Context manager to handle the pre and post instance boot,
4922            LXC specific disk operations.
4923 
4924            An image or a volume path will be prepared and setup to be
4925            used by the container, prior to starting it.
4926            The disk will be disconnected and unmounted if a container has
4927            failed to start.
4928         """
4929 
4930         if CONF.libvirt.virt_type != 'lxc':
4931             yield
4932             return
4933 
4934         self._create_domain_setup_lxc(instance, image_meta,
4935                                       block_device_info, disk_info)
4936 
4937         try:
4938             yield
4939         finally:
4940             self._create_domain_cleanup_lxc(instance)
4941 
4942     # TODO(sahid): Consider renaming this to _create_guest.
4943     def _create_domain(self, xml=None, domain=None,
4944                        power_on=True, pause=False, post_xml_callback=None):
4945         """Create a domain.
4946 
4947         Either domain or xml must be passed in. If both are passed, then
4948         the domain definition is overwritten from the xml.
4949 
4950         :returns guest.Guest: Guest just created
4951         """
4952         if xml:
4953             guest = libvirt_guest.Guest.create(xml, self._host)
4954             if post_xml_callback is not None:
4955                 post_xml_callback()
4956         else:
4957             guest = libvirt_guest.Guest(domain)
4958 
4959         if power_on or pause:
4960             guest.launch(pause=pause)
4961 
4962         if not utils.is_neutron():
4963             guest.enable_hairpin()
4964 
4965         return guest
4966 
4967     def _neutron_failed_callback(self, event_name, instance):
4968         LOG.error(_LE('Neutron Reported failure on event '
4969                       '%(event)s for instance %(uuid)s'),
4970                   {'event': event_name, 'uuid': instance.uuid},
4971                   instance=instance)
4972         if CONF.vif_plugging_is_fatal:
4973             raise exception.VirtualInterfaceCreateException()
4974 
4975     def _get_neutron_events(self, network_info):
4976         # NOTE(danms): We need to collect any VIFs that are currently
4977         # down that we expect a down->up event for. Anything that is
4978         # already up will not undergo that transition, and for
4979         # anything that might be stale (cache-wise) assume it's
4980         # already up so we don't block on it.
4981         return [('network-vif-plugged', vif['id'])
4982                 for vif in network_info if vif.get('active', True) is False]
4983 
4984     def _create_domain_and_network(self, context, xml, instance, network_info,
4985                                    disk_info, block_device_info=None,
4986                                    power_on=True, reboot=False,
4987                                    vifs_already_plugged=False,
4988                                    post_xml_callback=None):
4989 
4990         """Do required network setup and create domain."""
4991         block_device_mapping = driver.block_device_info_get_mapping(
4992             block_device_info)
4993 
4994         for vol in block_device_mapping:
4995             connection_info = vol['connection_info']
4996 
4997             if (not reboot and 'data' in connection_info and
4998                     'volume_id' in connection_info['data']):
4999                 volume_id = connection_info['data']['volume_id']
5000                 encryption = encryptors.get_encryption_metadata(
5001                     context, self._volume_api, volume_id, connection_info)
5002 
5003                 if encryption:
5004                     encryptor = self._get_volume_encryptor(connection_info,
5005                                                            encryption)
5006                     encryptor.attach_volume(context, **encryption)
5007 
5008         timeout = CONF.vif_plugging_timeout
5009         if (self._conn_supports_start_paused and
5010             utils.is_neutron() and not
5011             vifs_already_plugged and power_on and timeout):
5012             events = self._get_neutron_events(network_info)
5013         else:
5014             events = []
5015 
5016         pause = bool(events)
5017         guest = None
5018         try:
5019             with self.virtapi.wait_for_instance_event(
5020                     instance, events, deadline=timeout,
5021                     error_callback=self._neutron_failed_callback):
5022                 self.plug_vifs(instance, network_info)
5023                 self.firewall_driver.setup_basic_filtering(instance,
5024                                                            network_info)
5025                 self.firewall_driver.prepare_instance_filter(instance,
5026                                                              network_info)
5027                 with self._lxc_disk_handler(instance, instance.image_meta,
5028                                             block_device_info, disk_info):
5029                     guest = self._create_domain(
5030                         xml, pause=pause, power_on=power_on,
5031                         post_xml_callback=post_xml_callback)
5032 
5033                 self.firewall_driver.apply_instance_filter(instance,
5034                                                            network_info)
5035         except exception.VirtualInterfaceCreateException:
5036             # Neutron reported failure and we didn't swallow it, so
5037             # bail here
5038             with excutils.save_and_reraise_exception():
5039                 if guest:
5040                     guest.poweroff()
5041                 self.cleanup(context, instance, network_info=network_info,
5042                              block_device_info=block_device_info)
5043         except eventlet.timeout.Timeout:
5044             # We never heard from Neutron
5045             LOG.warning(_LW('Timeout waiting for vif plugging callback for '
5046                          'instance %(uuid)s'), {'uuid': instance.uuid},
5047                      instance=instance)
5048             if CONF.vif_plugging_is_fatal:
5049                 if guest:
5050                     guest.poweroff()
5051                 self.cleanup(context, instance, network_info=network_info,
5052                              block_device_info=block_device_info)
5053                 raise exception.VirtualInterfaceCreateException()
5054 
5055         # Resume only if domain has been paused
5056         if pause:
5057             guest.resume()
5058         return guest
5059 
5060     def _get_vcpu_total(self):
5061         """Get available vcpu number of physical computer.
5062 
5063         :returns: the number of cpu core instances can be used.
5064 
5065         """
5066         try:
5067             total_pcpus = self._host.get_cpu_count()
5068         except libvirt.libvirtError:
5069             LOG.warning(_LW("Cannot get the number of cpu, because this "
5070                          "function is not implemented for this platform. "))
5071             return 0
5072 
5073         if not CONF.vcpu_pin_set:
5074             return total_pcpus
5075 
5076         available_ids = hardware.get_vcpu_pin_set()
5077         # We get the list of online CPUs on the host and see if the requested
5078         # set falls under these. If not, we retain the old behavior.
5079         online_pcpus = None
5080         try:
5081             online_pcpus = self._host.get_online_cpus()
5082         except libvirt.libvirtError as ex:
5083             error_code = ex.get_error_code()
5084             LOG.warning(
5085                 _LW("Couldn't retrieve the online CPUs due to a Libvirt "
5086                     "error: %(error)s with error code: %(error_code)s"),
5087                 {'error': ex, 'error_code': error_code})
5088         if online_pcpus:
5089             if not (available_ids <= online_pcpus):
5090                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5091                          "specified cpuset is not online. Online cpuset(s): "
5092                          "%(online)s, requested cpuset(s): %(req)s"),
5093                        {'online': sorted(online_pcpus),
5094                         'req': sorted(available_ids)})
5095                 raise exception.Invalid(msg)
5096         elif sorted(available_ids)[-1] >= total_pcpus:
5097             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5098                                       "out of hypervisor cpu range."))
5099         return len(available_ids)
5100 
5101     @staticmethod
5102     def _get_local_gb_info():
5103         """Get local storage info of the compute node in GB.
5104 
5105         :returns: A dict containing:
5106              :total: How big the overall usable filesystem is (in gigabytes)
5107              :free: How much space is free (in gigabytes)
5108              :used: How much space is used (in gigabytes)
5109         """
5110 
5111         if CONF.libvirt.images_type == 'lvm':
5112             info = lvm.get_volume_group_info(
5113                                CONF.libvirt.images_volume_group)
5114         elif CONF.libvirt.images_type == 'rbd':
5115             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5116         elif CONF.libvirt.images_type == 'sio':
5117             info = LibvirtDriver._get_sio_driver().get_pool_info()
5118         else:
5119             info = libvirt_utils.get_fs_info(CONF.instances_path)
5120 
5121         for (k, v) in six.iteritems(info):
5122             info[k] = v / units.Gi
5123 
5124         return info
5125 
5126     def _get_vcpu_used(self):
5127         """Get vcpu usage number of physical computer.
5128 
5129         :returns: The total number of vcpu(s) that are currently being used.
5130 
5131         """
5132 
5133         total = 0
5134         if CONF.libvirt.virt_type == 'lxc':
5135             return total + 1
5136 
5137         for guest in self._host.list_guests():
5138             try:
5139                 vcpus = guest.get_vcpus_info()
5140                 if vcpus is not None:
5141                     total += len(list(vcpus))
5142             except libvirt.libvirtError as e:
5143                 LOG.warning(
5144                     _LW("couldn't obtain the vcpu count from domain id:"
5145                         " %(uuid)s, exception: %(ex)s"),
5146                     {"uuid": guest.uuid, "ex": e})
5147             # NOTE(gtt116): give other tasks a chance.
5148             greenthread.sleep(0)
5149         return total
5150 
5151     def _get_instance_capabilities(self):
5152         """Get hypervisor instance capabilities
5153 
5154         Returns a list of tuples that describe instances the
5155         hypervisor is capable of hosting.  Each tuple consists
5156         of the triplet (arch, hypervisor_type, vm_mode).
5157 
5158         :returns: List of tuples describing instance capabilities
5159         """
5160         caps = self._host.get_capabilities()
5161         instance_caps = list()
5162         for g in caps.guests:
5163             for dt in g.domtype:
5164                 instance_cap = (
5165                     fields.Architecture.canonicalize(g.arch),
5166                     fields.HVType.canonicalize(dt),
5167                     fields.VMMode.canonicalize(g.ostype))
5168                 instance_caps.append(instance_cap)
5169 
5170         return instance_caps
5171 
5172     def _get_cpu_info(self):
5173         """Get cpuinfo information.
5174 
5175         Obtains cpu feature from virConnect.getCapabilities.
5176 
5177         :return: see above description
5178 
5179         """
5180 
5181         caps = self._host.get_capabilities()
5182         cpu_info = dict()
5183 
5184         cpu_info['arch'] = caps.host.cpu.arch
5185         cpu_info['model'] = caps.host.cpu.model
5186         cpu_info['vendor'] = caps.host.cpu.vendor
5187 
5188         topology = dict()
5189         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5190         topology['sockets'] = caps.host.cpu.sockets
5191         topology['cores'] = caps.host.cpu.cores
5192         topology['threads'] = caps.host.cpu.threads
5193         cpu_info['topology'] = topology
5194 
5195         features = set()
5196         for f in caps.host.cpu.features:
5197             features.add(f.name)
5198         cpu_info['features'] = features
5199         return cpu_info
5200 
5201     def _get_pcidev_info(self, devname):
5202         """Returns a dict of PCI device."""
5203 
5204         def _get_device_type(cfgdev, pci_address):
5205             """Get a PCI device's device type.
5206 
5207             An assignable PCI device can be a normal PCI device,
5208             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5209             Function (VF). Only normal PCI devices or SR-IOV VFs
5210             are assignable, while SR-IOV PFs are always owned by
5211             hypervisor.
5212             """
5213             for fun_cap in cfgdev.pci_capability.fun_capability:
5214                 if fun_cap.type == 'virt_functions':
5215                     return {
5216                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5217                     }
5218                 if (fun_cap.type == 'phys_function' and
5219                     len(fun_cap.device_addrs) != 0):
5220                     phys_address = "%04x:%02x:%02x.%01x" % (
5221                         fun_cap.device_addrs[0][0],
5222                         fun_cap.device_addrs[0][1],
5223                         fun_cap.device_addrs[0][2],
5224                         fun_cap.device_addrs[0][3])
5225                     return {
5226                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5227                         'parent_addr': phys_address,
5228                     }
5229 
5230             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5231             # only when VFs are enabled. The check below is a workaround
5232             # to get the correct report regardless of whether or not any
5233             # VFs are enabled for the device.
5234             if not self._host.has_min_version(
5235                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5236                 is_physical_function = pci_utils.is_physical_function(
5237                     *pci_utils.get_pci_address_fields(pci_address))
5238                 if is_physical_function:
5239                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5240 
5241             return {'dev_type': fields.PciDeviceType.STANDARD}
5242 
5243         virtdev = self._host.device_lookup_by_name(devname)
5244         xmlstr = virtdev.XMLDesc(0)
5245         cfgdev = vconfig.LibvirtConfigNodeDevice()
5246         cfgdev.parse_str(xmlstr)
5247 
5248         address = "%04x:%02x:%02x.%1x" % (
5249             cfgdev.pci_capability.domain,
5250             cfgdev.pci_capability.bus,
5251             cfgdev.pci_capability.slot,
5252             cfgdev.pci_capability.function)
5253 
5254         device = {
5255             "dev_id": cfgdev.name,
5256             "address": address,
5257             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5258             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5259             }
5260 
5261         device["numa_node"] = cfgdev.pci_capability.numa_node
5262 
5263         # requirement by DataBase Model
5264         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5265         device.update(_get_device_type(cfgdev, address))
5266         return device
5267 
5268     def _get_pci_passthrough_devices(self):
5269         """Get host PCI devices information.
5270 
5271         Obtains pci devices information from libvirt, and returns
5272         as a JSON string.
5273 
5274         Each device information is a dictionary, with mandatory keys
5275         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5276         'label' and other optional device specific information.
5277 
5278         Refer to the objects/pci_device.py for more idea of these keys.
5279 
5280         :returns: a JSON string containing a list of the assignable PCI
5281                   devices information
5282         """
5283         # Bail early if we know we can't support `listDevices` to avoid
5284         # repeated warnings within a periodic task
5285         if not getattr(self, '_list_devices_supported', True):
5286             return jsonutils.dumps([])
5287 
5288         try:
5289             dev_names = self._host.list_pci_devices() or []
5290         except libvirt.libvirtError as ex:
5291             error_code = ex.get_error_code()
5292             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5293                 self._list_devices_supported = False
5294                 LOG.warning(_LW("URI %(uri)s does not support "
5295                              "listDevices: %(error)s"),
5296                              {'uri': self._uri(), 'error': ex})
5297                 return jsonutils.dumps([])
5298             else:
5299                 raise
5300 
5301         pci_info = []
5302         for name in dev_names:
5303             pci_info.append(self._get_pcidev_info(name))
5304 
5305         return jsonutils.dumps(pci_info)
5306 
5307     def _has_numa_support(self):
5308         # This means that the host can support LibvirtConfigGuestNUMATune
5309         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
5310         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
5311             if self._host.has_version(ver):
5312                 if not getattr(self, '_bad_libvirt_numa_version_warn', False):
5313                     LOG.warning(_LW('You are running with libvirt version %s '
5314                                  'which is known to have broken NUMA support. '
5315                                  'Consider patching or updating libvirt on '
5316                                  'this host if you need NUMA support.'),
5317                              self._version_to_string(ver))
5318                     self._bad_libvirt_numa_version_warn = True
5319                 return False
5320 
5321         support_matrix = {
5322             (fields.Architecture.I686,
5323              fields.Architecture.X86_64,
5324              fields.Architecture.AARCH64): MIN_LIBVIRT_NUMA_VERSION,
5325             (fields.Architecture.PPC64,
5326              fields.Architecture.PPC64LE): MIN_LIBVIRT_NUMA_VERSION_PPC}
5327         caps = self._host.get_capabilities()
5328         is_supported = False
5329         for archs, libvirt_ver in support_matrix.items():
5330             if ((caps.host.cpu.arch in archs) and
5331                     self._host.has_min_version(libvirt_ver,
5332                                                MIN_QEMU_NUMA_HUGEPAGE_VERSION,
5333                                                host.HV_DRIVER_QEMU)):
5334                 is_supported = True
5335         return is_supported
5336 
5337     def _has_hugepage_support(self):
5338         # This means that the host can support multiple values for the size
5339         # field in LibvirtConfigGuestMemoryBackingPage
5340         supported_archs = [fields.Architecture.I686,
5341                            fields.Architecture.X86_64,
5342                            fields.Architecture.AARCH64,
5343                            fields.Architecture.PPC64LE,
5344                            fields.Architecture.PPC64]
5345         caps = self._host.get_capabilities()
5346         return ((caps.host.cpu.arch in supported_archs) and
5347                 self._host.has_min_version(MIN_LIBVIRT_HUGEPAGE_VERSION,
5348                                            MIN_QEMU_NUMA_HUGEPAGE_VERSION,
5349                                            host.HV_DRIVER_QEMU))
5350 
5351     def _get_host_numa_topology(self):
5352         if not self._has_numa_support():
5353             return
5354 
5355         caps = self._host.get_capabilities()
5356         topology = caps.host.topology
5357 
5358         if topology is None or not topology.cells:
5359             return
5360 
5361         cells = []
5362         allowed_cpus = hardware.get_vcpu_pin_set()
5363         online_cpus = self._host.get_online_cpus()
5364         if allowed_cpus:
5365             allowed_cpus &= online_cpus
5366         else:
5367             allowed_cpus = online_cpus
5368 
5369         def _get_reserved_memory_for_cell(self, cell_id, page_size):
5370             cell = self._reserved_hugepages.get(cell_id, {})
5371             return cell.get(page_size, 0)
5372 
5373         for cell in topology.cells:
5374             cpuset = set(cpu.id for cpu in cell.cpus)
5375             siblings = sorted(map(set,
5376                                   set(tuple(cpu.siblings)
5377                                         if cpu.siblings else ()
5378                                       for cpu in cell.cpus)
5379                                   ))
5380             cpuset &= allowed_cpus
5381             siblings = [sib & allowed_cpus for sib in siblings]
5382             # Filter out singles and empty sibling sets that may be left
5383             siblings = [sib for sib in siblings if len(sib) > 1]
5384 
5385             mempages = []
5386             if self._has_hugepage_support():
5387                 mempages = [
5388                     objects.NUMAPagesTopology(
5389                         size_kb=pages.size,
5390                         total=pages.total,
5391                         used=0,
5392                         reserved=_get_reserved_memory_for_cell(
5393                             self, cell.id, pages.size))
5394                     for pages in cell.mempages]
5395 
5396             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
5397                                     memory=cell.memory / units.Ki,
5398                                     cpu_usage=0, memory_usage=0,
5399                                     siblings=siblings,
5400                                     pinned_cpus=set([]),
5401                                     mempages=mempages)
5402             cells.append(cell)
5403 
5404         return objects.NUMATopology(cells=cells)
5405 
5406     def get_all_volume_usage(self, context, compute_host_bdms):
5407         """Return usage info for volumes attached to vms on
5408            a given host.
5409         """
5410         vol_usage = []
5411 
5412         for instance_bdms in compute_host_bdms:
5413             instance = instance_bdms['instance']
5414 
5415             for bdm in instance_bdms['instance_bdms']:
5416                 mountpoint = bdm['device_name']
5417                 if mountpoint.startswith('/dev/'):
5418                     mountpoint = mountpoint[5:]
5419                 volume_id = bdm['volume_id']
5420 
5421                 LOG.debug("Trying to get stats for the volume %s",
5422                           volume_id, instance=instance)
5423                 vol_stats = self.block_stats(instance, mountpoint)
5424 
5425                 if vol_stats:
5426                     stats = dict(volume=volume_id,
5427                                  instance=instance,
5428                                  rd_req=vol_stats[0],
5429                                  rd_bytes=vol_stats[1],
5430                                  wr_req=vol_stats[2],
5431                                  wr_bytes=vol_stats[3])
5432                     LOG.debug(
5433                         "Got volume usage stats for the volume=%(volume)s,"
5434                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
5435                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
5436                         stats, instance=instance)
5437                     vol_usage.append(stats)
5438 
5439         return vol_usage
5440 
5441     def block_stats(self, instance, disk_id):
5442         """Note that this function takes an instance name."""
5443         try:
5444             guest = self._host.get_guest(instance)
5445 
5446             # TODO(sahid): We are converting all calls from a
5447             # virDomain object to use nova.virt.libvirt.Guest.
5448             # We should be able to remove domain at the end.
5449             domain = guest._domain
5450             return domain.blockStats(disk_id)
5451         except libvirt.libvirtError as e:
5452             errcode = e.get_error_code()
5453             LOG.info(_LI('Getting block stats failed, device might have '
5454                          'been detached. Instance=%(instance_name)s '
5455                          'Disk=%(disk)s Code=%(errcode)s Error=%(e)s'),
5456                      {'instance_name': instance.name, 'disk': disk_id,
5457                       'errcode': errcode, 'e': e},
5458                      instance=instance)
5459         except exception.InstanceNotFound:
5460             LOG.info(_LI('Could not find domain in libvirt for instance %s. '
5461                          'Cannot get block stats for device'), instance.name,
5462                      instance=instance)
5463 
5464     def get_console_pool_info(self, console_type):
5465         # TODO(mdragon): console proxy should be implemented for libvirt,
5466         #                in case someone wants to use it with kvm or
5467         #                such. For now return fake data.
5468         return {'address': '127.0.0.1',
5469                 'username': 'fakeuser',
5470                 'password': 'fakepassword'}
5471 
5472     def refresh_security_group_rules(self, security_group_id):
5473         self.firewall_driver.refresh_security_group_rules(security_group_id)
5474 
5475     def refresh_instance_security_rules(self, instance):
5476         self.firewall_driver.refresh_instance_security_rules(instance)
5477 
5478     def get_available_resource(self, nodename):
5479         """Retrieve resource information.
5480 
5481         This method is called when nova-compute launches, and
5482         as part of a periodic task that records the results in the DB.
5483 
5484         :param nodename: unused in this driver
5485         :returns: dictionary containing resource info
5486         """
5487 
5488         disk_info_dict = self._get_local_gb_info()
5489         data = {}
5490 
5491         # NOTE(dprince): calling capabilities before getVersion works around
5492         # an initialization issue with some versions of Libvirt (1.0.5.5).
5493         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
5494         # See: https://bugs.launchpad.net/nova/+bug/1215593
5495         data["supported_instances"] = self._get_instance_capabilities()
5496 
5497         data["vcpus"] = self._get_vcpu_total()
5498         data["memory_mb"] = self._host.get_memory_mb_total()
5499         data["local_gb"] = disk_info_dict['total']
5500         data["vcpus_used"] = self._get_vcpu_used()
5501         data["memory_mb_used"] = self._host.get_memory_mb_used()
5502         data["local_gb_used"] = disk_info_dict['used']
5503         data["hypervisor_type"] = self._host.get_driver_type()
5504         data["hypervisor_version"] = self._host.get_version()
5505         data["hypervisor_hostname"] = self._host.get_hostname()
5506         # TODO(berrange): why do we bother converting the
5507         # libvirt capabilities XML into a special JSON format ?
5508         # The data format is different across all the drivers
5509         # so we could just return the raw capabilities XML
5510         # which 'compare_cpu' could use directly
5511         #
5512         # That said, arch_filter.py now seems to rely on
5513         # the libvirt drivers format which suggests this
5514         # data format needs to be standardized across drivers
5515         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
5516 
5517         disk_free_gb = disk_info_dict['free']
5518         disk_over_committed = self._get_disk_over_committed_size_total()
5519         available_least = disk_free_gb * units.Gi - disk_over_committed
5520         data['disk_available_least'] = available_least / units.Gi
5521 
5522         data['pci_passthrough_devices'] = \
5523             self._get_pci_passthrough_devices()
5524 
5525         numa_topology = self._get_host_numa_topology()
5526         if numa_topology:
5527             data['numa_topology'] = numa_topology._to_json()
5528         else:
5529             data['numa_topology'] = None
5530 
5531         return data
5532 
5533     def check_instance_shared_storage_local(self, context, instance):
5534         """Check if instance files located on shared storage.
5535 
5536         This runs check on the destination host, and then calls
5537         back to the source host to check the results.
5538 
5539         :param context: security context
5540         :param instance: nova.objects.instance.Instance object
5541         :returns:
5542          - tempfile: A dict containing the tempfile info on the destination
5543                      host
5544          - None:
5545 
5546             1. If the instance path is not existing.
5547             2. If the image backend is shared block storage type.
5548         """
5549         if self.image_backend.backend().is_shared_block_storage():
5550             return None
5551 
5552         dirpath = libvirt_utils.get_instance_path(instance)
5553 
5554         if not os.path.exists(dirpath):
5555             return None
5556 
5557         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
5558         LOG.debug("Creating tmpfile %s to verify with other "
5559                   "compute node that the instance is on "
5560                   "the same shared storage.",
5561                   tmp_file, instance=instance)
5562         os.close(fd)
5563         return {"filename": tmp_file}
5564 
5565     def check_instance_shared_storage_remote(self, context, data):
5566         return os.path.exists(data['filename'])
5567 
5568     def check_instance_shared_storage_cleanup(self, context, data):
5569         fileutils.delete_if_exists(data["filename"])
5570 
5571     def check_can_live_migrate_destination(self, context, instance,
5572                                            src_compute_info, dst_compute_info,
5573                                            block_migration=False,
5574                                            disk_over_commit=False):
5575         """Check if it is possible to execute live migration.
5576 
5577         This runs checks on the destination host, and then calls
5578         back to the source host to check the results.
5579 
5580         :param context: security context
5581         :param instance: nova.db.sqlalchemy.models.Instance
5582         :param block_migration: if true, prepare for block migration
5583         :param disk_over_commit: if true, allow disk over commit
5584         :returns: a LibvirtLiveMigrateData object
5585         """
5586         disk_available_gb = dst_compute_info['disk_available_least']
5587         disk_available_mb = (
5588             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
5589 
5590         # Compare CPU
5591         if not instance.vcpu_model or not instance.vcpu_model.model:
5592             source_cpu_info = src_compute_info['cpu_info']
5593             self._compare_cpu(None, source_cpu_info, instance)
5594         else:
5595             self._compare_cpu(instance.vcpu_model, None, instance)
5596 
5597         # Create file on storage, to be checked on source host
5598         filename = self._create_shared_storage_test_file(instance)
5599 
5600         data = objects.LibvirtLiveMigrateData()
5601         data.filename = filename
5602         data.image_type = CONF.libvirt.images_type
5603         data.graphics_listen_addr_vnc = CONF.vnc.vncserver_listen
5604         data.graphics_listen_addr_spice = CONF.spice.server_listen
5605         if CONF.serial_console.enabled:
5606             data.serial_listen_addr = CONF.serial_console.proxyclient_address
5607         else:
5608             data.serial_listen_addr = None
5609         # Notes(eliqiao): block_migration and disk_over_commit are not
5610         # nullable, so just don't set them if they are None
5611         if block_migration is not None:
5612             data.block_migration = block_migration
5613         if disk_over_commit is not None:
5614             data.disk_over_commit = disk_over_commit
5615         data.disk_available_mb = disk_available_mb
5616         return data
5617 
5618     def cleanup_live_migration_destination_check(self, context,
5619                                                  dest_check_data):
5620         """Do required cleanup on dest host after check_can_live_migrate calls
5621 
5622         :param context: security context
5623         """
5624         filename = dest_check_data.filename
5625         self._cleanup_shared_storage_test_file(filename)
5626 
5627     def check_can_live_migrate_source(self, context, instance,
5628                                       dest_check_data,
5629                                       block_device_info=None):
5630         """Check if it is possible to execute live migration.
5631 
5632         This checks if the live migration can succeed, based on the
5633         results from check_can_live_migrate_destination.
5634 
5635         :param context: security context
5636         :param instance: nova.db.sqlalchemy.models.Instance
5637         :param dest_check_data: result of check_can_live_migrate_destination
5638         :param block_device_info: result of _get_instance_block_device_info
5639         :returns: a LibvirtLiveMigrateData object
5640         """
5641         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
5642             md_obj = objects.LibvirtLiveMigrateData()
5643             md_obj.from_legacy_dict(dest_check_data)
5644             dest_check_data = md_obj
5645 
5646         # Checking shared storage connectivity
5647         # if block migration, instances_paths should not be on shared storage.
5648         source = CONF.host
5649 
5650         dest_check_data.is_shared_instance_path = (
5651             self._check_shared_storage_test_file(
5652                 dest_check_data.filename, instance))
5653 
5654         dest_check_data.is_shared_block_storage = (
5655             self._is_shared_block_storage(instance, dest_check_data,
5656                                           block_device_info))
5657 
5658         if 'block_migration' not in dest_check_data:
5659             dest_check_data.block_migration = (
5660                 not dest_check_data.is_on_shared_storage())
5661 
5662         if dest_check_data.block_migration:
5663             # TODO(eliqiao): Once block_migration flag is removed from the API
5664             # we can safely remove the if condition
5665             if dest_check_data.is_on_shared_storage():
5666                 reason = _("Block migration can not be used "
5667                            "with shared storage.")
5668                 raise exception.InvalidLocalStorage(reason=reason, path=source)
5669             if 'disk_over_commit' in dest_check_data:
5670                 self._assert_dest_node_has_enough_disk(context, instance,
5671                                         dest_check_data.disk_available_mb,
5672                                         dest_check_data.disk_over_commit,
5673                                         block_device_info)
5674             if block_device_info:
5675                 bdm = block_device_info.get('block_device_mapping')
5676                 # NOTE(pkoniszewski): libvirt from version 1.2.17 upwards
5677                 # supports selective block device migration. It means that it
5678                 # is possible to define subset of block devices to be copied
5679                 # during migration. If they are not specified - block devices
5680                 # won't be migrated. However, it does not work when live
5681                 # migration is tunnelled through libvirt.
5682                 if bdm and not self._host.has_min_version(
5683                         MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
5684                     # NOTE(stpierre): if this instance has mapped volumes,
5685                     # we can't do a block migration, since that will result
5686                     # in volumes being copied from themselves to themselves,
5687                     # which is a recipe for disaster.
5688                     ver = ".".join([str(x) for x in
5689                                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION])
5690                     msg = (_('Cannot block migrate instance %(uuid)s with'
5691                              ' mapped volumes. Selective block device'
5692                              ' migration feature requires libvirt version'
5693                              ' %(libvirt_ver)s') %
5694                            {'uuid': instance.uuid, 'libvirt_ver': ver})
5695                     LOG.error(msg, instance=instance)
5696                     raise exception.MigrationPreCheckError(reason=msg)
5697                 # NOTE(eliqiao): Selective disk migrations are not supported
5698                 # with tunnelled block migrations so we can block them early.
5699                 if (bdm and
5700                     (self._block_migration_flags &
5701                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
5702                     msg = (_('Cannot block migrate instance %(uuid)s with'
5703                              ' mapped volumes. Selective block device'
5704                              ' migration is not supported with tunnelled'
5705                              ' block migrations.') % {'uuid': instance.uuid})
5706                     LOG.error(msg, instance=instance)
5707                     raise exception.MigrationPreCheckError(reason=msg)
5708         elif not (dest_check_data.is_shared_block_storage or
5709                   dest_check_data.is_shared_instance_path):
5710             reason = _("Live migration can not be used "
5711                        "without shared storage except "
5712                        "a booted from volume VM which "
5713                        "does not have a local disk.")
5714             raise exception.InvalidSharedStorage(reason=reason, path=source)
5715 
5716         # NOTE(mikal): include the instance directory name here because it
5717         # doesn't yet exist on the destination but we want to force that
5718         # same name to be used
5719         instance_path = libvirt_utils.get_instance_path(instance,
5720                                                         relative=True)
5721         dest_check_data.instance_relative_path = instance_path
5722 
5723         return dest_check_data
5724 
5725     def _is_shared_block_storage(self, instance, dest_check_data,
5726                                  block_device_info=None):
5727         """Check if all block storage of an instance can be shared
5728         between source and destination of a live migration.
5729 
5730         Returns true if the instance is volume backed and has no local disks,
5731         or if the image backend is the same on source and destination and the
5732         backend shares block storage between compute nodes.
5733 
5734         :param instance: nova.objects.instance.Instance object
5735         :param dest_check_data: dict with boolean fields image_type,
5736                                 is_shared_instance_path, and is_volume_backed
5737         """
5738         if (dest_check_data.obj_attr_is_set('image_type') and
5739                 CONF.libvirt.images_type == dest_check_data.image_type and
5740                 self.image_backend.backend().is_shared_block_storage()):
5741             return True
5742 
5743         if (dest_check_data.is_shared_instance_path and
5744                 self.image_backend.backend().is_file_in_instance_path()):
5745             # NOTE(angdraug): file based image backends (Flat, Qcow2)
5746             # place block device files under the instance path
5747             return True
5748 
5749         if (dest_check_data.is_volume_backed and
5750                 not bool(jsonutils.loads(
5751                     self.get_instance_disk_info(instance,
5752                                                 block_device_info)))):
5753             return True
5754 
5755         return False
5756 
5757     def _assert_dest_node_has_enough_disk(self, context, instance,
5758                                              available_mb, disk_over_commit,
5759                                              block_device_info=None):
5760         """Checks if destination has enough disk for block migration."""
5761         # Libvirt supports qcow2 disk format,which is usually compressed
5762         # on compute nodes.
5763         # Real disk image (compressed) may enlarged to "virtual disk size",
5764         # that is specified as the maximum disk size.
5765         # (See qemu-img -f path-to-disk)
5766         # Scheduler recognizes destination host still has enough disk space
5767         # if real disk size < available disk size
5768         # if disk_over_commit is True,
5769         #  otherwise virtual disk size < available disk size.
5770 
5771         available = 0
5772         if available_mb:
5773             available = available_mb * units.Mi
5774 
5775         ret = self.get_instance_disk_info(instance,
5776                                           block_device_info=block_device_info)
5777         disk_infos = jsonutils.loads(ret)
5778 
5779         necessary = 0
5780         if disk_over_commit:
5781             for info in disk_infos:
5782                 necessary += int(info['disk_size'])
5783         else:
5784             for info in disk_infos:
5785                 necessary += int(info['virt_disk_size'])
5786 
5787         # Check that available disk > necessary disk
5788         if (available - necessary) < 0:
5789             reason = (_('Unable to migrate %(instance_uuid)s: '
5790                         'Disk of instance is too large(available'
5791                         ' on destination host:%(available)s '
5792                         '< need:%(necessary)s)') %
5793                       {'instance_uuid': instance.uuid,
5794                        'available': available,
5795                        'necessary': necessary})
5796             raise exception.MigrationPreCheckError(reason=reason)
5797 
5798     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
5799         """Check the host is compatible with the requested CPU
5800 
5801         :param guest_cpu: nova.objects.VirtCPUModel or None
5802         :param host_cpu_str: JSON from _get_cpu_info() method
5803 
5804         If the 'guest_cpu' parameter is not None, this will be
5805         validated for migration compatibility with the host.
5806         Otherwise the 'host_cpu_str' JSON string will be used for
5807         validation.
5808 
5809         :returns:
5810             None. if given cpu info is not compatible to this server,
5811             raise exception.
5812         """
5813 
5814         # NOTE(kchamart): Comparing host to guest CPU model for emulated
5815         # guests (<domain type='qemu'>) should not matter -- in this
5816         # mode (QEMU "TCG") the CPU is fully emulated in software and no
5817         # hardware acceleration, like KVM, is involved. So, skip the CPU
5818         # compatibility check for the QEMU domain type, and retain it for
5819         # KVM guests.
5820         if CONF.libvirt.virt_type not in ['kvm']:
5821             return
5822 
5823         if guest_cpu is None:
5824             info = jsonutils.loads(host_cpu_str)
5825             LOG.info(_LI('Instance launched has CPU info: %s'), host_cpu_str)
5826             cpu = vconfig.LibvirtConfigCPU()
5827             cpu.arch = info['arch']
5828             cpu.model = info['model']
5829             cpu.vendor = info['vendor']
5830             cpu.sockets = info['topology']['sockets']
5831             cpu.cores = info['topology']['cores']
5832             cpu.threads = info['topology']['threads']
5833             for f in info['features']:
5834                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
5835         else:
5836             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
5837 
5838         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
5839              "virCPUCompareResult")
5840         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
5841         # unknown character exists in xml, then libvirt complains
5842         try:
5843             cpu_xml = cpu.to_xml()
5844             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
5845             ret = self._host.compare_cpu(cpu_xml)
5846         except libvirt.libvirtError as e:
5847             error_code = e.get_error_code()
5848             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5849                 LOG.debug("URI %(uri)s does not support cpu comparison. "
5850                           "It will be proceeded though. Error: %(error)s",
5851                           {'uri': self._uri(), 'error': e})
5852                 return
5853             else:
5854                 LOG.error(m, {'ret': e, 'u': u})
5855                 raise exception.MigrationPreCheckError(
5856                     reason=m % {'ret': e, 'u': u})
5857 
5858         if ret <= 0:
5859             LOG.error(m, {'ret': ret, 'u': u})
5860             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
5861 
5862     def _create_shared_storage_test_file(self, instance):
5863         """Makes tmpfile under CONF.instances_path."""
5864         dirpath = CONF.instances_path
5865         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
5866         LOG.debug("Creating tmpfile %s to notify to other "
5867                   "compute nodes that they should mount "
5868                   "the same storage.", tmp_file, instance=instance)
5869         os.close(fd)
5870         return os.path.basename(tmp_file)
5871 
5872     def _check_shared_storage_test_file(self, filename, instance):
5873         """Confirms existence of the tmpfile under CONF.instances_path.
5874 
5875         Cannot confirm tmpfile return False.
5876         """
5877         # NOTE(tpatzig): if instances_path is a shared volume that is
5878         # under heavy IO (many instances on many compute nodes),
5879         # then checking the existence of the testfile fails,
5880         # just because it takes longer until the client refreshes and new
5881         # content gets visible.
5882         # os.utime (like touch) on the directory forces the client to refresh.
5883         os.utime(CONF.instances_path, None)
5884 
5885         tmp_file = os.path.join(CONF.instances_path, filename)
5886         if not os.path.exists(tmp_file):
5887             exists = False
5888         else:
5889             exists = True
5890         LOG.debug('Check if temp file %s exists to indicate shared storage '
5891                   'is being used for migration. Exists? %s', tmp_file, exists,
5892                   instance=instance)
5893         return exists
5894 
5895     def _cleanup_shared_storage_test_file(self, filename):
5896         """Removes existence of the tmpfile under CONF.instances_path."""
5897         tmp_file = os.path.join(CONF.instances_path, filename)
5898         os.remove(tmp_file)
5899 
5900     def ensure_filtering_rules_for_instance(self, instance, network_info):
5901         """Ensure that an instance's filtering rules are enabled.
5902 
5903         When migrating an instance, we need the filtering rules to
5904         be configured on the destination host before starting the
5905         migration.
5906 
5907         Also, when restarting the compute service, we need to ensure
5908         that filtering rules exist for all running services.
5909         """
5910 
5911         self.firewall_driver.setup_basic_filtering(instance, network_info)
5912         self.firewall_driver.prepare_instance_filter(instance,
5913                 network_info)
5914 
5915         # nwfilters may be defined in a separate thread in the case
5916         # of libvirt non-blocking mode, so we wait for completion
5917         timeout_count = list(range(CONF.live_migration_retry_count))
5918         while timeout_count:
5919             if self.firewall_driver.instance_filter_exists(instance,
5920                                                            network_info):
5921                 break
5922             timeout_count.pop()
5923             if len(timeout_count) == 0:
5924                 msg = _('The firewall filter for %s does not exist')
5925                 raise exception.NovaException(msg % instance.name)
5926             greenthread.sleep(1)
5927 
5928     def filter_defer_apply_on(self):
5929         self.firewall_driver.filter_defer_apply_on()
5930 
5931     def filter_defer_apply_off(self):
5932         self.firewall_driver.filter_defer_apply_off()
5933 
5934     def live_migration(self, context, instance, dest,
5935                        post_method, recover_method, block_migration=False,
5936                        migrate_data=None):
5937         """Spawning live_migration operation for distributing high-load.
5938 
5939         :param context: security context
5940         :param instance:
5941             nova.db.sqlalchemy.models.Instance object
5942             instance object that is migrated.
5943         :param dest: destination host
5944         :param post_method:
5945             post operation method.
5946             expected nova.compute.manager._post_live_migration.
5947         :param recover_method:
5948             recovery method when any exception occurs.
5949             expected nova.compute.manager._rollback_live_migration.
5950         :param block_migration: if true, do block migration.
5951         :param migrate_data: a LibvirtLiveMigrateData object
5952 
5953         """
5954 
5955         # 'dest' will be substituted into 'migration_uri' so ensure
5956         # it does't contain any characters that could be used to
5957         # exploit the URI accepted by libivrt
5958         if not libvirt_utils.is_valid_hostname(dest):
5959             raise exception.InvalidHostname(hostname=dest)
5960 
5961         self._live_migration(context, instance, dest,
5962                              post_method, recover_method, block_migration,
5963                              migrate_data)
5964 
5965     def live_migration_abort(self, instance):
5966         """Aborting a running live-migration.
5967 
5968         :param instance: instance object that is in migration
5969 
5970         """
5971 
5972         guest = self._host.get_guest(instance)
5973         dom = guest._domain
5974 
5975         try:
5976             dom.abortJob()
5977         except libvirt.libvirtError as e:
5978             LOG.error(_LE("Failed to cancel migration %s"),
5979                       e, instance=instance)
5980             raise
5981 
5982     def _verify_serial_console_is_disabled(self):
5983         if CONF.serial_console.enabled:
5984 
5985             msg = _('Your destination node does not support'
5986                     ' retrieving listen addresses.  In order'
5987                     ' for live migration to work properly you'
5988                     ' must disable serial console.')
5989             raise exception.MigrationError(reason=msg)
5990 
5991     def _live_migration_operation(self, context, instance, dest,
5992                                   block_migration, migrate_data, guest,
5993                                   device_names):
5994         """Invoke the live migration operation
5995 
5996         :param context: security context
5997         :param instance:
5998             nova.db.sqlalchemy.models.Instance object
5999             instance object that is migrated.
6000         :param dest: destination host
6001         :param block_migration: if true, do block migration.
6002         :param migrate_data: a LibvirtLiveMigrateData object
6003         :param guest: the guest domain object
6004         :param device_names: list of device names that are being migrated with
6005             instance
6006 
6007         This method is intended to be run in a background thread and will
6008         block that thread until the migration is finished or failed.
6009         """
6010         try:
6011             if migrate_data.block_migration:
6012                 migration_flags = self._block_migration_flags
6013             else:
6014                 migration_flags = self._live_migration_flags
6015 
6016             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6017                 migrate_data)
6018             if not serial_listen_addr:
6019                 # In this context we want to ensure that serial console is
6020                 # disabled on source node. This is because nova couldn't
6021                 # retrieve serial listen address from destination node, so we
6022                 # consider that destination node might have serial console
6023                 # disabled as well.
6024                 self._verify_serial_console_is_disabled()
6025 
6026             # NOTE(aplanas) migrate_uri will have a value only in the
6027             # case that `live_migration_inbound_addr` parameter is
6028             # set, and we propose a non tunneled migration.
6029             migrate_uri = None
6030             if ('target_connect_addr' in migrate_data and
6031                     migrate_data.target_connect_addr is not None):
6032                 dest = migrate_data.target_connect_addr
6033                 if (migration_flags &
6034                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6035                     migrate_uri = self._migrate_uri(dest)
6036 
6037             params = None
6038             new_xml_str = libvirt_migrate.get_updated_guest_xml(
6039                 # TODO(sahid): It's not a really good idea to pass
6040                 # the method _get_volume_config and we should to find
6041                 # a way to avoid this in future.
6042                 guest, migrate_data, self._get_volume_config)
6043             if self._host.has_min_version(
6044                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6045                 params = {
6046                     'bandwidth': CONF.libvirt.live_migration_bandwidth,
6047                     'destination_xml': new_xml_str,
6048                     'migrate_disks': device_names,
6049                 }
6050                 # NOTE(pkoniszewski): Because of precheck which blocks
6051                 # tunnelled block live migration with mapped volumes we
6052                 # can safely remove migrate_disks when tunnelling is on.
6053                 # Otherwise we will block all tunnelled block migrations,
6054                 # even when an instance does not have volumes mapped.
6055                 # This is because selective disk migration is not
6056                 # supported in tunnelled block live migration. Also we
6057                 # cannot fallback to migrateToURI2 in this case because of
6058                 # bug #1398999
6059                 if (migration_flags &
6060                     libvirt.VIR_MIGRATE_TUNNELLED != 0):
6061                     params.pop('migrate_disks')
6062 
6063             # TODO(sahid): This should be in
6064             # post_live_migration_at_source but no way to retrieve
6065             # ports acquired on the host for the guest at this
6066             # step. Since the domain is going to be removed from
6067             # libvird on source host after migration, we backup the
6068             # serial ports to release them if all went well.
6069             serial_ports = []
6070             if CONF.serial_console.enabled:
6071                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6072 
6073             guest.migrate(self._live_migration_uri(dest),
6074                           migrate_uri=migrate_uri,
6075                           flags=migration_flags,
6076                           params=params,
6077                           domain_xml=new_xml_str,
6078                           bandwidth=CONF.libvirt.live_migration_bandwidth)
6079 
6080             for hostname, port in serial_ports:
6081                 serial_console.release_port(host=hostname, port=port)
6082         except Exception as e:
6083             with excutils.save_and_reraise_exception():
6084                 LOG.error(_LE("Live Migration failure: %s"), e,
6085                           instance=instance)
6086 
6087         # If 'migrateToURI' fails we don't know what state the
6088         # VM instances on each host are in. Possibilities include
6089         #
6090         #  1. src==running, dst==none
6091         #
6092         #     Migration failed & rolled back, or never started
6093         #
6094         #  2. src==running, dst==paused
6095         #
6096         #     Migration started but is still ongoing
6097         #
6098         #  3. src==paused,  dst==paused
6099         #
6100         #     Migration data transfer completed, but switchover
6101         #     is still ongoing, or failed
6102         #
6103         #  4. src==paused,  dst==running
6104         #
6105         #     Migration data transfer completed, switchover
6106         #     happened but cleanup on source failed
6107         #
6108         #  5. src==none,    dst==running
6109         #
6110         #     Migration fully succeeded.
6111         #
6112         # Libvirt will aim to complete any migration operation
6113         # or roll it back. So even if the migrateToURI call has
6114         # returned an error, if the migration was not finished
6115         # libvirt should clean up.
6116         #
6117         # So we take the error raise here with a pinch of salt
6118         # and rely on the domain job info status to figure out
6119         # what really happened to the VM, which is a much more
6120         # reliable indicator.
6121         #
6122         # In particular we need to try very hard to ensure that
6123         # Nova does not "forget" about the guest. ie leaving it
6124         # running on a different host to the one recorded in
6125         # the database, as that would be a serious resource leak
6126 
6127         LOG.debug("Migration operation thread has finished",
6128                   instance=instance)
6129 
6130     @staticmethod
6131     def _migration_downtime_steps(data_gb):
6132         '''Calculate downtime value steps and time between increases.
6133 
6134         :param data_gb: total GB of RAM and disk to transfer
6135 
6136         This looks at the total downtime steps and upper bound
6137         downtime value and uses an exponential backoff. So initially
6138         max downtime is increased by small amounts, and as time goes
6139         by it is increased by ever larger amounts
6140 
6141         For example, with 10 steps, 30 second step delay, 3 GB
6142         of RAM and 400ms target maximum downtime, the downtime will
6143         be increased every 90 seconds in the following progression:
6144 
6145         -   0 seconds -> set downtime to  37ms
6146         -  90 seconds -> set downtime to  38ms
6147         - 180 seconds -> set downtime to  39ms
6148         - 270 seconds -> set downtime to  42ms
6149         - 360 seconds -> set downtime to  46ms
6150         - 450 seconds -> set downtime to  55ms
6151         - 540 seconds -> set downtime to  70ms
6152         - 630 seconds -> set downtime to  98ms
6153         - 720 seconds -> set downtime to 148ms
6154         - 810 seconds -> set downtime to 238ms
6155         - 900 seconds -> set downtime to 400ms
6156 
6157         This allows the guest a good chance to complete migration
6158         with a small downtime value.
6159         '''
6160         downtime = CONF.libvirt.live_migration_downtime
6161         steps = CONF.libvirt.live_migration_downtime_steps
6162         delay = CONF.libvirt.live_migration_downtime_delay
6163 
6164         downtime_min = nova.conf.libvirt.LIVE_MIGRATION_DOWNTIME_MIN
6165         steps_min = nova.conf.libvirt.LIVE_MIGRATION_DOWNTIME_STEPS_MIN
6166         delay_min = nova.conf.libvirt.LIVE_MIGRATION_DOWNTIME_DELAY_MIN
6167 
6168         # TODO(hieulq): Need to move min/max value into the config option,
6169         # currently oslo_config will raise ValueError instead of setting
6170         # option value to its min/max.
6171         if downtime < downtime_min:
6172             LOG.warning(_LW("Config option live_migration_downtime's value "
6173                             "is less than minimum value %dms, rounded up to "
6174                             "the minimum value and will raise ValueError in "
6175                             "the future release."), downtime_min)
6176             downtime = downtime_min
6177 
6178         if steps < steps_min:
6179             LOG.warning(_LW("Config option live_migration_downtime_steps's "
6180                             "value is less than minimum value %dms, rounded "
6181                             "up to the minimum value and will raise "
6182                             "ValueError in the future release."), steps_min)
6183             steps = steps_min
6184         if delay < delay_min:
6185             LOG.warning(_LW("Config option live_migration_downtime_delay's "
6186                             "value is less than minimum value %dms, rounded "
6187                             "up to the minimum value and will raise "
6188                             "ValueError in the future release."), delay_min)
6189             delay = delay_min
6190         delay = int(delay * data_gb)
6191 
6192         offset = downtime / float(steps + 1)
6193         base = (downtime - offset) ** (1 / float(steps))
6194 
6195         for i in range(steps + 1):
6196             yield (int(delay * i), int(offset + base ** i))
6197 
6198     def _live_migration_copy_disk_paths(self, context, instance, guest):
6199         '''Get list of disks to copy during migration
6200 
6201         :param context: security context
6202         :param instance: the instance being migrated
6203         :param guest: the Guest instance being migrated
6204 
6205         Get the list of disks to copy during migration.
6206 
6207         :returns: a list of local source paths and a list of device names to
6208             copy
6209         '''
6210 
6211         disk_paths = []
6212         device_names = []
6213         block_devices = []
6214 
6215         # TODO(pkoniszewski): Remove version check when we bump min libvirt
6216         # version to >= 1.2.17.
6217         if (self._block_migration_flags &
6218                 libvirt.VIR_MIGRATE_TUNNELLED == 0 and
6219                 self._host.has_min_version(
6220                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION)):
6221             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
6222                 context, instance.uuid)
6223             block_device_info = driver.get_block_device_info(instance,
6224                                                              bdm_list)
6225 
6226             block_device_mappings = driver.block_device_info_get_mapping(
6227                 block_device_info)
6228             for bdm in block_device_mappings:
6229                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
6230                 block_devices.append(device_name)
6231 
6232         for dev in guest.get_all_disks():
6233             if dev.readonly or dev.shareable:
6234                 continue
6235             if dev.source_type not in ["file", "block"]:
6236                 continue
6237             if dev.target_dev in block_devices:
6238                 continue
6239             disk_paths.append(dev.source_path)
6240             device_names.append(dev.target_dev)
6241         return (disk_paths, device_names)
6242 
6243     def _live_migration_data_gb(self, instance, disk_paths):
6244         '''Calculate total amount of data to be transferred
6245 
6246         :param instance: the nova.objects.Instance being migrated
6247         :param disk_paths: list of disk paths that are being migrated
6248         with instance
6249 
6250         Calculates the total amount of data that needs to be
6251         transferred during the live migration. The actual
6252         amount copied will be larger than this, due to the
6253         guest OS continuing to dirty RAM while the migration
6254         is taking place. So this value represents the minimal
6255         data size possible.
6256 
6257         :returns: data size to be copied in GB
6258         '''
6259 
6260         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
6261         if ram_gb < 2:
6262             ram_gb = 2
6263 
6264         disk_gb = 0
6265         for path in disk_paths:
6266             try:
6267                 size = os.stat(path).st_size
6268                 size_gb = (size / units.Gi)
6269                 if size_gb < 2:
6270                     size_gb = 2
6271                 disk_gb += size_gb
6272             except OSError as e:
6273                 LOG.warning(_LW("Unable to stat %(disk)s: %(ex)s"),
6274                          {'disk': path, 'ex': e})
6275                 # Ignore error since we don't want to break
6276                 # the migration monitoring thread operation
6277 
6278         return ram_gb + disk_gb
6279 
6280     def _get_migration_flags(self, is_block_migration):
6281         if is_block_migration:
6282             return self._block_migration_flags
6283         return self._live_migration_flags
6284 
6285     def _live_migration_monitor(self, context, instance, guest,
6286                                 dest, post_method,
6287                                 recover_method, block_migration,
6288                                 migrate_data, finish_event,
6289                                 disk_paths):
6290         on_migration_failure = deque()
6291         data_gb = self._live_migration_data_gb(instance, disk_paths)
6292         downtime_steps = list(self._migration_downtime_steps(data_gb))
6293         migration = migrate_data.migration
6294         curdowntime = None
6295 
6296         migration_flags = self._get_migration_flags(
6297                                   migrate_data.block_migration)
6298 
6299         n = 0
6300         start = time.time()
6301         progress_time = start
6302         progress_watermark = None
6303         previous_data_remaining = -1
6304         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
6305         while True:
6306             info = guest.get_job_info()
6307 
6308             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6309                 # Either still running, or failed or completed,
6310                 # lets untangle the mess
6311                 if not finish_event.ready():
6312                     LOG.debug("Operation thread is still running",
6313                               instance=instance)
6314                 else:
6315                     info.type = libvirt_migrate.find_job_type(guest, instance)
6316                     LOG.debug("Fixed incorrect job type to be %d",
6317                               info.type, instance=instance)
6318 
6319             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6320                 # Migration is not yet started
6321                 LOG.debug("Migration not running yet",
6322                           instance=instance)
6323             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
6324                 # Migration is still running
6325                 #
6326                 # This is where we wire up calls to change live
6327                 # migration status. eg change max downtime, cancel
6328                 # the operation, change max bandwidth
6329                 libvirt_migrate.run_tasks(guest, instance,
6330                                           self.active_migrations,
6331                                           on_migration_failure,
6332                                           migration,
6333                                           is_post_copy_enabled)
6334 
6335                 now = time.time()
6336                 elapsed = now - start
6337 
6338                 if ((progress_watermark is None) or
6339                     (progress_watermark == 0) or
6340                     (progress_watermark > info.data_remaining)):
6341                     progress_watermark = info.data_remaining
6342                     progress_time = now
6343 
6344                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
6345                 completion_timeout = int(
6346                     CONF.libvirt.live_migration_completion_timeout * data_gb)
6347                 if libvirt_migrate.should_abort(instance, now, progress_time,
6348                                                 progress_timeout, elapsed,
6349                                                 completion_timeout,
6350                                                 migration.status):
6351                     try:
6352                         guest.abort_job()
6353                     except libvirt.libvirtError as e:
6354                         LOG.warning(_LW("Failed to abort migration %s"),
6355                                     e, instance=instance)
6356                         self._clear_empty_migration(instance)
6357                         raise
6358 
6359                 if (is_post_copy_enabled and
6360                     libvirt_migrate.should_switch_to_postcopy(
6361                     info.memory_iteration, info.data_remaining,
6362                     previous_data_remaining, migration.status)):
6363                     libvirt_migrate.trigger_postcopy_switch(guest,
6364                                                             instance,
6365                                                             migration)
6366                 previous_data_remaining = info.data_remaining
6367 
6368                 curdowntime = libvirt_migrate.update_downtime(
6369                     guest, instance, curdowntime,
6370                     downtime_steps, elapsed)
6371 
6372                 # We loop every 500ms, so don't log on every
6373                 # iteration to avoid spamming logs for long
6374                 # running migrations. Just once every 5 secs
6375                 # is sufficient for developers to debug problems.
6376                 # We log once every 30 seconds at info to help
6377                 # admins see slow running migration operations
6378                 # when debug logs are off.
6379                 if (n % 10) == 0:
6380                     # Ignoring memory_processed, as due to repeated
6381                     # dirtying of data, this can be way larger than
6382                     # memory_total. Best to just look at what's
6383                     # remaining to copy and ignore what's done already
6384                     #
6385                     # TODO(berrange) perhaps we could include disk
6386                     # transfer stats in the progress too, but it
6387                     # might make memory info more obscure as large
6388                     # disk sizes might dwarf memory size
6389                     remaining = 100
6390                     if info.memory_total != 0:
6391                         remaining = round(info.memory_remaining *
6392                                           100 / info.memory_total)
6393 
6394                     libvirt_migrate.save_stats(instance, migration,
6395                                                info, remaining)
6396 
6397                     lg = LOG.debug
6398                     if (n % 60) == 0:
6399                         lg = LOG.info
6400 
6401                     lg(_LI("Migration running for %(secs)d secs, "
6402                            "memory %(remaining)d%% remaining; "
6403                            "(bytes processed=%(processed_memory)d, "
6404                            "remaining=%(remaining_memory)d, "
6405                            "total=%(total_memory)d)"),
6406                        {"secs": n / 2, "remaining": remaining,
6407                         "processed_memory": info.memory_processed,
6408                         "remaining_memory": info.memory_remaining,
6409                         "total_memory": info.memory_total}, instance=instance)
6410                     if info.data_remaining > progress_watermark:
6411                         lg(_LI("Data remaining %(remaining)d bytes, "
6412                                "low watermark %(watermark)d bytes "
6413                                "%(last)d seconds ago"),
6414                            {"remaining": info.data_remaining,
6415                             "watermark": progress_watermark,
6416                             "last": (now - progress_time)}, instance=instance)
6417 
6418                 n = n + 1
6419             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
6420                 # Migration is all done
6421                 LOG.info(_LI("Migration operation has completed"),
6422                          instance=instance)
6423                 post_method(context, instance, dest, block_migration,
6424                             migrate_data)
6425                 break
6426             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
6427                 # Migration did not succeed
6428                 LOG.error(_LE("Migration operation has aborted"),
6429                           instance=instance)
6430                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6431                                                   on_migration_failure)
6432                 recover_method(context, instance, dest, migrate_data)
6433                 break
6434             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
6435                 # Migration was stopped by admin
6436                 LOG.warning(_LW("Migration operation was cancelled"),
6437                          instance=instance)
6438                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6439                                                   on_migration_failure)
6440                 recover_method(context, instance, dest, migrate_data,
6441                                migration_status='cancelled')
6442                 break
6443             else:
6444                 LOG.warning(_LW("Unexpected migration job type: %d"),
6445                          info.type, instance=instance)
6446 
6447             time.sleep(0.5)
6448         self._clear_empty_migration(instance)
6449 
6450     def _clear_empty_migration(self, instance):
6451         try:
6452             del self.active_migrations[instance.uuid]
6453         except KeyError:
6454             LOG.warning(_LW("There are no records in active migrations "
6455                             "for instance"), instance=instance)
6456 
6457     def _live_migration(self, context, instance, dest, post_method,
6458                         recover_method, block_migration,
6459                         migrate_data):
6460         """Do live migration.
6461 
6462         :param context: security context
6463         :param instance:
6464             nova.db.sqlalchemy.models.Instance object
6465             instance object that is migrated.
6466         :param dest: destination host
6467         :param post_method:
6468             post operation method.
6469             expected nova.compute.manager._post_live_migration.
6470         :param recover_method:
6471             recovery method when any exception occurs.
6472             expected nova.compute.manager._rollback_live_migration.
6473         :param block_migration: if true, do block migration.
6474         :param migrate_data: a LibvirtLiveMigrateData object
6475 
6476         This fires off a new thread to run the blocking migration
6477         operation, and then this thread monitors the progress of
6478         migration and controls its operation
6479         """
6480 
6481         guest = self._host.get_guest(instance)
6482 
6483         disk_paths = []
6484         device_names = []
6485         if migrate_data.block_migration:
6486             disk_paths, device_names = self._live_migration_copy_disk_paths(
6487                 context, instance, guest)
6488 
6489         opthread = utils.spawn(self._live_migration_operation,
6490                                      context, instance, dest,
6491                                      block_migration,
6492                                      migrate_data, guest,
6493                                      device_names)
6494 
6495         finish_event = eventlet.event.Event()
6496         self.active_migrations[instance.uuid] = deque()
6497 
6498         def thread_finished(thread, event):
6499             LOG.debug("Migration operation thread notification",
6500                       instance=instance)
6501             event.send()
6502         opthread.link(thread_finished, finish_event)
6503 
6504         # Let eventlet schedule the new thread right away
6505         time.sleep(0)
6506 
6507         try:
6508             LOG.debug("Starting monitoring of live migration",
6509                       instance=instance)
6510             self._live_migration_monitor(context, instance, guest, dest,
6511                                          post_method, recover_method,
6512                                          block_migration, migrate_data,
6513                                          finish_event, disk_paths)
6514         except Exception as ex:
6515             LOG.warning(_LW("Error monitoring migration: %(ex)s"),
6516                      {"ex": ex}, instance=instance, exc_info=True)
6517             raise
6518         finally:
6519             LOG.debug("Live migration monitoring is all done",
6520                       instance=instance)
6521 
6522     def _is_post_copy_enabled(self, migration_flags):
6523         if self._is_post_copy_available():
6524             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
6525                 return True
6526         return False
6527 
6528     def live_migration_force_complete(self, instance):
6529         try:
6530             self.active_migrations[instance.uuid].append('force-complete')
6531         except KeyError:
6532             raise exception.NoActiveMigrationForInstance(
6533                 instance_id=instance.uuid)
6534 
6535     def _try_fetch_image(self, context, path, image_id, instance,
6536                          fallback_from_host=None):
6537         try:
6538             libvirt_utils.fetch_image(context, path, image_id)
6539         except exception.ImageNotFound:
6540             if not fallback_from_host:
6541                 raise
6542             LOG.debug("Image %(image_id)s doesn't exist anymore on "
6543                       "image service, attempting to copy image "
6544                       "from %(host)s",
6545                       {'image_id': image_id, 'host': fallback_from_host})
6546             libvirt_utils.copy_image(src=path, dest=path,
6547                                      host=fallback_from_host,
6548                                      receive=True)
6549 
6550     def _fetch_instance_kernel_ramdisk(self, context, instance,
6551                                        fallback_from_host=None):
6552         """Download kernel and ramdisk for instance in instance directory."""
6553         instance_dir = libvirt_utils.get_instance_path(instance)
6554         if instance.kernel_id:
6555             kernel_path = os.path.join(instance_dir, 'kernel')
6556             # NOTE(dsanders): only fetch image if it's not available at
6557             # kernel_path. This also avoids ImageNotFound exception if
6558             # the image has been deleted from glance
6559             if not os.path.exists(kernel_path):
6560                 self._try_fetch_image(context,
6561                                       kernel_path,
6562                                       instance.kernel_id,
6563                                       instance, fallback_from_host)
6564             if instance.ramdisk_id:
6565                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
6566                 # NOTE(dsanders): only fetch image if it's not available at
6567                 # ramdisk_path. This also avoids ImageNotFound exception if
6568                 # the image has been deleted from glance
6569                 if not os.path.exists(ramdisk_path):
6570                     self._try_fetch_image(context,
6571                                           ramdisk_path,
6572                                           instance.ramdisk_id,
6573                                           instance, fallback_from_host)
6574 
6575     def rollback_live_migration_at_destination(self, context, instance,
6576                                                network_info,
6577                                                block_device_info,
6578                                                destroy_disks=True,
6579                                                migrate_data=None):
6580         """Clean up destination node after a failed live migration."""
6581         try:
6582             self.destroy(context, instance, network_info, block_device_info,
6583                          destroy_disks, migrate_data)
6584         finally:
6585             # NOTE(gcb): Failed block live migration may leave instance
6586             # directory at destination node, ensure it is always deleted.
6587             is_shared_instance_path = True
6588             if migrate_data:
6589                 is_shared_instance_path = migrate_data.is_shared_instance_path
6590                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
6591                     and migrate_data.serial_listen_ports):
6592                     # Releases serial ports reserved.
6593                     for port in migrate_data.serial_listen_ports:
6594                         serial_console.release_port(
6595                             host=migrate_data.serial_listen_addr, port=port)
6596 
6597             if not is_shared_instance_path:
6598                 instance_dir = libvirt_utils.get_instance_path_at_destination(
6599                     instance, migrate_data)
6600                 if os.path.exists(instance_dir):
6601                         shutil.rmtree(instance_dir)
6602 
6603     def pre_live_migration(self, context, instance, block_device_info,
6604                            network_info, disk_info, migrate_data):
6605         """Preparation live migration."""
6606         if disk_info is not None:
6607             disk_info = jsonutils.loads(disk_info)
6608 
6609         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
6610                   instance=instance)
6611         is_shared_block_storage = migrate_data.is_shared_block_storage
6612         is_shared_instance_path = migrate_data.is_shared_instance_path
6613         is_block_migration = migrate_data.block_migration
6614 
6615         if not is_shared_instance_path:
6616             instance_dir = libvirt_utils.get_instance_path_at_destination(
6617                             instance, migrate_data)
6618 
6619             if os.path.exists(instance_dir):
6620                 raise exception.DestinationDiskExists(path=instance_dir)
6621 
6622             LOG.debug('Creating instance directory: %s', instance_dir,
6623                       instance=instance)
6624             os.mkdir(instance_dir)
6625 
6626             # Recreate the disk.info file and in doing so stop the
6627             # imagebackend from recreating it incorrectly by inspecting the
6628             # contents of each file when using the Raw backend.
6629             if disk_info:
6630                 image_disk_info = {}
6631                 for info in disk_info:
6632                     image_file = os.path.basename(info['path'])
6633                     image_path = os.path.join(instance_dir, image_file)
6634                     image_disk_info[image_path] = info['type']
6635 
6636                 LOG.debug('Creating disk.info with the contents: %s',
6637                           image_disk_info, instance=instance)
6638 
6639                 image_disk_info_path = os.path.join(instance_dir,
6640                                                     'disk.info')
6641                 libvirt_utils.write_to_file(image_disk_info_path,
6642                                             jsonutils.dumps(image_disk_info))
6643 
6644             if is_shared_block_storage:
6645                 self.image_backend.backend().connect_disks(instance)
6646             else:
6647                 # Ensure images and backing files are present.
6648                 LOG.debug('Checking to make sure images and backing files are '
6649                           'present before live migration.', instance=instance)
6650                 self._create_images_and_backing(
6651                     context, instance, instance_dir, disk_info,
6652                     fallback_from_host=instance.host)
6653 
6654             if (configdrive.required_by(instance) and
6655                     CONF.config_drive_format == 'iso9660' and
6656                     (not is_shared_block_storage or
6657                      self._get_disk_config_image_type() !=
6658                      CONF.libvirt.images_type)):
6659                 # NOTE(pkoniszewski): Due to a bug in libvirt iso config
6660                 # drive needs to be copied to destination prior to
6661                 # migration when instance path is not shared and block
6662                 # storage is not shared. Files that are already present
6663                 # on destination are excluded from a list of files that
6664                 # need to be copied to destination. If we don't do that
6665                 # live migration will fail on copying iso config drive to
6666                 # destination and writing to read-only device.
6667                 # Please see bug/1246201 for more details.
6668                 src = "%s:%s/disk.config" % (instance.host, instance_dir)
6669                 self._remotefs.copy_file(src, instance_dir)
6670 
6671             if not is_block_migration:
6672                 # NOTE(angdraug): when block storage is shared between source
6673                 # and destination and instance path isn't (e.g. volume backed
6674                 # or rbd backed instance), instance path on destination has to
6675                 # be prepared
6676 
6677                 # Required by Quobyte CI
6678                 self._ensure_console_log_for_instance(instance)
6679 
6680                 # if image has kernel and ramdisk, just download
6681                 # following normal way.
6682                 self._fetch_instance_kernel_ramdisk(context, instance)
6683 
6684         # Establishing connection to volume server.
6685         block_device_mapping = driver.block_device_info_get_mapping(
6686             block_device_info)
6687 
6688         if len(block_device_mapping):
6689             LOG.debug('Connecting volumes before live migration.',
6690                       instance=instance)
6691 
6692         for bdm in block_device_mapping:
6693             connection_info = bdm['connection_info']
6694             disk_info = blockinfo.get_info_from_bdm(
6695                 instance, CONF.libvirt.virt_type,
6696                 instance.image_meta, bdm)
6697             self._connect_volume(connection_info, disk_info)
6698 
6699         # We call plug_vifs before the compute manager calls
6700         # ensure_filtering_rules_for_instance, to ensure bridge is set up
6701         # Retry operation is necessary because continuously request comes,
6702         # concurrent request occurs to iptables, then it complains.
6703         LOG.debug('Plugging VIFs before live migration.', instance=instance)
6704         max_retry = CONF.live_migration_retry_count
6705         for cnt in range(max_retry):
6706             try:
6707                 self.plug_vifs(instance, network_info)
6708                 break
6709             except processutils.ProcessExecutionError:
6710                 if cnt == max_retry - 1:
6711                     raise
6712                 else:
6713                     LOG.warning(_LW('plug_vifs() failed %(cnt)d. Retry up to '
6714                                  '%(max_retry)d.'),
6715                              {'cnt': cnt,
6716                               'max_retry': max_retry},
6717                              instance=instance)
6718                     greenthread.sleep(1)
6719 
6720         # Store vncserver_listen and latest disk device info
6721         if not migrate_data:
6722             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
6723         else:
6724             migrate_data.bdms = []
6725         # Store live_migration_inbound_addr
6726         migrate_data.target_connect_addr = \
6727             CONF.libvirt.live_migration_inbound_addr
6728         migrate_data.supported_perf_events = self._supported_perf_events
6729 
6730         migrate_data.serial_listen_ports = []
6731         if CONF.serial_console.enabled:
6732             num_ports = hardware.get_number_of_serial_ports(
6733                 instance.flavor, instance.image_meta)
6734             for port in six.moves.range(num_ports):
6735                 migrate_data.serial_listen_ports.append(
6736                     serial_console.acquire_port(
6737                         migrate_data.serial_listen_addr))
6738 
6739         for vol in block_device_mapping:
6740             connection_info = vol['connection_info']
6741             if connection_info.get('serial'):
6742                 disk_info = blockinfo.get_info_from_bdm(
6743                     instance, CONF.libvirt.virt_type,
6744                     instance.image_meta, vol)
6745 
6746                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
6747                 bdmi.serial = connection_info['serial']
6748                 bdmi.connection_info = connection_info
6749                 bdmi.bus = disk_info['bus']
6750                 bdmi.dev = disk_info['dev']
6751                 bdmi.type = disk_info['type']
6752                 bdmi.format = disk_info.get('format')
6753                 bdmi.boot_index = disk_info.get('boot_index')
6754                 migrate_data.bdms.append(bdmi)
6755 
6756         return migrate_data
6757 
6758     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
6759                                image_id, instance, size,
6760                                fallback_from_host=None):
6761         try:
6762             image.cache(fetch_func=fetch_func,
6763                         context=context,
6764                         filename=filename,
6765                         image_id=image_id,
6766                         size=size)
6767         except exception.ImageNotFound:
6768             if not fallback_from_host:
6769                 raise
6770             LOG.debug("Image %(image_id)s doesn't exist anymore "
6771                       "on image service, attempting to copy "
6772                       "image from %(host)s",
6773                       {'image_id': image_id, 'host': fallback_from_host},
6774                       instance=instance)
6775 
6776             def copy_from_host(target):
6777                 libvirt_utils.copy_image(src=target,
6778                                          dest=target,
6779                                          host=fallback_from_host,
6780                                          receive=True)
6781             image.cache(fetch_func=copy_from_host,
6782                         filename=filename)
6783 
6784     def _create_images_and_backing(self, context, instance, instance_dir,
6785                                    disk_info, fallback_from_host=None):
6786         """:param context: security context
6787            :param instance:
6788                nova.db.sqlalchemy.models.Instance object
6789                instance object that is migrated.
6790            :param instance_dir:
6791                instance path to use, calculated externally to handle block
6792                migrating an instance with an old style instance path
6793            :param disk_info:
6794                disk info specified in _get_instance_disk_info (list of dicts)
6795            :param fallback_from_host:
6796                host where we can retrieve images if the glance images are
6797                not available.
6798 
6799         """
6800         if not disk_info:
6801             disk_info = []
6802 
6803         for info in disk_info:
6804             base = os.path.basename(info['path'])
6805             # Get image type and create empty disk image, and
6806             # create backing file in case of qcow2.
6807             instance_disk = os.path.join(instance_dir, base)
6808             if not info['backing_file'] and not os.path.exists(instance_disk):
6809                 libvirt_utils.create_image(info['type'], instance_disk,
6810                                            info['virt_disk_size'])
6811             elif info['backing_file']:
6812                 # Creating backing file follows same way as spawning instances.
6813                 cache_name = os.path.basename(info['backing_file'])
6814 
6815                 disk = self.image_backend.by_name(instance, instance_disk,
6816                                                   CONF.libvirt.images_type)
6817                 if cache_name.startswith('ephemeral'):
6818                     # The argument 'size' is used by image.cache to
6819                     # validate disk size retrieved from cache against
6820                     # the instance disk size (should always return OK)
6821                     # and ephemeral_size is used by _create_ephemeral
6822                     # to build the image if the disk is not already
6823                     # cached.
6824                     disk.cache(
6825                         fetch_func=self._create_ephemeral,
6826                         fs_label=cache_name,
6827                         os_type=instance.os_type,
6828                         filename=cache_name,
6829                         size=info['virt_disk_size'],
6830                         ephemeral_size=info['virt_disk_size'] / units.Gi)
6831                 elif cache_name.startswith('swap'):
6832                     inst_type = instance.get_flavor()
6833                     swap_mb = inst_type.swap
6834                     disk.cache(fetch_func=self._create_swap,
6835                                 filename="swap_%s" % swap_mb,
6836                                 size=swap_mb * units.Mi,
6837                                 swap_mb=swap_mb)
6838                 else:
6839                     self._try_fetch_image_cache(disk,
6840                                                 libvirt_utils.fetch_image,
6841                                                 context, cache_name,
6842                                                 instance.image_ref,
6843                                                 instance,
6844                                                 info['virt_disk_size'],
6845                                                 fallback_from_host)
6846 
6847         # if disk has kernel and ramdisk, just download
6848         # following normal way.
6849         self._fetch_instance_kernel_ramdisk(
6850             context, instance, fallback_from_host=fallback_from_host)
6851 
6852     def post_live_migration(self, context, instance, block_device_info,
6853                             migrate_data=None):
6854         # Disconnect from volume server
6855         block_device_mapping = driver.block_device_info_get_mapping(
6856                 block_device_info)
6857         connector = self.get_volume_connector(instance)
6858         volume_api = self._volume_api
6859         for vol in block_device_mapping:
6860             # Retrieve connection info from Cinder's initialize_connection API.
6861             # The info returned will be accurate for the source server.
6862             volume_id = vol['connection_info']['serial']
6863             connection_info = volume_api.initialize_connection(context,
6864                                                                volume_id,
6865                                                                connector)
6866 
6867             # TODO(leeantho) The following multipath_id logic is temporary
6868             # and will be removed in the future once os-brick is updated
6869             # to handle multipath for drivers in a more efficient way.
6870             # For now this logic is needed to ensure the connection info
6871             # data is correct.
6872 
6873             # Pull out multipath_id from the bdm information. The
6874             # multipath_id can be placed into the connection info
6875             # because it is based off of the volume and will be the
6876             # same on the source and destination hosts.
6877             if 'multipath_id' in vol['connection_info']['data']:
6878                 multipath_id = vol['connection_info']['data']['multipath_id']
6879                 connection_info['data']['multipath_id'] = multipath_id
6880 
6881             disk_dev = vol['mount_device'].rpartition("/")[2]
6882             self._disconnect_volume(connection_info, disk_dev)
6883 
6884     def post_live_migration_at_source(self, context, instance, network_info):
6885         """Unplug VIFs from networks at source.
6886 
6887         :param context: security context
6888         :param instance: instance object reference
6889         :param network_info: instance network information
6890         """
6891         self.unplug_vifs(instance, network_info)
6892 
6893     def post_live_migration_at_destination(self, context,
6894                                            instance,
6895                                            network_info,
6896                                            block_migration=False,
6897                                            block_device_info=None):
6898         """Post operation of live migration at destination host.
6899 
6900         :param context: security context
6901         :param instance:
6902             nova.db.sqlalchemy.models.Instance object
6903             instance object that is migrated.
6904         :param network_info: instance network information
6905         :param block_migration: if true, post operation of block_migration.
6906         """
6907         guest = self._host.get_guest(instance)
6908 
6909         # TODO(sahid): In Ocata we have added the migration flag
6910         # VIR_MIGRATE_PERSIST_DEST to libvirt, which means that the
6911         # guest XML is going to be set in libvirtd on destination node
6912         # automatically. However we do not remove that part until P*
6913         # because during an upgrade, to ensure migrating instances
6914         # from node running Newton is still going to set the guest XML
6915         # in libvirtd on destination node.
6916 
6917         # Make sure we define the migrated instance in libvirt
6918         xml = guest.get_xml_desc()
6919         self._host.write_instance_config(xml)
6920 
6921     def _get_instance_disk_info(self, instance_name, xml,
6922                                 block_device_info=None):
6923         """Get the non-volume disk information from the domain xml
6924 
6925         :param str instance_name: the name of the instance (domain)
6926         :param str xml: the libvirt domain xml for the instance
6927         :param dict block_device_info: block device info for BDMs
6928         :returns disk_info: list of dicts with keys:
6929 
6930           * 'type': the disk type (str)
6931           * 'path': the disk path (str)
6932           * 'virt_disk_size': the virtual disk size (int)
6933           * 'backing_file': backing file of a disk image (str)
6934           * 'disk_size': physical disk size (int)
6935           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
6936         """
6937         block_device_mapping = driver.block_device_info_get_mapping(
6938             block_device_info)
6939 
6940         volume_devices = set()
6941         for vol in block_device_mapping:
6942             disk_dev = vol['mount_device'].rpartition("/")[2]
6943             volume_devices.add(disk_dev)
6944 
6945         no_block_devices = (
6946             block_device_info is not None and
6947             self.image_backend.backend().is_shared_block_storage())
6948 
6949         disk_info = []
6950         doc = etree.fromstring(xml)
6951 
6952         def find_nodes(doc, device_type):
6953             return (doc.findall('.//devices/%s' % device_type),
6954                     doc.findall('.//devices/%s/source' % device_type),
6955                     doc.findall('.//devices/%s/driver' % device_type),
6956                     doc.findall('.//devices/%s/target' % device_type))
6957 
6958         if (CONF.libvirt.virt_type == 'parallels' and
6959             doc.find('os/type').text == fields.VMMode.EXE):
6960             node_type = 'filesystem'
6961         else:
6962             node_type = 'disk'
6963 
6964         (disk_nodes, path_nodes,
6965          driver_nodes, target_nodes) = find_nodes(doc, node_type)
6966 
6967         for cnt, path_node in enumerate(path_nodes):
6968             disk_type = disk_nodes[cnt].get('type')
6969             path = path_node.get('file') or path_node.get('dev')
6970             if (node_type == 'filesystem'):
6971                 target = target_nodes[cnt].attrib['dir']
6972             else:
6973                 target = target_nodes[cnt].attrib['dev']
6974 
6975             if not path:
6976                 LOG.debug('skipping disk for %s as it does not have a path',
6977                           instance_name)
6978                 continue
6979 
6980             if disk_type not in ['file', 'block']:
6981                 LOG.debug('skipping disk because it looks like a volume', path)
6982                 continue
6983 
6984             if target in volume_devices:
6985                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
6986                           'volume', {'path': path, 'target': target})
6987                 continue
6988 
6989             if no_block_devices and disk_type == 'block':
6990                 LOG.debug('skipping disk %(path)s as it may belong to '
6991                           'used shared block storage')
6992                 continue
6993 
6994             # get the real disk size or
6995             # raise a localized error if image is unavailable
6996             if disk_type == 'file':
6997                 if driver_nodes[cnt].get('type') == 'ploop':
6998                     dk_size = 0
6999                     for dirpath, dirnames, filenames in os.walk(path):
7000                         for f in filenames:
7001                             fp = os.path.join(dirpath, f)
7002                             dk_size += os.path.getsize(fp)
7003                 else:
7004                     dk_size = int(os.path.getsize(path))
7005             elif disk_type == 'block' and block_device_info:
7006                 dk_size = lvm.get_volume_size(path)
7007             else:
7008                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7009                           'determine if volume',
7010                           {'path': path, 'target': target})
7011                 continue
7012 
7013             disk_type = driver_nodes[cnt].get('type')
7014 
7015             if disk_type in ("qcow2", "ploop"):
7016                 backing_file = libvirt_utils.get_disk_backing_file(path)
7017                 virt_size = disk_api.get_disk_size(path)
7018                 over_commit_size = int(virt_size) - dk_size
7019             else:
7020                 backing_file = ""
7021                 virt_size = dk_size
7022                 over_commit_size = 0
7023 
7024             disk_info.append({'type': disk_type,
7025                               'path': path,
7026                               'virt_disk_size': virt_size,
7027                               'backing_file': backing_file,
7028                               'disk_size': dk_size,
7029                               'over_committed_disk_size': over_commit_size})
7030         return disk_info
7031 
7032     def get_instance_disk_info(self, instance,
7033                                block_device_info=None):
7034         try:
7035             guest = self._host.get_guest(instance)
7036             xml = guest.get_xml_desc()
7037         except libvirt.libvirtError as ex:
7038             error_code = ex.get_error_code()
7039             LOG.warning(_LW('Error from libvirt while getting description of '
7040                          '%(instance_name)s: [Error Code %(error_code)s] '
7041                          '%(ex)s'),
7042                      {'instance_name': instance.name,
7043                       'error_code': error_code,
7044                       'ex': ex},
7045                      instance=instance)
7046             raise exception.InstanceNotFound(instance_id=instance.uuid)
7047 
7048         return jsonutils.dumps(
7049                 self._get_instance_disk_info(instance.name, xml,
7050                                              block_device_info))
7051 
7052     def _get_disk_over_committed_size_total(self):
7053         """Return total over committed disk size for all instances."""
7054         # Disk size that all instance uses : virtual_size - disk_size
7055         disk_over_committed_size = 0
7056         instance_domains = self._host.list_instance_domains()
7057         if not instance_domains:
7058             return disk_over_committed_size
7059 
7060         # Get all instance uuids
7061         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7062         ctx = nova_context.get_admin_context()
7063         # Get instance object list by uuid filter
7064         filters = {'uuid': instance_uuids}
7065         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7066         # getting called twice one is here and another in the
7067         # _update_available_resource method of resource_tracker. Since
7068         # _update_available_resource method is synchronized, there is a
7069         # possibility the instances list retrieved here to calculate
7070         # disk_over_committed_size would differ to the list you would get
7071         # in _update_available_resource method for calculating usages based
7072         # on instance utilization.
7073         local_instance_list = objects.InstanceList.get_by_filters(
7074             ctx, filters, use_slave=True)
7075         # Convert instance list to dictionary with instance uuid as key.
7076         local_instances = {inst.uuid: inst for inst in local_instance_list}
7077 
7078         # Get bdms by instance uuids
7079         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7080             ctx, instance_uuids)
7081 
7082         for dom in instance_domains:
7083             try:
7084                 guest = libvirt_guest.Guest(dom)
7085                 xml = guest.get_xml_desc()
7086 
7087                 block_device_info = None
7088                 if guest.uuid in local_instances \
7089                         and (bdms and guest.uuid in bdms):
7090                     # Get block device info for instance
7091                     block_device_info = driver.get_block_device_info(
7092                         local_instances[guest.uuid], bdms[guest.uuid])
7093 
7094                 disk_infos = self._get_instance_disk_info(guest.name, xml,
7095                                  block_device_info=block_device_info)
7096                 if not disk_infos:
7097                     continue
7098 
7099                 for info in disk_infos:
7100                     disk_over_committed_size += int(
7101                         info['over_committed_disk_size'])
7102             except libvirt.libvirtError as ex:
7103                 error_code = ex.get_error_code()
7104                 LOG.warning(_LW(
7105                     'Error from libvirt while getting description of '
7106                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s'
7107                 ), {'instance_name': guest.name,
7108                     'error_code': error_code,
7109                     'ex': ex})
7110             except OSError as e:
7111                 if e.errno in (errno.ENOENT, errno.ESTALE):
7112                     LOG.warning(_LW('Periodic task is updating the host stat, '
7113                                  'it is trying to get disk %(i_name)s, '
7114                                  'but disk file was removed by concurrent '
7115                                  'operations such as resize.'),
7116                                 {'i_name': guest.name})
7117                 elif e.errno == errno.EACCES:
7118                     LOG.warning(_LW('Periodic task is updating the host stat, '
7119                                  'it is trying to get disk %(i_name)s, '
7120                                  'but access is denied. It is most likely '
7121                                  'due to a VM that exists on the compute '
7122                                  'node but is not managed by Nova.'),
7123                              {'i_name': guest.name})
7124                 else:
7125                     raise
7126             except exception.VolumeBDMPathNotFound as e:
7127                 LOG.warning(_LW('Periodic task is updating the host stats, '
7128                              'it is trying to get disk info for %(i_name)s, '
7129                              'but the backing volume block device was removed '
7130                              'by concurrent operations such as resize. '
7131                              'Error: %(error)s'),
7132                          {'i_name': guest.name,
7133                           'error': e})
7134             # NOTE(gtt116): give other tasks a chance.
7135             greenthread.sleep(0)
7136         return disk_over_committed_size
7137 
7138     def unfilter_instance(self, instance, network_info):
7139         """See comments of same method in firewall_driver."""
7140         self.firewall_driver.unfilter_instance(instance,
7141                                                network_info=network_info)
7142 
7143     def get_available_nodes(self, refresh=False):
7144         return [self._host.get_hostname()]
7145 
7146     def get_host_cpu_stats(self):
7147         """Return the current CPU state of the host."""
7148         return self._host.get_cpu_stats()
7149 
7150     def get_host_uptime(self):
7151         """Returns the result of calling "uptime"."""
7152         out, err = utils.execute('env', 'LANG=C', 'uptime')
7153         return out
7154 
7155     def manage_image_cache(self, context, all_instances):
7156         """Manage the local cache of images."""
7157         self.image_cache_manager.update(context, all_instances)
7158 
7159     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
7160                                   shared_storage=False):
7161         """Used only for cleanup in case migrate_disk_and_power_off fails."""
7162         try:
7163             if os.path.exists(inst_base_resize):
7164                 utils.execute('rm', '-rf', inst_base)
7165                 utils.execute('mv', inst_base_resize, inst_base)
7166                 if not shared_storage:
7167                     self._remotefs.remove_dir(dest, inst_base)
7168         except Exception:
7169             pass
7170 
7171     def _is_storage_shared_with(self, dest, inst_base):
7172         # NOTE (rmk): There are two methods of determining whether we are
7173         #             on the same filesystem: the source and dest IP are the
7174         #             same, or we create a file on the dest system via SSH
7175         #             and check whether the source system can also see it.
7176         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
7177         #                it will always be shared storage
7178         if CONF.libvirt.images_type == 'rbd':
7179             return True
7180         shared_storage = (dest == self.get_host_ip_addr())
7181         if not shared_storage:
7182             tmp_file = uuid.uuid4().hex + '.tmp'
7183             tmp_path = os.path.join(inst_base, tmp_file)
7184 
7185             try:
7186                 self._remotefs.create_file(dest, tmp_path)
7187                 if os.path.exists(tmp_path):
7188                     shared_storage = True
7189                     os.unlink(tmp_path)
7190                 else:
7191                     self._remotefs.remove_file(dest, tmp_path)
7192             except Exception:
7193                 pass
7194         return shared_storage
7195 
7196     def migrate_disk_and_power_off(self, context, instance, dest,
7197                                    flavor, network_info,
7198                                    block_device_info=None,
7199                                    timeout=0, retry_interval=0):
7200         LOG.debug("Starting migrate_disk_and_power_off",
7201                    instance=instance)
7202 
7203         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
7204 
7205         # get_bdm_ephemeral_disk_size() will return 0 if the new
7206         # instance's requested block device mapping contain no
7207         # ephemeral devices. However, we still want to check if
7208         # the original instance's ephemeral_gb property was set and
7209         # ensure that the new requested flavor ephemeral size is greater
7210         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
7211                     instance.flavor.ephemeral_gb)
7212 
7213         # Checks if the migration needs a disk resize down.
7214         root_down = flavor.root_gb < instance.flavor.root_gb
7215         ephemeral_down = flavor.ephemeral_gb < eph_size
7216         booted_from_volume = self._is_booted_from_volume(block_device_info)
7217 
7218         if (root_down and not booted_from_volume) or ephemeral_down:
7219             reason = _("Unable to resize disk down.")
7220             raise exception.InstanceFaultRollback(
7221                 exception.ResizeError(reason=reason))
7222 
7223         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
7224         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
7225             reason = _("Migration is not supported for LVM backed instances")
7226             raise exception.InstanceFaultRollback(
7227                 exception.MigrationPreCheckError(reason=reason))
7228 
7229         # copy disks to destination
7230         # rename instance dir to +_resize at first for using
7231         # shared storage for instance dir (eg. NFS).
7232         inst_base = libvirt_utils.get_instance_path(instance)
7233         inst_base_resize = inst_base + "_resize"
7234         shared_storage = self._is_storage_shared_with(dest, inst_base)
7235 
7236         # try to create the directory on the remote compute node
7237         # if this fails we pass the exception up the stack so we can catch
7238         # failures here earlier
7239         if not shared_storage:
7240             try:
7241                 self._remotefs.create_dir(dest, inst_base)
7242             except processutils.ProcessExecutionError as e:
7243                 reason = _("not able to execute ssh command: %s") % e
7244                 raise exception.InstanceFaultRollback(
7245                     exception.ResizeError(reason=reason))
7246 
7247         self.power_off(instance, timeout, retry_interval)
7248 
7249         block_device_mapping = driver.block_device_info_get_mapping(
7250             block_device_info)
7251         for vol in block_device_mapping:
7252             connection_info = vol['connection_info']
7253             disk_dev = vol['mount_device'].rpartition("/")[2]
7254             self._disconnect_volume(connection_info, disk_dev)
7255 
7256         disk_info_text = self.get_instance_disk_info(
7257             instance, block_device_info=block_device_info)
7258         disk_info = jsonutils.loads(disk_info_text)
7259 
7260         try:
7261             utils.execute('mv', inst_base, inst_base_resize)
7262             # if we are migrating the instance with shared storage then
7263             # create the directory.  If it is a remote node the directory
7264             # has already been created
7265             if shared_storage:
7266                 dest = None
7267                 utils.execute('mkdir', '-p', inst_base)
7268 
7269             on_execute = lambda process: \
7270                 self.job_tracker.add_job(instance, process.pid)
7271             on_completion = lambda process: \
7272                 self.job_tracker.remove_job(instance, process.pid)
7273 
7274             for info in disk_info:
7275                 # assume inst_base == dirname(info['path'])
7276                 img_path = info['path']
7277                 fname = os.path.basename(img_path)
7278                 from_path = os.path.join(inst_base_resize, fname)
7279 
7280                 # We will not copy over the swap disk here, and rely on
7281                 # finish_migration to re-create it for us. This is ok because
7282                 # the OS is shut down, and as recreating a swap disk is very
7283                 # cheap it is more efficient than copying either locally or
7284                 # over the network. This also means we don't have to resize it.
7285                 if fname == 'disk.swap':
7286                     continue
7287 
7288                 compression = info['type'] not in NO_COMPRESSION_TYPES
7289                 libvirt_utils.copy_image(from_path, img_path, host=dest,
7290                                          on_execute=on_execute,
7291                                          on_completion=on_completion,
7292                                          compression=compression)
7293 
7294             # Ensure disk.info is written to the new path to avoid disks being
7295             # reinspected and potentially changing format.
7296             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
7297             if os.path.exists(src_disk_info_path):
7298                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
7299                 libvirt_utils.copy_image(src_disk_info_path,
7300                                          dst_disk_info_path,
7301                                          host=dest, on_execute=on_execute,
7302                                          on_completion=on_completion)
7303         except Exception:
7304             with excutils.save_and_reraise_exception():
7305                 self._cleanup_remote_migration(dest, inst_base,
7306                                                inst_base_resize,
7307                                                shared_storage)
7308 
7309         return disk_info_text
7310 
7311     def _wait_for_running(self, instance):
7312         state = self.get_info(instance).state
7313 
7314         if state == power_state.RUNNING:
7315             LOG.info(_LI("Instance running successfully."), instance=instance)
7316             raise loopingcall.LoopingCallDone()
7317 
7318     @staticmethod
7319     def _disk_raw_to_qcow2(path):
7320         """Converts a raw disk to qcow2."""
7321         path_qcow = path + '_qcow'
7322         utils.execute('qemu-img', 'convert', '-f', 'raw',
7323                       '-O', 'qcow2', path, path_qcow)
7324         utils.execute('mv', path_qcow, path)
7325 
7326     @staticmethod
7327     def _disk_qcow2_to_raw(path):
7328         """Converts a qcow2 disk to raw."""
7329         path_raw = path + '_raw'
7330         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
7331                       '-O', 'raw', path, path_raw)
7332         utils.execute('mv', path_raw, path)
7333 
7334     def finish_migration(self, context, migration, instance, disk_info,
7335                          network_info, image_meta, resize_instance,
7336                          block_device_info=None, power_on=True):
7337         LOG.debug("Starting finish_migration", instance=instance)
7338 
7339         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7340                                                   instance,
7341                                                   image_meta,
7342                                                   block_device_info)
7343         # assume _create_image does nothing if a target file exists.
7344         # NOTE: This has the intended side-effect of fetching a missing
7345         # backing file.
7346         self._create_image(context, instance, block_disk_info['mapping'],
7347                            block_device_info=block_device_info,
7348                            ignore_bdi_for_swap=True,
7349                            fallback_from_host=migration.source_compute)
7350 
7351         # Required by Quobyte CI
7352         self._ensure_console_log_for_instance(instance)
7353 
7354         gen_confdrive = functools.partial(
7355             self._create_configdrive, context, instance,
7356             InjectionInfo(admin_pass=None, network_info=network_info,
7357                           files=None))
7358 
7359         # Convert raw disks to qcow2 if migrating to host which uses
7360         # qcow2 from host which uses raw.
7361         disk_info = jsonutils.loads(disk_info)
7362         for info in disk_info:
7363             path = info['path']
7364             disk_name = os.path.basename(path)
7365 
7366             # NOTE(mdbooth): The code below looks wrong, but is actually
7367             # required to prevent a security hole when migrating from a host
7368             # with use_cow_images=False to one with use_cow_images=True.
7369             # Imagebackend uses use_cow_images to select between the
7370             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
7371             # writes to disk.info, but does not read it as it assumes qcow2.
7372             # Therefore if we don't convert raw to qcow2 here, a raw disk will
7373             # be incorrectly assumed to be qcow2, which is a severe security
7374             # flaw. The reverse is not true, because the atrociously-named-Raw
7375             # backend supports both qcow2 and raw disks, and will choose
7376             # appropriately between them as long as disk.info exists and is
7377             # correctly populated, which it is because Qcow2 writes to
7378             # disk.info.
7379             #
7380             # In general, we do not yet support format conversion during
7381             # migration. For example:
7382             #   * Converting from use_cow_images=True to use_cow_images=False
7383             #     isn't handled. This isn't a security bug, but is almost
7384             #     certainly buggy in other cases, as the 'Raw' backend doesn't
7385             #     expect a backing file.
7386             #   * Converting to/from lvm and rbd backends is not supported.
7387             #
7388             # This behaviour is inconsistent, and therefore undesirable for
7389             # users. It is tightly-coupled to implementation quirks of 2
7390             # out of 5 backends in imagebackend and defends against a severe
7391             # security flaw which is not at all obvious without deep analysis,
7392             # and is therefore undesirable to developers. We should aim to
7393             # remove it. This will not be possible, though, until we can
7394             # represent the storage layout of a specific instance
7395             # independent of the default configuration of the local compute
7396             # host.
7397 
7398             # Config disks are hard-coded to be raw even when
7399             # use_cow_images=True (see _get_disk_config_image_type),so don't
7400             # need to be converted.
7401             if (disk_name != 'disk.config' and
7402                         info['type'] == 'raw' and CONF.use_cow_images):
7403                 self._disk_raw_to_qcow2(info['path'])
7404 
7405         xml = self._get_guest_xml(context, instance, network_info,
7406                                   block_disk_info, image_meta,
7407                                   block_device_info=block_device_info)
7408         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
7409         # or not we've migrated to another host, because we unplug VIFs locally
7410         # and the status change in the port might go undetected by the neutron
7411         # L2 agent (or neutron server) so neutron may not know that the VIF was
7412         # unplugged in the first place and never send an event.
7413         guest = self._create_domain_and_network(context, xml, instance,
7414                                         network_info,
7415                                         block_disk_info,
7416                                         block_device_info=block_device_info,
7417                                         power_on=power_on,
7418                                         vifs_already_plugged=True,
7419                                         post_xml_callback=gen_confdrive)
7420         if power_on:
7421             timer = loopingcall.FixedIntervalLoopingCall(
7422                                                     self._wait_for_running,
7423                                                     instance)
7424             timer.start(interval=0.5).wait()
7425 
7426             # Sync guest time after migration.
7427             guest.sync_guest_time()
7428 
7429         LOG.debug("finish_migration finished successfully.", instance=instance)
7430 
7431     def _cleanup_failed_migration(self, inst_base):
7432         """Make sure that a failed migrate doesn't prevent us from rolling
7433         back in a revert.
7434         """
7435         try:
7436             shutil.rmtree(inst_base)
7437         except OSError as e:
7438             if e.errno != errno.ENOENT:
7439                 raise
7440 
7441     def finish_revert_migration(self, context, instance, network_info,
7442                                 block_device_info=None, power_on=True):
7443         LOG.debug("Starting finish_revert_migration",
7444                   instance=instance)
7445 
7446         inst_base = libvirt_utils.get_instance_path(instance)
7447         inst_base_resize = inst_base + "_resize"
7448 
7449         # NOTE(danms): if we're recovering from a failed migration,
7450         # make sure we don't have a left-over same-host base directory
7451         # that would conflict. Also, don't fail on the rename if the
7452         # failure happened early.
7453         if os.path.exists(inst_base_resize):
7454             self._cleanup_failed_migration(inst_base)
7455             utils.execute('mv', inst_base_resize, inst_base)
7456 
7457         self.image_backend.backend().connect_disks(instance, with_no_wait=True)
7458         root_disk = self.image_backend.by_name(instance, 'disk')
7459         # Once we rollback, the snapshot is no longer needed, so remove it
7460         # TODO(nic): Remove the try/except/finally in a future release
7461         # To avoid any upgrade issues surrounding instances being in pending
7462         # resize state when the software is updated, this portion of the
7463         # method logs exceptions rather than failing on them.  Once it can be
7464         # reasonably assumed that no such instances exist in the wild
7465         # anymore, the try/except/finally should be removed,
7466         # and ignore_errors should be set back to False (the default) so
7467         # that problems throw errors, like they should.
7468         if root_disk.exists():
7469             try:
7470                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
7471             except exception.SnapshotNotFound:
7472                 LOG.warning(_LW("Failed to rollback snapshot (%s)"),
7473                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
7474             finally:
7475                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
7476                                       ignore_errors=True)
7477 
7478         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7479                                             instance,
7480                                             instance.image_meta,
7481                                             block_device_info)
7482         xml = self._get_guest_xml(context, instance, network_info, disk_info,
7483                                   instance.image_meta,
7484                                   block_device_info=block_device_info)
7485         self._create_domain_and_network(context, xml, instance, network_info,
7486                                         disk_info,
7487                                         block_device_info=block_device_info,
7488                                         power_on=power_on,
7489                                         vifs_already_plugged=True)
7490 
7491         if power_on:
7492             timer = loopingcall.FixedIntervalLoopingCall(
7493                                                     self._wait_for_running,
7494                                                     instance)
7495             timer.start(interval=0.5).wait()
7496 
7497         LOG.debug("finish_revert_migration finished successfully.",
7498                   instance=instance)
7499 
7500     def confirm_migration(self, context, migration, instance, network_info):
7501         """Confirms a resize, destroying the source VM."""
7502         self._cleanup_resize(instance, network_info)
7503 
7504     @staticmethod
7505     def _get_io_devices(xml_doc):
7506         """get the list of io devices from the xml document."""
7507         result = {"volumes": [], "ifaces": []}
7508         try:
7509             doc = etree.fromstring(xml_doc)
7510         except Exception:
7511             return result
7512         blocks = [('./devices/disk', 'volumes'),
7513             ('./devices/interface', 'ifaces')]
7514         for block, key in blocks:
7515             section = doc.findall(block)
7516             for node in section:
7517                 for child in node.getchildren():
7518                     if child.tag == 'target' and child.get('dev'):
7519                         result[key].append(child.get('dev'))
7520         return result
7521 
7522     def get_diagnostics(self, instance):
7523         guest = self._host.get_guest(instance)
7524 
7525         # TODO(sahid): We are converting all calls from a
7526         # virDomain object to use nova.virt.libvirt.Guest.
7527         # We should be able to remove domain at the end.
7528         domain = guest._domain
7529         output = {}
7530         # get cpu time, might launch an exception if the method
7531         # is not supported by the underlying hypervisor being
7532         # used by libvirt
7533         try:
7534             for vcpu in guest.get_vcpus_info():
7535                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
7536         except libvirt.libvirtError:
7537             pass
7538         # get io status
7539         xml = guest.get_xml_desc()
7540         dom_io = LibvirtDriver._get_io_devices(xml)
7541         for guest_disk in dom_io["volumes"]:
7542             try:
7543                 # blockStats might launch an exception if the method
7544                 # is not supported by the underlying hypervisor being
7545                 # used by libvirt
7546                 stats = domain.blockStats(guest_disk)
7547                 output[guest_disk + "_read_req"] = stats[0]
7548                 output[guest_disk + "_read"] = stats[1]
7549                 output[guest_disk + "_write_req"] = stats[2]
7550                 output[guest_disk + "_write"] = stats[3]
7551                 output[guest_disk + "_errors"] = stats[4]
7552             except libvirt.libvirtError:
7553                 pass
7554         for interface in dom_io["ifaces"]:
7555             try:
7556                 # interfaceStats might launch an exception if the method
7557                 # is not supported by the underlying hypervisor being
7558                 # used by libvirt
7559                 stats = domain.interfaceStats(interface)
7560                 output[interface + "_rx"] = stats[0]
7561                 output[interface + "_rx_packets"] = stats[1]
7562                 output[interface + "_rx_errors"] = stats[2]
7563                 output[interface + "_rx_drop"] = stats[3]
7564                 output[interface + "_tx"] = stats[4]
7565                 output[interface + "_tx_packets"] = stats[5]
7566                 output[interface + "_tx_errors"] = stats[6]
7567                 output[interface + "_tx_drop"] = stats[7]
7568             except libvirt.libvirtError:
7569                 pass
7570         output["memory"] = domain.maxMemory()
7571         # memoryStats might launch an exception if the method
7572         # is not supported by the underlying hypervisor being
7573         # used by libvirt
7574         try:
7575             mem = domain.memoryStats()
7576             for key in mem.keys():
7577                 output["memory-" + key] = mem[key]
7578         except (libvirt.libvirtError, AttributeError):
7579             pass
7580         return output
7581 
7582     def get_instance_diagnostics(self, instance):
7583         guest = self._host.get_guest(instance)
7584 
7585         # TODO(sahid): We are converting all calls from a
7586         # virDomain object to use nova.virt.libvirt.Guest.
7587         # We should be able to remove domain at the end.
7588         domain = guest._domain
7589 
7590         xml = guest.get_xml_desc()
7591         xml_doc = etree.fromstring(xml)
7592 
7593         # TODO(sahid): Needs to use get_info but more changes have to
7594         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
7595         # needed.
7596         (state, max_mem, mem, num_cpu, cpu_time) = \
7597             guest._get_domain_info(self._host)
7598         config_drive = configdrive.required_by(instance)
7599         launched_at = timeutils.normalize_time(instance.launched_at)
7600         uptime = timeutils.delta_seconds(launched_at,
7601                                          timeutils.utcnow())
7602         diags = diagnostics.Diagnostics(state=power_state.STATE_MAP[state],
7603                                         driver='libvirt',
7604                                         config_drive=config_drive,
7605                                         hypervisor_os='linux',
7606                                         uptime=uptime)
7607         diags.memory_details.maximum = max_mem / units.Mi
7608         diags.memory_details.used = mem / units.Mi
7609 
7610         # get cpu time, might launch an exception if the method
7611         # is not supported by the underlying hypervisor being
7612         # used by libvirt
7613         try:
7614             for vcpu in guest.get_vcpus_info():
7615                 diags.add_cpu(time=vcpu.time)
7616         except libvirt.libvirtError:
7617             pass
7618         # get io status
7619         dom_io = LibvirtDriver._get_io_devices(xml)
7620         for guest_disk in dom_io["volumes"]:
7621             try:
7622                 # blockStats might launch an exception if the method
7623                 # is not supported by the underlying hypervisor being
7624                 # used by libvirt
7625                 stats = domain.blockStats(guest_disk)
7626                 diags.add_disk(read_bytes=stats[1],
7627                                read_requests=stats[0],
7628                                write_bytes=stats[3],
7629                                write_requests=stats[2])
7630             except libvirt.libvirtError:
7631                 pass
7632         for interface in dom_io["ifaces"]:
7633             try:
7634                 # interfaceStats might launch an exception if the method
7635                 # is not supported by the underlying hypervisor being
7636                 # used by libvirt
7637                 stats = domain.interfaceStats(interface)
7638                 diags.add_nic(rx_octets=stats[0],
7639                               rx_errors=stats[2],
7640                               rx_drop=stats[3],
7641                               rx_packets=stats[1],
7642                               tx_octets=stats[4],
7643                               tx_errors=stats[6],
7644                               tx_drop=stats[7],
7645                               tx_packets=stats[5])
7646             except libvirt.libvirtError:
7647                 pass
7648 
7649         # Update mac addresses of interface if stats have been reported
7650         if diags.nic_details:
7651             nodes = xml_doc.findall('./devices/interface/mac')
7652             for index, node in enumerate(nodes):
7653                 diags.nic_details[index].mac_address = node.get('address')
7654         return diags
7655 
7656     @staticmethod
7657     def _prepare_device_bus(dev):
7658         """Determines the device bus and its hypervisor assigned address
7659         """
7660         bus = None
7661         address = (dev.device_addr.format_address() if
7662                    dev.device_addr else None)
7663         if isinstance(dev.device_addr,
7664                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
7665             bus = objects.PCIDeviceBus()
7666         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7667             if dev.target_bus == 'scsi':
7668                 bus = objects.SCSIDeviceBus()
7669             elif dev.target_bus == 'ide':
7670                 bus = objects.IDEDeviceBus()
7671             elif dev.target_bus == 'usb':
7672                 bus = objects.USBDeviceBus()
7673         if address is not None and bus is not None:
7674             bus.address = address
7675         return bus
7676 
7677     def _build_device_metadata(self, context, instance):
7678         """Builds a metadata object for instance devices, that maps the user
7679            provided tag to the hypervisor assigned device address.
7680         """
7681         def _get_device_name(bdm):
7682             return block_device.strip_dev(bdm.device_name)
7683 
7684         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
7685                                                                  instance.uuid)
7686         tagged_vifs = {vif.address: vif for vif in vifs if vif.tag}
7687         # TODO(mriedem): We should be able to avoid the DB query here by using
7688         # block_device_info['block_device_mapping'] which is passed into most
7689         # methods that call this function.
7690         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7691             context, instance.uuid)
7692         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
7693 
7694         devices = []
7695         guest = self._host.get_guest(instance)
7696         xml = guest.get_xml_desc()
7697         xml_dom = etree.fromstring(xml)
7698         guest_config = vconfig.LibvirtConfigGuest()
7699         guest_config.parse_dom(xml_dom)
7700 
7701         for dev in guest_config.devices:
7702             # Build network interfaces related metadata
7703             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
7704                 vif = tagged_vifs.get(dev.mac_addr)
7705                 if not vif:
7706                     continue
7707                 bus = self._prepare_device_bus(dev)
7708                 device = objects.NetworkInterfaceMetadata(
7709                     mac=vif.address,
7710                     tags=[vif.tag]
7711                 )
7712                 if bus:
7713                     device.bus = bus
7714                 devices.append(device)
7715 
7716             # Build disks related metadata
7717             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7718                 bdm = tagged_bdms.get(dev.target_dev)
7719                 if not bdm:
7720                     continue
7721                 bus = self._prepare_device_bus(dev)
7722                 device = objects.DiskMetadata(tags=[bdm.tag])
7723                 if bus:
7724                     device.bus = bus
7725                 devices.append(device)
7726         if devices:
7727             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
7728             return dev_meta
7729 
7730     def instance_on_disk(self, instance):
7731         # ensure directories exist and are writable
7732         instance_path = libvirt_utils.get_instance_path(instance)
7733         LOG.debug('Checking instance files accessibility %s', instance_path,
7734                   instance=instance)
7735         shared_instance_path = os.access(instance_path, os.W_OK)
7736         # NOTE(flwang): For shared block storage scenario, the file system is
7737         # not really shared by the two hosts, but the volume of evacuated
7738         # instance is reachable.
7739         shared_block_storage = (self.image_backend.backend().
7740                                 is_shared_block_storage())
7741         return shared_instance_path or shared_block_storage
7742 
7743     def inject_network_info(self, instance, nw_info):
7744         self.firewall_driver.setup_basic_filtering(instance, nw_info)
7745 
7746     def delete_instance_files(self, instance):
7747         target = libvirt_utils.get_instance_path(instance)
7748         # A resize may be in progress
7749         target_resize = target + '_resize'
7750         # Other threads may attempt to rename the path, so renaming the path
7751         # to target + '_del' (because it is atomic) and iterating through
7752         # twice in the unlikely event that a concurrent rename occurs between
7753         # the two rename attempts in this method. In general this method
7754         # should be fairly thread-safe without these additional checks, since
7755         # other operations involving renames are not permitted when the task
7756         # state is not None and the task state should be set to something
7757         # other than None by the time this method is invoked.
7758         target_del = target + '_del'
7759         for i in six.moves.range(2):
7760             try:
7761                 utils.execute('mv', target, target_del)
7762                 break
7763             except Exception:
7764                 pass
7765             try:
7766                 utils.execute('mv', target_resize, target_del)
7767                 break
7768             except Exception:
7769                 pass
7770         # Either the target or target_resize path may still exist if all
7771         # rename attempts failed.
7772         remaining_path = None
7773         for p in (target, target_resize):
7774             if os.path.exists(p):
7775                 remaining_path = p
7776                 break
7777 
7778         # A previous delete attempt may have been interrupted, so target_del
7779         # may exist even if all rename attempts during the present method
7780         # invocation failed due to the absence of both target and
7781         # target_resize.
7782         if not remaining_path and os.path.exists(target_del):
7783             self.job_tracker.terminate_jobs(instance)
7784 
7785             LOG.info(_LI('Deleting instance files %s'), target_del,
7786                      instance=instance)
7787             remaining_path = target_del
7788             try:
7789                 shutil.rmtree(target_del)
7790             except OSError as e:
7791                 LOG.error(_LE('Failed to cleanup directory %(target)s: '
7792                               '%(e)s'), {'target': target_del, 'e': e},
7793                             instance=instance)
7794 
7795         # It is possible that the delete failed, if so don't mark the instance
7796         # as cleaned.
7797         if remaining_path and os.path.exists(remaining_path):
7798             LOG.info(_LI('Deletion of %s failed'), remaining_path,
7799                      instance=instance)
7800             return False
7801 
7802         LOG.info(_LI('Deletion of %s complete'), target_del, instance=instance)
7803         return True
7804 
7805     @property
7806     def need_legacy_block_device_info(self):
7807         return False
7808 
7809     def default_root_device_name(self, instance, image_meta, root_bdm):
7810         disk_bus = blockinfo.get_disk_bus_for_device_type(
7811             instance, CONF.libvirt.virt_type, image_meta, "disk")
7812         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
7813             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
7814         root_info = blockinfo.get_root_info(
7815             instance, CONF.libvirt.virt_type, image_meta,
7816             root_bdm, disk_bus, cdrom_bus)
7817         return block_device.prepend_dev(root_info['dev'])
7818 
7819     def default_device_names_for_instance(self, instance, root_device_name,
7820                                           *block_device_lists):
7821         block_device_mapping = list(itertools.chain(*block_device_lists))
7822         # NOTE(ndipanov): Null out the device names so that blockinfo code
7823         #                 will assign them
7824         for bdm in block_device_mapping:
7825             if bdm.device_name is not None:
7826                 LOG.warning(
7827                     _LW("Ignoring supplied device name: %(device_name)s. "
7828                         "Libvirt can't honour user-supplied dev names"),
7829                     {'device_name': bdm.device_name}, instance=instance)
7830                 bdm.device_name = None
7831         block_device_info = driver.get_block_device_info(instance,
7832                                                          block_device_mapping)
7833 
7834         blockinfo.default_device_names(CONF.libvirt.virt_type,
7835                                        nova_context.get_admin_context(),
7836                                        instance,
7837                                        block_device_info,
7838                                        instance.image_meta)
7839 
7840     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
7841         block_device_info = driver.get_block_device_info(instance, bdms)
7842         instance_info = blockinfo.get_disk_info(
7843                 CONF.libvirt.virt_type, instance,
7844                 instance.image_meta, block_device_info=block_device_info)
7845 
7846         suggested_dev_name = block_device_obj.device_name
7847         if suggested_dev_name is not None:
7848             LOG.warning(
7849                 _LW('Ignoring supplied device name: %(suggested_dev)s'),
7850                 {'suggested_dev': suggested_dev_name}, instance=instance)
7851 
7852         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
7853         #                 only when it's actually not set on the bd object
7854         block_device_obj.device_name = None
7855         disk_info = blockinfo.get_info_from_bdm(
7856             instance, CONF.libvirt.virt_type, instance.image_meta,
7857             block_device_obj, mapping=instance_info['mapping'])
7858         return block_device.prepend_dev(disk_info['dev'])
7859 
7860     def is_supported_fs_format(self, fs_type):
7861         return fs_type in [disk_api.FS_FORMAT_EXT2, disk_api.FS_FORMAT_EXT3,
7862                            disk_api.FS_FORMAT_EXT4, disk_api.FS_FORMAT_XFS]
