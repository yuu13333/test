Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import collections
29 import contextlib
30 import errno
31 import functools
32 import glob
33 import itertools
34 import mmap
35 import operator
36 import os
37 import shutil
38 import tempfile
39 import time
40 import uuid
41 
42 import eventlet
43 from eventlet import greenthread
44 from eventlet import tpool
45 from lxml import etree
46 from os_brick.initiator import connector
47 from oslo_concurrency import processutils
48 from oslo_config import cfg
49 from oslo_log import log as logging
50 from oslo_serialization import jsonutils
51 from oslo_service import loopingcall
52 from oslo_utils import excutils
53 from oslo_utils import fileutils
54 from oslo_utils import importutils
55 from oslo_utils import strutils
56 from oslo_utils import timeutils
57 from oslo_utils import units
58 import six
59 from six.moves import range
60 
61 from nova.api.metadata import base as instance_metadata
62 from nova import block_device
63 from nova.compute import arch
64 from nova.compute import hv_type
65 from nova.compute import power_state
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 from nova.compute import vm_mode
69 from nova.console import serial as serial_console
70 from nova.console import type as ctype
71 from nova import context as nova_context
72 from nova import exception
73 from nova.i18n import _
74 from nova.i18n import _LE
75 from nova.i18n import _LI
76 from nova.i18n import _LW
77 from nova import image
78 from nova.network import model as network_model
79 from nova import objects
80 from nova.pci import manager as pci_manager
81 from nova.pci import utils as pci_utils
82 from nova import utils
83 from nova import version
84 from nova.virt import block_device as driver_block_device
85 from nova.virt import configdrive
86 from nova.virt import diagnostics
87 from nova.virt.disk import api as disk
88 from nova.virt.disk.vfs import guestfs
89 from nova.virt import driver
90 from nova.virt import firewall
91 from nova.virt import hardware
92 from nova.virt.image import model as imgmodel
93 from nova.virt.libvirt import blockinfo
94 from nova.virt.libvirt import config as vconfig
95 from nova.virt.libvirt import firewall as libvirt_firewall
96 from nova.virt.libvirt import guest as libvirt_guest
97 from nova.virt.libvirt import host
98 from nova.virt.libvirt import imagebackend
99 from nova.virt.libvirt import imagecache
100 from nova.virt.libvirt.storage import dmcrypt
101 from nova.virt.libvirt.storage import lvm
102 from nova.virt.libvirt.storage import rbd_utils
103 from nova.virt.libvirt import utils as libvirt_utils
104 from nova.virt.libvirt import vif as libvirt_vif
105 from nova.virt import netutils
106 from nova.virt import watchdog_actions
107 from nova import volume
108 from nova.volume import encryptors
109 
110 libvirt = None
111 
112 LOG = logging.getLogger(__name__)
113 
114 libvirt_opts = [
115     cfg.StrOpt('rescue_image_id',
116                help='Rescue ami image. This will not be used if an image id '
117                     'is provided by the user.'),
118     cfg.StrOpt('rescue_kernel_id',
119                help='Rescue aki image'),
120     cfg.StrOpt('rescue_ramdisk_id',
121                help='Rescue ari image'),
122     cfg.StrOpt('virt_type',
123                default='kvm',
124                choices=('kvm', 'lxc', 'qemu', 'uml', 'xen', 'parallels'),
125                help='Libvirt domain type'),
126     cfg.StrOpt('connection_uri',
127                default='',
128                help='Override the default libvirt URI '
129                     '(which is dependent on virt_type)'),
130     cfg.BoolOpt('inject_password',
131                 default=False,
132                 help='Inject the admin password at boot time, '
133                      'without an agent.'),
134     cfg.BoolOpt('inject_key',
135                 default=False,
136                 help='Inject the ssh public key at boot time'),
137     cfg.IntOpt('inject_partition',
138                 default=-2,
139                 help='The partition to inject to : '
140                      '-2 => disable, -1 => inspect (libguestfs only), '
141                      '0 => not partitioned, >0 => partition number'),
142     cfg.BoolOpt('use_usb_tablet',
143                 default=True,
144                 help='Sync virtual and real mouse cursors in Windows VMs'),
145     cfg.StrOpt('live_migration_uri',
146                default="qemu+tcp://%s/system",
147                help='Migration target URI '
148                     '(any included "%s" is replaced with '
149                     'the migration target hostname)'),
150     cfg.StrOpt('live_migration_flag',
151                default='VIR_MIGRATE_UNDEFINE_SOURCE, VIR_MIGRATE_PEER2PEER, '
152                        'VIR_MIGRATE_LIVE, VIR_MIGRATE_TUNNELLED',
153                help='Migration flags to be set for live migration'),
154     cfg.StrOpt('block_migration_flag',
155                default='VIR_MIGRATE_UNDEFINE_SOURCE, VIR_MIGRATE_PEER2PEER, '
156                        'VIR_MIGRATE_LIVE, VIR_MIGRATE_TUNNELLED, '
157                        'VIR_MIGRATE_NON_SHARED_INC',
158                help='Migration flags to be set for block migration'),
159     cfg.IntOpt('live_migration_bandwidth',
160                default=0,
161                help='Maximum bandwidth to be used during migration, in Mbps'),
162     cfg.StrOpt('snapshot_image_format',
163                choices=('raw', 'qcow2', 'vmdk', 'vdi'),
164                help='Snapshot image format. Defaults to same as source image'),
165     cfg.StrOpt('disk_prefix',
166                help='Override the default disk prefix for the devices attached'
167                     ' to a server, which is dependent on virt_type. '
168                     '(valid options are: sd, xvd, uvd, vd)'),
169     cfg.IntOpt('wait_soft_reboot_seconds',
170                default=120,
171                help='Number of seconds to wait for instance to shut down after'
172                     ' soft reboot request is made. We fall back to hard reboot'
173                     ' if instance does not shutdown within this window.'),
174     cfg.StrOpt('cpu_mode',
175                choices=('host-model', 'host-passthrough', 'custom', 'none'),
176                help='Set to "host-model" to clone the host CPU feature flags; '
177                     'to "host-passthrough" to use the host CPU model exactly; '
178                     'to "custom" to use a named CPU model; '
179                     'to "none" to not set any CPU model. '
180                     'If virt_type="kvm|qemu", it will default to '
181                     '"host-model", otherwise it will default to "none"'),
182     cfg.StrOpt('cpu_model',
183                help='Set to a named libvirt CPU model (see names listed '
184                     'in /usr/share/libvirt/cpu_map.xml). Only has effect if '
185                     'cpu_mode="custom" and virt_type="kvm|qemu"'),
186     cfg.StrOpt('snapshots_directory',
187                default='$instances_path/snapshots',
188                help='Location where libvirt driver will store snapshots '
189                     'before uploading them to image service'),
190     cfg.StrOpt('xen_hvmloader_path',
191                 default='/usr/lib/xen/boot/hvmloader',
192                 help='Location where the Xen hvmloader is kept'),
193     cfg.ListOpt('disk_cachemodes',
194                  default=[],
195                  help='Specific cachemodes to use for different disk types '
196                       'e.g: file=directsync,block=none'),
197     cfg.StrOpt('rng_dev_path',
198                 help='A path to a device that will be used as source of '
199                      'entropy on the host. Permitted options are: '
200                      '/dev/random or /dev/hwrng'),
201     cfg.ListOpt('hw_machine_type',
202                help='For qemu or KVM guests, set this option to specify '
203                     'a default machine type per host architecture. '
204                     'You can find a list of supported machine types '
205                     'in your environment by checking the output of '
206                     'the "virsh capabilities"command. The format of the '
207                     'value for this config option is host-arch=machine-type. '
208                     'For example: x86_64=machinetype1,armv7l=machinetype2'),
209     cfg.StrOpt('sysinfo_serial',
210                default='auto',
211                choices=('none', 'os', 'hardware', 'auto'),
212                help='The data source used to the populate the host "serial" '
213                     'UUID exposed to guest in the virtual BIOS.'),
214     cfg.IntOpt('mem_stats_period_seconds',
215                 default=10,
216                 help='A number of seconds to memory usage statistics period. '
217                      'Zero or negative value mean to disable memory usage '
218                      'statistics.'),
219     cfg.ListOpt('uid_maps',
220                 default=[],
221                 help='List of uid targets and ranges.'
222                      'Syntax is guest-uid:host-uid:count'
223                      'Maximum of 5 allowed.'),
224     cfg.ListOpt('gid_maps',
225                 default=[],
226                 help='List of guid targets and ranges.'
227                      'Syntax is guest-gid:host-gid:count'
228                      'Maximum of 5 allowed.')
229     ]
230 
231 CONF = cfg.CONF
232 CONF.register_opts(libvirt_opts, 'libvirt')
233 CONF.import_opt('host', 'nova.netconf')
234 CONF.import_opt('my_ip', 'nova.netconf')
235 CONF.import_opt('use_cow_images', 'nova.virt.driver')
236 CONF.import_opt('enabled', 'nova.compute.api',
237                 group='ephemeral_storage_encryption')
238 CONF.import_opt('cipher', 'nova.compute.api',
239                 group='ephemeral_storage_encryption')
240 CONF.import_opt('key_size', 'nova.compute.api',
241                 group='ephemeral_storage_encryption')
242 CONF.import_opt('live_migration_retry_count', 'nova.compute.manager')
243 CONF.import_opt('vncserver_proxyclient_address', 'nova.vnc', group='vnc')
244 CONF.import_opt('server_proxyclient_address', 'nova.spice', group='spice')
245 CONF.import_opt('vcpu_pin_set', 'nova.virt.hardware')
246 CONF.import_opt('vif_plugging_is_fatal', 'nova.virt.driver')
247 CONF.import_opt('vif_plugging_timeout', 'nova.virt.driver')
248 CONF.import_opt('enabled', 'nova.console.serial', group='serial_console')
249 CONF.import_opt('proxyclient_address', 'nova.console.serial',
250                 group='serial_console')
251 CONF.import_opt('hw_disk_discard', 'nova.virt.libvirt.imagebackend',
252                 group='libvirt')
253 CONF.import_group('workarounds', 'nova.utils')
254 CONF.import_opt('iscsi_use_multipath', 'nova.virt.libvirt.volume',
255                 group='libvirt')
256 
257 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
258     libvirt_firewall.__name__,
259     libvirt_firewall.IptablesFirewallDriver.__name__)
260 
261 MAX_CONSOLE_BYTES = 100 * units.Ki
262 
263 # The libvirt driver will prefix any disable reason codes with this string.
264 DISABLE_PREFIX = 'AUTO: '
265 # Disable reason for the service which was enabled or disabled without reason
266 DISABLE_REASON_UNDEFINED = None
267 
268 # Guest config console string
269 CONSOLE = "console=tty0 console=ttyS0"
270 
271 GuestNumaConfig = collections.namedtuple(
272     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
273 
274 libvirt_volume_drivers = [
275     'iscsi=nova.virt.libvirt.volume.LibvirtISCSIVolumeDriver',
276     'iser=nova.virt.libvirt.volume.LibvirtISERVolumeDriver',
277     'local=nova.virt.libvirt.volume.LibvirtVolumeDriver',
278     'fake=nova.virt.libvirt.volume.LibvirtFakeVolumeDriver',
279     'rbd=nova.virt.libvirt.volume.LibvirtNetVolumeDriver',
280     'sheepdog=nova.virt.libvirt.volume.LibvirtNetVolumeDriver',
281     'nfs=nova.virt.libvirt.volume.LibvirtNFSVolumeDriver',
282     'smbfs=nova.virt.libvirt.volume.LibvirtSMBFSVolumeDriver',
283     'aoe=nova.virt.libvirt.volume.LibvirtAOEVolumeDriver',
284     'glusterfs=nova.virt.libvirt.volume.LibvirtGlusterfsVolumeDriver',
285     'fibre_channel=nova.virt.libvirt.volume.LibvirtFibreChannelVolumeDriver',
286     'scality=nova.virt.libvirt.volume.LibvirtScalityVolumeDriver',
287     'gpfs=nova.virt.libvirt.volume.LibvirtGPFSVolumeDriver',
288     'quobyte=nova.virt.libvirt.volume.LibvirtQuobyteVolumeDriver',
289 ]
290 
291 
292 def patch_tpool_proxy():
293     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
294     or __repr__() calls. See bug #962840 for details.
295     We perform a monkey patch to replace those two instance methods.
296     """
297     def str_method(self):
298         return str(self._obj)
299 
300     def repr_method(self):
301         return repr(self._obj)
302 
303     tpool.Proxy.__str__ = str_method
304     tpool.Proxy.__repr__ = repr_method
305 
306 
307 patch_tpool_proxy()
308 
309 VIR_DOMAIN_NOSTATE = 0
310 VIR_DOMAIN_RUNNING = 1
311 VIR_DOMAIN_BLOCKED = 2
312 VIR_DOMAIN_PAUSED = 3
313 VIR_DOMAIN_SHUTDOWN = 4
314 VIR_DOMAIN_SHUTOFF = 5
315 VIR_DOMAIN_CRASHED = 6
316 VIR_DOMAIN_PMSUSPENDED = 7
317 
318 LIBVIRT_POWER_STATE = {
319     VIR_DOMAIN_NOSTATE: power_state.NOSTATE,
320     VIR_DOMAIN_RUNNING: power_state.RUNNING,
321     # NOTE(maoy): The DOMAIN_BLOCKED state is only valid in Xen.
322     # It means that the VM is running and the vCPU is idle. So,
323     # we map it to RUNNING
324     VIR_DOMAIN_BLOCKED: power_state.RUNNING,
325     VIR_DOMAIN_PAUSED: power_state.PAUSED,
326     # NOTE(maoy): The libvirt API doc says that DOMAIN_SHUTDOWN
327     # means the domain is being shut down. So technically the domain
328     # is still running. SHUTOFF is the real powered off state.
329     # But we will map both to SHUTDOWN anyway.
330     # http://libvirt.org/html/libvirt-libvirt.html
331     VIR_DOMAIN_SHUTDOWN: power_state.SHUTDOWN,
332     VIR_DOMAIN_SHUTOFF: power_state.SHUTDOWN,
333     VIR_DOMAIN_CRASHED: power_state.CRASHED,
334     VIR_DOMAIN_PMSUSPENDED: power_state.SUSPENDED,
335 }
336 
337 MIN_LIBVIRT_VERSION = (0, 9, 11)
338 # When the above version matches/exceeds this version
339 # delete it & corresponding code using it
340 MIN_LIBVIRT_DEVICE_CALLBACK_VERSION = (1, 1, 1)
341 # TODO(mriedem): Change MIN_LIB_VERSION to this in the 13.0.0 'M' release.
342 NEXT_MIN_LIBVIRT_VERSION = (0, 10, 2)
343 # Live snapshot requirements
344 MIN_LIBVIRT_LIVESNAPSHOT_VERSION = (1, 0, 0)
345 MIN_QEMU_LIVESNAPSHOT_VERSION = (1, 3, 0)
346 # block size tuning requirements
347 MIN_LIBVIRT_BLOCKIO_VERSION = (0, 10, 2)
348 # BlockJobInfo management requirement
349 MIN_LIBVIRT_BLOCKJOBINFO_VERSION = (1, 1, 1)
350 # Relative block commit & rebase (feature is detected,
351 # this version is only used for messaging)
352 MIN_LIBVIRT_BLOCKJOB_RELATIVE_VERSION = (1, 2, 7)
353 # libvirt discard feature
354 MIN_LIBVIRT_DISCARD_VERSION = (1, 0, 6)
355 MIN_QEMU_DISCARD_VERSION = (1, 6, 0)
356 # While earlier versions could support NUMA reporting and
357 # NUMA placement, not until 1.2.7 was there the ability
358 # to pin guest nodes to host nodes, so mandate that. Without
359 # this the scheduler cannot make guaranteed decisions, as the
360 # guest placement may not match what was requested
361 MIN_LIBVIRT_NUMA_VERSION = (1, 2, 7)
362 # Versions of libvirt with known NUMA topology issues
363 # See bug #1449028
364 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
365 # While earlier versions could support hugepage backed
366 # guests, not until 1.2.8 was there the ability to request
367 # a particular huge page size. Without this the scheduler
368 # cannot make guaranteed decisions, as the huge page size
369 # used by the guest may not match what was requested
370 MIN_LIBVIRT_HUGEPAGE_VERSION = (1, 2, 8)
371 # Versions of libvirt with broken cpu pinning support. This excludes
372 # versions of libvirt with broken NUMA support since pinning needs
373 # NUMA
374 # See bug #1438226
375 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
376 # qemu 2.1 introduces support for pinning memory on host
377 # NUMA nodes, along with the ability to specify hugepage
378 # sizes per guest NUMA node
379 MIN_QEMU_NUMA_HUGEPAGE_VERSION = (2, 1, 0)
380 # fsFreeze/fsThaw requirement
381 MIN_LIBVIRT_FSFREEZE_VERSION = (1, 2, 5)
382 
383 # Hyper-V paravirtualized time source
384 MIN_LIBVIRT_HYPERV_TIMER_VERSION = (1, 2, 2)
385 MIN_QEMU_HYPERV_TIMER_VERSION = (2, 0, 0)
386 
387 MIN_LIBVIRT_HYPERV_FEATURE_VERSION = (1, 0, 0)
388 MIN_LIBVIRT_HYPERV_FEATURE_EXTRA_VERSION = (1, 1, 0)
389 MIN_QEMU_HYPERV_FEATURE_VERSION = (1, 1, 0)
390 
391 # parallels driver support
392 MIN_LIBVIRT_PARALLELS_VERSION = (1, 2, 12)
393 
394 
395 class LibvirtDriver(driver.ComputeDriver):
396     capabilities = {
397         "has_imagecache": True,
398         "supports_recreate": True,
399         "supports_migrate_to_same_host": False
400     }
401 
402     def __init__(self, virtapi, read_only=False):
403         super(LibvirtDriver, self).__init__(virtapi)
404 
405         global libvirt
406         if libvirt is None:
407             libvirt = importutils.import_module('libvirt')
408 
409         self._host = host.Host(self._uri(), read_only,
410                                lifecycle_event_handler=self.emit_event,
411                                conn_event_handler=self._handle_conn_event)
412         self._initiator = None
413         self._fc_wwnns = None
414         self._fc_wwpns = None
415         self._caps = None
416         self.firewall_driver = firewall.load_driver(
417             DEFAULT_FIREWALL_DRIVER,
418             self.virtapi,
419             host=self._host)
420 
421         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
422 
423         self.volume_drivers = driver.driver_dict_from_config(
424             self._get_volume_drivers(), self)
425 
426         self._disk_cachemode = None
427         self.image_cache_manager = imagecache.ImageCacheManager()
428         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
429 
430         self.disk_cachemodes = {}
431 
432         self.valid_cachemodes = ["default",
433                                  "none",
434                                  "writethrough",
435                                  "writeback",
436                                  "directsync",
437                                  "unsafe",
438                                 ]
439         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
440                                                                       'qemu')
441 
442         for mode_str in CONF.libvirt.disk_cachemodes:
443             disk_type, sep, cache_mode = mode_str.partition('=')
444             if cache_mode not in self.valid_cachemodes:
445                 LOG.warn(_LW('Invalid cachemode %(cache_mode)s specified '
446                              'for disk type %(disk_type)s.'),
447                          {'cache_mode': cache_mode, 'disk_type': disk_type})
448                 continue
449             self.disk_cachemodes[disk_type] = cache_mode
450 
451         self._volume_api = volume.API()
452         self._image_api = image.API()
453 
454         sysinfo_serial_funcs = {
455             'none': lambda: None,
456             'hardware': self._get_host_sysinfo_serial_hardware,
457             'os': self._get_host_sysinfo_serial_os,
458             'auto': self._get_host_sysinfo_serial_auto,
459         }
460 
461         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
462             CONF.libvirt.sysinfo_serial)
463         if not self._sysinfo_serial_func:
464             raise exception.NovaException(
465                 _("Unexpected sysinfo_serial setting '%(actual)s'. "
466                   "Permitted values are %(expect)s'") %
467                   {'actual': CONF.libvirt.sysinfo_serial,
468                    'expect': ', '.join("'%s'" % k for k in
469                                        sysinfo_serial_funcs.keys())})
470 
471     def _get_volume_drivers(self):
472         return libvirt_volume_drivers
473 
474     @property
475     def disk_cachemode(self):
476         if self._disk_cachemode is None:
477             # We prefer 'none' for consistent performance, host crash
478             # safety & migration correctness by avoiding host page cache.
479             # Some filesystems (eg GlusterFS via FUSE) don't support
480             # O_DIRECT though. For those we fallback to 'writethrough'
481             # which gives host crash safety, and is safe for migration
482             # provided the filesystem is cache coherent (cluster filesystems
483             # typically are, but things like NFS are not).
484             self._disk_cachemode = "none"
485             if not self._supports_direct_io(CONF.instances_path):
486                 self._disk_cachemode = "writethrough"
487         return self._disk_cachemode
488 
489     def _set_cache_mode(self, conf):
490         """Set cache mode on LibvirtConfigGuestDisk object."""
491         try:
492             source_type = conf.source_type
493             driver_cache = conf.driver_cache
494         except AttributeError:
495             return
496 
497         cache_mode = self.disk_cachemodes.get(source_type,
498                                               driver_cache)
499         conf.driver_cache = cache_mode
500 
501     def _do_quality_warnings(self):
502         """Warn about untested driver configurations.
503 
504         This will log a warning message about untested driver or host arch
505         configurations to indicate to administrators that the quality is
506         unknown. Currently, only qemu or kvm on intel 32- or 64-bit systems
507         is tested upstream.
508         """
509         caps = self._host.get_capabilities()
510         hostarch = caps.host.cpu.arch
511         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
512             hostarch not in (arch.I686, arch.X86_64)):
513             LOG.warn(_LW('The libvirt driver is not tested on '
514                          '%(type)s/%(arch)s by the OpenStack project and '
515                          'thus its quality can not be ensured. For more '
516                          'information, see: https://wiki.openstack.org/wiki/'
517                          'HypervisorSupportMatrix'),
518                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
519 
520     def _handle_conn_event(self, enabled, reason):
521         LOG.info(_LI("Connection event '%(enabled)d' reason '%(reason)s'"),
522                  {'enabled': enabled, 'reason': reason})
523         self._set_host_enabled(enabled, reason)
524 
525     def _version_to_string(self, version):
526         return '.'.join([str(x) for x in version])
527 
528     def init_host(self, host):
529         self._host.initialize()
530 
531         self._do_quality_warnings()
532 
533         if (CONF.libvirt.virt_type == 'lxc' and
534                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
535             LOG.warn(_LW("Running libvirt-lxc without user namespaces is "
536                          "dangerous. Containers spawned by Nova will be run "
537                          "as the host's root user. It is highly suggested "
538                          "that user namespaces be used in a public or "
539                          "multi-tenant environment."))
540 
541         # Stop libguestfs using KVM unless we're also configured
542         # to use this. This solves problem where people need to
543         # stop Nova use of KVM because nested-virt is broken
544         if CONF.libvirt.virt_type != "kvm":
545             guestfs.force_tcg()
546 
547         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
548             raise exception.NovaException(
549                 _('Nova requires libvirt version %s or greater.') %
550                 self._version_to_string(MIN_LIBVIRT_VERSION))
551 
552         if (CONF.libvirt.virt_type == 'parallels' and
553             not self._host.has_min_version(MIN_LIBVIRT_PARALLELS_VERSION)):
554             raise exception.NovaException(
555                 _('Running Nova with parallels virt_type requires '
556                   'libvirt version %s') %
557                 self._version_to_string(MIN_LIBVIRT_PARALLELS_VERSION))
558 
559         # TODO(mriedem): We plan to move to a minimum required version of
560         # libvirt 0.10.2 in the 13.0.0 'M' release so if we're running with
561         # less than that now, log a warning.
562         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
563             LOG.warning(_LW('Running Nova with a libvirt version less than '
564                             '%(version)s is deprecated. The required minimum '
565                             'version of libvirt will be raised to %(version)s '
566                             'in the 13.0.0 release.'),
567                         {'version': self._version_to_string(
568                             NEXT_MIN_LIBVIRT_VERSION)})
569 
570     # TODO(sahid): This method is targeted for removal when the tests
571     # have been updated to avoid its use
572     #
573     # All libvirt API calls on the libvirt.Connect object should be
574     # encapsulated by methods on the nova.virt.libvirt.host.Host
575     # object, rather than directly invoking the libvirt APIs. The goal
576     # is to avoid a direct dependency on the libvirt API from the
577     # driver.py file.
578     def _get_connection(self):
579         return self._host.get_connection()
580 
581     _conn = property(_get_connection)
582 
583     @staticmethod
584     def _uri():
585         if CONF.libvirt.virt_type == 'uml':
586             uri = CONF.libvirt.connection_uri or 'uml:///system'
587         elif CONF.libvirt.virt_type == 'xen':
588             uri = CONF.libvirt.connection_uri or 'xen:///'
589         elif CONF.libvirt.virt_type == 'lxc':
590             uri = CONF.libvirt.connection_uri or 'lxc:///'
591         elif CONF.libvirt.virt_type == 'parallels':
592             uri = CONF.libvirt.connection_uri or 'parallels:///system'
593         else:
594             uri = CONF.libvirt.connection_uri or 'qemu:///system'
595         return uri
596 
597     def instance_exists(self, instance):
598         """Efficient override of base instance_exists method."""
599         try:
600             self._host.get_guest(instance)
601             return True
602         except exception.NovaException:
603             return False
604 
605     def list_instances(self):
606         names = []
607         for dom in self._host.list_instance_domains(only_running=False):
608             names.append(dom.name())
609 
610         return names
611 
612     def list_instance_uuids(self):
613         uuids = []
614         for dom in self._host.list_instance_domains(only_running=False):
615             uuids.append(dom.UUIDString())
616 
617         return uuids
618 
619     def plug_vifs(self, instance, network_info):
620         """Plug VIFs into networks."""
621         for vif in network_info:
622             self.vif_driver.plug(instance, vif)
623 
624     def _unplug_vifs(self, instance, network_info, ignore_errors):
625         """Unplug VIFs from networks."""
626         for vif in network_info:
627             try:
628                 self.vif_driver.unplug(instance, vif)
629             except exception.NovaException:
630                 if not ignore_errors:
631                     raise
632 
633     def unplug_vifs(self, instance, network_info):
634         self._unplug_vifs(instance, network_info, False)
635 
636     def _teardown_container(self, instance):
637         inst_path = libvirt_utils.get_instance_path(instance)
638         container_dir = os.path.join(inst_path, 'rootfs')
639         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
640         disk.teardown_container(container_dir, rootfs_dev)
641 
642     def _destroy(self, instance, attempt=1):
643         try:
644             guest = self._host.get_guest(instance)
645         except exception.InstanceNotFound:
646             guest = None
647 
648         # If the instance is already terminated, we're still happy
649         # Otherwise, destroy it
650         old_domid = -1
651         if guest is not None:
652             try:
653                 old_domid = guest.id
654                 guest.poweroff()
655 
656             except libvirt.libvirtError as e:
657                 is_okay = False
658                 errcode = e.get_error_code()
659                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
660                     # Domain already gone. This can safely be ignored.
661                     is_okay = True
662                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
663                     # If the instance is already shut off, we get this:
664                     # Code=55 Error=Requested operation is not valid:
665                     # domain is not running
666 
667                     # TODO(sahid): At this point we should be a Guest object
668                     state = self._get_power_state(guest._domain)
669                     if state == power_state.SHUTDOWN:
670                         is_okay = True
671                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
672                     errmsg = e.get_error_message()
673                     if (CONF.libvirt.virt_type == 'lxc' and
674                         errmsg == 'internal error: '
675                                   'Some processes refused to die'):
676                         # Some processes in the container didn't die
677                         # fast enough for libvirt. The container will
678                         # eventually die. For now, move on and let
679                         # the wait_for_destroy logic take over.
680                         is_okay = True
681                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
682                     LOG.warn(_LW("Cannot destroy instance, operation time "
683                                  "out"),
684                             instance=instance)
685                     reason = _("operation time out")
686                     raise exception.InstancePowerOffFailure(reason=reason)
687                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
688                     if e.get_int1() == errno.EBUSY:
689                         # NOTE(danpb): When libvirt kills a process it sends it
690                         # SIGTERM first and waits 10 seconds. If it hasn't gone
691                         # it sends SIGKILL and waits another 5 seconds. If it
692                         # still hasn't gone then you get this EBUSY error.
693                         # Usually when a QEMU process fails to go away upon
694                         # SIGKILL it is because it is stuck in an
695                         # uninterruptable kernel sleep waiting on I/O from
696                         # some non-responsive server.
697                         # Given the CPU load of the gate tests though, it is
698                         # conceivable that the 15 second timeout is too short,
699                         # particularly if the VM running tempest has a high
700                         # steal time from the cloud host. ie 15 wallclock
701                         # seconds may have passed, but the VM might have only
702                         # have a few seconds of scheduled run time.
703                         LOG.warn(_LW('Error from libvirt during destroy. '
704                                      'Code=%(errcode)s Error=%(e)s; '
705                                      'attempt %(attempt)d of 3'),
706                                  {'errcode': errcode, 'e': e,
707                                   'attempt': attempt},
708                                  instance=instance)
709                         with excutils.save_and_reraise_exception() as ctxt:
710                             # Try up to 3 times before giving up.
711                             if attempt < 3:
712                                 ctxt.reraise = False
713                                 self._destroy(instance, attempt + 1)
714                                 return
715 
716                 if not is_okay:
717                     with excutils.save_and_reraise_exception():
718                         LOG.error(_LE('Error from libvirt during destroy. '
719                                       'Code=%(errcode)s Error=%(e)s'),
720                                   {'errcode': errcode, 'e': e},
721                                   instance=instance)
722 
723         def _wait_for_destroy(expected_domid):
724             """Called at an interval until the VM is gone."""
725             # NOTE(vish): If the instance disappears during the destroy
726             #             we ignore it so the cleanup can still be
727             #             attempted because we would prefer destroy to
728             #             never fail.
729             try:
730                 dom_info = self.get_info(instance)
731                 state = dom_info.state
732                 new_domid = dom_info.id
733             except exception.InstanceNotFound:
734                 LOG.info(_LI("During wait destroy, instance disappeared."),
735                          instance=instance)
736                 raise loopingcall.LoopingCallDone()
737 
738             if state == power_state.SHUTDOWN:
739                 LOG.info(_LI("Instance destroyed successfully."),
740                          instance=instance)
741                 raise loopingcall.LoopingCallDone()
742 
743             # NOTE(wangpan): If the instance was booted again after destroy,
744             #                this may be a endless loop, so check the id of
745             #                domain here, if it changed and the instance is
746             #                still running, we should destroy it again.
747             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
748             if new_domid != expected_domid:
749                 LOG.info(_LI("Instance may be started again."),
750                          instance=instance)
751                 kwargs['is_running'] = True
752                 raise loopingcall.LoopingCallDone()
753 
754         kwargs = {'is_running': False}
755         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
756                                                      old_domid)
757         timer.start(interval=0.5).wait()
758         if kwargs['is_running']:
759             LOG.info(_LI("Going to destroy instance again."),
760                      instance=instance)
761             self._destroy(instance)
762         else:
763             # NOTE(GuanQiang): teardown container to avoid resource leak
764             if CONF.libvirt.virt_type == 'lxc':
765                 self._teardown_container(instance)
766 
767     def destroy(self, context, instance, network_info, block_device_info=None,
768                 destroy_disks=True, migrate_data=None):
769         self._destroy(instance)
770         self.cleanup(context, instance, network_info, block_device_info,
771                      destroy_disks, migrate_data)
772 
773     def _undefine_domain(self, instance):
774         try:
775             guest = self._host.get_guest(instance)
776             try:
777                 guest.delete_configuration()
778             except libvirt.libvirtError as e:
779                 with excutils.save_and_reraise_exception():
780                     errcode = e.get_error_code()
781                     LOG.error(_LE('Error from libvirt during undefine. '
782                                   'Code=%(errcode)s Error=%(e)s'),
783                               {'errcode': errcode, 'e': e}, instance=instance)
784         except exception.InstanceNotFound:
785             pass
786 
787     def cleanup(self, context, instance, network_info, block_device_info=None,
788                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
789         if destroy_vifs:
790             self._unplug_vifs(instance, network_info, True)
791 
792         retry = True
793         while retry:
794             try:
795                 self.unfilter_instance(instance, network_info)
796             except libvirt.libvirtError as e:
797                 try:
798                     state = self.get_info(instance).state
799                 except exception.InstanceNotFound:
800                     state = power_state.SHUTDOWN
801 
802                 if state != power_state.SHUTDOWN:
803                     LOG.warn(_LW("Instance may be still running, destroy "
804                                  "it again."), instance=instance)
805                     self._destroy(instance)
806                 else:
807                     retry = False
808                     errcode = e.get_error_code()
809                     LOG.exception(_LE('Error from libvirt during unfilter. '
810                                       'Code=%(errcode)s Error=%(e)s'),
811                                   {'errcode': errcode, 'e': e},
812                                   instance=instance)
813                     reason = "Error unfiltering instance."
814                     raise exception.InstanceTerminationFailure(reason=reason)
815             except Exception:
816                 retry = False
817                 raise
818             else:
819                 retry = False
820 
821         # FIXME(wangpan): if the instance is booted again here, such as the
822         #                 the soft reboot operation boot it here, it will
823         #                 become "running deleted", should we check and destroy
824         #                 it at the end of this method?
825 
826         # NOTE(vish): we disconnect from volumes regardless
827         block_device_mapping = driver.block_device_info_get_mapping(
828             block_device_info)
829         for vol in block_device_mapping:
830             connection_info = vol['connection_info']
831             disk_dev = vol['mount_device']
832             if disk_dev is not None:
833                 disk_dev = disk_dev.rpartition("/")[2]
834 
835             if ('data' in connection_info and
836                     'volume_id' in connection_info['data']):
837                 volume_id = connection_info['data']['volume_id']
838                 encryption = encryptors.get_encryption_metadata(
839                     context, self._volume_api, volume_id, connection_info)
840 
841                 if encryption:
842                     # The volume must be detached from the VM before
843                     # disconnecting it from its encryptor. Otherwise, the
844                     # encryptor may report that the volume is still in use.
845                     encryptor = self._get_volume_encryptor(connection_info,
846                                                            encryption)
847                     encryptor.detach_volume(**encryption)
848 
849             try:
850                 self._disconnect_volume(connection_info, disk_dev)
851             except Exception as exc:
852                 with excutils.save_and_reraise_exception() as ctxt:
853                     if destroy_disks:
854                         # Don't block on Volume errors if we're trying to
855                         # delete the instance as we may be partially created
856                         # or deleted
857                         ctxt.reraise = False
858                         LOG.warn(_LW("Ignoring Volume Error on vol %(vol_id)s "
859                                      "during delete %(exc)s"),
860                                  {'vol_id': vol.get('volume_id'), 'exc': exc},
861                                  instance=instance)
862 
863         if destroy_disks:
864             # NOTE(haomai): destroy volumes if needed
865             if CONF.libvirt.images_type == 'lvm':
866                 self._cleanup_lvm(instance)
867             if CONF.libvirt.images_type == 'rbd':
868                 self._cleanup_rbd(instance)
869 
870         if destroy_disks or (
871                 migrate_data and migrate_data.get('is_shared_block_storage',
872                                                   False)):
873             attempts = int(instance.system_metadata.get('clean_attempts',
874                                                         '0'))
875             success = self.delete_instance_files(instance)
876             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
877             # task in the compute manager. The tight coupling is not great...
878             instance.system_metadata['clean_attempts'] = str(attempts + 1)
879             if success:
880                 instance.cleaned = True
881             instance.save()
882 
883         if CONF.serial_console.enabled:
884             try:
885                 serials = self._get_serial_ports_from_instance(instance)
886             except exception.InstanceNotFound:
887                 # Serial ports already gone. Nothing to release.
888                 serials = ()
889             for hostname, port in serials:
890                 serial_console.release_port(host=hostname, port=port)
891 
892         self._undefine_domain(instance)
893 
894     def _detach_encrypted_volumes(self, instance):
895         """Detaches encrypted volumes attached to instance."""
896         disks = jsonutils.loads(self.get_instance_disk_info(instance))
897         encrypted_volumes = filter(dmcrypt.is_encrypted,
898                                    [disk['path'] for disk in disks])
899         for path in encrypted_volumes:
900             dmcrypt.delete_volume(path)
901 
902     def _get_serial_ports_from_instance(self, instance, mode=None):
903         """Returns an iterator over serial port(s) configured on instance.
904 
905         :param mode: Should be a value in (None, bind, connect)
906         """
907         guest = self._host.get_guest(instance)
908 
909         xml = guest.get_xml_desc()
910         tree = etree.fromstring(xml)
911 
912         # The 'serial' device is the base for x86 platforms. Other platforms
913         # (e.g. kvm on system z = arch.S390X) can only use 'console' devices.
914         xpath_mode = "[@mode='%s']" % mode if mode else ""
915         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
916         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
917 
918         tcp_devices = tree.findall(serial_tcp)
919         if len(tcp_devices) == 0:
920             tcp_devices = tree.findall(console_tcp)
921         for source in tcp_devices:
922             yield (source.get("host"), int(source.get("service")))
923 
924     @staticmethod
925     def _get_rbd_driver():
926         return rbd_utils.RBDDriver(
927                 pool=CONF.libvirt.images_rbd_pool,
928                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
929                 rbd_user=CONF.libvirt.rbd_user)
930 
931     def _cleanup_rbd(self, instance):
932         LibvirtDriver._get_rbd_driver().cleanup_volumes(instance)
933 
934     def _cleanup_lvm(self, instance):
935         """Delete all LVM disks for given instance object."""
936         if instance.get('ephemeral_key_uuid') is not None:
937             self._detach_encrypted_volumes(instance)
938 
939         disks = self._lvm_disks(instance)
940         if disks:
941             lvm.remove_volumes(disks)
942 
943     def _lvm_disks(self, instance):
944         """Returns all LVM disks for given instance object."""
945         if CONF.libvirt.images_volume_group:
946             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
947             if not os.path.exists(vg):
948                 return []
949             pattern = '%s_' % instance.uuid
950 
951             def belongs_to_instance(disk):
952                 return disk.startswith(pattern)
953 
954             def fullpath(name):
955                 return os.path.join(vg, name)
956 
957             logical_volumes = lvm.list_volumes(vg)
958 
959             disk_names = filter(belongs_to_instance, logical_volumes)
960             disks = map(fullpath, disk_names)
961             return disks
962         return []
963 
964     def get_volume_connector(self, instance):
965         root_helper = utils._get_root_helper()
966         return connector.get_connector_properties(
967             root_helper, CONF.my_block_storage_ip,
968             CONF.libvirt.iscsi_use_multipath,
969             enforce_multipath=True,
970             host=CONF.host)
971 
972     def _cleanup_resize(self, instance, network_info):
973         # NOTE(wangpan): we get the pre-grizzly instance path firstly,
974         #                so the backup dir of pre-grizzly instance can
975         #                be deleted correctly with grizzly or later nova.
976         pre_grizzly_name = libvirt_utils.get_instance_path(instance,
977                                                            forceold=True)
978         target = pre_grizzly_name + '_resize'
979         if not os.path.exists(target):
980             target = libvirt_utils.get_instance_path(instance) + '_resize'
981 
982         if os.path.exists(target):
983             # Deletion can fail over NFS, so retry the deletion as required.
984             # Set maximum attempt as 5, most test can remove the directory
985             # for the second time.
986             utils.execute('rm', '-rf', target, delay_on_retry=True,
987                           attempts=5)
988 
989         if instance.host != CONF.host:
990             self._undefine_domain(instance)
991             self.unplug_vifs(instance, network_info)
992             self.unfilter_instance(instance, network_info)
993 
994     def _get_volume_driver(self, connection_info):
995         driver_type = connection_info.get('driver_volume_type')
996         if driver_type not in self.volume_drivers:
997             raise exception.VolumeDriverNotFound(driver_type=driver_type)
998         return self.volume_drivers[driver_type]
999 
1000     def _connect_volume(self, connection_info, disk_info):
1001         driver = self._get_volume_driver(connection_info)
1002         driver.connect_volume(connection_info, disk_info)
1003 
1004     def _disconnect_volume(self, connection_info, disk_dev):
1005         driver = self._get_volume_driver(connection_info)
1006         driver.disconnect_volume(connection_info, disk_dev)
1007 
1008     def _get_volume_config(self, connection_info, disk_info):
1009         driver = self._get_volume_driver(connection_info)
1010         return driver.get_config(connection_info, disk_info)
1011 
1012     def _get_volume_encryptor(self, connection_info, encryption):
1013         encryptor = encryptors.get_volume_encryptor(connection_info,
1014                                                     **encryption)
1015         return encryptor
1016 
1017     def attach_volume(self, context, connection_info, instance, mountpoint,
1018                       disk_bus=None, device_type=None, encryption=None):
1019         image_meta = utils.get_image_from_system_metadata(
1020             instance.system_metadata)
1021 
1022         guest = self._host.get_guest(instance)
1023 
1024         disk_dev = mountpoint.rpartition("/")[2]
1025         bdm = {
1026             'device_name': disk_dev,
1027             'disk_bus': disk_bus,
1028             'device_type': device_type}
1029 
1030         # Note(cfb): If the volume has a custom block size, check that
1031         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1032         #            presence of a block size is considered mandatory by
1033         #            cinder so we fail if we can't honor the request.
1034         data = {}
1035         if ('data' in connection_info):
1036             data = connection_info['data']
1037         if ('logical_block_size' in data or 'physical_block_size' in data):
1038             if ((CONF.libvirt.virt_type != "kvm" and
1039                  CONF.libvirt.virt_type != "qemu")):
1040                 msg = _("Volume sets block size, but the current "
1041                         "libvirt hypervisor '%s' does not support custom "
1042                         "block size") % CONF.libvirt.virt_type
1043                 raise exception.InvalidHypervisorType(msg)
1044 
1045             if not self._host.has_min_version(MIN_LIBVIRT_BLOCKIO_VERSION):
1046                 ver = ".".join([str(x) for x in MIN_LIBVIRT_BLOCKIO_VERSION])
1047                 msg = _("Volume sets block size, but libvirt '%s' or later is "
1048                         "required.") % ver
1049                 raise exception.Invalid(msg)
1050 
1051         disk_info = blockinfo.get_info_from_bdm(CONF.libvirt.virt_type,
1052                                                 image_meta, bdm)
1053         self._connect_volume(connection_info, disk_info)
1054         conf = self._get_volume_config(connection_info, disk_info)
1055         self._set_cache_mode(conf)
1056 
1057         try:
1058             state = self._get_power_state(guest._domain)
1059             live = state in (power_state.RUNNING, power_state.PAUSED)
1060 
1061             if encryption:
1062                 encryptor = self._get_volume_encryptor(connection_info,
1063                                                        encryption)
1064                 encryptor.attach_volume(context, **encryption)
1065 
1066             guest.attach_device(conf, persistent=True, live=live)
1067         except Exception as ex:
1068             LOG.exception(_LE('Failed to attach volume at mountpoint: %s'),
1069                           mountpoint, instance=instance)
1070             if isinstance(ex, libvirt.libvirtError):
1071                 errcode = ex.get_error_code()
1072                 if errcode == libvirt.VIR_ERR_OPERATION_FAILED:
1073                     self._disconnect_volume(connection_info, disk_dev)
1074                     raise exception.DeviceIsBusy(device=disk_dev)
1075 
1076             with excutils.save_and_reraise_exception():
1077                 self._disconnect_volume(connection_info, disk_dev)
1078 
1079     def _swap_volume(self, guest, disk_path, new_path, resize_to):
1080         """Swap existing disk with a new block device."""
1081         dev = guest.get_block_device(disk_path)
1082 
1083         # Save a copy of the domain's persistent XML file
1084         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1085 
1086         # Abort is an idempotent operation, so make sure any block
1087         # jobs which may have failed are ended.
1088         try:
1089             dev.abort_job()
1090         except Exception:
1091             pass
1092 
1093         try:
1094             # NOTE (rmk): blockRebase cannot be executed on persistent
1095             #             domains, so we need to temporarily undefine it.
1096             #             If any part of this block fails, the domain is
1097             #             re-defined regardless.
1098             if guest.has_persistent_configuration():
1099                 guest.delete_configuration()
1100 
1101             # Start copy with VIR_DOMAIN_REBASE_REUSE_EXT flag to
1102             # allow writing to existing external volume file
1103             dev.rebase(new_path, copy=True, reuse_ext=True)
1104 
1105             while dev.wait_for_job():
1106                 time.sleep(0.5)
1107 
1108             dev.abort_job(pivot=True)
1109             if resize_to:
1110                 # NOTE(alex_xu): domain.blockJobAbort isn't sync call. This
1111                 # is bug in libvirt. So we need waiting for the pivot is
1112                 # finished. libvirt bug #1119173
1113                 while dev.wait_for_job(wait_for_job_clean=True):
1114                     time.sleep(0.5)
1115                 dev.resize(resize_to * units.Gi / units.Ki)
1116         finally:
1117             self._host.write_instance_config(xml)
1118 
1119     def swap_volume(self, old_connection_info,
1120                     new_connection_info, instance, mountpoint, resize_to):
1121 
1122         guest = self._host.get_guest(instance)
1123 
1124         disk_dev = mountpoint.rpartition("/")[2]
1125         if not guest.get_disk(disk_dev):
1126             raise exception.DiskNotFound(location=disk_dev)
1127         disk_info = {
1128             'dev': disk_dev,
1129             'bus': blockinfo.get_disk_bus_for_disk_dev(
1130                 CONF.libvirt.virt_type, disk_dev),
1131             'type': 'disk',
1132             }
1133         self._connect_volume(new_connection_info, disk_info)
1134         conf = self._get_volume_config(new_connection_info, disk_info)
1135         if not conf.source_path:
1136             self._disconnect_volume(new_connection_info, disk_dev)
1137             raise NotImplementedError(_("Swap only supports host devices"))
1138 
1139         # Save updates made in connection_info when connect_volume was called
1140         volume_id = new_connection_info.get('serial')
1141         bdm = objects.BlockDeviceMapping.get_by_volume_id(
1142             nova_context.get_admin_context(), volume_id)
1143         driver_bdm = driver_block_device.DriverVolumeBlockDevice(bdm)
1144         driver_bdm['connection_info'] = new_connection_info
1145         driver_bdm.save()
1146 
1147         self._swap_volume(guest, disk_dev, conf.source_path, resize_to)
1148         self._disconnect_volume(old_connection_info, disk_dev)
1149 
1150     def _get_existing_domain_xml(self, instance, network_info,
1151                                  block_device_info=None):
1152         try:
1153             guest = self._host.get_guest(instance)
1154             xml = guest.get_xml_desc()
1155         except exception.InstanceNotFound:
1156             image_meta = utils.get_image_from_system_metadata(
1157                 instance.system_metadata)
1158             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1159                                                 instance,
1160                                                 image_meta,
1161                                                 block_device_info)
1162             xml = self._get_guest_xml(nova_context.get_admin_context(),
1163                                       instance, network_info, disk_info,
1164                                       image_meta,
1165                                       block_device_info=block_device_info)
1166         return xml
1167 
1168     def detach_volume(self, connection_info, instance, mountpoint,
1169                       encryption=None):
1170         disk_dev = mountpoint.rpartition("/")[2]
1171         try:
1172             guest = self._host.get_guest(instance)
1173             conf = guest.get_disk(disk_dev)
1174             if not conf:
1175                 raise exception.DiskNotFound(location=disk_dev)
1176 
1177             state = self._get_power_state(guest._domain)
1178             live = state in (power_state.RUNNING, power_state.PAUSED)
1179             guest.detach_device(conf, persistent=True, live=live)
1180 
1181             if encryption:
1182                 # The volume must be detached from the VM before
1183                 # disconnecting it from its encryptor. Otherwise, the
1184                 # encryptor may report that the volume is still in use.
1185                 encryptor = self._get_volume_encryptor(connection_info,
1186                                                        encryption)
1187                 encryptor.detach_volume(**encryption)
1188         except exception.InstanceNotFound:
1189             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1190             #                will throw InstanceNotFound exception. Need to
1191             #                disconnect volume under this circumstance.
1192             LOG.warn(_LW("During detach_volume, instance disappeared."))
1193         except libvirt.libvirtError as ex:
1194             # NOTE(vish): This is called to cleanup volumes after live
1195             #             migration, so we should still disconnect even if
1196             #             the instance doesn't exist here anymore.
1197             error_code = ex.get_error_code()
1198             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1199                 # NOTE(vish):
1200                 LOG.warn(_LW("During detach_volume, instance disappeared."))
1201             else:
1202                 raise
1203 
1204         self._disconnect_volume(connection_info, disk_dev)
1205 
1206     def attach_interface(self, instance, image_meta, vif):
1207         guest = self._host.get_guest(instance)
1208 
1209         self.vif_driver.plug(instance, vif)
1210         self.firewall_driver.setup_basic_filtering(instance, [vif])
1211         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1212                                          instance.flavor,
1213                                          CONF.libvirt.virt_type)
1214         try:
1215             state = self._get_power_state(guest._domain)
1216             live = state in (power_state.RUNNING, power_state.PAUSED)
1217             guest.attach_device(cfg, persistent=True, live=live)
1218         except libvirt.libvirtError:
1219             LOG.error(_LE('attaching network adapter failed.'),
1220                      instance=instance, exc_info=True)
1221             self.vif_driver.unplug(instance, vif)
1222             raise exception.InterfaceAttachFailed(
1223                     instance_uuid=instance.uuid)
1224 
1225     def detach_interface(self, instance, vif):
1226         guest = self._host.get_guest(instance)
1227         cfg = self.vif_driver.get_config(instance, vif, None, instance.flavor,
1228                                          CONF.libvirt.virt_type)
1229         try:
1230             self.vif_driver.unplug(instance, vif)
1231             state = self._get_power_state(guest._domain)
1232             live = state in (power_state.RUNNING, power_state.PAUSED)
1233             guest.detach_device(cfg, persistent=True, live=live)
1234         except libvirt.libvirtError as ex:
1235             error_code = ex.get_error_code()
1236             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1237                 LOG.warn(_LW("During detach_interface, "
1238                              "instance disappeared."),
1239                          instance=instance)
1240             else:
1241                 LOG.error(_LE('detaching network adapter failed.'),
1242                          instance=instance, exc_info=True)
1243                 raise exception.InterfaceDetachFailed(
1244                         instance_uuid=instance.uuid)
1245 
1246     def _create_snapshot_metadata(self, image_meta, instance,
1247                                   img_fmt, snp_name):
1248         metadata = {'is_public': False,
1249                     'status': 'active',
1250                     'name': snp_name,
1251                     'properties': {
1252                                    'kernel_id': instance.kernel_id,
1253                                    'image_location': 'snapshot',
1254                                    'image_state': 'available',
1255                                    'owner_id': instance.project_id,
1256                                    'ramdisk_id': instance.ramdisk_id,
1257                                    }
1258                     }
1259         if instance.os_type:
1260             metadata['properties']['os_type'] = instance.os_type
1261 
1262         # NOTE(vish): glance forces ami disk format to be ami
1263         if image_meta.get('disk_format') == 'ami':
1264             metadata['disk_format'] = 'ami'
1265         else:
1266             metadata['disk_format'] = img_fmt
1267 
1268         metadata['container_format'] = image_meta.get(
1269             'container_format', 'bare')
1270 
1271         return metadata
1272 
1273     def snapshot(self, context, instance, image_id, update_task_state):
1274         """Create snapshot from a running VM instance.
1275 
1276         This command only works with qemu 0.14+
1277         """
1278         try:
1279             guest = self._host.get_guest(instance)
1280 
1281             # TODO(sahid): We are converting all calls from a
1282             # virDomain object to use nova.virt.libvirt.Guest.
1283             # We should be able to remove virt_dom at the end.
1284             virt_dom = guest._domain
1285         except exception.InstanceNotFound:
1286             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1287 
1288         image_meta = utils.get_image_from_system_metadata(
1289             instance.system_metadata)
1290 
1291         snapshot = self._image_api.get(context, image_id)
1292 
1293         disk_path = libvirt_utils.find_disk(virt_dom)
1294         source_format = libvirt_utils.get_disk_type(disk_path)
1295 
1296         image_format = CONF.libvirt.snapshot_image_format or source_format
1297 
1298         # NOTE(bfilippov): save lvm and rbd as raw
1299         if image_format == 'lvm' or image_format == 'rbd':
1300             image_format = 'raw'
1301 
1302         metadata = self._create_snapshot_metadata(image_meta,
1303                                                   instance,
1304                                                   image_format,
1305                                                   snapshot['name'])
1306 
1307         snapshot_name = uuid.uuid4().hex
1308 
1309         state = self._get_power_state(virt_dom)
1310 
1311         # NOTE(rmk): Live snapshots require QEMU 1.3 and Libvirt 1.0.0.
1312         #            These restrictions can be relaxed as other configurations
1313         #            can be validated.
1314         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1315         #               cold snapshots. Currently, checking for encryption is
1316         #               redundant because LVM supports only cold snapshots.
1317         #               It is necessary in case this situation changes in the
1318         #               future.
1319         if (self._host.has_min_version(MIN_LIBVIRT_LIVESNAPSHOT_VERSION,
1320                                        MIN_QEMU_LIVESNAPSHOT_VERSION,
1321                                        host.HV_DRIVER_QEMU)
1322              and source_format not in ('lvm', 'rbd')
1323              and not CONF.ephemeral_storage_encryption.enabled
1324              and not CONF.workarounds.disable_libvirt_livesnapshot):
1325             live_snapshot = True
1326             # Abort is an idempotent operation, so make sure any block
1327             # jobs which may have failed are ended. This operation also
1328             # confirms the running instance, as opposed to the system as a
1329             # whole, has a new enough version of the hypervisor (bug 1193146).
1330             try:
1331                 virt_dom.blockJobAbort(disk_path, 0)
1332             except libvirt.libvirtError as ex:
1333                 error_code = ex.get_error_code()
1334                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1335                     live_snapshot = False
1336                 else:
1337                     pass
1338         else:
1339             live_snapshot = False
1340 
1341         # NOTE(rmk): We cannot perform live snapshots when a managedSave
1342         #            file is present, so we will use the cold/legacy method
1343         #            for instances which are shutdown.
1344         if state == power_state.SHUTDOWN:
1345             live_snapshot = False
1346 
1347         # NOTE(dkang): managedSave does not work for LXC
1348         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1349             if state == power_state.RUNNING or state == power_state.PAUSED:
1350                 self._detach_pci_devices(guest,
1351                     pci_manager.get_instance_pci_devs(instance))
1352                 self._detach_sriov_ports(context, instance, guest)
1353                 guest.save_memory_state()
1354 
1355         snapshot_backend = self.image_backend.snapshot(instance,
1356                 disk_path,
1357                 image_type=source_format)
1358 
1359         if live_snapshot:
1360             LOG.info(_LI("Beginning live snapshot process"),
1361                      instance=instance)
1362         else:
1363             LOG.info(_LI("Beginning cold snapshot process"),
1364                      instance=instance)
1365 
1366         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1367 
1368         try:
1369             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1370                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1371             metadata['location'] = snapshot_backend.direct_snapshot(
1372                 context, snapshot_name, image_format, image_id,
1373                 instance.image_ref)
1374             self._image_api.update(context, image_id, metadata,
1375                                    purge_props=False)
1376             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1377                                   instance)
1378         except (exception.ImageUnacceptable, exception.Forbidden) as e:
1379             LOG.warn(_LW("Performing standard snapshot because "
1380                          "direct snapshot failed: %(error)s"), {'error': e})
1381             metadata.pop('location', None)
1382             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1383                               expected_state=task_states.IMAGE_UPLOADING)
1384 
1385             snapshot_directory = CONF.libvirt.snapshots_directory
1386             fileutils.ensure_tree(snapshot_directory)
1387             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1388                 try:
1389                     out_path = os.path.join(tmpdir, snapshot_name)
1390                     if live_snapshot:
1391                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1392                         os.chmod(tmpdir, 0o701)
1393                         self._live_snapshot(context, instance, guest,
1394                                             disk_path, out_path,
1395                                             image_format, image_meta)
1396                     else:
1397                         snapshot_backend.snapshot_extract(out_path,
1398                                                           image_format)
1399                 finally:
1400                     self._snapshot_domain(context, live_snapshot, virt_dom,
1401                                           state, instance)
1402                     LOG.info(_LI("Snapshot extracted, beginning image upload"),
1403                              instance=instance)
1404 
1405                 # Upload that image to the image service
1406                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1407                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1408                 with libvirt_utils.file_open(out_path) as image_file:
1409                     self._image_api.update(context,
1410                                            image_id,
1411                                            metadata,
1412                                            image_file)
1413 
1414         LOG.info(_LI("Snapshot image upload complete"), instance=instance)
1415 
1416     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1417                          instance):
1418         guest = None
1419         # NOTE(dkang): because previous managedSave is not called
1420         #              for LXC, _create_domain must not be called.
1421         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1422             if state == power_state.RUNNING:
1423                 guest = self._create_domain(domain=virt_dom)
1424             elif state == power_state.PAUSED:
1425                 guest = self._create_domain(domain=virt_dom, pause=True)
1426 
1427             if guest is not None:
1428                 self._attach_pci_devices(
1429                     guest, pci_manager.get_instance_pci_devs(instance))
1430                 self._attach_sriov_ports(context, instance, guest)
1431 
1432     def _can_quiesce(self, image_meta):
1433         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
1434             return (False, _('Only KVM and QEMU are supported'))
1435 
1436         if not self._host.has_min_version(MIN_LIBVIRT_FSFREEZE_VERSION):
1437             ver = ".".join([str(x) for x in MIN_LIBVIRT_FSFREEZE_VERSION])
1438             return (False, _('Quiescing requires libvirt version %(version)s '
1439                              'or greater') % {'version': ver})
1440 
1441         img_meta_prop = image_meta.get('properties', {}) if image_meta else {}
1442         hw_qga = img_meta_prop.get('hw_qemu_guest_agent', '')
1443         if not strutils.bool_from_string(hw_qga):
1444             return (False, _('QEMU guest agent is not enabled'))
1445 
1446         return (True, None)
1447 
1448     def _set_quiesced(self, context, instance, image_meta, quiesced):
1449         supported, reason = self._can_quiesce(image_meta)
1450         if not supported:
1451             raise exception.InstanceQuiesceNotSupported(
1452                 instance_id=instance.uuid, reason=reason)
1453 
1454         try:
1455             guest = self._host.get_guest(instance)
1456 
1457             # TODO(sahid): We are converting all calls from a
1458             # virDomain object to use nova.virt.libvirt.Guest.
1459             # We should be able to remove domain at the end.
1460             domain = guest._domain
1461             if quiesced:
1462                 domain.fsFreeze()
1463             else:
1464                 domain.fsThaw()
1465         except libvirt.libvirtError as ex:
1466             error_code = ex.get_error_code()
1467             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
1468                      '[Error Code %(error_code)s] %(ex)s')
1469                    % {'instance_name': instance.name,
1470                       'error_code': error_code, 'ex': ex})
1471             raise exception.NovaException(msg)
1472 
1473     def quiesce(self, context, instance, image_meta):
1474         """Freeze the guest filesystems to prepare for snapshot.
1475 
1476         The qemu-guest-agent must be setup to execute fsfreeze.
1477         """
1478         self._set_quiesced(context, instance, image_meta, True)
1479 
1480     def unquiesce(self, context, instance, image_meta):
1481         """Thaw the guest filesystems after snapshot."""
1482         self._set_quiesced(context, instance, image_meta, False)
1483 
1484     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
1485                        image_format, image_meta):
1486         """Snapshot an instance without downtime."""
1487         dev = guest.get_block_device(disk_path)
1488 
1489         # Save a copy of the domain's persistent XML file
1490         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1491 
1492         # Abort is an idempotent operation, so make sure any block
1493         # jobs which may have failed are ended.
1494         try:
1495             dev.abort_job()
1496         except Exception:
1497             pass
1498 
1499         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
1500         #             in QEMU 1.3. In order to do this, we need to create
1501         #             a destination image with the original backing file
1502         #             and matching size of the instance root disk.
1503         src_disk_size = libvirt_utils.get_disk_size(disk_path)
1504         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
1505                                                             basename=False)
1506         disk_delta = out_path + '.delta'
1507         libvirt_utils.create_cow_image(src_back_path, disk_delta,
1508                                        src_disk_size)
1509 
1510         img_meta_prop = image_meta.get('properties', {}) if image_meta else {}
1511         require_quiesce = strutils.bool_from_string(
1512             img_meta_prop.get('os_require_quiesce', ''))
1513         if require_quiesce:
1514             self.quiesce(context, instance, image_meta)
1515 
1516         try:
1517             # NOTE (rmk): blockRebase cannot be executed on persistent
1518             #             domains, so we need to temporarily undefine it.
1519             #             If any part of this block fails, the domain is
1520             #             re-defined regardless.
1521             if guest.has_persistent_configuration():
1522                 guest.delete_configuration()
1523 
1524             # NOTE (rmk): Establish a temporary mirror of our root disk and
1525             #             issue an abort once we have a complete copy.
1526             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
1527 
1528             while dev.wait_for_job():
1529                 time.sleep(0.5)
1530 
1531             dev.abort_job()
1532             libvirt_utils.chown(disk_delta, os.getuid())
1533         finally:
1534             self._host.write_instance_config(xml)
1535             if require_quiesce:
1536                 self.unquiesce(context, instance, image_meta)
1537 
1538         # Convert the delta (CoW) image with a backing file to a flat
1539         # image with no backing file.
1540         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
1541                                        out_path, image_format)
1542 
1543     def _volume_snapshot_update_status(self, context, snapshot_id, status):
1544         """Send a snapshot status update to Cinder.
1545 
1546         This method captures and logs exceptions that occur
1547         since callers cannot do anything useful with these exceptions.
1548 
1549         Operations on the Cinder side waiting for this will time out if
1550         a failure occurs sending the update.
1551 
1552         :param context: security context
1553         :param snapshot_id: id of snapshot being updated
1554         :param status: new status value
1555 
1556         """
1557 
1558         try:
1559             self._volume_api.update_snapshot_status(context,
1560                                                     snapshot_id,
1561                                                     status)
1562         except Exception:
1563             LOG.exception(_LE('Failed to send updated snapshot status '
1564                               'to volume service.'))
1565 
1566     def _volume_snapshot_create(self, context, instance, domain,
1567                                 volume_id, new_file):
1568         """Perform volume snapshot.
1569 
1570            :param domain: VM that volume is attached to
1571            :param volume_id: volume UUID to snapshot
1572            :param new_file: relative path to new qcow2 file present on share
1573 
1574         """
1575 
1576         # TODO(sahid): An object Guest should be passed instead of
1577         # a "domain" as virDomain.
1578         guest = libvirt_guest.Guest(domain)
1579         xml = guest.get_xml_desc()
1580         xml_doc = etree.fromstring(xml)
1581 
1582         device_info = vconfig.LibvirtConfigGuest()
1583         device_info.parse_dom(xml_doc)
1584 
1585         disks_to_snap = []          # to be snapshotted by libvirt
1586         network_disks_to_snap = []  # network disks (netfs, gluster, etc.)
1587         disks_to_skip = []          # local disks not snapshotted
1588 
1589         for guest_disk in device_info.devices:
1590             if (guest_disk.root_name != 'disk'):
1591                 continue
1592 
1593             if (guest_disk.target_dev is None):
1594                 continue
1595 
1596             if (guest_disk.serial is None or guest_disk.serial != volume_id):
1597                 disks_to_skip.append(guest_disk.target_dev)
1598                 continue
1599 
1600             # disk is a Cinder volume with the correct volume_id
1601 
1602             disk_info = {
1603                 'dev': guest_disk.target_dev,
1604                 'serial': guest_disk.serial,
1605                 'current_file': guest_disk.source_path,
1606                 'source_protocol': guest_disk.source_protocol,
1607                 'source_name': guest_disk.source_name,
1608                 'source_hosts': guest_disk.source_hosts,
1609                 'source_ports': guest_disk.source_ports
1610             }
1611 
1612             # Determine path for new_file based on current path
1613             if disk_info['current_file'] is not None:
1614                 current_file = disk_info['current_file']
1615                 new_file_path = os.path.join(os.path.dirname(current_file),
1616                                              new_file)
1617                 disks_to_snap.append((current_file, new_file_path))
1618             elif disk_info['source_protocol'] in ('gluster', 'netfs'):
1619                 network_disks_to_snap.append((disk_info, new_file))
1620 
1621         if not disks_to_snap and not network_disks_to_snap:
1622             msg = _('Found no disk to snapshot.')
1623             raise exception.NovaException(msg)
1624 
1625         snapshot = vconfig.LibvirtConfigGuestSnapshot()
1626 
1627         for current_name, new_filename in disks_to_snap:
1628             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
1629             snap_disk.name = current_name
1630             snap_disk.source_path = new_filename
1631             snap_disk.source_type = 'file'
1632             snap_disk.snapshot = 'external'
1633             snap_disk.driver_name = 'qcow2'
1634 
1635             snapshot.add_disk(snap_disk)
1636 
1637         for disk_info, new_filename in network_disks_to_snap:
1638             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
1639             snap_disk.name = disk_info['dev']
1640             snap_disk.source_type = 'network'
1641             snap_disk.source_protocol = disk_info['source_protocol']
1642             snap_disk.snapshot = 'external'
1643             snap_disk.source_path = new_filename
1644             old_dir = disk_info['source_name'].split('/')[0]
1645             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
1646             snap_disk.source_hosts = disk_info['source_hosts']
1647             snap_disk.source_ports = disk_info['source_ports']
1648 
1649             snapshot.add_disk(snap_disk)
1650 
1651         for dev in disks_to_skip:
1652             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
1653             snap_disk.name = dev
1654             snap_disk.snapshot = 'no'
1655 
1656             snapshot.add_disk(snap_disk)
1657 
1658         snapshot_xml = snapshot.to_xml()
1659         LOG.debug("snap xml: %s", snapshot_xml)
1660 
1661         snap_flags = (libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_DISK_ONLY |
1662                       libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_NO_METADATA |
1663                       libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_REUSE_EXT)
1664 
1665         QUIESCE = libvirt.VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE
1666 
1667         try:
1668             domain.snapshotCreateXML(snapshot_xml,
1669                                      snap_flags | QUIESCE)
1670 
1671             return
1672         except libvirt.libvirtError:
1673             LOG.exception(_LE('Unable to create quiesced VM snapshot, '
1674                               'attempting again with quiescing disabled.'))
1675 
1676         try:
1677             domain.snapshotCreateXML(snapshot_xml, snap_flags)
1678         except libvirt.libvirtError:
1679             LOG.exception(_LE('Unable to create VM snapshot, '
1680                               'failing volume_snapshot operation.'))
1681 
1682             raise
1683 
1684     def _volume_refresh_connection_info(self, context, instance, volume_id):
1685         bdm = objects.BlockDeviceMapping.get_by_volume_id(context,
1686                                                           volume_id)
1687 
1688         driver_bdm = driver_block_device.convert_volume(bdm)
1689         if driver_bdm:
1690             driver_bdm.refresh_connection_info(context, instance,
1691                                                self._volume_api, self)
1692 
1693     def volume_snapshot_create(self, context, instance, volume_id,
1694                                create_info):
1695         """Create snapshots of a Cinder volume via libvirt.
1696 
1697         :param instance: VM instance object reference
1698         :param volume_id: id of volume being snapshotted
1699         :param create_info: dict of information used to create snapshots
1700                      - snapshot_id : ID of snapshot
1701                      - type : qcow2 / <other>
1702                      - new_file : qcow2 file created by Cinder which
1703                      becomes the VM's active image after
1704                      the snapshot is complete
1705         """
1706 
1707         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
1708                   {'c_info': create_info}, instance=instance)
1709 
1710         try:
1711             guest = self._host.get_guest(instance)
1712 
1713             # TODO(sahid): We are converting all calls from a
1714             # virDomain object to use nova.virt.libvirt.Guest.
1715             # We should be able to remove virt_dom at the end.
1716             virt_dom = guest._domain
1717         except exception.InstanceNotFound:
1718             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1719 
1720         if create_info['type'] != 'qcow2':
1721             raise exception.NovaException(_('Unknown type: %s') %
1722                                           create_info['type'])
1723 
1724         snapshot_id = create_info.get('snapshot_id', None)
1725         if snapshot_id is None:
1726             raise exception.NovaException(_('snapshot_id required '
1727                                             'in create_info'))
1728 
1729         try:
1730             self._volume_snapshot_create(context, instance, virt_dom,
1731                                          volume_id, create_info['new_file'])
1732         except Exception:
1733             with excutils.save_and_reraise_exception():
1734                 LOG.exception(_LE('Error occurred during '
1735                                   'volume_snapshot_create, '
1736                                   'sending error status to Cinder.'))
1737                 self._volume_snapshot_update_status(
1738                     context, snapshot_id, 'error')
1739 
1740         self._volume_snapshot_update_status(
1741             context, snapshot_id, 'creating')
1742 
1743         def _wait_for_snapshot():
1744             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
1745 
1746             if snapshot.get('status') != 'creating':
1747                 self._volume_refresh_connection_info(context, instance,
1748                                                      volume_id)
1749                 raise loopingcall.LoopingCallDone()
1750 
1751         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
1752         timer.start(interval=0.5).wait()
1753 
1754     def _volume_snapshot_delete(self, context, instance, volume_id,
1755                                 snapshot_id, delete_info=None):
1756         """Note:
1757             if file being merged into == active image:
1758                 do a blockRebase (pull) operation
1759             else:
1760                 do a blockCommit operation
1761             Files must be adjacent in snap chain.
1762 
1763         :param instance: instance object reference
1764         :param volume_id: volume UUID
1765         :param snapshot_id: snapshot UUID (unused currently)
1766         :param delete_info: {
1767             'type':              'qcow2',
1768             'file_to_merge':     'a.img',
1769             'merge_target_file': 'b.img' or None (if merging file_to_merge into
1770                                                   active image)
1771           }
1772 
1773 
1774         Libvirt blockjob handling required for this method is broken
1775         in versions of libvirt that do not contain:
1776         http://libvirt.org/git/?p=libvirt.git;h=0f9e67bfad (1.1.1)
1777         (Patch is pending in 1.0.5-maint branch as well, but we cannot detect
1778         libvirt 1.0.5.5 vs. 1.0.5.6 here.)
1779         """
1780 
1781         if not self._host.has_min_version(MIN_LIBVIRT_BLOCKJOBINFO_VERSION):
1782             ver = '.'.join([str(x) for x in MIN_LIBVIRT_BLOCKJOBINFO_VERSION])
1783             msg = _("Libvirt '%s' or later is required for online deletion "
1784                     "of volume snapshots.") % ver
1785             raise exception.Invalid(msg)
1786 
1787         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info)
1788 
1789         if delete_info['type'] != 'qcow2':
1790             msg = _('Unknown delete_info type %s') % delete_info['type']
1791             raise exception.NovaException(msg)
1792 
1793         try:
1794             guest = self._host.get_guest(instance)
1795         except exception.InstanceNotFound:
1796             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1797 
1798         # Find dev name
1799         my_dev = None
1800         active_disk = None
1801 
1802         xml = guest.get_xml_desc()
1803         xml_doc = etree.fromstring(xml)
1804 
1805         device_info = vconfig.LibvirtConfigGuest()
1806         device_info.parse_dom(xml_doc)
1807 
1808         active_disk_object = None
1809 
1810         for guest_disk in device_info.devices:
1811             if (guest_disk.root_name != 'disk'):
1812                 continue
1813 
1814             if (guest_disk.target_dev is None or guest_disk.serial is None):
1815                 continue
1816 
1817             if guest_disk.serial == volume_id:
1818                 my_dev = guest_disk.target_dev
1819 
1820                 active_disk = guest_disk.source_path
1821                 active_protocol = guest_disk.source_protocol
1822                 active_disk_object = guest_disk
1823                 break
1824 
1825         if my_dev is None or (active_disk is None and active_protocol is None):
1826             msg = _('Disk with id: %s '
1827                     'not found attached to instance.') % volume_id
1828             LOG.debug('Domain XML: %s', xml)
1829             raise exception.NovaException(msg)
1830 
1831         LOG.debug("found device at %s", my_dev)
1832 
1833         def _get_snap_dev(filename, backing_store):
1834             if filename is None:
1835                 msg = _('filename cannot be None')
1836                 raise exception.NovaException(msg)
1837 
1838             # libgfapi delete
1839             LOG.debug("XML: %s" % xml)
1840 
1841             LOG.debug("active disk object: %s" % active_disk_object)
1842 
1843             # determine reference within backing store for desired image
1844             filename_to_merge = filename
1845             matched_name = None
1846             b = backing_store
1847             index = None
1848 
1849             current_filename = active_disk_object.source_name.split('/')[1]
1850             if current_filename == filename_to_merge:
1851                 return my_dev + '[0]'
1852 
1853             while b is not None:
1854                 source_filename = b.source_name.split('/')[1]
1855                 if source_filename == filename_to_merge:
1856                     LOG.debug('found match: %s' % b.source_name)
1857                     matched_name = b.source_name
1858                     index = b.index
1859                     break
1860 
1861                 b = b.backing_store
1862 
1863             if matched_name is None:
1864                 msg = _('no match found for %s') % (filename_to_merge)
1865                 raise exception.NovaException(msg)
1866 
1867             LOG.debug('index of match (%s) is %s' % (b.source_name, index))
1868 
1869             my_snap_dev = '%s[%s]' % (my_dev, index)
1870             return my_snap_dev
1871 
1872         if delete_info['merge_target_file'] is None:
1873             # pull via blockRebase()
1874 
1875             # Merge the most recent snapshot into the active image
1876 
1877             rebase_disk = my_dev
1878             rebase_base = delete_info['file_to_merge']  # often None
1879             if active_protocol is not None:
1880                 rebase_base = _get_snap_dev(delete_info['file_to_merge'],
1881                                             active_disk_object.backing_store)
1882 
1883             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
1884             # and when available this flag _must_ be used to ensure backing
1885             # paths are maintained relative by qemu.
1886             #
1887             # If _RELATIVE flag not found, continue with old behaviour
1888             # (relative backing path seems to work for this case)
1889             try:
1890                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
1891                 relative = True
1892             except AttributeError:
1893                 LOG.warn(_LW("Relative blockrebase support was not detected. "
1894                              "Continuing with old behaviour."))
1895                 relative = False
1896 
1897             LOG.debug(
1898                 'disk: %(disk)s, base: %(base)s, '
1899                 'bw: %(bw)s, relative: %(relative)s',
1900                 {'disk': rebase_disk,
1901                  'base': rebase_base,
1902                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
1903                  'relative': str(relative)})
1904 
1905             dev = guest.get_block_device(rebase_disk)
1906             result = dev.rebase(rebase_base, relative=relative)
1907             if result == 0:
1908                 LOG.debug('blockRebase started successfully')
1909 
1910             while dev.wait_for_job(abort_on_error=True):
1911                 LOG.debug('waiting for blockRebase job completion')
1912                 time.sleep(0.5)
1913 
1914         else:
1915             # commit with blockCommit()
1916             my_snap_base = None
1917             my_snap_top = None
1918             commit_disk = my_dev
1919 
1920             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
1921             # and when available this flag _must_ be used to ensure backing
1922             # paths are maintained relative by qemu.
1923             #
1924             # If _RELATIVE flag not found, raise exception as relative backing
1925             # path may not be maintained and Cinder flow is broken if allowed
1926             # to continue.
1927             try:
1928                 libvirt.VIR_DOMAIN_BLOCK_COMMIT_RELATIVE
1929             except AttributeError:
1930                 ver = '.'.join(
1931                     [str(x) for x in
1932                      MIN_LIBVIRT_BLOCKJOB_RELATIVE_VERSION])
1933                 msg = _("Relative blockcommit support was not detected. "
1934                         "Libvirt '%s' or later is required for online "
1935                         "deletion of file/network storage-backed volume "
1936                         "snapshots.") % ver
1937                 raise exception.Invalid(msg)
1938 
1939             if active_protocol is not None:
1940                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
1941                                              active_disk_object.backing_store)
1942                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
1943                                             active_disk_object.backing_store)
1944 
1945             commit_base = my_snap_base or delete_info['merge_target_file']
1946             commit_top = my_snap_top or delete_info['file_to_merge']
1947 
1948             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
1949                       'commit_base=%(commit_base)s '
1950                       'commit_top=%(commit_top)s '
1951                       % {'commit_disk': commit_disk,
1952                          'commit_base': commit_base,
1953                          'commit_top': commit_top})
1954 
1955             dev = guest.get_block_device(commit_disk)
1956             result = dev.commit(commit_base, commit_top, relative=True)
1957 
1958             if result == 0:
1959                 LOG.debug('blockCommit started successfully')
1960 
1961             while dev.wait_for_job(abort_on_error=True):
1962                 LOG.debug('waiting for blockCommit job completion')
1963                 time.sleep(0.5)
1964 
1965     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
1966                                delete_info):
1967         try:
1968             self._volume_snapshot_delete(context, instance, volume_id,
1969                                          snapshot_id, delete_info=delete_info)
1970         except Exception:
1971             with excutils.save_and_reraise_exception():
1972                 LOG.exception(_LE('Error occurred during '
1973                                   'volume_snapshot_delete, '
1974                                   'sending error status to Cinder.'))
1975                 self._volume_snapshot_update_status(
1976                     context, snapshot_id, 'error_deleting')
1977 
1978         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
1979         self._volume_refresh_connection_info(context, instance, volume_id)
1980 
1981     def reboot(self, context, instance, network_info, reboot_type,
1982                block_device_info=None, bad_volumes_callback=None):
1983         """Reboot a virtual machine, given an instance reference."""
1984         if reboot_type == 'SOFT':
1985             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
1986             try:
1987                 soft_reboot_success = self._soft_reboot(instance)
1988             except libvirt.libvirtError as e:
1989                 LOG.debug("Instance soft reboot failed: %s", e)
1990                 soft_reboot_success = False
1991 
1992             if soft_reboot_success:
1993                 LOG.info(_LI("Instance soft rebooted successfully."),
1994                          instance=instance)
1995                 return
1996             else:
1997                 LOG.warn(_LW("Failed to soft reboot instance. "
1998                              "Trying hard reboot."),
1999                          instance=instance)
2000         return self._hard_reboot(context, instance, network_info,
2001                                  block_device_info)
2002 
2003     def _soft_reboot(self, instance):
2004         """Attempt to shutdown and restart the instance gracefully.
2005 
2006         We use shutdown and create here so we can return if the guest
2007         responded and actually rebooted. Note that this method only
2008         succeeds if the guest responds to acpi. Therefore we return
2009         success or failure so we can fall back to a hard reboot if
2010         necessary.
2011 
2012         :returns: True if the reboot succeeded
2013         """
2014         guest = self._host.get_guest(instance)
2015 
2016         # TODO(sahid): We are converting all calls from a
2017         # virDomain object to use nova.virt.libvirt.Guest.
2018         # We should be able to remove dom at the end.
2019         dom = guest._domain
2020         state = self._get_power_state(dom)
2021         old_domid = dom.ID()
2022         # NOTE(vish): This check allows us to reboot an instance that
2023         #             is already shutdown.
2024         if state == power_state.RUNNING:
2025             dom.shutdown()
2026         # NOTE(vish): This actually could take slightly longer than the
2027         #             FLAG defines depending on how long the get_info
2028         #             call takes to return.
2029         self._prepare_pci_devices_for_use(
2030             pci_manager.get_instance_pci_devs(instance, 'all'))
2031         for x in xrange(CONF.libvirt.wait_soft_reboot_seconds):
2032             guest = self._host.get_guest(instance)
2033 
2034             # TODO(sahid): We are converting all calls from a
2035             # virDomain object to use nova.virt.libvirt.Guest.
2036             # We should be able to remove dom at the end.
2037             dom = guest._domain
2038             state = self._get_power_state(dom)
2039             new_domid = dom.ID()
2040 
2041             # NOTE(ivoks): By checking domain IDs, we make sure we are
2042             #              not recreating domain that's already running.
2043             if old_domid != new_domid:
2044                 if state in [power_state.SHUTDOWN,
2045                              power_state.CRASHED]:
2046                     LOG.info(_LI("Instance shutdown successfully."),
2047                              instance=instance)
2048                     self._create_domain(domain=dom)
2049                     timer = loopingcall.FixedIntervalLoopingCall(
2050                         self._wait_for_running, instance)
2051                     timer.start(interval=0.5).wait()
2052                     return True
2053                 else:
2054                     LOG.info(_LI("Instance may have been rebooted during soft "
2055                                  "reboot, so return now."), instance=instance)
2056                     return True
2057             greenthread.sleep(1)
2058         return False
2059 
2060     def _hard_reboot(self, context, instance, network_info,
2061                      block_device_info=None):
2062         """Reboot a virtual machine, given an instance reference.
2063 
2064         Performs a Libvirt reset (if supported) on the domain.
2065 
2066         If Libvirt reset is unavailable this method actually destroys and
2067         re-creates the domain to ensure the reboot happens, as the guest
2068         OS cannot ignore this action.
2069         """
2070 
2071         self._destroy(instance)
2072 
2073         # Convert the system metadata to image metadata
2074         image_meta = utils.get_image_from_system_metadata(
2075             instance.system_metadata)
2076 
2077         instance_dir = libvirt_utils.get_instance_path(instance)
2078         fileutils.ensure_tree(instance_dir)
2079 
2080         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2081                                             instance,
2082                                             image_meta,
2083                                             block_device_info)
2084         # NOTE(vish): This could generate the wrong device_format if we are
2085         #             using the raw backend and the images don't exist yet.
2086         #             The create_images_and_backing below doesn't properly
2087         #             regenerate raw backend images, however, so when it
2088         #             does we need to (re)generate the xml after the images
2089         #             are in place.
2090         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2091                                   image_meta,
2092                                   block_device_info=block_device_info,
2093                                   write_to_disk=True)
2094 
2095         # NOTE (rmk): Re-populate any missing backing files.
2096         disk_info = self._get_instance_disk_info(instance.name, xml,
2097                                                  block_device_info)
2098 
2099         if context.auth_token is not None:
2100             self._create_images_and_backing(context, instance, instance_dir,
2101                                             disk_info)
2102 
2103         # Initialize all the necessary networking, block devices and
2104         # start the instance.
2105         self._create_domain_and_network(context, xml, instance, network_info,
2106                                         disk_info,
2107                                         block_device_info=block_device_info,
2108                                         reboot=True,
2109                                         vifs_already_plugged=True)
2110         self._prepare_pci_devices_for_use(
2111             pci_manager.get_instance_pci_devs(instance, 'all'))
2112 
2113         def _wait_for_reboot():
2114             """Called at an interval until the VM is running again."""
2115             state = self.get_info(instance).state
2116 
2117             if state == power_state.RUNNING:
2118                 LOG.info(_LI("Instance rebooted successfully."),
2119                          instance=instance)
2120                 raise loopingcall.LoopingCallDone()
2121 
2122         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2123         timer.start(interval=0.5).wait()
2124 
2125     def pause(self, instance):
2126         """Pause VM instance."""
2127         guest = self._host.get_guest(instance)
2128 
2129         # TODO(sahid): We are converting all calls from a
2130         # virDomain object to use nova.virt.libvirt.Guest.
2131         # We should be able to remove dom at the end.
2132         dom = guest._domain
2133         dom.suspend()
2134 
2135     def unpause(self, instance):
2136         """Unpause paused VM instance."""
2137         self._host.get_guest(instance).resume()
2138 
2139     def _clean_shutdown(self, instance, timeout, retry_interval):
2140         """Attempt to shutdown the instance gracefully.
2141 
2142         :param instance: The instance to be shutdown
2143         :param timeout: How long to wait in seconds for the instance to
2144                         shutdown
2145         :param retry_interval: How often in seconds to signal the instance
2146                                to shutdown while waiting
2147 
2148         :returns: True if the shutdown succeeded
2149         """
2150 
2151         # List of states that represent a shutdown instance
2152         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2153                            power_state.CRASHED]
2154 
2155         try:
2156             guest = self._host.get_guest(instance)
2157 
2158             # TODO(sahid): We are converting all calls from a
2159             # virDomain object to use nova.virt.libvirt.Guest.
2160             # We should be able to remove dom at the end.
2161             dom = guest._domain
2162         except exception.InstanceNotFound:
2163             # If the instance has gone then we don't need to
2164             # wait for it to shutdown
2165             return True
2166 
2167         state = self._get_power_state(dom)
2168         if state in SHUTDOWN_STATES:
2169             LOG.info(_LI("Instance already shutdown."),
2170                      instance=instance)
2171             return True
2172 
2173         LOG.debug("Shutting down instance from state %s", state,
2174                   instance=instance)
2175         dom.shutdown()
2176         retry_countdown = retry_interval
2177 
2178         for sec in six.moves.range(timeout):
2179 
2180             guest = self._host.get_guest(instance)
2181 
2182             # TODO(sahid): We are converting all calls from a
2183             # virDomain object to use nova.virt.libvirt.Guest.
2184             # We should be able to remove dom at the end.
2185             dom = guest._domain
2186 
2187             state = self._get_power_state(dom)
2188 
2189             if state in SHUTDOWN_STATES:
2190                 LOG.info(_LI("Instance shutdown successfully after %d "
2191                               "seconds."), sec, instance=instance)
2192                 return True
2193 
2194             # Note(PhilD): We can't assume that the Guest was able to process
2195             #              any previous shutdown signal (for example it may
2196             #              have still been startingup, so within the overall
2197             #              timeout we re-trigger the shutdown every
2198             #              retry_interval
2199             if retry_countdown == 0:
2200                 retry_countdown = retry_interval
2201                 # Instance could shutdown at any time, in which case we
2202                 # will get an exception when we call shutdown
2203                 try:
2204                     LOG.debug("Instance in state %s after %d seconds - "
2205                               "resending shutdown", state, sec,
2206                               instance=instance)
2207                     dom.shutdown()
2208                 except libvirt.libvirtError:
2209                     # Assume this is because its now shutdown, so loop
2210                     # one more time to clean up.
2211                     LOG.debug("Ignoring libvirt exception from shutdown "
2212                               "request.", instance=instance)
2213                     continue
2214             else:
2215                 retry_countdown -= 1
2216 
2217             time.sleep(1)
2218 
2219         LOG.info(_LI("Instance failed to shutdown in %d seconds."),
2220                  timeout, instance=instance)
2221         return False
2222 
2223     def power_off(self, instance, timeout=0, retry_interval=0):
2224         """Power off the specified instance."""
2225         if timeout:
2226             self._clean_shutdown(instance, timeout, retry_interval)
2227         self._destroy(instance)
2228 
2229     def power_on(self, context, instance, network_info,
2230                  block_device_info=None):
2231         """Power on the specified instance."""
2232         # We use _hard_reboot here to ensure that all backing files,
2233         # network, and block device connections, etc. are established
2234         # and available before we attempt to start the instance.
2235         self._hard_reboot(context, instance, network_info, block_device_info)
2236 
2237     def suspend(self, context, instance):
2238         """Suspend the specified instance."""
2239         guest = self._host.get_guest(instance)
2240 
2241         self._detach_pci_devices(guest,
2242             pci_manager.get_instance_pci_devs(instance))
2243         self._detach_sriov_ports(context, instance, guest)
2244         guest.save_memory_state()
2245 
2246     def resume(self, context, instance, network_info, block_device_info=None):
2247         """resume the specified instance."""
2248         image_meta = utils.get_image_from_system_metadata(
2249             instance.system_metadata)
2250 
2251         disk_info = blockinfo.get_disk_info(
2252                 CONF.libvirt.virt_type, instance, image_meta,
2253                 block_device_info=block_device_info)
2254 
2255         xml = self._get_existing_domain_xml(instance, network_info,
2256                                             block_device_info)
2257         guest = self._create_domain_and_network(context, xml, instance,
2258                            network_info, disk_info,
2259                            block_device_info=block_device_info,
2260                            vifs_already_plugged=True)
2261         self._attach_pci_devices(guest,
2262             pci_manager.get_instance_pci_devs(instance))
2263         self._attach_sriov_ports(context, instance, guest, network_info)
2264 
2265     def resume_state_on_host_boot(self, context, instance, network_info,
2266                                   block_device_info=None):
2267         """resume guest state when a host is booted."""
2268         # Check if the instance is running already and avoid doing
2269         # anything if it is.
2270         try:
2271             guest = self._host.get_guest(instance)
2272 
2273             # TODO(sahid): We are converting all calls from a
2274             # virDomain object to use nova.virt.libvirt.Guest.
2275             # We should be able to remove domain at the end.
2276             domain = guest._domain
2277 
2278             state = self._get_power_state(domain)
2279 
2280             ignored_states = (power_state.RUNNING,
2281                               power_state.SUSPENDED,
2282                               power_state.NOSTATE,
2283                               power_state.PAUSED)
2284 
2285             if state in ignored_states:
2286                 return
2287         except exception.NovaException:
2288             pass
2289 
2290         # Instance is not up and could be in an unknown state.
2291         # Be as absolute as possible about getting it back into
2292         # a known and running state.
2293         self._hard_reboot(context, instance, network_info, block_device_info)
2294 
2295     def rescue(self, context, instance, network_info, image_meta,
2296                rescue_password):
2297         """Loads a VM using rescue images.
2298 
2299         A rescue is normally performed when something goes wrong with the
2300         primary images and data needs to be corrected/recovered. Rescuing
2301         should not edit or over-ride the original image, only allow for
2302         data recovery.
2303 
2304         """
2305         instance_dir = libvirt_utils.get_instance_path(instance)
2306         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2307         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2308         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2309 
2310         if image_meta is not None:
2311             rescue_image_id = image_meta.get('id')
2312         else:
2313             rescue_image_id = None
2314 
2315         rescue_images = {
2316             'image_id': (rescue_image_id or
2317                         CONF.libvirt.rescue_image_id or instance.image_ref),
2318             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2319                           instance.kernel_id),
2320             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2321                            instance.ramdisk_id),
2322         }
2323         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2324                                             instance,
2325                                             image_meta,
2326                                             rescue=True)
2327         self._create_image(context, instance, disk_info['mapping'],
2328                            suffix='.rescue', disk_images=rescue_images,
2329                            network_info=network_info,
2330                            admin_pass=rescue_password)
2331         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2332                                   image_meta, rescue=rescue_images,
2333                                   write_to_disk=True)
2334         self._destroy(instance)
2335         self._create_domain(xml)
2336 
2337     def unrescue(self, instance, network_info):
2338         """Reboot the VM which is being rescued back into primary images.
2339         """
2340         instance_dir = libvirt_utils.get_instance_path(instance)
2341         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2342         xml = libvirt_utils.load_file(unrescue_xml_path)
2343         guest = self._host.get_guest(instance)
2344 
2345         # TODO(sahid): We are converting all calls from a
2346         # virDomain object to use nova.virt.libvirt.Guest.
2347         # We should be able to remove virt_dom at the end.
2348         virt_dom = guest._domain
2349         self._destroy(instance)
2350         self._create_domain(xml, virt_dom)
2351         libvirt_utils.file_delete(unrescue_xml_path)
2352         rescue_files = os.path.join(instance_dir, "*.rescue")
2353         for rescue_file in glob.iglob(rescue_files):
2354             libvirt_utils.file_delete(rescue_file)
2355         # cleanup rescue volume
2356         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2357                                 if lvmdisk.endswith('.rescue')])
2358 
2359     def poll_rebooting_instances(self, timeout, instances):
2360         pass
2361 
2362     # NOTE(ilyaalekseyev): Implementation like in multinics
2363     # for xenapi(tr3buchet)
2364     def spawn(self, context, instance, image_meta, injected_files,
2365               admin_password, network_info=None, block_device_info=None):
2366         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2367                                             instance,
2368                                             image_meta,
2369                                             block_device_info)
2370         self._create_image(context, instance,
2371                            disk_info['mapping'],
2372                            network_info=network_info,
2373                            block_device_info=block_device_info,
2374                            files=injected_files,
2375                            admin_pass=admin_password)
2376         xml = self._get_guest_xml(context, instance, network_info,
2377                                   disk_info, image_meta,
2378                                   block_device_info=block_device_info,
2379                                   write_to_disk=True)
2380         self._create_domain_and_network(context, xml, instance, network_info,
2381                                         disk_info,
2382                                         block_device_info=block_device_info)
2383         LOG.debug("Instance is running", instance=instance)
2384 
2385         def _wait_for_boot():
2386             """Called at an interval until the VM is running."""
2387             state = self.get_info(instance).state
2388 
2389             if state == power_state.RUNNING:
2390                 LOG.info(_LI("Instance spawned successfully."),
2391                          instance=instance)
2392                 raise loopingcall.LoopingCallDone()
2393 
2394         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
2395         timer.start(interval=0.5).wait()
2396 
2397     def _flush_libvirt_console(self, pty):
2398         out, err = utils.execute('dd',
2399                                  'if=%s' % pty,
2400                                  'iflag=nonblock',
2401                                  run_as_root=True,
2402                                  check_exit_code=False)
2403         return out
2404 
2405     def _append_to_file(self, data, fpath):
2406         LOG.info(_LI('data: %(data)r, fpath: %(fpath)r'),
2407                  {'data': data, 'fpath': fpath})
2408         with open(fpath, 'a+') as fp:
2409             fp.write(data)
2410 
2411         return fpath
2412 
2413     def get_console_output(self, context, instance):
2414         guest = self._host.get_guest(instance)
2415 
2416         xml = guest.get_xml_desc()
2417         tree = etree.fromstring(xml)
2418 
2419         console_types = {}
2420 
2421         # NOTE(comstud): We want to try 'file' types first, then try 'pty'
2422         # types.  We can't use Python 2.7 syntax of:
2423         # tree.find("./devices/console[@type='file']/source")
2424         # because we need to support 2.6.
2425         console_nodes = tree.findall('./devices/console')
2426         for console_node in console_nodes:
2427             console_type = console_node.get('type')
2428             console_types.setdefault(console_type, [])
2429             console_types[console_type].append(console_node)
2430 
2431         # If the guest has a console logging to a file prefer to use that
2432         if console_types.get('file'):
2433             for file_console in console_types.get('file'):
2434                 source_node = file_console.find('./source')
2435                 if source_node is None:
2436                     continue
2437                 path = source_node.get("path")
2438                 if not path:
2439                     continue
2440 
2441                 if not os.path.exists(path):
2442                     LOG.info(_LI('Instance is configured with a file console, '
2443                                  'but the backing file is not (yet?) present'),
2444                              instance=instance)
2445                     return ""
2446 
2447                 libvirt_utils.chown(path, os.getuid())
2448 
2449                 with libvirt_utils.file_open(path, 'rb') as fp:
2450                     log_data, remaining = utils.last_bytes(fp,
2451                                                            MAX_CONSOLE_BYTES)
2452                     if remaining > 0:
2453                         LOG.info(_LI('Truncated console log returned, '
2454                                      '%d bytes ignored'), remaining,
2455                                  instance=instance)
2456                     return log_data
2457 
2458         # Try 'pty' types
2459         if console_types.get('pty'):
2460             for pty_console in console_types.get('pty'):
2461                 source_node = pty_console.find('./source')
2462                 if source_node is None:
2463                     continue
2464                 pty = source_node.get("path")
2465                 if not pty:
2466                     continue
2467                 break
2468         else:
2469             msg = _("Guest does not have a console available")
2470             raise exception.NovaException(msg)
2471 
2472         self._chown_console_log_for_instance(instance)
2473         data = self._flush_libvirt_console(pty)
2474         console_log = self._get_console_log_path(instance)
2475         fpath = self._append_to_file(data, console_log)
2476 
2477         with libvirt_utils.file_open(fpath, 'rb') as fp:
2478             log_data, remaining = utils.last_bytes(fp, MAX_CONSOLE_BYTES)
2479             if remaining > 0:
2480                 LOG.info(_LI('Truncated console log returned, '
2481                              '%d bytes ignored'),
2482                          remaining, instance=instance)
2483             return log_data
2484 
2485     def get_host_ip_addr(self):
2486         ips = compute_utils.get_machine_ips()
2487         if CONF.my_ip not in ips:
2488             LOG.warn(_LW('my_ip address (%(my_ip)s) was not found on '
2489                          'any of the interfaces: %(ifaces)s'),
2490                      {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
2491         return CONF.my_ip
2492 
2493     def get_vnc_console(self, context, instance):
2494         def get_vnc_port_for_instance(instance_name):
2495             guest = self._host.get_guest(instance)
2496 
2497             xml = guest.get_xml_desc()
2498             xml_dom = etree.fromstring(xml)
2499 
2500             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
2501             if graphic is not None:
2502                 return graphic.get('port')
2503             # NOTE(rmk): We had VNC consoles enabled but the instance in
2504             # question is not actually listening for connections.
2505             raise exception.ConsoleTypeUnavailable(console_type='vnc')
2506 
2507         port = get_vnc_port_for_instance(instance.name)
2508         host = CONF.vnc.vncserver_proxyclient_address
2509 
2510         return ctype.ConsoleVNC(host=host, port=port)
2511 
2512     def get_spice_console(self, context, instance):
2513         def get_spice_ports_for_instance(instance_name):
2514             guest = self._host.get_guest(instance)
2515 
2516             xml = guest.get_xml_desc()
2517             xml_dom = etree.fromstring(xml)
2518 
2519             graphic = xml_dom.find("./devices/graphics[@type='spice']")
2520             if graphic is not None:
2521                 return (graphic.get('port'), graphic.get('tlsPort'))
2522             # NOTE(rmk): We had Spice consoles enabled but the instance in
2523             # question is not actually listening for connections.
2524             raise exception.ConsoleTypeUnavailable(console_type='spice')
2525 
2526         ports = get_spice_ports_for_instance(instance.name)
2527         host = CONF.spice.server_proxyclient_address
2528 
2529         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
2530 
2531     def get_serial_console(self, context, instance):
2532         for hostname, port in self._get_serial_ports_from_instance(
2533                 instance, mode='bind'):
2534             return ctype.ConsoleSerial(host=hostname, port=port)
2535         raise exception.ConsoleTypeUnavailable(console_type='serial')
2536 
2537     @staticmethod
2538     def _supports_direct_io(dirpath):
2539 
2540         if not hasattr(os, 'O_DIRECT'):
2541             LOG.debug("This python runtime does not support direct I/O")
2542             return False
2543 
2544         testfile = os.path.join(dirpath, ".directio.test")
2545 
2546         hasDirectIO = True
2547         try:
2548             f = os.open(testfile, os.O_CREAT | os.O_WRONLY | os.O_DIRECT)
2549             # Check is the write allowed with 512 byte alignment
2550             align_size = 512
2551             m = mmap.mmap(-1, align_size)
2552             m.write(r"x" * align_size)
2553             os.write(f, m)
2554             os.close(f)
2555             LOG.debug("Path '%(path)s' supports direct I/O",
2556                       {'path': dirpath})
2557         except OSError as e:
2558             if e.errno == errno.EINVAL:
2559                 LOG.debug("Path '%(path)s' does not support direct I/O: "
2560                           "'%(ex)s'", {'path': dirpath, 'ex': e})
2561                 hasDirectIO = False
2562             else:
2563                 with excutils.save_and_reraise_exception():
2564                     LOG.error(_LE("Error on '%(path)s' while checking "
2565                                   "direct I/O: '%(ex)s'"),
2566                               {'path': dirpath, 'ex': e})
2567         except Exception as e:
2568             with excutils.save_and_reraise_exception():
2569                 LOG.error(_LE("Error on '%(path)s' while checking direct I/O: "
2570                               "'%(ex)s'"), {'path': dirpath, 'ex': e})
2571         finally:
2572             try:
2573                 os.unlink(testfile)
2574             except Exception:
2575                 pass
2576 
2577         return hasDirectIO
2578 
2579     @staticmethod
2580     def _create_local(target, local_size, unit='G',
2581                       fs_format=None, label=None):
2582         """Create a blank image of specified size."""
2583 
2584         libvirt_utils.create_image('raw', target,
2585                                     '%d%c' % (local_size, unit))
2586 
2587     def _create_ephemeral(self, target, ephemeral_size,
2588                           fs_label, os_type, is_block_dev=False,
2589                           max_size=None, context=None, specified_fs=None):
2590         if not is_block_dev:
2591             self._create_local(target, ephemeral_size)
2592 
2593         # Run as root only for block devices.
2594         disk.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
2595                   specified_fs=specified_fs)
2596 
2597     @staticmethod
2598     def _create_swap(target, swap_mb, max_size=None, context=None):
2599         """Create a swap file of specified size."""
2600         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
2601         utils.mkfs('swap', target)
2602 
2603     @staticmethod
2604     def _get_console_log_path(instance):
2605         return os.path.join(libvirt_utils.get_instance_path(instance),
2606                             'console.log')
2607 
2608     @staticmethod
2609     def _get_disk_config_path(instance, suffix=''):
2610         return os.path.join(libvirt_utils.get_instance_path(instance),
2611                             'disk.config' + suffix)
2612 
2613     def _chown_console_log_for_instance(self, instance):
2614         console_log = self._get_console_log_path(instance)
2615         if os.path.exists(console_log):
2616             libvirt_utils.chown(console_log, os.getuid())
2617 
2618     def _chown_disk_config_for_instance(self, instance):
2619         disk_config = self._get_disk_config_path(instance)
2620         if os.path.exists(disk_config):
2621             libvirt_utils.chown(disk_config, os.getuid())
2622 
2623     @staticmethod
2624     def _is_booted_from_volume(instance, disk_mapping):
2625         """Determines whether the VM is booting from volume
2626 
2627         Determines whether the disk mapping indicates that the VM
2628         is booting from a volume.
2629         """
2630         return ((not bool(instance.get('image_ref')))
2631                 or 'disk' not in disk_mapping)
2632 
2633     def _inject_data(self, instance, network_info, admin_pass, files, suffix):
2634         """Injects data in a disk image
2635 
2636         Helper used for injecting data in a disk image file system.
2637 
2638         Keyword arguments:
2639           instance -- a dict that refers instance specifications
2640           network_info -- a dict that refers network speficications
2641           admin_pass -- a string used to set an admin password
2642           files -- a list of files needs to be injected
2643           suffix -- a string used as an image name suffix
2644         """
2645         # Handles the partition need to be used.
2646         target_partition = None
2647         if not instance.kernel_id:
2648             target_partition = CONF.libvirt.inject_partition
2649             if target_partition == 0:
2650                 target_partition = None
2651         if CONF.libvirt.virt_type == 'lxc':
2652             target_partition = None
2653 
2654         # Handles the key injection.
2655         if CONF.libvirt.inject_key and instance.get('key_data'):
2656             key = str(instance.key_data)
2657         else:
2658             key = None
2659 
2660         # Handles the admin password injection.
2661         if not CONF.libvirt.inject_password:
2662             admin_pass = None
2663 
2664         # Handles the network injection.
2665         net = netutils.get_injected_network_template(
2666                 network_info, libvirt_virt_type=CONF.libvirt.virt_type)
2667 
2668         # Handles the metadata injection
2669         metadata = instance.get('metadata')
2670 
2671         image_type = CONF.libvirt.images_type
2672         if any((key, net, metadata, admin_pass, files)):
2673             injection_image = self.image_backend.image(
2674                 instance,
2675                 'disk' + suffix,
2676                 image_type)
2677             img_id = instance.image_ref
2678 
2679             if not injection_image.check_image_exists():
2680                 LOG.warn(_LW('Image %s not found on disk storage. '
2681                          'Continue without injecting data'),
2682                          injection_image.path, instance=instance)
2683                 return
2684             try:
2685                 disk.inject_data(injection_image.get_model(self._conn),
2686                                  key, net, metadata, admin_pass, files,
2687                                  partition=target_partition,
2688                                  mandatory=('files',))
2689             except Exception as e:
2690                 with excutils.save_and_reraise_exception():
2691                     LOG.error(_LE('Error injecting data into image '
2692                                   '%(img_id)s (%(e)s)'),
2693                               {'img_id': img_id, 'e': e},
2694                               instance=instance)
2695 
2696     def _create_image(self, context, instance,
2697                       disk_mapping, suffix='',
2698                       disk_images=None, network_info=None,
2699                       block_device_info=None, files=None,
2700                       admin_pass=None, inject_files=True,
2701                       fallback_from_host=None):
2702         booted_from_volume = self._is_booted_from_volume(
2703             instance, disk_mapping)
2704 
2705         def image(fname, image_type=CONF.libvirt.images_type):
2706             return self.image_backend.image(instance,
2707                                             fname + suffix, image_type)
2708 
2709         def raw(fname):
2710             return image(fname, image_type='raw')
2711 
2712         # ensure directories exist and are writable
2713         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
2714 
2715         LOG.info(_LI('Creating image'), instance=instance)
2716 
2717         # NOTE(dprince): for rescue console.log may already exist... chown it.
2718         self._chown_console_log_for_instance(instance)
2719 
2720         # NOTE(yaguang): For evacuate disk.config already exist in shared
2721         # storage, chown it.
2722         self._chown_disk_config_for_instance(instance)
2723 
2724         # NOTE(vish): No need add the suffix to console.log
2725         libvirt_utils.write_to_file(
2726             self._get_console_log_path(instance), '', 7)
2727 
2728         if not disk_images:
2729             disk_images = {'image_id': instance.image_ref,
2730                            'kernel_id': instance.kernel_id,
2731                            'ramdisk_id': instance.ramdisk_id}
2732 
2733         if disk_images['kernel_id']:
2734             fname = imagecache.get_cache_fname(disk_images, 'kernel_id')
2735             raw('kernel').cache(fetch_func=libvirt_utils.fetch_image,
2736                                 context=context,
2737                                 filename=fname,
2738                                 image_id=disk_images['kernel_id'],
2739                                 user_id=instance.user_id,
2740                                 project_id=instance.project_id)
2741             if disk_images['ramdisk_id']:
2742                 fname = imagecache.get_cache_fname(disk_images, 'ramdisk_id')
2743                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_image,
2744                                      context=context,
2745                                      filename=fname,
2746                                      image_id=disk_images['ramdisk_id'],
2747                                      user_id=instance.user_id,
2748                                      project_id=instance.project_id)
2749 
2750         inst_type = instance.get_flavor()
2751 
2752         # NOTE(ndipanov): Even if disk_mapping was passed in, which
2753         # currently happens only on rescue - we still don't want to
2754         # create a base image.
2755         if not booted_from_volume:
2756             root_fname = imagecache.get_cache_fname(disk_images, 'image_id')
2757             size = instance.root_gb * units.Gi
2758 
2759             if size == 0 or suffix == '.rescue':
2760                 size = None
2761 
2762             backend = image('disk')
2763             if backend.SUPPORTS_CLONE:
2764                 def clone_fallback_to_fetch(*args, **kwargs):
2765                     try:
2766                         backend.clone(context, disk_images['image_id'])
2767                     except exception.ImageUnacceptable:
2768                         libvirt_utils.fetch_image(*args, **kwargs)
2769                 fetch_func = clone_fallback_to_fetch
2770             else:
2771                 fetch_func = libvirt_utils.fetch_image
2772             self._try_fetch_image_cache(backend, fetch_func, context,
2773                                         root_fname, disk_images['image_id'],
2774                                         instance, size, fallback_from_host)
2775 
2776         # Lookup the filesystem type if required
2777         os_type_with_default = disk.get_fs_type_for_os_type(instance.os_type)
2778         # Generate a file extension based on the file system
2779         # type and the mkfs commands configured if any
2780         file_extension = disk.get_file_extension_for_os_type(
2781                                                           os_type_with_default)
2782 
2783         ephemeral_gb = instance.ephemeral_gb
2784         if 'disk.local' in disk_mapping:
2785             disk_image = image('disk.local')
2786             fn = functools.partial(self._create_ephemeral,
2787                                    fs_label='ephemeral0',
2788                                    os_type=instance.os_type,
2789                                    is_block_dev=disk_image.is_block_dev)
2790             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
2791             size = ephemeral_gb * units.Gi
2792             disk_image.cache(fetch_func=fn,
2793                              context=context,
2794                              filename=fname,
2795                              size=size,
2796                              ephemeral_size=ephemeral_gb)
2797 
2798         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
2799                 block_device_info)):
2800             disk_image = image(blockinfo.get_eph_disk(idx))
2801 
2802             specified_fs = eph.get('guest_format')
2803             if specified_fs and not self.is_supported_fs_format(specified_fs):
2804                 msg = _("%s format is not supported") % specified_fs
2805                 raise exception.InvalidBDMFormat(details=msg)
2806 
2807             fn = functools.partial(self._create_ephemeral,
2808                                    fs_label='ephemeral%d' % idx,
2809                                    os_type=instance.os_type,
2810                                    is_block_dev=disk_image.is_block_dev)
2811             size = eph['size'] * units.Gi
2812             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
2813             disk_image.cache(fetch_func=fn,
2814                              context=context,
2815                              filename=fname,
2816                              size=size,
2817                              ephemeral_size=eph['size'],
2818                              specified_fs=specified_fs)
2819 
2820         if 'disk.swap' in disk_mapping:
2821             mapping = disk_mapping['disk.swap']
2822             swap_mb = 0
2823 
2824             swap = driver.block_device_info_get_swap(block_device_info)
2825             if driver.swap_is_usable(swap):
2826                 swap_mb = swap['swap_size']
2827             elif (inst_type['swap'] > 0 and
2828                   not block_device.volume_in_mapping(
2829                     mapping['dev'], block_device_info)):
2830                 swap_mb = inst_type['swap']
2831 
2832             if swap_mb > 0:
2833                 size = swap_mb * units.Mi
2834                 image('disk.swap').cache(fetch_func=self._create_swap,
2835                                          context=context,
2836                                          filename="swap_%s" % swap_mb,
2837                                          size=size,
2838                                          swap_mb=swap_mb)
2839 
2840         # Config drive
2841         if configdrive.required_by(instance):
2842             LOG.info(_LI('Using config drive'), instance=instance)
2843             extra_md = {}
2844             if admin_pass:
2845                 extra_md['admin_pass'] = admin_pass
2846 
2847             inst_md = instance_metadata.InstanceMetadata(instance,
2848                 content=files, extra_md=extra_md, network_info=network_info)
2849             with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
2850                 configdrive_path = self._get_disk_config_path(instance, suffix)
2851                 LOG.info(_LI('Creating config drive at %(path)s'),
2852                          {'path': configdrive_path}, instance=instance)
2853 
2854                 try:
2855                     cdb.make_drive(configdrive_path)
2856                 except processutils.ProcessExecutionError as e:
2857                     with excutils.save_and_reraise_exception():
2858                         LOG.error(_LE('Creating config drive failed '
2859                                       'with error: %s'),
2860                                   e, instance=instance)
2861 
2862         # File injection only if needed
2863         elif inject_files and CONF.libvirt.inject_partition != -2:
2864             if booted_from_volume:
2865                 LOG.warn(_LW('File injection into a boot from volume '
2866                              'instance is not supported'), instance=instance)
2867             self._inject_data(
2868                 instance, network_info, admin_pass, files, suffix)
2869 
2870         if CONF.libvirt.virt_type == 'uml':
2871             libvirt_utils.chown(image('disk').path, 'root')
2872 
2873     def _prepare_pci_devices_for_use(self, pci_devices):
2874         # kvm , qemu support managed mode
2875         # In managed mode, the configured device will be automatically
2876         # detached from the host OS drivers when the guest is started,
2877         # and then re-attached when the guest shuts down.
2878         if CONF.libvirt.virt_type != 'xen':
2879             # we do manual detach only for xen
2880             return
2881         try:
2882             for dev in pci_devices:
2883                 libvirt_dev_addr = dev['hypervisor_name']
2884                 libvirt_dev = \
2885                         self._host.device_lookup_by_name(libvirt_dev_addr)
2886                 # Note(yjiang5) Spelling for 'dettach' is correct, see
2887                 # http://libvirt.org/html/libvirt-libvirt.html.
2888                 libvirt_dev.dettach()
2889 
2890             # Note(yjiang5): A reset of one PCI device may impact other
2891             # devices on the same bus, thus we need two separated loops
2892             # to detach and then reset it.
2893             for dev in pci_devices:
2894                 libvirt_dev_addr = dev['hypervisor_name']
2895                 libvirt_dev = \
2896                         self._host.device_lookup_by_name(libvirt_dev_addr)
2897                 libvirt_dev.reset()
2898 
2899         except libvirt.libvirtError as exc:
2900             raise exception.PciDevicePrepareFailed(id=dev['id'],
2901                                                    instance_uuid=
2902                                                    dev['instance_uuid'],
2903                                                    reason=six.text_type(exc))
2904 
2905     def _detach_pci_devices(self, guest, pci_devs):
2906 
2907         # for libvirt version < 1.1.1, this is race condition
2908         # so forbid detach if not had this version
2909         if not self._host.has_min_version(MIN_LIBVIRT_DEVICE_CALLBACK_VERSION):
2910             if pci_devs:
2911                 reason = (_("Detaching PCI devices with libvirt < %(ver)s"
2912                            " is not permitted") %
2913                            {'ver': MIN_LIBVIRT_DEVICE_CALLBACK_VERSION})
2914                 raise exception.PciDeviceDetachFailed(reason=reason,
2915                                                       dev=pci_devs)
2916         try:
2917             for dev in pci_devs:
2918                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
2919                 # after detachDeviceFlags returned, we should check the dom to
2920                 # ensure the detaching is finished
2921                 xml = guest.get_xml_desc()
2922                 xml_doc = etree.fromstring(xml)
2923                 guest_config = vconfig.LibvirtConfigGuest()
2924                 guest_config.parse_dom(xml_doc)
2925 
2926                 for hdev in [d for d in guest_config.devices
2927                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
2928                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
2929                     dbsf = pci_utils.parse_address(dev['address'])
2930                     if [int(x, 16) for x in hdbsf] ==\
2931                             [int(x, 16) for x in dbsf]:
2932                         raise exception.PciDeviceDetachFailed(reason=
2933                                                               "timeout",
2934                                                               dev=dev)
2935 
2936         except libvirt.libvirtError as ex:
2937             error_code = ex.get_error_code()
2938             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2939                 LOG.warn(_LW("Instance disappeared while detaching "
2940                              "a PCI device from it."))
2941             else:
2942                 raise
2943 
2944     def _attach_pci_devices(self, guest, pci_devs):
2945         try:
2946             for dev in pci_devs:
2947                 guest.attach_device(self._get_guest_pci_device(dev))
2948 
2949         except libvirt.libvirtError:
2950             LOG.error(_LE('Attaching PCI devices %(dev)s to %(dom)s failed.'),
2951                       {'dev': pci_devs, 'dom': guest.id})
2952             raise
2953 
2954     @staticmethod
2955     def _has_sriov_port(network_info):
2956         for vif in network_info:
2957             if vif['vnic_type'] == network_model.VNIC_TYPE_DIRECT:
2958                 return True
2959         return False
2960 
2961     def _attach_sriov_ports(self, context, instance, guest, network_info=None):
2962         if network_info is None:
2963             network_info = instance.info_cache.network_info
2964         if network_info is None:
2965             return
2966 
2967         if self._has_sriov_port(network_info):
2968             image_meta = utils.get_image_from_system_metadata(
2969                 instance.system_metadata)
2970             for vif in network_info:
2971                 if vif['vnic_type'] == network_model.VNIC_TYPE_DIRECT:
2972                     cfg = self.vif_driver.get_config(instance,
2973                                                      vif,
2974                                                      image_meta,
2975                                                      instance.flavor,
2976                                                      CONF.libvirt.virt_type)
2977                     LOG.debug('Attaching SR-IOV port %(port)s to %(dom)s',
2978                           {'port': vif, 'dom': guest.id})
2979                     guest.attach_device(cfg)
2980 
2981     def _detach_sriov_ports(self, context, instance, guest):
2982         network_info = instance.info_cache.network_info
2983         if network_info is None:
2984             return
2985 
2986         if self._has_sriov_port(network_info):
2987             # for libvirt version < 1.1.1, this is race condition
2988             # so forbid detach if it's an older version
2989             if not self._host.has_min_version(
2990                             MIN_LIBVIRT_DEVICE_CALLBACK_VERSION):
2991                 reason = (_("Detaching SR-IOV ports with"
2992                            " libvirt < %(ver)s is not permitted") %
2993                            {'ver': MIN_LIBVIRT_DEVICE_CALLBACK_VERSION})
2994                 raise exception.PciDeviceDetachFailed(reason=reason,
2995                                                       dev=network_info)
2996 
2997             image_meta = utils.get_image_from_system_metadata(
2998                 instance.system_metadata)
2999             for vif in network_info:
3000                 if vif['vnic_type'] == network_model.VNIC_TYPE_DIRECT:
3001                     cfg = self.vif_driver.get_config(instance,
3002                                                      vif,
3003                                                      image_meta,
3004                                                      instance.flavor,
3005                                                      CONF.libvirt.virt_type)
3006                     guest.detach_device(cfg, live=True)
3007 
3008     def _set_host_enabled(self, enabled,
3009                           disable_reason=DISABLE_REASON_UNDEFINED):
3010         """Enables / Disables the compute service on this host.
3011 
3012            This doesn't override non-automatic disablement with an automatic
3013            setting; thereby permitting operators to keep otherwise
3014            healthy hosts out of rotation.
3015         """
3016 
3017         status_name = {True: 'disabled',
3018                        False: 'enabled'}
3019 
3020         disable_service = not enabled
3021 
3022         ctx = nova_context.get_admin_context()
3023         try:
3024             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3025 
3026             if service.disabled != disable_service:
3027                 # Note(jang): this is a quick fix to stop operator-
3028                 # disabled compute hosts from re-enabling themselves
3029                 # automatically. We prefix any automatic reason code
3030                 # with a fixed string. We only re-enable a host
3031                 # automatically if we find that string in place.
3032                 # This should probably be replaced with a separate flag.
3033                 if not service.disabled or (
3034                         service.disabled_reason and
3035                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3036                     service.disabled = disable_service
3037                     service.disabled_reason = (
3038                        DISABLE_PREFIX + disable_reason
3039                        if disable_service else DISABLE_REASON_UNDEFINED)
3040                     service.save()
3041                     LOG.debug('Updating compute service status to %s',
3042                               status_name[disable_service])
3043                 else:
3044                     LOG.debug('Not overriding manual compute service '
3045                               'status with: %s',
3046                               status_name[disable_service])
3047         except exception.ComputeHostNotFound:
3048             LOG.warn(_LW('Cannot update service status on host "%s" '
3049                          'since it is not registered.'), CONF.host)
3050         except Exception:
3051             LOG.warn(_LW('Cannot update service status on host "%s" '
3052                          'due to an unexpected exception.'), CONF.host,
3053                      exc_info=True)
3054 
3055     def _get_guest_cpu_model_config(self):
3056         mode = CONF.libvirt.cpu_mode
3057         model = CONF.libvirt.cpu_model
3058 
3059         if (CONF.libvirt.virt_type == "kvm" or
3060             CONF.libvirt.virt_type == "qemu"):
3061             if mode is None:
3062                 mode = "host-model"
3063             if mode == "none":
3064                 return vconfig.LibvirtConfigGuestCPU()
3065         else:
3066             if mode is None or mode == "none":
3067                 return None
3068 
3069         if ((CONF.libvirt.virt_type != "kvm" and
3070              CONF.libvirt.virt_type != "qemu")):
3071             msg = _("Config requested an explicit CPU model, but "
3072                     "the current libvirt hypervisor '%s' does not "
3073                     "support selecting CPU models") % CONF.libvirt.virt_type
3074             raise exception.Invalid(msg)
3075 
3076         if mode == "custom" and model is None:
3077             msg = _("Config requested a custom CPU model, but no "
3078                     "model name was provided")
3079             raise exception.Invalid(msg)
3080         elif mode != "custom" and model is not None:
3081             msg = _("A CPU model name should not be set when a "
3082                     "host CPU model is requested")
3083             raise exception.Invalid(msg)
3084 
3085         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3086                   {'mode': mode, 'model': (model or "")})
3087 
3088         cpu = vconfig.LibvirtConfigGuestCPU()
3089         cpu.mode = mode
3090         cpu.model = model
3091 
3092         return cpu
3093 
3094     def _get_guest_cpu_config(self, flavor, image,
3095                               guest_cpu_numa_config, instance_numa_topology):
3096         cpu = self._get_guest_cpu_model_config()
3097 
3098         if cpu is None:
3099             return None
3100 
3101         topology = hardware.get_best_cpu_topology(
3102                 flavor, image, numa_topology=instance_numa_topology)
3103 
3104         cpu.sockets = topology.sockets
3105         cpu.cores = topology.cores
3106         cpu.threads = topology.threads
3107         cpu.numa = guest_cpu_numa_config
3108 
3109         return cpu
3110 
3111     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3112                                image_type=None):
3113         if CONF.libvirt.hw_disk_discard:
3114             if not self._host.has_min_version(MIN_LIBVIRT_DISCARD_VERSION,
3115                                               MIN_QEMU_DISCARD_VERSION,
3116                                               host.HV_DRIVER_QEMU):
3117                 msg = (_('Volume sets discard option, but libvirt %(libvirt)s'
3118                          ' or later is required, qemu %(qemu)s'
3119                          ' or later is required.') %
3120                       {'libvirt': MIN_LIBVIRT_DISCARD_VERSION,
3121                        'qemu': MIN_QEMU_DISCARD_VERSION})
3122                 raise exception.Invalid(msg)
3123 
3124         image = self.image_backend.image(instance,
3125                                          name,
3126                                          image_type)
3127         disk_info = disk_mapping[name]
3128         return image.libvirt_info(disk_info['bus'],
3129                                   disk_info['dev'],
3130                                   disk_info['type'],
3131                                   self.disk_cachemode,
3132                                   inst_type['extra_specs'],
3133                                   self._host.get_version())
3134 
3135     def _get_guest_fs_config(self, instance, name, image_type=None):
3136         image = self.image_backend.image(instance,
3137                                          name,
3138                                          image_type)
3139         return image.libvirt_fs_info("/", "ploop")
3140 
3141     def _get_guest_storage_config(self, instance, image_meta,
3142                                   disk_info,
3143                                   rescue, block_device_info,
3144                                   inst_type, os_type):
3145         devices = []
3146         disk_mapping = disk_info['mapping']
3147 
3148         block_device_mapping = driver.block_device_info_get_mapping(
3149             block_device_info)
3150         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3151         if mount_rootfs:
3152             fs = vconfig.LibvirtConfigGuestFilesys()
3153             fs.source_type = "mount"
3154             fs.source_dir = os.path.join(
3155                 libvirt_utils.get_instance_path(instance), 'rootfs')
3156             devices.append(fs)
3157         elif os_type == vm_mode.EXE and CONF.libvirt.virt_type == "parallels":
3158             fs = self._get_guest_fs_config(instance, "disk")
3159             devices.append(fs)
3160         else:
3161 
3162             if rescue:
3163                 diskrescue = self._get_guest_disk_config(instance,
3164                                                          'disk.rescue',
3165                                                          disk_mapping,
3166                                                          inst_type)
3167                 devices.append(diskrescue)
3168 
3169                 diskos = self._get_guest_disk_config(instance,
3170                                                      'disk',
3171                                                      disk_mapping,
3172                                                      inst_type)
3173                 devices.append(diskos)
3174             else:
3175                 if 'disk' in disk_mapping:
3176                     diskos = self._get_guest_disk_config(instance,
3177                                                          'disk',
3178                                                          disk_mapping,
3179                                                          inst_type)
3180                     devices.append(diskos)
3181 
3182                 if 'disk.local' in disk_mapping:
3183                     disklocal = self._get_guest_disk_config(instance,
3184                                                             'disk.local',
3185                                                             disk_mapping,
3186                                                             inst_type)
3187                     devices.append(disklocal)
3188                     instance.default_ephemeral_device = (
3189                         block_device.prepend_dev(disklocal.target_dev))
3190 
3191                 for idx, eph in enumerate(
3192                     driver.block_device_info_get_ephemerals(
3193                         block_device_info)):
3194                     diskeph = self._get_guest_disk_config(
3195                         instance,
3196                         blockinfo.get_eph_disk(idx),
3197                         disk_mapping, inst_type)
3198                     devices.append(diskeph)
3199 
3200                 if 'disk.swap' in disk_mapping:
3201                     diskswap = self._get_guest_disk_config(instance,
3202                                                            'disk.swap',
3203                                                            disk_mapping,
3204                                                            inst_type)
3205                     devices.append(diskswap)
3206                     instance.default_swap_device = (
3207                         block_device.prepend_dev(diskswap.target_dev))
3208 
3209             if 'disk.config' in disk_mapping:
3210                 diskconfig = self._get_guest_disk_config(instance,
3211                                                          'disk.config',
3212                                                          disk_mapping,
3213                                                          inst_type,
3214                                                          'raw')
3215                 devices.append(diskconfig)
3216 
3217         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3218                                                    mount_rootfs):
3219             connection_info = vol['connection_info']
3220             vol_dev = block_device.prepend_dev(vol['mount_device'])
3221             info = disk_mapping[vol_dev]
3222             self._connect_volume(connection_info, info)
3223             cfg = self._get_volume_config(connection_info, info)
3224             devices.append(cfg)
3225             vol['connection_info'] = connection_info
3226             vol.save()
3227 
3228         for d in devices:
3229             self._set_cache_mode(d)
3230 
3231         if (image_meta and
3232                 image_meta.get('properties', {}).get('hw_scsi_model')):
3233             hw_scsi_model = image_meta['properties']['hw_scsi_model']
3234             scsi_controller = vconfig.LibvirtConfigGuestController()
3235             scsi_controller.type = 'scsi'
3236             scsi_controller.model = hw_scsi_model
3237             devices.append(scsi_controller)
3238 
3239         return devices
3240 
3241     def _get_host_sysinfo_serial_hardware(self):
3242         """Get a UUID from the host hardware
3243 
3244         Get a UUID for the host hardware reported by libvirt.
3245         This is typically from the SMBIOS data, unless it has
3246         been overridden in /etc/libvirt/libvirtd.conf
3247         """
3248         caps = self._host.get_capabilities()
3249         return caps.host.uuid
3250 
3251     def _get_host_sysinfo_serial_os(self):
3252         """Get a UUID from the host operating system
3253 
3254         Get a UUID for the host operating system. Modern Linux
3255         distros based on systemd provide a /etc/machine-id
3256         file containing a UUID. This is also provided inside
3257         systemd based containers and can be provided by other
3258         init systems too, since it is just a plain text file.
3259         """
3260         with open("/etc/machine-id") as f:
3261             # We want to have '-' in the right place
3262             # so we parse & reformat the value
3263             return str(uuid.UUID(f.read().split()[0]))
3264 
3265     def _get_host_sysinfo_serial_auto(self):
3266         if os.path.exists("/etc/machine-id"):
3267             return self._get_host_sysinfo_serial_os()
3268         else:
3269             return self._get_host_sysinfo_serial_hardware()
3270 
3271     def _get_guest_config_sysinfo(self, instance):
3272         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
3273 
3274         sysinfo.system_manufacturer = version.vendor_string()
3275         sysinfo.system_product = version.product_string()
3276         sysinfo.system_version = version.version_string_with_package()
3277 
3278         sysinfo.system_serial = self._sysinfo_serial_func()
3279         sysinfo.system_uuid = instance.uuid
3280 
3281         sysinfo.system_family = "Virtual Machine"
3282 
3283         return sysinfo
3284 
3285     def _get_guest_pci_device(self, pci_device):
3286 
3287         dbsf = pci_utils.parse_address(pci_device['address'])
3288         dev = vconfig.LibvirtConfigGuestHostdevPCI()
3289         dev.domain, dev.bus, dev.slot, dev.function = dbsf
3290 
3291         # only kvm support managed mode
3292         if CONF.libvirt.virt_type in ('xen', 'parallels',):
3293             dev.managed = 'no'
3294         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3295             dev.managed = 'yes'
3296 
3297         return dev
3298 
3299     def _get_guest_config_meta(self, context, instance):
3300         """Get metadata config for guest."""
3301 
3302         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
3303         meta.package = version.version_string_with_package()
3304         meta.name = instance.display_name
3305         meta.creationTime = time.time()
3306 
3307         if instance.image_ref not in ("", None):
3308             meta.roottype = "image"
3309             meta.rootid = instance.image_ref
3310 
3311         if context is not None:
3312             ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
3313             ometa.userid = context.user_id
3314             ometa.username = context.user_name
3315             ometa.projectid = context.project_id
3316             ometa.projectname = context.project_name
3317             meta.owner = ometa
3318 
3319         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
3320         flavor = instance.flavor
3321         fmeta.name = flavor.name
3322         fmeta.memory = flavor.memory_mb
3323         fmeta.vcpus = flavor.vcpus
3324         fmeta.ephemeral = flavor.ephemeral_gb
3325         fmeta.disk = flavor.root_gb
3326         fmeta.swap = flavor.swap
3327 
3328         meta.flavor = fmeta
3329 
3330         return meta
3331 
3332     def _machine_type_mappings(self):
3333         mappings = {}
3334         for mapping in CONF.libvirt.hw_machine_type:
3335             host_arch, _, machine_type = mapping.partition('=')
3336             mappings[host_arch] = machine_type
3337         return mappings
3338 
3339     def _get_machine_type(self, image_meta, caps):
3340         # The underlying machine type can be set as an image attribute,
3341         # or otherwise based on some architecture specific defaults
3342 
3343         mach_type = None
3344 
3345         if (image_meta is not None and image_meta.get('properties') and
3346                image_meta['properties'].get('hw_machine_type')
3347                is not None):
3348             mach_type = image_meta['properties']['hw_machine_type']
3349         else:
3350             # For ARM systems we will default to vexpress-a15 for armv7
3351             # and virt for aarch64
3352             if caps.host.cpu.arch == arch.ARMV7:
3353                 mach_type = "vexpress-a15"
3354 
3355             if caps.host.cpu.arch == arch.AARCH64:
3356                 mach_type = "virt"
3357 
3358             if caps.host.cpu.arch in (arch.S390, arch.S390X):
3359                 mach_type = 's390-ccw-virtio'
3360 
3361             # If set in the config, use that as the default.
3362             if CONF.libvirt.hw_machine_type:
3363                 mappings = self._machine_type_mappings()
3364                 mach_type = mappings.get(caps.host.cpu.arch)
3365 
3366         return mach_type
3367 
3368     @staticmethod
3369     def _create_idmaps(klass, map_strings):
3370         idmaps = []
3371         if len(map_strings) > 5:
3372             map_strings = map_strings[0:5]
3373             LOG.warn(_LW("Too many id maps, only included first five."))
3374         for map_string in map_strings:
3375             try:
3376                 idmap = klass()
3377                 values = [int(i) for i in map_string.split(":")]
3378                 idmap.start = values[0]
3379                 idmap.target = values[1]
3380                 idmap.count = values[2]
3381                 idmaps.append(idmap)
3382             except (ValueError, IndexError):
3383                 LOG.warn(_LW("Invalid value for id mapping %s"), map_string)
3384         return idmaps
3385 
3386     def _get_guest_idmaps(self):
3387         id_maps = []
3388         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
3389             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
3390                                            CONF.libvirt.uid_maps)
3391             id_maps.extend(uid_maps)
3392         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
3393             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
3394                                            CONF.libvirt.gid_maps)
3395             id_maps.extend(gid_maps)
3396         return id_maps
3397 
3398     def _update_guest_cputune(self, guest, flavor, virt_type):
3399         if virt_type in ('lxc', 'kvm', 'qemu'):
3400             if guest.cputune is None:
3401                 guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
3402             # Setting the default cpu.shares value to be a value
3403             # dependent on the number of vcpus
3404             guest.cputune.shares = 1024 * guest.vcpus
3405 
3406             cputuning = ['shares', 'period', 'quota']
3407             for name in cputuning:
3408                 key = "quota:cpu_" + name
3409                 if key in flavor.extra_specs:
3410                     setattr(guest.cputune, name,
3411                             int(flavor.extra_specs[key]))
3412 
3413     def _get_cpu_numa_config_from_instance(self, instance_numa_topology):
3414         if instance_numa_topology:
3415             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
3416             for instance_cell in instance_numa_topology.cells:
3417                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
3418                 guest_cell.id = instance_cell.id
3419                 guest_cell.cpus = instance_cell.cpuset
3420                 guest_cell.memory = instance_cell.memory * units.Ki
3421                 guest_cpu_numa.cells.append(guest_cell)
3422             return guest_cpu_numa
3423 
3424     def _has_cpu_policy_support(self):
3425         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
3426             if self._host.has_version(ver):
3427                 ver_ = self._version_to_string(ver)
3428                 raise exception.CPUPinningNotSupported(reason=_(
3429                     'Invalid libvirt version %(version)s') % {'version': ver_})
3430         return True
3431 
3432     def _get_guest_numa_config(self, instance_numa_topology, flavor, pci_devs,
3433                                allowed_cpus=None):
3434         """Returns the config objects for the guest NUMA specs.
3435 
3436         Determines the CPUs that the guest can be pinned to if the guest
3437         specifies a cell topology and the host supports it. Constructs the
3438         libvirt XML config object representing the NUMA topology selected
3439         for the guest. Returns a tuple of:
3440 
3441             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
3442 
3443         With the following caveats:
3444 
3445             a) If there is no specified guest NUMA topology, then
3446                all tuple elements except cpu_set shall be None. cpu_set
3447                will be populated with the chosen CPUs that the guest
3448                allowed CPUs fit within, which could be the supplied
3449                allowed_cpus value if the host doesn't support NUMA
3450                topologies.
3451 
3452             b) If there is a specified guest NUMA topology, then
3453                cpu_set will be None and guest_cpu_numa will be the
3454                LibvirtConfigGuestCPUNUMA object representing the guest's
3455                NUMA topology. If the host supports NUMA, then guest_cpu_tune
3456                will contain a LibvirtConfigGuestCPUTune object representing
3457                the optimized chosen cells that match the host capabilities
3458                with the instance's requested topology. If the host does
3459                not support NUMA, then guest_cpu_tune and guest_numa_tune
3460                will be None.
3461         """
3462 
3463         if (not self._has_numa_support() and
3464                 instance_numa_topology is not None):
3465             # We should not get here, since we should have avoided
3466             # reporting NUMA topology from _get_host_numa_topology
3467             # in the first place. Just in case of a scheduler
3468             # mess up though, raise an exception
3469             raise exception.NUMATopologyUnsupported()
3470 
3471         topology = self._get_host_numa_topology()
3472         # We have instance NUMA so translate it to the config class
3473         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
3474                 instance_numa_topology)
3475 
3476         if not guest_cpu_numa_config:
3477             # No NUMA topology defined for instance - let the host kernel deal
3478             # with the NUMA effects.
3479             # TODO(ndipanov): Attempt to spread the instance
3480             # across NUMA nodes and expose the topology to the
3481             # instance as an optimisation
3482             return GuestNumaConfig(allowed_cpus, None, None, None)
3483         else:
3484             if topology:
3485                 # Now get the CpuTune configuration from the numa_topology
3486                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
3487                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
3488                 allpcpus = []
3489 
3490                 numa_mem = vconfig.LibvirtConfigGuestNUMATuneMemory()
3491                 numa_memnodes = [vconfig.LibvirtConfigGuestNUMATuneMemNode()
3492                                  for _ in guest_cpu_numa_config.cells]
3493 
3494                 for host_cell in topology.cells:
3495                     for guest_node_id, guest_config_cell in enumerate(
3496                             guest_cpu_numa_config.cells):
3497                         if guest_config_cell.id == host_cell.id:
3498                             node = numa_memnodes[guest_node_id]
3499                             node.cellid = guest_config_cell.id
3500                             node.nodeset = [host_cell.id]
3501                             node.mode = "strict"
3502 
3503                             numa_mem.nodeset.append(host_cell.id)
3504 
3505                             object_numa_cell = (
3506                                     instance_numa_topology.cells[guest_node_id]
3507                                 )
3508                             for cpu in guest_config_cell.cpus:
3509                                 pin_cpuset = (
3510                                     vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
3511                                 pin_cpuset.id = cpu
3512                                 # If there is pinning information in the cell
3513                                 # we pin to individual CPUs, otherwise we float
3514                                 # over the whole host NUMA node
3515 
3516                                 if (object_numa_cell.cpu_pinning and
3517                                         self._has_cpu_policy_support()):
3518                                     pcpu = object_numa_cell.cpu_pinning[cpu]
3519                                     pin_cpuset.cpuset = set([pcpu])
3520                                 else:
3521                                     pin_cpuset.cpuset = host_cell.cpuset
3522                                 allpcpus.extend(pin_cpuset.cpuset)
3523                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
3524 
3525                 # TODO(berrange) When the guest has >1 NUMA node, it will
3526                 # span multiple host NUMA nodes. By pinning emulator threads
3527                 # to the union of all nodes, we guarantee there will be
3528                 # cross-node memory access by the emulator threads when
3529                 # responding to guest I/O operations. The only way to avoid
3530                 # this would be to pin emulator threads to a single node and
3531                 # tell the guest OS to only do I/O from one of its virtual
3532                 # NUMA nodes. This is not even remotely practical.
3533                 #
3534                 # The long term solution is to make use of a new QEMU feature
3535                 # called "I/O Threads" which will let us configure an explicit
3536                 # I/O thread for each guest vCPU or guest NUMA node. It is
3537                 # still TBD how to make use of this feature though, especially
3538                 # how to associate IO threads with guest devices to eliminiate
3539                 # cross NUMA node traffic. This is an area of investigation
3540                 # for QEMU community devs.
3541                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
3542                 emulatorpin.cpuset = set(allpcpus)
3543                 guest_cpu_tune.emulatorpin = emulatorpin
3544                 # Sort the vcpupin list per vCPU id for human-friendlier XML
3545                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
3546 
3547                 guest_numa_tune.memory = numa_mem
3548                 guest_numa_tune.memnodes = numa_memnodes
3549 
3550                 # normalize cell.id
3551                 for i, (cell, memnode) in enumerate(
3552                                             zip(guest_cpu_numa_config.cells,
3553                                                 guest_numa_tune.memnodes)):
3554                     cell.id = i
3555                     memnode.cellid = i
3556 
3557                 return GuestNumaConfig(None, guest_cpu_tune,
3558                                        guest_cpu_numa_config,
3559                                        guest_numa_tune)
3560             else:
3561                 return GuestNumaConfig(allowed_cpus, None,
3562                                        guest_cpu_numa_config, None)
3563 
3564     def _get_guest_os_type(self, virt_type):
3565         """Returns the guest OS type based on virt type."""
3566         if virt_type == "lxc":
3567             ret = vm_mode.EXE
3568         elif virt_type == "uml":
3569             ret = vm_mode.UML
3570         elif virt_type == "xen":
3571             ret = vm_mode.XEN
3572         else:
3573             ret = vm_mode.HVM
3574         return ret
3575 
3576     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
3577                               root_device_name):
3578         if rescue.get('kernel_id'):
3579             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
3580             if virt_type == "xen":
3581                 guest.os_cmdline = "ro root=%s" % root_device_name
3582             else:
3583                 guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
3584                 if virt_type == "qemu":
3585                     guest.os_cmdline += " no_timer_check"
3586         if rescue.get('ramdisk_id'):
3587             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
3588 
3589     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
3590                                 root_device_name, image_meta):
3591         guest.os_kernel = os.path.join(inst_path, "kernel")
3592         if virt_type == "xen":
3593             guest.os_cmdline = "ro root=%s" % root_device_name
3594         else:
3595             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
3596             if virt_type == "qemu":
3597                 guest.os_cmdline += " no_timer_check"
3598         if instance.ramdisk_id:
3599             guest.os_initrd = os.path.join(inst_path, "ramdisk")
3600         # we only support os_command_line with images with an explicit
3601         # kernel set and don't want to break nova if there's an
3602         # os_command_line property without a specified kernel_id param
3603         if image_meta:
3604             img_props = image_meta.get('properties', {})
3605             if img_props.get('os_command_line'):
3606                 guest.os_cmdline = img_props.get('os_command_line')
3607 
3608     def _set_clock(self, guest, os_type, image_meta, virt_type):
3609         # NOTE(mikal): Microsoft Windows expects the clock to be in
3610         # "localtime". If the clock is set to UTC, then you can use a
3611         # registry key to let windows know, but Microsoft says this is
3612         # buggy in http://support.microsoft.com/kb/2687252
3613         clk = vconfig.LibvirtConfigGuestClock()
3614         if os_type == 'windows':
3615             LOG.info(_LI('Configuring timezone for windows instance to '
3616                          'localtime'))
3617             clk.offset = 'localtime'
3618         else:
3619             clk.offset = 'utc'
3620         guest.set_clock(clk)
3621 
3622         if virt_type == "kvm":
3623             self._set_kvm_timers(clk, os_type, image_meta)
3624 
3625     def _set_kvm_timers(self, clk, os_type, image_meta):
3626         # TODO(berrange) One day this should be per-guest
3627         # OS type configurable
3628         tmpit = vconfig.LibvirtConfigGuestTimer()
3629         tmpit.name = "pit"
3630         tmpit.tickpolicy = "delay"
3631 
3632         tmrtc = vconfig.LibvirtConfigGuestTimer()
3633         tmrtc.name = "rtc"
3634         tmrtc.tickpolicy = "catchup"
3635 
3636         clk.add_timer(tmpit)
3637         clk.add_timer(tmrtc)
3638 
3639         guestarch = libvirt_utils.get_arch(image_meta)
3640         if guestarch in (arch.I686, arch.X86_64):
3641             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
3642             # qemu -no-hpet is not supported on non-x86 targets.
3643             tmhpet = vconfig.LibvirtConfigGuestTimer()
3644             tmhpet.name = "hpet"
3645             tmhpet.present = False
3646             clk.add_timer(tmhpet)
3647 
3648         # With new enough QEMU we can provide Windows guests
3649         # with the paravirtualized hyperv timer source. This
3650         # is the windows equiv of kvm-clock, allowing Windows
3651         # guests to accurately keep time.
3652         if (os_type == 'windows' and
3653             self._host.has_min_version(MIN_LIBVIRT_HYPERV_TIMER_VERSION,
3654                                        MIN_QEMU_HYPERV_TIMER_VERSION)):
3655             tmhyperv = vconfig.LibvirtConfigGuestTimer()
3656             tmhyperv.name = "hypervclock"
3657             tmhyperv.present = True
3658             clk.add_timer(tmhyperv)
3659 
3660     def _set_features(self, guest, os_type, caps, virt_type):
3661         if virt_type == "xen":
3662             # PAE only makes sense in X86
3663             if caps.host.cpu.arch in (arch.I686, arch.X86_64):
3664                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
3665 
3666         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
3667                 (virt_type == "xen" and guest.os_type == vm_mode.HVM)):
3668             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
3669             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
3670 
3671         if (virt_type in ("qemu", "kvm") and
3672                 os_type == 'windows' and
3673                 self._host.has_min_version(MIN_LIBVIRT_HYPERV_FEATURE_VERSION,
3674                                            MIN_QEMU_HYPERV_FEATURE_VERSION)):
3675             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
3676             hv.relaxed = True
3677 
3678             if self._host.has_min_version(
3679                     MIN_LIBVIRT_HYPERV_FEATURE_EXTRA_VERSION):
3680                 hv.spinlocks = True
3681                 # Increase spinlock retries - value recommended by
3682                 # KVM maintainers who certify Windows guests
3683                 # with Microsoft
3684                 hv.spinlock_retries = 8191
3685                 hv.vapic = True
3686             guest.features.append(hv)
3687 
3688     def _create_serial_console_devices(self, guest, instance, flavor,
3689                                        image_meta):
3690         guest_arch = libvirt_utils.get_arch(image_meta)
3691 
3692         if CONF.serial_console.enabled:
3693             num_ports = hardware.get_number_of_serial_ports(
3694                 flavor, image_meta)
3695             for port in six.moves.range(num_ports):
3696                 if guest_arch in (arch.S390, arch.S390X):
3697                     console = vconfig.LibvirtConfigGuestConsole()
3698                 else:
3699                     console = vconfig.LibvirtConfigGuestSerial()
3700                 console.port = port
3701                 console.type = "tcp"
3702                 console.listen_host = (
3703                     CONF.serial_console.proxyclient_address)
3704                 console.listen_port = (
3705                     serial_console.acquire_port(
3706                         console.listen_host))
3707                 guest.add_device(console)
3708         else:
3709             # The QEMU 'pty' driver throws away any data if no
3710             # client app is connected. Thus we can't get away
3711             # with a single type=pty console. Instead we have
3712             # to configure two separate consoles.
3713             if guest_arch in (arch.S390, arch.S390X):
3714                 consolelog = vconfig.LibvirtConfigGuestConsole()
3715                 consolelog.target_type = "sclplm"
3716             else:
3717                 consolelog = vconfig.LibvirtConfigGuestSerial()
3718             consolelog.type = "file"
3719             consolelog.source_path = self._get_console_log_path(instance)
3720             guest.add_device(consolelog)
3721 
3722     def _add_video_driver(self, guest, image_meta, img_meta_prop, flavor):
3723         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga", "xen", "qxl")
3724         video = vconfig.LibvirtConfigGuestVideo()
3725         # NOTE(ldbragst): The following logic sets the video.type
3726         # depending on supported defaults given the architecture,
3727         # virtualization type, and features. The video.type attribute can
3728         # be overridden by the user with image_meta['properties'], which
3729         # is carried out in the next if statement below this one.
3730         guestarch = libvirt_utils.get_arch(image_meta)
3731         if guest.os_type == vm_mode.XEN:
3732             video.type = 'xen'
3733         elif CONF.libvirt.virt_type == 'parallels':
3734             video.type = 'vga'
3735         elif guestarch in (arch.PPC, arch.PPC64):
3736             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
3737             # so use 'vga' instead when running on Power hardware.
3738             video.type = 'vga'
3739         elif CONF.spice.enabled:
3740             video.type = 'qxl'
3741         if img_meta_prop.get('hw_video_model'):
3742             video.type = img_meta_prop.get('hw_video_model')
3743             if (video.type not in VALID_VIDEO_DEVICES):
3744                 raise exception.InvalidVideoMode(model=video.type)
3745 
3746         # Set video memory, only if the flavor's limit is set
3747         video_ram = int(img_meta_prop.get('hw_video_ram', 0))
3748         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
3749         if video_ram > max_vram:
3750             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
3751                                                  max_vram=max_vram)
3752         if max_vram and video_ram:
3753             video.vram = video_ram * units.Mi / units.Ki
3754         guest.add_device(video)
3755 
3756     def _add_qga_device(self, guest, instance):
3757         qga = vconfig.LibvirtConfigGuestChannel()
3758         qga.type = "unix"
3759         qga.target_name = "org.qemu.guest_agent.0"
3760         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
3761                           ("org.qemu.guest_agent.0", instance.name))
3762         guest.add_device(qga)
3763 
3764     def _add_rng_device(self, guest, flavor):
3765         rng_device = vconfig.LibvirtConfigGuestRng()
3766         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
3767         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
3768         if rate_bytes:
3769             rng_device.rate_bytes = int(rate_bytes)
3770             rng_device.rate_period = int(period)
3771         rng_path = CONF.libvirt.rng_dev_path
3772         if (rng_path and not os.path.exists(rng_path)):
3773             raise exception.RngDeviceNotExist(path=rng_path)
3774         rng_device.backend = rng_path
3775         guest.add_device(rng_device)
3776 
3777     def _set_qemu_guest_agent(self, guest, flavor, instance, img_meta_prop):
3778         qga_enabled = False
3779         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
3780         hw_qga = img_meta_prop.get('hw_qemu_guest_agent', '')
3781         if strutils.bool_from_string(hw_qga):
3782             LOG.debug("Qemu guest agent is enabled through image "
3783                       "metadata", instance=instance)
3784             qga_enabled = True
3785         if qga_enabled:
3786             self._add_qga_device(guest, instance)
3787         rng_is_virtio = img_meta_prop.get('hw_rng_model') == 'virtio'
3788         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
3789         rng_allowed = strutils.bool_from_string(rng_allowed_str)
3790         if rng_is_virtio and rng_allowed:
3791             self._add_rng_device(guest, flavor)
3792 
3793     def _get_guest_memory_backing_config(self, inst_topology, numatune):
3794         wantsmempages = False
3795         if inst_topology:
3796             for cell in inst_topology.cells:
3797                 if cell.pagesize:
3798                     wantsmempages = True
3799                     break
3800 
3801         if not wantsmempages:
3802             return
3803 
3804         if not self._has_hugepage_support():
3805             # We should not get here, since we should have avoided
3806             # reporting NUMA topology from _get_host_numa_topology
3807             # in the first place. Just in case of a scheduler
3808             # mess up though, raise an exception
3809             raise exception.MemoryPagesUnsupported()
3810 
3811         host_topology = self._get_host_numa_topology()
3812 
3813         if host_topology is None:
3814             # As above, we should not get here but just in case...
3815             raise exception.MemoryPagesUnsupported()
3816 
3817         # Currently libvirt does not support the smallest
3818         # pagesize set as a backend memory.
3819         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
3820         avail_pagesize = [page.size_kb
3821                           for page in host_topology.cells[0].mempages]
3822         avail_pagesize.sort()
3823         smallest = avail_pagesize[0]
3824 
3825         pages = []
3826         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
3827             if inst_cell.pagesize and inst_cell.pagesize > smallest:
3828                 for memnode in numatune.memnodes:
3829                     if guest_cellid == memnode.cellid:
3830                         page = (
3831                             vconfig.LibvirtConfigGuestMemoryBackingPage())
3832                         page.nodeset = [guest_cellid]
3833                         page.size_kb = inst_cell.pagesize
3834                         pages.append(page)
3835                         break  # Quit early...
3836 
3837         if pages:
3838             membacking = vconfig.LibvirtConfigGuestMemoryBacking()
3839             membacking.hugepages = pages
3840             return membacking
3841 
3842     def _get_flavor(self, ctxt, instance, flavor):
3843         if flavor is not None:
3844             return flavor
3845         return instance.flavor
3846 
3847     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
3848                                       image_meta, flavor, root_device_name):
3849         if virt_type == "xen":
3850             if guest.os_type == vm_mode.HVM:
3851                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
3852         elif virt_type in ("kvm", "qemu"):
3853             if caps.host.cpu.arch in (arch.I686, arch.X86_64):
3854                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
3855                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
3856             guest.os_mach_type = self._get_machine_type(image_meta, caps)
3857             guest.os_bootmenu = strutils.bool_from_string(
3858                 flavor.extra_specs.get(
3859                     'hw:boot_menu', image_meta.get('properties', {}).get(
3860                         'hw_boot_menu', 'no')))
3861         elif virt_type == "lxc":
3862             guest.os_init_path = "/sbin/init"
3863             guest.os_cmdline = CONSOLE
3864         elif virt_type == "uml":
3865             guest.os_kernel = "/usr/bin/linux"
3866             guest.os_root = root_device_name
3867         elif virt_type == "parallels":
3868             if guest.os_type == vm_mode.EXE:
3869                 guest.os_init_path = "/sbin/init"
3870 
3871     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
3872                     instance, inst_path, image_meta, disk_info):
3873         if rescue:
3874             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
3875                                        root_device_name)
3876         elif instance.kernel_id:
3877             self._set_guest_for_inst_kernel(instance, guest, inst_path,
3878                                             virt_type, root_device_name,
3879                                             image_meta)
3880         else:
3881             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
3882 
3883     def _create_consoles(self, virt_type, guest, instance, flavor, image_meta,
3884                          caps):
3885         if virt_type in ("qemu", "kvm"):
3886             # Create the serial console char devices
3887             self._create_serial_console_devices(guest, instance, flavor,
3888                                                 image_meta)
3889             if caps.host.cpu.arch in (arch.S390, arch.S390X):
3890                 consolepty = vconfig.LibvirtConfigGuestConsole()
3891                 consolepty.target_type = "sclp"
3892             else:
3893                 consolepty = vconfig.LibvirtConfigGuestSerial()
3894         else:
3895             consolepty = vconfig.LibvirtConfigGuestConsole()
3896         return consolepty
3897 
3898     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
3899         """Update VirtCPUModel object according to libvirt CPU config.
3900 
3901         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
3902                            instance's virtual cpu configuration.
3903         :param:vcpu_model: VirtCPUModel object. A new object will be created
3904                            if None.
3905 
3906         :return: Updated VirtCPUModel object, or None if cpu_config is None
3907 
3908         """
3909 
3910         if not cpu_config:
3911             return
3912         if not vcpu_model:
3913             vcpu_model = objects.VirtCPUModel()
3914 
3915         vcpu_model.arch = cpu_config.arch
3916         vcpu_model.vendor = cpu_config.vendor
3917         vcpu_model.model = cpu_config.model
3918         vcpu_model.mode = cpu_config.mode
3919         vcpu_model.match = cpu_config.match
3920 
3921         if cpu_config.sockets:
3922             vcpu_model.topology = objects.VirtCPUTopology(
3923                 sockets=cpu_config.sockets,
3924                 cores=cpu_config.cores,
3925                 threads=cpu_config.threads)
3926         else:
3927             vcpu_model.topology = None
3928 
3929         features = [objects.VirtCPUFeature(
3930             name=f.name,
3931             policy=f.policy) for f in cpu_config.features]
3932         vcpu_model.features = features
3933 
3934         return vcpu_model
3935 
3936     def _vcpu_model_to_cpu_config(self, vcpu_model):
3937         """Create libvirt CPU config according to VirtCPUModel object.
3938 
3939         :param:vcpu_model: VirtCPUModel object.
3940 
3941         :return: vconfig.LibvirtConfigGuestCPU.
3942 
3943         """
3944 
3945         cpu_config = vconfig.LibvirtConfigGuestCPU()
3946         cpu_config.arch = vcpu_model.arch
3947         cpu_config.model = vcpu_model.model
3948         cpu_config.mode = vcpu_model.mode
3949         cpu_config.match = vcpu_model.match
3950         cpu_config.vendor = vcpu_model.vendor
3951         if vcpu_model.topology:
3952             cpu_config.sockets = vcpu_model.topology.sockets
3953             cpu_config.cores = vcpu_model.topology.cores
3954             cpu_config.threads = vcpu_model.topology.threads
3955         if vcpu_model.features:
3956             for f in vcpu_model.features:
3957                 xf = vconfig.LibvirtConfigGuestCPUFeature()
3958                 xf.name = f.name
3959                 xf.policy = f.policy
3960                 cpu_config.features.add(xf)
3961         return cpu_config
3962 
3963     def _get_guest_config(self, instance, network_info, image_meta,
3964                           disk_info, rescue=None, block_device_info=None,
3965                           context=None):
3966         """Get config data for parameters.
3967 
3968         :param rescue: optional dictionary that should contain the key
3969             'ramdisk_id' if a ramdisk is needed for the rescue image and
3970             'kernel_id' if a kernel is needed for the rescue image.
3971         """
3972         flavor = instance.flavor
3973         inst_path = libvirt_utils.get_instance_path(instance)
3974         disk_mapping = disk_info['mapping']
3975         img_meta_prop = image_meta.get('properties', {}) if image_meta else {}
3976 
3977         virt_type = CONF.libvirt.virt_type
3978         guest = vconfig.LibvirtConfigGuest()
3979         guest.virt_type = virt_type
3980         guest.name = instance.name
3981         guest.uuid = instance.uuid
3982         # We are using default unit for memory: KiB
3983         guest.memory = flavor.memory_mb * units.Ki
3984         guest.vcpus = flavor.vcpus
3985         allowed_cpus = hardware.get_vcpu_pin_set()
3986         pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3987 
3988         guest_numa_config = self._get_guest_numa_config(
3989                 instance.numa_topology, flavor, pci_devs, allowed_cpus)
3990 
3991         guest.cpuset = guest_numa_config.cpuset
3992         guest.cputune = guest_numa_config.cputune
3993         guest.numatune = guest_numa_config.numatune
3994 
3995         guest.membacking = self._get_guest_memory_backing_config(
3996             instance.numa_topology, guest_numa_config.numatune)
3997 
3998         guest.metadata.append(self._get_guest_config_meta(context,
3999                                                           instance))
4000         guest.idmaps = self._get_guest_idmaps()
4001 
4002         self._update_guest_cputune(guest, flavor, virt_type)
4003 
4004         guest.cpu = self._get_guest_cpu_config(
4005             flavor, image_meta, guest_numa_config.numaconfig,
4006             instance.numa_topology)
4007 
4008         # Notes(yjiang5): we always sync the instance's vcpu model with
4009         # the corresponding config file.
4010         instance.vcpu_model = self._cpu_config_to_vcpu_model(
4011             guest.cpu, instance.vcpu_model)
4012 
4013         if 'root' in disk_mapping:
4014             root_device_name = block_device.prepend_dev(
4015                 disk_mapping['root']['dev'])
4016         else:
4017             root_device_name = None
4018 
4019         if root_device_name:
4020             # NOTE(yamahata):
4021             # for nova.api.ec2.cloud.CloudController.get_metadata()
4022             instance.root_device_name = root_device_name
4023 
4024         guest.os_type = (vm_mode.get_from_instance(instance) or
4025                 self._get_guest_os_type(virt_type))
4026         caps = self._host.get_capabilities()
4027 
4028         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
4029                                            image_meta, flavor,
4030                                            root_device_name)
4031         if virt_type not in ('lxc', 'uml'):
4032             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
4033                     instance, inst_path, image_meta, disk_info)
4034 
4035         self._set_features(guest, instance.os_type, caps, virt_type)
4036         self._set_clock(guest, instance.os_type, image_meta, virt_type)
4037 
4038         storage_configs = self._get_guest_storage_config(
4039                 instance, image_meta, disk_info, rescue, block_device_info,
4040                 flavor, guest.os_type)
4041         for config in storage_configs:
4042             guest.add_device(config)
4043 
4044         for vif in network_info:
4045             config = self.vif_driver.get_config(
4046                 instance, vif, image_meta,
4047                 flavor, virt_type)
4048             guest.add_device(config)
4049 
4050         consolepty = self._create_consoles(virt_type, guest, instance, flavor,
4051                                            image_meta, caps)
4052         if virt_type != 'parallels':
4053             consolepty.type = "pty"
4054             guest.add_device(consolepty)
4055 
4056         tablet = self._get_guest_usb_tablet(guest.os_type)
4057         if tablet:
4058             guest.add_device(tablet)
4059 
4060         if (CONF.spice.enabled and CONF.spice.agent_enabled and
4061                 virt_type not in ('lxc', 'uml', 'xen')):
4062             channel = vconfig.LibvirtConfigGuestChannel()
4063             channel.target_name = "com.redhat.spice.0"
4064             guest.add_device(channel)
4065 
4066         # NB some versions of libvirt support both SPICE and VNC
4067         # at the same time. We're not trying to second guess which
4068         # those versions are. We'll just let libvirt report the
4069         # errors appropriately if the user enables both.
4070         add_video_driver = False
4071         if ((CONF.vnc.enabled and
4072              virt_type not in ('lxc', 'uml'))):
4073             graphics = vconfig.LibvirtConfigGuestGraphics()
4074             graphics.type = "vnc"
4075             graphics.keymap = CONF.vnc.keymap
4076             graphics.listen = CONF.vnc.vncserver_listen
4077             guest.add_device(graphics)
4078             add_video_driver = True
4079 
4080         if (CONF.spice.enabled and
4081                 virt_type not in ('lxc', 'uml', 'xen')):
4082             graphics = vconfig.LibvirtConfigGuestGraphics()
4083             graphics.type = "spice"
4084             graphics.keymap = CONF.spice.keymap
4085             graphics.listen = CONF.spice.server_listen
4086             guest.add_device(graphics)
4087             add_video_driver = True
4088 
4089         if add_video_driver:
4090             self._add_video_driver(guest, image_meta, img_meta_prop, flavor)
4091 
4092         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
4093         if virt_type in ('qemu', 'kvm'):
4094             self._set_qemu_guest_agent(guest, flavor, instance, img_meta_prop)
4095 
4096         if virt_type in ('xen', 'qemu', 'kvm'):
4097             for pci_dev in pci_manager.get_instance_pci_devs(instance):
4098                 guest.add_device(self._get_guest_pci_device(pci_dev))
4099         else:
4100             if len(pci_devs) > 0:
4101                 raise exception.PciDeviceUnsupportedHypervisor(
4102                     type=virt_type)
4103 
4104         if 'hw_watchdog_action' in flavor.extra_specs:
4105             LOG.warn(_LW('Old property name "hw_watchdog_action" is now '
4106                          'deprecated and will be removed in the next release. '
4107                          'Use updated property name '
4108                          '"hw:watchdog_action" instead'))
4109         # TODO(pkholkin): accepting old property name 'hw_watchdog_action'
4110         #                should be removed in the next release
4111         watchdog_action = (flavor.extra_specs.get('hw_watchdog_action') or
4112                            flavor.extra_specs.get('hw:watchdog_action')
4113                            or 'disabled')
4114         if (image_meta is not None and
4115                 image_meta.get('properties', {}).get('hw_watchdog_action')):
4116             watchdog_action = image_meta['properties']['hw_watchdog_action']
4117 
4118         # NB(sross): currently only actually supported by KVM/QEmu
4119         if watchdog_action != 'disabled':
4120             if watchdog_actions.is_valid_watchdog_action(watchdog_action):
4121                 bark = vconfig.LibvirtConfigGuestWatchdog()
4122                 bark.action = watchdog_action
4123                 guest.add_device(bark)
4124             else:
4125                 raise exception.InvalidWatchdogAction(action=watchdog_action)
4126 
4127         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
4128         if (virt_type in ('xen', 'qemu', 'kvm') and
4129                 CONF.libvirt.mem_stats_period_seconds > 0):
4130             balloon = vconfig.LibvirtConfigMemoryBalloon()
4131             if virt_type in ('qemu', 'kvm'):
4132                 balloon.model = 'virtio'
4133             else:
4134                 balloon.model = 'xen'
4135             balloon.period = CONF.libvirt.mem_stats_period_seconds
4136             guest.add_device(balloon)
4137 
4138         return guest
4139 
4140     def _get_guest_usb_tablet(self, os_type):
4141         # We want a tablet if VNC is enabled, or SPICE is enabled and
4142         # the SPICE agent is disabled. If the SPICE agent is enabled
4143         # it provides a paravirt mouse which drastically reduces
4144         # overhead (by eliminating USB polling).
4145         #
4146         # NB: this implies that if both SPICE + VNC are enabled
4147         # at the same time, we'll get the tablet whether the
4148         # SPICE agent is used or not.
4149         need_usb_tablet = False
4150         if CONF.vnc.enabled:
4151             need_usb_tablet = CONF.libvirt.use_usb_tablet
4152         elif CONF.spice.enabled and not CONF.spice.agent_enabled:
4153             need_usb_tablet = CONF.libvirt.use_usb_tablet
4154 
4155         tablet = None
4156         if need_usb_tablet and os_type == vm_mode.HVM:
4157             tablet = vconfig.LibvirtConfigGuestInput()
4158             tablet.type = "tablet"
4159             tablet.bus = "usb"
4160         return tablet
4161 
4162     def _get_guest_xml(self, context, instance, network_info, disk_info,
4163                        image_meta, rescue=None,
4164                        block_device_info=None, write_to_disk=False):
4165         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
4166         # this ahead of time so that we don't acquire it while also
4167         # holding the logging lock.
4168         network_info_str = str(network_info)
4169         msg = ('Start _get_guest_xml '
4170                'network_info=%(network_info)s '
4171                'disk_info=%(disk_info)s '
4172                'image_meta=%(image_meta)s rescue=%(rescue)s '
4173                'block_device_info=%(block_device_info)s' %
4174                {'network_info': network_info_str, 'disk_info': disk_info,
4175                 'image_meta': image_meta, 'rescue': rescue,
4176                 'block_device_info': block_device_info})
4177         # NOTE(mriedem): block_device_info can contain auth_password so we
4178         # need to sanitize the password in the message.
4179         LOG.debug(strutils.mask_password(msg), instance=instance)
4180         conf = self._get_guest_config(instance, network_info, image_meta,
4181                                       disk_info, rescue, block_device_info,
4182                                       context)
4183         xml = conf.to_xml()
4184 
4185         if write_to_disk:
4186             instance_dir = libvirt_utils.get_instance_path(instance)
4187             xml_path = os.path.join(instance_dir, 'libvirt.xml')
4188             libvirt_utils.write_to_file(xml_path, xml)
4189 
4190         LOG.debug('End _get_guest_xml xml=%(xml)s',
4191                   {'xml': xml}, instance=instance)
4192         return xml
4193 
4194     def get_info(self, instance):
4195         """Retrieve information from libvirt for a specific instance name.
4196 
4197         If a libvirt error is encountered during lookup, we might raise a
4198         NotFound exception or Error exception depending on how severe the
4199         libvirt error is.
4200 
4201         """
4202         guest = self._host.get_guest(instance)
4203 
4204         # TODO(sahid): We are converting all calls from a
4205         # virDomain object to use nova.virt.libvirt.Guest.
4206         # We should be able to remove virt_dom at the end.
4207         virt_dom = guest._domain
4208         try:
4209             dom_info = self._host.get_domain_info(virt_dom)
4210         except libvirt.libvirtError as ex:
4211             error_code = ex.get_error_code()
4212             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
4213                 raise exception.InstanceNotFound(instance_id=instance.name)
4214 
4215             msg = (_('Error from libvirt while getting domain info for '
4216                      '%(instance_name)s: [Error Code %(error_code)s] %(ex)s') %
4217                    {'instance_name': instance.name,
4218                     'error_code': error_code,
4219                     'ex': ex})
4220             raise exception.NovaException(msg)
4221 
4222         return hardware.InstanceInfo(state=LIBVIRT_POWER_STATE[dom_info[0]],
4223                                      max_mem_kb=dom_info[1],
4224                                      mem_kb=dom_info[2],
4225                                      num_cpu=dom_info[3],
4226                                      cpu_time_ns=dom_info[4],
4227                                      id=virt_dom.ID())
4228 
4229     def _create_domain_setup_lxc(self, instance, image_meta,
4230                                  block_device_info, disk_info):
4231         inst_path = libvirt_utils.get_instance_path(instance)
4232         block_device_mapping = driver.block_device_info_get_mapping(
4233                                                   block_device_info)
4234         disk_info = disk_info or {}
4235         disk_mapping = disk_info.get('mapping', [])
4236 
4237         if self._is_booted_from_volume(instance, disk_mapping):
4238             root_disk = block_device.get_root_bdm(block_device_mapping)
4239             disk_path = root_disk['connection_info']['data']['device_path']
4240             disk_info = blockinfo.get_info_from_bdm(
4241                 CONF.libvirt.virt_type, image_meta, root_disk)
4242             self._connect_volume(root_disk['connection_info'], disk_info)
4243 
4244             # Get the system metadata from the instance
4245             use_cow = instance.system_metadata['image_disk_format'] == 'qcow2'
4246         else:
4247             image = self.image_backend.image(instance, 'disk')
4248             disk_path = image.path
4249             use_cow = CONF.use_cow_images
4250 
4251         container_dir = os.path.join(inst_path, 'rootfs')
4252         fileutils.ensure_tree(container_dir)
4253         fmt = imgmodel.FORMAT_RAW
4254         if use_cow:
4255             fmt = imgmodel.FORMAT_QCOW2
4256         image = imgmodel.LocalFileImage(disk_path, fmt)
4257         rootfs_dev = disk.setup_container(image,
4258                                           container_dir=container_dir)
4259 
4260         try:
4261             # Save rootfs device to disconnect it when deleting the instance
4262             if rootfs_dev:
4263                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
4264             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
4265                 id_maps = self._get_guest_idmaps()
4266                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
4267         except Exception:
4268             with excutils.save_and_reraise_exception():
4269                 self._create_domain_cleanup_lxc(instance)
4270 
4271     def _create_domain_cleanup_lxc(self, instance):
4272         inst_path = libvirt_utils.get_instance_path(instance)
4273         container_dir = os.path.join(inst_path, 'rootfs')
4274 
4275         try:
4276             state = self.get_info(instance).state
4277         except exception.InstanceNotFound:
4278             # The domain may not be present if the instance failed to start
4279             state = None
4280 
4281         if state == power_state.RUNNING:
4282             # NOTE(uni): Now the container is running with its own private
4283             # mount namespace and so there is no need to keep the container
4284             # rootfs mounted in the host namespace
4285             disk.clean_lxc_namespace(container_dir=container_dir)
4286         else:
4287             disk.teardown_container(container_dir=container_dir)
4288 
4289     @contextlib.contextmanager
4290     def _lxc_disk_handler(self, instance, image_meta,
4291                           block_device_info, disk_info):
4292         """Context manager to handle the pre and post instance boot,
4293            LXC specific disk operations.
4294 
4295            An image or a volume path will be prepared and setup to be
4296            used by the container, prior to starting it.
4297            The disk will be disconnected and unmounted if a container has
4298            failed to start.
4299         """
4300 
4301         if CONF.libvirt.virt_type != 'lxc':
4302             yield
4303             return
4304 
4305         self._create_domain_setup_lxc(instance, image_meta,
4306                                       block_device_info, disk_info)
4307 
4308         try:
4309             yield
4310         finally:
4311             self._create_domain_cleanup_lxc(instance)
4312 
4313     # TODO(sahid): Consider renaming this to _create_guest.
4314     def _create_domain(self, xml=None, domain=None,
4315                        power_on=True, pause=False):
4316         """Create a domain.
4317 
4318         Either domain or xml must be passed in. If both are passed, then
4319         the domain definition is overwritten from the xml.
4320 
4321         :returns guest.Guest: Guest just created
4322         """
4323         if xml:
4324             guest = libvirt_guest.Guest.create(xml, self._host)
4325         else:
4326             guest = libvirt_guest.Guest(domain)
4327 
4328         if power_on or pause:
4329             guest.launch(pause=pause)
4330 
4331         if not utils.is_neutron():
4332             guest.enable_hairpin()
4333 
4334         return guest
4335 
4336     def _neutron_failed_callback(self, event_name, instance):
4337         LOG.error(_LE('Neutron Reported failure on event '
4338                       '%(event)s for instance %(uuid)s'),
4339                   {'event': event_name, 'uuid': instance.uuid})
4340         if CONF.vif_plugging_is_fatal:
4341             raise exception.VirtualInterfaceCreateException()
4342 
4343     def _get_neutron_events(self, network_info):
4344         # NOTE(danms): We need to collect any VIFs that are currently
4345         # down that we expect a down->up event for. Anything that is
4346         # already up will not undergo that transition, and for
4347         # anything that might be stale (cache-wise) assume it's
4348         # already up so we don't block on it.
4349         return [('network-vif-plugged', vif['id'])
4350                 for vif in network_info if vif.get('active', True) is False]
4351 
4352     def _create_domain_and_network(self, context, xml, instance, network_info,
4353                                    disk_info, block_device_info=None,
4354                                    power_on=True, reboot=False,
4355                                    vifs_already_plugged=False):
4356 
4357         """Do required network setup and create domain."""
4358         block_device_mapping = driver.block_device_info_get_mapping(
4359             block_device_info)
4360         image_meta = utils.get_image_from_system_metadata(
4361             instance.system_metadata)
4362 
4363         for vol in block_device_mapping:
4364             connection_info = vol['connection_info']
4365 
4366             if (not reboot and 'data' in connection_info and
4367                     'volume_id' in connection_info['data']):
4368                 volume_id = connection_info['data']['volume_id']
4369                 encryption = encryptors.get_encryption_metadata(
4370                     context, self._volume_api, volume_id, connection_info)
4371 
4372                 if encryption:
4373                     encryptor = self._get_volume_encryptor(connection_info,
4374                                                            encryption)
4375                     encryptor.attach_volume(context, **encryption)
4376 
4377         timeout = CONF.vif_plugging_timeout
4378         if (self._conn_supports_start_paused and
4379             utils.is_neutron() and not
4380             vifs_already_plugged and power_on and timeout):
4381             events = self._get_neutron_events(network_info)
4382         else:
4383             events = []
4384 
4385         pause = bool(events)
4386         guest = None
4387         try:
4388             with self.virtapi.wait_for_instance_event(
4389                     instance, events, deadline=timeout,
4390                     error_callback=self._neutron_failed_callback):
4391                 self.plug_vifs(instance, network_info)
4392                 self.firewall_driver.setup_basic_filtering(instance,
4393                                                            network_info)
4394                 self.firewall_driver.prepare_instance_filter(instance,
4395                                                              network_info)
4396                 with self._lxc_disk_handler(instance, image_meta,
4397                                             block_device_info, disk_info):
4398                     guest = self._create_domain(
4399                         xml, pause=pause, power_on=power_on)
4400 
4401                 self.firewall_driver.apply_instance_filter(instance,
4402                                                            network_info)
4403         except exception.VirtualInterfaceCreateException:
4404             # Neutron reported failure and we didn't swallow it, so
4405             # bail here
4406             with excutils.save_and_reraise_exception():
4407                 if guest:
4408                     guest.poweroff()
4409                 self.cleanup(context, instance, network_info=network_info,
4410                              block_device_info=block_device_info)
4411         except eventlet.timeout.Timeout:
4412             # We never heard from Neutron
4413             LOG.warn(_LW('Timeout waiting for vif plugging callback for '
4414                          'instance %(uuid)s'), {'uuid': instance.uuid})
4415             if CONF.vif_plugging_is_fatal:
4416                 if guest:
4417                     guest.poweroff()
4418                 self.cleanup(context, instance, network_info=network_info,
4419                              block_device_info=block_device_info)
4420                 raise exception.VirtualInterfaceCreateException()
4421 
4422         # Resume only if domain has been paused
4423         if pause:
4424             guest.resume()
4425         return guest
4426 
4427     def _get_all_block_devices(self):
4428         """Return all block devices in use on this node."""
4429         devices = []
4430         for dom in self._host.list_instance_domains():
4431             try:
4432                 # TODO(sahid): list_instance_domain should
4433                 # be renamed as list_guest and so returning
4434                 # Guest objects.
4435                 guest = libvirt_guest.Guest(dom)
4436                 doc = etree.fromstring(guest.get_xml_desc())
4437             except libvirt.libvirtError as e:
4438                 LOG.warn(_LW("couldn't obtain the XML from domain:"
4439                              " %(uuid)s, exception: %(ex)s") %
4440                          {"uuid": guest.id, "ex": e})
4441                 continue
4442             except Exception:
4443                 continue
4444             sources = doc.findall("./devices/disk[@type='block']/source")
4445             for source in sources:
4446                 devices.append(source.get('dev'))
4447         return devices
4448 
4449     def _get_interfaces(self, xml):
4450         """Note that this function takes a domain xml.
4451 
4452         Returns a list of all network interfaces for this instance.
4453         """
4454         doc = None
4455 
4456         try:
4457             doc = etree.fromstring(xml)
4458         except Exception:
4459             return []
4460 
4461         interfaces = []
4462 
4463         nodes = doc.findall('./devices/interface/target')
4464         for target in nodes:
4465             interfaces.append(target.get('dev'))
4466 
4467         return interfaces
4468 
4469     def _get_vcpu_total(self):
4470         """Get available vcpu number of physical computer.
4471 
4472         :returns: the number of cpu core instances can be used.
4473 
4474         """
4475         try:
4476             total_pcpus = self._host.get_cpu_count()
4477         except libvirt.libvirtError:
4478             LOG.warn(_LW("Cannot get the number of cpu, because this "
4479                          "function is not implemented for this platform. "))
4480             return 0
4481 
4482         if CONF.vcpu_pin_set is None:
4483             return total_pcpus
4484 
4485         available_ids = hardware.get_vcpu_pin_set()
4486         # We get the list of online CPUs on the host and see if the requested
4487         # set falls under these. If not, we retain the old behavior.
4488         online_pcpus = None
4489         try:
4490             online_pcpus = self._host.get_online_cpus()
4491         except libvirt.libvirtError as ex:
4492             error_code = ex.get_error_code()
4493             LOG.warn(_LW("Couldn't retrieve the online CPUs due to a Libvirt "
4494                          "error: %(error)s with error code: %(error_code)s"),
4495                      {'error': ex, 'error_code': error_code})
4496         if online_pcpus:
4497             if not (available_ids <= online_pcpus):
4498                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
4499                          "specified cpuset is not online. Online cpuset(s): "
4500                          "%(online)s, requested cpuset(s): %(req)s"),
4501                        {'online': sorted(online_pcpus),
4502                         'req': sorted(available_ids)})
4503                 raise exception.Invalid(msg)
4504         elif sorted(available_ids)[-1] >= total_pcpus:
4505             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
4506                                       "out of hypervisor cpu range."))
4507         return len(available_ids)
4508 
4509     @staticmethod
4510     def _get_local_gb_info():
4511         """Get local storage info of the compute node in GB.
4512 
4513         :returns: A dict containing:
4514              :total: How big the overall usable filesystem is (in gigabytes)
4515              :free: How much space is free (in gigabytes)
4516              :used: How much space is used (in gigabytes)
4517         """
4518 
4519         if CONF.libvirt.images_type == 'lvm':
4520             info = lvm.get_volume_group_info(
4521                                CONF.libvirt.images_volume_group)
4522         elif CONF.libvirt.images_type == 'rbd':
4523             info = LibvirtDriver._get_rbd_driver().get_pool_info()
4524         else:
4525             info = libvirt_utils.get_fs_info(CONF.instances_path)
4526 
4527         for (k, v) in six.iteritems(info):
4528             info[k] = v / units.Gi
4529 
4530         return info
4531 
4532     def _get_vcpu_used(self):
4533         """Get vcpu usage number of physical computer.
4534 
4535         :returns: The total number of vcpu(s) that are currently being used.
4536 
4537         """
4538 
4539         total = 0
4540         if CONF.libvirt.virt_type == 'lxc':
4541             return total + 1
4542 
4543         for dom in self._host.list_instance_domains():
4544             try:
4545                 # TODO(sahid): list_instance_domains should
4546                 # return Guest objects.
4547                 vcpus = libvirt_guest.Guest(dom).get_vcpus_info()
4548                 if vcpus is not None:
4549                     total += len(list(vcpus))
4550             except libvirt.libvirtError as e:
4551                 LOG.warn(_LW("couldn't obtain the vpu count from domain id:"
4552                              " %(uuid)s, exception: %(ex)s") %
4553                          {"uuid": dom.UUIDString(), "ex": e})
4554             # NOTE(gtt116): give other tasks a chance.
4555             greenthread.sleep(0)
4556         return total
4557 
4558     def _get_instance_capabilities(self):
4559         """Get hypervisor instance capabilities
4560 
4561         Returns a list of tuples that describe instances the
4562         hypervisor is capable of hosting.  Each tuple consists
4563         of the triplet (arch, hypervisor_type, vm_mode).
4564 
4565         :returns: List of tuples describing instance capabilities
4566         """
4567         caps = self._host.get_capabilities()
4568         instance_caps = list()
4569         for g in caps.guests:
4570             for dt in g.domtype:
4571                 instance_cap = (
4572                     arch.canonicalize(g.arch),
4573                     hv_type.canonicalize(dt),
4574                     vm_mode.canonicalize(g.ostype))
4575                 instance_caps.append(instance_cap)
4576 
4577         return instance_caps
4578 
4579     def _get_cpu_info(self):
4580         """Get cpuinfo information.
4581 
4582         Obtains cpu feature from virConnect.getCapabilities.
4583 
4584         :return: see above description
4585 
4586         """
4587 
4588         caps = self._host.get_capabilities()
4589         cpu_info = dict()
4590 
4591         cpu_info['arch'] = caps.host.cpu.arch
4592         cpu_info['model'] = caps.host.cpu.model
4593         cpu_info['vendor'] = caps.host.cpu.vendor
4594 
4595         topology = dict()
4596         topology['sockets'] = caps.host.cpu.sockets
4597         topology['cores'] = caps.host.cpu.cores
4598         topology['threads'] = caps.host.cpu.threads
4599         cpu_info['topology'] = topology
4600 
4601         features = set()
4602         for f in caps.host.cpu.features:
4603             features.add(f.name)
4604         cpu_info['features'] = features
4605         return cpu_info
4606 
4607     def _get_pcidev_info(self, devname):
4608         """Returns a dict of PCI device."""
4609 
4610         def _get_device_type(cfgdev):
4611             """Get a PCI device's device type.
4612 
4613             An assignable PCI device can be a normal PCI device,
4614             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
4615             Function (VF). Only normal PCI devices or SR-IOV VFs
4616             are assignable, while SR-IOV PFs are always owned by
4617             hypervisor.
4618 
4619             Please notice that a PCI device with SR-IOV
4620             capability but not enabled is reported as normal PCI device.
4621             """
4622             for fun_cap in cfgdev.pci_capability.fun_capability:
4623                 if len(fun_cap.device_addrs) != 0:
4624                     if fun_cap.type == 'virt_functions':
4625                         return {'dev_type': 'type-PF'}
4626                     if fun_cap.type == 'phys_function':
4627                         phys_address = "%04x:%02x:%02x.%01x" % (
4628                             fun_cap.device_addrs[0][0],
4629                             fun_cap.device_addrs[0][1],
4630                             fun_cap.device_addrs[0][2],
4631                             fun_cap.device_addrs[0][3])
4632                         return {'dev_type': 'type-VF',
4633                                 'phys_function': phys_address}
4634             return {'dev_type': 'type-PCI'}
4635 
4636         virtdev = self._host.device_lookup_by_name(devname)
4637         xmlstr = virtdev.XMLDesc(0)
4638         cfgdev = vconfig.LibvirtConfigNodeDevice()
4639         cfgdev.parse_str(xmlstr)
4640 
4641         address = "%04x:%02x:%02x.%1x" % (
4642             cfgdev.pci_capability.domain,
4643             cfgdev.pci_capability.bus,
4644             cfgdev.pci_capability.slot,
4645             cfgdev.pci_capability.function)
4646 
4647         device = {
4648             "dev_id": cfgdev.name,
4649             "address": address,
4650             "product_id": "%04x" % cfgdev.pci_capability.product_id,
4651             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
4652             }
4653 
4654         device["numa_node"] = cfgdev.pci_capability.numa_node
4655 
4656         # requirement by DataBase Model
4657         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
4658         device.update(_get_device_type(cfgdev))
4659         return device
4660 
4661     def _get_pci_passthrough_devices(self):
4662         """Get host PCI devices information.
4663 
4664         Obtains pci devices information from libvirt, and returns
4665         as a JSON string.
4666 
4667         Each device information is a dictionary, with mandatory keys
4668         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
4669         'label' and other optional device specific information.
4670 
4671         Refer to the objects/pci_device.py for more idea of these keys.
4672 
4673         :returns: a JSON string containaing a list of the assignable PCI
4674                   devices information
4675         """
4676         # Bail early if we know we can't support `listDevices` to avoid
4677         # repeated warnings within a periodic task
4678         if not getattr(self, '_list_devices_supported', True):
4679             return jsonutils.dumps([])
4680 
4681         try:
4682             dev_names = self._host.list_pci_devices() or []
4683         except libvirt.libvirtError as ex:
4684             error_code = ex.get_error_code()
4685             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
4686                 self._list_devices_supported = False
4687                 LOG.warn(_LW("URI %(uri)s does not support "
4688                              "listDevices: %(error)s"),
4689                              {'uri': self._uri(), 'error': ex})
4690                 return jsonutils.dumps([])
4691             else:
4692                 raise
4693 
4694         pci_info = []
4695         for name in dev_names:
4696             pci_info.append(self._get_pcidev_info(name))
4697 
4698         return jsonutils.dumps(pci_info)
4699 
4700     def _has_numa_support(self):
4701         # This means that the host can support LibvirtConfigGuestNUMATune
4702         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
4703         supported_archs = [arch.I686, arch.X86_64]
4704         caps = self._host.get_capabilities()
4705 
4706         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
4707             if self._host.has_version(ver):
4708                 return False
4709 
4710         return ((caps.host.cpu.arch in supported_archs) and
4711                 self._host.has_min_version(MIN_LIBVIRT_NUMA_VERSION,
4712                                            MIN_QEMU_NUMA_HUGEPAGE_VERSION,
4713                                            host.HV_DRIVER_QEMU))
4714 
4715     def _has_hugepage_support(self):
4716         # This means that the host can support multiple values for the size
4717         # field in LibvirtConfigGuestMemoryBackingPage
4718         supported_archs = [arch.I686, arch.X86_64]
4719         caps = self._host.get_capabilities()
4720         return ((caps.host.cpu.arch in supported_archs) and
4721                 self._host.has_min_version(MIN_LIBVIRT_HUGEPAGE_VERSION,
4722                                            MIN_QEMU_NUMA_HUGEPAGE_VERSION,
4723                                            host.HV_DRIVER_QEMU))
4724 
4725     def _get_host_numa_topology(self):
4726         if not self._has_numa_support():
4727             return
4728 
4729         caps = self._host.get_capabilities()
4730         topology = caps.host.topology
4731 
4732         if topology is None or not topology.cells:
4733             return
4734 
4735         cells = []
4736         allowed_cpus = hardware.get_vcpu_pin_set()
4737         online_cpus = self._host.get_online_cpus()
4738         if allowed_cpus:
4739             allowed_cpus &= online_cpus
4740         else:
4741             allowed_cpus = online_cpus
4742 
4743         for cell in topology.cells:
4744             cpuset = set(cpu.id for cpu in cell.cpus)
4745             siblings = sorted(map(set,
4746                                   set(tuple(cpu.siblings)
4747                                         if cpu.siblings else ()
4748                                       for cpu in cell.cpus)
4749                                   ))
4750             cpuset &= allowed_cpus
4751             siblings = [sib & allowed_cpus for sib in siblings]
4752             # Filter out singles and empty sibling sets that may be left
4753             siblings = [sib for sib in siblings if len(sib) > 1]
4754 
4755             mempages = []
4756             if self._has_hugepage_support():
4757                 mempages = [
4758                     objects.NUMAPagesTopology(
4759                         size_kb=pages.size,
4760                         total=pages.total,
4761                         used=0)
4762                     for pages in cell.mempages]
4763 
4764             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
4765                                     memory=cell.memory / units.Ki,
4766                                     cpu_usage=0, memory_usage=0,
4767                                     siblings=siblings,
4768                                     pinned_cpus=set([]),
4769                                     mempages=mempages)
4770             cells.append(cell)
4771 
4772         return objects.NUMATopology(cells=cells)
4773 
4774     def get_all_volume_usage(self, context, compute_host_bdms):
4775         """Return usage info for volumes attached to vms on
4776            a given host.
4777         """
4778         vol_usage = []
4779 
4780         for instance_bdms in compute_host_bdms:
4781             instance = instance_bdms['instance']
4782 
4783             for bdm in instance_bdms['instance_bdms']:
4784                 mountpoint = bdm['device_name']
4785                 if mountpoint.startswith('/dev/'):
4786                     mountpoint = mountpoint[5:]
4787                 volume_id = bdm['volume_id']
4788 
4789                 LOG.debug("Trying to get stats for the volume %s",
4790                           volume_id)
4791                 vol_stats = self.block_stats(instance, mountpoint)
4792 
4793                 if vol_stats:
4794                     stats = dict(volume=volume_id,
4795                                  instance=instance,
4796                                  rd_req=vol_stats[0],
4797                                  rd_bytes=vol_stats[1],
4798                                  wr_req=vol_stats[2],
4799                                  wr_bytes=vol_stats[3])
4800                     LOG.debug(
4801                         "Got volume usage stats for the volume=%(volume)s,"
4802                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
4803                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
4804                         stats, instance=instance)
4805                     vol_usage.append(stats)
4806 
4807         return vol_usage
4808 
4809     def block_stats(self, instance, disk_id):
4810         """Note that this function takes an instance name."""
4811         try:
4812             guest = self._host.get_guest(instance)
4813 
4814             # TODO(sahid): We are converting all calls from a
4815             # virDomain object to use nova.virt.libvirt.Guest.
4816             # We should be able to remove domain at the end.
4817             domain = guest._domain
4818             return domain.blockStats(disk_id)
4819         except libvirt.libvirtError as e:
4820             errcode = e.get_error_code()
4821             LOG.info(_LI('Getting block stats failed, device might have '
4822                          'been detached. Instance=%(instance_name)s '
4823                          'Disk=%(disk)s Code=%(errcode)s Error=%(e)s'),
4824                      {'instance_name': instance.name, 'disk': disk_id,
4825                       'errcode': errcode, 'e': e},
4826                      instance=instance)
4827         except exception.InstanceNotFound:
4828             LOG.info(_LI('Could not find domain in libvirt for instance %s. '
4829                          'Cannot get block stats for device'), instance.name,
4830                      instance=instance)
4831 
4832     def get_console_pool_info(self, console_type):
4833         # TODO(mdragon): console proxy should be implemented for libvirt,
4834         #                in case someone wants to use it with kvm or
4835         #                such. For now return fake data.
4836         return {'address': '127.0.0.1',
4837                 'username': 'fakeuser',
4838                 'password': 'fakepassword'}
4839 
4840     def refresh_security_group_rules(self, security_group_id):
4841         self.firewall_driver.refresh_security_group_rules(security_group_id)
4842 
4843     def refresh_security_group_members(self, security_group_id):
4844         self.firewall_driver.refresh_security_group_members(security_group_id)
4845 
4846     def refresh_instance_security_rules(self, instance):
4847         self.firewall_driver.refresh_instance_security_rules(instance)
4848 
4849     def refresh_provider_fw_rules(self):
4850         self.firewall_driver.refresh_provider_fw_rules()
4851 
4852     def get_available_resource(self, nodename):
4853         """Retrieve resource information.
4854 
4855         This method is called when nova-compute launches, and
4856         as part of a periodic task that records the results in the DB.
4857 
4858         :param nodename: will be put in PCI device
4859         :returns: dictionary containing resource info
4860         """
4861 
4862         disk_info_dict = self._get_local_gb_info()
4863         data = {}
4864 
4865         # NOTE(dprince): calling capabilities before getVersion works around
4866         # an initialization issue with some versions of Libvirt (1.0.5.5).
4867         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
4868         # See: https://bugs.launchpad.net/nova/+bug/1215593
4869 
4870         # Temporary convert supported_instances into a string, while keeping
4871         # the RPC version as JSON. Can be changed when RPC broadcast is removed
4872         data["supported_instances"] = jsonutils.dumps(
4873             self._get_instance_capabilities())
4874 
4875         data["vcpus"] = self._get_vcpu_total()
4876         data["memory_mb"] = self._host.get_memory_mb_total()
4877         data["local_gb"] = disk_info_dict['total']
4878         data["vcpus_used"] = self._get_vcpu_used()
4879         data["memory_mb_used"] = self._host.get_memory_mb_used()
4880         data["local_gb_used"] = disk_info_dict['used']
4881         data["hypervisor_type"] = self._host.get_driver_type()
4882         data["hypervisor_version"] = self._host.get_version()
4883         data["hypervisor_hostname"] = self._host.get_hostname()
4884         # TODO(berrange): why do we bother converting the
4885         # libvirt capabilities XML into a special JSON format ?
4886         # The data format is different across all the drivers
4887         # so we could just return the raw capabilities XML
4888         # which 'compare_cpu' could use directly
4889         #
4890         # That said, arch_filter.py now seems to rely on
4891         # the libvirt drivers format which suggests this
4892         # data format needs to be standardized across drivers
4893         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
4894 
4895         disk_free_gb = disk_info_dict['free']
4896         disk_over_committed = self._get_disk_over_committed_size_total()
4897         available_least = disk_free_gb * units.Gi - disk_over_committed
4898         data['disk_available_least'] = available_least / units.Gi
4899 
4900         data['pci_passthrough_devices'] = \
4901             self._get_pci_passthrough_devices()
4902 
4903         numa_topology = self._get_host_numa_topology()
4904         if numa_topology:
4905             data['numa_topology'] = numa_topology._to_json()
4906         else:
4907             data['numa_topology'] = None
4908 
4909         return data
4910 
4911     def check_instance_shared_storage_local(self, context, instance):
4912         """Check if instance files located on shared storage.
4913 
4914         This runs check on the destination host, and then calls
4915         back to the source host to check the results.
4916 
4917         :param context: security context
4918         :param instance: nova.objects.instance.Instance object
4919         :returns
4920             :tempfile: A dict containing the tempfile info on the destination
4921                        host
4922             :None: 1. If the instance path is not existing.
4923                    2. If the image backend is shared block storage type.
4924         """
4925         if self.image_backend.backend().is_shared_block_storage():
4926             return None
4927 
4928         dirpath = libvirt_utils.get_instance_path(instance)
4929 
4930         if not os.path.exists(dirpath):
4931             return None
4932 
4933         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
4934         LOG.debug("Creating tmpfile %s to verify with other "
4935                   "compute node that the instance is on "
4936                   "the same shared storage.",
4937                   tmp_file, instance=instance)
4938         os.close(fd)
4939         return {"filename": tmp_file}
4940 
4941     def check_instance_shared_storage_remote(self, context, data):
4942         return os.path.exists(data['filename'])
4943 
4944     def check_instance_shared_storage_cleanup(self, context, data):
4945         fileutils.delete_if_exists(data["filename"])
4946 
4947     def check_can_live_migrate_destination(self, context, instance,
4948                                            src_compute_info, dst_compute_info,
4949                                            block_migration=False,
4950                                            disk_over_commit=False):
4951         """Check if it is possible to execute live migration.
4952 
4953         This runs checks on the destination host, and then calls
4954         back to the source host to check the results.
4955 
4956         :param context: security context
4957         :param instance: nova.db.sqlalchemy.models.Instance
4958         :param block_migration: if true, prepare for block migration
4959         :param disk_over_commit: if true, allow disk over commit
4960         :returns: a dict containing:
4961              :filename: name of the tmpfile under CONF.instances_path
4962              :block_migration: whether this is block migration
4963              :disk_over_commit: disk-over-commit factor on dest host
4964              :disk_available_mb: available disk space on dest host
4965         """
4966         disk_available_mb = None
4967         if block_migration:
4968             disk_available_gb = dst_compute_info['disk_available_least']
4969             disk_available_mb = \
4970                     (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb
4971 
4972         # Compare CPU
4973         if not instance.vcpu_model or not instance.vcpu_model.model:
4974             source_cpu_info = src_compute_info['cpu_info']
4975             self._compare_cpu(None, source_cpu_info)
4976         else:
4977             self._compare_cpu(instance.vcpu_model, None)
4978 
4979         # Create file on storage, to be checked on source host
4980         filename = self._create_shared_storage_test_file()
4981 
4982         return {"filename": filename,
4983                 "image_type": CONF.libvirt.images_type,
4984                 "block_migration": block_migration,
4985                 "disk_over_commit": disk_over_commit,
4986                 "disk_available_mb": disk_available_mb}
4987 
4988     def check_can_live_migrate_destination_cleanup(self, context,
4989                                                    dest_check_data):
4990         """Do required cleanup on dest host after check_can_live_migrate calls
4991 
4992         :param context: security context
4993         """
4994         filename = dest_check_data["filename"]
4995         self._cleanup_shared_storage_test_file(filename)
4996 
4997     def check_can_live_migrate_source(self, context, instance,
4998                                       dest_check_data,
4999                                       block_device_info=None):
5000         """Check if it is possible to execute live migration.
5001 
5002         This checks if the live migration can succeed, based on the
5003         results from check_can_live_migrate_destination.
5004 
5005         :param context: security context
5006         :param instance: nova.db.sqlalchemy.models.Instance
5007         :param dest_check_data: result of check_can_live_migrate_destination
5008         :param block_device_info: result of _get_instance_block_device_info
5009         :returns: a dict containing migration info
5010         """
5011         # Checking shared storage connectivity
5012         # if block migration, instances_paths should not be on shared storage.
5013         source = CONF.host
5014 
5015         dest_check_data.update({'is_shared_instance_path':
5016                 self._check_shared_storage_test_file(
5017                     dest_check_data['filename'])})
5018 
5019         dest_check_data.update({'is_shared_block_storage':
5020                 self._is_shared_block_storage(instance, dest_check_data,
5021                                               block_device_info)})
5022 
5023         if dest_check_data['block_migration']:
5024             if (dest_check_data['is_shared_block_storage'] or
5025                     dest_check_data['is_shared_instance_path']):
5026                 reason = _("Block migration can not be used "
5027                            "with shared storage.")
5028                 raise exception.InvalidLocalStorage(reason=reason, path=source)
5029             self._assert_dest_node_has_enough_disk(context, instance,
5030                                     dest_check_data['disk_available_mb'],
5031                                     dest_check_data['disk_over_commit'],
5032                                     block_device_info)
5033 
5034         elif not (dest_check_data['is_shared_block_storage'] or
5035                   dest_check_data['is_shared_instance_path']):
5036             reason = _("Live migration can not be used "
5037                        "without shared storage.")
5038             raise exception.InvalidSharedStorage(reason=reason, path=source)
5039 
5040         # NOTE(mikal): include the instance directory name here because it
5041         # doesn't yet exist on the destination but we want to force that
5042         # same name to be used
5043         instance_path = libvirt_utils.get_instance_path(instance,
5044                                                         relative=True)
5045         dest_check_data['instance_relative_path'] = instance_path
5046 
5047         # NOTE(danms): Emulate this old flag in case we're talking to
5048         # an older client (<= Juno). We can remove this when we bump the
5049         # compute RPC API to 4.0.
5050         dest_check_data['is_shared_storage'] = (
5051             dest_check_data['is_shared_instance_path'])
5052 
5053         return dest_check_data
5054 
5055     def _is_shared_block_storage(self, instance, dest_check_data,
5056                                  block_device_info=None):
5057         """Check if all block storage of an instance can be shared
5058         between source and destination of a live migration.
5059 
5060         Returns true if the instance is volume backed and has no local disks,
5061         or if the image backend is the same on source and destination and the
5062         backend shares block storage between compute nodes.
5063 
5064         :param instance: nova.objects.instance.Instance object
5065         :param dest_check_data: dict with boolean fields image_type,
5066                                 is_shared_instance_path, and is_volume_backed
5067         """
5068         if (CONF.libvirt.images_type == dest_check_data.get('image_type') and
5069                 self.image_backend.backend().is_shared_block_storage()):
5070             # NOTE(dgenin): currently true only for RBD image backend
5071             return True
5072 
5073         if (dest_check_data.get('is_shared_instance_path') and
5074                 self.image_backend.backend().is_file_in_instance_path()):
5075             # NOTE(angdraug): file based image backends (Raw, Qcow2)
5076             # place block device files under the instance path
5077             return True
5078 
5079         if (dest_check_data.get('is_volume_backed') and
5080                 not bool(jsonutils.loads(
5081                     self.get_instance_disk_info(instance,
5082                                                 block_device_info)))):
5083             return True
5084 
5085         return False
5086 
5087     def _assert_dest_node_has_enough_disk(self, context, instance,
5088                                              available_mb, disk_over_commit,
5089                                              block_device_info=None):
5090         """Checks if destination has enough disk for block migration."""
5091         # Libvirt supports qcow2 disk format,which is usually compressed
5092         # on compute nodes.
5093         # Real disk image (compressed) may enlarged to "virtual disk size",
5094         # that is specified as the maximum disk size.
5095         # (See qemu-img -f path-to-disk)
5096         # Scheduler recognizes destination host still has enough disk space
5097         # if real disk size < available disk size
5098         # if disk_over_commit is True,
5099         #  otherwise virtual disk size < available disk size.
5100 
5101         available = 0
5102         if available_mb:
5103             available = available_mb * units.Mi
5104 
5105         ret = self.get_instance_disk_info(instance,
5106                                           block_device_info=block_device_info)
5107         disk_infos = jsonutils.loads(ret)
5108 
5109         necessary = 0
5110         if disk_over_commit:
5111             for info in disk_infos:
5112                 necessary += int(info['disk_size'])
5113         else:
5114             for info in disk_infos:
5115                 necessary += int(info['virt_disk_size'])
5116 
5117         # Check that available disk > necessary disk
5118         if (available - necessary) < 0:
5119             reason = (_('Unable to migrate %(instance_uuid)s: '
5120                         'Disk of instance is too large(available'
5121                         ' on destination host:%(available)s '
5122                         '< need:%(necessary)s)') %
5123                       {'instance_uuid': instance.uuid,
5124                        'available': available,
5125                        'necessary': necessary})
5126             raise exception.MigrationPreCheckError(reason=reason)
5127 
5128     def _compare_cpu(self, guest_cpu, host_cpu_str):
5129         """Check the host is compatible with the requested CPU
5130 
5131         :param guest_cpu: nova.objects.VirtCPUModel or None
5132         :param host_cpu_str: JSON from _get_cpu_info() method
5133 
5134         If the 'guest_cpu' parameter is not None, this will be
5135         validated for migration compatibility with the host.
5136         Otherwise the 'host_cpu_str' JSON string will be used for
5137         validation.
5138 
5139         :returns:
5140             None. if given cpu info is not compatible to this server,
5141             raise exception.
5142         """
5143 
5144         # NOTE(berendt): virConnectCompareCPU not working for Xen
5145         if CONF.libvirt.virt_type not in ['qemu', 'kvm']:
5146             return
5147 
5148         if guest_cpu is None:
5149             info = jsonutils.loads(host_cpu_str)
5150             LOG.info(_LI('Instance launched has CPU info: %s'), host_cpu_str)
5151             cpu = vconfig.LibvirtConfigCPU()
5152             cpu.arch = info['arch']
5153             cpu.model = info['model']
5154             cpu.vendor = info['vendor']
5155             cpu.sockets = info['topology']['sockets']
5156             cpu.cores = info['topology']['cores']
5157             cpu.threads = info['topology']['threads']
5158             for f in info['features']:
5159                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
5160         else:
5161             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
5162 
5163         u = "http://libvirt.org/html/libvirt-libvirt.html#virCPUCompareResult"
5164         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
5165         # unknown character exists in xml, then libvirt complains
5166         try:
5167             ret = self._host.compare_cpu(cpu.to_xml())
5168         except libvirt.libvirtError as e:
5169             error_code = e.get_error_code()
5170             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5171                 LOG.debug("URI %(uri)s does not support cpu comparison. "
5172                           "It will be proceeded though. Error: %(error)s",
5173                           {'uri': self._uri(), 'error': e})
5174                 return
5175             else:
5176                 LOG.error(m, {'ret': e, 'u': u})
5177                 raise exception.MigrationPreCheckError(
5178                     reason=m % {'ret': e, 'u': u})
5179 
5180         if ret <= 0:
5181             LOG.error(m, {'ret': ret, 'u': u})
5182             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
5183 
5184     def _create_shared_storage_test_file(self):
5185         """Makes tmpfile under CONF.instances_path."""
5186         dirpath = CONF.instances_path
5187         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
5188         LOG.debug("Creating tmpfile %s to notify to other "
5189                   "compute nodes that they should mount "
5190                   "the same storage.", tmp_file)
5191         os.close(fd)
5192         return os.path.basename(tmp_file)
5193 
5194     def _check_shared_storage_test_file(self, filename):
5195         """Confirms existence of the tmpfile under CONF.instances_path.
5196 
5197         Cannot confirm tmpfile return False.
5198         """
5199         tmp_file = os.path.join(CONF.instances_path, filename)
5200         if not os.path.exists(tmp_file):
5201             return False
5202         else:
5203             return True
5204 
5205     def _cleanup_shared_storage_test_file(self, filename):
5206         """Removes existence of the tmpfile under CONF.instances_path."""
5207         tmp_file = os.path.join(CONF.instances_path, filename)
5208         os.remove(tmp_file)
5209 
5210     def ensure_filtering_rules_for_instance(self, instance, network_info):
5211         """Ensure that an instance's filtering rules are enabled.
5212 
5213         When migrating an instance, we need the filtering rules to
5214         be configured on the destination host before starting the
5215         migration.
5216 
5217         Also, when restarting the compute service, we need to ensure
5218         that filtering rules exist for all running services.
5219         """
5220 
5221         self.firewall_driver.setup_basic_filtering(instance, network_info)
5222         self.firewall_driver.prepare_instance_filter(instance,
5223                 network_info)
5224 
5225         # nwfilters may be defined in a separate thread in the case
5226         # of libvirt non-blocking mode, so we wait for completion
5227         timeout_count = list(range(CONF.live_migration_retry_count))
5228         while timeout_count:
5229             if self.firewall_driver.instance_filter_exists(instance,
5230                                                            network_info):
5231                 break
5232             timeout_count.pop()
5233             if len(timeout_count) == 0:
5234                 msg = _('The firewall filter for %s does not exist')
5235                 raise exception.NovaException(msg % instance.name)
5236             greenthread.sleep(1)
5237 
5238     def filter_defer_apply_on(self):
5239         self.firewall_driver.filter_defer_apply_on()
5240 
5241     def filter_defer_apply_off(self):
5242         self.firewall_driver.filter_defer_apply_off()
5243 
5244     def live_migration(self, context, instance, dest,
5245                        post_method, recover_method, block_migration=False,
5246                        migrate_data=None):
5247         """Spawning live_migration operation for distributing high-load.
5248 
5249         :param context: security context
5250         :param instance:
5251             nova.db.sqlalchemy.models.Instance object
5252             instance object that is migrated.
5253         :param dest: destination host
5254         :param post_method:
5255             post operation method.
5256             expected nova.compute.manager._post_live_migration.
5257         :param recover_method:
5258             recovery method when any exception occurs.
5259             expected nova.compute.manager._rollback_live_migration.
5260         :param block_migration: if true, do block migration.
5261         :param migrate_data: implementation specific params
5262 
5263         """
5264 
5265         # 'dest' will be substituted into 'migration_uri' so ensure
5266         # it does't contain any characters that could be used to
5267         # exploit the URI accepted by libivrt
5268         if not libvirt_utils.is_valid_hostname(dest):
5269             raise exception.InvalidHostname(hostname=dest)
5270 
5271         utils.spawn(self._live_migration, context, instance, dest,
5272                           post_method, recover_method, block_migration,
5273                           migrate_data)
5274 
5275     def _update_xml(self, xml_str, volume, listen_addrs):
5276         xml_doc = etree.fromstring(xml_str)
5277 
5278         if volume:
5279             xml_doc = self._update_volume_xml(xml_doc, volume)
5280         if listen_addrs:
5281             xml_doc = self._update_graphics_xml(xml_doc, listen_addrs)
5282         else:
5283             self._check_graphics_addresses_can_live_migrate(listen_addrs)
5284 
5285         return etree.tostring(xml_doc)
5286 
5287     def _update_graphics_xml(self, xml_doc, listen_addrs):
5288 
5289         # change over listen addresses
5290         for dev in xml_doc.findall('./devices/graphics'):
5291             gr_type = dev.get('type')
5292             listen_tag = dev.find('listen')
5293             if gr_type in ('vnc', 'spice'):
5294                 if listen_tag is not None:
5295                     listen_tag.set('address', listen_addrs[gr_type])
5296                 if dev.get('listen') is not None:
5297                     dev.set('listen', listen_addrs[gr_type])
5298 
5299         return xml_doc
5300 
5301     def _update_volume_xml(self, xml_doc, volume):
5302         """Update XML using device information of destination host."""
5303 
5304         # Update volume xml
5305         parser = etree.XMLParser(remove_blank_text=True)
5306         disk_nodes = xml_doc.findall('./devices/disk')
5307         for pos, disk_dev in enumerate(disk_nodes):
5308             serial_source = disk_dev.findtext('serial')
5309             if serial_source is None or volume.get(serial_source) is None:
5310                 continue
5311 
5312             if ('connection_info' not in volume[serial_source] or
5313                     'disk_info' not in volume[serial_source]):
5314                 continue
5315 
5316             conf = self._get_volume_config(
5317                 volume[serial_source]['connection_info'],
5318                 volume[serial_source]['disk_info'])
5319             xml_doc2 = etree.XML(conf.to_xml(), parser)
5320             serial_dest = xml_doc2.findtext('serial')
5321 
5322             # Compare source serial and destination serial number.
5323             # If these serial numbers match, continue the process.
5324             if (serial_dest and (serial_source == serial_dest)):
5325                 LOG.debug("Find same serial number: pos=%(pos)s, "
5326                           "serial=%(num)s",
5327                           {'pos': pos, 'num': serial_source})
5328                 for cnt, item_src in enumerate(disk_dev):
5329                     # If source and destination have same item, update
5330                     # the item using destination value.
5331                     for item_dst in xml_doc2.findall(item_src.tag):
5332                         disk_dev.remove(item_src)
5333                         item_dst.tail = None
5334                         disk_dev.insert(cnt, item_dst)
5335 
5336                 # If destination has additional items, thses items should be
5337                 # added here.
5338                 for item_dst in list(xml_doc2):
5339                     item_dst.tail = None
5340                     disk_dev.insert(cnt, item_dst)
5341 
5342         return xml_doc
5343 
5344     def _check_graphics_addresses_can_live_migrate(self, listen_addrs):
5345         LOCAL_ADDRS = ('0.0.0.0', '127.0.0.1', '::', '::1')
5346 
5347         local_vnc = CONF.vnc.vncserver_listen in LOCAL_ADDRS
5348         local_spice = CONF.spice.server_listen in LOCAL_ADDRS
5349 
5350         if ((CONF.vnc.enabled and not local_vnc) or
5351             (CONF.spice.enabled and not local_spice)):
5352 
5353             msg = _('Your libvirt version does not support the'
5354                     ' VIR_DOMAIN_XML_MIGRATABLE flag or your'
5355                     ' destination node does not support'
5356                     ' retrieving listen addresses.  In order'
5357                     ' for live migration to work properly, you'
5358                     ' must configure the graphics (VNC and/or'
5359                     ' SPICE) listen addresses to be either'
5360                     ' the catch-all address (0.0.0.0 or ::) or'
5361                     ' the local address (127.0.0.1 or ::1).')
5362             raise exception.MigrationError(reason=msg)
5363 
5364         if listen_addrs is not None:
5365             dest_local_vnc = listen_addrs['vnc'] in LOCAL_ADDRS
5366             dest_local_spice = listen_addrs['spice'] in LOCAL_ADDRS
5367 
5368             if ((CONF.vnc.enabled and not dest_local_vnc) or
5369                 (CONF.spice.enabled and not dest_local_spice)):
5370 
5371                 LOG.warn(_LW('Your libvirt version does not support the'
5372                              ' VIR_DOMAIN_XML_MIGRATABLE flag, and the '
5373                              ' graphics (VNC and/or SPICE) listen'
5374                              ' addresses on the destination node do not'
5375                              ' match the addresses on the source node.'
5376                              ' Since the source node has listen'
5377                              ' addresses set to either the catch-all'
5378                              ' address (0.0.0.0 or ::) or the local'
5379                              ' address (127.0.0.1 or ::1), the live'
5380                              ' migration will succeed, but the VM will'
5381                              ' continue to listen on the current'
5382                              ' addresses.'))
5383 
5384     def _live_migration_operation(self, context, instance, dest,
5385                                   block_migration, migrate_data, dom):
5386         """Invoke the live migration operation
5387 
5388         :param context: security context
5389         :param instance:
5390             nova.db.sqlalchemy.models.Instance object
5391             instance object that is migrated.
5392         :param dest: destination host
5393         :param block_migration: if true, do block migration.
5394         :param migrate_data: implementation specific params
5395         :param dom: the libvirt domain object
5396 
5397         This method is intended to be run in a background thread and will
5398         block that thread until the migration is finished or failed.
5399         """
5400         # TODO(sahid): Should pass a guest to this method.
5401         guest = libvirt_guest.Guest(dom)
5402 
5403         try:
5404             if block_migration:
5405                 flaglist = CONF.libvirt.block_migration_flag.split(',')
5406             else:
5407                 flaglist = CONF.libvirt.live_migration_flag.split(',')
5408             flagvals = [getattr(libvirt, x.strip()) for x in flaglist]
5409             logical_sum = reduce(lambda x, y: x | y, flagvals)
5410 
5411             pre_live_migrate_data = (migrate_data or {}).get(
5412                                         'pre_live_migration_result', {})
5413             listen_addrs = pre_live_migrate_data.get('graphics_listen_addrs')
5414             volume = pre_live_migrate_data.get('volume')
5415 
5416             migratable_flag = getattr(libvirt, 'VIR_DOMAIN_XML_MIGRATABLE',
5417                                       None)
5418 
5419             if (migratable_flag is None or
5420                     (listen_addrs is None and not volume)):
5421                 self._check_graphics_addresses_can_live_migrate(listen_addrs)
5422                 dom.migrateToURI(CONF.libvirt.live_migration_uri % dest,
5423                                  logical_sum,
5424                                  None,
5425                                  CONF.libvirt.live_migration_bandwidth)
5426             else:
5427                 old_xml_str = guest.get_xml_desc(dump_migratable=True)
5428                 new_xml_str = self._update_xml(old_xml_str,
5429                                                volume,
5430                                                listen_addrs)
5431                 try:
5432                     dom.migrateToURI2(CONF.libvirt.live_migration_uri % dest,
5433                                       None,
5434                                       new_xml_str,
5435                                       logical_sum,
5436                                       None,
5437                                       CONF.libvirt.live_migration_bandwidth)
5438                 except libvirt.libvirtError as ex:
5439                     # NOTE(mriedem): There is a bug in older versions of
5440                     # libvirt where the VIR_DOMAIN_XML_MIGRATABLE flag causes
5441                     # virDomainDefCheckABIStability to not compare the source
5442                     # and target domain xml's correctly for the CPU model.
5443                     # We try to handle that error here and attempt the legacy
5444                     # migrateToURI path, which could fail if the console
5445                     # addresses are not correct, but in that case we have the
5446                     # _check_graphics_addresses_can_live_migrate check in place
5447                     # to catch it.
5448                     # TODO(mriedem): Remove this workaround when
5449                     # Red Hat BZ #1141838 is closed.
5450                     error_code = ex.get_error_code()
5451                     if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
5452                         LOG.warn(_LW('An error occurred trying to live '
5453                                      'migrate. Falling back to legacy live '
5454                                      'migrate flow. Error: %s'), ex,
5455                                  instance=instance)
5456                         self._check_graphics_addresses_can_live_migrate(
5457                             listen_addrs)
5458                         dom.migrateToURI(
5459                             CONF.libvirt.live_migration_uri % dest,
5460                             logical_sum,
5461                             None,
5462                             CONF.libvirt.live_migration_bandwidth)
5463                     else:
5464                         raise
5465         except Exception as e:
5466             with excutils.save_and_reraise_exception():
5467                 LOG.error(_LE("Live Migration failure: %s"), e,
5468                           instance=instance)
5469 
5470         # If 'migrateToURI' fails we don't know what state the
5471         # VM instances on each host are in. Possibilities include
5472         #
5473         #  1. src==running, dst==none
5474         #
5475         #     Migration failed & rolled back, or never started
5476         #
5477         #  2. src==running, dst==paused
5478         #
5479         #     Migration started but is still ongoing
5480         #
5481         #  3. src==paused,  dst==paused
5482         #
5483         #     Migration data transfer completed, but switchover
5484         #     is still ongoing, or failed
5485         #
5486         #  4. src==paused,  dst==running
5487         #
5488         #     Migration data transfer completed, switchover
5489         #     happened but cleanup on source failed
5490         #
5491         #  5. src==none,    dst==running
5492         #
5493         #     Migration fully succeeded.
5494         #
5495         # Libvirt will aim to complete any migration operation
5496         # or roll it back. So even if the migrateToURI call has
5497         # returned an error, if the migration was not finished
5498         # libvirt should clean up.
5499         #
5500         # So we take the error raise here with a pinch of salt
5501         # and rely on the domain job info status to figure out
5502         # what really happened to the VM, which is a much more
5503         # reliable indicator.
5504         #
5505         # In particular we need to try very hard to ensure that
5506         # Nova does not "forget" about the guest. ie leaving it
5507         # running on a different host to the one recorded in
5508         # the database, as that would be a serious resource leak
5509 
5510         LOG.debug("Migration operation thread has finished",
5511                   instance=instance)
5512 
5513     def _live_migration_monitor(self, context, instance, dest, post_method,
5514                                 recover_method, block_migration,
5515                                 migrate_data, dom, finish_event):
5516         n = 0
5517         while True:
5518             info = host.DomainJobInfo.for_domain(dom)
5519 
5520             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
5521                 # Annoyingly this could indicate many possible
5522                 # states, so we must fix the mess:
5523                 #
5524                 #   1. Migration has not yet begun
5525                 #   2. Migration has stopped due to failure
5526                 #   3. Migration has stopped due to completion
5527                 #
5528                 # We can detect option 1 by seeing if thread is still
5529                 # running. We can distinguish 2 vs 3 by seeing if the
5530                 # VM still exists & running on the current host
5531                 #
5532                 if not finish_event.ready():
5533                     LOG.debug("Operation thread is still running",
5534                               instance=instance)
5535                     # Leave type untouched
5536                 else:
5537                     try:
5538                         if dom.isActive():
5539                             LOG.debug("VM running on src, migration failed",
5540                                       instance=instance)
5541                             info.type = libvirt.VIR_DOMAIN_JOB_FAILED
5542                         else:
5543                             LOG.debug("VM is shutoff, migration finished",
5544                                       instance=instance)
5545                             info.type = libvirt.VIR_DOMAIN_JOB_COMPLETED
5546                     except libvirt.libvirtError as ex:
5547                         LOG.debug("Error checking domain status %(ex)s",
5548                                   ex, instance=instance)
5549                         if ex.get_error_code() == libvirt.VIR_ERR_NO_DOMAIN:
5550                             LOG.debug("VM is missing, migration finished",
5551                                       instance=instance)
5552                             info.type = libvirt.VIR_DOMAIN_JOB_COMPLETED
5553                         else:
5554                             LOG.info(_LI("Error %(ex)s, migration failed"),
5555                                      instance=instance)
5556                             info.type = libvirt.VIR_DOMAIN_JOB_FAILED
5557 
5558                 if info.type != libvirt.VIR_DOMAIN_JOB_NONE:
5559                     LOG.debug("Fixed incorrect job type to be %d",
5560                               info.type, instance=instance)
5561 
5562             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
5563                 # Migration is not yet started
5564                 LOG.debug("Migration not running yet",
5565                           instance=instance)
5566             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
5567                 # We loop every 500ms, so don't log on every
5568                 # iteration to avoid spamming logs for long
5569                 # running migrations. Just once every 5 secs
5570                 # is sufficient for developers to debug problems.
5571                 # We log once every 30 seconds at info to help
5572                 # admins see slow running migration operations
5573                 # when debug logs are off.
5574                 if (n % 10) == 0:
5575                     # Ignoring memory_processed, as due to repeated
5576                     # dirtying of data, this can be way larger than
5577                     # memory_total. Best to just look at what's
5578                     # remaining to copy and ignore what's done already
5579                     #
5580                     # TODO(berrange) perhaps we could include disk
5581                     # transfer stats in the progress too, but it
5582                     # might make memory info more obscure as large
5583                     # disk sizes might dwarf memory size
5584                     remaining = 100
5585                     if info.memory_total != 0:
5586                         remaining = round(info.memory_remaining *
5587                                           100 / info.memory_total)
5588                     instance.progress = 100 - remaining
5589                     instance.save()
5590 
5591                     lg = LOG.debug
5592                     if (n % 60) == 0:
5593                         lg = LOG.info
5594 
5595                     lg(_LI("Migration running for %(secs)d secs, "
5596                            "memory %(remaining)d%% remaining; "
5597                            "(bytes processed=%(processed_memory)d, "
5598                            "remaining=%(remaining_memory)d, "
5599                            "total=%(total_memory)d)"),
5600                        {"secs": n / 2, "remaining": remaining,
5601                         "processed_memory": info.memory_processed,
5602                         "remaining_memory": info.memory_remaining,
5603                         "total_memory": info.memory_total}, instance=instance)
5604 
5605                 # Migration is still running
5606                 #
5607                 # This is where we'd wire up calls to change live
5608                 # migration status. eg change max downtime, cancel
5609                 # the operation, change max bandwidth
5610                 n = n + 1
5611             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
5612                 # Migration is all done
5613                 LOG.info(_LI("Migration operation has completed"),
5614                          instance=instance)
5615                 post_method(context, instance, dest, block_migration,
5616                             migrate_data)
5617                 break
5618             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
5619                 # Migration did not succeed
5620                 LOG.error(_LE("Migration operation has aborted"),
5621                           instance=instance)
5622                 recover_method(context, instance, dest, block_migration,
5623                                migrate_data)
5624                 break
5625             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
5626                 # Migration was stopped by admin
5627                 LOG.warn(_LW("Migration operation was cancelled"),
5628                          instance=instance)
5629                 recover_method(context, instance, dest, block_migration,
5630                                migrate_data)
5631                 break
5632             else:
5633                 LOG.warn(_LW("Unexpected migration job type: %d"),
5634                          info.type, instance=instance)
5635 
5636             time.sleep(0.5)
5637 
5638     def _live_migration(self, context, instance, dest, post_method,
5639                         recover_method, block_migration,
5640                         migrate_data):
5641         """Do live migration.
5642 
5643         :param context: security context
5644         :param instance:
5645             nova.db.sqlalchemy.models.Instance object
5646             instance object that is migrated.
5647         :param dest: destination host
5648         :param post_method:
5649             post operation method.
5650             expected nova.compute.manager._post_live_migration.
5651         :param recover_method:
5652             recovery method when any exception occurs.
5653             expected nova.compute.manager._rollback_live_migration.
5654         :param block_migration: if true, do block migration.
5655         :param migrate_data: implementation specific params
5656 
5657         This fires off a new thread to run the blocking migration
5658         operation, and then this thread monitors the progress of
5659         migration and controls its operation
5660         """
5661 
5662         guest = self._host.get_guest(instance)
5663 
5664         # TODO(sahid): We are converting all calls from a
5665         # virDomain object to use nova.virt.libvirt.Guest.
5666         # We should be able to remove dom at the end.
5667         dom = guest._domain
5668 
5669         opthread = utils.spawn(self._live_migration_operation,
5670                                      context, instance, dest,
5671                                      block_migration,
5672                                      migrate_data, dom)
5673 
5674         finish_event = eventlet.event.Event()
5675 
5676         def thread_finished(thread, event):
5677             LOG.debug("Migration operation thread notification",
5678                       instance=instance)
5679             event.send()
5680         opthread.link(thread_finished, finish_event)
5681 
5682         # Let eventlet schedule the new thread right away
5683         time.sleep(0)
5684 
5685         try:
5686             LOG.debug("Starting monitoring of live migration",
5687                       instance=instance)
5688             self._live_migration_monitor(context, instance, dest,
5689                                          post_method, recover_method,
5690                                          block_migration, migrate_data,
5691                                          dom, finish_event)
5692         except Exception as ex:
5693             LOG.warn(_LW("Error monitoring migration: %(ex)s"),
5694                      {"ex": ex}, instance=instance, exc_info=True)
5695             raise
5696         finally:
5697             LOG.debug("Live migration monitoring is all done",
5698                       instance=instance)
5699 
5700     def _try_fetch_image(self, context, path, image_id, instance,
5701                          fallback_from_host=None):
5702         try:
5703             libvirt_utils.fetch_image(context, path,
5704                                       image_id,
5705                                       instance.user_id,
5706                                       instance.project_id)
5707         except exception.ImageNotFound:
5708             if not fallback_from_host:
5709                 raise
5710             LOG.debug("Image %(image_id)s doesn't exist anymore on "
5711                       "image service, attempting to copy image "
5712                       "from %(host)s",
5713                       {'image_id': image_id, 'host': fallback_from_host})
5714             libvirt_utils.copy_image(src=path, dest=path,
5715                                      host=fallback_from_host,
5716                                      receive=True)
5717 
5718     def _fetch_instance_kernel_ramdisk(self, context, instance,
5719                                        fallback_from_host=None):
5720         """Download kernel and ramdisk for instance in instance directory."""
5721         instance_dir = libvirt_utils.get_instance_path(instance)
5722         if instance.kernel_id:
5723             kernel_path = os.path.join(instance_dir, 'kernel')
5724             # NOTE(dsanders): only fetch image if it's not available at
5725             # kernel_path. This also avoids ImageNotFound exception if
5726             # the image has been deleted from glance
5727             if not os.path.exists(kernel_path):
5728                 self._try_fetch_image(context,
5729                                       kernel_path,
5730                                       instance.kernel_id,
5731                                       instance, fallback_from_host)
5732             if instance.ramdisk_id:
5733                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
5734                 # NOTE(dsanders): only fetch image if it's not available at
5735                 # ramdisk_path. This also avoids ImageNotFound exception if
5736                 # the image has been deleted from glance
5737                 if not os.path.exists(ramdisk_path):
5738                     self._try_fetch_image(context,
5739                                           ramdisk_path,
5740                                           instance.ramdisk_id,
5741                                           instance, fallback_from_host)
5742 
5743     def rollback_live_migration_at_destination(self, context, instance,
5744                                                network_info,
5745                                                block_device_info,
5746                                                destroy_disks=True,
5747                                                migrate_data=None):
5748         """Clean up destination node after a failed live migration."""
5749         try:
5750             self.destroy(context, instance, network_info, block_device_info,
5751                          destroy_disks, migrate_data)
5752         finally:
5753             # NOTE(gcb): Failed block live migration may leave instance
5754             # directory at destination node, ensure it is always deleted.
5755             is_shared_instance_path = True
5756             if migrate_data:
5757                 is_shared_instance_path = migrate_data.get(
5758                         'is_shared_instance_path', True)
5759             if not is_shared_instance_path:
5760                 instance_dir = libvirt_utils.get_instance_path_at_destination(
5761                                 instance, migrate_data)
5762                 if os.path.exists(instance_dir):
5763                         shutil.rmtree(instance_dir)
5764 
5765     def pre_live_migration(self, context, instance, block_device_info,
5766                            network_info, disk_info, migrate_data=None):
5767         """Preparation live migration."""
5768         if disk_info is not None:
5769             disk_info = jsonutils.loads(disk_info)
5770 
5771         # Steps for volume backed instance live migration w/o shared storage.
5772         is_shared_block_storage = True
5773         is_shared_instance_path = True
5774         is_block_migration = True
5775         if migrate_data:
5776             LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
5777                       instance=instance)
5778             is_shared_block_storage = migrate_data.get(
5779                     'is_shared_block_storage', True)
5780             is_shared_instance_path = migrate_data.get(
5781                     'is_shared_instance_path', True)
5782             is_block_migration = migrate_data.get('block_migration', True)
5783 
5784         image_meta = utils.get_image_from_system_metadata(
5785             instance.system_metadata)
5786 
5787         if not (is_shared_instance_path and is_shared_block_storage):
5788             # NOTE(dims): Using config drive with iso format does not work
5789             # because of a bug in libvirt with read only devices. However
5790             # one can use vfat as config_drive_format which works fine.
5791             # Please see bug/1246201 for details on the libvirt bug.
5792             if CONF.config_drive_format != 'vfat':
5793                 if configdrive.required_by(instance):
5794                     raise exception.NoLiveMigrationForConfigDriveInLibVirt()
5795 
5796         if not is_shared_instance_path:
5797             instance_dir = libvirt_utils.get_instance_path_at_destination(
5798                             instance, migrate_data)
5799 
5800             if os.path.exists(instance_dir):
5801                 raise exception.DestinationDiskExists(path=instance_dir)
5802 
5803             LOG.debug('Creating instance directory: %s', instance_dir,
5804                       instance=instance)
5805             os.mkdir(instance_dir)
5806 
5807             if not is_shared_block_storage:
5808                 # Ensure images and backing files are present.
5809                 LOG.debug('Checking to make sure images and backing files are '
5810                           'present before live migration.', instance=instance)
5811                 self._create_images_and_backing(
5812                     context, instance, instance_dir, disk_info,
5813                     fallback_from_host=instance.host)
5814 
5815         if not (is_block_migration or is_shared_instance_path):
5816             # NOTE(angdraug): when block storage is shared between source and
5817             # destination and instance path isn't (e.g. volume backed or rbd
5818             # backed instance), instance path on destination has to be prepared
5819 
5820             # Touch the console.log file, required by libvirt.
5821             console_file = self._get_console_log_path(instance)
5822             LOG.debug('Touch instance console log: %s', console_file,
5823                       instance=instance)
5824             libvirt_utils.file_open(console_file, 'a').close()
5825 
5826             # if image has kernel and ramdisk, just download
5827             # following normal way.
5828             self._fetch_instance_kernel_ramdisk(context, instance)
5829 
5830         # Establishing connection to volume server.
5831         block_device_mapping = driver.block_device_info_get_mapping(
5832             block_device_info)
5833 
5834         if len(block_device_mapping):
5835             LOG.debug('Connecting volumes before live migration.',
5836                       instance=instance)
5837 
5838         for vol in block_device_mapping:
5839             connection_info = vol['connection_info']
5840             disk_info = blockinfo.get_info_from_bdm(
5841                 CONF.libvirt.virt_type, image_meta, vol)
5842             self._connect_volume(connection_info, disk_info)
5843 
5844         if is_block_migration and len(block_device_mapping):
5845             # NOTE(stpierre): if this instance has mapped volumes,
5846             # we can't do a block migration, since that will
5847             # result in volumes being copied from themselves to
5848             # themselves, which is a recipe for disaster.
5849             LOG.error(
5850                 _LE('Cannot block migrate instance %s with mapped volumes'),
5851                 instance.uuid)
5852             msg = (_('Cannot block migrate instance %s with mapped volumes') %
5853                    instance.uuid)
5854             raise exception.MigrationError(reason=msg)
5855 
5856         # We call plug_vifs before the compute manager calls
5857         # ensure_filtering_rules_for_instance, to ensure bridge is set up
5858         # Retry operation is necessary because continuously request comes,
5859         # concurrent request occurs to iptables, then it complains.
5860         LOG.debug('Plugging VIFs before live migration.', instance=instance)
5861         max_retry = CONF.live_migration_retry_count
5862         for cnt in range(max_retry):
5863             try:
5864                 self.plug_vifs(instance, network_info)
5865                 break
5866             except processutils.ProcessExecutionError:
5867                 if cnt == max_retry - 1:
5868                     raise
5869                 else:
5870                     LOG.warn(_LW('plug_vifs() failed %(cnt)d. Retry up to '
5871                                  '%(max_retry)d.'),
5872                              {'cnt': cnt,
5873                               'max_retry': max_retry},
5874                              instance=instance)
5875                     greenthread.sleep(1)
5876 
5877         # Store vncserver_listen and latest disk device info
5878         res_data = {'graphics_listen_addrs': {}, 'volume': {}}
5879         res_data['graphics_listen_addrs']['vnc'] = CONF.vnc.vncserver_listen
5880         res_data['graphics_listen_addrs']['spice'] = CONF.spice.server_listen
5881         for vol in block_device_mapping:
5882             connection_info = vol['connection_info']
5883             if connection_info.get('serial'):
5884                 serial = connection_info['serial']
5885                 res_data['volume'][serial] = {'connection_info': {},
5886                                               'disk_info': {}}
5887                 res_data['volume'][serial]['connection_info'] = \
5888                     connection_info
5889                 disk_info = blockinfo.get_info_from_bdm(
5890                     CONF.libvirt.virt_type, image_meta, vol)
5891                 res_data['volume'][serial]['disk_info'] = disk_info
5892 
5893         return res_data
5894 
5895     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
5896                                image_id, instance, size,
5897                                fallback_from_host=None):
5898         try:
5899             image.cache(fetch_func=fetch_func,
5900                         context=context,
5901                         filename=filename,
5902                         image_id=image_id,
5903                         user_id=instance.user_id,
5904                         project_id=instance.project_id,
5905                         size=size)
5906         except exception.ImageNotFound:
5907             if not fallback_from_host:
5908                 raise
5909             LOG.debug("Image %(image_id)s doesn't exist anymore "
5910                       "on image service, attempting to copy "
5911                       "image from %(host)s",
5912                       {'image_id': image_id, 'host': fallback_from_host})
5913 
5914             def copy_from_host(target, max_size):
5915                 libvirt_utils.copy_image(src=target,
5916                                          dest=target,
5917                                          host=fallback_from_host,
5918                                          receive=True)
5919             image.cache(fetch_func=copy_from_host,
5920                         filename=filename)
5921 
5922     def _create_images_and_backing(self, context, instance, instance_dir,
5923                                    disk_info, fallback_from_host=None):
5924         """:param context: security context
5925            :param instance:
5926                nova.db.sqlalchemy.models.Instance object
5927                instance object that is migrated.
5928            :param instance_dir:
5929                instance path to use, calculated externally to handle block
5930                migrating an instance with an old style instance path
5931            :param disk_info:
5932                disk info specified in _get_instance_disk_info
5933            :param fallback_from_host:
5934                host where we can retrieve images if the glance images are
5935                not available.
5936 
5937         """
5938         if not disk_info:
5939             disk_info = []
5940 
5941         for info in disk_info:
5942             base = os.path.basename(info['path'])
5943             # Get image type and create empty disk image, and
5944             # create backing file in case of qcow2.
5945             instance_disk = os.path.join(instance_dir, base)
5946             if not info['backing_file'] and not os.path.exists(instance_disk):
5947                 libvirt_utils.create_image(info['type'], instance_disk,
5948                                            info['virt_disk_size'])
5949             elif info['backing_file']:
5950                 # Creating backing file follows same way as spawning instances.
5951                 cache_name = os.path.basename(info['backing_file'])
5952 
5953                 image = self.image_backend.image(instance,
5954                                                  instance_disk,
5955                                                  CONF.libvirt.images_type)
5956                 if cache_name.startswith('ephemeral'):
5957                     image.cache(fetch_func=self._create_ephemeral,
5958                                 fs_label=cache_name,
5959                                 os_type=instance.os_type,
5960                                 filename=cache_name,
5961                                 size=info['virt_disk_size'],
5962                                 ephemeral_size=instance.ephemeral_gb)
5963                 elif cache_name.startswith('swap'):
5964                     inst_type = instance.get_flavor()
5965                     swap_mb = inst_type.swap
5966                     image.cache(fetch_func=self._create_swap,
5967                                 filename="swap_%s" % swap_mb,
5968                                 size=swap_mb * units.Mi,
5969                                 swap_mb=swap_mb)
5970                 else:
5971                     self._try_fetch_image_cache(image,
5972                                                 libvirt_utils.fetch_image,
5973                                                 context, cache_name,
5974                                                 instance.image_ref,
5975                                                 instance,
5976                                                 info['virt_disk_size'],
5977                                                 fallback_from_host)
5978 
5979         # if image has kernel and ramdisk, just download
5980         # following normal way.
5981         self._fetch_instance_kernel_ramdisk(
5982             context, instance, fallback_from_host=fallback_from_host)
5983 
5984     def post_live_migration(self, context, instance, block_device_info,
5985                             migrate_data=None):
5986         # Disconnect from volume server
5987         block_device_mapping = driver.block_device_info_get_mapping(
5988                 block_device_info)
5989         for vol in block_device_mapping:
5990             connection_info = vol['connection_info']
5991             disk_dev = vol['mount_device'].rpartition("/")[2]
5992             self._disconnect_volume(connection_info, disk_dev)
5993 
5994     def post_live_migration_at_source(self, context, instance, network_info):
5995         """Unplug VIFs from networks at source.
5996 
5997         :param context: security context
5998         :param instance: instance object reference
5999         :param network_info: instance network information
6000         """
6001         self.unplug_vifs(instance, network_info)
6002 
6003     def post_live_migration_at_destination(self, context,
6004                                            instance,
6005                                            network_info,
6006                                            block_migration=False,
6007                                            block_device_info=None):
6008         """Post operation of live migration at destination host.
6009 
6010         :param context: security context
6011         :param instance:
6012             nova.db.sqlalchemy.models.Instance object
6013             instance object that is migrated.
6014         :param network_info: instance network information
6015         :param block_migration: if true, post operation of block_migration.
6016         """
6017         # Define migrated instance, otherwise, suspend/destroy does not work.
6018         image_meta = utils.get_image_from_system_metadata(
6019             instance.system_metadata)
6020         # In case of block migration, destination does not have
6021         # libvirt.xml
6022         disk_info = blockinfo.get_disk_info(
6023             CONF.libvirt.virt_type, instance,
6024             image_meta, block_device_info)
6025         xml = self._get_guest_xml(context, instance,
6026                                   network_info, disk_info,
6027                                   image_meta,
6028                                   block_device_info=block_device_info,
6029                                   write_to_disk=True)
6030         self._host.write_instance_config(xml)
6031 
6032     def _get_instance_disk_info(self, instance_name, xml,
6033                                 block_device_info=None):
6034         block_device_mapping = driver.block_device_info_get_mapping(
6035             block_device_info)
6036 
6037         volume_devices = set()
6038         for vol in block_device_mapping:
6039             disk_dev = vol['mount_device'].rpartition("/")[2]
6040             volume_devices.add(disk_dev)
6041 
6042         disk_info = []
6043         doc = etree.fromstring(xml)
6044         disk_nodes = doc.findall('.//devices/disk')
6045         path_nodes = doc.findall('.//devices/disk/source')
6046         driver_nodes = doc.findall('.//devices/disk/driver')
6047         target_nodes = doc.findall('.//devices/disk/target')
6048 
6049         for cnt, path_node in enumerate(path_nodes):
6050             disk_type = disk_nodes[cnt].get('type')
6051             path = path_node.get('file') or path_node.get('dev')
6052             target = target_nodes[cnt].attrib['dev']
6053 
6054             if not path:
6055                 LOG.debug('skipping disk for %s as it does not have a path',
6056                           instance_name)
6057                 continue
6058 
6059             if disk_type not in ['file', 'block']:
6060                 LOG.debug('skipping disk because it looks like a volume', path)
6061                 continue
6062 
6063             if target in volume_devices:
6064                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
6065                           'volume', {'path': path, 'target': target})
6066                 continue
6067 
6068             # get the real disk size or
6069             # raise a localized error if image is unavailable
6070             if disk_type == 'file':
6071                 dk_size = int(os.path.getsize(path))
6072             elif disk_type == 'block' and block_device_info:
6073                 dk_size = lvm.get_volume_size(path)
6074             else:
6075                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
6076                           'determine if volume',
6077                           {'path': path, 'target': target})
6078                 continue
6079 
6080             disk_type = driver_nodes[cnt].get('type')
6081             if disk_type == "qcow2":
6082                 backing_file = libvirt_utils.get_disk_backing_file(path)
6083                 virt_size = disk.get_disk_size(path)
6084                 over_commit_size = int(virt_size) - dk_size
6085             else:
6086                 backing_file = ""
6087                 virt_size = dk_size
6088                 over_commit_size = 0
6089 
6090             disk_info.append({'type': disk_type,
6091                               'path': path,
6092                               'virt_disk_size': virt_size,
6093                               'backing_file': backing_file,
6094                               'disk_size': dk_size,
6095                               'over_committed_disk_size': over_commit_size})
6096         return disk_info
6097 
6098     def get_instance_disk_info(self, instance,
6099                                block_device_info=None):
6100         try:
6101             guest = self._host.get_guest(instance)
6102             xml = guest.get_xml_desc()
6103         except libvirt.libvirtError as ex:
6104             error_code = ex.get_error_code()
6105             LOG.warn(_LW('Error from libvirt while getting description of '
6106                          '%(instance_name)s: [Error Code %(error_code)s] '
6107                          '%(ex)s'),
6108                      {'instance_name': instance.name,
6109                       'error_code': error_code,
6110                       'ex': ex},
6111                      instance=instance)
6112             raise exception.InstanceNotFound(instance_id=instance.uuid)
6113 
6114         return jsonutils.dumps(
6115                 self._get_instance_disk_info(instance.name, xml,
6116                                              block_device_info))
6117 
6118     def _get_disk_over_committed_size_total(self):
6119         """Return total over committed disk size for all instances."""
6120         # Disk size that all instance uses : virtual_size - disk_size
6121         disk_over_committed_size = 0
6122         for dom in self._host.list_instance_domains():
6123             try:
6124                 # TODO(sahid): list_instance_domain should
6125                 # be renamed as list_guest and so returning
6126                 # Guest objects.
6127                 guest = libvirt_guest.Guest(dom)
6128                 xml = guest.get_xml_desc()
6129 
6130                 disk_infos = self._get_instance_disk_info(guest.name, xml)
6131                 for info in disk_infos:
6132                     disk_over_committed_size += int(
6133                         info['over_committed_disk_size'])
6134             except libvirt.libvirtError as ex:
6135                 error_code = ex.get_error_code()
6136                 LOG.warn(_LW(
6137                     'Error from libvirt while getting description of '
6138                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s'
6139                 ) % {'instance_name': guest.name,
6140                      'error_code': error_code,
6141                      'ex': ex})
6142             except OSError as e:
6143                 if e.errno == errno.ENOENT:
6144                     LOG.warn(_LW('Periodic task is updating the host stat, '
6145                                  'it is trying to get disk %(i_name)s, '
6146                                  'but disk file was removed by concurrent '
6147                                  'operations such as resize.'),
6148                                 {'i_name': guest.name})
6149                 elif e.errno == errno.EACCES:
6150                     LOG.warn(_LW('Periodic task is updating the host stat, '
6151                                  'it is trying to get disk %(i_name)s, '
6152                                  'but access is denied. It is most likely '
6153                                  'due to a VM that exists on the compute '
6154                                  'node but is not managed by Nova.'),
6155                              {'i_name': guest.name})
6156                 else:
6157                     raise
6158             except exception.VolumeBDMPathNotFound as e:
6159                 LOG.warn(_LW('Periodic task is updating the host stats, '
6160                              'it is trying to get disk info for %(i_name)s, '
6161                              'but the backing volume block device was removed '
6162                              'by concurrent operations such as resize. '
6163                              'Error: %(error)s'),
6164                          {'i_name': guest.name,
6165                           'error': e})
6166             # NOTE(gtt116): give other tasks a chance.
6167             greenthread.sleep(0)
6168         return disk_over_committed_size
6169 
6170     def unfilter_instance(self, instance, network_info):
6171         """See comments of same method in firewall_driver."""
6172         self.firewall_driver.unfilter_instance(instance,
6173                                                network_info=network_info)
6174 
6175     def get_available_nodes(self, refresh=False):
6176         return [self._host.get_hostname()]
6177 
6178     def get_host_cpu_stats(self):
6179         """Return the current CPU state of the host."""
6180         return self._host.get_cpu_stats()
6181 
6182     def get_host_uptime(self):
6183         """Returns the result of calling "uptime"."""
6184         out, err = utils.execute('env', 'LANG=C', 'uptime')
6185         return out
6186 
6187     def manage_image_cache(self, context, all_instances):
6188         """Manage the local cache of images."""
6189         self.image_cache_manager.update(context, all_instances)
6190 
6191     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
6192                                   shared_storage=False):
6193         """Used only for cleanup in case migrate_disk_and_power_off fails."""
6194         try:
6195             if os.path.exists(inst_base_resize):
6196                 utils.execute('rm', '-rf', inst_base)
6197                 utils.execute('mv', inst_base_resize, inst_base)
6198                 if not shared_storage:
6199                     utils.ssh_execute(dest, 'rm', '-rf', inst_base)
6200         except Exception:
6201             pass
6202 
6203     def _is_storage_shared_with(self, dest, inst_base):
6204         # NOTE (rmk): There are two methods of determining whether we are
6205         #             on the same filesystem: the source and dest IP are the
6206         #             same, or we create a file on the dest system via SSH
6207         #             and check whether the source system can also see it.
6208         shared_storage = (dest == self.get_host_ip_addr())
6209         if not shared_storage:
6210             tmp_file = uuid.uuid4().hex + '.tmp'
6211             tmp_path = os.path.join(inst_base, tmp_file)
6212 
6213             try:
6214                 utils.ssh_execute(dest, 'touch', tmp_path)
6215                 if os.path.exists(tmp_path):
6216                     shared_storage = True
6217                     os.unlink(tmp_path)
6218                 else:
6219                     utils.ssh_execute(dest, 'rm', tmp_path)
6220             except Exception:
6221                 pass
6222         return shared_storage
6223 
6224     def migrate_disk_and_power_off(self, context, instance, dest,
6225                                    flavor, network_info,
6226                                    block_device_info=None,
6227                                    timeout=0, retry_interval=0):
6228         LOG.debug("Starting migrate_disk_and_power_off",
6229                    instance=instance)
6230 
6231         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
6232 
6233         # get_bdm_ephemeral_disk_size() will return 0 if the new
6234         # instance's requested block device mapping contain no
6235         # ephemeral devices. However, we still want to check if
6236         # the original instance's ephemeral_gb property was set and
6237         # ensure that the new requested flavor ephemeral size is greater
6238         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
6239                     instance.ephemeral_gb)
6240 
6241         # Checks if the migration needs a disk resize down.
6242         root_down = flavor.root_gb < instance.root_gb
6243         ephemeral_down = flavor.ephemeral_gb < eph_size
6244         disk_info_text = self.get_instance_disk_info(
6245             instance, block_device_info=block_device_info)
6246         booted_from_volume = self._is_booted_from_volume(instance,
6247                                                          disk_info_text)
6248         if (root_down and not booted_from_volume) or ephemeral_down:
6249             reason = _("Unable to resize disk down.")
6250             raise exception.InstanceFaultRollback(
6251                 exception.ResizeError(reason=reason))
6252 
6253         disk_info = jsonutils.loads(disk_info_text)
6254 
6255         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
6256         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
6257             reason = _("Migration is not supported for LVM backed instances")
6258             raise exception.InstanceFaultRollback(
6259                 exception.MigrationPreCheckError(reason=reason))
6260 
6261         # copy disks to destination
6262         # rename instance dir to +_resize at first for using
6263         # shared storage for instance dir (eg. NFS).
6264         inst_base = libvirt_utils.get_instance_path(instance)
6265         inst_base_resize = inst_base + "_resize"
6266         shared_storage = self._is_storage_shared_with(dest, inst_base)
6267 
6268         # try to create the directory on the remote compute node
6269         # if this fails we pass the exception up the stack so we can catch
6270         # failures here earlier
6271         if not shared_storage:
6272             try:
6273                 utils.ssh_execute(dest, 'mkdir', '-p', inst_base)
6274             except processutils.ProcessExecutionError as e:
6275                 reason = _("not able to execute ssh command: %s") % e
6276                 raise exception.InstanceFaultRollback(
6277                     exception.ResizeError(reason=reason))
6278 
6279         self.power_off(instance, timeout, retry_interval)
6280 
6281         block_device_mapping = driver.block_device_info_get_mapping(
6282             block_device_info)
6283         for vol in block_device_mapping:
6284             connection_info = vol['connection_info']
6285             disk_dev = vol['mount_device'].rpartition("/")[2]
6286             self._disconnect_volume(connection_info, disk_dev)
6287 
6288         try:
6289             utils.execute('mv', inst_base, inst_base_resize)
6290             # if we are migrating the instance with shared storage then
6291             # create the directory.  If it is a remote node the directory
6292             # has already been created
6293             if shared_storage:
6294                 dest = None
6295                 utils.execute('mkdir', '-p', inst_base)
6296 
6297             active_flavor = instance.get_flavor()
6298             for info in disk_info:
6299                 # assume inst_base == dirname(info['path'])
6300                 img_path = info['path']
6301                 fname = os.path.basename(img_path)
6302                 from_path = os.path.join(inst_base_resize, fname)
6303 
6304                 if (fname == 'disk.swap' and
6305                     active_flavor.get('swap', 0) != flavor.get('swap', 0)):
6306                     # To properly resize the swap partition, it must be
6307                     # re-created with the proper size.  This is acceptable
6308                     # because when an OS is shut down, the contents of the
6309                     # swap space are just garbage, the OS doesn't bother about
6310                     # what is in it.
6311 
6312                     # We will not copy over the swap disk here, and rely on
6313                     # finish_migration/_create_image to re-create it for us.
6314                     continue
6315 
6316                 if info['type'] == 'qcow2' and info['backing_file']:
6317                     tmp_path = from_path + "_rbase"
6318                     # merge backing file
6319                     utils.execute('qemu-img', 'convert', '-f', 'qcow2',
6320                                   '-O', 'qcow2', from_path, tmp_path)
6321 
6322                     if shared_storage:
6323                         utils.execute('mv', tmp_path, img_path)
6324                     else:
6325                         libvirt_utils.copy_image(tmp_path, img_path, host=dest)
6326                         utils.execute('rm', '-f', tmp_path)
6327 
6328                 else:  # raw or qcow2 with no backing file
6329                     libvirt_utils.copy_image(from_path, img_path, host=dest)
6330         except Exception:
6331             with excutils.save_and_reraise_exception():
6332                 self._cleanup_remote_migration(dest, inst_base,
6333                                                inst_base_resize,
6334                                                shared_storage)
6335 
6336         return disk_info_text
6337 
6338     def _wait_for_running(self, instance):
6339         state = self.get_info(instance).state
6340 
6341         if state == power_state.RUNNING:
6342             LOG.info(_LI("Instance running successfully."), instance=instance)
6343             raise loopingcall.LoopingCallDone()
6344 
6345     @staticmethod
6346     def _disk_size_from_instance(instance, info):
6347         """Determines the disk size from instance properties
6348 
6349         Returns the disk size by using the disk name to determine whether it
6350         is a root or an ephemeral disk, then by checking properties of the
6351         instance returns the size converted to bytes.
6352 
6353         Returns 0 if the disk name not match (disk, disk.local).
6354         """
6355         fname = os.path.basename(info['path'])
6356         if fname == 'disk':
6357             size = instance.root_gb
6358         elif fname == 'disk.local':
6359             size = instance.ephemeral_gb
6360         else:
6361             size = 0
6362         return size * units.Gi
6363 
6364     @staticmethod
6365     def _disk_raw_to_qcow2(path):
6366         """Converts a raw disk to qcow2."""
6367         path_qcow = path + '_qcow'
6368         utils.execute('qemu-img', 'convert', '-f', 'raw',
6369                       '-O', 'qcow2', path, path_qcow)
6370         utils.execute('mv', path_qcow, path)
6371 
6372     @staticmethod
6373     def _disk_qcow2_to_raw(path):
6374         """Converts a qcow2 disk to raw."""
6375         path_raw = path + '_raw'
6376         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
6377                       '-O', 'raw', path, path_raw)
6378         utils.execute('mv', path_raw, path)
6379 
6380     def _disk_resize(self, image, size):
6381         """Attempts to resize a disk to size
6382 
6383         :param image: an instance of nova.virt.image.model.Image
6384 
6385         Attempts to resize a disk by checking the capabilities and
6386         preparing the format, then calling disk.api.extend.
6387 
6388         Note: Currently only support disk extend.
6389         """
6390 
6391         if not isinstance(image, imgmodel.LocalFileImage):
6392             LOG.debug("Skipping resize of non-local image")
6393             return
6394 
6395         # If we have a non partitioned image that we can extend
6396         # then ensure we're in 'raw' format so we can extend file system.
6397         converted = False
6398         if (size and
6399             image.format == imgmodel.FORMAT_QCOW2 and
6400             disk.can_resize_image(image.path, size) and
6401             disk.is_image_extendable(image)):
6402             self._disk_qcow2_to_raw(image.path)
6403             converted = True
6404             image = imgmodel.LocalFileImage(image.path,
6405                                             imgmodel.FORMAT_RAW)
6406 
6407         if size:
6408             disk.extend(image, size)
6409 
6410         if converted:
6411             # back to qcow2 (no backing_file though) so that snapshot
6412             # will be available
6413             self._disk_raw_to_qcow2(image.path)
6414 
6415     def finish_migration(self, context, migration, instance, disk_info,
6416                          network_info, image_meta, resize_instance,
6417                          block_device_info=None, power_on=True):
6418         LOG.debug("Starting finish_migration", instance=instance)
6419 
6420         # resize disks. only "disk" and "disk.local" are necessary.
6421         disk_info = jsonutils.loads(disk_info)
6422         for info in disk_info:
6423             size = self._disk_size_from_instance(instance, info)
6424             if resize_instance:
6425                 image = imgmodel.LocalFileImage(info['path'],
6426                                                 info['type'])
6427                 self._disk_resize(image, size)
6428             if info['type'] == 'raw' and CONF.use_cow_images:
6429                 self._disk_raw_to_qcow2(info['path'])
6430 
6431         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
6432                                             instance,
6433                                             image_meta,
6434                                             block_device_info)
6435         # assume _create_image do nothing if a target file exists.
6436         self._create_image(context, instance, disk_info['mapping'],
6437                            network_info=network_info,
6438                            block_device_info=None, inject_files=False,
6439                            fallback_from_host=migration.source_compute)
6440         xml = self._get_guest_xml(context, instance, network_info, disk_info,
6441                                   image_meta,
6442                                   block_device_info=block_device_info,
6443                                   write_to_disk=True)
6444         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
6445         # or not we've migrated to another host, because we unplug VIFs locally
6446         # and the status change in the port might go undetected by the neutron
6447         # L2 agent (or neutron server) so neutron may not know that the VIF was
6448         # unplugged in the first place and never send an event.
6449         self._create_domain_and_network(context, xml, instance, network_info,
6450                                         disk_info,
6451                                         block_device_info=block_device_info,
6452                                         power_on=power_on,
6453                                         vifs_already_plugged=True)
6454         if power_on:
6455             timer = loopingcall.FixedIntervalLoopingCall(
6456                                                     self._wait_for_running,
6457                                                     instance)
6458             timer.start(interval=0.5).wait()
6459 
6460     def _cleanup_failed_migration(self, inst_base):
6461         """Make sure that a failed migrate doesn't prevent us from rolling
6462         back in a revert.
6463         """
6464         try:
6465             shutil.rmtree(inst_base)
6466         except OSError as e:
6467             if e.errno != errno.ENOENT:
6468                 raise
6469 
6470     def finish_revert_migration(self, context, instance, network_info,
6471                                 block_device_info=None, power_on=True):
6472         LOG.debug("Starting finish_revert_migration",
6473                   instance=instance)
6474 
6475         inst_base = libvirt_utils.get_instance_path(instance)
6476         inst_base_resize = inst_base + "_resize"
6477 
6478         # NOTE(danms): if we're recovering from a failed migration,
6479         # make sure we don't have a left-over same-host base directory
6480         # that would conflict. Also, don't fail on the rename if the
6481         # failure happened early.
6482         if os.path.exists(inst_base_resize):
6483             self._cleanup_failed_migration(inst_base)
6484             utils.execute('mv', inst_base_resize, inst_base)
6485 
6486         image_meta = utils.get_image_from_system_metadata(
6487             instance.system_metadata)
6488 
6489         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
6490                                             instance,
6491                                             image_meta,
6492                                             block_device_info)
6493         xml = self._get_guest_xml(context, instance, network_info, disk_info,
6494                                   image_meta,
6495                                   block_device_info=block_device_info)
6496         self._create_domain_and_network(context, xml, instance, network_info,
6497                                         disk_info,
6498                                         block_device_info=block_device_info,
6499                                         power_on=power_on,
6500                                         vifs_already_plugged=True)
6501 
6502         if power_on:
6503             timer = loopingcall.FixedIntervalLoopingCall(
6504                                                     self._wait_for_running,
6505                                                     instance)
6506             timer.start(interval=0.5).wait()
6507 
6508         LOG.debug("finish_revert_migration finished successfully.",
6509                   instance=instance)
6510 
6511     def confirm_migration(self, migration, instance, network_info):
6512         """Confirms a resize, destroying the source VM."""
6513         self._cleanup_resize(instance, network_info)
6514 
6515     @staticmethod
6516     def _get_io_devices(xml_doc):
6517         """get the list of io devices from the xml document."""
6518         result = {"volumes": [], "ifaces": []}
6519         try:
6520             doc = etree.fromstring(xml_doc)
6521         except Exception:
6522             return result
6523         blocks = [('./devices/disk', 'volumes'),
6524             ('./devices/interface', 'ifaces')]
6525         for block, key in blocks:
6526             section = doc.findall(block)
6527             for node in section:
6528                 for child in node.getchildren():
6529                     if child.tag == 'target' and child.get('dev'):
6530                         result[key].append(child.get('dev'))
6531         return result
6532 
6533     def get_diagnostics(self, instance):
6534         guest = self._host.get_guest(instance)
6535 
6536         # TODO(sahid): We are converting all calls from a
6537         # virDomain object to use nova.virt.libvirt.Guest.
6538         # We should be able to remove domain at the end.
6539         domain = guest._domain
6540         output = {}
6541         # get cpu time, might launch an exception if the method
6542         # is not supported by the underlying hypervisor being
6543         # used by libvirt
6544         try:
6545             for vcpu in guest.get_vcpus_info():
6546                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
6547         except libvirt.libvirtError:
6548             pass
6549         # get io status
6550         xml = guest.get_xml_desc()
6551         dom_io = LibvirtDriver._get_io_devices(xml)
6552         for guest_disk in dom_io["volumes"]:
6553             try:
6554                 # blockStats might launch an exception if the method
6555                 # is not supported by the underlying hypervisor being
6556                 # used by libvirt
6557                 stats = domain.blockStats(guest_disk)
6558                 output[guest_disk + "_read_req"] = stats[0]
6559                 output[guest_disk + "_read"] = stats[1]
6560                 output[guest_disk + "_write_req"] = stats[2]
6561                 output[guest_disk + "_write"] = stats[3]
6562                 output[guest_disk + "_errors"] = stats[4]
6563             except libvirt.libvirtError:
6564                 pass
6565         for interface in dom_io["ifaces"]:
6566             try:
6567                 # interfaceStats might launch an exception if the method
6568                 # is not supported by the underlying hypervisor being
6569                 # used by libvirt
6570                 stats = domain.interfaceStats(interface)
6571                 output[interface + "_rx"] = stats[0]
6572                 output[interface + "_rx_packets"] = stats[1]
6573                 output[interface + "_rx_errors"] = stats[2]
6574                 output[interface + "_rx_drop"] = stats[3]
6575                 output[interface + "_tx"] = stats[4]
6576                 output[interface + "_tx_packets"] = stats[5]
6577                 output[interface + "_tx_errors"] = stats[6]
6578                 output[interface + "_tx_drop"] = stats[7]
6579             except libvirt.libvirtError:
6580                 pass
6581         output["memory"] = domain.maxMemory()
6582         # memoryStats might launch an exception if the method
6583         # is not supported by the underlying hypervisor being
6584         # used by libvirt
6585         try:
6586             mem = domain.memoryStats()
6587             for key in mem.keys():
6588                 output["memory-" + key] = mem[key]
6589         except (libvirt.libvirtError, AttributeError):
6590             pass
6591         return output
6592 
6593     def get_instance_diagnostics(self, instance):
6594         guest = self._host.get_guest(instance)
6595 
6596         # TODO(sahid): We are converting all calls from a
6597         # virDomain object to use nova.virt.libvirt.Guest.
6598         # We should be able to remove domain at the end.
6599         domain = guest._domain
6600 
6601         xml = guest.get_xml_desc()
6602         xml_doc = etree.fromstring(xml)
6603 
6604         (state, max_mem, mem, num_cpu, cpu_time) = \
6605             self._host.get_domain_info(domain)
6606         config_drive = configdrive.required_by(instance)
6607         launched_at = timeutils.normalize_time(instance.launched_at)
6608         uptime = timeutils.delta_seconds(launched_at,
6609                                          timeutils.utcnow())
6610         diags = diagnostics.Diagnostics(state=power_state.STATE_MAP[state],
6611                                         driver='libvirt',
6612                                         config_drive=config_drive,
6613                                         hypervisor_os='linux',
6614                                         uptime=uptime)
6615         diags.memory_details.maximum = max_mem / units.Mi
6616         diags.memory_details.used = mem / units.Mi
6617 
6618         # get cpu time, might launch an exception if the method
6619         # is not supported by the underlying hypervisor being
6620         # used by libvirt
6621         try:
6622             for vcpu in guest.get_vcpus_info():
6623                 diags.add_cpu(time=vcpu.time)
6624         except libvirt.libvirtError:
6625             pass
6626         # get io status
6627         dom_io = LibvirtDriver._get_io_devices(xml)
6628         for guest_disk in dom_io["volumes"]:
6629             try:
6630                 # blockStats might launch an exception if the method
6631                 # is not supported by the underlying hypervisor being
6632                 # used by libvirt
6633                 stats = domain.blockStats(guest_disk)
6634                 diags.add_disk(read_bytes=stats[1],
6635                                read_requests=stats[0],
6636                                write_bytes=stats[3],
6637                                write_requests=stats[2])
6638             except libvirt.libvirtError:
6639                 pass
6640         for interface in dom_io["ifaces"]:
6641             try:
6642                 # interfaceStats might launch an exception if the method
6643                 # is not supported by the underlying hypervisor being
6644                 # used by libvirt
6645                 stats = domain.interfaceStats(interface)
6646                 diags.add_nic(rx_octets=stats[0],
6647                               rx_errors=stats[2],
6648                               rx_drop=stats[3],
6649                               rx_packets=stats[1],
6650                               tx_octets=stats[4],
6651                               tx_errors=stats[6],
6652                               tx_drop=stats[7],
6653                               tx_packets=stats[5])
6654             except libvirt.libvirtError:
6655                 pass
6656 
6657         # Update mac addresses of interface if stats have been reported
6658         if diags.nic_details:
6659             nodes = xml_doc.findall('./devices/interface/mac')
6660             for index, node in enumerate(nodes):
6661                 diags.nic_details[index].mac_address = node.get('address')
6662         return diags
6663 
6664     def instance_on_disk(self, instance):
6665         # ensure directories exist and are writable
6666         instance_path = libvirt_utils.get_instance_path(instance)
6667         LOG.debug('Checking instance files accessibility %s', instance_path)
6668         shared_instance_path = os.access(instance_path, os.W_OK)
6669         # NOTE(flwang): For shared block storage scenario, the file system is
6670         # not really shared by the two hosts, but the volume of evacuated
6671         # instance is reachable.
6672         shared_block_storage = (self.image_backend.backend().
6673                                 is_shared_block_storage())
6674         return shared_instance_path or shared_block_storage
6675 
6676     def inject_network_info(self, instance, nw_info):
6677         self.firewall_driver.setup_basic_filtering(instance, nw_info)
6678 
6679     def delete_instance_files(self, instance):
6680         target = libvirt_utils.get_instance_path(instance)
6681         # A resize may be in progress
6682         target_resize = target + '_resize'
6683         # Other threads may attempt to rename the path, so renaming the path
6684         # to target + '_del' (because it is atomic) and iterating through
6685         # twice in the unlikely event that a concurrent rename occurs between
6686         # the two rename attempts in this method. In general this method
6687         # should be fairly thread-safe without these additional checks, since
6688         # other operations involving renames are not permitted when the task
6689         # state is not None and the task state should be set to something
6690         # other than None by the time this method is invoked.
6691         target_del = target + '_del'
6692         for i in six.moves.range(2):
6693             try:
6694                 utils.execute('mv', target, target_del)
6695                 break
6696             except Exception:
6697                 pass
6698             try:
6699                 utils.execute('mv', target_resize, target_del)
6700                 break
6701             except Exception:
6702                 pass
6703         # Either the target or target_resize path may still exist if all
6704         # rename attempts failed.
6705         remaining_path = None
6706         for p in (target, target_resize):
6707             if os.path.exists(p):
6708                 remaining_path = p
6709                 break
6710 
6711         # A previous delete attempt may have been interrupted, so target_del
6712         # may exist even if all rename attempts during the present method
6713         # invocation failed due to the absence of both target and
6714         # target_resize.
6715         if not remaining_path and os.path.exists(target_del):
6716             LOG.info(_LI('Deleting instance files %s'), target_del,
6717                      instance=instance)
6718             remaining_path = target_del
6719             try:
6720                 shutil.rmtree(target_del)
6721             except OSError as e:
6722                 LOG.error(_LE('Failed to cleanup directory %(target)s: '
6723                               '%(e)s'), {'target': target_del, 'e': e},
6724                             instance=instance)
6725 
6726         # It is possible that the delete failed, if so don't mark the instance
6727         # as cleaned.
6728         if remaining_path and os.path.exists(remaining_path):
6729             LOG.info(_LI('Deletion of %s failed'), remaining_path,
6730                      instance=instance)
6731             return False
6732 
6733         LOG.info(_LI('Deletion of %s complete'), target_del, instance=instance)
6734         return True
6735 
6736     @property
6737     def need_legacy_block_device_info(self):
6738         return False
6739 
6740     def default_root_device_name(self, instance, image_meta, root_bdm):
6741 
6742         disk_bus = blockinfo.get_disk_bus_for_device_type(
6743             CONF.libvirt.virt_type, image_meta, "disk")
6744         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
6745             CONF.libvirt.virt_type, image_meta, "cdrom")
6746         root_info = blockinfo.get_root_info(
6747             CONF.libvirt.virt_type, image_meta, root_bdm, disk_bus,
6748             cdrom_bus)
6749         return block_device.prepend_dev(root_info['dev'])
6750 
6751     def default_device_names_for_instance(self, instance, root_device_name,
6752                                           *block_device_lists):
6753         image_meta = utils.get_image_from_system_metadata(
6754             instance.system_metadata)
6755 
6756         block_device_mapping = list(itertools.chain(*block_device_lists))
6757         # NOTE(ndipanov): Null out the device names so that blockinfo code
6758         #                 will assign them
6759         for bdm in block_device_mapping:
6760             if bdm.device_name is not None:
6761                 LOG.warn(_LW("Ignoring supplied device name: %(device_name)s. "
6762                              "Libvirt can't honour user-supplied dev names"),
6763                          {'device_name': bdm.device_name}, instance=instance)
6764                 bdm.device_name = None
6765         block_device_info = driver.get_block_device_info(instance,
6766                                                          block_device_mapping)
6767 
6768         blockinfo.default_device_names(CONF.libvirt.virt_type,
6769                                        nova_context.get_admin_context(),
6770                                        instance,
6771                                        block_device_info,
6772                                        image_meta)
6773 
6774     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
6775         image_meta = utils.get_image_from_system_metadata(
6776             instance.system_metadata)
6777         block_device_info = driver.get_block_device_info(instance, bdms)
6778         instance_info = blockinfo.get_disk_info(
6779                 CONF.libvirt.virt_type, instance,
6780                 image_meta, block_device_info=block_device_info)
6781 
6782         suggested_dev_name = block_device_obj.device_name
6783         if suggested_dev_name is not None:
6784             LOG.warn(_LW('Ignoring supplied device name: %(suggested_dev)s'),
6785                      {'suggested_dev': suggested_dev_name}, instance=instance)
6786 
6787         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
6788         #                 only when it's actually not set on the bd object
6789         block_device_obj.device_name = None
6790         disk_info = blockinfo.get_info_from_bdm(
6791                 CONF.libvirt.virt_type, image_meta, block_device_obj,
6792                 mapping=instance_info['mapping'])
6793         return block_device.prepend_dev(disk_info['dev'])
6794 
6795     def is_supported_fs_format(self, fs_type):
6796         return fs_type in [disk.FS_FORMAT_EXT2, disk.FS_FORMAT_EXT3,
6797                            disk.FS_FORMAT_EXT4, disk.FS_FORMAT_XFS]
6798 
6799     def _get_power_state(self, virt_dom):
6800         dom_info = self._host.get_domain_info(virt_dom)
6801         return LIBVIRT_POWER_STATE[dom_info[0]]
