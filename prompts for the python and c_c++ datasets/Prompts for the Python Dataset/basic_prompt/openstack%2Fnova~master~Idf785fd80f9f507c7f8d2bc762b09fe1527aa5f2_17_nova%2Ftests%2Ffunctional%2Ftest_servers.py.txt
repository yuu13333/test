Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2011 Justin Santa Barbara
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 from __future__ import absolute_import
16 
17 import collections
18 import copy
19 import datetime
20 import time
21 import zlib
22 
23 from keystoneauth1 import adapter
24 import mock
25 import os_resource_classes as orc
26 from oslo_config import cfg
27 from oslo_log import log as logging
28 from oslo_serialization import base64
29 from oslo_serialization import jsonutils
30 from oslo_utils.fixture import uuidsentinel as uuids
31 from oslo_utils import timeutils
32 import six
33 
34 from nova.compute import api as compute_api
35 from nova.compute import instance_actions
36 from nova.compute import manager as compute_manager
37 from nova import context
38 from nova import exception
39 from nova.network.neutronv2 import constants
40 from nova import objects
41 from nova.objects import block_device as block_device_obj
42 from nova.scheduler import utils
43 from nova.scheduler import weights
44 from nova import test
45 from nova.tests import fixtures as nova_fixtures
46 from nova.tests.functional.api import client
47 from nova.tests.functional import integrated_helpers
48 from nova.tests.unit.api.openstack import fakes
49 from nova.tests.unit import fake_block_device
50 from nova.tests.unit import fake_notifier
51 from nova.tests.unit import fake_requests
52 import nova.tests.unit.image.fake
53 from nova.tests.unit.objects import test_instance_info_cache
54 from nova.virt import fake
55 from nova import volume
56 
57 CONF = cfg.CONF
58 
59 LOG = logging.getLogger(__name__)
60 
61 
62 class AltHostWeigher(weights.BaseHostWeigher):
63     """Used in the alternate host tests to return a pre-determined list of
64     hosts.
65     """
66     def _weigh_object(self, host_state, weight_properties):
67         """Return a defined order of hosts."""
68         weights = {"selection": 999, "alt_host1": 888, "alt_host2": 777,
69                    "alt_host3": 666, "host1": 0, "host2": 0}
70         return weights.get(host_state.host, 0)
71 
72 
73 class ServersTestBase(integrated_helpers._IntegratedTestBase):
74     api_major_version = 'v2'
75     _force_delete_parameter = 'forceDelete'
76     _image_ref_parameter = 'imageRef'
77     _flavor_ref_parameter = 'flavorRef'
78     _access_ipv4_parameter = 'accessIPv4'
79     _access_ipv6_parameter = 'accessIPv6'
80     _return_resv_id_parameter = 'return_reservation_id'
81     _min_count_parameter = 'min_count'
82 
83     USE_NEUTRON = True
84 
85     def setUp(self):
86         self.computes = {}
87         super(ServersTestBase, self).setUp()
88 
89     def _wait_for_state_change(self, server, from_status):
90         for i in range(0, 50):
91             server = self.api.get_server(server['id'])
92             if server['status'] != from_status:
93                 break
94             time.sleep(.1)
95 
96         return server
97 
98     def _wait_for_deletion(self, server_id):
99         # Wait (briefly) for deletion
100         for _retries in range(50):
101             try:
102                 found_server = self.api.get_server(server_id)
103             except client.OpenStackApiNotFoundException:
104                 found_server = None
105                 LOG.debug("Got 404, proceeding")
106                 break
107 
108             LOG.debug("Found_server=%s", found_server)
109 
110             # TODO(justinsb): Mock doesn't yet do accurate state changes
111             # if found_server['status'] != 'deleting':
112             #    break
113             time.sleep(.1)
114 
115         # Should be gone
116         self.assertFalse(found_server)
117 
118     def _delete_server(self, server_id):
119         # Delete the server
120         self.api.delete_server(server_id)
121         self._wait_for_deletion(server_id)
122 
123     def _get_access_ips_params(self):
124         return {self._access_ipv4_parameter: "172.19.0.2",
125                 self._access_ipv6_parameter: "fe80::2"}
126 
127     def _verify_access_ips(self, server):
128         self.assertEqual('172.19.0.2',
129                          server[self._access_ipv4_parameter])
130         self.assertEqual('fe80::2', server[self._access_ipv6_parameter])
131 
132 
133 class ServersTest(ServersTestBase):
134 
135     def test_get_servers(self):
136         # Simple check that listing servers works.
137         servers = self.api.get_servers()
138         for server in servers:
139             LOG.debug("server: %s", server)
140 
141     def _get_node_build_failures(self):
142         ctxt = context.get_admin_context()
143         computes = objects.ComputeNodeList.get_all(ctxt)
144         return {
145             node.hypervisor_hostname: int(node.stats.get('failed_builds', 0))
146             for node in computes}
147 
148     def _run_periodics(self):
149         """Run the update_available_resource task on every compute manager
150 
151         This runs periodics on the computes in an undefined order; some child
152         class redefined this function to force a specific order.
153         """
154 
155         if self.compute.host not in self.computes:
156             self.computes[self.compute.host] = self.compute
157 
158         ctx = context.get_admin_context()
159         for compute in self.computes.values():
160             LOG.info('Running periodic for compute (%s)',
161                 compute.manager.host)
162             compute.manager.update_available_resource(ctx)
163         LOG.info('Finished with periodics')
164 
165     def test_create_server_with_error(self):
166         # Create a server which will enter error state.
167 
168         def throw_error(*args, **kwargs):
169             raise exception.BuildAbortException(reason='',
170                     instance_uuid='fake')
171 
172         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
173 
174         server = self._build_minimal_create_server_request()
175         created_server = self.api.post_server({"server": server})
176         created_server_id = created_server['id']
177 
178         found_server = self.api.get_server(created_server_id)
179         self.assertEqual(created_server_id, found_server['id'])
180 
181         found_server = self._wait_for_state_change(found_server, 'BUILD')
182 
183         self.assertEqual('ERROR', found_server['status'])
184         self._delete_server(created_server_id)
185 
186         # We should have no (persisted) build failures until we update
187         # resources, after which we should have one
188         self.assertEqual([0], list(self._get_node_build_failures().values()))
189         self._run_periodics()
190         self.assertEqual([1], list(self._get_node_build_failures().values()))
191 
192     def test_create_server_with_image_type_filter(self):
193         self.flags(query_placement_for_image_type_support=True,
194                    group='scheduler')
195 
196         raw_image = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
197         vhd_image = 'a440c04b-79fa-479c-bed1-0b816eaec379'
198 
199         server = self._build_minimal_create_server_request(
200             image_uuid=vhd_image)
201         server = self.api.post_server({'server': server})
202         server = self.api.get_server(server['id'])
203         errored_server = self._wait_for_state_change(server, server['status'])
204         self.assertEqual('ERROR', errored_server['status'])
205         self.assertIn('No valid host', errored_server['fault']['message'])
206 
207         server = self._build_minimal_create_server_request(
208             image_uuid=raw_image)
209         server = self.api.post_server({'server': server})
210         server = self.api.get_server(server['id'])
211         created_server = self._wait_for_state_change(server, server['status'])
212         self.assertEqual('ACTIVE', created_server['status'])
213 
214     def _test_create_server_with_error_with_retries(self):
215         # Create a server which will enter error state.
216 
217         self.compute2 = self.start_service('compute', host='host2')
218         self.computes['compute2'] = self.compute2
219 
220         fails = []
221 
222         def throw_error(*args, **kwargs):
223             fails.append('one')
224             raise test.TestingException('Please retry me')
225 
226         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
227 
228         server = self._build_minimal_create_server_request()
229         created_server = self.api.post_server({"server": server})
230         created_server_id = created_server['id']
231 
232         found_server = self.api.get_server(created_server_id)
233         self.assertEqual(created_server_id, found_server['id'])
234 
235         found_server = self._wait_for_state_change(found_server, 'BUILD')
236 
237         self.assertEqual('ERROR', found_server['status'])
238         self._delete_server(created_server_id)
239 
240         return len(fails)
241 
242     def test_create_server_with_error_with_retries(self):
243         self.flags(max_attempts=2, group='scheduler')
244         fails = self._test_create_server_with_error_with_retries()
245         self.assertEqual(2, fails)
246         self._run_periodics()
247         self.assertEqual(
248             [1, 1], list(self._get_node_build_failures().values()))
249 
250     def test_create_server_with_error_with_no_retries(self):
251         self.flags(max_attempts=1, group='scheduler')
252         fails = self._test_create_server_with_error_with_retries()
253         self.assertEqual(1, fails)
254         self._run_periodics()
255         self.assertEqual(
256             [0, 1], list(sorted(self._get_node_build_failures().values())))
257 
258     def test_create_and_delete_server(self):
259         # Creates and deletes a server.
260 
261         # Create server
262         # Build the server data gradually, checking errors along the way
263         server = {}
264         good_server = self._build_minimal_create_server_request()
265 
266         post = {'server': server}
267 
268         # Without an imageRef, this throws 500.
269         # TODO(justinsb): Check whatever the spec says should be thrown here
270         self.assertRaises(client.OpenStackApiException,
271                           self.api.post_server, post)
272 
273         # With an invalid imageRef, this throws 500.
274         server[self._image_ref_parameter] = self.get_invalid_image()
275         # TODO(justinsb): Check whatever the spec says should be thrown here
276         self.assertRaises(client.OpenStackApiException,
277                           self.api.post_server, post)
278 
279         # Add a valid imageRef
280         server[self._image_ref_parameter] = good_server.get(
281             self._image_ref_parameter)
282 
283         # Without flavorRef, this throws 500
284         # TODO(justinsb): Check whatever the spec says should be thrown here
285         self.assertRaises(client.OpenStackApiException,
286                           self.api.post_server, post)
287 
288         server[self._flavor_ref_parameter] = good_server.get(
289             self._flavor_ref_parameter)
290 
291         # Without a name, this throws 500
292         # TODO(justinsb): Check whatever the spec says should be thrown here
293         self.assertRaises(client.OpenStackApiException,
294                           self.api.post_server, post)
295 
296         # Set a valid server name
297         server['name'] = good_server['name']
298 
299         created_server = self.api.post_server(post)
300         LOG.debug("created_server: %s", created_server)
301         self.assertTrue(created_server['id'])
302         created_server_id = created_server['id']
303 
304         # Check it's there
305         found_server = self.api.get_server(created_server_id)
306         self.assertEqual(created_server_id, found_server['id'])
307 
308         # It should also be in the all-servers list
309         servers = self.api.get_servers()
310         server_ids = [s['id'] for s in servers]
311         self.assertIn(created_server_id, server_ids)
312 
313         found_server = self._wait_for_state_change(found_server, 'BUILD')
314         # It should be available...
315         # TODO(justinsb): Mock doesn't yet do this...
316         self.assertEqual('ACTIVE', found_server['status'])
317         servers = self.api.get_servers(detail=True)
318         for server in servers:
319             self.assertIn("image", server)
320             self.assertIn("flavor", server)
321 
322         self._delete_server(created_server_id)
323 
324     def _force_reclaim(self):
325         # Make sure that compute manager thinks the instance is
326         # old enough to be expired
327         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
328         timeutils.set_time_override(override_time=the_past)
329         self.addCleanup(timeutils.clear_time_override)
330         ctxt = context.get_admin_context()
331         self.compute._reclaim_queued_deletes(ctxt)
332 
333     def test_deferred_delete(self):
334         # Creates, deletes and waits for server to be reclaimed.
335         self.flags(reclaim_instance_interval=1)
336 
337         # Create server
338         server = self._build_minimal_create_server_request()
339 
340         created_server = self.api.post_server({'server': server})
341         LOG.debug("created_server: %s", created_server)
342         self.assertTrue(created_server['id'])
343         created_server_id = created_server['id']
344 
345         # Wait for it to finish being created
346         found_server = self._wait_for_state_change(created_server, 'BUILD')
347 
348         # It should be available...
349         self.assertEqual('ACTIVE', found_server['status'])
350 
351         # Cannot restore unless instance is deleted
352         self.assertRaises(client.OpenStackApiException,
353                           self.api.post_server_action, created_server_id,
354                           {'restore': {}})
355 
356         # Delete the server
357         self.api.delete_server(created_server_id)
358 
359         # Wait for queued deletion
360         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
361         self.assertEqual('SOFT_DELETED', found_server['status'])
362 
363         self._force_reclaim()
364 
365         # Wait for real deletion
366         self._wait_for_deletion(created_server_id)
367 
368     def test_deferred_delete_restore(self):
369         # Creates, deletes and restores a server.
370         self.flags(reclaim_instance_interval=3600)
371 
372         # Create server
373         server = self._build_minimal_create_server_request()
374 
375         created_server = self.api.post_server({'server': server})
376         LOG.debug("created_server: %s", created_server)
377         self.assertTrue(created_server['id'])
378         created_server_id = created_server['id']
379 
380         # Wait for it to finish being created
381         found_server = self._wait_for_state_change(created_server, 'BUILD')
382 
383         # It should be available...
384         self.assertEqual('ACTIVE', found_server['status'])
385 
386         # Delete the server
387         self.api.delete_server(created_server_id)
388 
389         # Wait for queued deletion
390         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
391         self.assertEqual('SOFT_DELETED', found_server['status'])
392 
393         # Restore server
394         self.api.post_server_action(created_server_id, {'restore': {}})
395 
396         # Wait for server to become active again
397         found_server = self._wait_for_state_change(found_server, 'DELETED')
398         self.assertEqual('ACTIVE', found_server['status'])
399 
400     def test_deferred_delete_restore_overquota(self):
401         # Test that a restore that would put the user over quota fails
402         self.flags(instances=1, group='quota')
403         # Creates, deletes and restores a server.
404         self.flags(reclaim_instance_interval=3600)
405 
406         # Create server
407         server = self._build_minimal_create_server_request()
408 
409         created_server1 = self.api.post_server({'server': server})
410         LOG.debug("created_server: %s", created_server1)
411         self.assertTrue(created_server1['id'])
412         created_server_id1 = created_server1['id']
413 
414         # Wait for it to finish being created
415         found_server1 = self._wait_for_state_change(created_server1, 'BUILD')
416 
417         # It should be available...
418         self.assertEqual('ACTIVE', found_server1['status'])
419 
420         # Delete the server
421         self.api.delete_server(created_server_id1)
422 
423         # Wait for queued deletion
424         found_server1 = self._wait_for_state_change(found_server1, 'ACTIVE')
425         self.assertEqual('SOFT_DELETED', found_server1['status'])
426 
427         # Create a second server
428         server = self._build_minimal_create_server_request()
429 
430         created_server2 = self.api.post_server({'server': server})
431         LOG.debug("created_server: %s", created_server2)
432         self.assertTrue(created_server2['id'])
433 
434         # Wait for it to finish being created
435         found_server2 = self._wait_for_state_change(created_server2, 'BUILD')
436 
437         # It should be available...
438         self.assertEqual('ACTIVE', found_server2['status'])
439 
440         # Try to restore the first server, it should fail
441         ex = self.assertRaises(client.OpenStackApiException,
442                                self.api.post_server_action,
443                                created_server_id1, {'restore': {}})
444         self.assertEqual(403, ex.response.status_code)
445         self.assertEqual('SOFT_DELETED', found_server1['status'])
446 
447     def test_deferred_delete_force(self):
448         # Creates, deletes and force deletes a server.
449         self.flags(reclaim_instance_interval=3600)
450 
451         # Create server
452         server = self._build_minimal_create_server_request()
453 
454         created_server = self.api.post_server({'server': server})
455         LOG.debug("created_server: %s", created_server)
456         self.assertTrue(created_server['id'])
457         created_server_id = created_server['id']
458 
459         # Wait for it to finish being created
460         found_server = self._wait_for_state_change(created_server, 'BUILD')
461 
462         # It should be available...
463         self.assertEqual('ACTIVE', found_server['status'])
464 
465         # Delete the server
466         self.api.delete_server(created_server_id)
467 
468         # Wait for queued deletion
469         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
470         self.assertEqual('SOFT_DELETED', found_server['status'])
471 
472         # Force delete server
473         self.api.post_server_action(created_server_id,
474                                     {self._force_delete_parameter: {}})
475 
476         # Wait for real deletion
477         self._wait_for_deletion(created_server_id)
478 
479     def test_create_server_with_metadata(self):
480         # Creates a server with metadata.
481 
482         # Build the server data gradually, checking errors along the way
483         server = self._build_minimal_create_server_request()
484 
485         metadata = {}
486         for i in range(30):
487             metadata['key_%s' % i] = 'value_%s' % i
488 
489         server['metadata'] = metadata
490 
491         post = {'server': server}
492         created_server = self.api.post_server(post)
493         LOG.debug("created_server: %s", created_server)
494         self.assertTrue(created_server['id'])
495         created_server_id = created_server['id']
496 
497         found_server = self.api.get_server(created_server_id)
498         self.assertEqual(created_server_id, found_server['id'])
499         self.assertEqual(metadata, found_server.get('metadata'))
500 
501         # The server should also be in the all-servers details list
502         servers = self.api.get_servers(detail=True)
503         server_map = {server['id']: server for server in servers}
504         found_server = server_map.get(created_server_id)
505         self.assertTrue(found_server)
506         # Details do include metadata
507         self.assertEqual(metadata, found_server.get('metadata'))
508 
509         # The server should also be in the all-servers summary list
510         servers = self.api.get_servers(detail=False)
511         server_map = {server['id']: server for server in servers}
512         found_server = server_map.get(created_server_id)
513         self.assertTrue(found_server)
514         # Summary should not include metadata
515         self.assertFalse(found_server.get('metadata'))
516 
517         # Cleanup
518         self._delete_server(created_server_id)
519 
520     def test_server_metadata_actions_negative_invalid_state(self):
521         # Create server with metadata
522         server = self._build_minimal_create_server_request()
523 
524         metadata = {'key_1': 'value_1'}
525 
526         server['metadata'] = metadata
527 
528         post = {'server': server}
529         created_server = self.api.post_server(post)
530 
531         found_server = self._wait_for_state_change(created_server, 'BUILD')
532         self.assertEqual('ACTIVE', found_server['status'])
533         self.assertEqual(metadata, found_server.get('metadata'))
534         server_id = found_server['id']
535 
536         # Change status from ACTIVE to SHELVED for negative test
537         self.flags(shelved_offload_time = -1)
538         self.api.post_server_action(server_id, {'shelve': {}})
539         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
540         self.assertEqual('SHELVED', found_server['status'])
541 
542         metadata = {'key_2': 'value_2'}
543 
544         # Update Metadata item in SHELVED (not ACTIVE, etc.)
545         ex = self.assertRaises(client.OpenStackApiException,
546                                self.api.post_server_metadata,
547                                server_id, metadata)
548         self.assertEqual(409, ex.response.status_code)
549         self.assertEqual('SHELVED', found_server['status'])
550 
551         # Delete Metadata item in SHELVED (not ACTIVE, etc.)
552         ex = self.assertRaises(client.OpenStackApiException,
553                                self.api.delete_server_metadata,
554                                server_id, 'key_1')
555         self.assertEqual(409, ex.response.status_code)
556         self.assertEqual('SHELVED', found_server['status'])
557 
558         # Cleanup
559         self._delete_server(server_id)
560 
561     def test_create_and_rebuild_server(self):
562         # Rebuild a server with metadata.
563 
564         # create a server with initially has no metadata
565         server = self._build_minimal_create_server_request()
566         server_post = {'server': server}
567 
568         metadata = {}
569         for i in range(30):
570             metadata['key_%s' % i] = 'value_%s' % i
571 
572         server_post['server']['metadata'] = metadata
573 
574         created_server = self.api.post_server(server_post)
575         LOG.debug("created_server: %s", created_server)
576         self.assertTrue(created_server['id'])
577         created_server_id = created_server['id']
578 
579         created_server = self._wait_for_state_change(created_server, 'BUILD')
580 
581         # rebuild the server with metadata and other server attributes
582         post = {}
583         post['rebuild'] = {
584             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
585             "name": "blah",
586             self._access_ipv4_parameter: "172.19.0.2",
587             self._access_ipv6_parameter: "fe80::2",
588             "metadata": {'some': 'thing'},
589         }
590         post['rebuild'].update(self._get_access_ips_params())
591 
592         self.api.post_server_action(created_server_id, post)
593         LOG.debug("rebuilt server: %s", created_server)
594         self.assertTrue(created_server['id'])
595 
596         found_server = self.api.get_server(created_server_id)
597         self.assertEqual(created_server_id, found_server['id'])
598         self.assertEqual({'some': 'thing'}, found_server.get('metadata'))
599         self.assertEqual('blah', found_server.get('name'))
600         self.assertEqual(post['rebuild'][self._image_ref_parameter],
601                          found_server.get('image')['id'])
602         self._verify_access_ips(found_server)
603 
604         # rebuild the server with empty metadata and nothing else
605         post = {}
606         post['rebuild'] = {
607             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
608             "metadata": {},
609         }
610 
611         self.api.post_server_action(created_server_id, post)
612         LOG.debug("rebuilt server: %s", created_server)
613         self.assertTrue(created_server['id'])
614 
615         found_server = self.api.get_server(created_server_id)
616         self.assertEqual(created_server_id, found_server['id'])
617         self.assertEqual({}, found_server.get('metadata'))
618         self.assertEqual('blah', found_server.get('name'))
619         self.assertEqual(post['rebuild'][self._image_ref_parameter],
620                          found_server.get('image')['id'])
621         self._verify_access_ips(found_server)
622 
623         # Cleanup
624         self._delete_server(created_server_id)
625 
626     def test_rename_server(self):
627         # Test building and renaming a server.
628 
629         # Create a server
630         server = self._build_minimal_create_server_request()
631         created_server = self.api.post_server({'server': server})
632         LOG.debug("created_server: %s", created_server)
633         server_id = created_server['id']
634         self.assertTrue(server_id)
635 
636         # Rename the server to 'new-name'
637         self.api.put_server(server_id, {'server': {'name': 'new-name'}})
638 
639         # Check the name of the server
640         created_server = self.api.get_server(server_id)
641         self.assertEqual(created_server['name'], 'new-name')
642 
643         # Cleanup
644         self._delete_server(server_id)
645 
646     def test_create_multiple_servers(self):
647         # Creates multiple servers and checks for reservation_id.
648 
649         # Create 2 servers, setting 'return_reservation_id, which should
650         # return a reservation_id
651         server = self._build_minimal_create_server_request()
652         server[self._min_count_parameter] = 2
653         server[self._return_resv_id_parameter] = True
654         post = {'server': server}
655         response = self.api.post_server(post)
656         self.assertIn('reservation_id', response)
657         reservation_id = response['reservation_id']
658         self.assertNotIn(reservation_id, ['', None])
659         # Assert that the reservation_id itself has the expected format
660         self.assertRegex(reservation_id, 'r-[0-9a-zA-Z]{8}')
661 
662         # Create 1 more server, which should not return a reservation_id
663         server = self._build_minimal_create_server_request()
664         post = {'server': server}
665         created_server = self.api.post_server(post)
666         self.assertTrue(created_server['id'])
667         created_server_id = created_server['id']
668 
669         # lookup servers created by the first request.
670         servers = self.api.get_servers(detail=True,
671                 search_opts={'reservation_id': reservation_id})
672         server_map = {server['id']: server for server in servers}
673         found_server = server_map.get(created_server_id)
674         # The server from the 2nd request should not be there.
675         self.assertIsNone(found_server)
676         # Should have found 2 servers.
677         self.assertEqual(len(server_map), 2)
678 
679         # Cleanup
680         self._delete_server(created_server_id)
681         for server_id in server_map:
682             self._delete_server(server_id)
683 
684     def test_create_server_with_injected_files(self):
685         # Creates a server with injected_files.
686         personality = []
687 
688         # Inject a text file
689         data = 'Hello, World!'
690         personality.append({
691             'path': '/helloworld.txt',
692             'contents': base64.encode_as_bytes(data),
693         })
694 
695         # Inject a binary file
696         data = zlib.compress(b'Hello, World!')
697         personality.append({
698             'path': '/helloworld.zip',
699             'contents': base64.encode_as_bytes(data),
700         })
701 
702         # Create server
703         server = self._build_minimal_create_server_request()
704         server['personality'] = personality
705 
706         post = {'server': server}
707 
708         created_server = self.api.post_server(post)
709         LOG.debug("created_server: %s", created_server)
710         self.assertTrue(created_server['id'])
711         created_server_id = created_server['id']
712 
713         # Check it's there
714         found_server = self.api.get_server(created_server_id)
715         self.assertEqual(created_server_id, found_server['id'])
716 
717         found_server = self._wait_for_state_change(found_server, 'BUILD')
718         self.assertEqual('ACTIVE', found_server['status'])
719 
720         # Cleanup
721         self._delete_server(created_server_id)
722 
723     def test_stop_start_servers_negative_invalid_state(self):
724         # Create server
725         server = self._build_minimal_create_server_request()
726         created_server = self.api.post_server({"server": server})
727         created_server_id = created_server['id']
728 
729         found_server = self._wait_for_state_change(created_server, 'BUILD')
730         self.assertEqual('ACTIVE', found_server['status'])
731 
732         # Start server in ACTIVE
733         # NOTE(mkoshiya): When os-start API runs, the server status
734         # must be SHUTOFF.
735         # By returning 409, I want to confirm that the ACTIVE server does not
736         # cause unexpected behavior.
737         post = {'os-start': {}}
738         ex = self.assertRaises(client.OpenStackApiException,
739                                self.api.post_server_action,
740                                created_server_id, post)
741         self.assertEqual(409, ex.response.status_code)
742         self.assertEqual('ACTIVE', found_server['status'])
743 
744         # Stop server
745         post = {'os-stop': {}}
746         self.api.post_server_action(created_server_id, post)
747         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
748         self.assertEqual('SHUTOFF', found_server['status'])
749 
750         # Stop server in SHUTOFF
751         # NOTE(mkoshiya): When os-stop API runs, the server status
752         # must be ACTIVE or ERROR.
753         # By returning 409, I want to confirm that the SHUTOFF server does not
754         # cause unexpected behavior.
755         post = {'os-stop': {}}
756         ex = self.assertRaises(client.OpenStackApiException,
757                                self.api.post_server_action,
758                                created_server_id, post)
759         self.assertEqual(409, ex.response.status_code)
760         self.assertEqual('SHUTOFF', found_server['status'])
761 
762         # Cleanup
763         self._delete_server(created_server_id)
764 
765     def test_revert_resized_server_negative_invalid_state(self):
766         # Create server
767         server = self._build_minimal_create_server_request()
768         created_server = self.api.post_server({"server": server})
769         created_server_id = created_server['id']
770         found_server = self._wait_for_state_change(created_server, 'BUILD')
771         self.assertEqual('ACTIVE', found_server['status'])
772 
773         # Revert resized server in ACTIVE
774         # NOTE(yatsumi): When revert resized server API runs,
775         # the server status must be VERIFY_RESIZE.
776         # By returning 409, I want to confirm that the ACTIVE server does not
777         # cause unexpected behavior.
778         post = {'revertResize': {}}
779         ex = self.assertRaises(client.OpenStackApiException,
780                                self.api.post_server_action,
781                                created_server_id, post)
782         self.assertEqual(409, ex.response.status_code)
783         self.assertEqual('ACTIVE', found_server['status'])
784 
785         # Cleanup
786         self._delete_server(created_server_id)
787 
788     def test_resize_server_negative_invalid_state(self):
789         # Avoid migration
790         self.flags(allow_resize_to_same_host=True)
791 
792         # Create server
793         server = self._build_minimal_create_server_request()
794         created_server = self.api.post_server({"server": server})
795         created_server_id = created_server['id']
796         found_server = self._wait_for_state_change(created_server, 'BUILD')
797         self.assertEqual('ACTIVE', found_server['status'])
798 
799         # Resize server(flavorRef: 1 -> 2)
800         post = {'resize': {"flavorRef": "2", "OS-DCF:diskConfig": "AUTO"}}
801         self.api.post_server_action(created_server_id, post)
802         found_server = self._wait_for_state_change(found_server, 'RESIZE')
803         self.assertEqual('VERIFY_RESIZE', found_server['status'])
804 
805         # Resize server in VERIFY_RESIZE(flavorRef: 2 -> 1)
806         # NOTE(yatsumi): When resize API runs, the server status
807         # must be ACTIVE or SHUTOFF.
808         # By returning 409, I want to confirm that the VERIFY_RESIZE server
809         # does not cause unexpected behavior.
810         post = {'resize': {"flavorRef": "1", "OS-DCF:diskConfig": "AUTO"}}
811         ex = self.assertRaises(client.OpenStackApiException,
812                                self.api.post_server_action,
813                                created_server_id, post)
814         self.assertEqual(409, ex.response.status_code)
815         self.assertEqual('VERIFY_RESIZE', found_server['status'])
816 
817         # Cleanup
818         self._delete_server(created_server_id)
819 
820     def test_confirm_resized_server_negative_invalid_state(self):
821         # Create server
822         server = self._build_minimal_create_server_request()
823         created_server = self.api.post_server({"server": server})
824         created_server_id = created_server['id']
825         found_server = self._wait_for_state_change(created_server, 'BUILD')
826         self.assertEqual('ACTIVE', found_server['status'])
827 
828         # Confirm resized server in ACTIVE
829         # NOTE(yatsumi): When confirm resized server API runs,
830         # the server status must be VERIFY_RESIZE.
831         # By returning 409, I want to confirm that the ACTIVE server does not
832         # cause unexpected behavior.
833         post = {'confirmResize': {}}
834         ex = self.assertRaises(client.OpenStackApiException,
835                                self.api.post_server_action,
836                                created_server_id, post)
837         self.assertEqual(409, ex.response.status_code)
838         self.assertEqual('ACTIVE', found_server['status'])
839 
840         # Cleanup
841         self._delete_server(created_server_id)
842 
843     def test_resize_server_overquota(self):
844         self.flags(cores=1, group='quota')
845         self.flags(ram=512, group='quota')
846         # Create server with default flavor, 1 core, 512 ram
847         server = self._build_minimal_create_server_request()
848         created_server = self.api.post_server({"server": server})
849         created_server_id = created_server['id']
850 
851         found_server = self._wait_for_state_change(created_server, 'BUILD')
852         self.assertEqual('ACTIVE', found_server['status'])
853 
854         # Try to resize to flavorid 2, 1 core, 2048 ram
855         post = {'resize': {'flavorRef': '2'}}
856         ex = self.assertRaises(client.OpenStackApiException,
857                                self.api.post_server_action,
858                                created_server_id, post)
859         self.assertEqual(403, ex.response.status_code)
860 
861     def test_attach_vol_maximum_disk_devices_exceeded(self):
862         self.useFixture(nova_fixtures.CinderFixture(self))
863 
864         server = self._build_minimal_create_server_request()
865         created_server = self.api.post_server({"server": server})
866         server_id = created_server['id']
867         self._wait_for_state_change(created_server, 'BUILD')
868 
869         volume_id = '9a695496-44aa-4404-b2cc-ccab2501f87e'
870         LOG.info('Attaching volume %s to server %s', volume_id, server_id)
871 
872         # The fake driver doesn't implement get_device_name_for_instance, so
873         # we'll just raise the exception directly here, instead of simuluating
874         # an instance with 26 disk devices already attached.
875         with mock.patch.object(self.compute.driver,
876                                'get_device_name_for_instance') as mock_get:
877             mock_get.side_effect = exception.TooManyDiskDevices(maximum=26)
878             ex = self.assertRaises(
879                 client.OpenStackApiException, self.api.post_server_volume,
880                 server_id, dict(volumeAttachment=dict(volumeId=volume_id)))
881             expected = ('The maximum allowed number of disk devices (26) to '
882                         'attach to a single instance has been exceeded.')
883             self.assertEqual(403, ex.response.status_code)
884             self.assertIn(expected, six.text_type(ex))
885 
886 
887 class ServersTestV21(ServersTest):
888     api_major_version = 'v2.1'
889 
890 
891 class ServersTestV219(ServersTestBase):
892     api_major_version = 'v2.1'
893 
894     def _create_server(self, set_desc = True, desc = None):
895         server = self._build_minimal_create_server_request()
896         if set_desc:
897             server['description'] = desc
898         post = {'server': server}
899         response = self.api.api_post('/servers', post).body
900         return (server, response['server'])
901 
902     def _update_server(self, server_id, set_desc = True, desc = None):
903         new_name = integrated_helpers.generate_random_alphanumeric(8)
904         server = {'server': {'name': new_name}}
905         if set_desc:
906             server['server']['description'] = desc
907         self.api.api_put('/servers/%s' % server_id, server)
908 
909     def _rebuild_server(self, server_id, set_desc = True, desc = None):
910         new_name = integrated_helpers.generate_random_alphanumeric(8)
911         post = {}
912         post['rebuild'] = {
913             "name": new_name,
914             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
915             self._access_ipv4_parameter: "172.19.0.2",
916             self._access_ipv6_parameter: "fe80::2",
917             "metadata": {'some': 'thing'},
918         }
919         post['rebuild'].update(self._get_access_ips_params())
920         if set_desc:
921             post['rebuild']['description'] = desc
922         self.api.api_post('/servers/%s/action' % server_id, post)
923 
924     def _create_server_and_verify(self, set_desc = True, expected_desc = None):
925         # Creates a server with a description and verifies it is
926         # in the GET responses.
927         created_server_id = self._create_server(set_desc,
928                                                 expected_desc)[1]['id']
929         self._verify_server_description(created_server_id, expected_desc)
930         self._delete_server(created_server_id)
931 
932     def _update_server_and_verify(self, server_id, set_desc = True,
933                                   expected_desc = None):
934         # Updates a server with a description and verifies it is
935         # in the GET responses.
936         self._update_server(server_id, set_desc, expected_desc)
937         self._verify_server_description(server_id, expected_desc)
938 
939     def _rebuild_server_and_verify(self, server_id, set_desc = True,
940                                   expected_desc = None):
941         # Rebuilds a server with a description and verifies it is
942         # in the GET responses.
943         self._rebuild_server(server_id, set_desc, expected_desc)
944         self._verify_server_description(server_id, expected_desc)
945 
946     def _verify_server_description(self, server_id, expected_desc = None,
947                                    desc_in_resp = True):
948         # Calls GET on the servers and verifies that the description
949         # is set as expected in the response, or not set at all.
950         response = self.api.api_get('/servers/%s' % server_id)
951         found_server = response.body['server']
952         self.assertEqual(server_id, found_server['id'])
953         if desc_in_resp:
954             # Verify the description is set as expected (can be None)
955             self.assertEqual(expected_desc, found_server.get('description'))
956         else:
957             # Verify the description is not included in the response.
958             self.assertNotIn('description', found_server)
959 
960         servers = self.api.api_get('/servers/detail').body['servers']
961         server_map = {server['id']: server for server in servers}
962         found_server = server_map.get(server_id)
963         self.assertTrue(found_server)
964         if desc_in_resp:
965             # Verify the description is set as expected (can be None)
966             self.assertEqual(expected_desc, found_server.get('description'))
967         else:
968             # Verify the description is not included in the response.
969             self.assertNotIn('description', found_server)
970 
971     def _create_assertRaisesRegex(self, desc):
972         # Verifies that a 400 error is thrown on create server
973         with self.assertRaisesRegex(client.OpenStackApiException,
974                                     ".*Unexpected status code.*") as cm:
975             self._create_server(True, desc)
976             self.assertEqual(400, cm.exception.response.status_code)
977 
978     def _update_assertRaisesRegex(self, server_id, desc):
979         # Verifies that a 400 error is thrown on update server
980         with self.assertRaisesRegex(client.OpenStackApiException,
981                                     ".*Unexpected status code.*") as cm:
982             self._update_server(server_id, True, desc)
983             self.assertEqual(400, cm.exception.response.status_code)
984 
985     def _rebuild_assertRaisesRegex(self, server_id, desc):
986         # Verifies that a 400 error is thrown on rebuild server
987         with self.assertRaisesRegex(client.OpenStackApiException,
988                                     ".*Unexpected status code.*") as cm:
989             self._rebuild_server(server_id, True, desc)
990             self.assertEqual(400, cm.exception.response.status_code)
991 
992     def test_create_server_with_description(self):
993         self.api.microversion = '2.19'
994         # Create and get a server with a description
995         self._create_server_and_verify(True, 'test description')
996         # Create and get a server with an empty description
997         self._create_server_and_verify(True, '')
998         # Create and get a server with description set to None
999         self._create_server_and_verify()
1000         # Create and get a server without setting the description
1001         self._create_server_and_verify(False)
1002 
1003     def test_update_server_with_description(self):
1004         self.api.microversion = '2.19'
1005         # Create a server with an initial description
1006         server_id = self._create_server(True, 'test desc 1')[1]['id']
1007 
1008         # Update and get the server with a description
1009         self._update_server_and_verify(server_id, True, 'updated desc')
1010         # Update and get the server name without changing the description
1011         self._update_server_and_verify(server_id, False, 'updated desc')
1012         # Update and get the server with an empty description
1013         self._update_server_and_verify(server_id, True, '')
1014         # Update and get the server by removing the description (set to None)
1015         self._update_server_and_verify(server_id)
1016         # Update and get the server with a 2nd new description
1017         self._update_server_and_verify(server_id, True, 'updated desc2')
1018 
1019         # Cleanup
1020         self._delete_server(server_id)
1021 
1022     def test_rebuild_server_with_description(self):
1023         self.api.microversion = '2.19'
1024 
1025         # Create a server with an initial description
1026         server = self._create_server(True, 'test desc 1')[1]
1027         server_id = server['id']
1028         self._wait_for_state_change(server, 'BUILD')
1029 
1030         # Rebuild and get the server with a description
1031         self._rebuild_server_and_verify(server_id, True, 'updated desc')
1032         # Rebuild and get the server name without changing the description
1033         self._rebuild_server_and_verify(server_id, False, 'updated desc')
1034         # Rebuild and get the server with an empty description
1035         self._rebuild_server_and_verify(server_id, True, '')
1036         # Rebuild and get the server by removing the description (set to None)
1037         self._rebuild_server_and_verify(server_id)
1038         # Rebuild and get the server with a 2nd new description
1039         self._rebuild_server_and_verify(server_id, True, 'updated desc2')
1040 
1041         # Cleanup
1042         self._delete_server(server_id)
1043 
1044     def test_version_compatibility(self):
1045         # Create a server with microversion v2.19 and a description.
1046         self.api.microversion = '2.19'
1047         server_id = self._create_server(True, 'test desc 1')[1]['id']
1048         # Verify that the description is not included on V2.18 GETs
1049         self.api.microversion = '2.18'
1050         self._verify_server_description(server_id, desc_in_resp = False)
1051         # Verify that updating the server with description on V2.18
1052         # results in a 400 error
1053         self._update_assertRaisesRegex(server_id, 'test update 2.18')
1054         # Verify that rebuilding the server with description on V2.18
1055         # results in a 400 error
1056         self._rebuild_assertRaisesRegex(server_id, 'test rebuild 2.18')
1057 
1058         # Cleanup
1059         self._delete_server(server_id)
1060 
1061         # Create a server on V2.18 and verify that the description
1062         # defaults to the name on a V2.19 GET
1063         server_req, response = self._create_server(False)
1064         server_id = response['id']
1065         self.api.microversion = '2.19'
1066         self._verify_server_description(server_id, server_req['name'])
1067 
1068         # Cleanup
1069         self._delete_server(server_id)
1070 
1071         # Verify that creating a server with description on V2.18
1072         # results in a 400 error
1073         self.api.microversion = '2.18'
1074         self._create_assertRaisesRegex('test create 2.18')
1075 
1076     def test_description_errors(self):
1077         self.api.microversion = '2.19'
1078         # Create servers with invalid descriptions.  These throw 400.
1079         # Invalid unicode with non-printable control char
1080         self._create_assertRaisesRegex(u'invalid\0dstring')
1081         # Description is longer than 255 chars
1082         self._create_assertRaisesRegex('x' * 256)
1083 
1084         # Update and rebuild servers with invalid descriptions.
1085         # These throw 400.
1086         server_id = self._create_server(True, "desc")[1]['id']
1087         # Invalid unicode with non-printable control char
1088         self._update_assertRaisesRegex(server_id, u'invalid\u0604string')
1089         self._rebuild_assertRaisesRegex(server_id, u'invalid\u0604string')
1090         # Description is longer than 255 chars
1091         self._update_assertRaisesRegex(server_id, 'x' * 256)
1092         self._rebuild_assertRaisesRegex(server_id, 'x' * 256)
1093 
1094 
1095 class ServerTestV220(ServersTestBase):
1096     api_major_version = 'v2.1'
1097 
1098     def setUp(self):
1099         super(ServerTestV220, self).setUp()
1100         self.api.microversion = '2.20'
1101         self.ctxt = context.get_admin_context()
1102 
1103     def _create_server(self):
1104         server = self._build_minimal_create_server_request()
1105         post = {'server': server}
1106         response = self.api.api_post('/servers', post).body
1107         return (server, response['server'])
1108 
1109     def _shelve_server(self):
1110         server = self._create_server()[1]
1111         server_id = server['id']
1112         self._wait_for_state_change(server, 'BUILD')
1113         self.api.post_server_action(server_id, {'shelve': None})
1114         return self._wait_for_state_change(server, 'ACTIVE')
1115 
1116     def _get_fake_bdms(self, ctxt):
1117         return block_device_obj.block_device_make_list(self.ctxt,
1118                     [fake_block_device.FakeDbBlockDeviceDict(
1119                     {'device_name': '/dev/vda',
1120                      'source_type': 'volume',
1121                      'destination_type': 'volume',
1122                      'volume_id': '5d721593-f033-4f6d-ab6f-b5b067e61bc4'})])
1123 
1124     def test_attach_detach_vol_to_shelved_offloaded_server_new_flow(self):
1125         self.flags(shelved_offload_time=0)
1126         found_server = self._shelve_server()
1127         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1128         server_id = found_server['id']
1129         fake_bdms = self._get_fake_bdms(self.ctxt)
1130 
1131         # Test attach volume
1132         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1133         with test.nested(mock.patch.object(compute_api.API,
1134                             '_check_volume_already_attached_to_instance'),
1135                          mock.patch.object(volume.cinder.API,
1136                                         'check_availability_zone'),
1137                          mock.patch.object(volume.cinder.API,
1138                                         'attachment_create'),
1139                          mock.patch.object(volume.cinder.API,
1140                                         'attachment_complete')
1141                          ) as (mock_check_vol_attached,
1142                                mock_check_av_zone, mock_attach_create,
1143                                mock_attachment_complete):
1144             mock_attach_create.return_value = {'id': uuids.volume}
1145             volume_attachment = {"volumeAttachment": {"volumeId":
1146                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1147             attach_response = self.api.api_post(
1148                              '/servers/%s/os-volume_attachments' % (server_id),
1149                              volume_attachment).body['volumeAttachment']
1150             self.assertTrue(mock_attach_create.called)
1151             mock_attachment_complete.assert_called_once_with(
1152                 mock.ANY, uuids.volume)
1153             self.assertIsNone(attach_response['device'])
1154 
1155         # Test detach volume
1156         with test.nested(mock.patch.object(objects.BlockDeviceMappingList,
1157                                            'get_by_instance_uuid'),
1158                          mock.patch.object(compute_api.API,
1159                                            '_local_cleanup_bdm_volumes')
1160                          ) as (mock_get_bdms, mock_clean_vols):
1161 
1162             mock_get_bdms.return_value = fake_bdms
1163             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1164             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1165                             (server_id, attachment_id))
1166             self.assertTrue(mock_clean_vols.called)
1167 
1168         self._delete_server(server_id)
1169 
1170 
1171 class ServerTestV269(ServersTestBase):
1172     api_major_version = 'v2.1'
1173     NUMBER_OF_CELLS = 3
1174 
1175     def setUp(self):
1176         super(ServerTestV269, self).setUp()
1177         self.api.microversion = '2.69'
1178 
1179         self.ctxt = context.get_admin_context()
1180         self.project_id = self.api.project_id
1181         self.cells = objects.CellMappingList.get_all(self.ctxt)
1182         self.down_cell_insts = []
1183         self.up_cell_insts = []
1184         self.down_cell_mappings = objects.CellMappingList()
1185         flavor = objects.Flavor(id=1, name='flavor1',
1186                                 memory_mb=256, vcpus=1,
1187                                 root_gb=1, ephemeral_gb=1,
1188                                 flavorid='1',
1189                                 swap=0, rxtx_factor=1.0,
1190                                 vcpu_weight=1,
1191                                 disabled=False,
1192                                 is_public=True,
1193                                 extra_specs={},
1194                                 projects=[])
1195         _info_cache = objects.InstanceInfoCache(context)
1196         objects.InstanceInfoCache._from_db_object(context, _info_cache,
1197             test_instance_info_cache.fake_info_cache)
1198         # cell1 and cell2 will be the down cells while
1199         # cell0 and cell3 will be the up cells.
1200         down_cell_names = ['cell1', 'cell2']
1201         for cell in self.cells:
1202             # create 2 instances and their mappings in all the 4 cells
1203             for i in range(2):
1204                 with context.target_cell(self.ctxt, cell) as cctxt:
1205                     inst = objects.Instance(
1206                         context=cctxt,
1207                         project_id=self.project_id,
1208                         user_id=self.project_id,
1209                         instance_type_id=flavor.id,
1210                         hostname='%s-inst%i' % (cell.name, i),
1211                         flavor=flavor,
1212                         info_cache=_info_cache,
1213                         display_name='server-test')
1214                     inst.create()
1215                 im = objects.InstanceMapping(context=self.ctxt,
1216                                              instance_uuid=inst.uuid,
1217                                              cell_mapping=cell,
1218                                              project_id=self.project_id,
1219                                              queued_for_delete=False)
1220                 im.create()
1221                 if cell.name in down_cell_names:
1222                     self.down_cell_insts.append(inst.uuid)
1223                 else:
1224                     self.up_cell_insts.append(inst.uuid)
1225             # In cell1 and cell3 add a third instance in a different project
1226             # to show the --all-tenants case.
1227             if cell.name == 'cell1' or cell.name == 'cell3':
1228                 with context.target_cell(self.ctxt, cell) as cctxt:
1229                     inst = objects.Instance(
1230                         context=cctxt,
1231                         project_id='faker',
1232                         user_id='faker',
1233                         instance_type_id=flavor.id,
1234                         hostname='%s-inst%i' % (cell.name, 3),
1235                         flavor=flavor,
1236                         info_cache=_info_cache,
1237                         display_name='server-test')
1238                     inst.create()
1239                 im = objects.InstanceMapping(context=self.ctxt,
1240                                              instance_uuid=inst.uuid,
1241                                              cell_mapping=cell,
1242                                              project_id='faker',
1243                                              queued_for_delete=False)
1244                 im.create()
1245             if cell.name in down_cell_names:
1246                 self.down_cell_mappings.objects.append(cell)
1247         self.useFixture(nova_fixtures.DownCellFixture(self.down_cell_mappings))
1248 
1249     def test_get_servers_with_down_cells(self):
1250         servers = self.api.get_servers(detail=False)
1251         # 4 servers from the up cells and 4 servers from the down cells
1252         self.assertEqual(8, len(servers))
1253         for server in servers:
1254             if 'name' not in server:
1255                 # server is in the down cell.
1256                 self.assertEqual('UNKNOWN', server['status'])
1257                 self.assertIn(server['id'], self.down_cell_insts)
1258                 self.assertIn('links', server)
1259                 # the partial construct will have only the above 3 keys
1260                 self.assertEqual(3, len(server))
1261             else:
1262                 # server in up cell
1263                 self.assertIn(server['id'], self.up_cell_insts)
1264                 # has all the keys
1265                 self.assertEqual(server['name'], 'server-test')
1266                 self.assertIn('links', server)
1267 
1268     def test_get_servers_detail_with_down_cells(self):
1269         servers = self.api.get_servers()
1270         # 4 servers from the up cells and 4 servers from the down cells
1271         self.assertEqual(8, len(servers))
1272         for server in servers:
1273             if 'user_id' not in server:
1274                 # server is in the down cell.
1275                 self.assertEqual('UNKNOWN', server['status'])
1276                 self.assertIn(server['id'], self.down_cell_insts)
1277                 # the partial construct will have only 5 keys:
1278                 # created, tenant_id, status, id and links.
1279                 self.assertEqual(5, len(server))
1280             else:
1281                 # server in up cell
1282                 self.assertIn(server['id'], self.up_cell_insts)
1283                 # has all the keys
1284                 self.assertEqual(server['user_id'], self.project_id)
1285                 self.assertIn('image', server)
1286 
1287     def test_get_servers_detail_limits_with_down_cells(self):
1288         servers = self.api.get_servers(search_opts={'limit': 5})
1289         # 4 servers from the up cells since we skip down cell
1290         # results by default for paging.
1291         self.assertEqual(4, len(servers), servers)
1292         for server in servers:
1293             # server in up cell
1294             self.assertIn(server['id'], self.up_cell_insts)
1295             # has all the keys
1296             self.assertEqual(server['user_id'], self.project_id)
1297             self.assertIn('image', server)
1298 
1299     def test_get_servers_detail_limits_with_down_cells_the_500_gift(self):
1300         self.flags(list_records_by_skipping_down_cells=False, group='api')
1301         # We get an API error with a 500 response code since the
1302         # list_records_by_skipping_down_cells config option is False.
1303         exp = self.assertRaises(client.OpenStackApiException,
1304                                 self.api.get_servers,
1305                                 search_opts={'limit': 5})
1306         self.assertEqual(500, exp.response.status_code)
1307         self.assertIn('NovaException', six.text_type(exp))
1308 
1309     def test_get_servers_detail_marker_in_down_cells(self):
1310         marker = self.down_cell_insts[2]
1311         # It will fail with a 500 if the marker is in the down cell.
1312         exp = self.assertRaises(client.OpenStackApiException,
1313                                 self.api.get_servers,
1314                                 search_opts={'marker': marker})
1315         self.assertEqual(500, exp.response.status_code)
1316         self.assertIn('oslo_db.exception.DBError', six.text_type(exp))
1317 
1318     def test_get_servers_detail_marker_sorting(self):
1319         marker = self.up_cell_insts[1]
1320         # It will give the results from the up cell if
1321         # list_records_by_skipping_down_cells config option is True.
1322         servers = self.api.get_servers(search_opts={'marker': marker,
1323                                                     'sort_key': "created_at",
1324                                                     'sort_dir': "asc"})
1325         # since there are 4 servers from the up cells, when giving the
1326         # second instance as marker, sorted by creation time in ascending
1327         # third and fourth instances will be returned.
1328         self.assertEqual(2, len(servers))
1329         for server in servers:
1330             self.assertIn(
1331                 server['id'], [self.up_cell_insts[2], self.up_cell_insts[3]])
1332 
1333     def test_get_servers_detail_non_admin_with_deleted_flag(self):
1334         # if list_records_by_skipping_down_cells config option is True
1335         # this deleted option should be ignored and the rest of the instances
1336         # from the up cells and the partial results from the down cells should
1337         # be returned.
1338         # Set the policy so we don't have permission to allow
1339         # all filters but are able to get server details.
1340         servers_rule = 'os_compute_api:servers:detail'
1341         extraspec_rule = 'os_compute_api:servers:allow_all_filters'
1342         self.policy.set_rules({
1343             extraspec_rule: 'rule:admin_api',
1344             servers_rule: '@'})
1345         servers = self.api.get_servers(search_opts={'deleted': True})
1346         # gets 4 results from up cells and 4 from down cells.
1347         self.assertEqual(8, len(servers))
1348         for server in servers:
1349             if "image" not in server:
1350                 self.assertIn(server['id'], self.down_cell_insts)
1351             else:
1352                 self.assertIn(server['id'], self.up_cell_insts)
1353 
1354     def test_get_servers_detail_filters(self):
1355         # We get the results only from the up cells, this ignoring the down
1356         # cells if list_records_by_skipping_down_cells config option is True.
1357         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1358             api_version='v2.1'))
1359         self.admin_api = api_fixture.admin_api
1360         self.admin_api.microversion = '2.69'
1361         servers = self.admin_api.get_servers(
1362             search_opts={'hostname': "cell3-inst0"})
1363         self.assertEqual(1, len(servers))
1364         self.assertEqual(self.up_cell_insts[2], servers[0]['id'])
1365 
1366     def test_get_servers_detail_all_tenants_with_down_cells(self):
1367         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1368             api_version='v2.1'))
1369         self.admin_api = api_fixture.admin_api
1370         self.admin_api.microversion = '2.69'
1371         servers = self.admin_api.get_servers(search_opts={'all_tenants': True})
1372         # 4 servers from the up cells and 4 servers from the down cells
1373         # plus the 2 instances from cell1 and cell3 which are in a different
1374         # project.
1375         self.assertEqual(10, len(servers))
1376         for server in servers:
1377             if 'user_id' not in server:
1378                 # server is in the down cell.
1379                 self.assertEqual('UNKNOWN', server['status'])
1380                 if server['tenant_id'] != 'faker':
1381                     self.assertIn(server['id'], self.down_cell_insts)
1382                 # the partial construct will have only 5 keys:
1383                 # created, tenant_id, status, id and links
1384                 self.assertEqual(5, len(server))
1385             else:
1386                 # server in up cell
1387                 if server['tenant_id'] != 'faker':
1388                     self.assertIn(server['id'], self.up_cell_insts)
1389                     self.assertEqual(server['user_id'], self.project_id)
1390                 self.assertIn('image', server)
1391 
1392 
1393 class ServerRebuildTestCase(integrated_helpers._IntegratedTestBase,
1394                             integrated_helpers.InstanceHelperMixin):
1395     api_major_version = 'v2.1'
1396     # We have to cap the microversion at 2.38 because that's the max we
1397     # can use to update image metadata via our compute images proxy API.
1398     microversion = '2.38'
1399 
1400     def _disable_compute_for(self, server):
1401         # Refresh to get its host
1402         server = self.api.get_server(server['id'])
1403         host = server['OS-EXT-SRV-ATTR:host']
1404 
1405         # Disable the service it is on
1406         self.api_fixture.admin_api.put_service('disable',
1407                                                {'host': host,
1408                                                 'binary': 'nova-compute'})
1409 
1410     def test_rebuild_with_image_novalidhost(self):
1411         """Creates a server with an image that is valid for the single compute
1412         that we have. Then rebuilds the server, passing in an image with
1413         metadata that does not fit the single compute which should result in
1414         a NoValidHost error. The ImagePropertiesFilter filter is enabled by
1415         default so that should filter out the host based on the image meta.
1416         """
1417 
1418         self.compute2 = self.start_service('compute', host='host2')
1419 
1420         # We hard-code from a fake image since we can't get images
1421         # via the compute /images proxy API with microversion > 2.35.
1422         original_image_ref = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
1423         server_req_body = {
1424             'server': {
1425                 'imageRef': original_image_ref,
1426                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1427                 'name': 'test_rebuild_with_image_novalidhost',
1428                 # We don't care about networking for this test. This requires
1429                 # microversion >= 2.37.
1430                 'networks': 'none'
1431             }
1432         }
1433         server = self.api.post_server(server_req_body)
1434         self._wait_for_state_change(self.api, server, 'ACTIVE')
1435 
1436         # Disable the host we're on so ComputeFilter would have ruled it out
1437         # normally
1438         self._disable_compute_for(server)
1439 
1440         # Now update the image metadata to be something that won't work with
1441         # the fake compute driver we're using since the fake driver has an
1442         # "x86_64" architecture.
1443         rebuild_image_ref = (
1444             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1445         self.api.put_image_meta_key(
1446             rebuild_image_ref, 'hw_architecture', 'unicore32')
1447         # Now rebuild the server with that updated image and it should result
1448         # in a NoValidHost failure from the scheduler.
1449         rebuild_req_body = {
1450             'rebuild': {
1451                 'imageRef': rebuild_image_ref
1452             }
1453         }
1454         # Since we're using the CastAsCall fixture, the NoValidHost error
1455         # should actually come back to the API and result in a 500 error.
1456         # Normally the user would get a 202 response because nova-api RPC casts
1457         # to nova-conductor which RPC calls the scheduler which raises the
1458         # NoValidHost. We can mimic the end user way to figure out the failure
1459         # by looking for the failed 'rebuild' instance action event.
1460         self.api.api_post('/servers/%s/action' % server['id'],
1461                           rebuild_req_body, check_response_status=[500])
1462         # Look for the failed rebuild action.
1463         self._wait_for_action_fail_completion(
1464             server, instance_actions.REBUILD, 'rebuild_server',
1465             # Before microversion 2.51 events are only returned for instance
1466             # actions if you're an admin.
1467             self.api_fixture.admin_api)
1468         # Assert the server image_ref was rolled back on failure.
1469         server = self.api.get_server(server['id'])
1470         self.assertEqual(original_image_ref, server['image']['id'])
1471 
1472         # The server should be in ERROR state
1473         self.assertEqual('ERROR', server['status'])
1474         self.assertIn('No valid host', server['fault']['message'])
1475 
1476         # Rebuild it again with the same bad image to make sure it's rejected
1477         # again. Since we're using CastAsCall here, there is no 202 from the
1478         # API, and the exception from conductor gets passed back through the
1479         # API.
1480         ex = self.assertRaises(
1481             client.OpenStackApiException, self.api.api_post,
1482             '/servers/%s/action' % server['id'], rebuild_req_body)
1483         self.assertIn('NoValidHost', six.text_type(ex))
1484 
1485     # A rebuild to the same host should never attempt a rebuild claim.
1486     @mock.patch('nova.compute.resource_tracker.ResourceTracker.rebuild_claim',
1487                 new_callable=mock.NonCallableMock)
1488     def test_rebuild_with_new_image(self, mock_rebuild_claim):
1489         """Rebuilds a server with a different image which will run it through
1490         the scheduler to validate the image is still OK with the compute host
1491         that the instance is running on.
1492 
1493         Validates that additional resources are not allocated against the
1494         instance.host in Placement due to the rebuild on same host.
1495         """
1496         admin_api = self.api_fixture.admin_api
1497         admin_api.microversion = '2.53'
1498 
1499         def _get_provider_uuid_by_host(host):
1500             resp = admin_api.api_get(
1501                 'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body
1502             return resp['hypervisors'][0]['id']
1503 
1504         def _get_provider_usages(provider_uuid):
1505             return self.placement_api.get(
1506                 '/resource_providers/%s/usages' % provider_uuid).body['usages']
1507 
1508         def _get_allocations_by_server_uuid(server_uuid):
1509             return self.placement_api.get(
1510                 '/allocations/%s' % server_uuid).body['allocations']
1511 
1512         def _set_provider_inventory(rp_uuid, resource_class, inventory):
1513             # Get the resource provider generation for the inventory update.
1514             rp = self.placement_api.get(
1515                 '/resource_providers/%s' % rp_uuid).body
1516             inventory['resource_provider_generation'] = rp['generation']
1517             return self.placement_api.put(
1518                 '/resource_providers/%s/inventories/%s' %
1519                 (rp_uuid, resource_class), inventory).body
1520 
1521         def assertFlavorMatchesAllocation(flavor, allocation):
1522             self.assertEqual(flavor['vcpus'], allocation['VCPU'])
1523             self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])
1524             self.assertEqual(flavor['disk'], allocation['DISK_GB'])
1525 
1526         nodename = self.compute.manager._get_nodename(None)
1527         rp_uuid = _get_provider_uuid_by_host(nodename)
1528         # make sure we start with no usage on the compute node
1529         rp_usages = _get_provider_usages(rp_uuid)
1530         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
1531 
1532         server_req_body = {
1533             'server': {
1534                 # We hard-code from a fake image since we can't get images
1535                 # via the compute /images proxy API with microversion > 2.35.
1536                 'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',
1537                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1538                 'name': 'test_rebuild_with_new_image',
1539                 # We don't care about networking for this test. This requires
1540                 # microversion >= 2.37.
1541                 'networks': 'none'
1542             }
1543         }
1544         server = self.api.post_server(server_req_body)
1545         self._wait_for_state_change(self.api, server, 'ACTIVE')
1546 
1547         flavor = self.api.api_get('/flavors/1').body['flavor']
1548 
1549         # make the compute node full and ensure rebuild still succeed
1550         _set_provider_inventory(rp_uuid, "VCPU", {"total": 1})
1551 
1552         # There should be usage for the server on the compute node now.
1553         rp_usages = _get_provider_usages(rp_uuid)
1554         assertFlavorMatchesAllocation(flavor, rp_usages)
1555         allocs = _get_allocations_by_server_uuid(server['id'])
1556         self.assertIn(rp_uuid, allocs)
1557         allocs = allocs[rp_uuid]['resources']
1558         assertFlavorMatchesAllocation(flavor, allocs)
1559 
1560         rebuild_image_ref = (
1561             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1562         # Now rebuild the server with a different image.
1563         rebuild_req_body = {
1564             'rebuild': {
1565                 'imageRef': rebuild_image_ref
1566             }
1567         }
1568         self.api.api_post('/servers/%s/action' % server['id'],
1569                           rebuild_req_body)
1570         self._wait_for_server_parameter(
1571             self.api, server, {'OS-EXT-STS:task_state': None})
1572 
1573         # The usage and allocations should not have changed.
1574         rp_usages = _get_provider_usages(rp_uuid)
1575         assertFlavorMatchesAllocation(flavor, rp_usages)
1576 
1577         allocs = _get_allocations_by_server_uuid(server['id'])
1578         self.assertIn(rp_uuid, allocs)
1579         allocs = allocs[rp_uuid]['resources']
1580         assertFlavorMatchesAllocation(flavor, allocs)
1581 
1582     def test_volume_backed_rebuild_different_image(self):
1583         """Tests that trying to rebuild a volume-backed instance with a
1584         different image than what is in the root disk of the root volume
1585         will result in a 400 BadRequest error.
1586         """
1587         self.useFixture(nova_fixtures.CinderFixture(self))
1588         # First create our server as normal.
1589         server_req_body = {
1590             # There is no imageRef because this is boot from volume.
1591             'server': {
1592                 'flavorRef': '1',  # m1.tiny from DefaultFlavorsFixture,
1593                 'name': 'test_volume_backed_rebuild_different_image',
1594                 # We don't care about networking for this test. This requires
1595                 # microversion >= 2.37.
1596                 'networks': 'none',
1597                 'block_device_mapping_v2': [{
1598                     'boot_index': 0,
1599                     'uuid':
1600                     nova_fixtures.CinderFixture.IMAGE_BACKED_VOL,
1601                     'source_type': 'volume',
1602                     'destination_type': 'volume'
1603                 }]
1604             }
1605         }
1606         server = self.api.post_server(server_req_body)
1607         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
1608         # For a volume-backed server, the image ref will be an empty string
1609         # in the server response.
1610         self.assertEqual('', server['image'])
1611 
1612         # Now rebuild the server with a different image than was used to create
1613         # our fake volume.
1614         rebuild_image_ref = (
1615             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1616         rebuild_req_body = {
1617             'rebuild': {
1618                 'imageRef': rebuild_image_ref
1619             }
1620         }
1621         resp = self.api.api_post('/servers/%s/action' % server['id'],
1622                                  rebuild_req_body, check_response_status=[400])
1623         # Assert that we failed because of the image change and not something
1624         # else.
1625         self.assertIn('Unable to rebuild with a different image for a '
1626                       'volume-backed server', six.text_type(resp))
1627 
1628 
1629 class ServerMovingTests(integrated_helpers.ProviderUsageBaseTestCase):
1630     """Tests moving servers while checking the resource allocations and usages
1631 
1632     These tests use two compute hosts. Boot a server on one of them then try to
1633     move the server to the other. At every step resource allocation of the
1634     server and the resource usages of the computes are queried from placement
1635     API and asserted.
1636     """
1637 
1638     REQUIRES_LOCKING = True
1639     # NOTE(danms): The test defaults to using SmallFakeDriver,
1640     # which only has one vcpu, which can't take the doubled allocation
1641     # we're now giving it. So, use the bigger MediumFakeDriver here.
1642     compute_driver = 'fake.MediumFakeDriver'
1643 
1644     def setUp(self):
1645         super(ServerMovingTests, self).setUp()
1646         fake_notifier.stub_notifier(self)
1647         self.addCleanup(fake_notifier.reset)
1648 
1649         self.compute1 = self._start_compute(host='host1')
1650         self.compute2 = self._start_compute(host='host2')
1651 
1652         flavors = self.api.get_flavors()
1653         self.flavor1 = flavors[0]
1654         self.flavor2 = flavors[1]
1655         # create flavor3 which has less MEMORY_MB but more DISK_GB than flavor2
1656         flavor_body = {'flavor':
1657                            {'name': 'test_flavor3',
1658                             'ram': int(self.flavor2['ram'] / 2),
1659                             'vcpus': 1,
1660                             'disk': self.flavor2['disk'] * 2,
1661                             'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e3'
1662                             }}
1663 
1664         self.flavor3 = self.api.post_flavor(flavor_body)
1665 
1666     def _other_hostname(self, host):
1667         other_host = {'host1': 'host2',
1668                       'host2': 'host1'}
1669         return other_host[host]
1670 
1671     def _run_periodics(self):
1672         # NOTE(jaypipes): We always run periodics in the same order: first on
1673         # compute1, then on compute2. However, we want to test scenarios when
1674         # the periodics run at different times during mover operations. This is
1675         # why we have the "reverse" tests which simply switch the source and
1676         # dest host while keeping the order in which we run the
1677         # periodics. This effectively allows us to test the matrix of timing
1678         # scenarios during move operations.
1679         ctx = context.get_admin_context()
1680         LOG.info('Running periodic for compute1 (%s)',
1681             self.compute1.manager.host)
1682         self.compute1.manager.update_available_resource(ctx)
1683         LOG.info('Running periodic for compute2 (%s)',
1684             self.compute2.manager.host)
1685         self.compute2.manager.update_available_resource(ctx)
1686         LOG.info('Finished with periodics')
1687 
1688     def test_resize_revert(self):
1689         self._test_resize_revert(dest_hostname='host1')
1690 
1691     def test_resize_revert_reverse(self):
1692         self._test_resize_revert(dest_hostname='host2')
1693 
1694     def test_resize_confirm(self):
1695         self._test_resize_confirm(dest_hostname='host1')
1696 
1697     def test_resize_confirm_reverse(self):
1698         self._test_resize_confirm(dest_hostname='host2')
1699 
1700     def _resize_and_check_allocations(self, server, old_flavor, new_flavor,
1701             source_rp_uuid, dest_rp_uuid):
1702         self.flags(allow_resize_to_same_host=False)
1703         resize_req = {
1704             'resize': {
1705                 'flavorRef': new_flavor['id']
1706             }
1707         }
1708         self._move_and_check_allocations(
1709             server, request=resize_req, old_flavor=old_flavor,
1710             new_flavor=new_flavor, source_rp_uuid=source_rp_uuid,
1711             dest_rp_uuid=dest_rp_uuid)
1712 
1713     def test_migration_confirm_resize_error(self):
1714         source_hostname = self.compute1.host
1715         dest_hostname = self.compute2.host
1716 
1717         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1718         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1719 
1720         server = self._boot_and_check_allocations(self.flavor1,
1721                                                   source_hostname)
1722 
1723         self._move_and_check_allocations(
1724             server, request={'migrate': None}, old_flavor=self.flavor1,
1725             new_flavor=self.flavor1, source_rp_uuid=source_rp_uuid,
1726             dest_rp_uuid=dest_rp_uuid)
1727 
1728         # Mock failure
1729         def fake_confirm_migration(context, migration, instance, network_info):
1730             raise exception.MigrationPreCheckError(
1731                 reason='test_migration_confirm_resize_error')
1732 
1733         with mock.patch('nova.virt.fake.FakeDriver.'
1734                         'confirm_migration',
1735                         side_effect=fake_confirm_migration):
1736 
1737             # Confirm the migration/resize and check the usages
1738             post = {'confirmResize': None}
1739             self.api.post_server_action(
1740                 server['id'], post, check_response_status=[204])
1741             server = self._wait_for_state_change(self.api, server, 'ERROR')
1742 
1743         # After confirming and error, we should have an allocation only on the
1744         # destination host
1745 
1746         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
1747         self.assertRequestMatchesUsage({'VCPU': 0,
1748                                         'MEMORY_MB': 0,
1749                                         'DISK_GB': 0}, source_rp_uuid)
1750         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1751                                            dest_rp_uuid)
1752 
1753         self._run_periodics()
1754 
1755         # Check we're still accurate after running the periodics
1756 
1757         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
1758         self.assertRequestMatchesUsage({'VCPU': 0,
1759                                         'MEMORY_MB': 0,
1760                                         'DISK_GB': 0}, source_rp_uuid)
1761         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1762                                            dest_rp_uuid)
1763 
1764         self._delete_and_check_allocations(server)
1765 
1766     def _test_resize_revert(self, dest_hostname):
1767         source_hostname = self._other_hostname(dest_hostname)
1768         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1769         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1770 
1771         server = self._boot_and_check_allocations(self.flavor1,
1772             source_hostname)
1773 
1774         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
1775             source_rp_uuid, dest_rp_uuid)
1776 
1777         # Revert the resize and check the usages
1778         post = {'revertResize': None}
1779         self.api.post_server_action(server['id'], post)
1780         self._wait_for_state_change(self.api, server, 'ACTIVE')
1781 
1782         # Make sure the RequestSpec.flavor matches the original flavor.
1783         ctxt = context.get_admin_context()
1784         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
1785         self.assertEqual(self.flavor1['id'], reqspec.flavor.flavorid)
1786 
1787         self._run_periodics()
1788 
1789         # the original host expected to have the old resource allocation
1790         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
1791 
1792         self.assertRequestMatchesUsage({'VCPU': 0,
1793                                         'MEMORY_MB': 0,
1794                                         'DISK_GB': 0}, dest_rp_uuid)
1795 
1796         # Check that the server only allocates resource from the original host
1797         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1798                                            source_rp_uuid)
1799 
1800         self._delete_and_check_allocations(server)
1801 
1802     def _test_resize_confirm(self, dest_hostname):
1803         source_hostname = self._other_hostname(dest_hostname)
1804         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1805         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1806 
1807         server = self._boot_and_check_allocations(self.flavor1,
1808             source_hostname)
1809 
1810         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
1811             source_rp_uuid, dest_rp_uuid)
1812 
1813         # Confirm the resize and check the usages
1814         post = {'confirmResize': None}
1815         self.api.post_server_action(
1816             server['id'], post, check_response_status=[204])
1817         self._wait_for_state_change(self.api, server, 'ACTIVE')
1818 
1819         # After confirming, we should have an allocation only on the
1820         # destination host
1821 
1822         # The target host usage should be according to the new flavor
1823         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
1824         self.assertRequestMatchesUsage({'VCPU': 0,
1825                                         'MEMORY_MB': 0,
1826                                         'DISK_GB': 0}, source_rp_uuid)
1827 
1828         # and the target host allocation should be according to the new flavor
1829         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
1830                                            dest_rp_uuid)
1831 
1832         self._run_periodics()
1833 
1834         # Check we're still accurate after running the periodics
1835 
1836         # and the target host usage should be according to the new flavor
1837         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
1838         self.assertRequestMatchesUsage({'VCPU': 0,
1839                                         'MEMORY_MB': 0,
1840                                         'DISK_GB': 0}, source_rp_uuid)
1841 
1842         # and the server allocates only from the target host
1843         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
1844                                            dest_rp_uuid)
1845 
1846         self._delete_and_check_allocations(server)
1847 
1848     def test_resize_revert_same_host(self):
1849         # make sure that the test only uses a single host
1850         compute2_service_id = self.admin_api.get_services(
1851             host=self.compute2.host, binary='nova-compute')[0]['id']
1852         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
1853 
1854         hostname = self.compute1.manager.host
1855         rp_uuid = self._get_provider_uuid_by_host(hostname)
1856 
1857         server = self._boot_and_check_allocations(self.flavor2, hostname)
1858 
1859         self._resize_to_same_host_and_check_allocations(
1860             server, self.flavor2, self.flavor3, rp_uuid)
1861 
1862         # Revert the resize and check the usages
1863         post = {'revertResize': None}
1864         self.api.post_server_action(server['id'], post)
1865         self._wait_for_state_change(self.api, server, 'ACTIVE')
1866 
1867         self._run_periodics()
1868 
1869         # after revert only allocations due to the old flavor should remain
1870         self.assertFlavorMatchesUsage(rp_uuid, self.flavor2)
1871 
1872         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
1873                                            rp_uuid)
1874 
1875         self._delete_and_check_allocations(server)
1876 
1877     def test_resize_confirm_same_host(self):
1878         # make sure that the test only uses a single host
1879         compute2_service_id = self.admin_api.get_services(
1880             host=self.compute2.host, binary='nova-compute')[0]['id']
1881         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
1882 
1883         hostname = self.compute1.manager.host
1884         rp_uuid = self._get_provider_uuid_by_host(hostname)
1885 
1886         server = self._boot_and_check_allocations(self.flavor2, hostname)
1887 
1888         self._resize_to_same_host_and_check_allocations(
1889             server, self.flavor2, self.flavor3, rp_uuid)
1890 
1891         # Confirm the resize and check the usages
1892         post = {'confirmResize': None}
1893         self.api.post_server_action(
1894             server['id'], post, check_response_status=[204])
1895         self._wait_for_state_change(self.api, server, 'ACTIVE')
1896 
1897         self._run_periodics()
1898 
1899         # after confirm only allocations due to the new flavor should remain
1900         self.assertFlavorMatchesUsage(rp_uuid, self.flavor3)
1901 
1902         self.assertFlavorMatchesAllocation(self.flavor3, server['id'],
1903                                            rp_uuid)
1904 
1905         self._delete_and_check_allocations(server)
1906 
1907     def test_resize_not_enough_resource(self):
1908         # Try to resize to a flavor that requests more VCPU than what the
1909         # compute hosts has available and expect the resize to fail
1910 
1911         flavor_body = {'flavor':
1912                            {'name': 'test_too_big_flavor',
1913                             'ram': 1024,
1914                             'vcpus': fake.MediumFakeDriver.vcpus + 1,
1915                             'disk': 20,
1916                             }}
1917 
1918         big_flavor = self.api.post_flavor(flavor_body)
1919 
1920         dest_hostname = self.compute2.host
1921         source_hostname = self._other_hostname(dest_hostname)
1922         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1923         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1924 
1925         server = self._boot_and_check_allocations(
1926             self.flavor1, source_hostname)
1927 
1928         self.flags(allow_resize_to_same_host=False)
1929         resize_req = {
1930             'resize': {
1931                 'flavorRef': big_flavor['id']
1932             }
1933         }
1934 
1935         resp = self.api.post_server_action(
1936             server['id'], resize_req, check_response_status=[400])
1937         self.assertEqual(
1938             resp['badRequest']['message'],
1939             "No valid host was found. No valid host found for resize")
1940         server = self.admin_api.get_server(server['id'])
1941         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
1942 
1943         # only the source host shall have usages after the failed resize
1944         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
1945 
1946         # Check that the other provider has no usage
1947         self.assertRequestMatchesUsage(
1948             {'VCPU': 0,
1949              'MEMORY_MB': 0,
1950              'DISK_GB': 0}, dest_rp_uuid)
1951 
1952         # Check that the server only allocates resource from the host it is
1953         # booted on
1954         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
1955                                            source_rp_uuid)
1956 
1957         self._delete_and_check_allocations(server)
1958 
1959     def test_resize_delete_while_verify(self):
1960         """Test scenario where the server is deleted while in the
1961         VERIFY_RESIZE state and ensures the allocations are properly
1962         cleaned up from the source and target compute node resource providers.
1963         The _confirm_resize_on_deleting() method in the API is actually
1964         responsible for making sure the migration-based allocations get
1965         cleaned up by confirming the resize on the source host before deleting
1966         the server from the target host.
1967         """
1968         dest_hostname = 'host2'
1969         source_hostname = self._other_hostname(dest_hostname)
1970         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
1971         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
1972 
1973         server = self._boot_and_check_allocations(self.flavor1,
1974                                                   source_hostname)
1975 
1976         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
1977                                            source_rp_uuid, dest_rp_uuid)
1978 
1979         self._delete_and_check_allocations(server)
1980 
1981     def test_resize_confirm_assert_hypervisor_usage_no_periodics(self):
1982         """Resize confirm test for bug 1818914 to make sure the tracked
1983         resource usage in the os-hypervisors API (not placement) is as
1984         expected during a confirmed resize. This intentionally does not
1985         use _test_resize_confirm in order to avoid running periodics.
1986         """
1987         # There should be no usage from a server on either hypervisor.
1988         source_rp_uuid = self._get_provider_uuid_by_host('host1')
1989         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
1990         no_usage = {'vcpus': 0, 'disk': 0, 'ram': 0}
1991         for rp_uuid in (source_rp_uuid, dest_rp_uuid):
1992             self.assert_hypervisor_usage(
1993                 rp_uuid, no_usage, volume_backed=False)
1994 
1995         # Create the server and wait for it to be ACTIVE.
1996         server = self._boot_and_check_allocations(self.flavor1, 'host1')
1997 
1998         # There should be resource usage for flavor1 on the source host.
1999         self.assert_hypervisor_usage(
2000             source_rp_uuid, self.flavor1, volume_backed=False)
2001         # And still no usage on the dest host.
2002         self.assert_hypervisor_usage(
2003             dest_rp_uuid, no_usage, volume_backed=False)
2004 
2005         # Resize the server to flavor2 and wait for VERIFY_RESIZE.
2006         self.flags(allow_resize_to_same_host=False)
2007         resize_req = {
2008             'resize': {
2009                 'flavorRef': self.flavor2['id']
2010             }
2011         }
2012         self.api.post_server_action(server['id'], resize_req)
2013         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
2014 
2015         # There should be resource usage for flavor1 on the source host.
2016         self.assert_hypervisor_usage(
2017             source_rp_uuid, self.flavor1, volume_backed=False)
2018         # And resource usage for flavor2 on the target host.
2019         self.assert_hypervisor_usage(
2020             dest_rp_uuid, self.flavor2, volume_backed=False)
2021 
2022         # Now confirm the resize and check hypervisor usage again.
2023         self.api.post_server_action(server['id'], {'confirmResize': None})
2024         self._wait_for_state_change(self.api, server, 'ACTIVE')
2025 
2026         # There should no resource usage for flavor1 on the source host.
2027         self.assert_hypervisor_usage(
2028             source_rp_uuid, no_usage, volume_backed=False)
2029         # And resource usage for flavor2 should still be on the target host.
2030         self.assert_hypervisor_usage(
2031             dest_rp_uuid, self.flavor2, volume_backed=False)
2032 
2033         # Run periodics and make sure usage is still as expected.
2034         self._run_periodics()
2035         self.assert_hypervisor_usage(
2036             source_rp_uuid, no_usage, volume_backed=False)
2037         self.assert_hypervisor_usage(
2038             dest_rp_uuid, self.flavor2, volume_backed=False)
2039 
2040     def _wait_for_notification_event_type(self, event_type, max_retries=50):
2041         retry_counter = 0
2042         while True:
2043             if len(fake_notifier.NOTIFICATIONS) > 0:
2044                 for notification in fake_notifier.NOTIFICATIONS:
2045                     if notification.event_type == event_type:
2046                         return
2047             if retry_counter == max_retries:
2048                 self.fail('Wait for notification event type (%s) failed'
2049                           % event_type)
2050             retry_counter += 1
2051             time.sleep(0.1)
2052 
2053     def test_evacuate_with_no_compute(self):
2054         source_hostname = self.compute1.host
2055         dest_hostname = self.compute2.host
2056         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2057         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2058 
2059         # Disable compute service on destination host
2060         compute2_service_id = self.admin_api.get_services(
2061             host=dest_hostname, binary='nova-compute')[0]['id']
2062         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2063 
2064         server = self._boot_and_check_allocations(
2065             self.flavor1, source_hostname)
2066 
2067         # Force source compute down
2068         source_compute_id = self.admin_api.get_services(
2069             host=source_hostname, binary='nova-compute')[0]['id']
2070         self.compute1.stop()
2071         self.admin_api.put_service(
2072             source_compute_id, {'forced_down': 'true'})
2073 
2074         # Initialize fake_notifier
2075         fake_notifier.stub_notifier(self)
2076         fake_notifier.reset()
2077 
2078         # Initiate evacuation
2079         post = {'evacuate': {}}
2080         self.api.post_server_action(server['id'], post)
2081 
2082         # NOTE(elod.illes): Should be changed to non-polling solution when
2083         # patch https://review.opendev.org/#/c/482629/ gets merged:
2084         # fake_notifier.wait_for_versioned_notifications(
2085         #     'compute_task.rebuild_server')
2086         self._wait_for_notification_event_type('compute_task.rebuild_server')
2087 
2088         self._run_periodics()
2089 
2090         # There is no other host to evacuate to so the rebuild should put the
2091         # VM to ERROR state, but it should remain on source compute
2092         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2093                            'status': 'ERROR'}
2094         server = self._wait_for_server_parameter(self.api, server,
2095                                                  expected_params)
2096 
2097         # Check migrations
2098         migrations = self.api.get_migrations()
2099         self.assertEqual(1, len(migrations))
2100         self.assertEqual('evacuation', migrations[0]['migration_type'])
2101         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
2102         self.assertEqual(source_hostname, migrations[0]['source_compute'])
2103         self.assertEqual('error', migrations[0]['status'])
2104 
2105         # Restart source host
2106         self.admin_api.put_service(
2107             source_compute_id, {'forced_down': 'false'})
2108         self.compute1.start()
2109 
2110         self._run_periodics()
2111 
2112         # Check allocation and usages: should only use resources on source host
2113         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2114 
2115         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2116                                            source_rp_uuid)
2117         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2118         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2119 
2120         self._delete_and_check_allocations(server)
2121 
2122     def test_migrate_no_valid_host(self):
2123         source_hostname = self.compute1.host
2124         dest_hostname = self.compute2.host
2125         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2126         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2127 
2128         server = self._boot_and_check_allocations(
2129             self.flavor1, source_hostname)
2130 
2131         dest_compute_id = self.admin_api.get_services(
2132             host=dest_hostname, binary='nova-compute')[0]['id']
2133         self.compute2.stop()
2134         # force it down to avoid waiting for the service group to time out
2135         self.admin_api.put_service(
2136             dest_compute_id, {'forced_down': 'true'})
2137 
2138         # migrate the server
2139         post = {'migrate': None}
2140         ex = self.assertRaises(client.OpenStackApiException,
2141                                self.api.post_server_action,
2142                                server['id'], post)
2143         self.assertIn('No valid host', six.text_type(ex))
2144         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2145                            'status': 'ACTIVE'}
2146         self._wait_for_server_parameter(self.api, server, expected_params)
2147 
2148         self._run_periodics()
2149 
2150         # Expect to have allocation only on source_host
2151         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2152         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2153         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2154 
2155         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2156                                            source_rp_uuid)
2157 
2158         self._delete_and_check_allocations(server)
2159 
2160     def _test_evacuate(self, keep_hypervisor_state):
2161         source_hostname = self.compute1.host
2162         dest_hostname = self.compute2.host
2163         server = self._boot_and_check_allocations(
2164             self.flavor1, source_hostname)
2165 
2166         source_compute_id = self.admin_api.get_services(
2167             host=source_hostname, binary='nova-compute')[0]['id']
2168 
2169         self.compute1.stop()
2170         # force it down to avoid waiting for the service group to time out
2171         self.admin_api.put_service(
2172             source_compute_id, {'forced_down': 'true'})
2173 
2174         # evacuate the server
2175         post = {'evacuate': {}}
2176         self.api.post_server_action(
2177             server['id'], post)
2178         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2179                            'status': 'ACTIVE'}
2180         server = self._wait_for_server_parameter(self.api, server,
2181                                                  expected_params)
2182 
2183         # Expect to have allocation and usages on both computes as the
2184         # source compute is still down
2185         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2186         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2187 
2188         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2189 
2190         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2191 
2192         self._check_allocation_during_evacuate(
2193             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2194 
2195         # restart the source compute
2196         self.compute1 = self.restart_compute_service(
2197             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2198 
2199         self.admin_api.put_service(
2200             source_compute_id, {'forced_down': 'false'})
2201 
2202         source_usages = self._get_provider_usages(source_rp_uuid)
2203         self.assertEqual({'VCPU': 0,
2204                           'MEMORY_MB': 0,
2205                           'DISK_GB': 0},
2206                          source_usages)
2207 
2208         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2209 
2210         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2211                                            dest_rp_uuid)
2212 
2213         self._delete_and_check_allocations(server)
2214 
2215     def test_evacuate_instance_kept_on_the_hypervisor(self):
2216         self._test_evacuate(keep_hypervisor_state=True)
2217 
2218     def test_evacuate_clean_hypervisor(self):
2219         self._test_evacuate(keep_hypervisor_state=False)
2220 
2221     def _test_evacuate_forced_host(self, keep_hypervisor_state):
2222         """Evacuating a server with a forced host bypasses the scheduler
2223         which means conductor has to create the allocations against the
2224         destination node. This test recreates the scenarios and asserts
2225         the allocations on the source and destination nodes are as expected.
2226         """
2227         source_hostname = self.compute1.host
2228         dest_hostname = self.compute2.host
2229 
2230         # the ability to force evacuate a server is removed entirely in 2.68
2231         self.api.microversion = '2.67'
2232 
2233         server = self._boot_and_check_allocations(
2234             self.flavor1, source_hostname)
2235 
2236         source_compute_id = self.admin_api.get_services(
2237             host=source_hostname, binary='nova-compute')[0]['id']
2238 
2239         self.compute1.stop()
2240         # force it down to avoid waiting for the service group to time out
2241         self.admin_api.put_service(
2242             source_compute_id, {'forced_down': 'true'})
2243 
2244         # evacuate the server and force the destination host which bypasses
2245         # the scheduler
2246         post = {
2247             'evacuate': {
2248                 'host': dest_hostname,
2249                 'force': True
2250             }
2251         }
2252         self.api.post_server_action(server['id'], post)
2253         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2254                            'status': 'ACTIVE'}
2255         server = self._wait_for_server_parameter(self.api, server,
2256                                                  expected_params)
2257 
2258         # Run the periodics to show those don't modify allocations.
2259         self._run_periodics()
2260 
2261         # Expect to have allocation and usages on both computes as the
2262         # source compute is still down
2263         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2264         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2265 
2266         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2267 
2268         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2269 
2270         self._check_allocation_during_evacuate(
2271             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2272 
2273         # restart the source compute
2274         self.compute1 = self.restart_compute_service(
2275             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2276         self.admin_api.put_service(
2277             source_compute_id, {'forced_down': 'false'})
2278 
2279         # Run the periodics again to show they don't change anything.
2280         self._run_periodics()
2281 
2282         # When the source node starts up, the instance has moved so the
2283         # ResourceTracker should cleanup allocations for the source node.
2284         source_usages = self._get_provider_usages(source_rp_uuid)
2285         self.assertEqual(
2286             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
2287 
2288         # The usages/allocations should still exist on the destination node
2289         # after the source node starts back up.
2290         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2291 
2292         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2293                                            dest_rp_uuid)
2294 
2295         self._delete_and_check_allocations(server)
2296 
2297     def test_evacuate_forced_host_instance_kept_on_the_hypervisor(self):
2298         self._test_evacuate_forced_host(keep_hypervisor_state=True)
2299 
2300     def test_evacuate_forced_host_clean_hypervisor(self):
2301         self._test_evacuate_forced_host(keep_hypervisor_state=False)
2302 
2303     def test_evacuate_forced_host_v268(self):
2304         """Evacuating a server with a forced host was removed in API
2305         microversion 2.68. This test ensures that the request is rejected.
2306         """
2307         source_hostname = self.compute1.host
2308         dest_hostname = self.compute2.host
2309 
2310         server = self._boot_and_check_allocations(
2311             self.flavor1, source_hostname)
2312 
2313         # evacuate the server and force the destination host which bypasses
2314         # the scheduler
2315         post = {
2316             'evacuate': {
2317                 'host': dest_hostname,
2318                 'force': True
2319             }
2320         }
2321         ex = self.assertRaises(client.OpenStackApiException,
2322                                self.api.post_server_action,
2323                                server['id'], post)
2324         self.assertIn("'force' was unexpected", six.text_type(ex))
2325 
2326     # NOTE(gibi): there is a similar test in SchedulerOnlyChecksTargetTest but
2327     # we want this test here as well because ServerMovingTest is a parent class
2328     # of multiple test classes that run this test case with different compute
2329     # node setups.
2330     def test_evacuate_host_specified_but_not_forced(self):
2331         """Evacuating a server with a host but using the scheduler to create
2332         the allocations against the destination node. This test recreates the
2333         scenarios and asserts the allocations on the source and destination
2334         nodes are as expected.
2335         """
2336         source_hostname = self.compute1.host
2337         dest_hostname = self.compute2.host
2338 
2339         server = self._boot_and_check_allocations(
2340             self.flavor1, source_hostname)
2341 
2342         source_compute_id = self.admin_api.get_services(
2343             host=source_hostname, binary='nova-compute')[0]['id']
2344 
2345         self.compute1.stop()
2346         # force it down to avoid waiting for the service group to time out
2347         self.admin_api.put_service(
2348             source_compute_id, {'forced_down': 'true'})
2349 
2350         # evacuate the server specify the target but do not force the
2351         # destination host to use the scheduler to validate the target host
2352         post = {
2353             'evacuate': {
2354                 'host': dest_hostname,
2355             }
2356         }
2357         self.api.post_server_action(server['id'], post)
2358         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2359                            'status': 'ACTIVE'}
2360         server = self._wait_for_server_parameter(self.api, server,
2361                                                  expected_params)
2362 
2363         # Run the periodics to show those don't modify allocations.
2364         self._run_periodics()
2365 
2366         # Expect to have allocation and usages on both computes as the
2367         # source compute is still down
2368         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2369         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2370 
2371         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2372 
2373         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2374 
2375         self._check_allocation_during_evacuate(
2376             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2377 
2378         # restart the source compute
2379         self.compute1 = self.restart_compute_service(self.compute1)
2380         self.admin_api.put_service(
2381             source_compute_id, {'forced_down': 'false'})
2382 
2383         # Run the periodics again to show they don't change anything.
2384         self._run_periodics()
2385 
2386         # When the source node starts up, the instance has moved so the
2387         # ResourceTracker should cleanup allocations for the source node.
2388         source_usages = self._get_provider_usages(source_rp_uuid)
2389         self.assertEqual(
2390             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
2391 
2392         # The usages/allocations should still exist on the destination node
2393         # after the source node starts back up.
2394         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2395 
2396         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2397                                            dest_rp_uuid)
2398 
2399         self._delete_and_check_allocations(server)
2400 
2401     def _test_evacuate_claim_on_dest_fails(self, keep_hypervisor_state):
2402         """Tests that the allocations on the destination node are cleaned up
2403         when the rebuild move claim fails due to insufficient resources.
2404         """
2405         source_hostname = self.compute1.host
2406         dest_hostname = self.compute2.host
2407         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2408 
2409         server = self._boot_and_check_allocations(
2410             self.flavor1, source_hostname)
2411 
2412         source_compute_id = self.admin_api.get_services(
2413             host=source_hostname, binary='nova-compute')[0]['id']
2414 
2415         self.compute1.stop()
2416         # force it down to avoid waiting for the service group to time out
2417         self.admin_api.put_service(
2418             source_compute_id, {'forced_down': 'true'})
2419 
2420         # NOTE(mriedem): This isn't great, and I'd like to fake out the driver
2421         # to make the claim fail, by doing something like returning a too high
2422         # memory_mb overhead, but the limits dict passed to the claim is empty
2423         # so the claim test is considering it as unlimited and never actually
2424         # performs a claim test.
2425         def fake_move_claim(*args, **kwargs):
2426             # Assert the destination node allocation exists.
2427             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2428             raise exception.ComputeResourcesUnavailable(
2429                     reason='test_evacuate_claim_on_dest_fails')
2430 
2431         with mock.patch('nova.compute.claims.MoveClaim', fake_move_claim):
2432             # evacuate the server
2433             self.api.post_server_action(server['id'], {'evacuate': {}})
2434             # the migration will fail on the dest node and the instance will
2435             # go into error state
2436             server = self._wait_for_state_change(self.api, server, 'ERROR')
2437 
2438         # Run the periodics to show those don't modify allocations.
2439         self._run_periodics()
2440 
2441         # The allocation should still exist on the source node since it's
2442         # still down, and the allocation on the destination node should be
2443         # cleaned up.
2444         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2445 
2446         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2447 
2448         self.assertRequestMatchesUsage(
2449             {'VCPU': 0,
2450              'MEMORY_MB': 0,
2451              'DISK_GB': 0}, dest_rp_uuid)
2452 
2453         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2454                                            source_rp_uuid)
2455 
2456         # restart the source compute
2457         self.compute1 = self.restart_compute_service(
2458             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2459         self.admin_api.put_service(
2460             source_compute_id, {'forced_down': 'false'})
2461 
2462         # Run the periodics again to show they don't change anything.
2463         self._run_periodics()
2464 
2465         # The source compute shouldn't have cleaned up the allocation for
2466         # itself since the instance didn't move.
2467         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2468 
2469         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2470                                            source_rp_uuid)
2471 
2472     def test_evacuate_claim_on_dest_fails_instance_kept_on_the_hypervisor(
2473             self):
2474         self._test_evacuate_claim_on_dest_fails(keep_hypervisor_state=True)
2475 
2476     def test_evacuate_claim_on_dest_fails_clean_hypervisor(self):
2477         self._test_evacuate_claim_on_dest_fails(keep_hypervisor_state=False)
2478 
2479     def _test_evacuate_rebuild_on_dest_fails(self, keep_hypervisor_state):
2480         """Tests that the allocations on the destination node are cleaned up
2481         automatically when the claim is made but the actual rebuild
2482         via the driver fails.
2483 
2484         """
2485         source_hostname = self.compute1.host
2486         dest_hostname = self.compute2.host
2487         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2488 
2489         server = self._boot_and_check_allocations(
2490             self.flavor1, source_hostname)
2491 
2492         source_compute_id = self.admin_api.get_services(
2493             host=source_hostname, binary='nova-compute')[0]['id']
2494 
2495         self.compute1.stop()
2496         # force it down to avoid waiting for the service group to time out
2497         self.admin_api.put_service(
2498             source_compute_id, {'forced_down': 'true'})
2499 
2500         def fake_rebuild(*args, **kwargs):
2501             # Assert the destination node allocation exists.
2502             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2503             raise test.TestingException('test_evacuate_rebuild_on_dest_fails')
2504 
2505         with mock.patch.object(
2506                 self.compute2.driver, 'rebuild', fake_rebuild):
2507             # evacuate the server
2508             self.api.post_server_action(server['id'], {'evacuate': {}})
2509             # the migration will fail on the dest node and the instance will
2510             # go into error state
2511             server = self._wait_for_state_change(self.api, server, 'ERROR')
2512 
2513         # Run the periodics to show those don't modify allocations.
2514         self._run_periodics()
2515 
2516         # The allocation should still exist on the source node since it's
2517         # still down, and the allocation on the destination node should be
2518         # cleaned up.
2519         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2520 
2521         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2522 
2523         self.assertRequestMatchesUsage(
2524             {'VCPU': 0,
2525              'MEMORY_MB': 0,
2526              'DISK_GB': 0}, dest_rp_uuid)
2527 
2528         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2529                                            source_rp_uuid)
2530 
2531         # restart the source compute
2532         self.compute1 = self.restart_compute_service(
2533             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
2534         self.admin_api.put_service(
2535             source_compute_id, {'forced_down': 'false'})
2536 
2537         # Run the periodics again to show they don't change anything.
2538         self._run_periodics()
2539 
2540         # The source compute shouldn't have cleaned up the allocation for
2541         # itself since the instance didn't move.
2542         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2543 
2544         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2545                                            source_rp_uuid)
2546 
2547     def test_evacuate_rebuild_on_dest_fails_instance_kept_on_the_hypervisor(
2548             self):
2549         self._test_evacuate_rebuild_on_dest_fails(keep_hypervisor_state=True)
2550 
2551     def test_evacuate_rebuild_on_dest_fails_clean_hypervisor(self):
2552         self._test_evacuate_rebuild_on_dest_fails(keep_hypervisor_state=False)
2553 
2554     def _boot_then_shelve_and_check_allocations(self, hostname, rp_uuid):
2555         # avoid automatic shelve offloading
2556         self.flags(shelved_offload_time=-1)
2557         server = self._boot_and_check_allocations(
2558             self.flavor1, hostname)
2559         req = {
2560             'shelve': {}
2561         }
2562         self.api.post_server_action(server['id'], req)
2563         self._wait_for_state_change(self.api, server, 'SHELVED')
2564         # the host should maintain the existing allocation for this instance
2565         # while the instance is shelved
2566         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
2567         # Check that the server only allocates resource from the host it is
2568         # booted on
2569         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2570                                            rp_uuid)
2571         return server
2572 
2573     def test_shelve_unshelve(self):
2574         source_hostname = self.compute1.host
2575         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2576         server = self._boot_then_shelve_and_check_allocations(
2577             source_hostname, source_rp_uuid)
2578 
2579         req = {
2580             'unshelve': None
2581         }
2582         self.api.post_server_action(server['id'], req)
2583         self._wait_for_state_change(self.api, server, 'ACTIVE')
2584 
2585         # the host should have resource usage as the instance is ACTIVE
2586         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2587 
2588         # Check that the server only allocates resource from the host it is
2589         # booted on
2590         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2591                                            source_rp_uuid)
2592 
2593         self._delete_and_check_allocations(server)
2594 
2595     def _shelve_offload_and_check_allocations(self, server, source_rp_uuid):
2596         req = {
2597             'shelveOffload': {}
2598         }
2599         self.api.post_server_action(server['id'], req)
2600         self._wait_for_server_parameter(
2601             self.api, server, {'status': 'SHELVED_OFFLOADED',
2602                                'OS-EXT-SRV-ATTR:host': None,
2603                                'OS-EXT-AZ:availability_zone': ''})
2604         source_usages = self._get_provider_usages(source_rp_uuid)
2605         self.assertEqual({'VCPU': 0,
2606                           'MEMORY_MB': 0,
2607                           'DISK_GB': 0},
2608                          source_usages)
2609 
2610         allocations = self._get_allocations_by_server_uuid(server['id'])
2611         self.assertEqual(0, len(allocations))
2612 
2613     def test_shelve_offload_unshelve_diff_host(self):
2614         source_hostname = self.compute1.host
2615         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2616         server = self._boot_then_shelve_and_check_allocations(
2617             source_hostname, source_rp_uuid)
2618 
2619         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
2620 
2621         # unshelve after shelve offload will do scheduling. this test case
2622         # wants to test the scenario when the scheduler select a different host
2623         # to ushelve the instance. So we disable the original host.
2624         source_service_id = self.admin_api.get_services(
2625             host=source_hostname, binary='nova-compute')[0]['id']
2626         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
2627 
2628         req = {
2629             'unshelve': None
2630         }
2631         self.api.post_server_action(server['id'], req)
2632         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
2633         # unshelving an offloaded instance will call the scheduler so the
2634         # instance might end up on a different host
2635         current_hostname = server['OS-EXT-SRV-ATTR:host']
2636         self.assertEqual(current_hostname, self._other_hostname(
2637             source_hostname))
2638 
2639         # the host running the instance should have resource usage
2640         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
2641         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
2642 
2643         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2644                                            current_rp_uuid)
2645 
2646         self._delete_and_check_allocations(server)
2647 
2648     def test_shelve_offload_unshelve_same_host(self):
2649         source_hostname = self.compute1.host
2650         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2651         server = self._boot_then_shelve_and_check_allocations(
2652             source_hostname, source_rp_uuid)
2653 
2654         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
2655 
2656         # unshelve after shelve offload will do scheduling. this test case
2657         # wants to test the scenario when the scheduler select the same host
2658         # to ushelve the instance. So we disable the other host.
2659         source_service_id = self.admin_api.get_services(
2660             host=self._other_hostname(source_hostname),
2661             binary='nova-compute')[0]['id']
2662         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
2663 
2664         req = {
2665             'unshelve': None
2666         }
2667         self.api.post_server_action(server['id'], req)
2668         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
2669         # unshelving an offloaded instance will call the scheduler so the
2670         # instance might end up on a different host
2671         current_hostname = server['OS-EXT-SRV-ATTR:host']
2672         self.assertEqual(current_hostname, source_hostname)
2673 
2674         # the host running the instance should have resource usage
2675         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
2676         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
2677 
2678         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2679                                            current_rp_uuid)
2680 
2681         self._delete_and_check_allocations(server)
2682 
2683     def test_live_migrate_force(self):
2684         source_hostname = self.compute1.host
2685         dest_hostname = self.compute2.host
2686         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2687         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2688 
2689         # the ability to force live migrate a server is removed entirely in
2690         # 2.68
2691         self.api.microversion = '2.67'
2692 
2693         server = self._boot_and_check_allocations(
2694             self.flavor1, source_hostname)
2695 
2696         # live migrate the server and force the destination host which bypasses
2697         # the scheduler
2698         post = {
2699             'os-migrateLive': {
2700                 'host': dest_hostname,
2701                 'block_migration': True,
2702                 'force': True,
2703             }
2704         }
2705 
2706         self.api.post_server_action(server['id'], post)
2707         self._wait_for_server_parameter(self.api, server,
2708             {'OS-EXT-SRV-ATTR:host': dest_hostname,
2709              'status': 'ACTIVE'})
2710 
2711         self._run_periodics()
2712 
2713         # NOTE(danms): There should be no usage for the source
2714         self.assertRequestMatchesUsage(
2715             {'VCPU': 0,
2716              'MEMORY_MB': 0,
2717              'DISK_GB': 0}, source_rp_uuid)
2718 
2719         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2720 
2721         # the server has an allocation on only the dest node
2722         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2723                                            dest_rp_uuid)
2724 
2725         self._delete_and_check_allocations(server)
2726 
2727     def test_live_migrate_forced_v268(self):
2728         """Live migrating a server with a forced host was removed in API
2729         microversion 2.68. This test ensures that the request is rejected.
2730         """
2731         source_hostname = self.compute1.host
2732         dest_hostname = self.compute2.host
2733 
2734         server = self._boot_and_check_allocations(
2735             self.flavor1, source_hostname)
2736 
2737         # live migrate the server and force the destination host which bypasses
2738         # the scheduler
2739         post = {
2740             'os-migrateLive': {
2741                 'host': dest_hostname,
2742                 'block_migration': True,
2743                 'force': True,
2744             }
2745         }
2746 
2747         ex = self.assertRaises(client.OpenStackApiException,
2748                                self.api.post_server_action,
2749                                server['id'], post)
2750         self.assertIn("'force' was unexpected", six.text_type(ex))
2751 
2752     def test_live_migrate(self):
2753         source_hostname = self.compute1.host
2754         dest_hostname = self.compute2.host
2755         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2756         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2757 
2758         server = self._boot_and_check_allocations(
2759             self.flavor1, source_hostname)
2760         post = {
2761             'os-migrateLive': {
2762                 'host': dest_hostname,
2763                 'block_migration': True,
2764             }
2765         }
2766 
2767         self.api.post_server_action(server['id'], post)
2768         self._wait_for_server_parameter(self.api, server,
2769                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
2770                                          'status': 'ACTIVE'})
2771 
2772         self._run_periodics()
2773 
2774         # NOTE(danms): There should be no usage for the source
2775         self.assertRequestMatchesUsage(
2776             {'VCPU': 0,
2777              'MEMORY_MB': 0,
2778              'DISK_GB': 0}, source_rp_uuid)
2779 
2780         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2781 
2782         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2783                                            dest_rp_uuid)
2784 
2785         self._delete_and_check_allocations(server)
2786 
2787     def test_live_migrate_pre_check_fails(self):
2788         """Tests the case that the LiveMigrationTask in conductor has
2789         called the scheduler which picked a host and created allocations
2790         against it in Placement, but then when the conductor task calls
2791         check_can_live_migrate_destination on the destination compute it
2792         fails. The allocations on the destination compute node should be
2793         cleaned up before the conductor task asks the scheduler for another
2794         host to try the live migration.
2795         """
2796         self.failed_hostname = None
2797         source_hostname = self.compute1.host
2798         dest_hostname = self.compute2.host
2799         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2800         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2801 
2802         server = self._boot_and_check_allocations(
2803             self.flavor1, source_hostname)
2804 
2805         def fake_check_can_live_migrate_destination(
2806                 context, instance, src_compute_info, dst_compute_info,
2807                 block_migration=False, disk_over_commit=False):
2808             self.failed_hostname = dst_compute_info['host']
2809             raise exception.MigrationPreCheckError(
2810                 reason='test_live_migrate_pre_check_fails')
2811 
2812         with mock.patch('nova.virt.fake.FakeDriver.'
2813                         'check_can_live_migrate_destination',
2814                         side_effect=fake_check_can_live_migrate_destination):
2815             post = {
2816                 'os-migrateLive': {
2817                     'host': dest_hostname,
2818                     'block_migration': True,
2819                 }
2820             }
2821             self.api.post_server_action(server['id'], post)
2822             # As there are only two computes and we failed to live migrate to
2823             # the only other destination host, the LiveMigrationTask raises
2824             # MaxRetriesExceeded back to the conductor manager which handles it
2825             # generically and sets the instance back to ACTIVE status and
2826             # clears the task_state. The migration record status is set to
2827             # 'error', so that's what we need to look for to know when this
2828             # is done.
2829             migration = self._wait_for_migration_status(server, ['error'])
2830 
2831         # The source_compute should be set on the migration record, but the
2832         # destination shouldn't be as we never made it to one.
2833         self.assertEqual(source_hostname, migration['source_compute'])
2834         self.assertIsNone(migration['dest_compute'])
2835         # Make sure the destination host (the only other host) is the failed
2836         # host.
2837         self.assertEqual(dest_hostname, self.failed_hostname)
2838 
2839         # Since the instance didn't move, assert the allocations are still
2840         # on the source node.
2841         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2842 
2843         # Assert the allocations, created by the scheduler, are cleaned up
2844         # after the migration pre-check error happens.
2845         self.assertRequestMatchesUsage(
2846             {'VCPU': 0,
2847              'MEMORY_MB': 0,
2848              'DISK_GB': 0}, dest_rp_uuid)
2849 
2850         # There should only be 1 allocation for the instance on the source node
2851         self.assertFlavorMatchesAllocation(
2852             self.flavor1, server['id'], source_rp_uuid)
2853 
2854         self._delete_and_check_allocations(server)
2855 
2856     @mock.patch('nova.virt.fake.FakeDriver.pre_live_migration')
2857     def test_live_migrate_rollback_cleans_dest_node_allocations(
2858             self, mock_pre_live_migration, force=False):
2859         """Tests the case that when live migration fails, either during the
2860         call to pre_live_migration on the destination, or during the actual
2861         live migration in the virt driver, the allocations on the destination
2862         node are rolled back since the instance is still on the source node.
2863         """
2864         source_hostname = self.compute1.host
2865         dest_hostname = self.compute2.host
2866         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2867         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2868 
2869         # the ability to force live migrate a server is removed entirely in
2870         # 2.68
2871         self.api.microversion = '2.67'
2872 
2873         server = self._boot_and_check_allocations(
2874             self.flavor1, source_hostname)
2875 
2876         def stub_pre_live_migration(context, instance, block_device_info,
2877                                     network_info, disk_info, migrate_data):
2878             # Make sure the source node allocations are against the migration
2879             # record and the dest node allocations are against the instance.
2880             self.assertFlavorMatchesAllocation(
2881                 self.flavor1, migrate_data.migration.uuid, source_rp_uuid)
2882 
2883             self.assertFlavorMatchesAllocation(
2884                 self.flavor1, server['id'], dest_rp_uuid)
2885             # The actual type of exception here doesn't matter. The point
2886             # is that the virt driver raised an exception from the
2887             # pre_live_migration method on the destination host.
2888             raise test.TestingException(
2889                 'test_live_migrate_rollback_cleans_dest_node_allocations')
2890 
2891         mock_pre_live_migration.side_effect = stub_pre_live_migration
2892 
2893         post = {
2894             'os-migrateLive': {
2895                 'host': dest_hostname,
2896                 'block_migration': True,
2897                 'force': force
2898             }
2899         }
2900         self.api.post_server_action(server['id'], post)
2901         # The compute manager will put the migration record into error status
2902         # when pre_live_migration fails, so wait for that to happen.
2903         migration = self._wait_for_migration_status(server, ['error'])
2904         # The _rollback_live_migration method in the compute manager will reset
2905         # the task_state on the instance, so wait for that to happen.
2906         server = self._wait_for_server_parameter(
2907             self.api, server, {'OS-EXT-STS:task_state': None})
2908 
2909         self.assertEqual(source_hostname, migration['source_compute'])
2910         self.assertEqual(dest_hostname, migration['dest_compute'])
2911 
2912         # Since the instance didn't move, assert the allocations are still
2913         # on the source node.
2914         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2915 
2916         # Assert the allocations, created by the scheduler, are cleaned up
2917         # after the rollback happens.
2918         self.assertRequestMatchesUsage(
2919             {'VCPU': 0,
2920              'MEMORY_MB': 0,
2921              'DISK_GB': 0}, dest_rp_uuid)
2922 
2923         # There should only be 1 allocation for the instance on the source node
2924         self.assertFlavorMatchesAllocation(
2925             self.flavor1, server['id'], source_rp_uuid)
2926 
2927         self._delete_and_check_allocations(server)
2928 
2929     def test_live_migrate_rollback_cleans_dest_node_allocations_forced(self):
2930         """Tests the case that when a forced host live migration fails, either
2931         during the call to pre_live_migration on the destination, or during
2932         the actual live migration in the virt driver, the allocations on the
2933         destination node are rolled back since the instance is still on the
2934         source node.
2935         """
2936         self.test_live_migrate_rollback_cleans_dest_node_allocations(
2937             force=True)
2938 
2939     def test_rescheduling_when_migrating_instance(self):
2940         """Tests that allocations are removed from the destination node by
2941         the compute service when a cold migrate / resize fails and a reschedule
2942         request is sent back to conductor.
2943         """
2944         source_hostname = self.compute1.manager.host
2945         server = self._boot_and_check_allocations(
2946             self.flavor1, source_hostname)
2947 
2948         def fake_prep_resize(*args, **kwargs):
2949             dest_hostname = self._other_hostname(source_hostname)
2950             dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2951             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2952             allocations = self._get_allocations_by_server_uuid(server['id'])
2953             self.assertIn(dest_rp_uuid, allocations)
2954 
2955             source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2956             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2957             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
2958             allocations = self._get_allocations_by_server_uuid(migration_uuid)
2959             self.assertIn(source_rp_uuid, allocations)
2960 
2961             raise test.TestingException('Simulated _prep_resize failure.')
2962 
2963         # Yes this isn't great in a functional test, but it's simple.
2964         self.stub_out('nova.compute.manager.ComputeManager._prep_resize',
2965                       fake_prep_resize)
2966 
2967         # Now migrate the server which is going to fail on the destination.
2968         self.api.post_server_action(server['id'], {'migrate': None})
2969 
2970         self._wait_for_action_fail_completion(
2971             server, instance_actions.MIGRATE, 'compute_prep_resize')
2972 
2973         dest_hostname = self._other_hostname(source_hostname)
2974         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2975 
2976         # Expects no allocation records on the failed host.
2977         self.assertRequestMatchesUsage(
2978             {'VCPU': 0,
2979              'MEMORY_MB': 0,
2980              'DISK_GB': 0}, dest_rp_uuid)
2981 
2982         # Ensure the allocation records still exist on the source host.
2983         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2984         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2985         allocations = self._get_allocations_by_server_uuid(server['id'])
2986         self.assertIn(source_rp_uuid, allocations)
2987 
2988     def _test_resize_to_same_host_instance_fails(self, failing_method,
2989                                                  event_name):
2990         """Tests that when we resize to the same host and resize fails in
2991         the given method, we cleanup the allocations before rescheduling.
2992         """
2993         # make sure that the test only uses a single host
2994         compute2_service_id = self.admin_api.get_services(
2995             host=self.compute2.host, binary='nova-compute')[0]['id']
2996         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2997 
2998         hostname = self.compute1.manager.host
2999         rp_uuid = self._get_provider_uuid_by_host(hostname)
3000 
3001         server = self._boot_and_check_allocations(self.flavor1, hostname)
3002 
3003         def fake_resize_method(*args, **kwargs):
3004             # Ensure the allocations are doubled now before we fail.
3005             self.assertFlavorMatchesUsage(rp_uuid, self.flavor1, self.flavor2)
3006             raise test.TestingException('Simulated resize failure.')
3007 
3008         # Yes this isn't great in a functional test, but it's simple.
3009         self.stub_out(
3010             'nova.compute.manager.ComputeManager.%s' % failing_method,
3011             fake_resize_method)
3012 
3013         self.flags(allow_resize_to_same_host=True)
3014         resize_req = {
3015             'resize': {
3016                 'flavorRef': self.flavor2['id']
3017             }
3018         }
3019         self.api.post_server_action(server['id'], resize_req)
3020 
3021         self._wait_for_action_fail_completion(
3022             server, instance_actions.RESIZE, event_name)
3023 
3024         # Ensure the allocation records still exist on the host.
3025         source_rp_uuid = self._get_provider_uuid_by_host(hostname)
3026         if failing_method == '_finish_resize':
3027             # finish_resize will drop the old flavor allocations.
3028             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor2)
3029         else:
3030             # The new_flavor should have been subtracted from the doubled
3031             # allocation which just leaves us with the original flavor.
3032             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3033 
3034     def test_resize_to_same_host_prep_resize_fails(self):
3035         self._test_resize_to_same_host_instance_fails(
3036             '_prep_resize', 'compute_prep_resize')
3037 
3038     def test_resize_instance_fails_allocation_cleanup(self):
3039         self._test_resize_to_same_host_instance_fails(
3040             '_resize_instance', 'compute_resize_instance')
3041 
3042     def test_finish_resize_fails_allocation_cleanup(self):
3043         self._test_resize_to_same_host_instance_fails(
3044             '_finish_resize', 'compute_finish_resize')
3045 
3046     def _server_created_with_host(self):
3047         hostname = self.compute1.host
3048         server_req = self._build_minimal_create_server_request(
3049             self.api, "some-server", flavor_id=self.flavor1["id"],
3050             image_uuid="155d900f-4e14-4e4c-a73d-069cbf4541e6",
3051             networks='none')
3052         server_req['host'] = hostname
3053 
3054         created_server = self.api.post_server({"server": server_req})
3055         server = self._wait_for_state_change(
3056             self.api, created_server, "ACTIVE")
3057         return server
3058 
3059     def test_live_migration_after_server_created_with_host(self):
3060         """Test after creating server with requested host, and then
3061         do live-migration for the server. The requested host will not
3062         effect the new moving operation.
3063         """
3064         dest_hostname = self.compute2.host
3065         created_server = self._server_created_with_host()
3066 
3067         post = {
3068             'os-migrateLive': {
3069                 'host': None,
3070                 'block_migration': 'auto'
3071             }
3072         }
3073         self.api.post_server_action(created_server['id'], post)
3074         new_server = self._wait_for_server_parameter(
3075             self.api, created_server, {'status': 'ACTIVE'})
3076         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3077 
3078         self.assertEqual(dest_hostname, inst_dest_host)
3079 
3080     def test_evacuate_after_server_created_with_host(self):
3081         """Test after creating server with requested host, and then
3082         do evacuation for the server. The requested host will not
3083         effect the new moving operation.
3084         """
3085         dest_hostname = self.compute2.host
3086         created_server = self._server_created_with_host()
3087 
3088         source_compute_id = self.admin_api.get_services(
3089             host=created_server["OS-EXT-SRV-ATTR:host"],
3090             binary='nova-compute')[0]['id']
3091 
3092         self.compute1.stop()
3093         # force it down to avoid waiting for the service group to time out
3094         self.admin_api.put_service(
3095             source_compute_id, {'forced_down': 'true'})
3096 
3097         post = {
3098             'evacuate': {}
3099         }
3100         self.api.post_server_action(created_server['id'], post)
3101         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
3102                            'status': 'ACTIVE'}
3103         new_server = self._wait_for_server_parameter(self.api, created_server,
3104                                                      expected_params)
3105         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3106 
3107         self.assertEqual(dest_hostname, inst_dest_host)
3108 
3109     def test_resize_and_confirm_after_server_created_with_host(self):
3110         """Test after creating server with requested host, and then
3111         do resize for the server. The requested host will not
3112         effect the new moving operation.
3113         """
3114         dest_hostname = self.compute2.host
3115         created_server = self._server_created_with_host()
3116 
3117         # resize server
3118         self.flags(allow_resize_to_same_host=False)
3119         resize_req = {
3120             'resize': {
3121                 'flavorRef': self.flavor2['id']
3122             }
3123         }
3124         self.api.post_server_action(created_server['id'], resize_req)
3125         self._wait_for_state_change(self.api, created_server, 'VERIFY_RESIZE')
3126 
3127         # Confirm the resize
3128         post = {'confirmResize': None}
3129         self.api.post_server_action(
3130             created_server['id'], post, check_response_status=[204])
3131         new_server = self._wait_for_state_change(self.api, created_server,
3132                                                  'ACTIVE')
3133         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3134 
3135         self.assertEqual(dest_hostname, inst_dest_host)
3136 
3137     def test_shelve_unshelve_after_server_created_with_host(self):
3138         """Test after creating server with requested host, and then
3139         do shelve and unshelve for the server. The requested host
3140         will not effect the new moving operation.
3141         """
3142         dest_hostname = self.compute2.host
3143         created_server = self._server_created_with_host()
3144 
3145         self.flags(shelved_offload_time=-1)
3146         req = {'shelve': {}}
3147         self.api.post_server_action(created_server['id'], req)
3148         self._wait_for_state_change(self.api, created_server, 'SHELVED')
3149 
3150         req = {'shelveOffload': {}}
3151         self.api.post_server_action(created_server['id'], req)
3152         self._wait_for_server_parameter(
3153             self.api, created_server, {'status': 'SHELVED_OFFLOADED',
3154                                        'OS-EXT-SRV-ATTR:host': None,
3155                                        'OS-EXT-AZ:availability_zone': ''})
3156 
3157         # unshelve after shelve offload will do scheduling. this test case
3158         # wants to test the scenario when the scheduler select a different host
3159         # to ushelve the instance. So we disable the original host.
3160         source_service_id = self.admin_api.get_services(
3161             host=created_server["OS-EXT-SRV-ATTR:host"],
3162             binary='nova-compute')[0]['id']
3163         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3164 
3165         req = {'unshelve': None}
3166         self.api.post_server_action(created_server['id'], req)
3167         new_server = self._wait_for_state_change(
3168             self.api, created_server, 'ACTIVE')
3169         inst_dest_host = new_server["OS-EXT-SRV-ATTR:host"]
3170 
3171         self.assertEqual(dest_hostname, inst_dest_host)
3172 
3173     @mock.patch.object(utils, 'fill_provider_mapping',
3174                        wraps=utils.fill_provider_mapping)
3175     def _test_resize_reschedule_uses_host_lists(self, mock_fill_provider_map,
3176                                                 fails, num_alts=None):
3177         """Test that when a resize attempt fails, the retry comes from the
3178         supplied host_list, and does not call the scheduler.
3179         """
3180         server_req = self._build_minimal_create_server_request(
3181                 self.api, "some-server", flavor_id=self.flavor1["id"],
3182                 image_uuid="155d900f-4e14-4e4c-a73d-069cbf4541e6",
3183                 networks='none')
3184 
3185         created_server = self.api.post_server({"server": server_req})
3186         server = self._wait_for_state_change(self.api, created_server,
3187                 "ACTIVE")
3188         inst_host = server["OS-EXT-SRV-ATTR:host"]
3189         uuid_orig = self._get_provider_uuid_by_host(inst_host)
3190 
3191         # We will need four new compute nodes to test the resize, representing
3192         # the host selected by select_destinations(), along with 3 alternates.
3193         self._start_compute(host="selection")
3194         self._start_compute(host="alt_host1")
3195         self._start_compute(host="alt_host2")
3196         self._start_compute(host="alt_host3")
3197         uuid_sel = self._get_provider_uuid_by_host("selection")
3198         uuid_alt1 = self._get_provider_uuid_by_host("alt_host1")
3199         uuid_alt2 = self._get_provider_uuid_by_host("alt_host2")
3200         uuid_alt3 = self._get_provider_uuid_by_host("alt_host3")
3201         hosts = [{"name": "selection", "uuid": uuid_sel},
3202                  {"name": "alt_host1", "uuid": uuid_alt1},
3203                  {"name": "alt_host2", "uuid": uuid_alt2},
3204                  {"name": "alt_host3", "uuid": uuid_alt3},
3205                 ]
3206 
3207         self.flags(weight_classes=[__name__ + '.AltHostWeigher'],
3208                    group='filter_scheduler')
3209         self.scheduler_service.stop()
3210         self.scheduler_service = self.start_service('scheduler')
3211 
3212         def fake_prep_resize(*args, **kwargs):
3213             if self.num_fails < fails:
3214                 self.num_fails += 1
3215                 raise Exception("fake_prep_resize")
3216             actual_prep_resize(*args, **kwargs)
3217 
3218         # Yes this isn't great in a functional test, but it's simple.
3219         actual_prep_resize = compute_manager.ComputeManager._prep_resize
3220         self.stub_out("nova.compute.manager.ComputeManager._prep_resize",
3221                       fake_prep_resize)
3222         self.num_fails = 0
3223         num_alts = 4 if num_alts is None else num_alts
3224         # Make sure we have enough retries available for the number of
3225         # requested fails.
3226         attempts = min(fails + 2, num_alts)
3227         self.flags(max_attempts=attempts, group='scheduler')
3228         server_uuid = server["id"]
3229         data = {"resize": {"flavorRef": self.flavor2["id"]}}
3230         self.api.post_server_action(server_uuid, data)
3231 
3232         # fill_provider_mapping should have been called once for the initial
3233         # build, once for the resize scheduling to the primary host and then
3234         # once per reschedule.
3235         expected_fill_count = 2
3236         if num_alts > 1:
3237             expected_fill_count += self.num_fails - 1
3238         self.assertGreaterEqual(mock_fill_provider_map.call_count,
3239                                 expected_fill_count)
3240 
3241         if num_alts < fails:
3242             # We will run out of alternates before populate_retry will
3243             # raise a MaxRetriesExceeded exception, so the migration will
3244             # fail and the server should be in status "ERROR"
3245             server = self._wait_for_state_change(self.api, created_server,
3246                     "ERROR")
3247             # The usage should be unchanged from the original flavor
3248             self.assertFlavorMatchesUsage(uuid_orig, self.flavor1)
3249             # There should be no usages on any of the hosts
3250             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3251             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3252             for target_uuid in target_uuids:
3253                 usage = self._get_provider_usages(target_uuid)
3254                 self.assertEqual(empty_usage, usage)
3255         else:
3256             server = self._wait_for_state_change(self.api, created_server,
3257                     "VERIFY_RESIZE")
3258             # Verify that the selected host failed, and was rescheduled to
3259             # an alternate host.
3260             new_server_host = server.get("OS-EXT-SRV-ATTR:host")
3261             expected_host = hosts[fails]["name"]
3262             self.assertEqual(expected_host, new_server_host)
3263             uuid_dest = hosts[fails]["uuid"]
3264             # The usage should match the resized flavor
3265             self.assertFlavorMatchesUsage(uuid_dest, self.flavor2)
3266             # Verify that the other host have no allocations
3267             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3268             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3269             for target_uuid in target_uuids:
3270                 if target_uuid == uuid_dest:
3271                     continue
3272                 usage = self._get_provider_usages(target_uuid)
3273                 self.assertEqual(empty_usage, usage)
3274 
3275             # Verify that there is only one migration record for the instance.
3276             ctxt = context.get_admin_context()
3277             filters = {"instance_uuid": server["id"]}
3278             migrations = objects.MigrationList.get_by_filters(ctxt, filters)
3279             self.assertEqual(1, len(migrations.objects))
3280 
3281     def test_resize_reschedule_uses_host_lists_1_fail(self):
3282         self._test_resize_reschedule_uses_host_lists(fails=1)
3283 
3284     def test_resize_reschedule_uses_host_lists_3_fails(self):
3285         self._test_resize_reschedule_uses_host_lists(fails=3)
3286 
3287     def test_resize_reschedule_uses_host_lists_not_enough_alts(self):
3288         self._test_resize_reschedule_uses_host_lists(fails=3, num_alts=1)
3289 
3290     def test_migrate_confirm(self):
3291         source_hostname = self.compute1.host
3292         dest_hostname = self.compute2.host
3293         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3294         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3295 
3296         server = self._boot_and_check_allocations(
3297             self.flavor1, source_hostname)
3298 
3299         self._migrate_and_check_allocations(
3300             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3301 
3302         # Confirm the move and check the usages
3303         post = {'confirmResize': None}
3304         self.api.post_server_action(
3305             server['id'], post, check_response_status=[204])
3306         self._wait_for_state_change(self.api, server, 'ACTIVE')
3307 
3308         def _check_allocation():
3309             # the target host usage should be according to the flavor
3310             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3311             # the source host has no usage
3312             self.assertRequestMatchesUsage({'VCPU': 0,
3313                                             'MEMORY_MB': 0,
3314                                             'DISK_GB': 0}, source_rp_uuid)
3315 
3316             # and the target host allocation should be according to the flavor
3317             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3318                                                dest_rp_uuid)
3319 
3320         # After confirming, we should have an allocation only on the
3321         # destination host
3322         _check_allocation()
3323         self._run_periodics()
3324 
3325         # Check we're still accurate after running the periodics
3326         _check_allocation()
3327 
3328         self._delete_and_check_allocations(server)
3329 
3330     def test_migrate_revert(self):
3331         source_hostname = self.compute1.host
3332         dest_hostname = self.compute2.host
3333         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3334         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3335 
3336         server = self._boot_and_check_allocations(
3337             self.flavor1, source_hostname)
3338 
3339         self._migrate_and_check_allocations(
3340             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3341 
3342         # Revert the move and check the usages
3343         post = {'revertResize': None}
3344         self.api.post_server_action(server['id'], post)
3345         self._wait_for_state_change(self.api, server, 'ACTIVE')
3346 
3347         def _check_allocation():
3348             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3349             self.assertRequestMatchesUsage({'VCPU': 0,
3350                                             'MEMORY_MB': 0,
3351                                             'DISK_GB': 0}, dest_rp_uuid)
3352 
3353             # Check that the server only allocates resource from the original
3354             # host
3355             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3356                                                source_rp_uuid)
3357 
3358         # the original host expected to have the old resource allocation
3359         _check_allocation()
3360         self._run_periodics()
3361         _check_allocation()
3362 
3363         self._delete_and_check_allocations(server)
3364 
3365 
3366 class ServerLiveMigrateForceAndAbort(
3367         integrated_helpers.ProviderUsageBaseTestCase):
3368     """Test Server live migrations, which delete the migration or
3369     force_complete it, and check the allocations after the operations.
3370 
3371     The test are using fakedriver to handle the force_completion and deletion
3372     of live migration.
3373     """
3374 
3375     compute_driver = 'fake.FakeLiveMigrateDriver'
3376 
3377     def setUp(self):
3378         super(ServerLiveMigrateForceAndAbort, self).setUp()
3379 
3380         self.compute1 = self._start_compute(host='host1')
3381         self.compute2 = self._start_compute(host='host2')
3382 
3383         flavors = self.api.get_flavors()
3384         self.flavor1 = flavors[0]
3385 
3386     def test_live_migrate_force_complete(self):
3387         source_hostname = self.compute1.host
3388         dest_hostname = self.compute2.host
3389         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3390         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3391 
3392         server = self._boot_and_check_allocations(
3393             self.flavor1, source_hostname)
3394 
3395         post = {
3396             'os-migrateLive': {
3397                 'host': dest_hostname,
3398                 'block_migration': True,
3399             }
3400         }
3401         self.api.post_server_action(server['id'], post)
3402 
3403         migration = self._wait_for_migration_status(server, ['running'])
3404         self.api.force_complete_migration(server['id'],
3405                                           migration['id'])
3406 
3407         self._wait_for_server_parameter(self.api, server,
3408                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3409                                          'status': 'ACTIVE'})
3410 
3411         self._run_periodics()
3412 
3413         self.assertRequestMatchesUsage(
3414             {'VCPU': 0,
3415              'MEMORY_MB': 0,
3416              'DISK_GB': 0}, source_rp_uuid)
3417 
3418         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3419         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3420                                            dest_rp_uuid)
3421 
3422         self._delete_and_check_allocations(server)
3423 
3424     def test_live_migrate_delete(self):
3425         source_hostname = self.compute1.host
3426         dest_hostname = self.compute2.host
3427         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3428         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3429 
3430         server = self._boot_and_check_allocations(
3431             self.flavor1, source_hostname)
3432 
3433         post = {
3434             'os-migrateLive': {
3435                 'host': dest_hostname,
3436                 'block_migration': True,
3437             }
3438         }
3439         self.api.post_server_action(server['id'], post)
3440 
3441         migration = self._wait_for_migration_status(server, ['running'])
3442 
3443         self.api.delete_migration(server['id'], migration['id'])
3444         self._wait_for_server_parameter(self.api, server,
3445             {'OS-EXT-SRV-ATTR:host': source_hostname,
3446              'status': 'ACTIVE'})
3447 
3448         self._run_periodics()
3449 
3450         allocations = self._get_allocations_by_server_uuid(server['id'])
3451         self.assertNotIn(dest_rp_uuid, allocations)
3452 
3453         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3454 
3455         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3456                                            source_rp_uuid)
3457 
3458         self.assertRequestMatchesUsage({'VCPU': 0,
3459                                         'MEMORY_MB': 0,
3460                                         'DISK_GB': 0}, dest_rp_uuid)
3461 
3462         self._delete_and_check_allocations(server)
3463 
3464 
3465 class ServerLiveMigrateForceAndAbortWithNestedResourcesRequest(
3466         ServerLiveMigrateForceAndAbort):
3467     compute_driver = 'fake.FakeLiveMigrateDriverWithNestedCustomResources'
3468 
3469     def setUp(self):
3470         super(ServerLiveMigrateForceAndAbortWithNestedResourcesRequest,
3471               self).setUp()
3472         # modify the flavor used in the test base class to require one piece of
3473         # CUSTOM_MAGIC resource as well.
3474 
3475         self.api.post_extra_spec(
3476             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3477         # save the extra_specs in the flavor stored in the test case as
3478         # well
3479         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3480 
3481 
3482 class ServerRescheduleTests(integrated_helpers.ProviderUsageBaseTestCase):
3483     """Tests server create scenarios which trigger a reschedule during
3484     a server build and validates that allocations in Placement
3485     are properly cleaned up.
3486 
3487     Uses a fake virt driver that fails the build on the first attempt.
3488     """
3489 
3490     compute_driver = 'fake.FakeRescheduleDriver'
3491 
3492     def setUp(self):
3493         super(ServerRescheduleTests, self).setUp()
3494         self.compute1 = self._start_compute(host='host1')
3495         self.compute2 = self._start_compute(host='host2')
3496 
3497         flavors = self.api.get_flavors()
3498         self.flavor1 = flavors[0]
3499 
3500     def _other_hostname(self, host):
3501         other_host = {'host1': 'host2',
3502                       'host2': 'host1'}
3503         return other_host[host]
3504 
3505     def test_rescheduling_when_booting_instance(self):
3506         """Tests that allocations, created by the scheduler, are cleaned
3507         from the source node when the build fails on that node and is
3508         rescheduled to another node.
3509         """
3510         server_req = self._build_minimal_create_server_request(
3511                 self.api, 'some-server', flavor_id=self.flavor1['id'],
3512                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3513                 networks='none')
3514 
3515         created_server = self.api.post_server({'server': server_req})
3516         server = self._wait_for_state_change(
3517                 self.api, created_server, 'ACTIVE')
3518         dest_hostname = server['OS-EXT-SRV-ATTR:host']
3519         failed_hostname = self._other_hostname(dest_hostname)
3520 
3521         LOG.info('failed on %s', failed_hostname)
3522         LOG.info('booting on %s', dest_hostname)
3523 
3524         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
3525         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3526 
3527         # Expects no allocation records on the failed host.
3528         self.assertRequestMatchesUsage(
3529             {'VCPU': 0,
3530              'MEMORY_MB': 0,
3531              'DISK_GB': 0}, failed_rp_uuid)
3532 
3533         # Ensure the allocation records on the destination host.
3534         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3535 
3536     def test_allocation_fails_during_reschedule(self):
3537         """Verify that if nova fails to allocate resources during re-schedule
3538         then the server is put into ERROR state properly.
3539         """
3540 
3541         server_req = self._build_minimal_create_server_request(
3542             self.api, 'some-server', flavor_id=self.flavor1['id'],
3543             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3544             networks='none')
3545 
3546         orig_claim = utils.claim_resources
3547         # First call is during boot, we want that to succeed normally. Then the
3548         # fake virt driver triggers a re-schedule. During that re-schedule we
3549         # simulate that the placement call fails.
3550         with mock.patch('nova.scheduler.utils.claim_resources',
3551                         side_effect=[
3552                             orig_claim,
3553                             exception.AllocationUpdateFailed(
3554                                 consumer_uuid=uuids.inst1, error='testing')]):
3555 
3556             server = self.api.post_server({'server': server_req})
3557             server = self._wait_for_state_change(
3558                 self.admin_api, server, 'ERROR')
3559 
3560         self._delete_and_check_allocations(server)
3561 
3562 
3563 class ServerRescheduleTestsWithNestedResourcesRequest(ServerRescheduleTests):
3564     compute_driver = 'fake.FakeRescheduleDriverWithNestedCustomResources'
3565 
3566     def setUp(self):
3567         super(ServerRescheduleTestsWithNestedResourcesRequest, self).setUp()
3568         # modify the flavor used in the test base class to require one piece of
3569         # CUSTOM_MAGIC resource as well.
3570 
3571         self.api.post_extra_spec(
3572             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3573         # save the extra_specs in the flavor stored in the test case as
3574         # well
3575         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3576 
3577 
3578 class ServerBuildAbortTests(integrated_helpers.ProviderUsageBaseTestCase):
3579     """Tests server create scenarios which trigger a build abort during
3580     a server build and validates that allocations in Placement
3581     are properly cleaned up.
3582 
3583     Uses a fake virt driver that aborts the build on the first attempt.
3584     """
3585 
3586     compute_driver = 'fake.FakeBuildAbortDriver'
3587 
3588     def setUp(self):
3589         super(ServerBuildAbortTests, self).setUp()
3590         # We only need one compute service/host/node for these tests.
3591         self.compute1 = self._start_compute(host='host1')
3592 
3593         flavors = self.api.get_flavors()
3594         self.flavor1 = flavors[0]
3595 
3596     def test_abort_when_booting_instance(self):
3597         """Tests that allocations, created by the scheduler, are cleaned
3598         from the source node when the build is aborted on that node.
3599         """
3600         server_req = self._build_minimal_create_server_request(
3601                 self.api, 'some-server', flavor_id=self.flavor1['id'],
3602                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3603                 networks='none')
3604 
3605         created_server = self.api.post_server({'server': server_req})
3606         self._wait_for_state_change(self.api, created_server, 'ERROR')
3607 
3608         failed_hostname = self.compute1.manager.host
3609 
3610         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
3611         # Expects no allocation records on the failed host.
3612         self.assertRequestMatchesUsage({'VCPU': 0,
3613                                         'MEMORY_MB': 0,
3614                                         'DISK_GB': 0}, failed_rp_uuid)
3615 
3616 
3617 class ServerBuildAbortTestsWithNestedResourceRequest(ServerBuildAbortTests):
3618     compute_driver = 'fake.FakeBuildAbortDriverWithNestedCustomResources'
3619 
3620     def setUp(self):
3621         super(ServerBuildAbortTestsWithNestedResourceRequest, self).setUp()
3622         # modify the flavor used in the test base class to require one piece of
3623         # CUSTOM_MAGIC resource as well.
3624 
3625         self.api.post_extra_spec(
3626             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3627         # save the extra_specs in the flavor stored in the test case as
3628         # well
3629         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3630 
3631 
3632 class ServerUnshelveSpawnFailTests(
3633         integrated_helpers.ProviderUsageBaseTestCase):
3634     """Tests server unshelve scenarios which trigger a
3635     VirtualInterfaceCreateException during driver.spawn() and validates that
3636     allocations in Placement are properly cleaned up.
3637     """
3638 
3639     compute_driver = 'fake.FakeUnshelveSpawnFailDriver'
3640 
3641     def setUp(self):
3642         super(ServerUnshelveSpawnFailTests, self).setUp()
3643         # We only need one compute service/host/node for these tests.
3644         self.compute1 = self._start_compute('host1')
3645 
3646         flavors = self.api.get_flavors()
3647         self.flavor1 = flavors[0]
3648 
3649     def test_driver_spawn_fail_when_unshelving_instance(self):
3650         """Tests that allocations, created by the scheduler, are cleaned
3651         from the target node when the unshelve driver.spawn fails on that node.
3652         """
3653         hostname = self.compute1.manager.host
3654         rp_uuid = self._get_provider_uuid_by_host(hostname)
3655         # We start with no usages on the host.
3656         self.assertRequestMatchesUsage(
3657             {'VCPU': 0,
3658              'MEMORY_MB': 0,
3659              'DISK_GB': 0}, rp_uuid)
3660 
3661         server_req = self._build_minimal_create_server_request(
3662             self.api, 'unshelve-spawn-fail', flavor_id=self.flavor1['id'],
3663             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3664             networks='none')
3665 
3666         server = self.api.post_server({'server': server_req})
3667         self._wait_for_state_change(self.api, server, 'ACTIVE')
3668 
3669         # assert allocations exist for the host
3670         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3671 
3672         # shelve offload the server
3673         self.flags(shelved_offload_time=0)
3674         self.api.post_server_action(server['id'], {'shelve': None})
3675         self._wait_for_server_parameter(
3676             self.api, server, {'status': 'SHELVED_OFFLOADED',
3677                                'OS-EXT-SRV-ATTR:host': None})
3678 
3679         # assert allocations were removed from the host
3680         self.assertRequestMatchesUsage(
3681             {'VCPU': 0,
3682              'MEMORY_MB': 0,
3683              'DISK_GB': 0}, rp_uuid)
3684 
3685         # unshelve the server, which should fail
3686         self.api.post_server_action(server['id'], {'unshelve': None})
3687         self._wait_for_action_fail_completion(
3688             server, instance_actions.UNSHELVE, 'compute_unshelve_instance')
3689 
3690         # assert allocations were removed from the host
3691         self.assertRequestMatchesUsage(
3692             {'VCPU': 0,
3693              'MEMORY_MB': 0,
3694              'DISK_GB': 0}, rp_uuid)
3695 
3696 
3697 class ServerUnshelveSpawnFailTestsWithNestedResourceRequest(
3698     ServerUnshelveSpawnFailTests):
3699     compute_driver = ('fake.'
3700                       'FakeUnshelveSpawnFailDriverWithNestedCustomResources')
3701 
3702     def setUp(self):
3703         super(ServerUnshelveSpawnFailTestsWithNestedResourceRequest,
3704               self).setUp()
3705         # modify the flavor used in the test base class to require one piece of
3706         # CUSTOM_MAGIC resource as well.
3707 
3708         self.api.post_extra_spec(
3709             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3710         # save the extra_specs in the flavor stored in the test case as
3711         # well
3712         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3713 
3714 
3715 class ServerSoftDeleteTests(integrated_helpers.ProviderUsageBaseTestCase):
3716 
3717     compute_driver = 'fake.SmallFakeDriver'
3718 
3719     def setUp(self):
3720         super(ServerSoftDeleteTests, self).setUp()
3721         # We only need one compute service/host/node for these tests.
3722         self.compute1 = self._start_compute('host1')
3723 
3724         flavors = self.api.get_flavors()
3725         self.flavor1 = flavors[0]
3726 
3727     def _soft_delete_and_check_allocation(self, server, hostname):
3728         self.api.delete_server(server['id'])
3729         server = self._wait_for_state_change(self.api, server, 'SOFT_DELETED')
3730 
3731         self._run_periodics()
3732 
3733         # in soft delete state nova should keep the resource allocation as
3734         # the instance can be restored
3735         rp_uuid = self._get_provider_uuid_by_host(hostname)
3736 
3737         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3738 
3739         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3740                                            rp_uuid)
3741 
3742         # run the periodic reclaim but as time isn't advanced it should not
3743         # reclaim the instance
3744         ctxt = context.get_admin_context()
3745         self.compute1._reclaim_queued_deletes(ctxt)
3746 
3747         self._run_periodics()
3748 
3749         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3750 
3751         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3752                                            rp_uuid)
3753 
3754     def test_soft_delete_then_reclaim(self):
3755         """Asserts that the automatic reclaim of soft deleted instance cleans
3756         up the allocations in placement.
3757         """
3758 
3759         # make sure that instance will go to SOFT_DELETED state instead of
3760         # deleted immediately
3761         self.flags(reclaim_instance_interval=30)
3762 
3763         hostname = self.compute1.host
3764         rp_uuid = self._get_provider_uuid_by_host(hostname)
3765 
3766         server = self._boot_and_check_allocations(self.flavor1, hostname)
3767 
3768         self._soft_delete_and_check_allocation(server, hostname)
3769 
3770         # advance the time and run periodic reclaim, instance should be deleted
3771         # and resources should be freed
3772         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
3773         timeutils.set_time_override(override_time=the_past)
3774         self.addCleanup(timeutils.clear_time_override)
3775         ctxt = context.get_admin_context()
3776         self.compute1._reclaim_queued_deletes(ctxt)
3777 
3778         # Wait for real deletion
3779         self._wait_until_deleted(server)
3780 
3781         usages = self._get_provider_usages(rp_uuid)
3782         self.assertEqual({'VCPU': 0,
3783                           'MEMORY_MB': 0,
3784                           'DISK_GB': 0}, usages)
3785         allocations = self._get_allocations_by_server_uuid(server['id'])
3786         self.assertEqual(0, len(allocations))
3787 
3788     def test_soft_delete_then_restore(self):
3789         """Asserts that restoring a soft deleted instance keeps the proper
3790         allocation in placement.
3791         """
3792 
3793         # make sure that instance will go to SOFT_DELETED state instead of
3794         # deleted immediately
3795         self.flags(reclaim_instance_interval=30)
3796 
3797         hostname = self.compute1.host
3798         rp_uuid = self._get_provider_uuid_by_host(hostname)
3799 
3800         server = self._boot_and_check_allocations(
3801             self.flavor1, hostname)
3802 
3803         self._soft_delete_and_check_allocation(server, hostname)
3804 
3805         post = {'restore': {}}
3806         self.api.post_server_action(server['id'], post)
3807         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3808 
3809         # after restore the allocations should be kept
3810         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3811 
3812         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3813                                            rp_uuid)
3814 
3815         # Now we want a real delete
3816         self.flags(reclaim_instance_interval=0)
3817         self._delete_and_check_allocations(server)
3818 
3819 
3820 class ServerSoftDeleteTestsWithNestedResourceRequest(ServerSoftDeleteTests):
3821     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
3822 
3823     def setUp(self):
3824         super(ServerSoftDeleteTestsWithNestedResourceRequest, self).setUp()
3825         # modify the flavor used in the test base class to require one piece of
3826         # CUSTOM_MAGIC resource as well.
3827 
3828         self.api.post_extra_spec(
3829             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3830         # save the extra_specs in the flavor stored in the test case as
3831         # well
3832         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3833 
3834 
3835 class VolumeBackedServerTest(integrated_helpers.ProviderUsageBaseTestCase):
3836     """Tests for volume-backed servers."""
3837 
3838     compute_driver = 'fake.SmallFakeDriver'
3839 
3840     def setUp(self):
3841         super(VolumeBackedServerTest, self).setUp()
3842         self.compute1 = self._start_compute('host1')
3843         self.flavor_id = self._create_flavor()
3844 
3845     def _create_flavor(self):
3846         body = {
3847             'flavor': {
3848                 'id': 'vbst',
3849                 'name': 'special',
3850                 'ram': 512,
3851                 'vcpus': 1,
3852                 'disk': 10,
3853                 'OS-FLV-EXT-DATA:ephemeral': 20,
3854                 'swap': 5 * 1024,
3855                 'rxtx_factor': 1.0,
3856                 'os-flavor-access:is_public': True,
3857             },
3858         }
3859         self.admin_api.post_flavor(body)
3860         return body['flavor']['id']
3861 
3862     def _create_server(self):
3863         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
3864             image_id = self.api.get_images()[0]['id']
3865         server_req = self._build_minimal_create_server_request(
3866             self.api, 'trait-based-server',
3867             image_uuid=image_id,
3868             flavor_id=self.flavor_id, networks='none')
3869         server = self.api.post_server({'server': server_req})
3870         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3871         return server
3872 
3873     def _create_volume_backed_server(self):
3874         self.useFixture(nova_fixtures.CinderFixture(self))
3875         volume_id = nova_fixtures.CinderFixture.IMAGE_BACKED_VOL
3876         server_req_body = {
3877             # There is no imageRef because this is boot from volume.
3878             'server': {
3879                 'flavorRef': self.flavor_id,
3880                 'name': 'test_volume_backed',
3881                 # We don't care about networking for this test. This
3882                 # requires microversion >= 2.37.
3883                 'networks': 'none',
3884                 'block_device_mapping_v2': [{
3885                     'boot_index': 0,
3886                     'uuid': volume_id,
3887                     'source_type': 'volume',
3888                     'destination_type': 'volume'
3889                 }]
3890             }
3891         }
3892         server = self.api.post_server(server_req_body)
3893         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3894         return server
3895 
3896     def test_ephemeral_has_disk_allocation(self):
3897         server = self._create_server()
3898         allocs = self._get_allocations_by_server_uuid(server['id'])
3899         resources = list(allocs.values())[0]['resources']
3900         self.assertIn('MEMORY_MB', resources)
3901         # 10gb root, 20gb ephemeral, 5gb swap
3902         expected_usage = 35
3903         self.assertEqual(expected_usage, resources['DISK_GB'])
3904         # Ensure the compute node is reporting the correct disk usage
3905         self.assertEqual(
3906             expected_usage,
3907             self.admin_api.get_hypervisor_stats()['local_gb_used'])
3908 
3909     def test_volume_backed_image_type_filter(self):
3910         # Enable the image type support filter and ensure that a
3911         # non-image-having volume-backed server can still boot
3912         self.flags(query_placement_for_image_type_support=True,
3913                    group='scheduler')
3914         server = self._create_volume_backed_server()
3915         created_server = self.api.get_server(server['id'])
3916         self.assertEqual('ACTIVE', created_server['status'])
3917 
3918     def test_volume_backed_no_disk_allocation(self):
3919         server = self._create_volume_backed_server()
3920         allocs = self._get_allocations_by_server_uuid(server['id'])
3921         resources = list(allocs.values())[0]['resources']
3922         self.assertIn('MEMORY_MB', resources)
3923         # 0gb root, 20gb ephemeral, 5gb swap
3924         expected_usage = 25
3925         self.assertEqual(expected_usage, resources['DISK_GB'])
3926         # Ensure the compute node is reporting the correct disk usage
3927         self.assertEqual(
3928             expected_usage,
3929             self.admin_api.get_hypervisor_stats()['local_gb_used'])
3930 
3931         # Now let's hack the RequestSpec.is_bfv field to mimic migrating an
3932         # old instance created before RequestSpec.is_bfv was set in the API,
3933         # move the instance and verify that the RequestSpec.is_bfv is set
3934         # and the instance still reports the same DISK_GB allocations as during
3935         # the initial create.
3936         ctxt = context.get_admin_context()
3937         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
3938         # Make sure it's set.
3939         self.assertTrue(reqspec.is_bfv)
3940         del reqspec.is_bfv
3941         reqspec.save()
3942         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
3943         # Make sure it's not set.
3944         self.assertNotIn('is_bfv', reqspec)
3945         # Now migrate the instance to another host and check the request spec
3946         # and allocations after the migration.
3947         self._start_compute('host2')
3948         self.admin_api.post_server_action(server['id'], {'migrate': None})
3949         # Wait for the server to complete the cold migration.
3950         server = self._wait_for_state_change(
3951             self.admin_api, server, 'VERIFY_RESIZE')
3952         self.assertEqual('host2', server['OS-EXT-SRV-ATTR:host'])
3953         # Confirm the cold migration and check usage and the request spec.
3954         self.api.post_server_action(server['id'], {'confirmResize': None})
3955         self._wait_for_state_change(self.api, server, 'ACTIVE')
3956         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
3957         # Make sure it's set.
3958         self.assertTrue(reqspec.is_bfv)
3959         allocs = self._get_allocations_by_server_uuid(server['id'])
3960         resources = list(allocs.values())[0]['resources']
3961         self.assertEqual(expected_usage, resources['DISK_GB'])
3962 
3963         # Now shelve and unshelve the server to make sure root_gb DISK_GB
3964         # isn't reported for allocations after we unshelve the server.
3965         fake_notifier.stub_notifier(self)
3966         self.addCleanup(fake_notifier.reset)
3967         self.api.post_server_action(server['id'], {'shelve': None})
3968         self._wait_for_state_change(self.api, server, 'SHELVED_OFFLOADED')
3969         fake_notifier.wait_for_versioned_notifications(
3970                 'instance.shelve_offload.end')
3971         # The server should not have any allocations since it's not currently
3972         # hosted on any compute service.
3973         allocs = self._get_allocations_by_server_uuid(server['id'])
3974         self.assertDictEqual({}, allocs)
3975         # Now unshelve the server and make sure there are still no DISK_GB
3976         # allocations for the root disk.
3977         self.api.post_server_action(server['id'], {'unshelve': None})
3978         self._wait_for_state_change(self.api, server, 'ACTIVE')
3979         allocs = self._get_allocations_by_server_uuid(server['id'])
3980         resources = list(allocs.values())[0]['resources']
3981         self.assertEqual(expected_usage, resources['DISK_GB'])
3982 
3983 
3984 class TraitsBasedSchedulingTest(integrated_helpers.ProviderUsageBaseTestCase):
3985     """Tests for requesting a server with required traits in Placement"""
3986 
3987     compute_driver = 'fake.SmallFakeDriver'
3988 
3989     def setUp(self):
3990         super(TraitsBasedSchedulingTest, self).setUp()
3991         self.compute1 = self._start_compute('host1')
3992         self.compute2 = self._start_compute('host2')
3993         # Using a standard trait from the os-traits library, set a required
3994         # trait extra spec on the flavor.
3995         flavors = self.api.get_flavors()
3996         self.flavor_with_trait = flavors[0]
3997         self.admin_api.post_extra_spec(
3998             self.flavor_with_trait['id'],
3999             {'extra_specs': {'trait:HW_CPU_X86_VMX': 'required'}})
4000         self.flavor_without_trait = flavors[1]
4001         self.flavor_with_forbidden_trait = flavors[2]
4002         self.admin_api.post_extra_spec(
4003             self.flavor_with_forbidden_trait['id'],
4004             {'extra_specs': {'trait:HW_CPU_X86_SGX': 'forbidden'}})
4005 
4006         # Note that we're using v2.35 explicitly as the api returns 404
4007         # starting with 2.36
4008         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
4009             images = self.api.get_images()
4010             self.image_id_with_trait = images[0]['id']
4011             self.api.api_put('/images/%s/metadata' % self.image_id_with_trait,
4012                              {'metadata': {
4013                                  'trait:HW_CPU_X86_SGX': 'required'}})
4014             self.image_id_without_trait = images[1]['id']
4015 
4016     def _create_server_with_traits(self, flavor_id, image_id):
4017         """Create a server with given flavor and image id's
4018         :param flavor_id: the flavor id
4019         :param image_id: the image id
4020         :return: create server response
4021         """
4022 
4023         server_req = self._build_minimal_create_server_request(
4024             self.api, 'trait-based-server',
4025             image_uuid=image_id,
4026             flavor_id=flavor_id, networks='none')
4027         return self.api.post_server({'server': server_req})
4028 
4029     def _create_volume_backed_server_with_traits(self, flavor_id, volume_id):
4030         """Create a server with block device mapping(volume) with the given
4031         flavor and volume id's. Either the flavor or the image backing the
4032         volume is expected to have the traits
4033         :param flavor_id: the flavor id
4034         :param volume_id: the volume id
4035         :return: create server response
4036         """
4037 
4038         server_req_body = {
4039             # There is no imageRef because this is boot from volume.
4040             'server': {
4041                 'flavorRef': flavor_id,
4042                 'name': 'test_image_trait_on_volume_backed',
4043                 # We don't care about networking for this test. This
4044                 # requires microversion >= 2.37.
4045                 'networks': 'none',
4046                 'block_device_mapping_v2': [{
4047                     'boot_index': 0,
4048                     'uuid': volume_id,
4049                     'source_type': 'volume',
4050                     'destination_type': 'volume'
4051                 }]
4052             }
4053         }
4054         server = self.api.post_server(server_req_body)
4055         return server
4056 
4057     def test_flavor_traits_based_scheduling(self):
4058         """Tests that a server create request using a required trait in the
4059         flavor ends up on the single compute node resource provider that also
4060         has that trait in Placement. That test will however pass half of the
4061         times even if the trait is not taken into consideration, so we are
4062         also disabling the compute node that has the required trait and try
4063         again, which should result in a no valid host error.
4064         """
4065 
4066         # Decorate compute1 resource provider with the required trait.
4067         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4068         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4069 
4070         # Create server using flavor with required trait
4071         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4072                                                  self.image_id_without_trait)
4073         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4074         # Assert the server ended up on the expected compute host that has
4075         # the required trait.
4076         self.assertEqual(self.compute1.host, server['OS-EXT-SRV-ATTR:host'])
4077 
4078         # Disable the compute node that has the required trait
4079         compute1_service_id = self.admin_api.get_services(
4080             host=self.compute1.host, binary='nova-compute')[0]['id']
4081         self.admin_api.put_service(compute1_service_id, {'status': 'disabled'})
4082 
4083         # Create server using flavor with required trait
4084         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4085                                                  self.image_id_without_trait)
4086 
4087         # The server should go to ERROR state because there is no valid host.
4088         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4089         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4090         # Make sure the failure was due to NoValidHost by checking the fault.
4091         self.assertIn('fault', server)
4092         self.assertIn('No valid host', server['fault']['message'])
4093 
4094     def test_flavor_forbidden_traits_based_scheduling(self):
4095         """Tests that a server create request using a forbidden trait in the
4096         flavor ends up on the single compute host that doesn't have that
4097         trait in Placement. That test will however pass half of the times even
4098         if the trait is not taken into consideration, so we are also disabling
4099         the compute node that doesn't have the forbidden trait and try again,
4100         which should result in a no valid host error.
4101         """
4102 
4103         # Decorate compute1 resource provider with forbidden trait
4104         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4105         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4106 
4107         # Create server using flavor with forbidden trait
4108         server = self._create_server_with_traits(
4109             self.flavor_with_forbidden_trait['id'],
4110             self.image_id_without_trait
4111         )
4112 
4113         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4114 
4115         # Assert the server ended up on the expected compute host that doesn't
4116         # have the forbidden trait.
4117         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4118 
4119         # Disable the compute node that doesn't have the forbidden trait
4120         compute2_service_id = self.admin_api.get_services(
4121             host=self.compute2.host, binary='nova-compute')[0]['id']
4122         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
4123 
4124         # Create server using flavor with forbidden trait
4125         server = self._create_server_with_traits(
4126             self.flavor_with_forbidden_trait['id'],
4127             self.image_id_without_trait
4128         )
4129 
4130         # The server should go to ERROR state because there is no valid host.
4131         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4132         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4133         # Make sure the failure was due to NoValidHost by checking the fault.
4134         self.assertIn('fault', server)
4135         self.assertIn('No valid host', server['fault']['message'])
4136 
4137     def test_image_traits_based_scheduling(self):
4138         """Tests that a server create request using a required trait on image
4139         ends up on the single compute node resource provider that also has that
4140         trait in Placement.
4141         """
4142 
4143         # Decorate compute2 resource provider with image trait.
4144         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4145         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4146 
4147         # Create server using only image trait
4148         server = self._create_server_with_traits(
4149             self.flavor_without_trait['id'], self.image_id_with_trait)
4150         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4151         # Assert the server ended up on the expected compute host that has
4152         # the required trait.
4153         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4154 
4155     def test_flavor_image_traits_based_scheduling(self):
4156         """Tests that a server create request using a required trait on flavor
4157         AND a required trait on the image ends up on the single compute node
4158         resource provider that also has that trait in Placement.
4159         """
4160 
4161         # Decorate compute2 resource provider with both flavor and image trait.
4162         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4163         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4164                                             'HW_CPU_X86_SGX'])
4165 
4166         # Create server using flavor and image trait
4167         server = self._create_server_with_traits(
4168             self.flavor_with_trait['id'], self.image_id_with_trait)
4169         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4170         # Assert the server ended up on the expected compute host that has
4171         # the required trait.
4172         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4173 
4174     def test_image_trait_on_volume_backed_instance(self):
4175         """Tests that when trying to launch a volume-backed instance with a
4176         required trait on the image metadata contained within the volume,
4177         the instance ends up on the single compute node resource provider
4178         that also has that trait in Placement.
4179         """
4180         # Decorate compute2 resource provider with volume image metadata trait.
4181         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4182         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4183 
4184         self.useFixture(nova_fixtures.CinderFixture(self))
4185         # Create our server with a volume containing the image meta data with a
4186         # required trait
4187         server = self._create_volume_backed_server_with_traits(
4188             self.flavor_without_trait['id'],
4189             nova_fixtures.CinderFixture.
4190             IMAGE_WITH_TRAITS_BACKED_VOL)
4191 
4192         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4193         # Assert the server ended up on the expected compute host that has
4194         # the required trait.
4195         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4196 
4197     def test_flavor_image_trait_on_volume_backed_instance(self):
4198         """Tests that when trying to launch a volume-backed instance with a
4199         required trait on flavor AND a required trait on the image metadata
4200         contained within the volume, the instance ends up on the single
4201         compute node resource provider that also has those traits in Placement.
4202         """
4203         # Decorate compute2 resource provider with volume image metadata trait.
4204         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4205         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4206                                             'HW_CPU_X86_SGX'])
4207 
4208         self.useFixture(nova_fixtures.CinderFixture(self))
4209         # Create our server with a flavor trait and a volume containing the
4210         # image meta data with a required trait
4211         server = self._create_volume_backed_server_with_traits(
4212             self.flavor_with_trait['id'],
4213             nova_fixtures.CinderFixture.
4214             IMAGE_WITH_TRAITS_BACKED_VOL)
4215 
4216         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4217         # Assert the server ended up on the expected compute host that has
4218         # the required trait.
4219         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4220 
4221     def test_flavor_traits_based_scheduling_no_valid_host(self):
4222         """Tests that a server create request using a required trait expressed
4223          in flavor fails to find a valid host since no compute node resource
4224          providers have the trait.
4225         """
4226 
4227         # Decorate compute1 resource provider with the image trait.
4228         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4229         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4230 
4231         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4232                                                  self.image_id_without_trait)
4233         # The server should go to ERROR state because there is no valid host.
4234         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4235         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4236         # Make sure the failure was due to NoValidHost by checking the fault.
4237         self.assertIn('fault', server)
4238         self.assertIn('No valid host', server['fault']['message'])
4239 
4240     def test_image_traits_based_scheduling_no_valid_host(self):
4241         """Tests that a server create request using a required trait expressed
4242          in image fails to find a valid host since no compute node resource
4243          providers have the trait.
4244         """
4245 
4246         # Decorate compute1 resource provider with that flavor trait.
4247         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4248         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4249 
4250         server = self._create_server_with_traits(
4251             self.flavor_without_trait['id'], self.image_id_with_trait)
4252         # The server should go to ERROR state because there is no valid host.
4253         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4254         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4255         # Make sure the failure was due to NoValidHost by checking the fault.
4256         self.assertIn('fault', server)
4257         self.assertIn('No valid host', server['fault']['message'])
4258 
4259     def test_flavor_image_traits_based_scheduling_no_valid_host(self):
4260         """Tests that a server create request using a required trait expressed
4261          in flavor AND a required trait expressed in the image fails to find a
4262          valid host since no compute node resource providers have the trait.
4263         """
4264 
4265         server = self._create_server_with_traits(
4266             self.flavor_with_trait['id'], self.image_id_with_trait)
4267         # The server should go to ERROR state because there is no valid host.
4268         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4269         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4270         # Make sure the failure was due to NoValidHost by checking the fault.
4271         self.assertIn('fault', server)
4272         self.assertIn('No valid host', server['fault']['message'])
4273 
4274     def test_image_trait_on_volume_backed_instance_no_valid_host(self):
4275         """Tests that when trying to launch a volume-backed instance with a
4276         required trait on the image metadata contained within the volume
4277         fails to find a valid host since no compute node resource providers
4278         have the trait.
4279         """
4280         self.useFixture(nova_fixtures.CinderFixture(self))
4281         # Create our server with a volume
4282         server = self._create_volume_backed_server_with_traits(
4283             self.flavor_without_trait['id'],
4284             nova_fixtures.CinderFixture.
4285             IMAGE_WITH_TRAITS_BACKED_VOL)
4286 
4287         # The server should go to ERROR state because there is no valid host.
4288         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4289         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4290         # Make sure the failure was due to NoValidHost by checking the fault.
4291         self.assertIn('fault', server)
4292         self.assertIn('No valid host', server['fault']['message'])
4293 
4294     def test_rebuild_instance_with_image_traits(self):
4295         """Rebuilds a server with a different image which has traits
4296         associated with it and which will run it through the scheduler to
4297         validate the image is still OK with the compute host that the
4298         instance is running on.
4299          """
4300         # Decorate compute2 resource provider with both flavor and image trait.
4301         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4302         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4303                                             'HW_CPU_X86_SGX'])
4304         # make sure we start with no usage on the compute node
4305         rp_usages = self._get_provider_usages(rp_uuid)
4306         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4307 
4308         # create a server without traits on image and with traits on flavour
4309         server = self._create_server_with_traits(
4310             self.flavor_with_trait['id'], self.image_id_without_trait)
4311         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4312 
4313         # make the compute node full and ensure rebuild still succeed
4314         inv = {"resource_class": "VCPU",
4315                "total": 1}
4316         self._set_inventory(rp_uuid, inv)
4317 
4318         # Now rebuild the server with a different image with traits
4319         rebuild_req_body = {
4320             'rebuild': {
4321                 'imageRef': self.image_id_with_trait
4322             }
4323         }
4324         self.api.api_post('/servers/%s/action' % server['id'],
4325                           rebuild_req_body)
4326         self._wait_for_server_parameter(
4327             self.api, server, {'OS-EXT-STS:task_state': None})
4328 
4329         allocs = self._get_allocations_by_server_uuid(server['id'])
4330         self.assertIn(rp_uuid, allocs)
4331 
4332         # Assert the server ended up on the expected compute host that has
4333         # the required trait.
4334         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4335 
4336     def test_rebuild_instance_with_image_traits_no_host(self):
4337         """Rebuilding a server with a different image which has required
4338         traits on the image fails to valid the host that this server is
4339         currently running, cause the compute host resource provider is not
4340         associated with similar trait.
4341         """
4342         # Decorate compute2 resource provider with traits on flavor
4343         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4344         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4345 
4346         # make sure we start with no usage on the compute node
4347         rp_usages = self._get_provider_usages(rp_uuid)
4348         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4349 
4350         # create a server without traits on image and with traits on flavour
4351         server = self._create_server_with_traits(
4352             self.flavor_with_trait['id'], self.image_id_without_trait)
4353         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4354 
4355         # Now rebuild the server with a different image with traits
4356         rebuild_req_body = {
4357             'rebuild': {
4358                 'imageRef': self.image_id_with_trait
4359             }
4360         }
4361 
4362         self.api.api_post('/servers/%s/action' % server['id'],
4363                           rebuild_req_body)
4364         # Look for the failed rebuild action.
4365         self._wait_for_action_fail_completion(
4366             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4367         # Assert the server image_ref was rolled back on failure.
4368         server = self.api.get_server(server['id'])
4369         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4370 
4371         # The server should be in ERROR state
4372         self.assertEqual('ERROR', server['status'])
4373         self.assertEqual("No valid host was found. Image traits cannot be "
4374                          "satisfied by the current resource providers. "
4375                          "Either specify a different image during rebuild "
4376                          "or create a new server with the specified image.",
4377                          server['fault']['message'])
4378 
4379     def test_rebuild_instance_with_image_traits_no_image_change(self):
4380         """Rebuilds a server with a same image which has traits
4381         associated with it and which will run it through the scheduler to
4382         validate the image is still OK with the compute host that the
4383         instance is running on.
4384          """
4385         # Decorate compute2 resource provider with both flavor and image trait.
4386         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4387         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4388                                             'HW_CPU_X86_SGX'])
4389         # make sure we start with no usage on the compute node
4390         rp_usages = self._get_provider_usages(rp_uuid)
4391         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
4392                          rp_usages)
4393 
4394         # create a server with traits in both image and flavour
4395         server = self._create_server_with_traits(
4396             self.flavor_with_trait['id'], self.image_id_with_trait)
4397         server = self._wait_for_state_change(self.admin_api, server,
4398                                              'ACTIVE')
4399 
4400         # Now rebuild the server with a different image with traits
4401         rebuild_req_body = {
4402             'rebuild': {
4403                 'imageRef': self.image_id_with_trait
4404             }
4405         }
4406         self.api.api_post('/servers/%s/action' % server['id'],
4407                           rebuild_req_body)
4408         self._wait_for_server_parameter(
4409             self.api, server, {'OS-EXT-STS:task_state': None})
4410 
4411         allocs = self._get_allocations_by_server_uuid(server['id'])
4412         self.assertIn(rp_uuid, allocs)
4413 
4414         # Assert the server ended up on the expected compute host that has
4415         # the required trait.
4416         self.assertEqual(self.compute2.host,
4417                          server['OS-EXT-SRV-ATTR:host'])
4418 
4419     def test_rebuild_instance_with_image_traits_and_forbidden_flavor_traits(
4420                                                                         self):
4421         """Rebuilding a server with a different image which has required
4422         traits on the image fails to validate image traits because flavor
4423         associated with the current instance has the similar trait that is
4424         forbidden
4425         """
4426         # Decorate compute2 resource provider with traits on flavor
4427         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4428         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4429 
4430         # make sure we start with no usage on the compute node
4431         rp_usages = self._get_provider_usages(rp_uuid)
4432         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4433 
4434         # create a server with forbidden traits on flavor and no triats on
4435         # image
4436         server = self._create_server_with_traits(
4437             self.flavor_with_forbidden_trait['id'],
4438             self.image_id_without_trait)
4439         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4440 
4441         # Now rebuild the server with a different image with traits
4442         rebuild_req_body = {
4443             'rebuild': {
4444                 'imageRef': self.image_id_with_trait
4445             }
4446         }
4447 
4448         self.api.api_post('/servers/%s/action' % server['id'],
4449                           rebuild_req_body)
4450         # Look for the failed rebuild action.
4451         self._wait_for_action_fail_completion(
4452             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4453         # Assert the server image_ref was rolled back on failure.
4454         server = self.api.get_server(server['id'])
4455         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4456 
4457         # The server should be in ERROR state
4458         self.assertEqual('ERROR', server['status'])
4459         self.assertEqual("No valid host was found. Image traits are part of "
4460                          "forbidden traits in flavor associated with the "
4461                          "server. Either specify a different image during "
4462                          "rebuild or create a new server with the specified "
4463                          "image and a compatible flavor.",
4464                          server['fault']['message'])
4465 
4466 
4467 class ServerTestV256Common(ServersTestBase):
4468     api_major_version = 'v2.1'
4469     microversion = '2.56'
4470     ADMIN_API = True
4471 
4472     def _setup_compute_service(self):
4473         # Set up 3 compute services in the same cell
4474         for host in ('host1', 'host2', 'host3'):
4475             self.start_service('compute', host=host)
4476 
4477     def _create_server(self, target_host=None):
4478         server = self._build_minimal_create_server_request(
4479             image_uuid='a2459075-d96c-40d5-893e-577ff92e721c')
4480         server.update({'networks': 'auto'})
4481         if target_host is not None:
4482             server['availability_zone'] = 'nova:%s' % target_host
4483         post = {'server': server}
4484         response = self.api.api_post('/servers', post).body
4485         return response['server']
4486 
4487     @staticmethod
4488     def _get_target_and_other_hosts(host):
4489         target_other_hosts = {'host1': ['host2', 'host3'],
4490                               'host2': ['host3', 'host1'],
4491                               'host3': ['host1', 'host2']}
4492         return target_other_hosts[host]
4493 
4494 
4495 class ServerTestV256MultiCellTestCase(ServerTestV256Common):
4496     """Negative test to ensure we fail with ComputeHostNotFound if we try to
4497     target a host in another cell from where the instance lives.
4498     """
4499     NUMBER_OF_CELLS = 2
4500 
4501     def _setup_compute_service(self):
4502         # Set up 2 compute services in different cells
4503         host_to_cell_mappings = {
4504             'host1': 'cell1',
4505             'host2': 'cell2'}
4506         for host in sorted(host_to_cell_mappings):
4507             self.start_service('compute', host=host,
4508                                cell=host_to_cell_mappings[host])
4509 
4510     def test_migrate_server_to_host_in_different_cell(self):
4511         # We target host1 specifically so that we have a predictable target for
4512         # the cold migration in cell2.
4513         server = self._create_server(target_host='host1')
4514         server = self._wait_for_state_change(server, 'BUILD')
4515 
4516         self.assertEqual('host1', server['OS-EXT-SRV-ATTR:host'])
4517         ex = self.assertRaises(client.OpenStackApiException,
4518                                self.api.post_server_action,
4519                                server['id'],
4520                                {'migrate': {'host': 'host2'}})
4521         # When the API pulls the instance out of cell1, the context is targeted
4522         # to cell1, so when the compute API resize() method attempts to lookup
4523         # the target host in cell1, it will result in a ComputeHostNotFound
4524         # error.
4525         self.assertEqual(400, ex.response.status_code)
4526         self.assertIn('Compute host host2 could not be found',
4527                       six.text_type(ex))
4528 
4529 
4530 class ServerTestV256SingleCellMultiHostTestCase(ServerTestV256Common):
4531     """Happy path test where we create a server on one host, migrate it to
4532     another host of our choosing and ensure it lands there.
4533     """
4534     def test_migrate_server_to_host_in_same_cell(self):
4535         server = self._create_server()
4536         server = self._wait_for_state_change(server, 'BUILD')
4537         source_host = server['OS-EXT-SRV-ATTR:host']
4538         target_host = self._get_target_and_other_hosts(source_host)[0]
4539         self.api.post_server_action(server['id'],
4540                                     {'migrate': {'host': target_host}})
4541         # Assert the server is now on the target host.
4542         server = self.api.get_server(server['id'])
4543         self.assertEqual(target_host, server['OS-EXT-SRV-ATTR:host'])
4544 
4545 
4546 class ServerTestV256RescheduleTestCase(ServerTestV256Common):
4547 
4548     @mock.patch.object(compute_manager.ComputeManager, '_prep_resize',
4549                        side_effect=exception.MigrationError(
4550                            reason='Test Exception'))
4551     def test_migrate_server_not_reschedule(self, mock_prep_resize):
4552         server = self._create_server()
4553         found_server = self._wait_for_state_change(server, 'BUILD')
4554 
4555         target_host, other_host = self._get_target_and_other_hosts(
4556             found_server['OS-EXT-SRV-ATTR:host'])
4557 
4558         self.assertRaises(client.OpenStackApiException,
4559                           self.api.post_server_action,
4560                           server['id'],
4561                           {'migrate': {'host': target_host}})
4562         self.assertEqual(1, mock_prep_resize.call_count)
4563         found_server = self.api.get_server(server['id'])
4564         # Check that rescheduling is not occurred.
4565         self.assertNotEqual(other_host, found_server['OS-EXT-SRV-ATTR:host'])
4566 
4567 
4568 class ConsumerGenerationConflictTest(
4569         integrated_helpers.ProviderUsageBaseTestCase):
4570 
4571     # we need the medium driver to be able to allocate resource not just for
4572     # a single instance
4573     compute_driver = 'fake.MediumFakeDriver'
4574 
4575     def setUp(self):
4576         super(ConsumerGenerationConflictTest, self).setUp()
4577         flavors = self.api.get_flavors()
4578         self.flavor = flavors[0]
4579         self.other_flavor = flavors[1]
4580         self.compute1 = self._start_compute('compute1')
4581         self.compute2 = self._start_compute('compute2')
4582 
4583     def test_create_server_fails_as_placement_reports_consumer_conflict(self):
4584         server_req = self._build_minimal_create_server_request(
4585             self.api, 'some-server', flavor_id=self.flavor['id'],
4586             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4587             networks='none')
4588 
4589         # We cannot pre-create a consumer with the uuid of the instance created
4590         # below as that uuid is generated. Instead we have to simulate that
4591         # Placement returns 409, consumer generation conflict for the PUT
4592         # /allocation request the scheduler does for the instance.
4593         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
4594             rsp = fake_requests.FakeResponse(
4595                 409,
4596                 jsonutils.dumps(
4597                     {'errors': [
4598                         {'code': 'placement.concurrent_update',
4599                          'detail': 'consumer generation conflict'}]}))
4600             mock_put.return_value = rsp
4601 
4602             created_server = self.api.post_server({'server': server_req})
4603             server = self._wait_for_state_change(
4604                 self.admin_api, created_server, 'ERROR')
4605 
4606         # This is not a conflict that the API user can ever resolve. It is a
4607         # serious inconsistency in our database or a bug in the scheduler code
4608         # doing the claim.
4609         self.assertEqual(500, server['fault']['code'])
4610         self.assertIn('Failed to update allocations for consumer',
4611                       server['fault']['message'])
4612 
4613         allocations = self._get_allocations_by_server_uuid(server['id'])
4614         self.assertEqual(0, len(allocations))
4615 
4616         self._delete_and_check_allocations(server)
4617 
4618     def test_migrate_claim_on_dest_fails(self):
4619         source_hostname = self.compute1.host
4620         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4621 
4622         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4623 
4624         # We have to simulate that Placement returns 409, consumer generation
4625         # conflict for the PUT /allocation request the scheduler does on the
4626         # destination host for the instance.
4627         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
4628             rsp = fake_requests.FakeResponse(
4629                 409,
4630                 jsonutils.dumps(
4631                     {'errors': [
4632                         {'code': 'placement.concurrent_update',
4633                          'detail': 'consumer generation conflict'}]}))
4634             mock_put.return_value = rsp
4635 
4636             request = {'migrate': None}
4637             exception = self.assertRaises(client.OpenStackApiException,
4638                                           self.api.post_server_action,
4639                                           server['id'], request)
4640 
4641         # I know that HTTP 500 is harsh code but I think this conflict case
4642         # signals either a serious db inconsistency or a bug in nova's
4643         # claim code.
4644         self.assertEqual(500, exception.response.status_code)
4645 
4646         # The migration is aborted so the instance is ACTIVE on the source
4647         # host instead of being in VERIFY_RESIZE state.
4648         server = self.api.get_server(server['id'])
4649         self.assertEqual('ACTIVE', server['status'])
4650         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
4651 
4652         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4653 
4654         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4655                                            source_rp_uuid)
4656 
4657         self._delete_and_check_allocations(server)
4658 
4659     def test_migrate_move_allocation_fails_due_to_conflict(self):
4660         source_hostname = self.compute1.host
4661         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4662 
4663         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4664 
4665         rsp = fake_requests.FakeResponse(
4666             409,
4667             jsonutils.dumps(
4668                 {'errors': [
4669                     {'code': 'placement.concurrent_update',
4670                      'detail': 'consumer generation conflict'}]}))
4671 
4672         with mock.patch('keystoneauth1.adapter.Adapter.post',
4673                         autospec=True) as mock_post:
4674             mock_post.return_value = rsp
4675 
4676             request = {'migrate': None}
4677             exception = self.assertRaises(client.OpenStackApiException,
4678                                           self.api.post_server_action,
4679                                           server['id'], request)
4680 
4681         self.assertEqual(1, mock_post.call_count)
4682 
4683         self.assertEqual(409, exception.response.status_code)
4684         self.assertIn('Failed to move allocations', exception.response.text)
4685 
4686         migrations = self.api.get_migrations()
4687         self.assertEqual(1, len(migrations))
4688         self.assertEqual('migration', migrations[0]['migration_type'])
4689         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4690         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4691         self.assertEqual('error', migrations[0]['status'])
4692 
4693         # The migration is aborted so the instance is ACTIVE on the source
4694         # host instead of being in VERIFY_RESIZE state.
4695         server = self.api.get_server(server['id'])
4696         self.assertEqual('ACTIVE', server['status'])
4697         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
4698 
4699         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4700 
4701         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4702                                            source_rp_uuid)
4703 
4704         self._delete_and_check_allocations(server)
4705 
4706     def test_confirm_migrate_delete_alloc_on_source_fails(self):
4707         source_hostname = self.compute1.host
4708         dest_hostname = self.compute2.host
4709         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4710         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4711 
4712         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4713         self._migrate_and_check_allocations(
4714             server, self.flavor, source_rp_uuid, dest_rp_uuid)
4715 
4716         rsp = fake_requests.FakeResponse(
4717             409,
4718             jsonutils.dumps(
4719                 {'errors': [
4720                     {'code': 'placement.concurrent_update',
4721                      'detail': 'consumer generation conflict'}]}))
4722 
4723         with mock.patch('keystoneauth1.adapter.Adapter.put',
4724                         autospec=True) as mock_put:
4725             mock_put.return_value = rsp
4726 
4727             post = {'confirmResize': None}
4728             self.api.post_server_action(
4729                 server['id'], post, check_response_status=[204])
4730             server = self._wait_for_state_change(self.api, server, 'ERROR')
4731             self.assertIn('Failed to delete allocations',
4732                           server['fault']['message'])
4733 
4734         self.assertEqual(1, mock_put.call_count)
4735 
4736         migrations = self.api.get_migrations()
4737         self.assertEqual(1, len(migrations))
4738         self.assertEqual('migration', migrations[0]['migration_type'])
4739         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4740         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4741         self.assertEqual('error', migrations[0]['status'])
4742 
4743         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4744         # after the instance is deleted. At least nova logs a fat ERROR.
4745         self.assertIn('Deleting allocation in placement for migration %s '
4746                       'failed. The instance %s will be put to ERROR state but '
4747                       'the allocation held by the migration is leaked.' %
4748                       (migrations[0]['uuid'], server['id']),
4749                       self.stdlog.logger.output)
4750         self.api.delete_server(server['id'])
4751         self._wait_until_deleted(server)
4752         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4753 
4754         allocations = self._get_allocations_by_server_uuid(
4755             migrations[0]['uuid'])
4756         self.assertEqual(1, len(allocations))
4757 
4758     def test_revert_migrate_delete_dest_allocation_fails_due_to_conflict(self):
4759         source_hostname = self.compute1.host
4760         dest_hostname = self.compute2.host
4761         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4762         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4763 
4764         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4765         self._migrate_and_check_allocations(
4766             server, self.flavor, source_rp_uuid, dest_rp_uuid)
4767 
4768         rsp = fake_requests.FakeResponse(
4769             409,
4770             jsonutils.dumps(
4771                 {'errors': [
4772                     {'code': 'placement.concurrent_update',
4773                      'detail': 'consumer generation conflict'}]}))
4774 
4775         with mock.patch('keystoneauth1.adapter.Adapter.post',
4776                         autospec=True) as mock_post:
4777             mock_post.return_value = rsp
4778 
4779             post = {'revertResize': None}
4780             self.api.post_server_action(server['id'], post)
4781             server = self._wait_for_state_change(self.api, server, 'ERROR')
4782 
4783         self.assertEqual(1, mock_post.call_count)
4784 
4785         migrations = self.api.get_migrations()
4786         self.assertEqual(1, len(migrations))
4787         self.assertEqual('migration', migrations[0]['migration_type'])
4788         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4789         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4790         self.assertEqual('error', migrations[0]['status'])
4791 
4792         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4793         # after the instance is deleted. At least nova logs a fat ERROR.
4794         self.assertIn('Reverting allocation in placement for migration %s '
4795                       'failed. The instance %s will be put into ERROR state '
4796                       'but the allocation held by the migration is leaked.' %
4797                       (migrations[0]['uuid'], server['id']),
4798                       self.stdlog.logger.output)
4799         self.api.delete_server(server['id'])
4800         self._wait_until_deleted(server)
4801         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4802 
4803         allocations = self._get_allocations_by_server_uuid(
4804             migrations[0]['uuid'])
4805         self.assertEqual(1, len(allocations))
4806 
4807     def test_revert_resize_same_host_delete_dest_fails_due_to_conflict(self):
4808         # make sure that the test only uses a single host
4809         compute2_service_id = self.admin_api.get_services(
4810             host=self.compute2.host, binary='nova-compute')[0]['id']
4811         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
4812 
4813         hostname = self.compute1.manager.host
4814         rp_uuid = self._get_provider_uuid_by_host(hostname)
4815 
4816         server = self._boot_and_check_allocations(self.flavor, hostname)
4817 
4818         self._resize_to_same_host_and_check_allocations(
4819             server, self.flavor, self.other_flavor, rp_uuid)
4820 
4821         rsp = fake_requests.FakeResponse(
4822             409,
4823             jsonutils.dumps(
4824                 {'errors': [
4825                     {'code': 'placement.concurrent_update',
4826                      'detail': 'consumer generation conflict'}]}))
4827         with mock.patch('keystoneauth1.adapter.Adapter.post',
4828                         autospec=True) as mock_post:
4829             mock_post.return_value = rsp
4830 
4831             post = {'revertResize': None}
4832             self.api.post_server_action(server['id'], post)
4833             server = self._wait_for_state_change(self.api, server, 'ERROR',)
4834 
4835         self.assertEqual(1, mock_post.call_count)
4836 
4837         migrations = self.api.get_migrations()
4838         self.assertEqual(1, len(migrations))
4839         self.assertEqual('resize', migrations[0]['migration_type'])
4840         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4841         self.assertEqual(hostname, migrations[0]['source_compute'])
4842         self.assertEqual('error', migrations[0]['status'])
4843 
4844         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4845         # after the instance is deleted. At least nova logs a fat ERROR.
4846         self.assertIn('Reverting allocation in placement for migration %s '
4847                       'failed. The instance %s will be put into ERROR state '
4848                       'but the allocation held by the migration is leaked.' %
4849                       (migrations[0]['uuid'], server['id']),
4850                       self.stdlog.logger.output)
4851         self.api.delete_server(server['id'])
4852         self._wait_until_deleted(server)
4853         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4854 
4855         allocations = self._get_allocations_by_server_uuid(
4856             migrations[0]['uuid'])
4857         self.assertEqual(1, len(allocations))
4858 
4859     def test_force_live_migrate_claim_on_dest_fails(self):
4860         # Normal live migrate moves source allocation from instance to
4861         # migration like a normal migrate tested above.
4862         # Normal live migrate claims on dest like a normal boot tested above.
4863         source_hostname = self.compute1.host
4864         dest_hostname = self.compute2.host
4865 
4866         # the ability to force live migrate a server is removed entirely in
4867         # 2.68
4868         self.api.microversion = '2.67'
4869 
4870         server = self._boot_and_check_allocations(
4871             self.flavor, source_hostname)
4872 
4873         rsp = fake_requests.FakeResponse(
4874             409,
4875             jsonutils.dumps(
4876                 {'errors': [
4877                     {'code': 'placement.concurrent_update',
4878                      'detail': 'consumer generation conflict'}]}))
4879         with mock.patch('keystoneauth1.adapter.Adapter.put',
4880                         autospec=True) as mock_put:
4881             mock_put.return_value = rsp
4882 
4883             post = {
4884                 'os-migrateLive': {
4885                     'host': dest_hostname,
4886                     'block_migration': True,
4887                     'force': True,
4888                 }
4889             }
4890 
4891             self.api.post_server_action(server['id'], post)
4892             server = self._wait_for_state_change(self.api, server, 'ERROR')
4893 
4894         self.assertEqual(1, mock_put.call_count)
4895 
4896         # This is not a conflict that the API user can ever resolve. It is a
4897         # serious inconsistency in our database or a bug in the scheduler code
4898         # doing the claim.
4899         self.assertEqual(500, server['fault']['code'])
4900         # The instance is in ERROR state so the allocations are in limbo but
4901         # at least we expect that when the instance is deleted the allocations
4902         # are cleaned up properly.
4903         self._delete_and_check_allocations(server)
4904 
4905     def test_live_migrate_drop_allocation_on_source_fails(self):
4906         source_hostname = self.compute1.host
4907         dest_hostname = self.compute2.host
4908         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4909         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4910 
4911         # the ability to force live migrate a server is removed entirely in
4912         # 2.68
4913         self.api.microversion = '2.67'
4914 
4915         server = self._boot_and_check_allocations(
4916             self.flavor, source_hostname)
4917 
4918         fake_notifier.stub_notifier(self)
4919         self.addCleanup(fake_notifier.reset)
4920 
4921         orig_put = adapter.Adapter.put
4922 
4923         rsp = fake_requests.FakeResponse(
4924             409,
4925             jsonutils.dumps(
4926                 {'errors': [
4927                     {'code': 'placement.concurrent_update',
4928                      'detail': 'consumer generation conflict'}]}))
4929 
4930         self.adapter_put_call_count = 0
4931 
4932         def fake_put(_self, url, **kwargs):
4933             self.adapter_put_call_count += 1
4934             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
4935             if url == '/allocations/%s' % migration_uuid:
4936                 return rsp
4937             else:
4938                 return orig_put(_self, url, **kwargs)
4939 
4940         with mock.patch('keystoneauth1.adapter.Adapter.put', new=fake_put):
4941             post = {
4942                 'os-migrateLive': {
4943                     'host': dest_hostname,
4944                     'block_migration': True,
4945                     'force': True,
4946                 }
4947             }
4948 
4949             self.api.post_server_action(server['id'], post)
4950 
4951             # nova does the source host cleanup _after_ setting the migration
4952             # to completed and sending end notifications so we have to wait
4953             # here a bit.
4954             time.sleep(1)
4955 
4956             # Nova failed to clean up on the source host. This right now puts
4957             # the instance to ERROR state and fails the migration.
4958             server = self._wait_for_server_parameter(self.api, server,
4959                 {'OS-EXT-SRV-ATTR:host': dest_hostname,
4960                  'status': 'ERROR'})
4961             self._wait_for_migration_status(server, ['error'])
4962             fake_notifier.wait_for_versioned_notifications(
4963                 'instance.live_migration_post.end')
4964 
4965         # 1 claim on destination, 1 normal delete on dest that fails,
4966         self.assertEqual(2, self.adapter_put_call_count)
4967 
4968         # As the cleanup on the source host failed Nova leaks the allocation
4969         # held by the migration.
4970         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4971         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
4972         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
4973                                            source_rp_uuid)
4974 
4975         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor)
4976 
4977         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4978                                            dest_rp_uuid)
4979 
4980         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4981         # after the instance is deleted. At least nova logs a fat ERROR.
4982         self.assertIn('Deleting allocation in placement for migration %s '
4983                       'failed. The instance %s will be put to ERROR state but '
4984                       'the allocation held by the migration is leaked.' %
4985                       (migration_uuid, server['id']),
4986                       self.stdlog.logger.output)
4987 
4988         self.api.delete_server(server['id'])
4989         self._wait_until_deleted(server)
4990         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4991 
4992         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
4993                                            source_rp_uuid)
4994 
4995     def _test_evacuate_fails_allocating_on_dest_host(self, force):
4996         source_hostname = self.compute1.host
4997         dest_hostname = self.compute2.host
4998 
4999         # the ability to force evacuate a server is removed entirely in 2.68
5000         self.api.microversion = '2.67'
5001 
5002         server = self._boot_and_check_allocations(
5003             self.flavor, source_hostname)
5004 
5005         source_compute_id = self.admin_api.get_services(
5006             host=source_hostname, binary='nova-compute')[0]['id']
5007 
5008         self.compute1.stop()
5009         # force it down to avoid waiting for the service group to time out
5010         self.admin_api.put_service(
5011             source_compute_id, {'forced_down': 'true'})
5012 
5013         rsp = fake_requests.FakeResponse(
5014             409,
5015             jsonutils.dumps(
5016                 {'errors': [
5017                     {'code': 'placement.concurrent_update',
5018                      'detail': 'consumer generation conflict'}]}))
5019 
5020         with mock.patch('keystoneauth1.adapter.Adapter.put',
5021                         autospec=True) as mock_put:
5022             mock_put.return_value = rsp
5023             post = {
5024                 'evacuate': {
5025                     'force': force
5026                 }
5027             }
5028             if force:
5029                 post['evacuate']['host'] = dest_hostname
5030 
5031             self.api.post_server_action(server['id'], post)
5032             server = self._wait_for_state_change(self.api, server, 'ERROR')
5033 
5034         self.assertEqual(1, mock_put.call_count)
5035 
5036         # As nova failed to allocate on the dest host we only expect allocation
5037         # on the source
5038         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5039         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5040 
5041         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5042 
5043         self.assertRequestMatchesUsage({'VCPU': 0,
5044                                         'MEMORY_MB': 0,
5045                                         'DISK_GB': 0}, dest_rp_uuid)
5046 
5047         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5048                                            source_rp_uuid)
5049 
5050         self._delete_and_check_allocations(server)
5051 
5052     def test_force_evacuate_fails_allocating_on_dest_host(self):
5053         self._test_evacuate_fails_allocating_on_dest_host(force=True)
5054 
5055     def test_evacuate_fails_allocating_on_dest_host(self):
5056         self._test_evacuate_fails_allocating_on_dest_host(force=False)
5057 
5058     def test_server_delete_fails_due_to_conflict(self):
5059         source_hostname = self.compute1.host
5060 
5061         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5062 
5063         rsp = fake_requests.FakeResponse(
5064             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5065 
5066         with mock.patch('keystoneauth1.adapter.Adapter.put',
5067                         autospec=True) as mock_put:
5068             mock_put.return_value = rsp
5069 
5070             self.api.delete_server(server['id'])
5071             server = self._wait_for_state_change(self.admin_api, server,
5072                                                  'ERROR')
5073             self.assertEqual(1, mock_put.call_count)
5074 
5075         # We still have the allocations as deletion failed
5076         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5077         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5078 
5079         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5080                                            source_rp_uuid)
5081 
5082         # retry the delete to make sure that allocations are removed this time
5083         self._delete_and_check_allocations(server)
5084 
5085     def test_server_local_delete_fails_due_to_conflict(self):
5086         source_hostname = self.compute1.host
5087 
5088         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5089         source_compute_id = self.admin_api.get_services(
5090             host=self.compute1.host, binary='nova-compute')[0]['id']
5091         self.compute1.stop()
5092         self.admin_api.put_service(
5093             source_compute_id, {'forced_down': 'true'})
5094 
5095         rsp = fake_requests.FakeResponse(
5096             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5097 
5098         with mock.patch('keystoneauth1.adapter.Adapter.put',
5099                         autospec=True) as mock_put:
5100             mock_put.return_value = rsp
5101 
5102             ex = self.assertRaises(client.OpenStackApiException,
5103                                    self.api.delete_server, server['id'])
5104             self.assertEqual(409, ex.response.status_code)
5105             self.assertIn('Failed to delete allocations for consumer',
5106                           jsonutils.loads(ex.response.content)[
5107                               'conflictingRequest']['message'])
5108             self.assertEqual(1, mock_put.call_count)
5109 
5110         # We still have the allocations as deletion failed
5111         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5112         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5113 
5114         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5115                                            source_rp_uuid)
5116 
5117         # retry the delete to make sure that allocations are removed this time
5118         self._delete_and_check_allocations(server)
5119 
5120 
5121 class ServerMovingTestsWithNestedComputes(ServerMovingTests):
5122     """Runs all the server moving tests while the computes have nested trees.
5123     The servers still do not request resources from any child provider though.
5124     """
5125     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
5126 
5127 
5128 class ServerMovingTestsWithNestedResourceRequests(
5129     ServerMovingTestsWithNestedComputes):
5130     """Runs all the server moving tests while the computes have nested trees.
5131     The servers also request resources from child providers.
5132     """
5133 
5134     def setUp(self):
5135         super(ServerMovingTestsWithNestedResourceRequests, self).setUp()
5136         # modify the flavors used in the ServerMoving test base class to
5137         # require one piece of CUSTOM_MAGIC resource as well.
5138 
5139         for flavor in [self.flavor1, self.flavor2, self.flavor3]:
5140             self.api.post_extra_spec(
5141                 flavor['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5142             # save the extra_specs in the flavor stored in the test case as
5143             # well
5144             flavor['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5145 
5146     def _check_allocation_during_evacuate(
5147             self, flavor, server_uuid, source_root_rp_uuid, dest_root_rp_uuid):
5148         # NOTE(gibi): evacuate is the only case when the same consumer has
5149         # allocation from two different RP trees so we need a special check
5150         # here.
5151         allocations = self._get_allocations_by_server_uuid(server_uuid)
5152         source_rps = self._get_all_rp_uuids_in_a_tree(source_root_rp_uuid)
5153         dest_rps = self._get_all_rp_uuids_in_a_tree(dest_root_rp_uuid)
5154 
5155         self.assertEqual(set(source_rps + dest_rps), set(allocations))
5156 
5157         total_source_allocation = collections.defaultdict(int)
5158         total_dest_allocation = collections.defaultdict(int)
5159         for rp, alloc in allocations.items():
5160             for rc, value in alloc['resources'].items():
5161                 if rp in source_rps:
5162                     total_source_allocation[rc] += value
5163                 else:
5164                     total_dest_allocation[rc] += value
5165 
5166         self.assertEqual(
5167             self._resources_from_flavor(flavor), total_source_allocation)
5168         self.assertEqual(
5169             self._resources_from_flavor(flavor), total_dest_allocation)
5170 
5171     def test_live_migrate_force(self):
5172         # Nova intentionally does not support force live-migrating server
5173         # with nested allocations.
5174 
5175         source_hostname = self.compute1.host
5176         dest_hostname = self.compute2.host
5177         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5178         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5179 
5180         # the ability to force live migrate a server is removed entirely in
5181         # 2.68
5182         self.api.microversion = '2.67'
5183 
5184         server = self._boot_and_check_allocations(
5185             self.flavor1, source_hostname)
5186         post = {
5187             'os-migrateLive': {
5188                 'host': dest_hostname,
5189                 'block_migration': True,
5190                 'force': True,
5191             }
5192         }
5193 
5194         self.api.post_server_action(server['id'], post)
5195         self._wait_for_migration_status(server, ['error'])
5196         self._wait_for_server_parameter(self.api, server,
5197             {'OS-EXT-SRV-ATTR:host': source_hostname,
5198              'status': 'ACTIVE'})
5199         self.assertIn('Unable to move instance %s to host host2. The instance '
5200                       'has complex allocations on the source host so move '
5201                       'cannot be forced.' %
5202                       server['id'],
5203                       self.stdlog.logger.output)
5204 
5205         self._run_periodics()
5206 
5207         # NOTE(danms): There should be no usage for the dest
5208         self.assertRequestMatchesUsage(
5209             {'VCPU': 0,
5210              'MEMORY_MB': 0,
5211              'DISK_GB': 0}, dest_rp_uuid)
5212 
5213         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5214 
5215         # the server has an allocation on only the source node
5216         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5217                                            source_rp_uuid)
5218 
5219         self._delete_and_check_allocations(server)
5220 
5221     def _test_evacuate_forced_host(self, keep_hypervisor_state):
5222         # Nova intentionally does not support force evacuating server
5223         # with nested allocations.
5224 
5225         source_hostname = self.compute1.host
5226         dest_hostname = self.compute2.host
5227 
5228         # the ability to force evacuate a server is removed entirely in 2.68
5229         self.api.microversion = '2.67'
5230 
5231         server = self._boot_and_check_allocations(
5232             self.flavor1, source_hostname)
5233 
5234         source_compute_id = self.admin_api.get_services(
5235             host=source_hostname, binary='nova-compute')[0]['id']
5236 
5237         self.compute1.stop()
5238         # force it down to avoid waiting for the service group to time out
5239         self.admin_api.put_service(
5240             source_compute_id, {'forced_down': 'true'})
5241 
5242         # evacuate the server and force the destination host which bypasses
5243         # the scheduler
5244         post = {
5245             'evacuate': {
5246                 'host': dest_hostname,
5247                 'force': True
5248             }
5249         }
5250         self.api.post_server_action(server['id'], post)
5251         self._wait_for_migration_status(server, ['error'])
5252         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
5253                            'status': 'ACTIVE'}
5254         server = self._wait_for_server_parameter(self.api, server,
5255                                                  expected_params)
5256         self.assertIn('Unable to move instance %s to host host2. The instance '
5257                       'has complex allocations on the source host so move '
5258                       'cannot be forced.' %
5259                       server['id'],
5260                       self.stdlog.logger.output)
5261 
5262         # Run the periodics to show those don't modify allocations.
5263         self._run_periodics()
5264 
5265         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5266         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5267 
5268         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5269 
5270         self.assertRequestMatchesUsage(
5271             {'VCPU': 0,
5272              'MEMORY_MB': 0,
5273              'DISK_GB': 0}, dest_rp_uuid)
5274 
5275         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5276                                            source_rp_uuid)
5277 
5278         # restart the source compute
5279         self.compute1 = self.restart_compute_service(
5280             self.compute1, keep_hypervisor_state=keep_hypervisor_state)
5281         self.admin_api.put_service(
5282             source_compute_id, {'forced_down': 'false'})
5283 
5284         # Run the periodics again to show they don't change anything.
5285         self._run_periodics()
5286 
5287         # When the source node starts up nothing should change as the
5288         # evacuation failed
5289         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5290 
5291         self.assertRequestMatchesUsage(
5292             {'VCPU': 0,
5293              'MEMORY_MB': 0,
5294              'DISK_GB': 0}, dest_rp_uuid)
5295 
5296         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5297                                            source_rp_uuid)
5298 
5299         self._delete_and_check_allocations(server)
5300 
5301 
5302 # NOTE(gibi): There is another case NestedToFlat but that leads to the same
5303 # code path that NestedToNested as in both cases the instance will have
5304 # complex allocation on the source host which is already covered in
5305 # ServerMovingTestsWithNestedResourceRequests
5306 class ServerMovingTestsFromFlatToNested(
5307         integrated_helpers.ProviderUsageBaseTestCase):
5308     """Tests trying to move servers from a compute with a flat RP tree to a
5309     compute with a nested RP tree and assert that the blind allocation copy
5310     fails cleanly.
5311     """
5312 
5313     REQUIRES_LOCKING = True
5314     compute_driver = 'fake.MediumFakeDriver'
5315 
5316     def setUp(self):
5317         super(ServerMovingTestsFromFlatToNested, self).setUp()
5318         flavors = self.api.get_flavors()
5319         self.flavor1 = flavors[0]
5320         self.api.post_extra_spec(
5321             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5322         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5323 
5324     def test_force_live_migrate_from_flat_to_nested(self):
5325         # first compute will start with the flat RP tree but we add
5326         # CUSTOM_MAGIC inventory to the root compute RP
5327         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5328 
5329         # the ability to force live migrate a server is removed entirely in
5330         # 2.68
5331         self.api.microversion = '2.67'
5332 
5333         def stub_update_provider_tree(self, provider_tree, nodename,
5334                                       allocations=None):
5335             # do the regular inventory update
5336             orig_update_provider_tree(
5337                 self, provider_tree, nodename, allocations)
5338             if nodename == 'host1':
5339                 # add the extra resource
5340                 inv = provider_tree.data(nodename).inventory
5341                 inv['CUSTOM_MAGIC'] = {
5342                     'total': 10,
5343                     'reserved': 0,
5344                     'min_unit': 1,
5345                     'max_unit': 10,
5346                     'step_size': 1,
5347                     'allocation_ratio': 1,
5348                 }
5349                 provider_tree.update_inventory(nodename, inv)
5350 
5351         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5352                       stub_update_provider_tree)
5353         self.compute1 = self._start_compute(host='host1')
5354         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5355 
5356         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5357         # start the second compute with nested RP tree
5358         self.flags(
5359             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5360         self.compute2 = self._start_compute(host='host2')
5361 
5362         # try to force live migrate from flat to nested.
5363         post = {
5364             'os-migrateLive': {
5365                 'host': 'host2',
5366                 'block_migration': True,
5367                 'force': True,
5368             }
5369         }
5370 
5371         self.api.post_server_action(server['id'], post)
5372         # We expect that the migration will fail as force migrate tries to
5373         # blindly copy the source allocation to the destination but on the
5374         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5375         # provider as that resource is reported on a child provider.
5376         self._wait_for_server_parameter(self.api, server,
5377             {'OS-EXT-SRV-ATTR:host': 'host1',
5378              'status': 'ACTIVE'})
5379 
5380         migration = self._wait_for_migration_status(server, ['error'])
5381         self.assertEqual('host1', migration['source_compute'])
5382         self.assertEqual('host2', migration['dest_compute'])
5383 
5384         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5385         # from the dest node root RP and placement rejects the that allocation.
5386         self.assertIn("Unable to allocate inventory: Inventory for "
5387                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5388         self.assertIn('No valid host was found. Unable to move instance %s to '
5389                       'host host2. There is not enough capacity on the host '
5390                       'for the instance.' % server['id'],
5391                       self.stdlog.logger.output)
5392 
5393         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5394 
5395         # There should be no usage for the dest
5396         self.assertRequestMatchesUsage(
5397             {'VCPU': 0,
5398              'MEMORY_MB': 0,
5399              'DISK_GB': 0}, dest_rp_uuid)
5400 
5401         # and everything stays at the source
5402         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5403         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5404                                            source_rp_uuid)
5405 
5406         self._delete_and_check_allocations(server)
5407 
5408     def test_force_evacuate_from_flat_to_nested(self):
5409         # first compute will start with the flat RP tree but we add
5410         # CUSTOM_MAGIC inventory to the root compute RP
5411         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5412 
5413         # the ability to force evacuate a server is removed entirely in 2.68
5414         self.api.microversion = '2.67'
5415 
5416         def stub_update_provider_tree(self, provider_tree, nodename,
5417                                       allocations=None):
5418             # do the regular inventory update
5419             orig_update_provider_tree(
5420                 self, provider_tree, nodename, allocations)
5421             if nodename == 'host1':
5422                 # add the extra resource
5423                 inv = provider_tree.data(nodename).inventory
5424                 inv['CUSTOM_MAGIC'] = {
5425                     'total': 10,
5426                     'reserved': 0,
5427                     'min_unit': 1,
5428                     'max_unit': 10,
5429                     'step_size': 1,
5430                     'allocation_ratio': 1,
5431                 }
5432                 provider_tree.update_inventory(nodename, inv)
5433 
5434         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5435                       stub_update_provider_tree)
5436         self.compute1 = self._start_compute(host='host1')
5437         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5438 
5439         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5440         # start the second compute with nested RP tree
5441         self.flags(
5442             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5443         self.compute2 = self._start_compute(host='host2')
5444 
5445         source_compute_id = self.admin_api.get_services(
5446             host='host1', binary='nova-compute')[0]['id']
5447         self.compute1.stop()
5448         # force it down to avoid waiting for the service group to time out
5449         self.admin_api.put_service(
5450             source_compute_id, {'forced_down': 'true'})
5451 
5452         # try to force evacuate from flat to nested.
5453         post = {
5454             'evacuate': {
5455                 'host': 'host2',
5456                 'force': True,
5457             }
5458         }
5459 
5460         self.api.post_server_action(server['id'], post)
5461         # We expect that the evacuation will fail as force evacuate tries to
5462         # blindly copy the source allocation to the destination but on the
5463         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5464         # provider as that resource is reported on a child provider.
5465         self._wait_for_server_parameter(self.api, server,
5466             {'OS-EXT-SRV-ATTR:host': 'host1',
5467              'status': 'ACTIVE'})
5468 
5469         migration = self._wait_for_migration_status(server, ['error'])
5470         self.assertEqual('host1', migration['source_compute'])
5471         self.assertEqual('host2', migration['dest_compute'])
5472 
5473         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5474         # from the dest node root RP and placement rejects the that allocation.
5475         self.assertIn("Unable to allocate inventory: Inventory for "
5476                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5477         self.assertIn('No valid host was found. Unable to move instance %s to '
5478                       'host host2. There is not enough capacity on the host '
5479                       'for the instance.' % server['id'],
5480                       self.stdlog.logger.output)
5481 
5482         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5483 
5484         # There should be no usage for the dest
5485         self.assertRequestMatchesUsage(
5486             {'VCPU': 0,
5487              'MEMORY_MB': 0,
5488              'DISK_GB': 0}, dest_rp_uuid)
5489 
5490         # and everything stays at the source
5491         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5492         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5493                                            source_rp_uuid)
5494 
5495         self._delete_and_check_allocations(server)
5496 
5497 
5498 class PortResourceRequestBasedSchedulingTestBase(
5499         integrated_helpers.ProviderUsageBaseTestCase):
5500 
5501     compute_driver = 'fake.FakeDriverWithPciResources'
5502 
5503     CUSTOM_VNIC_TYPE_NORMAL = 'CUSTOM_VNIC_TYPE_NORMAL'
5504     CUSTOM_VNIC_TYPE_DIRECT = 'CUSTOM_VNIC_TYPE_DIRECT'
5505     CUSTOM_VNIC_TYPE_MACVTAP = 'CUSTOM_VNIC_TYPE_MACVTAP'
5506     CUSTOM_PHYSNET1 = 'CUSTOM_PHYSNET1'
5507     CUSTOM_PHYSNET2 = 'CUSTOM_PHYSNET2'
5508     CUSTOM_PHYSNET3 = 'CUSTOM_PHYSNET3'
5509 
5510     def setUp(self):
5511         # enable PciPassthroughFilter to support SRIOV before the base class
5512         # starts the scheduler
5513         if 'PciPassthroughFilter' not in CONF.filter_scheduler.enabled_filters:
5514             self.flags(
5515                 enabled_filters=CONF.filter_scheduler.enabled_filters +
5516                                 ['PciPassthroughFilter'],
5517                 group='filter_scheduler')
5518 
5519         self.useFixture(
5520             fake.FakeDriverWithPciResources.
5521                 FakeDriverWithPciResourcesConfigFixture())
5522 
5523         super(PortResourceRequestBasedSchedulingTestBase, self).setUp()
5524         self.compute1 = self._start_compute('host1')
5525         self.compute1_rp_uuid = self._get_provider_uuid_by_host('host1')
5526         self.ovs_bridge_rp_per_host = {}
5527         self.sriov_dev_rp_per_host = {}
5528         self.flavor = self.api.get_flavors()[0]
5529         self.flavor_with_group_policy = self.api.get_flavors()[1]
5530 
5531         # Setting group policy for placement. This is mandatory when more than
5532         # one request group is included in the allocation candidate request and
5533         # we have tests with two ports both having resource request modelled as
5534         # two separate request groups.
5535         self.admin_api.post_extra_spec(
5536             self.flavor_with_group_policy['id'],
5537             {'extra_specs': {'group_policy': 'isolate'}})
5538 
5539         self._create_networking_rp_tree(self.compute1_rp_uuid)
5540 
5541         # add extra ports and the related network to the neutron fixture
5542         # specifically for these tests. It cannot be added globally in the
5543         # fixture init as it adds a second network that makes auto allocation
5544         # based test to fail due to ambiguous networks.
5545         self.neutron._ports[
5546             self.neutron.port_with_sriov_resource_request['id']] = \
5547             copy.deepcopy(self.neutron.port_with_sriov_resource_request)
5548         self.neutron._ports[self.neutron.sriov_port['id']] = \
5549             copy.deepcopy(self.neutron.sriov_port)
5550         self.neutron._networks[
5551             self.neutron.network_2['id']] = self.neutron.network_2
5552         self.neutron._subnets[
5553             self.neutron.subnet_2['id']] = self.neutron.subnet_2
5554         macvtap = self.neutron.port_macvtap_with_resource_request
5555         self.neutron._ports[macvtap['id']] = copy.deepcopy(macvtap)
5556 
5557     def assertComputeAllocationMatchesFlavor(
5558             self, allocations, compute_rp_uuid, flavor):
5559         compute_allocations = allocations[compute_rp_uuid]['resources']
5560         self.assertEqual(
5561             self._resources_from_flavor(flavor),
5562             compute_allocations)
5563 
5564     def _create_server(self, flavor, networks, host=None):
5565         server_req = self._build_minimal_create_server_request(
5566             self.api, 'bandwidth-aware-server',
5567             image_uuid='76fa36fc-c930-4bf3-8c8a-ea2a2420deb6',
5568             flavor_id=flavor['id'], networks=networks,
5569             host=host)
5570         return self.api.post_server({'server': server_req})
5571 
5572     def _set_provider_inventories(self, rp_uuid, inventories):
5573         rp = self.placement_api.get(
5574             '/resource_providers/%s' % rp_uuid).body
5575         inventories['resource_provider_generation'] = rp['generation']
5576         return self._update_inventory(rp_uuid, inventories)
5577 
5578     def _create_ovs_networking_rp_tree(self, compute_rp_uuid):
5579         # we need uuid sentinel for the test to make pep8 happy but we need a
5580         # unique one per compute so here is some ugliness
5581         ovs_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'ovs agent')
5582         agent_rp_req = {
5583             "name": ovs_agent_rp_uuid,
5584             "uuid": ovs_agent_rp_uuid,
5585             "parent_provider_uuid": compute_rp_uuid
5586         }
5587         self.placement_api.post('/resource_providers',
5588                                 body=agent_rp_req,
5589                                 version='1.20')
5590         ovs_bridge_rp_uuid = getattr(uuids, ovs_agent_rp_uuid + 'ovs br')
5591         ovs_bridge_req = {
5592             "name": ovs_bridge_rp_uuid,
5593             "uuid": ovs_bridge_rp_uuid,
5594             "parent_provider_uuid": ovs_agent_rp_uuid
5595         }
5596         self.placement_api.post('/resource_providers',
5597                                 body=ovs_bridge_req,
5598                                 version='1.20')
5599         self.ovs_bridge_rp_per_host[compute_rp_uuid] = ovs_bridge_rp_uuid
5600 
5601         self._set_provider_inventories(
5602             ovs_bridge_rp_uuid,
5603             {"inventories": {
5604                 orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 10000},
5605                 orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 10000},
5606             }})
5607 
5608         self._create_trait(self.CUSTOM_VNIC_TYPE_NORMAL)
5609         self._create_trait(self.CUSTOM_PHYSNET2)
5610 
5611         self._set_provider_traits(
5612             ovs_bridge_rp_uuid,
5613             [self.CUSTOM_VNIC_TYPE_NORMAL, self.CUSTOM_PHYSNET2])
5614 
5615     def _create_pf_device_rp(
5616             self, device_rp_uuid, parent_rp_uuid, inventories, traits,
5617             device_rp_name=None):
5618         """Create a RP in placement for a physical function network device with
5619         traits and inventories.
5620         """
5621 
5622         if not device_rp_name:
5623             device_rp_name = device_rp_uuid
5624 
5625         sriov_pf_req = {
5626             "name": device_rp_name,
5627             "uuid": device_rp_uuid,
5628             "parent_provider_uuid": parent_rp_uuid
5629         }
5630         self.placement_api.post('/resource_providers',
5631                                 body=sriov_pf_req,
5632                                 version='1.20')
5633 
5634         self._set_provider_inventories(
5635             device_rp_uuid,
5636             {"inventories": inventories})
5637 
5638         for trait in traits:
5639             self._create_trait(trait)
5640 
5641         self._set_provider_traits(
5642             device_rp_uuid,
5643             traits)
5644 
5645     def _create_sriov_networking_rp_tree(self, compute_rp_uuid):
5646         # Create a matching RP tree in placement for the PCI devices added to
5647         # the passthrough_whitelist config during setUp() and PCI devices
5648         # present in the FakeDriverWithPciResources virt driver.
5649         #
5650         # * PF1 represents the PCI device 0000:01:00, it will be mapped to
5651         # physnet1 and it will have bandwidth inventory.
5652         # * PF2 represents the PCI device 0000:02:00, it will be mapped to
5653         # physnet2 it will have bandwidth inventory.
5654         # * PF3 represents the PCI device 0000:03:00 and, it will be mapped to
5655         # physnet2 but it will not have bandwidth inventory.
5656         self.sriov_dev_rp_per_host[compute_rp_uuid] = {}
5657 
5658         compute_name = compute_rp_uuid
5659         sriov_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'sriov agent')
5660         agent_rp_req = {
5661             "name": "%s:NIC Switch agent" % compute_name,
5662             "uuid": sriov_agent_rp_uuid,
5663             "parent_provider_uuid": compute_rp_uuid
5664         }
5665         self.placement_api.post('/resource_providers',
5666                                 body=agent_rp_req,
5667                                 version='1.20')
5668 
5669         sriov_pf1_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF1')
5670         self.sriov_dev_rp_per_host[compute_rp_uuid]['pf1'] = sriov_pf1_rp_uuid
5671 
5672         inventories = {
5673             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
5674             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
5675         }
5676         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET1]
5677         self._create_pf_device_rp(
5678             sriov_pf1_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5679             device_rp_name="%s:NIC Switch agent:ens1" % compute_name)
5680 
5681         sriov_pf2_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF2')
5682         self.sriov_dev_rp_per_host[compute_rp_uuid]['pf2'] = sriov_pf2_rp_uuid
5683         inventories = {
5684             orc.NET_BW_IGR_KILOBIT_PER_SEC: {"total": 100000},
5685             orc.NET_BW_EGR_KILOBIT_PER_SEC: {"total": 100000},
5686         }
5687         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_VNIC_TYPE_MACVTAP,
5688                   self.CUSTOM_PHYSNET2]
5689         self._create_pf_device_rp(
5690             sriov_pf2_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5691             device_rp_name="%s:NIC Switch agent:ens2" % compute_name)
5692 
5693         sriov_pf3_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF3')
5694         self.sriov_dev_rp_per_host[compute_rp_uuid]['pf3'] = sriov_pf3_rp_uuid
5695         inventories = {}
5696         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
5697         self._create_pf_device_rp(
5698             sriov_pf3_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5699             device_rp_name="%s:NIC Switch agent:ens3" % compute_name)
5700 
5701     def _create_networking_rp_tree(self, compute_rp_uuid):
5702         # let's simulate what the neutron would do
5703         self._create_ovs_networking_rp_tree(compute_rp_uuid)
5704         self._create_sriov_networking_rp_tree(compute_rp_uuid)
5705 
5706     def assertPortMatchesAllocation(self, port, allocations):
5707         port_request = port[constants.RESOURCE_REQUEST]['resources']
5708         for rc, amount in allocations.items():
5709             self.assertEqual(port_request[rc], amount,
5710                              'port %s requested %d %s '
5711                              'resources but got allocation %d' %
5712                              (port['id'], port_request[rc], rc,
5713                               amount))
5714 
5715 
5716 class UnsupportedPortResourceRequestBasedSchedulingTest(
5717         PortResourceRequestBasedSchedulingTestBase):
5718     """Tests for handling servers with ports having resource requests """
5719 
5720     def _add_resource_request_to_a_bound_port(self, port_id):
5721         # NOTE(gibi): self.neutron._ports contains a copy of each neutron port
5722         # defined on class level in the fixture. So modifying what is in the
5723         # _ports list is safe as it is re-created for each Neutron fixture
5724         # instance therefore for each individual test using that fixture.
5725         bound_port = self.neutron._ports[port_id]
5726         bound_port[constants.RESOURCE_REQUEST] = (
5727             self.neutron.port_with_resource_request[
5728                 constants.RESOURCE_REQUEST])
5729 
5730     def test_interface_attach_with_port_resource_request(self):
5731         # create a server
5732         server = self._create_server(
5733             flavor=self.flavor,
5734             networks=[{'port': self.neutron.port_1['id']}])
5735         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5736 
5737         # try to add a port with resource request
5738         post = {
5739             'interfaceAttachment': {
5740                 'port_id': self.neutron.port_with_resource_request['id']
5741         }}
5742         ex = self.assertRaises(client.OpenStackApiException,
5743                                self.api.attach_interface,
5744                                server['id'], post)
5745         self.assertEqual(400, ex.response.status_code)
5746         self.assertIn('Attaching interfaces with QoS policy is '
5747                       'not supported for instance',
5748                       six.text_type(ex))
5749 
5750     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
5751     def test_interface_attach_with_network_create_port_has_resource_request(
5752             self, mock_neutron_create_port):
5753         # create a server
5754         server = self._create_server(
5755             flavor=self.flavor,
5756             networks=[{'port': self.neutron.port_1['id']}])
5757         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5758 
5759         # the interfaceAttach operation below will result in a new port being
5760         # created in the network that is attached. Make sure that neutron
5761         # returns a port that has resource request.
5762         mock_neutron_create_port.return_value = (
5763             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
5764 
5765         # try to attach a network
5766         post = {
5767             'interfaceAttachment': {
5768                 'net_id': self.neutron.network_1['id']
5769         }}
5770         ex = self.assertRaises(client.OpenStackApiException,
5771                                self.api.attach_interface,
5772                                server['id'], post)
5773         self.assertEqual(400, ex.response.status_code)
5774         self.assertIn('Using networks with QoS policy is not supported for '
5775                       'instance',
5776                       six.text_type(ex))
5777 
5778     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
5779     def test_create_server_with_network_create_port_has_resource_request(
5780             self, mock_neutron_create_port):
5781         # the server create operation below will result in a new port being
5782         # created in the network. Make sure that neutron returns a port that
5783         # has resource request.
5784         mock_neutron_create_port.return_value = (
5785             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
5786 
5787         server = self._create_server(
5788             flavor=self.flavor,
5789             networks=[{'uuid': self.neutron.network_1['id']}])
5790         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
5791 
5792         self.assertEqual(500, server['fault']['code'])
5793         self.assertIn('Failed to allocate the network',
5794                       server['fault']['message'])
5795 
5796     def test_create_server_with_port_resource_request_old_microversion(self):
5797 
5798         # NOTE(gibi): 2.71 is the last microversion where nova does not support
5799         # this kind of create server
5800         self.api.microversion = '2.71'
5801         ex = self.assertRaises(
5802             client.OpenStackApiException, self._create_server,
5803             flavor=self.flavor,
5804             networks=[{'port': self.neutron.port_with_resource_request['id']}])
5805 
5806         self.assertEqual(400, ex.response.status_code)
5807         self.assertIn(
5808             "Creating servers with ports having resource requests, like a "
5809             "port with a QoS minimum bandwidth policy, is not supported "
5810             "until microversion 2.72.",
5811             six.text_type(ex))
5812 
5813     def test_resize_server_with_port_resource_request_old_microversion(self):
5814         server = self._create_server(
5815             flavor=self.flavor,
5816             networks=[{'port': self.neutron.port_1['id']}])
5817         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5818 
5819         # We need to simulate that the above server has a port that has
5820         # resource request; we cannot boot with such a port but legacy servers
5821         # can exist with such a port.
5822         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5823 
5824         resize_req = {
5825             'resize': {
5826                 'flavorRef': self.flavor['id']
5827             }
5828         }
5829         ex = self.assertRaises(
5830             client.OpenStackApiException,
5831             self.api.post_server_action, server['id'], resize_req)
5832 
5833         self.assertEqual(400, ex.response.status_code)
5834         self.assertIn(
5835             'The resize action on a server with ports having resource '
5836             'requests', six.text_type(ex))
5837 
5838     def test_migrate_server_with_port_resource_request_old_microversion(self):
5839         server = self._create_server(
5840             flavor=self.flavor,
5841             networks=[{'port': self.neutron.port_1['id']}])
5842         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5843 
5844         # We need to simulate that the above server has a port that has
5845         # resource request; we cannot boot with such a port but legacy servers
5846         # can exist with such a port.
5847         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5848 
5849         ex = self.assertRaises(
5850             client.OpenStackApiException,
5851             self.api.post_server_action, server['id'], {'migrate': None})
5852 
5853         self.assertEqual(400, ex.response.status_code)
5854         self.assertIn(
5855             'The migrate action on a server with ports having resource '
5856             'requests', six.text_type(ex))
5857 
5858     def test_live_migrate_server_with_port_resource_request_old_microversion(
5859             self):
5860         server = self._create_server(
5861             flavor=self.flavor,
5862             networks=[{'port': self.neutron.port_1['id']}])
5863         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5864 
5865         # We need to simulate that the above server has a port that has
5866         # resource request; we cannot boot with such a port but legacy servers
5867         # can exist with such a port.
5868         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5869 
5870         post = {
5871             'os-migrateLive': {
5872                 'host': None,
5873                 'block_migration': False,
5874             }
5875         }
5876         ex = self.assertRaises(
5877             client.OpenStackApiException,
5878             self.api.post_server_action, server['id'], post)
5879 
5880         self.assertEqual(400, ex.response.status_code)
5881         self.assertIn(
5882             'The os-migrateLive action on a server with ports having resource '
5883             'requests', six.text_type(ex))
5884 
5885     def test_evacuate_server_with_port_resource_request_old_microversion(
5886             self):
5887         server = self._create_server(
5888             flavor=self.flavor,
5889             networks=[{'port': self.neutron.port_1['id']}])
5890         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5891 
5892         # We need to simulate that the above server has a port that has
5893         # resource request; we cannot boot with such a port but legacy servers
5894         # can exist with such a port.
5895         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5896 
5897         ex = self.assertRaises(
5898             client.OpenStackApiException,
5899             self.api.post_server_action, server['id'], {'evacuate': {}})
5900 
5901         self.assertEqual(400, ex.response.status_code)
5902         self.assertIn(
5903             'The evacuate action on a server with ports having resource '
5904             'requests', six.text_type(ex))
5905 
5906     def test_unshelve_offloaded_server_with_port_resource_request_old_version(
5907             self):
5908         server = self._create_server(
5909             flavor=self.flavor,
5910             networks=[{'port': self.neutron.port_1['id']}])
5911         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5912 
5913         # with default config shelve means immediate offload as well
5914         req = {
5915             'shelve': {}
5916         }
5917         self.api.post_server_action(server['id'], req)
5918         self._wait_for_server_parameter(
5919             self.api, server, {'status': 'SHELVED_OFFLOADED'})
5920 
5921         # We need to simulate that the above server has a port that has
5922         # resource request; we cannot boot with such a port but legacy servers
5923         # can exist with such a port.
5924         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5925 
5926         ex = self.assertRaises(
5927             client.OpenStackApiException,
5928             self.api.post_server_action, server['id'], {'unshelve': None})
5929 
5930         self.assertEqual(400, ex.response.status_code)
5931         self.assertIn(
5932             'The unshelve action on a server with ports having resource '
5933             'requests', six.text_type(ex))
5934 
5935     def test_unshelve_not_offloaded_server_with_port_resource_request(
5936             self):
5937         """If the server is not offloaded then unshelving does not cause a new
5938         resource allocation therefore having port resource request is
5939         irrelevant. This test asserts that such unshelve request is not
5940         rejected.
5941         """
5942         server = self._create_server(
5943             flavor=self.flavor,
5944             networks=[{'port': self.neutron.port_1['id']}])
5945         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5946 
5947         # avoid automatic shelve offloading
5948         self.flags(shelved_offload_time=-1)
5949         req = {
5950             'shelve': {}
5951         }
5952         self.api.post_server_action(server['id'], req)
5953         self._wait_for_server_parameter(
5954             self.api, server, {'status': 'SHELVED'})
5955 
5956         # We need to simulate that the above server has a port that has
5957         # resource request; we cannot boot with such a port but legacy servers
5958         # can exist with such a port.
5959         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5960 
5961         self.api.post_server_action(server['id'], {'unshelve': None})
5962         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5963 
5964 
5965 class PortResourceRequestBasedSchedulingTest(
5966         PortResourceRequestBasedSchedulingTestBase):
5967     """Tests creating a server with a pre-existing port that has a resource
5968     request for a QoS minimum bandwidth policy.
5969     """
5970 
5971     def test_boot_server_with_two_ports_one_having_resource_request(self):
5972         non_qos_port = self.neutron.port_1
5973         qos_port = self.neutron.port_with_resource_request
5974 
5975         server = self._create_server(
5976             flavor=self.flavor,
5977             networks=[{'port': non_qos_port['id']},
5978                       {'port': qos_port['id']}])
5979         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5980         updated_non_qos_port = self.neutron.show_port(
5981             non_qos_port['id'])['port']
5982         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
5983 
5984         allocations = self.placement_api.get(
5985             '/allocations/%s' % server['id']).body['allocations']
5986 
5987         # We expect one set of allocations for the compute resources on the
5988         # compute rp and one set for the networking resources on the ovs bridge
5989         # rp due to the qos_port resource request
5990         self.assertEqual(2, len(allocations))
5991 
5992         self.assertComputeAllocationMatchesFlavor(
5993             allocations, self.compute1_rp_uuid, self.flavor)
5994         network_allocations = allocations[
5995             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
5996         self.assertPortMatchesAllocation(qos_port, network_allocations)
5997 
5998         # We expect that only the RP uuid of the networking RP having the port
5999         # allocation is sent in the port binding for the port having resource
6000         # request
6001         qos_binding_profile = updated_qos_port['binding:profile']
6002         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6003                          qos_binding_profile['allocation'])
6004 
6005         # And we expect not to have any allocation set in the port binding for
6006         # the port that doesn't have resource request
6007         self.assertNotIn('binding:profile', updated_non_qos_port)
6008 
6009         self._delete_and_check_allocations(server)
6010 
6011         # assert that unbind removes the allocation from the binding of the
6012         # port that got allocation during the bind
6013         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6014         binding_profile = updated_qos_port['binding:profile']
6015         self.assertNotIn('allocation', binding_profile)
6016 
6017     def test_one_ovs_one_sriov_port(self):
6018         ovs_port = self.neutron.port_with_resource_request
6019         sriov_port = self.neutron.port_with_sriov_resource_request
6020 
6021         server = self._create_server(flavor=self.flavor_with_group_policy,
6022                                      networks=[{'port': ovs_port['id']},
6023                                                {'port': sriov_port['id']}])
6024 
6025         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6026 
6027         ovs_port = self.neutron.show_port(ovs_port['id'])['port']
6028         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6029 
6030         allocations = self.placement_api.get(
6031             '/allocations/%s' % server['id']).body['allocations']
6032 
6033         # We expect one set of allocations for the compute resources on the
6034         # compute rp and one set for the networking resources on the ovs bridge
6035         # rp and on the sriov PF rp.
6036         self.assertEqual(3, len(allocations))
6037 
6038         self.assertComputeAllocationMatchesFlavor(
6039             allocations, self.compute1_rp_uuid, self.flavor_with_group_policy)
6040 
6041         ovs_allocations = allocations[
6042             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6043         sriov_allocations = allocations[
6044             self.sriov_dev_rp_per_host[
6045                 self.compute1_rp_uuid]['pf2']]['resources']
6046 
6047         self.assertPortMatchesAllocation(ovs_port, ovs_allocations)
6048         self.assertPortMatchesAllocation(sriov_port, sriov_allocations)
6049 
6050         # We expect that only the RP uuid of the networking RP having the port
6051         # allocation is sent in the port binding for the port having resource
6052         # request
6053         ovs_binding = ovs_port['binding:profile']
6054         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6055                          ovs_binding['allocation'])
6056         sriov_binding = sriov_port['binding:profile']
6057         self.assertEqual(
6058             self.sriov_dev_rp_per_host[self.compute1_rp_uuid]['pf2'],
6059             sriov_binding['allocation'])
6060 
6061     def test_interface_detach_with_port_with_bandwidth_request(self):
6062         port = self.neutron.port_with_resource_request
6063 
6064         # create a server
6065         server = self._create_server(
6066             flavor=self.flavor,
6067             networks=[{'port': port['id']}])
6068         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6069 
6070         allocations = self.placement_api.get(
6071             '/allocations/%s' % server['id']).body['allocations']
6072         # We expect one set of allocations for the compute resources on the
6073         # compute rp and one set for the networking resources on the ovs bridge
6074         # rp due to the port resource request
6075         self.assertEqual(2, len(allocations))
6076 
6077         self.assertComputeAllocationMatchesFlavor(
6078             allocations, self.compute1_rp_uuid, self.flavor)
6079 
6080         network_allocations = allocations[
6081             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6082         self.assertPortMatchesAllocation(port, network_allocations)
6083 
6084         # We expect that only the RP uuid of the networking RP having the port
6085         # allocation is sent in the port binding for the port having resource
6086         # request
6087         updated_port = self.neutron.show_port(port['id'])['port']
6088         binding_profile = updated_port['binding:profile']
6089         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6090                          binding_profile['allocation'])
6091 
6092         self.api.detach_interface(
6093             server['id'], self.neutron.port_with_resource_request['id'])
6094 
6095         fake_notifier.wait_for_versioned_notifications(
6096             'instance.interface_detach.end')
6097 
6098         updated_port = self.neutron.show_port(
6099             self.neutron.port_with_resource_request['id'])['port']
6100 
6101         allocations = self.placement_api.get(
6102             '/allocations/%s' % server['id']).body['allocations']
6103 
6104         # We expect that the port related resource allocations are removed
6105         self.assertEqual(1, len(allocations))
6106 
6107         self.assertComputeAllocationMatchesFlavor(
6108             allocations, self.compute1_rp_uuid, self.flavor)
6109 
6110         # We expect that the allocation is removed from the port too
6111         binding_profile = updated_port['binding:profile']
6112         self.assertNotIn('allocation', binding_profile)
6113 
6114     def test_delete_bound_port_in_neutron_with_resource_request(self):
6115         """Neutron sends a network-vif-deleted os-server-external-events
6116         notification to nova when a bound port is deleted. Nova detaches the
6117         vif from the server. If the port had a resource allocation then that
6118         allocation is leaked. This test makes sure that 1) an ERROR is logged
6119         when the leak happens. 2) the leaked resource is reclaimed when the
6120         server is deleted.
6121         """
6122         port = self.neutron.port_with_resource_request
6123 
6124         # create a server
6125         server = self._create_server(
6126             flavor=self.flavor,
6127             networks=[{'port': port['id']}])
6128         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6129 
6130         allocations = self.placement_api.get(
6131             '/allocations/%s' % server['id']).body['allocations']
6132         # We expect one set of allocations for the compute resources on the
6133         # compute rp and one set for the networking resources on the ovs bridge
6134         # rp due to the port resource request
6135         self.assertEqual(2, len(allocations))
6136         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6137         network_allocations = allocations[
6138             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6139 
6140         self.assertEqual(self._resources_from_flavor(self.flavor),
6141                          compute_allocations)
6142         self.assertPortMatchesAllocation(port, network_allocations)
6143 
6144         # We expect that only the RP uuid of the networking RP having the port
6145         # allocation is sent in the port binding for the port having resource
6146         # request
6147         updated_port = self.neutron.show_port(port['id'])['port']
6148         binding_profile = updated_port['binding:profile']
6149         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6150                          binding_profile['allocation'])
6151 
6152         # neutron is faked in the functional test so this test just sends in
6153         # a os-server-external-events notification to trigger the
6154         # detach + ERROR log.
6155         events = {
6156             "events": [
6157                 {
6158                     "name": "network-vif-deleted",
6159                     "server_uuid": server['id'],
6160                     "tag": port['id'],
6161                 }
6162             ]
6163         }
6164         response = self.api.api_post('/os-server-external-events', events).body
6165         self.assertEqual(200, response['events'][0]['code'])
6166 
6167         port_rp_uuid = self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]
6168 
6169         # 1) Nova logs an ERROR about the leak
6170         self._wait_for_log(
6171             'ERROR [nova.compute.manager] The bound port %(port_id)s is '
6172             'deleted in Neutron but the resource allocation on the resource '
6173             'provider %(rp_uuid)s is leaked until the server %(server_uuid)s '
6174             'is deleted.'
6175             % {'port_id': port['id'],
6176                'rp_uuid': port_rp_uuid,
6177                'server_uuid': server['id']})
6178 
6179         allocations = self.placement_api.get(
6180             '/allocations/%s' % server['id']).body['allocations']
6181 
6182         # Nova leaks the port allocation so the server still has the same
6183         # allocation before the port delete.
6184         self.assertEqual(2, len(allocations))
6185         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6186         network_allocations = allocations[port_rp_uuid]['resources']
6187 
6188         self.assertEqual(self._resources_from_flavor(self.flavor),
6189                          compute_allocations)
6190         self.assertPortMatchesAllocation(port, network_allocations)
6191 
6192         # 2) Also nova will reclaim the leaked resource during the server
6193         # delete
6194         self._delete_and_check_allocations(server)
6195 
6196     def test_two_sriov_ports_one_with_request_two_available_pfs(self):
6197         """Verify that the port's bandwidth allocated from the same PF as
6198         the allocated VF.
6199 
6200         One compute host:
6201         * PF1 (0000:01:00) is configured for physnet1
6202         * PF2 (0000:02:00) is configured for physnet2, with 1 VF and bandwidth
6203           inventory
6204         * PF3 (0000:03:00) is configured for physnet2, with 1 VF but without
6205           bandwidth inventory
6206 
6207         One instance will be booted with two neutron ports, both ports
6208         requested to be connected to physnet2. One port has resource request
6209         the other does not have resource request. The port having the resource
6210         request cannot be allocated to PF3 and PF1 while the other port that
6211         does not have resource request can be allocated to PF2 or PF3.
6212 
6213         For the detailed compute host config see the FakeDriverWithPciResources
6214         class. For the necessary passthrough_whitelist config see the setUp of
6215         the PortResourceRequestBasedSchedulingTestBase class.
6216         """
6217 
6218         sriov_port = self.neutron.sriov_port
6219         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6220         server = self._create_server(
6221             flavor=self.flavor_with_group_policy,
6222             networks=[
6223                 {'port': sriov_port_with_res_req['id']},
6224                 {'port': sriov_port['id']}])
6225 
6226         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6227 
6228         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6229         sriov_port_with_res_req = self.neutron.show_port(
6230             sriov_port_with_res_req['id'])['port']
6231 
6232         allocations = self.placement_api.get(
6233             '/allocations/%s' % server['id']).body['allocations']
6234 
6235         # We expect one set of allocations for the compute resources on the
6236         # compute rp and one set for the networking resources on the sriov PF2
6237         # rp.
6238         self.assertEqual(2, len(allocations))
6239 
6240         self.assertComputeAllocationMatchesFlavor(
6241             allocations, self.compute1_rp_uuid, self.flavor_with_group_policy)
6242 
6243         sriov_allocations = allocations[
6244             self.sriov_dev_rp_per_host[
6245                 self.compute1_rp_uuid]['pf2']]['resources']
6246         self.assertPortMatchesAllocation(
6247             sriov_port_with_res_req, sriov_allocations)
6248 
6249         # We expect that only the RP uuid of the networking RP having the port
6250         # allocation is sent in the port binding for the port having resource
6251         # request
6252         sriov_with_req_binding = sriov_port_with_res_req['binding:profile']
6253         self.assertEqual(
6254             self.sriov_dev_rp_per_host[self.compute1_rp_uuid]['pf2'],
6255             sriov_with_req_binding['allocation'])
6256         # and the port without resource request does not have allocation
6257         sriov_binding = sriov_port['binding:profile']
6258         self.assertNotIn('allocation', sriov_binding)
6259 
6260         # We expect that the selected PCI device matches with the RP from
6261         # where the bandwidth is allocated from. The bandwidth is allocated
6262         # from 0000:02:00 (PF2) so the PCI device should be a VF of that PF
6263         self.assertEqual(
6264             fake.FakeDriverWithPciResources.PCI_ADDR_PF2_VF1,
6265             sriov_with_req_binding['pci_slot'])
6266         # But also the port that has no resource request still gets a pci slot
6267         # allocated. The 0000:02:00 has no more VF available but 0000:03:00 has
6268         # one VF available and that PF is also on physnet2
6269         self.assertEqual(
6270             fake.FakeDriverWithPciResources.PCI_ADDR_PF3_VF1,
6271             sriov_binding['pci_slot'])
6272 
6273     def test_one_sriov_port_no_vf_and_bandwidth_available_on_the_same_pf(self):
6274         """Verify that if there is no PF that both provides bandwidth and VFs
6275         then the boot will fail.
6276         """
6277 
6278         # boot a server with a single sriov port that has no resource request
6279         sriov_port = self.neutron.sriov_port
6280         server = self._create_server(
6281             flavor=self.flavor_with_group_policy,
6282             networks=[{'port': sriov_port['id']}])
6283 
6284         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6285         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6286         sriov_binding = sriov_port['binding:profile']
6287 
6288         # We expect that this consume the last available VF from the PF2
6289         self.assertEqual(
6290             fake.FakeDriverWithPciResources.PCI_ADDR_PF2_VF1,
6291             sriov_binding['pci_slot'])
6292 
6293         # Now boot a second server with a port that has resource request
6294         # At this point PF2 has available bandwidth but no available VF
6295         # and PF3 has available VF but no available bandwidth so we expect
6296         # the boot to fail.
6297 
6298         sriov_port_with_res_req = self.neutron.port_with_sriov_resource_request
6299         server = self._create_server(
6300             flavor=self.flavor_with_group_policy,
6301             networks=[{'port': sriov_port_with_res_req['id']}])
6302 
6303         # NOTE(gibi): It should be NoValidHost in an ideal world but that would
6304         # require the scheduler to detect the situation instead of the pci
6305         # claim. However that is pretty hard as the scheduler does not know
6306         # anything about allocation candidates (e.g. that the only candidate
6307         # for the port in this case is PF2) it see the whole host as a
6308         # candidate and in our host there is available VF for the request even
6309         # if that is on the wrong PF.
6310         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
6311         self.assertIn(
6312             'Exceeded maximum number of retries. Exhausted all hosts '
6313             'available for retrying build failures for instance',
6314             server['fault']['message'])
6315 
6316     def test_sriov_macvtap_port_with_resource_request(self):
6317         """Verify that vnic type macvtap is also supported"""
6318 
6319         port = self.neutron.port_macvtap_with_resource_request
6320 
6321         server = self._create_server(
6322             flavor=self.flavor,
6323             networks=[{'port': port['id']}])
6324 
6325         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6326 
6327         port = self.neutron.show_port(port['id'])['port']
6328 
6329         allocations = self.placement_api.get(
6330             '/allocations/%s' % server['id']).body['allocations']
6331 
6332         # We expect one set of allocations for the compute resources on the
6333         # compute rp and one set for the networking resources on the sriov PF2
6334         # rp.
6335         self.assertEqual(2, len(allocations))
6336 
6337         self.assertComputeAllocationMatchesFlavor(
6338             allocations, self.compute1_rp_uuid, self.flavor)
6339 
6340         sriov_allocations = allocations[self.sriov_dev_rp_per_host[
6341                 self.compute1_rp_uuid]['pf2']]['resources']
6342         self.assertPortMatchesAllocation(
6343             port, sriov_allocations)
6344 
6345         # We expect that only the RP uuid of the networking RP having the port
6346         # allocation is sent in the port binding for the port having resource
6347         # request
6348         port_binding = port['binding:profile']
6349         self.assertEqual(
6350             self.sriov_dev_rp_per_host[self.compute1_rp_uuid]['pf2'],
6351             port_binding['allocation'])
6352 
6353         # We expect that the selected PCI device matches with the RP from
6354         # where the bandwidth is allocated from. The bandwidth is allocated
6355         # from 0000:02:00 (PF2) so the PCI device should be a VF of that PF
6356         self.assertEqual(
6357             fake.FakeDriverWithPciResources.PCI_ADDR_PF2_VF1,
6358             port_binding['pci_slot'])
6359 
6360 
6361 class HostNameWeigher(weights.BaseHostWeigher):
6362     # Weigher to make the scheduler alternate host list deterministic
6363     _weights = {'host1': 100, 'host2': 50, 'host3': 10}
6364 
6365     def _weigh_object(self, host_state, weight_properties):
6366         # Any undefined host gets no weight.
6367         return self._weights.get(host_state.host, 0)
6368 
6369 
6370 class ServerMoveWithPortResourceRequestTest(
6371         PortResourceRequestBasedSchedulingTestBase):
6372 
6373     def setUp(self):
6374         # Use our custom weigher defined above to make sure that we have
6375         # a predictable host order in the alternate list returned by the
6376         # scheduler for migration.
6377         self.flags(weight_classes=[__name__ + '.HostNameWeigher'],
6378                    group='filter_scheduler')
6379         super(ServerMoveWithPortResourceRequestTest, self).setUp()
6380 
6381         # The API actively rejecting the move operations with resource
6382         # request so we have to turn off that check.
6383         # TODO(gibi): Remove this when the move operations are supported and
6384         # the API check is removed.
6385         patcher = mock.patch(
6386             'nova.api.openstack.common.'
6387             'supports_port_resource_request_during_move',
6388             return_value=True)
6389         self.addCleanup(patcher.stop)
6390         patcher.start()
6391 
6392         self.compute2 = self._start_compute('host2')
6393         self.compute2_rp_uuid = self._get_provider_uuid_by_host('host2')
6394         self._create_networking_rp_tree(self.compute2_rp_uuid)
6395         self.compute2_service_id = self.admin_api.get_services(
6396             host='host2', binary='nova-compute')[0]['id']
6397 
6398     def _check_allocation(
6399             self, server, compute_rp_uuid, non_qos_port, qos_port,
6400             migration_uuid=None, source_compute_rp_uuid=None):
6401 
6402         updated_non_qos_port = self.neutron.show_port(
6403             non_qos_port['id'])['port']
6404         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6405 
6406         allocations = self.placement_api.get(
6407             '/allocations/%s' % server['id']).body['allocations']
6408 
6409         # We expect one set of allocations for the compute resources on the
6410         # compute rp and one set for the networking resources on the ovs bridge
6411         # rp due to the qos_port resource request
6412         self.assertEqual(2, len(allocations))
6413         self.assertComputeAllocationMatchesFlavor(
6414             allocations, compute_rp_uuid, self.flavor)
6415         network_allocations = allocations[
6416             self.ovs_bridge_rp_per_host[compute_rp_uuid]]['resources']
6417         self.assertPortMatchesAllocation(qos_port, network_allocations)
6418 
6419         # We expect that only the RP uuid of the networking RP having the port
6420         # allocation is sent in the port binding for the port having resource
6421         # request
6422         qos_binding_profile = updated_qos_port['binding:profile']
6423         self.assertEqual(self.ovs_bridge_rp_per_host[compute_rp_uuid],
6424                          qos_binding_profile['allocation'])
6425 
6426         # And we expect not to have any allocation set in the port binding for
6427         # the port that doesn't have resource request
6428         self.assertNotIn('binding:profile', updated_non_qos_port)
6429 
6430         if migration_uuid:
6431             migration_allocations = self.placement_api.get(
6432                 '/allocations/%s' % migration_uuid).body['allocations']
6433 
6434             # We expect one set of allocations for the compute resources on the
6435             # compute rp and one set for the networking resources on the ovs
6436             # bridge rp due to the qos_port resource request
6437             self.assertEqual(2, len(migration_allocations))
6438             self.assertComputeAllocationMatchesFlavor(
6439                 migration_allocations, source_compute_rp_uuid, self.flavor)
6440             network_allocations = migration_allocations[
6441                 self.ovs_bridge_rp_per_host[
6442                     source_compute_rp_uuid]]['resources']
6443             self.assertPortMatchesAllocation(qos_port, network_allocations)
6444 
6445     def _create_server_with_ports(self, non_qos_port, qos_port):
6446         server = self._create_server(
6447             flavor=self.flavor,
6448             networks=[{'port': non_qos_port['id']},
6449                       {'port': qos_port['id']}],
6450             host='host1')
6451         return self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6452 
6453     def _delete_server_and_check_allocations(self, qos_port, server):
6454         self._delete_and_check_allocations(server)
6455 
6456         # assert that unbind removes the allocation from the binding of the
6457         # port that got allocation during the bind
6458         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6459         binding_profile = updated_qos_port['binding:profile']
6460         self.assertNotIn('allocation', binding_profile)
6461 
6462     def test_migrate_server_with_qos_port_old_dest_compute_no_alternate(self):
6463         """Create a situation where the only migration target host returned
6464         by the scheduler is too old and therefore the migration fails.
6465         """
6466         non_qos_port = self.neutron.port_1
6467         qos_port = self.neutron.port_with_resource_request
6468 
6469         server = self._create_server_with_ports(non_qos_port, qos_port)
6470 
6471         # check that the server allocates from the current host properly
6472         self._check_allocation(
6473             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6474 
6475         orig_get_service = nova.objects.Service.get_by_host_and_binary
6476 
6477         def fake_get_service(context, host, binary):
6478             if host == 'host1':
6479                 return orig_get_service(context, host, binary)
6480             if host == 'host2':
6481                 service = orig_get_service(context, host, binary)
6482                 service.version = 38
6483                 return service
6484 
6485         with mock.patch(
6486                 'nova.objects.Service.get_by_host_and_binary',
6487                 side_effect=fake_get_service):
6488 
6489             ex = self.assertRaises(
6490                 client.OpenStackApiException,
6491                 self.api.post_server_action, server['id'], {'migrate': None})
6492 
6493         self.assertEqual(400, ex.response.status_code)
6494         self.assertIn('No valid host was found.', six.text_type(ex))
6495 
6496         # check that the server still allocates from the original host
6497         self._check_allocation(
6498             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6499 
6500         # but the migration allocation is gone
6501         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6502         migration_allocations = self.placement_api.get(
6503             '/allocations/%s' % migration_uuid).body['allocations']
6504         self.assertEqual({}, migration_allocations)
6505 
6506         self._delete_server_and_check_allocations(qos_port, server)
6507 
6508     def test_migrate_server_with_qos_port_old_dest_compute_alternate(self):
6509         """Create a situation where the first migration target host returned
6510         by the scheduler is too old and therefore the second host is selected
6511         by the MigrationTask.
6512         """
6513         self._start_compute('host3')
6514         compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
6515         self._create_networking_rp_tree(compute3_rp_uuid)
6516 
6517         non_qos_port = self.neutron.port_1
6518         qos_port = self.neutron.port_with_resource_request
6519 
6520         server = self._create_server_with_ports(non_qos_port, qos_port)
6521 
6522         # check that the server allocates from the current host properly
6523         self._check_allocation(
6524             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6525 
6526         orig_get_service = nova.objects.Service.get_by_host_and_binary
6527 
6528         def fake_get_service(context, host, binary):
6529             if host == 'host1':
6530                 return orig_get_service(context, host, binary)
6531             if host == 'host2':
6532                 service = orig_get_service(context, host, binary)
6533                 service.version = 38
6534                 return service
6535             if host == 'host3':
6536                 service = orig_get_service(context, host, binary)
6537                 service.version = 39
6538                 return service
6539 
6540         with mock.patch(
6541                 'nova.objects.Service.get_by_host_and_binary',
6542                 side_effect=fake_get_service):
6543 
6544             self.api.post_server_action(server['id'], {'migrate': None})
6545 
6546         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6547 
6548         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6549 
6550         # check that server allocates from host3
6551         self._check_allocation(
6552             server, compute3_rp_uuid, non_qos_port, qos_port,
6553             migration_uuid, source_compute_rp_uuid=self.compute1_rp_uuid)
6554 
6555         self.api.post_server_action(server['id'], {'confirmResize': None})
6556         self._wait_for_migration_status(server, ['confirmed'])
6557 
6558         # check that allocation is still OK
6559         self._check_allocation(
6560             server, compute3_rp_uuid, non_qos_port, qos_port)
6561         # but the migration allocation is gone
6562         migration_allocations = self.placement_api.get(
6563             '/allocations/%s' % migration_uuid).body['allocations']
6564         self.assertEqual({}, migration_allocations)
6565 
6566         self._delete_server_and_check_allocations(qos_port, server)
6567 
6568     def test_migrate_server_with_qos_port(self):
6569         non_qos_port = self.neutron.port_1
6570         qos_port = self.neutron.port_with_resource_request
6571 
6572         server = self._create_server_with_ports(non_qos_port, qos_port)
6573 
6574         # check that the server allocates from the current host properly
6575         self._check_allocation(
6576             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6577 
6578         self.api.post_server_action(server['id'], {'migrate': None})
6579         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6580 
6581         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6582 
6583         # check that server allocates from the new host properly
6584         self._check_allocation(
6585             server, self.compute2_rp_uuid, non_qos_port, qos_port,
6586             migration_uuid, source_compute_rp_uuid=self.compute1_rp_uuid)
6587 
6588         self.api.post_server_action(server['id'], {'confirmResize': None})
6589         self._wait_for_migration_status(server, ['confirmed'])
6590 
6591         # check that allocation is still OK
6592         self._check_allocation(
6593             server, self.compute2_rp_uuid, non_qos_port, qos_port)
6594         # but the migration allocation is gone
6595         migration_allocations = self.placement_api.get(
6596             '/allocations/%s' % migration_uuid).body['allocations']
6597         self.assertEqual({}, migration_allocations)
6598 
6599         self._delete_server_and_check_allocations(qos_port, server)
6600 
6601     def test_migrate_revert_with_qos_port(self):
6602         non_qos_port = self.neutron.port_1
6603         qos_port = self.neutron.port_with_resource_request
6604 
6605         server = self._create_server_with_ports(non_qos_port, qos_port)
6606 
6607         # check that the server allocates from the current host properly
6608         self._check_allocation(
6609             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6610 
6611         self.api.post_server_action(server['id'], {'migrate': None})
6612         self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6613 
6614         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6615 
6616         # check that server allocates from the new host properly
6617         self._check_allocation(
6618             server, self.compute2_rp_uuid, non_qos_port, qos_port,
6619             migration_uuid, source_compute_rp_uuid=self.compute1_rp_uuid)
6620 
6621         self.api.post_server_action(server['id'], {'revertResize': None})
6622         self._wait_for_state_change(self.api, server, 'ACTIVE')
6623 
6624         # check that allocation is moved back to the source host
6625         self._check_allocation(
6626             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6627 
6628         # check that the target host allocation is cleaned up.
6629         self.assertRequestMatchesUsage(
6630             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0,
6631              'NET_BW_IGR_KILOBIT_PER_SEC': 0, 'NET_BW_EGR_KILOBIT_PER_SEC': 0},
6632             self.compute2_rp_uuid)
6633         migration_allocations = self.placement_api.get(
6634             '/allocations/%s' % migration_uuid).body['allocations']
6635         self.assertEqual({}, migration_allocations)
6636 
6637         self._delete_server_and_check_allocations(qos_port, server)
6638 
6639     def test_migrate_server_with_qos_port_reschedule_success(self):
6640         self._start_compute('host3')
6641         compute3_rp_uuid = self._get_provider_uuid_by_host('host3')
6642         self._create_networking_rp_tree(compute3_rp_uuid)
6643 
6644         non_qos_port = self.neutron.port_1
6645         qos_port = self.neutron.port_with_resource_request
6646 
6647         server = self._create_server_with_ports(non_qos_port, qos_port)
6648 
6649         # check that the server allocates from the current host properly
6650         self._check_allocation(
6651             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6652 
6653         prep_resize_calls = []
6654 
6655         def fake_prep_resize(_self, *args, **kwargs):
6656             # Make the first prep_resize fail and the rest passing through
6657             # the original _prep_resize call
6658             if not prep_resize_calls:
6659                 prep_resize_calls.append(_self.host)
6660                 raise test.TestingException('Simulated prep_resize failure.')
6661             prep_resize_calls.append(_self.host)
6662             original_prep_resize(_self, *args, **kwargs)
6663 
6664         # Yes this isn't great in a functional test, but it's simple.
6665         original_prep_resize = compute_manager.ComputeManager._prep_resize
6666 
6667         # The patched compute manager will raise from _prep_resize on the
6668         # first host of the migration. Then the migration
6669         # is reschedule on the other host where it will succeed
6670         with mock.patch.object(
6671                 compute_manager.ComputeManager, '_prep_resize',
6672                 new=fake_prep_resize):
6673             self.api.post_server_action(server['id'], {'migrate': None})
6674             self._wait_for_state_change(self.api, server, 'VERIFY_RESIZE')
6675 
6676         # ensure that resize is tried on two hosts, so we had a re-schedule
6677         self.assertItemsEqual(['host2', 'host3'], prep_resize_calls)
6678 
6679         host_rp_uuid = (self.compute2_rp_uuid
6680                         if prep_resize_calls[-1] == 'host2'
6681                         else compute3_rp_uuid)
6682 
6683         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6684 
6685         # check that server is allocates from the final host properly while
6686         # the migration holds the allocation on the source host
6687         self._check_allocation(
6688             server, host_rp_uuid, non_qos_port, qos_port,
6689             migration_uuid, source_compute_rp_uuid=self.compute1_rp_uuid)
6690 
6691         self.api.post_server_action(server['id'], {'confirmResize': None})
6692         self._wait_for_migration_status(server, ['confirmed'])
6693 
6694         # check that allocation is still OK
6695         self._check_allocation(server, host_rp_uuid, non_qos_port, qos_port)
6696         # but the migration allocation is gone
6697         migration_allocations = self.placement_api.get(
6698             '/allocations/%s' % migration_uuid).body['allocations']
6699         self.assertEqual({}, migration_allocations)
6700 
6701         self._delete_server_and_check_allocations(qos_port, server)
6702 
6703     def test_migrate_server_with_qos_port_reschedule_failure(self):
6704         non_qos_port = self.neutron.port_1
6705         qos_port = self.neutron.port_with_resource_request
6706 
6707         server = self._create_server_with_ports(non_qos_port, qos_port)
6708 
6709         # check that the server allocates from the current host properly
6710         self._check_allocation(
6711             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6712 
6713         prep_resize_calls = []
6714 
6715         def fake_prep_resize(_self, *args, **kwargs):
6716             # Make the first prep_resize fail and the rest passing through
6717             # the original _prep_resize call
6718             if not prep_resize_calls:
6719                 prep_resize_calls.append(_self.host)
6720                 raise test.TestingException('Simulated prep_resize failure.')
6721             prep_resize_calls.append(_self.host)
6722             original_prep_resize(_self, *args, **kwargs)
6723 
6724         # Yes this isn't great in a functional test, but it's simple.
6725         original_prep_resize = compute_manager.ComputeManager._prep_resize
6726 
6727         # The patched compute manager on host2 will raise from _prep_resize.
6728         # Then the migration is reschedule but there is no other host to
6729         # choose from.
6730         with mock.patch.object(
6731                 compute_manager.ComputeManager, '_prep_resize',
6732                 new=fake_prep_resize):
6733             self.api.post_server_action(server['id'], {'migrate': None})
6734             self._wait_for_server_parameter(
6735                 self.api, server,
6736                 {'OS-EXT-SRV-ATTR:host': 'host1',
6737                  'status': 'ERROR'})
6738             self._wait_for_migration_status(server, ['error'])
6739 
6740         self.assertEqual(['host2'], prep_resize_calls)
6741 
6742         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
6743 
6744         # as the migration is failed we expect that the migration allocation
6745         # is deleted
6746         migration_allocations = self.placement_api.get(
6747             '/allocations/%s' % migration_uuid).body['allocations']
6748         self.assertEqual({}, migration_allocations)
6749 
6750         # and the instance is allocation from the source host
6751         self._check_allocation(
6752             server, self.compute1_rp_uuid, non_qos_port, qos_port)
6753 
6754 
6755 class PortResourceRequestReSchedulingTest(
6756         PortResourceRequestBasedSchedulingTestBase):
6757     """Similar to PortResourceRequestBasedSchedulingTest
6758     except this test uses FakeRescheduleDriver which will test reschedules
6759     during server create work as expected, i.e. that the resource request
6760     allocations are moved from the initially selected compute to the
6761     alternative compute.
6762     """
6763 
6764     compute_driver = 'fake.FakeRescheduleDriver'
6765 
6766     def setUp(self):
6767         super(PortResourceRequestReSchedulingTest, self).setUp()
6768         self.compute2 = self._start_compute('host2')
6769         self.compute2_rp_uuid = self._get_provider_uuid_by_host('host2')
6770         self._create_networking_rp_tree(self.compute2_rp_uuid)
6771 
6772     def _create_networking_rp_tree(self, compute_rp_uuid):
6773         # let's simulate what the neutron would do
6774         self._create_ovs_networking_rp_tree(compute_rp_uuid)
6775 
6776     def test_boot_reschedule_success(self):
6777         port = self.neutron.port_with_resource_request
6778 
6779         server = self._create_server(
6780             flavor=self.flavor,
6781             networks=[{'port': port['id']}])
6782         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6783         updated_port = self.neutron.show_port(port['id'])['port']
6784 
6785         dest_hostname = server['OS-EXT-SRV-ATTR:host']
6786         dest_compute_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
6787 
6788         failed_compute_rp = (self.compute1_rp_uuid
6789                              if dest_compute_rp_uuid == self.compute2_rp_uuid
6790                              else self.compute2_rp_uuid)
6791 
6792         allocations = self.placement_api.get(
6793             '/allocations/%s' % server['id']).body['allocations']
6794 
6795         # We expect one set of allocations for the compute resources on the
6796         # compute rp and one set for the networking resources on the ovs bridge
6797         # rp
6798         self.assertEqual(2, len(allocations))
6799 
6800         self.assertComputeAllocationMatchesFlavor(
6801             allocations, dest_compute_rp_uuid, self.flavor)
6802 
6803         network_allocations = allocations[
6804             self.ovs_bridge_rp_per_host[dest_compute_rp_uuid]]['resources']
6805         self.assertPortMatchesAllocation(port, network_allocations)
6806 
6807         # assert that the allocations against the host where the spawn
6808         # failed are cleaned up properly
6809         self.assertEqual(
6810             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
6811             self._get_provider_usages(failed_compute_rp))
6812         self.assertEqual(
6813             {'NET_BW_EGR_KILOBIT_PER_SEC': 0, 'NET_BW_IGR_KILOBIT_PER_SEC': 0},
6814             self._get_provider_usages(
6815                 self.ovs_bridge_rp_per_host[failed_compute_rp]))
6816 
6817         # We expect that only the RP uuid of the networking RP having the port
6818         # allocation is sent in the port binding
6819         binding_profile = updated_port['binding:profile']
6820         self.assertEqual(self.ovs_bridge_rp_per_host[dest_compute_rp_uuid],
6821                          binding_profile['allocation'])
6822 
6823         self._delete_and_check_allocations(server)
6824 
6825         # assert that unbind removes the allocation from the binding
6826         updated_port = self.neutron.show_port(port['id'])['port']
6827         binding_profile = updated_port['binding:profile']
6828         self.assertNotIn('allocation', binding_profile)
6829 
6830     def test_boot_reschedule_fill_provider_mapping_raises(self):
6831         """Verify that if the  _fill_provider_mapping raises during re-schedule
6832         then the instance is properly put into ERROR state.
6833         """
6834 
6835         port = self.neutron.port_with_resource_request
6836 
6837         # First call is during boot, we want that to succeed normally. Then the
6838         # fake virt driver triggers a re-schedule. During that re-schedule the
6839         # fill is called again, and we simulate that call raises.
6840         fill = nova.scheduler.utils.fill_provider_mapping
6841 
6842         with mock.patch(
6843                 'nova.scheduler.utils.fill_provider_mapping',
6844                 side_effect=[
6845                     fill,
6846                     exception.ResourceProviderTraitRetrievalFailed(
6847                         uuid=uuids.rp1)],
6848                 autospec=True):
6849             server = self._create_server(
6850                 flavor=self.flavor,
6851                 networks=[{'port': port['id']}])
6852             server = self._wait_for_state_change(
6853                 self.admin_api, server, 'ERROR')
6854 
6855         self.assertIn(
6856             'Failed to get traits for resource provider',
6857             server['fault']['message'])
6858 
6859         self._delete_and_check_allocations(server)
6860 
6861         # assert that unbind removes the allocation from the binding
6862         updated_port = self.neutron.show_port(port['id'])['port']
6863         binding_profile = updated_port['binding:profile']
6864         self.assertNotIn('allocation', binding_profile)
