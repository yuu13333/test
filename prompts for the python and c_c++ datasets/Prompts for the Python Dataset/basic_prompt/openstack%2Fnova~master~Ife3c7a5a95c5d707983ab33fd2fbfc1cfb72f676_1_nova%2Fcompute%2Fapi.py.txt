Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import functools
24 import re
25 import string
26 
27 from castellan import key_manager
28 from oslo_log import log as logging
29 from oslo_messaging import exceptions as oslo_exceptions
30 from oslo_serialization import base64 as base64utils
31 from oslo_utils import excutils
32 from oslo_utils import strutils
33 from oslo_utils import timeutils
34 from oslo_utils import units
35 from oslo_utils import uuidutils
36 import six
37 from six.moves import range
38 
39 from nova import availability_zones
40 from nova import block_device
41 from nova.compute import flavors
42 from nova.compute import instance_actions
43 from nova.compute import instance_list
44 from nova.compute import migration_list
45 from nova.compute import power_state
46 from nova.compute import rpcapi as compute_rpcapi
47 from nova.compute import task_states
48 from nova.compute import utils as compute_utils
49 from nova.compute.utils import wrap_instance_event
50 from nova.compute import vm_states
51 from nova import conductor
52 import nova.conf
53 from nova import context as nova_context
54 from nova import crypto
55 from nova.db import base
56 from nova.db.sqlalchemy import api as db_api
57 from nova import exception
58 from nova import exception_wrapper
59 from nova import hooks
60 from nova.i18n import _
61 from nova import image
62 from nova import network
63 from nova.network import model as network_model
64 from nova.network.neutronv2 import constants
65 from nova.network.security_group import openstack_driver
66 from nova.network.security_group import security_group_base
67 from nova import objects
68 from nova.objects import base as obj_base
69 from nova.objects import block_device as block_device_obj
70 from nova.objects import external_event as external_event_obj
71 from nova.objects import fields as fields_obj
72 from nova.objects import keypair as keypair_obj
73 from nova.objects import quotas as quotas_obj
74 from nova.pci import request as pci_request
75 from nova.policies import servers as servers_policies
76 import nova.policy
77 from nova import profiler
78 from nova import rpc
79 from nova.scheduler.client import query
80 from nova.scheduler.client import report
81 from nova.scheduler import utils as scheduler_utils
82 from nova import servicegroup
83 from nova import utils
84 from nova.virt import hardware
85 from nova.volume import cinder
86 
87 LOG = logging.getLogger(__name__)
88 
89 get_notifier = functools.partial(rpc.get_notifier, service='compute')
90 # NOTE(gibi): legacy notification used compute as a service but these
91 # calls still run on the client side of the compute service which is
92 # nova-api. By setting the binary to nova-api below, we can make sure
93 # that the new versioned notifications has the right publisher_id but the
94 # legacy notifications does not change.
95 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
96                                    get_notifier=get_notifier,
97                                    binary='nova-api')
98 CONF = nova.conf.CONF
99 
100 RO_SECURITY_GROUPS = ['default']
101 
102 AGGREGATE_ACTION_UPDATE = 'Update'
103 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
104 AGGREGATE_ACTION_DELETE = 'Delete'
105 AGGREGATE_ACTION_ADD = 'Add'
106 MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION = 34
107 MIN_COMPUTE_VOLUME_TYPE = 36
108 MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED = 38
109 
110 # FIXME(danms): Keep a global cache of the cells we find the
111 # first time we look. This needs to be refreshed on a timer or
112 # trigger.
113 CELLS = []
114 
115 
116 def check_instance_state(vm_state=None, task_state=(None,),
117                          must_have_launched=True):
118     """Decorator to check VM and/or task state before entry to API functions.
119 
120     If the instance is in the wrong state, or has not been successfully
121     started at least once the wrapper will raise an exception.
122     """
123 
124     if vm_state is not None and not isinstance(vm_state, set):
125         vm_state = set(vm_state)
126     if task_state is not None and not isinstance(task_state, set):
127         task_state = set(task_state)
128 
129     def outer(f):
130         @six.wraps(f)
131         def inner(self, context, instance, *args, **kw):
132             if vm_state is not None and instance.vm_state not in vm_state:
133                 raise exception.InstanceInvalidState(
134                     attr='vm_state',
135                     instance_uuid=instance.uuid,
136                     state=instance.vm_state,
137                     method=f.__name__)
138             if (task_state is not None and
139                     instance.task_state not in task_state):
140                 raise exception.InstanceInvalidState(
141                     attr='task_state',
142                     instance_uuid=instance.uuid,
143                     state=instance.task_state,
144                     method=f.__name__)
145             if must_have_launched and not instance.launched_at:
146                 raise exception.InstanceInvalidState(
147                     attr='launched_at',
148                     instance_uuid=instance.uuid,
149                     state=instance.launched_at,
150                     method=f.__name__)
151 
152             return f(self, context, instance, *args, **kw)
153         return inner
154     return outer
155 
156 
157 def _set_or_none(q):
158     return q if q is None or isinstance(q, set) else set(q)
159 
160 
161 def reject_instance_state(vm_state=None, task_state=None):
162     """Decorator.  Raise InstanceInvalidState if instance is in any of the
163     given states.
164     """
165 
166     vm_state = _set_or_none(vm_state)
167     task_state = _set_or_none(task_state)
168 
169     def outer(f):
170         @six.wraps(f)
171         def inner(self, context, instance, *args, **kw):
172             _InstanceInvalidState = functools.partial(
173                 exception.InstanceInvalidState,
174                 instance_uuid=instance.uuid,
175                 method=f.__name__)
176 
177             if vm_state is not None and instance.vm_state in vm_state:
178                 raise _InstanceInvalidState(
179                     attr='vm_state', state=instance.vm_state)
180 
181             if task_state is not None and instance.task_state in task_state:
182                 raise _InstanceInvalidState(
183                     attr='task_state', state=instance.task_state)
184 
185             return f(self, context, instance, *args, **kw)
186         return inner
187     return outer
188 
189 
190 def check_instance_host(function):
191     @six.wraps(function)
192     def wrapped(self, context, instance, *args, **kwargs):
193         if not instance.host:
194             raise exception.InstanceNotReady(instance_id=instance.uuid)
195         return function(self, context, instance, *args, **kwargs)
196     return wrapped
197 
198 
199 def check_instance_lock(function):
200     @six.wraps(function)
201     def inner(self, context, instance, *args, **kwargs):
202         if instance.locked and not context.is_admin:
203             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
204         return function(self, context, instance, *args, **kwargs)
205     return inner
206 
207 
208 def reject_sev_instances(operation):
209     """Decorator.  Raise OperationNotSupportedForSEV if instance has SEV
210     enabled.
211     """
212 
213     def outer(f):
214         @six.wraps(f)
215         def inner(self, context, instance, *args, **kw):
216             if hardware.get_mem_encryption_constraint(instance.flavor,
217                                                       instance.image_meta):
218                 raise exception.OperationNotSupportedForSEV(
219                     instance_uuid=instance.uuid,
220                     operation=operation)
221             return f(self, context, instance, *args, **kw)
222         return inner
223     return outer
224 
225 
226 def _diff_dict(orig, new):
227     """Return a dict describing how to change orig to new.  The keys
228     correspond to values that have changed; the value will be a list
229     of one or two elements.  The first element of the list will be
230     either '+' or '-', indicating whether the key was updated or
231     deleted; if the key was updated, the list will contain a second
232     element, giving the updated value.
233     """
234     # Figure out what keys went away
235     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
236     # Compute the updates
237     for key, value in new.items():
238         if key not in orig or value != orig[key]:
239             result[key] = ['+', value]
240     return result
241 
242 
243 def load_cells():
244     global CELLS
245     if not CELLS:
246         CELLS = objects.CellMappingList.get_all(
247             nova_context.get_admin_context())
248         LOG.debug('Found %(count)i cells: %(cells)s',
249                   dict(count=len(CELLS),
250                        cells=','.join([c.identity for c in CELLS])))
251 
252     if not CELLS:
253         LOG.error('No cells are configured, unable to continue')
254 
255 
256 def _get_image_meta_obj(image_meta_dict):
257     try:
258         image_meta = objects.ImageMeta.from_dict(image_meta_dict)
259     except ValueError as e:
260         # there must be invalid values in the image meta properties so
261         # consider this an invalid request
262         msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
263         raise exception.InvalidRequest(msg)
264     return image_meta
265 
266 
267 @profiler.trace_cls("compute_api")
268 class API(base.Base):
269     """API for interacting with the compute manager."""
270 
271     def __init__(self, image_api=None, network_api=None, volume_api=None,
272                  security_group_api=None, **kwargs):
273         self.image_api = image_api or image.API()
274         self.network_api = network_api or network.API()
275         self.volume_api = volume_api or cinder.API()
276         self._placementclient = None  # Lazy-load on first access.
277         self.security_group_api = (security_group_api or
278             openstack_driver.get_openstack_security_group_driver())
279         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
280         self.compute_task_api = conductor.ComputeTaskAPI()
281         self.servicegroup_api = servicegroup.API()
282         self.host_api = HostAPI(self.compute_rpcapi, self.servicegroup_api)
283         self.notifier = rpc.get_notifier('compute', CONF.host)
284         if CONF.ephemeral_storage_encryption.enabled:
285             self.key_manager = key_manager.API()
286         # Help us to record host in EventReporter
287         self.host = CONF.host
288         super(API, self).__init__(**kwargs)
289 
290     def _record_action_start(self, context, instance, action):
291         objects.InstanceAction.action_start(context, instance.uuid,
292                                             action, want_result=False)
293 
294     def _check_injected_file_quota(self, context, injected_files):
295         """Enforce quota limits on injected files.
296 
297         Raises a QuotaError if any limit is exceeded.
298         """
299         if injected_files is None:
300             return
301 
302         # Check number of files first
303         try:
304             objects.Quotas.limit_check(context,
305                                        injected_files=len(injected_files))
306         except exception.OverQuota:
307             raise exception.OnsetFileLimitExceeded()
308 
309         # OK, now count path and content lengths; we're looking for
310         # the max...
311         max_path = 0
312         max_content = 0
313         for path, content in injected_files:
314             max_path = max(max_path, len(path))
315             max_content = max(max_content, len(content))
316 
317         try:
318             objects.Quotas.limit_check(context,
319                                        injected_file_path_bytes=max_path,
320                                        injected_file_content_bytes=max_content)
321         except exception.OverQuota as exc:
322             # Favor path limit over content limit for reporting
323             # purposes
324             if 'injected_file_path_bytes' in exc.kwargs['overs']:
325                 raise exception.OnsetFilePathLimitExceeded(
326                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
327             else:
328                 raise exception.OnsetFileContentLimitExceeded(
329                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
330 
331     def _check_metadata_properties_quota(self, context, metadata=None):
332         """Enforce quota limits on metadata properties."""
333         if not metadata:
334             metadata = {}
335         if not isinstance(metadata, dict):
336             msg = (_("Metadata type should be dict."))
337             raise exception.InvalidMetadata(reason=msg)
338         num_metadata = len(metadata)
339         try:
340             objects.Quotas.limit_check(context, metadata_items=num_metadata)
341         except exception.OverQuota as exc:
342             quota_metadata = exc.kwargs['quotas']['metadata_items']
343             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
344 
345         # Because metadata is stored in the DB, we hard-code the size limits
346         # In future, we may support more variable length strings, so we act
347         #  as if this is quota-controlled for forwards compatibility.
348         # Those are only used in V2 API, from V2.1 API, those checks are
349         # validated at API layer schema validation.
350         for k, v in metadata.items():
351             try:
352                 utils.check_string_length(v)
353                 utils.check_string_length(k, min_length=1)
354             except exception.InvalidInput as e:
355                 raise exception.InvalidMetadata(reason=e.format_message())
356 
357             if len(k) > 255:
358                 msg = _("Metadata property key greater than 255 characters")
359                 raise exception.InvalidMetadataSize(reason=msg)
360             if len(v) > 255:
361                 msg = _("Metadata property value greater than 255 characters")
362                 raise exception.InvalidMetadataSize(reason=msg)
363 
364     def _check_requested_secgroups(self, context, secgroups):
365         """Check if the security group requested exists and belongs to
366         the project.
367 
368         :param context: The nova request context.
369         :type context: nova.context.RequestContext
370         :param secgroups: list of requested security group names, or uuids in
371             the case of Neutron.
372         :type secgroups: list
373         :returns: list of requested security group names unmodified if using
374             nova-network. If using Neutron, the list returned is all uuids.
375             Note that 'default' is a special case and will be unmodified if
376             it's requested.
377         """
378         security_groups = []
379         for secgroup in secgroups:
380             # NOTE(sdague): default is handled special
381             if secgroup == "default":
382                 security_groups.append(secgroup)
383                 continue
384             secgroup_dict = self.security_group_api.get(context, secgroup)
385             if not secgroup_dict:
386                 raise exception.SecurityGroupNotFoundForProject(
387                     project_id=context.project_id, security_group_id=secgroup)
388 
389             # Check to see if it's a nova-network or neutron type.
390             if isinstance(secgroup_dict['id'], int):
391                 # This is nova-network so just return the requested name.
392                 security_groups.append(secgroup)
393             else:
394                 # The id for neutron is a uuid, so we return the id (uuid).
395                 security_groups.append(secgroup_dict['id'])
396 
397         return security_groups
398 
399     def _check_requested_networks(self, context, requested_networks,
400                                   max_count):
401         """Check if the networks requested belongs to the project
402         and the fixed IP address for each network provided is within
403         same the network block
404         """
405         if requested_networks is not None:
406             if requested_networks.no_allocate:
407                 # If the network request was specifically 'none' meaning don't
408                 # allocate any networks, we just return the number of requested
409                 # instances since quotas don't change at all.
410                 return max_count
411 
412             # NOTE(danms): Temporary transition
413             requested_networks = requested_networks.as_tuples()
414 
415         return self.network_api.validate_networks(context, requested_networks,
416                                                   max_count)
417 
418     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
419                                    image):
420         """Choose kernel and ramdisk appropriate for the instance.
421 
422         The kernel and ramdisk can be chosen in one of two ways:
423 
424             1. Passed in with create-instance request.
425 
426             2. Inherited from image metadata.
427 
428         If inherited from image metadata, and if that image metadata value is
429         set to 'nokernel', both kernel and ramdisk will default to None.
430         """
431         # Inherit from image if not specified
432         image_properties = image.get('properties', {})
433 
434         if kernel_id is None:
435             kernel_id = image_properties.get('kernel_id')
436 
437         if ramdisk_id is None:
438             ramdisk_id = image_properties.get('ramdisk_id')
439 
440         # Force to None if kernel_id indicates that a kernel is not to be used
441         if kernel_id == 'nokernel':
442             kernel_id = None
443             ramdisk_id = None
444 
445         # Verify kernel and ramdisk exist (fail-fast)
446         if kernel_id is not None:
447             kernel_image = self.image_api.get(context, kernel_id)
448             # kernel_id could have been a URI, not a UUID, so to keep behaviour
449             # from before, which leaked that implementation detail out to the
450             # caller, we return the image UUID of the kernel image and ramdisk
451             # image (below) and not any image URIs that might have been
452             # supplied.
453             # TODO(jaypipes): Get rid of this silliness once we move to a real
454             # Image object and hide all of that stuff within nova.image.api.
455             kernel_id = kernel_image['id']
456 
457         if ramdisk_id is not None:
458             ramdisk_image = self.image_api.get(context, ramdisk_id)
459             ramdisk_id = ramdisk_image['id']
460 
461         return kernel_id, ramdisk_id
462 
463     @staticmethod
464     def parse_availability_zone(context, availability_zone):
465         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
466         #             via az using az:host:node. It might be nice to expose an
467         #             api to specify specific hosts to force onto, but for
468         #             now it just supports this legacy hack.
469         # NOTE(deva): It is also possible to specify az::node, in which case
470         #             the host manager will determine the correct host.
471         forced_host = None
472         forced_node = None
473         if availability_zone and ':' in availability_zone:
474             c = availability_zone.count(':')
475             if c == 1:
476                 availability_zone, forced_host = availability_zone.split(':')
477             elif c == 2:
478                 if '::' in availability_zone:
479                     availability_zone, forced_node = \
480                             availability_zone.split('::')
481                 else:
482                     availability_zone, forced_host, forced_node = \
483                             availability_zone.split(':')
484             else:
485                 raise exception.InvalidInput(
486                         reason="Unable to parse availability_zone")
487 
488         if not availability_zone:
489             availability_zone = CONF.default_schedule_zone
490 
491         return availability_zone, forced_host, forced_node
492 
493     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
494                                           auto_disk_config, image):
495         auto_disk_config_disabled = \
496                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
497         if auto_disk_config_disabled and auto_disk_config:
498             raise exception.AutoDiskConfigDisabledByImage(image=image)
499 
500     def _inherit_properties_from_image(self, image, auto_disk_config):
501         image_properties = image.get('properties', {})
502         auto_disk_config_img = \
503                 utils.get_auto_disk_config_from_image_props(image_properties)
504         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
505                                                auto_disk_config,
506                                                image.get("id"))
507         if auto_disk_config is None:
508             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
509 
510         return {
511             'os_type': image_properties.get('os_type'),
512             'architecture': image_properties.get('architecture'),
513             'vm_mode': image_properties.get('vm_mode'),
514             'auto_disk_config': auto_disk_config
515         }
516 
517     def _check_config_drive(self, config_drive):
518         if config_drive:
519             try:
520                 bool_val = strutils.bool_from_string(config_drive,
521                                                      strict=True)
522             except ValueError:
523                 raise exception.ConfigDriveInvalidValue(option=config_drive)
524         else:
525             bool_val = False
526         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
527         # but this is because the config drive column is a String.  False
528         # is represented by using an empty string.  And for whatever
529         # reason, we rely on the DB to cast True to a String.
530         return True if bool_val else ''
531 
532     def _validate_flavor_image(self, context, image_id, image,
533                                instance_type, root_bdm, validate_numa=True):
534         """Validate the flavor and image.
535 
536         This is called from the API service to ensure that the flavor
537         extra-specs and image properties are self-consistent and compatible
538         with each other.
539 
540         :param context: A context.RequestContext
541         :param image_id: UUID of the image
542         :param image: a dict representation of the image including properties,
543                       enforces the image status is active.
544         :param instance_type: Flavor object
545         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
546                the resize case.
547         :param validate_numa: Flag to indicate whether or not to validate
548                the NUMA-related metadata.
549         :raises: Many different possible exceptions.  See
550                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
551                  for the full list.
552         """
553         if image and image['status'] != 'active':
554             raise exception.ImageNotActive(image_id=image_id)
555         self._validate_flavor_image_nostatus(context, image, instance_type,
556                                              root_bdm, validate_numa)
557 
558     @staticmethod
559     def _validate_flavor_image_nostatus(context, image, instance_type,
560                                         root_bdm, validate_numa=True,
561                                         validate_pci=False):
562         """Validate the flavor and image.
563 
564         This is called from the API service to ensure that the flavor
565         extra-specs and image properties are self-consistent and compatible
566         with each other.
567 
568         :param context: A context.RequestContext
569         :param image: a dict representation of the image including properties
570         :param instance_type: Flavor object
571         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
572                the resize case.
573         :param validate_numa: Flag to indicate whether or not to validate
574                the NUMA-related metadata.
575         :param validate_pci: Flag to indicate whether or not to validate
576                the PCI-related metadata.
577         :raises: Many different possible exceptions.  See
578                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
579                  for the full list.
580         """
581         if not image:
582             return
583 
584         image_properties = image.get('properties', {})
585         config_drive_option = image_properties.get(
586             'img_config_drive', 'optional')
587         if config_drive_option not in ['optional', 'mandatory']:
588             raise exception.InvalidImageConfigDrive(
589                 config_drive=config_drive_option)
590 
591         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
592             raise exception.FlavorMemoryTooSmall()
593 
594         # Image min_disk is in gb, size is in bytes. For sanity, have them both
595         # in bytes.
596         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
597         image_size = int(image.get('size') or 0)
598 
599         # Target disk is a volume. Don't check flavor disk size because it
600         # doesn't make sense, and check min_disk against the volume size.
601         if root_bdm is not None and root_bdm.is_volume:
602             # There are 2 possibilities here:
603             #
604             # 1. The target volume already exists but bdm.volume_size is not
605             #    yet set because this method is called before
606             #    _bdm_validate_set_size_and_instance during server create.
607             # 2. The target volume doesn't exist, in which case the bdm will
608             #    contain the intended volume size
609             #
610             # Note that rebuild also calls this method with potentially a new
611             # image but you can't rebuild a volume-backed server with a new
612             # image (yet).
613             #
614             # Cinder does its own check against min_disk, so if the target
615             # volume already exists this has already been done and we don't
616             # need to check it again here. In this case, volume_size may not be
617             # set on the bdm.
618             #
619             # If we're going to create the volume, the bdm will contain
620             # volume_size. Therefore we should check it if it exists. This will
621             # still be checked again by cinder when the volume is created, but
622             # that will not happen until the request reaches a host. By
623             # checking it here, the user gets an immediate and useful failure
624             # indication.
625             #
626             # The third possibility is that we have failed to consider
627             # something, and there are actually more than 2 possibilities. In
628             # this case cinder will still do the check at volume creation time.
629             # The behaviour will still be correct, but the user will not get an
630             # immediate failure from the api, and will instead have to
631             # determine why the instance is in an error state with a task of
632             # block_device_mapping.
633             #
634             # We could reasonably refactor this check into _validate_bdm at
635             # some future date, as the various size logic is already split out
636             # in there.
637             dest_size = root_bdm.volume_size
638             if dest_size is not None:
639                 dest_size *= units.Gi
640 
641                 if image_min_disk > dest_size:
642                     raise exception.VolumeSmallerThanMinDisk(
643                         volume_size=dest_size, image_min_disk=image_min_disk)
644 
645         # Target disk is a local disk whose size is taken from the flavor
646         else:
647             dest_size = instance_type['root_gb'] * units.Gi
648 
649             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
650             # since libvirt interpreted the value differently than other
651             # drivers. A value of 0 means don't check size.
652             if dest_size != 0:
653                 if image_size > dest_size:
654                     raise exception.FlavorDiskSmallerThanImage(
655                         flavor_size=dest_size, image_size=image_size)
656 
657                 if image_min_disk > dest_size:
658                     raise exception.FlavorDiskSmallerThanMinDisk(
659                         flavor_size=dest_size, image_min_disk=image_min_disk)
660             else:
661                 # The user is attempting to create a server with a 0-disk
662                 # image-backed flavor, which can lead to issues with a large
663                 # image consuming an unexpectedly large amount of local disk
664                 # on the compute host. Check to see if the deployment will
665                 # allow that.
666                 if not context.can(
667                         servers_policies.ZERO_DISK_FLAVOR, fatal=False):
668                     raise exception.BootFromVolumeRequiredForZeroDiskFlavor()
669 
670         API._validate_flavor_image_numa_pci(
671             image, instance_type, validate_numa=validate_numa,
672             validate_pci=validate_pci)
673 
674     @staticmethod
675     def _validate_flavor_image_numa_pci(image, instance_type,
676                                         validate_numa=True,
677                                         validate_pci=False):
678         """Validate the flavor and image NUMA/PCI values.
679 
680         This is called from the API service to ensure that the flavor
681         extra-specs and image properties are self-consistent and compatible
682         with each other.
683 
684         :param image: a dict representation of the image including properties
685         :param instance_type: Flavor object
686         :param validate_numa: Flag to indicate whether or not to validate
687                the NUMA-related metadata.
688         :param validate_pci: Flag to indicate whether or not to validate
689                the PCI-related metadata.
690         :raises: Many different possible exceptions.  See
691                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
692                  for the full list.
693         """
694         image_meta = _get_image_meta_obj(image)
695 
696         API._validate_flavor_image_mem_encryption(instance_type, image_meta)
697 
698         # validate PMU extra spec and image metadata
699         flavor_pmu = instance_type.extra_specs.get('hw:pmu')
700         image_pmu = image_meta.properties.get('hw_pmu')
701         if (flavor_pmu is not None and image_pmu is not None and
702                 image_pmu != strutils.bool_from_string(flavor_pmu)):
703             raise exception.ImagePMUConflict()
704 
705         # Only validate values of flavor/image so the return results of
706         # following 'get' functions are not used.
707         hardware.get_number_of_serial_ports(instance_type, image_meta)
708         if hardware.is_realtime_enabled(instance_type):
709             hardware.vcpus_realtime_topology(instance_type, image_meta)
710         hardware.get_cpu_topology_constraints(instance_type, image_meta)
711         if validate_numa:
712             hardware.numa_get_constraints(instance_type, image_meta)
713         if validate_pci:
714             pci_request.get_pci_requests_from_flavor(instance_type)
715 
716     @staticmethod
717     def _validate_flavor_image_mem_encryption(instance_type, image):
718         """Validate that the flavor and image don't make contradictory
719         requests regarding memory encryption.
720 
721         :param instance_type: Flavor object
722         :param image: an ImageMeta object
723         :raises: nova.exception.FlavorImageConflict
724         """
725         # This library function will raise the exception for us if
726         # necessary; if not, we can ignore the result returned.
727         hardware.get_mem_encryption_constraint(instance_type, image)
728 
729     def _get_image_defined_bdms(self, instance_type, image_meta,
730                                 root_device_name):
731         image_properties = image_meta.get('properties', {})
732 
733         # Get the block device mappings defined by the image.
734         image_defined_bdms = image_properties.get('block_device_mapping', [])
735         legacy_image_defined = not image_properties.get('bdm_v2', False)
736 
737         image_mapping = image_properties.get('mappings', [])
738 
739         if legacy_image_defined:
740             image_defined_bdms = block_device.from_legacy_mapping(
741                 image_defined_bdms, None, root_device_name)
742         else:
743             image_defined_bdms = list(map(block_device.BlockDeviceDict,
744                                           image_defined_bdms))
745 
746         if image_mapping:
747             image_mapping = self._prepare_image_mapping(instance_type,
748                                                         image_mapping)
749             image_defined_bdms = self._merge_bdms_lists(
750                 image_mapping, image_defined_bdms)
751 
752         return image_defined_bdms
753 
754     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
755         flavor_defined_bdms = []
756 
757         have_ephemeral_bdms = any(filter(
758             block_device.new_format_is_ephemeral, block_device_mapping))
759         have_swap_bdms = any(filter(
760             block_device.new_format_is_swap, block_device_mapping))
761 
762         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
763             flavor_defined_bdms.append(
764                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
765         if instance_type.get('swap') and not have_swap_bdms:
766             flavor_defined_bdms.append(
767                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
768 
769         return flavor_defined_bdms
770 
771     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
772         """Override any block devices from the first list by device name
773 
774         :param overridable_mappings: list which items are overridden
775         :param overrider_mappings: list which items override
776 
777         :returns: A merged list of bdms
778         """
779         device_names = set(bdm['device_name'] for bdm in overrider_mappings
780                            if bdm['device_name'])
781         return (overrider_mappings +
782                 [bdm for bdm in overridable_mappings
783                  if bdm['device_name'] not in device_names])
784 
785     def _check_and_transform_bdm(self, context, base_options, instance_type,
786                                  image_meta, min_count, max_count,
787                                  block_device_mapping, legacy_bdm):
788         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
789         #                  It's needed for legacy conversion to work.
790         root_device_name = (base_options.get('root_device_name') or 'vda')
791         image_ref = base_options.get('image_ref', '')
792         # If the instance is booted by image and has a volume attached,
793         # the volume cannot have the same device name as root_device_name
794         if image_ref:
795             for bdm in block_device_mapping:
796                 if (bdm.get('destination_type') == 'volume' and
797                     block_device.strip_dev(bdm.get(
798                     'device_name')) == root_device_name):
799                     msg = _('The volume cannot be assigned the same device'
800                             ' name as the root device %s') % root_device_name
801                     raise exception.InvalidRequest(msg)
802 
803         image_defined_bdms = self._get_image_defined_bdms(
804             instance_type, image_meta, root_device_name)
805         root_in_image_bdms = (
806             block_device.get_root_bdm(image_defined_bdms) is not None)
807 
808         if legacy_bdm:
809             block_device_mapping = block_device.from_legacy_mapping(
810                 block_device_mapping, image_ref, root_device_name,
811                 no_root=root_in_image_bdms)
812         elif root_in_image_bdms:
813             # NOTE (ndipanov): client will insert an image mapping into the v2
814             # block_device_mapping, but if there is a bootable device in image
815             # mappings - we need to get rid of the inserted image
816             # NOTE (gibi): another case is when a server is booted with an
817             # image to bdm mapping where the image only contains a bdm to a
818             # snapshot. In this case the other image to bdm mapping
819             # contains an unnecessary device with boot_index == 0.
820             # Also in this case the image_ref is None as we are booting from
821             # an image to volume bdm.
822             def not_image_and_root_bdm(bdm):
823                 return not (bdm.get('boot_index') == 0 and
824                             bdm.get('source_type') == 'image')
825 
826             block_device_mapping = list(
827                 filter(not_image_and_root_bdm, block_device_mapping))
828 
829         block_device_mapping = self._merge_bdms_lists(
830             image_defined_bdms, block_device_mapping)
831 
832         if min_count > 1 or max_count > 1:
833             if any(map(lambda bdm: bdm['source_type'] == 'volume',
834                        block_device_mapping)):
835                 msg = _('Cannot attach one or more volumes to multiple'
836                         ' instances')
837                 raise exception.InvalidRequest(msg)
838 
839         block_device_mapping += self._get_flavor_defined_bdms(
840             instance_type, block_device_mapping)
841 
842         return block_device_obj.block_device_make_list_from_dicts(
843                 context, block_device_mapping)
844 
845     def _get_image(self, context, image_href):
846         if not image_href:
847             return None, {}
848 
849         image = self.image_api.get(context, image_href)
850         return image['id'], image
851 
852     def _checks_for_create_and_rebuild(self, context, image_id, image,
853                                        instance_type, metadata,
854                                        files_to_inject, root_bdm,
855                                        validate_numa=True):
856         self._check_metadata_properties_quota(context, metadata)
857         self._check_injected_file_quota(context, files_to_inject)
858         self._validate_flavor_image(context, image_id, image,
859                                     instance_type, root_bdm,
860                                     validate_numa=validate_numa)
861 
862     def _validate_and_build_base_options(self, context, instance_type,
863                                          boot_meta, image_href, image_id,
864                                          kernel_id, ramdisk_id, display_name,
865                                          display_description, key_name,
866                                          key_data, security_groups,
867                                          availability_zone, user_data,
868                                          metadata, access_ip_v4, access_ip_v6,
869                                          requested_networks, config_drive,
870                                          auto_disk_config, reservation_id,
871                                          max_count,
872                                          supports_port_resource_request):
873         """Verify all the input parameters regardless of the provisioning
874         strategy being performed.
875         """
876         if instance_type['disabled']:
877             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
878 
879         if user_data:
880             try:
881                 base64utils.decode_as_bytes(user_data)
882             except TypeError:
883                 raise exception.InstanceUserDataMalformed()
884 
885         # When using Neutron, _check_requested_secgroups will translate and
886         # return any requested security group names to uuids.
887         security_groups = (
888             self._check_requested_secgroups(context, security_groups))
889 
890         # Note:  max_count is the number of instances requested by the user,
891         # max_network_count is the maximum number of instances taking into
892         # account any network quotas
893         max_network_count = self._check_requested_networks(context,
894                                      requested_networks, max_count)
895 
896         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
897                 context, kernel_id, ramdisk_id, boot_meta)
898 
899         config_drive = self._check_config_drive(config_drive)
900 
901         if key_data is None and key_name is not None:
902             key_pair = objects.KeyPair.get_by_name(context,
903                                                    context.user_id,
904                                                    key_name)
905             key_data = key_pair.public_key
906         else:
907             key_pair = None
908 
909         root_device_name = block_device.prepend_dev(
910                 block_device.properties_root_device_name(
911                     boot_meta.get('properties', {})))
912 
913         image_meta = _get_image_meta_obj(boot_meta)
914         numa_topology = hardware.numa_get_constraints(
915                 instance_type, image_meta)
916 
917         system_metadata = {}
918 
919         # PCI requests come from two sources: instance flavor and
920         # requested_networks. The first call in below returns an
921         # InstancePCIRequests object which is a list of InstancePCIRequest
922         # objects. The second call in below creates an InstancePCIRequest
923         # object for each SR-IOV port, and append it to the list in the
924         # InstancePCIRequests object
925         pci_request_info = pci_request.get_pci_requests_from_flavor(
926             instance_type)
927         result = self.network_api.create_resource_requests(
928             context, requested_networks, pci_request_info)
929         network_metadata, port_resource_requests = result
930 
931         # Creating servers with ports that have resource requests, like QoS
932         # minimum bandwidth rules, is only supported in a requested minimum
933         # microversion.
934         if port_resource_requests and not supports_port_resource_request:
935             raise exception.CreateWithPortResourceRequestOldVersion()
936 
937         base_options = {
938             'reservation_id': reservation_id,
939             'image_ref': image_href,
940             'kernel_id': kernel_id or '',
941             'ramdisk_id': ramdisk_id or '',
942             'power_state': power_state.NOSTATE,
943             'vm_state': vm_states.BUILDING,
944             'config_drive': config_drive,
945             'user_id': context.user_id,
946             'project_id': context.project_id,
947             'instance_type_id': instance_type['id'],
948             'memory_mb': instance_type['memory_mb'],
949             'vcpus': instance_type['vcpus'],
950             'root_gb': instance_type['root_gb'],
951             'ephemeral_gb': instance_type['ephemeral_gb'],
952             'display_name': display_name,
953             'display_description': display_description,
954             'user_data': user_data,
955             'key_name': key_name,
956             'key_data': key_data,
957             'locked': False,
958             'metadata': metadata or {},
959             'access_ip_v4': access_ip_v4,
960             'access_ip_v6': access_ip_v6,
961             'availability_zone': availability_zone,
962             'root_device_name': root_device_name,
963             'progress': 0,
964             'pci_requests': pci_request_info,
965             'numa_topology': numa_topology,
966             'system_metadata': system_metadata,
967             'port_resource_requests': port_resource_requests}
968 
969         options_from_image = self._inherit_properties_from_image(
970                 boot_meta, auto_disk_config)
971 
972         base_options.update(options_from_image)
973 
974         # return the validated options and maximum number of instances allowed
975         # by the network quotas
976         return (base_options, max_network_count, key_pair, security_groups,
977                 network_metadata)
978 
979     @staticmethod
980     @db_api.api_context_manager.writer
981     def _create_reqspec_buildreq_instmapping(context, rs, br, im):
982         """Create the request spec, build request, and instance mapping in a
983         single database transaction.
984 
985         The RequestContext must be passed in to this method so that the
986         database transaction context manager decorator will nest properly and
987         include each create() into the same transaction context.
988         """
989         rs.create()
990         br.create()
991         im.create()
992 
993     def _validate_host_or_node(self, context, host, hypervisor_hostname):
994         """Check whether compute nodes exist by validating the host
995         and/or the hypervisor_hostname. There are three cases:
996         1. If only host is supplied, we can lookup the HostMapping in
997         the API DB.
998         2. If only node is supplied, we can query a resource provider
999         with that name in placement.
1000         3. If both host and node are supplied, we can get the cell from
1001         HostMapping and from that lookup the ComputeNode with the
1002         given cell.
1003 
1004         :param context: The API request context.
1005         :param host: Target host.
1006         :param hypervisor_hostname: Target node.
1007         :raises: ComputeHostNotFound if we find no compute nodes with host
1008                  and/or hypervisor_hostname.
1009         """
1010 
1011         if host:
1012             # When host is specified.
1013             try:
1014                 host_mapping = objects.HostMapping.get_by_host(context, host)
1015             except exception.HostMappingNotFound:
1016                 LOG.warning('No host-to-cell mapping found for host '
1017                             '%(host)s.', {'host': host})
1018                 raise exception.ComputeHostNotFound(host=host)
1019             # When both host and node are specified.
1020             if hypervisor_hostname:
1021                 cell = host_mapping.cell_mapping
1022                 with nova_context.target_cell(context, cell) as cctxt:
1023                     # Here we only do an existence check, so we don't
1024                     # need to store the return value into a variable.
1025                     objects.ComputeNode.get_by_host_and_nodename(
1026                         cctxt, host, hypervisor_hostname)
1027         elif hypervisor_hostname:
1028             # When only node is specified.
1029             try:
1030                 self.placementclient.get_provider_by_name(
1031                     context, hypervisor_hostname)
1032             except exception.ResourceProviderNotFound:
1033                 raise exception.ComputeHostNotFound(host=hypervisor_hostname)
1034 
1035     def _provision_instances(self, context, instance_type, min_count,
1036             max_count, base_options, boot_meta, security_groups,
1037             block_device_mapping, shutdown_terminate,
1038             instance_group, check_server_group_quota, filter_properties,
1039             key_pair, tags, trusted_certs, supports_multiattach,
1040             network_metadata=None, requested_host=None,
1041             requested_hypervisor_hostname=None):
1042         # NOTE(boxiang): Check whether compute nodes exist by validating
1043         # the host and/or the hypervisor_hostname. Pass the destination
1044         # to the scheduler with host and/or hypervisor_hostname(node).
1045         destination = None
1046         if requested_host or requested_hypervisor_hostname:
1047             self._validate_host_or_node(context, requested_host,
1048                                         requested_hypervisor_hostname)
1049             destination = objects.Destination()
1050             if requested_host:
1051                 destination.host = requested_host
1052             destination.node = requested_hypervisor_hostname
1053         # Check quotas
1054         num_instances = compute_utils.check_num_instances_quota(
1055                 context, instance_type, min_count, max_count)
1056         security_groups = self.security_group_api.populate_security_groups(
1057                 security_groups)
1058         self.security_group_api.ensure_default(context)
1059         port_resource_requests = base_options.pop('port_resource_requests')
1060         LOG.debug("Going to run %s instances...", num_instances)
1061         instances_to_build = []
1062         # We could be iterating over several instances with several BDMs per
1063         # instance and those BDMs could be using a lot of the same images so
1064         # we want to cache the image API GET results for performance.
1065         image_cache = {}  # dict of image dicts keyed by image id
1066         try:
1067             for i in range(num_instances):
1068                 # Create a uuid for the instance so we can store the
1069                 # RequestSpec before the instance is created.
1070                 instance_uuid = uuidutils.generate_uuid()
1071                 # Store the RequestSpec that will be used for scheduling.
1072                 req_spec = objects.RequestSpec.from_components(context,
1073                         instance_uuid, boot_meta, instance_type,
1074                         base_options['numa_topology'],
1075                         base_options['pci_requests'], filter_properties,
1076                         instance_group, base_options['availability_zone'],
1077                         security_groups=security_groups,
1078                         port_resource_requests=port_resource_requests)
1079 
1080                 if block_device_mapping:
1081                     # Record whether or not we are a BFV instance
1082                     root = block_device_mapping.root_bdm()
1083                     req_spec.is_bfv = bool(root and root.is_volume)
1084                 else:
1085                     # If we have no BDMs, we're clearly not BFV
1086                     req_spec.is_bfv = False
1087 
1088                 # NOTE(danms): We need to record num_instances on the request
1089                 # spec as this is how the conductor knows how many were in this
1090                 # batch.
1091                 req_spec.num_instances = num_instances
1092 
1093                 # NOTE(stephenfin): The network_metadata field is not persisted
1094                 # inside RequestSpec object.
1095                 if network_metadata:
1096                     req_spec.network_metadata = network_metadata
1097 
1098                 if destination:
1099                     req_spec.requested_destination = destination
1100 
1101                 # Create an instance object, but do not store in db yet.
1102                 instance = objects.Instance(context=context)
1103                 instance.uuid = instance_uuid
1104                 instance.update(base_options)
1105                 instance.keypairs = objects.KeyPairList(objects=[])
1106                 if key_pair:
1107                     instance.keypairs.objects.append(key_pair)
1108 
1109                 instance.trusted_certs = self._retrieve_trusted_certs_object(
1110                     context, trusted_certs)
1111 
1112                 instance = self.create_db_entry_for_new_instance(context,
1113                         instance_type, boot_meta, instance, security_groups,
1114                         block_device_mapping, num_instances, i,
1115                         shutdown_terminate, create_instance=False)
1116                 block_device_mapping = (
1117                     self._bdm_validate_set_size_and_instance(context,
1118                         instance, instance_type, block_device_mapping,
1119                         image_cache, supports_multiattach))
1120                 instance_tags = self._transform_tags(tags, instance.uuid)
1121 
1122                 build_request = objects.BuildRequest(context,
1123                         instance=instance, instance_uuid=instance.uuid,
1124                         project_id=instance.project_id,
1125                         block_device_mappings=block_device_mapping,
1126                         tags=instance_tags)
1127 
1128                 # Create an instance_mapping.  The null cell_mapping indicates
1129                 # that the instance doesn't yet exist in a cell, and lookups
1130                 # for it need to instead look for the RequestSpec.
1131                 # cell_mapping will be populated after scheduling, with a
1132                 # scheduling failure using the cell_mapping for the special
1133                 # cell0.
1134                 inst_mapping = objects.InstanceMapping(context=context)
1135                 inst_mapping.instance_uuid = instance_uuid
1136                 inst_mapping.project_id = context.project_id
1137                 inst_mapping.user_id = context.user_id
1138                 inst_mapping.cell_mapping = None
1139 
1140                 # Create the request spec, build request, and instance mapping
1141                 # records in a single transaction so that if a DBError is
1142                 # raised from any of them, all INSERTs will be rolled back and
1143                 # no orphaned records will be left behind.
1144                 self._create_reqspec_buildreq_instmapping(context, req_spec,
1145                                                           build_request,
1146                                                           inst_mapping)
1147 
1148                 instances_to_build.append(
1149                     (req_spec, build_request, inst_mapping))
1150 
1151                 if instance_group:
1152                     if check_server_group_quota:
1153                         try:
1154                             objects.Quotas.check_deltas(
1155                                 context, {'server_group_members': 1},
1156                                 instance_group, context.user_id)
1157                         except exception.OverQuota:
1158                             msg = _("Quota exceeded, too many servers in "
1159                                     "group")
1160                             raise exception.QuotaError(msg)
1161 
1162                     members = objects.InstanceGroup.add_members(
1163                         context, instance_group.uuid, [instance.uuid])
1164 
1165                     # NOTE(melwitt): We recheck the quota after creating the
1166                     # object to prevent users from allocating more resources
1167                     # than their allowed quota in the event of a race. This is
1168                     # configurable because it can be expensive if strict quota
1169                     # limits are not required in a deployment.
1170                     if CONF.quota.recheck_quota and check_server_group_quota:
1171                         try:
1172                             objects.Quotas.check_deltas(
1173                                 context, {'server_group_members': 0},
1174                                 instance_group, context.user_id)
1175                         except exception.OverQuota:
1176                             objects.InstanceGroup._remove_members_in_db(
1177                                 context, instance_group.id, [instance.uuid])
1178                             msg = _("Quota exceeded, too many servers in "
1179                                     "group")
1180                             raise exception.QuotaError(msg)
1181                     # list of members added to servers group in this iteration
1182                     # is needed to check quota of server group during add next
1183                     # instance
1184                     instance_group.members.extend(members)
1185 
1186         # In the case of any exceptions, attempt DB cleanup
1187         except Exception:
1188             with excutils.save_and_reraise_exception():
1189                 self._cleanup_build_artifacts(None, instances_to_build)
1190 
1191         return instances_to_build
1192 
1193     @staticmethod
1194     def _retrieve_trusted_certs_object(context, trusted_certs, rebuild=False):
1195         """Convert user-requested trusted cert IDs to TrustedCerts object
1196 
1197         Also validates that the deployment is new enough to support trusted
1198         image certification validation.
1199 
1200         :param context: The user request auth context
1201         :param trusted_certs: list of user-specified trusted cert string IDs,
1202             may be None
1203         :param rebuild: True if rebuilding the server, False if creating a
1204             new server
1205         :returns: nova.objects.TrustedCerts object or None if no user-specified
1206             trusted cert IDs were given and nova is not configured with
1207             default trusted cert IDs
1208         """
1209         # Retrieve trusted_certs parameter, or use CONF value if certificate
1210         # validation is enabled
1211         if trusted_certs:
1212             certs_to_return = objects.TrustedCerts(ids=trusted_certs)
1213         elif (CONF.glance.verify_glance_signatures and
1214               CONF.glance.enable_certificate_validation and
1215               CONF.glance.default_trusted_certificate_ids):
1216             certs_to_return = objects.TrustedCerts(
1217                 ids=CONF.glance.default_trusted_certificate_ids)
1218         else:
1219             return None
1220 
1221         return certs_to_return
1222 
1223     def _get_bdm_image_metadata(self, context, block_device_mapping,
1224                                 legacy_bdm=True):
1225         """If we are booting from a volume, we need to get the
1226         volume details from Cinder and make sure we pass the
1227         metadata back accordingly.
1228         """
1229         if not block_device_mapping:
1230             return {}
1231 
1232         for bdm in block_device_mapping:
1233             if (legacy_bdm and
1234                     block_device.get_device_letter(
1235                        bdm.get('device_name', '')) != 'a'):
1236                 continue
1237             elif not legacy_bdm and bdm.get('boot_index') != 0:
1238                 continue
1239 
1240             volume_id = bdm.get('volume_id')
1241             snapshot_id = bdm.get('snapshot_id')
1242             if snapshot_id:
1243                 # NOTE(alaski): A volume snapshot inherits metadata from the
1244                 # originating volume, but the API does not expose metadata
1245                 # on the snapshot itself.  So we query the volume for it below.
1246                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1247                 volume_id = snapshot['volume_id']
1248 
1249             if bdm.get('image_id'):
1250                 try:
1251                     image_id = bdm['image_id']
1252                     image_meta = self.image_api.get(context, image_id)
1253                     return image_meta
1254                 except Exception:
1255                     raise exception.InvalidBDMImage(id=image_id)
1256             elif volume_id:
1257                 try:
1258                     volume = self.volume_api.get(context, volume_id)
1259                 except exception.CinderConnectionFailed:
1260                     raise
1261                 except Exception:
1262                     raise exception.InvalidBDMVolume(id=volume_id)
1263 
1264                 if not volume.get('bootable', True):
1265                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1266 
1267                 return utils.get_image_metadata_from_volume(volume)
1268         return {}
1269 
1270     @staticmethod
1271     def _get_requested_instance_group(context, filter_properties):
1272         if (not filter_properties or
1273                 not filter_properties.get('scheduler_hints')):
1274             return
1275 
1276         group_hint = filter_properties.get('scheduler_hints').get('group')
1277         if not group_hint:
1278             return
1279 
1280         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1281 
1282     def _create_instance(self, context, instance_type,
1283                image_href, kernel_id, ramdisk_id,
1284                min_count, max_count,
1285                display_name, display_description,
1286                key_name, key_data, security_groups,
1287                availability_zone, user_data, metadata, injected_files,
1288                admin_password, access_ip_v4, access_ip_v6,
1289                requested_networks, config_drive,
1290                block_device_mapping, auto_disk_config, filter_properties,
1291                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1292                check_server_group_quota=False, tags=None,
1293                supports_multiattach=False, trusted_certs=None,
1294                supports_port_resource_request=False,
1295                requested_host=None, requested_hypervisor_hostname=None):
1296         """Verify all the input parameters regardless of the provisioning
1297         strategy being performed and schedule the instance(s) for
1298         creation.
1299         """
1300 
1301         # Normalize and setup some parameters
1302         if reservation_id is None:
1303             reservation_id = utils.generate_uid('r')
1304         security_groups = security_groups or ['default']
1305         min_count = min_count or 1
1306         max_count = max_count or min_count
1307         block_device_mapping = block_device_mapping or []
1308         tags = tags or []
1309 
1310         if image_href:
1311             image_id, boot_meta = self._get_image(context, image_href)
1312         else:
1313             # This is similar to the logic in _retrieve_trusted_certs_object.
1314             if (trusted_certs or
1315                 (CONF.glance.verify_glance_signatures and
1316                  CONF.glance.enable_certificate_validation and
1317                  CONF.glance.default_trusted_certificate_ids)):
1318                 msg = _("Image certificate validation is not supported "
1319                         "when booting from volume")
1320                 raise exception.CertificateValidationFailed(message=msg)
1321             image_id = None
1322             boot_meta = self._get_bdm_image_metadata(
1323                 context, block_device_mapping, legacy_bdm)
1324 
1325         self._check_auto_disk_config(image=boot_meta,
1326                                      auto_disk_config=auto_disk_config)
1327 
1328         base_options, max_net_count, key_pair, security_groups, \
1329             network_metadata = self._validate_and_build_base_options(
1330                     context, instance_type, boot_meta, image_href, image_id,
1331                     kernel_id, ramdisk_id, display_name, display_description,
1332                     key_name, key_data, security_groups, availability_zone,
1333                     user_data, metadata, access_ip_v4, access_ip_v6,
1334                     requested_networks, config_drive, auto_disk_config,
1335                     reservation_id, max_count, supports_port_resource_request)
1336 
1337         # max_net_count is the maximum number of instances requested by the
1338         # user adjusted for any network quota constraints, including
1339         # consideration of connections to each requested network
1340         if max_net_count < min_count:
1341             raise exception.PortLimitExceeded()
1342         elif max_net_count < max_count:
1343             LOG.info("max count reduced from %(max_count)d to "
1344                      "%(max_net_count)d due to network port quota",
1345                      {'max_count': max_count,
1346                       'max_net_count': max_net_count})
1347             max_count = max_net_count
1348 
1349         block_device_mapping = self._check_and_transform_bdm(context,
1350             base_options, instance_type, boot_meta, min_count, max_count,
1351             block_device_mapping, legacy_bdm)
1352 
1353         # We can't do this check earlier because we need bdms from all sources
1354         # to have been merged in order to get the root bdm.
1355         # Set validate_numa=False since numa validation is already done by
1356         # _validate_and_build_base_options().
1357         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1358                 instance_type, metadata, injected_files,
1359                 block_device_mapping.root_bdm(), validate_numa=False)
1360 
1361         instance_group = self._get_requested_instance_group(context,
1362                                    filter_properties)
1363 
1364         tags = self._create_tag_list_obj(context, tags)
1365 
1366         instances_to_build = self._provision_instances(
1367             context, instance_type, min_count, max_count, base_options,
1368             boot_meta, security_groups, block_device_mapping,
1369             shutdown_terminate, instance_group, check_server_group_quota,
1370             filter_properties, key_pair, tags, trusted_certs,
1371             supports_multiattach, network_metadata,
1372             requested_host, requested_hypervisor_hostname)
1373 
1374         instances = []
1375         request_specs = []
1376         build_requests = []
1377         for rs, build_request, im in instances_to_build:
1378             build_requests.append(build_request)
1379             instance = build_request.get_new_instance(context)
1380             instances.append(instance)
1381             request_specs.append(rs)
1382 
1383         self.compute_task_api.schedule_and_build_instances(
1384             context,
1385             build_requests=build_requests,
1386             request_spec=request_specs,
1387             image=boot_meta,
1388             admin_password=admin_password,
1389             injected_files=injected_files,
1390             requested_networks=requested_networks,
1391             block_device_mapping=block_device_mapping,
1392             tags=tags)
1393 
1394         return instances, reservation_id
1395 
1396     @staticmethod
1397     def _cleanup_build_artifacts(instances, instances_to_build):
1398         # instances_to_build is a list of tuples:
1399         # (RequestSpec, BuildRequest, InstanceMapping)
1400 
1401         # Be paranoid about artifacts being deleted underneath us.
1402         for instance in instances or []:
1403             try:
1404                 instance.destroy()
1405             except exception.InstanceNotFound:
1406                 pass
1407         for rs, build_request, im in instances_to_build or []:
1408             try:
1409                 rs.destroy()
1410             except exception.RequestSpecNotFound:
1411                 pass
1412             try:
1413                 build_request.destroy()
1414             except exception.BuildRequestNotFound:
1415                 pass
1416             try:
1417                 im.destroy()
1418             except exception.InstanceMappingNotFound:
1419                 pass
1420 
1421     @staticmethod
1422     def _volume_size(instance_type, bdm):
1423         size = bdm.get('volume_size')
1424         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1425         if (size is None and bdm.get('source_type') == 'blank' and
1426                 bdm.get('destination_type') == 'local'):
1427             if bdm.get('guest_format') == 'swap':
1428                 size = instance_type.get('swap', 0)
1429             else:
1430                 size = instance_type.get('ephemeral_gb', 0)
1431         return size
1432 
1433     def _prepare_image_mapping(self, instance_type, mappings):
1434         """Extract and format blank devices from image mappings."""
1435 
1436         prepared_mappings = []
1437 
1438         for bdm in block_device.mappings_prepend_dev(mappings):
1439             LOG.debug("Image bdm %s", bdm)
1440 
1441             virtual_name = bdm['virtual']
1442             if virtual_name == 'ami' or virtual_name == 'root':
1443                 continue
1444 
1445             if not block_device.is_swap_or_ephemeral(virtual_name):
1446                 continue
1447 
1448             guest_format = bdm.get('guest_format')
1449             if virtual_name == 'swap':
1450                 guest_format = 'swap'
1451             if not guest_format:
1452                 guest_format = CONF.default_ephemeral_format
1453 
1454             values = block_device.BlockDeviceDict({
1455                 'device_name': bdm['device'],
1456                 'source_type': 'blank',
1457                 'destination_type': 'local',
1458                 'device_type': 'disk',
1459                 'guest_format': guest_format,
1460                 'delete_on_termination': True,
1461                 'boot_index': -1})
1462 
1463             values['volume_size'] = self._volume_size(
1464                 instance_type, values)
1465             if values['volume_size'] == 0:
1466                 continue
1467 
1468             prepared_mappings.append(values)
1469 
1470         return prepared_mappings
1471 
1472     def _bdm_validate_set_size_and_instance(self, context, instance,
1473                                             instance_type,
1474                                             block_device_mapping,
1475                                             image_cache,
1476                                             supports_multiattach=False):
1477         """Ensure the bdms are valid, then set size and associate with instance
1478 
1479         Because this method can be called multiple times when more than one
1480         instance is booted in a single request it makes a copy of the bdm list.
1481 
1482         :param context: nova auth RequestContext
1483         :param instance: Instance object
1484         :param instance_type: Flavor object - used for swap and ephemeral BDMs
1485         :param block_device_mapping: BlockDeviceMappingList object
1486         :param image_cache: dict of image dicts keyed by id which is used as a
1487             cache in case there are multiple BDMs in the same request using
1488             the same image to avoid redundant GET calls to the image service
1489         :param supports_multiattach: True if the request supports multiattach
1490             volumes, False otherwise
1491         """
1492         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1493                   instance_uuid=instance.uuid)
1494         self._validate_bdm(
1495             context, instance, instance_type, block_device_mapping,
1496             image_cache, supports_multiattach)
1497         instance_block_device_mapping = block_device_mapping.obj_clone()
1498         for bdm in instance_block_device_mapping:
1499             bdm.volume_size = self._volume_size(instance_type, bdm)
1500             bdm.instance_uuid = instance.uuid
1501         return instance_block_device_mapping
1502 
1503     @staticmethod
1504     def _check_requested_volume_type(bdm, volume_type_id_or_name,
1505                                      volume_types):
1506         """If we are specifying a volume type, we need to get the
1507         volume type details from Cinder and make sure the ``volume_type``
1508         is available.
1509         """
1510 
1511         # NOTE(brinzhang): Verify that the specified volume type exists.
1512         # And save the volume type name internally for consistency in the
1513         # BlockDeviceMapping object.
1514         for vol_type in volume_types:
1515             if (volume_type_id_or_name == vol_type['id'] or
1516                         volume_type_id_or_name == vol_type['name']):
1517                 bdm.volume_type = vol_type['name']
1518                 break
1519         else:
1520             raise exception.VolumeTypeNotFound(
1521                 id_or_name=volume_type_id_or_name)
1522 
1523     @staticmethod
1524     def _check_compute_supports_volume_type(context):
1525         # NOTE(brinzhang): Checking the minimum nova-compute service
1526         # version across the deployment. Just make sure the volume
1527         # type can be supported when the bdm.volume_type is requested.
1528         min_compute_version = objects.service.get_minimum_version_all_cells(
1529             context, ['nova-compute'])
1530         if min_compute_version < MIN_COMPUTE_VOLUME_TYPE:
1531             raise exception.VolumeTypeSupportNotYetAvailable()
1532 
1533     def _validate_bdm(self, context, instance, instance_type,
1534                       block_device_mappings, image_cache,
1535                       supports_multiattach=False):
1536         """Validate requested block device mappings.
1537 
1538         :param context: nova auth RequestContext
1539         :param instance: Instance object
1540         :param instance_type: Flavor object - used for swap and ephemeral BDMs
1541         :param block_device_mappings: BlockDeviceMappingList object
1542         :param image_cache: dict of image dicts keyed by id which is used as a
1543             cache in case there are multiple BDMs in the same request using
1544             the same image to avoid redundant GET calls to the image service
1545         :param supports_multiattach: True if the request supports multiattach
1546             volumes, False otherwise
1547         """
1548         # Make sure that the boot indexes make sense.
1549         # Setting a negative value or None indicates that the device should not
1550         # be used for booting.
1551         boot_indexes = sorted([bdm.boot_index
1552                                for bdm in block_device_mappings
1553                                if bdm.boot_index is not None and
1554                                bdm.boot_index >= 0])
1555 
1556         # Each device which is capable of being used as boot device should
1557         # be given a unique boot index, starting from 0 in ascending order, and
1558         # there needs to be at least one boot device.
1559         if not boot_indexes or any(i != v for i, v in enumerate(boot_indexes)):
1560             # Convert the BlockDeviceMappingList to a list for repr details.
1561             LOG.debug('Invalid block device mapping boot sequence for '
1562                       'instance: %s', list(block_device_mappings),
1563                       instance=instance)
1564             raise exception.InvalidBDMBootSequence()
1565 
1566         volume_types = None
1567         volume_type_is_supported = False
1568         for bdm in block_device_mappings:
1569             volume_type = bdm.volume_type
1570             if volume_type:
1571                 if not volume_type_is_supported:
1572                     # The following method raises
1573                     # VolumeTypeSupportNotYetAvailable if the minimum
1574                     # nova-compute service version across the deployment is
1575                     # not new enough to support creating volumes with a
1576                     # specific type.
1577                     self._check_compute_supports_volume_type(context)
1578                     # Set the flag to avoid calling
1579                     # _check_compute_supports_volume_type more than once in
1580                     # this for loop.
1581                     volume_type_is_supported = True
1582 
1583                 if not volume_types:
1584                     # In order to reduce the number of hit cinder APIs,
1585                     # initialize our cache of volume types.
1586                     volume_types = self.volume_api.get_all_volume_types(
1587                         context)
1588                 # NOTE(brinzhang): Ensure the validity of volume_type.
1589                 self._check_requested_volume_type(bdm, volume_type,
1590                                                   volume_types)
1591 
1592             # NOTE(vish): For now, just make sure the volumes are accessible.
1593             # Additionally, check that the volume can be attached to this
1594             # instance.
1595             snapshot_id = bdm.snapshot_id
1596             volume_id = bdm.volume_id
1597             image_id = bdm.image_id
1598             if image_id is not None:
1599                 if (image_id != instance.get('image_ref') and
1600                         image_id not in image_cache):
1601                     try:
1602                         # Cache the results of the image GET so we do not make
1603                         # the same request for the same image if processing
1604                         # multiple BDMs or multiple servers with the same image
1605                         image_cache[image_id] = self._get_image(
1606                             context, image_id)
1607                     except Exception:
1608                         raise exception.InvalidBDMImage(id=image_id)
1609                 if (bdm.source_type == 'image' and
1610                         bdm.destination_type == 'volume' and
1611                         not bdm.volume_size):
1612                     raise exception.InvalidBDM(message=_("Images with "
1613                         "destination_type 'volume' need to have a non-zero "
1614                         "size specified"))
1615             elif volume_id is not None:
1616                 try:
1617                     volume = self.volume_api.get(context, volume_id)
1618                     self._check_attach_and_reserve_volume(
1619                         context, volume, instance, bdm, supports_multiattach)
1620                     bdm.volume_size = volume.get('size')
1621 
1622                     # NOTE(mnaser): If we end up reserving the volume, it will
1623                     #               not have an attachment_id which is needed
1624                     #               for cleanups.  This can be removed once
1625                     #               all calls to reserve_volume are gone.
1626                     if 'attachment_id' not in bdm:
1627                         bdm.attachment_id = None
1628                 except (exception.CinderConnectionFailed,
1629                         exception.InvalidVolume,
1630                         exception.MultiattachNotSupportedOldMicroversion):
1631                     raise
1632                 except exception.InvalidInput as exc:
1633                     raise exception.InvalidVolume(reason=exc.format_message())
1634                 except Exception as e:
1635                     LOG.info('Failed validating volume %s. Error: %s',
1636                              volume_id, e)
1637                     raise exception.InvalidBDMVolume(id=volume_id)
1638             elif snapshot_id is not None:
1639                 try:
1640                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1641                     bdm.volume_size = bdm.volume_size or snap.get('size')
1642                 except exception.CinderConnectionFailed:
1643                     raise
1644                 except Exception:
1645                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1646             elif (bdm.source_type == 'blank' and
1647                     bdm.destination_type == 'volume' and
1648                     not bdm.volume_size):
1649                 raise exception.InvalidBDM(message=_("Blank volumes "
1650                     "(source: 'blank', dest: 'volume') need to have non-zero "
1651                     "size"))
1652 
1653         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1654                 for bdm in block_device_mappings
1655                 if block_device.new_format_is_ephemeral(bdm))
1656         if ephemeral_size > instance_type['ephemeral_gb']:
1657             raise exception.InvalidBDMEphemeralSize()
1658 
1659         # There should be only one swap
1660         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1661         if len(swap_list) > 1:
1662             msg = _("More than one swap drive requested.")
1663             raise exception.InvalidBDMFormat(details=msg)
1664 
1665         if swap_list:
1666             swap_size = swap_list[0].volume_size or 0
1667             if swap_size > instance_type['swap']:
1668                 raise exception.InvalidBDMSwapSize()
1669 
1670         max_local = CONF.max_local_block_devices
1671         if max_local >= 0:
1672             num_local = len([bdm for bdm in block_device_mappings
1673                              if bdm.destination_type == 'local'])
1674             if num_local > max_local:
1675                 raise exception.InvalidBDMLocalsLimit()
1676 
1677     def _populate_instance_names(self, instance, num_instances, index):
1678         """Populate instance display_name and hostname.
1679 
1680         :param instance: The instance to set the display_name, hostname for
1681         :type instance: nova.objects.Instance
1682         :param num_instances: Total number of instances being created in this
1683             request
1684         :param index: The 0-based index of this particular instance
1685         """
1686         # NOTE(mriedem): This is only here for test simplicity since a server
1687         # name is required in the REST API.
1688         if 'display_name' not in instance or instance.display_name is None:
1689             instance.display_name = 'Server %s' % instance.uuid
1690 
1691         # if we're booting multiple instances, we need to add an indexing
1692         # suffix to both instance.hostname and instance.display_name. This is
1693         # not necessary for a single instance.
1694         if num_instances == 1:
1695             default_hostname = 'Server-%s' % instance.uuid
1696             instance.hostname = utils.sanitize_hostname(
1697                 instance.display_name, default_hostname)
1698         elif num_instances > 1:
1699             old_display_name = instance.display_name
1700             new_display_name = '%s-%d' % (old_display_name, index + 1)
1701 
1702             if utils.sanitize_hostname(old_display_name) == "":
1703                 instance.hostname = 'Server-%s' % instance.uuid
1704             else:
1705                 instance.hostname = utils.sanitize_hostname(
1706                     new_display_name)
1707 
1708             instance.display_name = new_display_name
1709 
1710     def _populate_instance_for_create(self, context, instance, image,
1711                                       index, security_groups, instance_type,
1712                                       num_instances, shutdown_terminate):
1713         """Build the beginning of a new instance."""
1714 
1715         instance.launch_index = index
1716         instance.vm_state = vm_states.BUILDING
1717         instance.task_state = task_states.SCHEDULING
1718         info_cache = objects.InstanceInfoCache()
1719         info_cache.instance_uuid = instance.uuid
1720         info_cache.network_info = network_model.NetworkInfo()
1721         instance.info_cache = info_cache
1722         instance.flavor = instance_type
1723         instance.old_flavor = None
1724         instance.new_flavor = None
1725         if CONF.ephemeral_storage_encryption.enabled:
1726             # NOTE(kfarr): dm-crypt expects the cipher in a
1727             # hyphenated format: cipher-chainmode-ivmode
1728             # (ex: aes-xts-plain64). The algorithm needs
1729             # to be parsed out to pass to the key manager (ex: aes).
1730             cipher = CONF.ephemeral_storage_encryption.cipher
1731             algorithm = cipher.split('-')[0] if cipher else None
1732             instance.ephemeral_key_uuid = self.key_manager.create_key(
1733                 context,
1734                 algorithm=algorithm,
1735                 length=CONF.ephemeral_storage_encryption.key_size)
1736         else:
1737             instance.ephemeral_key_uuid = None
1738 
1739         # Store image properties so we can use them later
1740         # (for notifications, etc).  Only store what we can.
1741         if not instance.obj_attr_is_set('system_metadata'):
1742             instance.system_metadata = {}
1743         # Make sure we have the dict form that we need for instance_update.
1744         instance.system_metadata = utils.instance_sys_meta(instance)
1745 
1746         system_meta = utils.get_system_metadata_from_image(
1747             image, instance_type)
1748 
1749         # In case we couldn't find any suitable base_image
1750         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1751 
1752         system_meta['owner_user_name'] = context.user_name
1753         system_meta['owner_project_name'] = context.project_name
1754 
1755         instance.system_metadata.update(system_meta)
1756 
1757         if CONF.use_neutron:
1758             # For Neutron we don't actually store anything in the database, we
1759             # proxy the security groups on the instance from the ports
1760             # attached to the instance.
1761             instance.security_groups = objects.SecurityGroupList()
1762         else:
1763             instance.security_groups = security_groups
1764 
1765         self._populate_instance_names(instance, num_instances, index)
1766         instance.shutdown_terminate = shutdown_terminate
1767 
1768         return instance
1769 
1770     def _create_tag_list_obj(self, context, tags):
1771         """Create TagList objects from simple string tags.
1772 
1773         :param context: security context.
1774         :param tags: simple string tags from API request.
1775         :returns: TagList object.
1776         """
1777         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1778         tag_list_obj = objects.TagList(objects=tag_list)
1779         return tag_list_obj
1780 
1781     def _transform_tags(self, tags, resource_id):
1782         """Change the resource_id of the tags according to the input param.
1783 
1784         Because this method can be called multiple times when more than one
1785         instance is booted in a single request it makes a copy of the tags
1786         list.
1787 
1788         :param tags: TagList object.
1789         :param resource_id: string.
1790         :returns: TagList object.
1791         """
1792         instance_tags = tags.obj_clone()
1793         for tag in instance_tags:
1794             tag.resource_id = resource_id
1795         return instance_tags
1796 
1797     # This method remains because cellsv1 uses it in the scheduler
1798     def create_db_entry_for_new_instance(self, context, instance_type, image,
1799             instance, security_group, block_device_mapping, num_instances,
1800             index, shutdown_terminate=False, create_instance=True):
1801         """Create an entry in the DB for this new instance,
1802         including any related table updates (such as security group,
1803         etc).
1804 
1805         This is called by the scheduler after a location for the
1806         instance has been determined.
1807 
1808         :param create_instance: Determines if the instance is created here or
1809             just populated for later creation. This is done so that this code
1810             can be shared with cellsv1 which needs the instance creation to
1811             happen here. It should be removed and this method cleaned up when
1812             cellsv1 is a distant memory.
1813         """
1814         self._populate_instance_for_create(context, instance, image, index,
1815                                            security_group, instance_type,
1816                                            num_instances, shutdown_terminate)
1817 
1818         if create_instance:
1819             instance.create()
1820 
1821         return instance
1822 
1823     def _check_multiple_instances_with_neutron_ports(self,
1824                                                      requested_networks):
1825         """Check whether multiple instances are created from port id(s)."""
1826         for requested_net in requested_networks:
1827             if requested_net.port_id:
1828                 msg = _("Unable to launch multiple instances with"
1829                         " a single configured port ID. Please launch your"
1830                         " instance one by one with different ports.")
1831                 raise exception.MultiplePortsNotApplicable(reason=msg)
1832 
1833     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1834         """Check whether multiple instances are created with specified ip."""
1835 
1836         for requested_net in requested_networks:
1837             if requested_net.network_id and requested_net.address:
1838                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1839                         "is specified.")
1840                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1841 
1842     @hooks.add_hook("create_instance")
1843     def create(self, context, instance_type,
1844                image_href, kernel_id=None, ramdisk_id=None,
1845                min_count=None, max_count=None,
1846                display_name=None, display_description=None,
1847                key_name=None, key_data=None, security_groups=None,
1848                availability_zone=None, forced_host=None, forced_node=None,
1849                user_data=None, metadata=None, injected_files=None,
1850                admin_password=None, block_device_mapping=None,
1851                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1852                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1853                legacy_bdm=True, shutdown_terminate=False,
1854                check_server_group_quota=False, tags=None,
1855                supports_multiattach=False, trusted_certs=None,
1856                supports_port_resource_request=False,
1857                requested_host=None, requested_hypervisor_hostname=None):
1858         """Provision instances, sending instance information to the
1859         scheduler.  The scheduler will determine where the instance(s)
1860         go and will handle creating the DB entries.
1861 
1862         Returns a tuple of (instances, reservation_id)
1863         """
1864         if requested_networks and max_count is not None and max_count > 1:
1865             self._check_multiple_instances_with_specified_ip(
1866                 requested_networks)
1867             if utils.is_neutron():
1868                 self._check_multiple_instances_with_neutron_ports(
1869                     requested_networks)
1870 
1871         if availability_zone:
1872             available_zones = availability_zones.\
1873                 get_availability_zones(context.elevated(), self.host_api,
1874                                        get_only_available=True)
1875             if forced_host is None and availability_zone not in \
1876                     available_zones:
1877                 msg = _('The requested availability zone is not available')
1878                 raise exception.InvalidRequest(msg)
1879 
1880         filter_properties = scheduler_utils.build_filter_properties(
1881                 scheduler_hints, forced_host, forced_node, instance_type)
1882 
1883         return self._create_instance(
1884             context, instance_type,
1885             image_href, kernel_id, ramdisk_id,
1886             min_count, max_count,
1887             display_name, display_description,
1888             key_name, key_data, security_groups,
1889             availability_zone, user_data, metadata,
1890             injected_files, admin_password,
1891             access_ip_v4, access_ip_v6,
1892             requested_networks, config_drive,
1893             block_device_mapping, auto_disk_config,
1894             filter_properties=filter_properties,
1895             legacy_bdm=legacy_bdm,
1896             shutdown_terminate=shutdown_terminate,
1897             check_server_group_quota=check_server_group_quota,
1898             tags=tags, supports_multiattach=supports_multiattach,
1899             trusted_certs=trusted_certs,
1900             supports_port_resource_request=supports_port_resource_request,
1901             requested_host=requested_host,
1902             requested_hypervisor_hostname=requested_hypervisor_hostname)
1903 
1904     def _check_auto_disk_config(self, instance=None, image=None,
1905                                 **extra_instance_updates):
1906         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1907         if auto_disk_config is None:
1908             return
1909         if not image and not instance:
1910             return
1911 
1912         if image:
1913             image_props = image.get("properties", {})
1914             auto_disk_config_img = \
1915                 utils.get_auto_disk_config_from_image_props(image_props)
1916             image_ref = image.get("id")
1917         else:
1918             sys_meta = utils.instance_sys_meta(instance)
1919             image_ref = sys_meta.get('image_base_image_ref')
1920             auto_disk_config_img = \
1921                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1922 
1923         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1924                                                auto_disk_config,
1925                                                image_ref)
1926 
1927     def _lookup_instance(self, context, uuid):
1928         '''Helper method for pulling an instance object from a database.
1929 
1930         During the transition to cellsv2 there is some complexity around
1931         retrieving an instance from the database which this method hides. If
1932         there is an instance mapping then query the cell for the instance, if
1933         no mapping exists then query the configured nova database.
1934 
1935         Once we are past the point that all deployments can be assumed to be
1936         migrated to cellsv2 this method can go away.
1937         '''
1938         inst_map = None
1939         try:
1940             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1941                 context, uuid)
1942         except exception.InstanceMappingNotFound:
1943             # TODO(alaski): This exception block can be removed once we're
1944             # guaranteed everyone is using cellsv2.
1945             pass
1946 
1947         if inst_map is None or inst_map.cell_mapping is None:
1948             # If inst_map is None then the deployment has not migrated to
1949             # cellsv2 yet.
1950             # If inst_map.cell_mapping is None then the instance is not in a
1951             # cell yet. Until instance creation moves to the conductor the
1952             # instance can be found in the configured database, so attempt
1953             # to look it up.
1954             cell = None
1955             try:
1956                 instance = objects.Instance.get_by_uuid(context, uuid)
1957             except exception.InstanceNotFound:
1958                 # If we get here then the conductor is in charge of writing the
1959                 # instance to the database and hasn't done that yet. It's up to
1960                 # the caller of this method to determine what to do with that
1961                 # information.
1962                 return None, None
1963         else:
1964             cell = inst_map.cell_mapping
1965             with nova_context.target_cell(context, cell) as cctxt:
1966                 try:
1967                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1968                 except exception.InstanceNotFound:
1969                     # Since the cell_mapping exists we know the instance is in
1970                     # the cell, however InstanceNotFound means it's already
1971                     # deleted.
1972                     return None, None
1973         return cell, instance
1974 
1975     def _delete_while_booting(self, context, instance):
1976         """Handle deletion if the instance has not reached a cell yet
1977 
1978         Deletion before an instance reaches a cell needs to be handled
1979         differently. What we're attempting to do is delete the BuildRequest
1980         before the api level conductor does.  If we succeed here then the boot
1981         request stops before reaching a cell.  If not then the instance will
1982         need to be looked up in a cell db and the normal delete path taken.
1983         """
1984         deleted = self._attempt_delete_of_buildrequest(context, instance)
1985         if deleted:
1986             # If we've reached this block the successful deletion of the
1987             # buildrequest indicates that the build process should be halted by
1988             # the conductor.
1989 
1990             # NOTE(alaski): Though the conductor halts the build process it
1991             # does not currently delete the instance record. This is
1992             # because in the near future the instance record will not be
1993             # created if the buildrequest has been deleted here. For now we
1994             # ensure the instance has been set to deleted at this point.
1995             # Yes this directly contradicts the comment earlier in this
1996             # method, but this is a temporary measure.
1997             # Look up the instance because the current instance object was
1998             # stashed on the buildrequest and therefore not complete enough
1999             # to run .destroy().
2000             try:
2001                 instance_uuid = instance.uuid
2002                 cell, instance = self._lookup_instance(context, instance_uuid)
2003                 if instance is not None:
2004                     # If instance is None it has already been deleted.
2005                     if cell:
2006                         with nova_context.target_cell(context, cell) as cctxt:
2007                             # FIXME: When the instance context is targeted,
2008                             # we can remove this
2009                             with compute_utils.notify_about_instance_delete(
2010                                     self.notifier, cctxt, instance):
2011                                 instance.destroy()
2012                     else:
2013                         instance.destroy()
2014             except exception.InstanceNotFound:
2015                 pass
2016 
2017             return True
2018         return False
2019 
2020     def _attempt_delete_of_buildrequest(self, context, instance):
2021         # If there is a BuildRequest then the instance may not have been
2022         # written to a cell db yet. Delete the BuildRequest here, which
2023         # will indicate that the Instance build should not proceed.
2024         try:
2025             build_req = objects.BuildRequest.get_by_instance_uuid(
2026                 context, instance.uuid)
2027             build_req.destroy()
2028         except exception.BuildRequestNotFound:
2029             # This means that conductor has deleted the BuildRequest so the
2030             # instance is now in a cell and the delete needs to proceed
2031             # normally.
2032             return False
2033 
2034         # We need to detach from any volumes so they aren't orphaned.
2035         self._local_cleanup_bdm_volumes(
2036             build_req.block_device_mappings, instance, context)
2037 
2038         return True
2039 
2040     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
2041         if instance.disable_terminate:
2042             LOG.info('instance termination disabled', instance=instance)
2043             return
2044 
2045         cell = None
2046         # If there is an instance.host (or the instance is shelved-offloaded or
2047         # in error state), the instance has been scheduled and sent to a
2048         # cell/compute which means it was pulled from the cell db.
2049         # Normal delete should be attempted.
2050         may_have_ports_or_volumes = compute_utils.may_have_ports_or_volumes(
2051             instance)
2052         if not instance.host and not may_have_ports_or_volumes:
2053             try:
2054                 if self._delete_while_booting(context, instance):
2055                     return
2056                 # If instance.host was not set it's possible that the Instance
2057                 # object here was pulled from a BuildRequest object and is not
2058                 # fully populated. Notably it will be missing an 'id' field
2059                 # which will prevent instance.destroy from functioning
2060                 # properly. A lookup is attempted which will either return a
2061                 # full Instance or None if not found. If not found then it's
2062                 # acceptable to skip the rest of the delete processing.
2063                 cell, instance = self._lookup_instance(context, instance.uuid)
2064                 if cell and instance:
2065                     try:
2066                         # Now destroy the instance from the cell it lives in.
2067                         with compute_utils.notify_about_instance_delete(
2068                                 self.notifier, context, instance):
2069                             instance.destroy()
2070                     except exception.InstanceNotFound:
2071                         pass
2072                     # The instance was deleted or is already gone.
2073                     return
2074                 if not instance:
2075                     # Instance is already deleted.
2076                     return
2077             except exception.ObjectActionError:
2078                 # NOTE(melwitt): This means the instance.host changed
2079                 # under us indicating the instance became scheduled
2080                 # during the destroy(). Refresh the instance from the DB and
2081                 # continue on with the delete logic for a scheduled instance.
2082                 # NOTE(danms): If instance.host is set, we should be able to
2083                 # do the following lookup. If not, there's not much we can
2084                 # do to recover.
2085                 cell, instance = self._lookup_instance(context, instance.uuid)
2086                 if not instance:
2087                     # Instance is already deleted
2088                     return
2089 
2090         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2091                 context, instance.uuid)
2092 
2093         # At these states an instance has a snapshot associate.
2094         if instance.vm_state in (vm_states.SHELVED,
2095                                  vm_states.SHELVED_OFFLOADED):
2096             snapshot_id = instance.system_metadata.get('shelved_image_id')
2097             LOG.info("Working on deleting snapshot %s "
2098                      "from shelved instance...",
2099                      snapshot_id, instance=instance)
2100             try:
2101                 self.image_api.delete(context, snapshot_id)
2102             except (exception.ImageNotFound,
2103                     exception.ImageNotAuthorized) as exc:
2104                 LOG.warning("Failed to delete snapshot "
2105                             "from shelved instance (%s).",
2106                             exc.format_message(), instance=instance)
2107             except Exception:
2108                 LOG.exception("Something wrong happened when trying to "
2109                               "delete snapshot from shelved instance.",
2110                               instance=instance)
2111 
2112         original_task_state = instance.task_state
2113         try:
2114             # NOTE(maoy): no expected_task_state needs to be set
2115             instance.update(instance_attrs)
2116             instance.progress = 0
2117             instance.save()
2118 
2119             if not instance.host and not may_have_ports_or_volumes:
2120                 try:
2121                     with compute_utils.notify_about_instance_delete(
2122                             self.notifier, context, instance,
2123                             delete_type
2124                             if delete_type != 'soft_delete'
2125                             else 'delete'):
2126                         instance.destroy()
2127                     LOG.info('Instance deleted and does not have host '
2128                              'field, its vm_state is %(state)s.',
2129                              {'state': instance.vm_state},
2130                               instance=instance)
2131                     return
2132                 except exception.ObjectActionError as ex:
2133                     # The instance's host likely changed under us as
2134                     # this instance could be building and has since been
2135                     # scheduled. Continue with attempts to delete it.
2136                     LOG.debug('Refreshing instance because: %s', ex,
2137                               instance=instance)
2138                     instance.refresh()
2139 
2140             if instance.vm_state == vm_states.RESIZED:
2141                 self._confirm_resize_on_deleting(context, instance)
2142                 # NOTE(neha_alhat): After confirm resize vm_state will become
2143                 # 'active' and task_state will be set to 'None'. But for soft
2144                 # deleting a vm, the _do_soft_delete callback requires
2145                 # task_state in 'SOFT_DELETING' status. So, we need to set
2146                 # task_state as 'SOFT_DELETING' again for soft_delete case.
2147                 # After confirm resize and before saving the task_state to
2148                 # "SOFT_DELETING", during the short window, user can submit
2149                 # soft delete vm request again and system will accept and
2150                 # process it without any errors.
2151                 if delete_type == 'soft_delete':
2152                     instance.task_state = instance_attrs['task_state']
2153                     instance.save()
2154 
2155             is_local_delete = True
2156             try:
2157                 # instance.host must be set in order to look up the service.
2158                 if instance.host is not None:
2159                     service = objects.Service.get_by_compute_host(
2160                         context.elevated(), instance.host)
2161                     is_local_delete = not self.servicegroup_api.service_is_up(
2162                         service)
2163                 if not is_local_delete:
2164                     if original_task_state in (task_states.DELETING,
2165                                                   task_states.SOFT_DELETING):
2166                         LOG.info('Instance is already in deleting state, '
2167                                  'ignoring this request',
2168                                  instance=instance)
2169                         return
2170                     self._record_action_start(context, instance,
2171                                               instance_actions.DELETE)
2172 
2173                     cb(context, instance, bdms)
2174             except exception.ComputeHostNotFound:
2175                 LOG.debug('Compute host %s not found during service up check, '
2176                           'going to local delete instance', instance.host,
2177                           instance=instance)
2178 
2179             if is_local_delete:
2180                 # If instance is in shelved_offloaded state or compute node
2181                 # isn't up, delete instance from db and clean bdms info and
2182                 # network info
2183                 if cell is None:
2184                     # NOTE(danms): If we didn't get our cell from one of the
2185                     # paths above, look it up now.
2186                     try:
2187                         im = objects.InstanceMapping.get_by_instance_uuid(
2188                             context, instance.uuid)
2189                         cell = im.cell_mapping
2190                     except exception.InstanceMappingNotFound:
2191                         LOG.warning('During local delete, failed to find '
2192                                     'instance mapping', instance=instance)
2193                         return
2194 
2195                 LOG.debug('Doing local delete in cell %s', cell.identity,
2196                           instance=instance)
2197                 with nova_context.target_cell(context, cell) as cctxt:
2198                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
2199 
2200         except exception.InstanceNotFound:
2201             # NOTE(comstud): Race condition. Instance already gone.
2202             pass
2203 
2204     def _confirm_resize_on_deleting(self, context, instance):
2205         # If in the middle of a resize, use confirm_resize to
2206         # ensure the original instance is cleaned up too along
2207         # with its allocations (and migration-based allocations)
2208         # in placement.
2209         migration = None
2210         for status in ('finished', 'confirming'):
2211             try:
2212                 migration = objects.Migration.get_by_instance_and_status(
2213                         context.elevated(), instance.uuid, status)
2214                 LOG.info('Found an unconfirmed migration during delete, '
2215                          'id: %(id)s, status: %(status)s',
2216                          {'id': migration.id,
2217                           'status': migration.status},
2218                          instance=instance)
2219                 break
2220             except exception.MigrationNotFoundByStatus:
2221                 pass
2222 
2223         if not migration:
2224             LOG.info('Instance may have been confirmed during delete',
2225                      instance=instance)
2226             return
2227 
2228         src_host = migration.source_compute
2229 
2230         self._record_action_start(context, instance,
2231                                   instance_actions.CONFIRM_RESIZE)
2232 
2233         self.compute_rpcapi.confirm_resize(context,
2234                 instance, migration, src_host, cast=False)
2235 
2236     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
2237         """The method deletes the bdm records and, if a bdm is a volume, call
2238         the terminate connection and the detach volume via the Volume API.
2239         """
2240         elevated = context.elevated()
2241         for bdm in bdms:
2242             if bdm.is_volume:
2243                 try:
2244                     if bdm.attachment_id:
2245                         self.volume_api.attachment_delete(context,
2246                                                           bdm.attachment_id)
2247                     else:
2248                         connector = compute_utils.get_stashed_volume_connector(
2249                             bdm, instance)
2250                         if connector:
2251                             self.volume_api.terminate_connection(context,
2252                                                                  bdm.volume_id,
2253                                                                  connector)
2254                         else:
2255                             LOG.debug('Unable to find connector for volume %s,'
2256                                       ' not attempting terminate_connection.',
2257                                       bdm.volume_id, instance=instance)
2258                         # Attempt to detach the volume. If there was no
2259                         # connection made in the first place this is just
2260                         # cleaning up the volume state in the Cinder DB.
2261                         self.volume_api.detach(elevated, bdm.volume_id,
2262                                                instance.uuid)
2263 
2264                     if bdm.delete_on_termination:
2265                         self.volume_api.delete(context, bdm.volume_id)
2266                 except Exception as exc:
2267                     LOG.warning("Ignoring volume cleanup failure due to %s",
2268                                 exc, instance=instance)
2269             # If we're cleaning up volumes from an instance that wasn't yet
2270             # created in a cell, i.e. the user deleted the server while
2271             # the BuildRequest still existed, then the BDM doesn't actually
2272             # exist in the DB to destroy it.
2273             if 'id' in bdm:
2274                 bdm.destroy()
2275 
2276     @property
2277     def placementclient(self):
2278         if self._placementclient is None:
2279             self._placementclient = report.SchedulerReportClient()
2280         return self._placementclient
2281 
2282     def _local_delete(self, context, instance, bdms, delete_type, cb):
2283         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2284             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2285                      " the instance's info from database.",
2286                      instance=instance)
2287         else:
2288             LOG.warning("instance's host %s is down, deleting from "
2289                         "database", instance.host, instance=instance)
2290         with compute_utils.notify_about_instance_delete(
2291                 self.notifier, context, instance,
2292                 delete_type if delete_type != 'soft_delete' else 'delete'):
2293 
2294             elevated = context.elevated()
2295             # NOTE(liusheng): In nova-network multi_host scenario,deleting
2296             # network info of the instance may need instance['host'] as
2297             # destination host of RPC call. If instance in
2298             # SHELVED_OFFLOADED state, instance['host'] is None, here, use
2299             # shelved_host as host to deallocate network info and reset
2300             # instance['host'] after that. Here we shouldn't use
2301             # instance.save(), because this will mislead user who may think
2302             # the instance's host has been changed, and actually, the
2303             # instance.host is always None.
2304             orig_host = instance.host
2305             try:
2306                 if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2307                     sysmeta = getattr(instance,
2308                                       obj_base.get_attrname(
2309                                           'system_metadata'))
2310                     instance.host = sysmeta.get('shelved_host')
2311                 self.network_api.deallocate_for_instance(elevated,
2312                                                          instance)
2313             finally:
2314                 instance.host = orig_host
2315 
2316             # cleanup volumes
2317             self._local_cleanup_bdm_volumes(bdms, instance, context)
2318             # Cleanup allocations in Placement since we can't do it from the
2319             # compute service.
2320             self.placementclient.delete_allocation_for_instance(
2321                 context, instance.uuid, force=True)
2322             cb(context, instance, bdms, local=True)
2323             instance.destroy()
2324 
2325     @staticmethod
2326     def _update_queued_for_deletion(context, instance, qfd):
2327         # NOTE(tssurya): We query the instance_mapping record of this instance
2328         # and update the queued_for_delete flag to True (or False according to
2329         # the state of the instance). This just means that the instance is
2330         # queued for deletion (or is no longer queued for deletion). It does
2331         # not guarantee its successful deletion (or restoration). Hence the
2332         # value could be stale which is fine, considering its use is only
2333         # during down cell (desperate) situation.
2334         im = objects.InstanceMapping.get_by_instance_uuid(context,
2335                                                           instance.uuid)
2336         im.queued_for_delete = qfd
2337         im.save()
2338 
2339     def _do_delete(self, context, instance, bdms, local=False):
2340         if local:
2341             instance.vm_state = vm_states.DELETED
2342             instance.task_state = None
2343             instance.terminated_at = timeutils.utcnow()
2344             instance.save()
2345         else:
2346             self.compute_rpcapi.terminate_instance(context, instance, bdms)
2347         self._update_queued_for_deletion(context, instance, True)
2348 
2349     def _do_soft_delete(self, context, instance, bdms, local=False):
2350         if local:
2351             instance.vm_state = vm_states.SOFT_DELETED
2352             instance.task_state = None
2353             instance.terminated_at = timeutils.utcnow()
2354             instance.save()
2355         else:
2356             self.compute_rpcapi.soft_delete_instance(context, instance)
2357         self._update_queued_for_deletion(context, instance, True)
2358 
2359     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2360     @check_instance_lock
2361     @check_instance_state(vm_state=None, task_state=None,
2362                           must_have_launched=True)
2363     def soft_delete(self, context, instance):
2364         """Terminate an instance."""
2365         LOG.debug('Going to try to soft delete instance',
2366                   instance=instance)
2367 
2368         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2369                      task_state=task_states.SOFT_DELETING,
2370                      deleted_at=timeutils.utcnow())
2371 
2372     def _delete_instance(self, context, instance):
2373         self._delete(context, instance, 'delete', self._do_delete,
2374                      task_state=task_states.DELETING)
2375 
2376     @check_instance_lock
2377     @check_instance_state(vm_state=None, task_state=None,
2378                           must_have_launched=False)
2379     def delete(self, context, instance):
2380         """Terminate an instance."""
2381         LOG.debug("Going to try to terminate instance", instance=instance)
2382         self._delete_instance(context, instance)
2383 
2384     @check_instance_lock
2385     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2386     def restore(self, context, instance):
2387         """Restore a previously deleted (but not reclaimed) instance."""
2388         # Check quotas
2389         flavor = instance.get_flavor()
2390         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2391         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2392                 project_id=project_id, user_id=user_id)
2393 
2394         self._record_action_start(context, instance, instance_actions.RESTORE)
2395 
2396         if instance.host:
2397             instance.task_state = task_states.RESTORING
2398             instance.deleted_at = None
2399             instance.save(expected_task_state=[None])
2400             # TODO(melwitt): We're not rechecking for strict quota here to
2401             # guard against going over quota during a race at this time because
2402             # the resource consumption for this operation is written to the
2403             # database by compute.
2404             self.compute_rpcapi.restore_instance(context, instance)
2405         else:
2406             instance.vm_state = vm_states.ACTIVE
2407             instance.task_state = None
2408             instance.deleted_at = None
2409             instance.save(expected_task_state=[None])
2410         self._update_queued_for_deletion(context, instance, False)
2411 
2412     @check_instance_lock
2413     @check_instance_state(task_state=None,
2414                           must_have_launched=False)
2415     def force_delete(self, context, instance):
2416         """Force delete an instance in any vm_state/task_state."""
2417         self._delete(context, instance, 'force_delete', self._do_delete,
2418                      task_state=task_states.DELETING)
2419 
2420     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2421         LOG.debug("Going to try to stop instance", instance=instance)
2422 
2423         instance.task_state = task_states.POWERING_OFF
2424         instance.progress = 0
2425         instance.save(expected_task_state=[None])
2426 
2427         self._record_action_start(context, instance, instance_actions.STOP)
2428 
2429         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2430                                           clean_shutdown=clean_shutdown)
2431 
2432     @check_instance_lock
2433     @check_instance_host
2434     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2435     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2436         """Stop an instance."""
2437         self.force_stop(context, instance, do_cast, clean_shutdown)
2438 
2439     @check_instance_lock
2440     @check_instance_host
2441     @check_instance_state(vm_state=[vm_states.STOPPED])
2442     def start(self, context, instance):
2443         """Start an instance."""
2444         LOG.debug("Going to try to start instance", instance=instance)
2445 
2446         instance.task_state = task_states.POWERING_ON
2447         instance.save(expected_task_state=[None])
2448 
2449         self._record_action_start(context, instance, instance_actions.START)
2450         self.compute_rpcapi.start_instance(context, instance)
2451 
2452     @check_instance_lock
2453     @check_instance_host
2454     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2455     def trigger_crash_dump(self, context, instance):
2456         """Trigger crash dump in an instance."""
2457         LOG.debug("Try to trigger crash dump", instance=instance)
2458 
2459         self._record_action_start(context, instance,
2460                                   instance_actions.TRIGGER_CRASH_DUMP)
2461 
2462         self.compute_rpcapi.trigger_crash_dump(context, instance)
2463 
2464     def _generate_minimal_construct_for_down_cells(self, context,
2465                                                    down_cell_uuids,
2466                                                    project, limit):
2467         """Generate a list of minimal instance constructs for a given list of
2468         cells that did not respond to a list operation. This will list
2469         every instance mapping in the affected cells and return a minimal
2470         objects.Instance for each (non-queued-for-delete) mapping.
2471 
2472         :param context: RequestContext
2473         :param down_cell_uuids: A list of cell UUIDs that did not respond
2474         :param project: A project ID to filter mappings, or None
2475         :param limit: A numeric limit on the number of results, or None
2476         :returns: An InstanceList() of partial Instance() objects
2477         """
2478         unavailable_servers = objects.InstanceList()
2479         for cell_uuid in down_cell_uuids:
2480             LOG.warning("Cell %s is not responding and hence only "
2481                         "partial results are available from this "
2482                         "cell if any.", cell_uuid)
2483             instance_mappings = (objects.InstanceMappingList.
2484                 get_not_deleted_by_cell_and_project(context, cell_uuid,
2485                                                     project, limit=limit))
2486             for im in instance_mappings:
2487                 unavailable_servers.objects.append(
2488                     objects.Instance(
2489                         context=context,
2490                         uuid=im.instance_uuid,
2491                         project_id=im.project_id,
2492                         created_at=im.created_at
2493                     )
2494                 )
2495             if limit is not None:
2496                 limit -= len(instance_mappings)
2497                 if limit <= 0:
2498                     break
2499         return unavailable_servers
2500 
2501     def _get_instance_map_or_none(self, context, instance_uuid):
2502         try:
2503             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2504                     context, instance_uuid)
2505         except exception.InstanceMappingNotFound:
2506             # InstanceMapping should always be found generally. This exception
2507             # may be raised if a deployment has partially migrated the nova-api
2508             # services.
2509             inst_map = None
2510         return inst_map
2511 
2512     @staticmethod
2513     def _save_user_id_in_instance_mapping(mapping, instance):
2514         # TODO(melwitt): We take the opportunity to migrate user_id on the
2515         # instance mapping if it's not yet been migrated. This can be removed
2516         # in a future release, when all migrations are complete.
2517         # If the instance came from a RequestSpec because of a down cell, its
2518         # user_id could be None and the InstanceMapping.user_id field is
2519         # non-nullable. Avoid trying to set/save the user_id in that case.
2520         if 'user_id' not in mapping and instance.user_id is not None:
2521             mapping.user_id = instance.user_id
2522             mapping.save()
2523 
2524     def _get_instance_from_cell(self, context, im, expected_attrs,
2525                                 cell_down_support):
2526         # NOTE(danms): Even though we're going to scatter/gather to the
2527         # right cell, other code depends on this being force targeted when
2528         # the get call returns.
2529         nova_context.set_target_cell(context, im.cell_mapping)
2530 
2531         uuid = im.instance_uuid
2532         result = nova_context.scatter_gather_single_cell(context,
2533             im.cell_mapping, objects.Instance.get_by_uuid, uuid,
2534             expected_attrs=expected_attrs)
2535         cell_uuid = im.cell_mapping.uuid
2536         if not nova_context.is_cell_failure_sentinel(result[cell_uuid]):
2537             inst = result[cell_uuid]
2538             self._save_user_id_in_instance_mapping(im, inst)
2539             return inst
2540         elif isinstance(result[cell_uuid], exception.InstanceNotFound):
2541             raise exception.InstanceNotFound(instance_id=uuid)
2542         elif cell_down_support:
2543             if im.queued_for_delete:
2544                 # should be treated like deleted instance.
2545                 raise exception.InstanceNotFound(instance_id=uuid)
2546 
2547             # instance in down cell, return a minimal construct
2548             LOG.warning("Cell %s is not responding and hence only "
2549                         "partial results are available from this "
2550                         "cell.", cell_uuid)
2551             try:
2552                 rs = objects.RequestSpec.get_by_instance_uuid(context,
2553                                                               uuid)
2554                 # For BFV case, we could have rs.image but rs.image.id might
2555                 # still not be set. So we check the existence of both image
2556                 # and its id.
2557                 image_ref = (rs.image.id if rs.image and
2558                              'id' in rs.image else None)
2559                 inst = objects.Instance(context=context, power_state=0,
2560                                         uuid=uuid,
2561                                         project_id=im.project_id,
2562                                         created_at=im.created_at,
2563                                         user_id=rs.user_id,
2564                                         flavor=rs.flavor,
2565                                         image_ref=image_ref,
2566                                         availability_zone=rs.availability_zone)
2567                 self._save_user_id_in_instance_mapping(im, inst)
2568                 return inst
2569             except exception.RequestSpecNotFound:
2570                 # could be that a deleted instance whose request
2571                 # spec has been archived is being queried.
2572                 raise exception.InstanceNotFound(instance_id=uuid)
2573         else:
2574             raise exception.NovaException(
2575                 _("Cell %s is not responding and hence instance "
2576                   "info is not available.") % cell_uuid)
2577 
2578     def _get_instance(self, context, instance_uuid, expected_attrs,
2579                       cell_down_support=False):
2580         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2581         if inst_map and (inst_map.cell_mapping is not None):
2582             instance = self._get_instance_from_cell(context, inst_map,
2583                 expected_attrs, cell_down_support)
2584         elif inst_map and (inst_map.cell_mapping is None):
2585             # This means the instance has not been scheduled and put in
2586             # a cell yet. For now it also may mean that the deployer
2587             # has not created their cell(s) yet.
2588             try:
2589                 build_req = objects.BuildRequest.get_by_instance_uuid(
2590                         context, instance_uuid)
2591                 instance = build_req.instance
2592             except exception.BuildRequestNotFound:
2593                 # Instance was mapped and the BuildRequest was deleted
2594                 # while fetching. Try again.
2595                 inst_map = self._get_instance_map_or_none(context,
2596                                                           instance_uuid)
2597                 if inst_map and (inst_map.cell_mapping is not None):
2598                     instance = self._get_instance_from_cell(context, inst_map,
2599                         expected_attrs, cell_down_support)
2600                 else:
2601                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2602         else:
2603             # If we got here, we don't have an instance mapping, but we aren't
2604             # sure why. The instance mapping might be missing because the
2605             # upgrade is incomplete (map_instances wasn't run). Or because the
2606             # instance was deleted and the DB was archived at which point the
2607             # mapping is deleted. The former case is bad, but because of the
2608             # latter case we can't really log any kind of warning/error here
2609             # since it might be normal.
2610             raise exception.InstanceNotFound(instance_id=instance_uuid)
2611 
2612         return instance
2613 
2614     def get(self, context, instance_id, expected_attrs=None,
2615             cell_down_support=False):
2616         """Get a single instance with the given instance_id.
2617 
2618         :param cell_down_support: True if the API (and caller) support
2619                                   returning a minimal instance
2620                                   construct if the relevant cell is
2621                                   down. If False, an error is raised
2622                                   since the instance cannot be retrieved
2623                                   due to the cell being down.
2624         """
2625         if not expected_attrs:
2626             expected_attrs = []
2627         expected_attrs.extend(['metadata', 'system_metadata',
2628                                'security_groups', 'info_cache'])
2629         # NOTE(ameade): we still need to support integer ids for ec2
2630         try:
2631             if uuidutils.is_uuid_like(instance_id):
2632                 LOG.debug("Fetching instance by UUID",
2633                            instance_uuid=instance_id)
2634 
2635                 instance = self._get_instance(context, instance_id,
2636                     expected_attrs, cell_down_support=cell_down_support)
2637             else:
2638                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2639                 raise exception.InstanceNotFound(instance_id=instance_id)
2640         except exception.InvalidID:
2641             LOG.debug("Invalid instance id %s", instance_id)
2642             raise exception.InstanceNotFound(instance_id=instance_id)
2643 
2644         return instance
2645 
2646     def get_all(self, context, search_opts=None, limit=None, marker=None,
2647                 expected_attrs=None, sort_keys=None, sort_dirs=None,
2648                 cell_down_support=False, all_tenants=False):
2649         """Get all instances filtered by one of the given parameters.
2650 
2651         If there is no filter and the context is an admin, it will retrieve
2652         all instances in the system.
2653 
2654         Deleted instances will be returned by default, unless there is a
2655         search option that says otherwise.
2656 
2657         The results will be sorted based on the list of sort keys in the
2658         'sort_keys' parameter (first value is primary sort key, second value is
2659         secondary sort ket, etc.). For each sort key, the associated sort
2660         direction is based on the list of sort directions in the 'sort_dirs'
2661         parameter.
2662 
2663         :param cell_down_support: True if the API (and caller) support
2664                                   returning a minimal instance
2665                                   construct if the relevant cell is
2666                                   down. If False, instances from
2667                                   unreachable cells will be omitted.
2668         :param all_tenants: True if the "all_tenants" filter was passed.
2669 
2670         """
2671         if search_opts is None:
2672             search_opts = {}
2673 
2674         LOG.debug("Searching by: %s", str(search_opts))
2675 
2676         # Fixups for the DB call
2677         filters = {}
2678 
2679         def _remap_flavor_filter(flavor_id):
2680             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2681             filters['instance_type_id'] = flavor.id
2682 
2683         def _remap_fixed_ip_filter(fixed_ip):
2684             # Turn fixed_ip into a regexp match. Since '.' matches
2685             # any character, we need to use regexp escaping for it.
2686             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2687 
2688         # search_option to filter_name mapping.
2689         filter_mapping = {
2690                 'image': 'image_ref',
2691                 'name': 'display_name',
2692                 'tenant_id': 'project_id',
2693                 'flavor': _remap_flavor_filter,
2694                 'fixed_ip': _remap_fixed_ip_filter}
2695 
2696         # copy from search_opts, doing various remappings as necessary
2697         for opt, value in search_opts.items():
2698             # Do remappings.
2699             # Values not in the filter_mapping table are copied as-is.
2700             # If remapping is None, option is not copied
2701             # If the remapping is a string, it is the filter_name to use
2702             try:
2703                 remap_object = filter_mapping[opt]
2704             except KeyError:
2705                 filters[opt] = value
2706             else:
2707                 # Remaps are strings to translate to, or functions to call
2708                 # to do the translating as defined by the table above.
2709                 if isinstance(remap_object, six.string_types):
2710                     filters[remap_object] = value
2711                 else:
2712                     try:
2713                         remap_object(value)
2714 
2715                     # We already know we can't match the filter, so
2716                     # return an empty list
2717                     except ValueError:
2718                         return objects.InstanceList()
2719 
2720         # IP address filtering cannot be applied at the DB layer, remove any DB
2721         # limit so that it can be applied after the IP filter.
2722         filter_ip = 'ip6' in filters or 'ip' in filters
2723         skip_build_request = False
2724         orig_limit = limit
2725         if filter_ip:
2726             # We cannot skip build requests if there is a marker since the
2727             # the marker could be a build request.
2728             skip_build_request = marker is None
2729             if self.network_api.has_substr_port_filtering_extension(context):
2730                 # We're going to filter by IP using Neutron so set filter_ip
2731                 # to False so we don't attempt post-DB query filtering in
2732                 # memory below.
2733                 filter_ip = False
2734                 instance_uuids = self._ip_filter_using_neutron(context,
2735                                                                filters)
2736                 if instance_uuids:
2737                     # Note that 'uuid' is not in the 2.1 GET /servers query
2738                     # parameter schema, however, we allow additionalProperties
2739                     # so someone could filter instances by uuid, which doesn't
2740                     # make a lot of sense but we have to account for it.
2741                     if 'uuid' in filters and filters['uuid']:
2742                         filter_uuids = filters['uuid']
2743                         if isinstance(filter_uuids, list):
2744                             instance_uuids.extend(filter_uuids)
2745                         else:
2746                             # Assume a string. If it's a dict or tuple or
2747                             # something, well...that's too bad. This is why
2748                             # we have query parameter schema definitions.
2749                             if filter_uuids not in instance_uuids:
2750                                 instance_uuids.append(filter_uuids)
2751                     filters['uuid'] = instance_uuids
2752                 else:
2753                     # No matches on the ip filter(s), return an empty list.
2754                     return objects.InstanceList()
2755             elif limit:
2756                 LOG.debug('Removing limit for DB query due to IP filter')
2757                 limit = None
2758 
2759         # Skip get BuildRequest if filtering by IP address, as building
2760         # instances will not have IP addresses.
2761         if skip_build_request:
2762             build_requests = objects.BuildRequestList()
2763         else:
2764             # The ordering of instances will be
2765             # [sorted instances with no host] + [sorted instances with host].
2766             # This means BuildRequest and cell0 instances first, then cell
2767             # instances
2768             try:
2769                 build_requests = objects.BuildRequestList.get_by_filters(
2770                     context, filters, limit=limit, marker=marker,
2771                     sort_keys=sort_keys, sort_dirs=sort_dirs)
2772                 # If we found the marker in we need to set it to None
2773                 # so we don't expect to find it in the cells below.
2774                 marker = None
2775             except exception.MarkerNotFound:
2776                 # If we didn't find the marker in the build requests then keep
2777                 # looking for it in the cells.
2778                 build_requests = objects.BuildRequestList()
2779 
2780         build_req_instances = objects.InstanceList(
2781             objects=[build_req.instance for build_req in build_requests])
2782         # Only subtract from limit if it is not None
2783         limit = (limit - len(build_req_instances)) if limit else limit
2784 
2785         # We could arguably avoid joining on security_groups if we're using
2786         # neutron (which is the default) but if you're using neutron then the
2787         # security_group_instance_association table should be empty anyway
2788         # and the DB should optimize out that join, making it insignificant.
2789         fields = ['metadata', 'info_cache', 'security_groups']
2790         if expected_attrs:
2791             fields.extend(expected_attrs)
2792 
2793         insts, down_cell_uuids = instance_list.get_instance_objects_sorted(
2794             context, filters, limit, marker, fields, sort_keys, sort_dirs,
2795             cell_down_support=cell_down_support)
2796 
2797         def _get_unique_filter_method():
2798             seen_uuids = set()
2799 
2800             def _filter(instance):
2801                 # During a cross-cell move operation we could have the instance
2802                 # in more than one cell database so we not only have to filter
2803                 # duplicates but we want to make sure we only return the
2804                 # "current" one which should also be the one that the instance
2805                 # mapping points to, but we don't want to do that expensive
2806                 # lookup here. The DB API will filter out hidden instances by
2807                 # default but there is a small window where two copies of an
2808                 # instance could be hidden=False in separate cell DBs.
2809                 # NOTE(mriedem): We could make this better in the case that we
2810                 # have duplicate instances that are both hidden=False by
2811                 # showing the one with the newer updated_at value, but that
2812                 # could be tricky if the user is filtering on
2813                 # changes-since/before or updated_at, or sorting on updated_at,
2814                 # but technically that was already potentially broken with this
2815                 # _filter method if we return an older BuildRequest.instance,
2816                 # and given the window should be very small where we have
2817                 # duplicates, it's probably not worth the complexity.
2818                 if instance.uuid in seen_uuids:
2819                     return False
2820                 seen_uuids.add(instance.uuid)
2821                 return True
2822 
2823             return _filter
2824 
2825         filter_method = _get_unique_filter_method()
2826         # Only subtract from limit if it is not None
2827         limit = (limit - len(insts)) if limit else limit
2828         # TODO(alaski): Clean up the objects concatenation when List objects
2829         # support it natively.
2830         instances = objects.InstanceList(
2831             objects=list(filter(filter_method,
2832                            build_req_instances.objects +
2833                            insts.objects)))
2834 
2835         if filter_ip:
2836             instances = self._ip_filter(instances, filters, orig_limit)
2837 
2838         if cell_down_support:
2839             # API and client want minimal construct instances for any cells
2840             # that didn't return, so generate and prefix those to the actual
2841             # results.
2842             project = search_opts.get('project_id', context.project_id)
2843             if all_tenants:
2844                 # NOTE(tssurya): The only scenario where project has to be None
2845                 # is when using "all_tenants" in which case we do not want
2846                 # the query to be restricted based on the project_id.
2847                 project = None
2848             limit = (orig_limit - len(instances)) if limit else limit
2849             return (self._generate_minimal_construct_for_down_cells(context,
2850                 down_cell_uuids, project, limit) + instances)
2851 
2852         return instances
2853 
2854     @staticmethod
2855     def _ip_filter(inst_models, filters, limit):
2856         ipv4_f = re.compile(str(filters.get('ip')))
2857         ipv6_f = re.compile(str(filters.get('ip6')))
2858 
2859         def _match_instance(instance):
2860             nw_info = instance.get_network_info()
2861             for vif in nw_info:
2862                 for fixed_ip in vif.fixed_ips():
2863                     address = fixed_ip.get('address')
2864                     if not address:
2865                         continue
2866                     version = fixed_ip.get('version')
2867                     if ((version == 4 and ipv4_f.match(address)) or
2868                         (version == 6 and ipv6_f.match(address))):
2869                         return True
2870             return False
2871 
2872         result_objs = []
2873         for instance in inst_models:
2874             if _match_instance(instance):
2875                 result_objs.append(instance)
2876                 if limit and len(result_objs) == limit:
2877                     break
2878         return objects.InstanceList(objects=result_objs)
2879 
2880     def _ip_filter_using_neutron(self, context, filters):
2881         ip4_address = filters.get('ip')
2882         ip6_address = filters.get('ip6')
2883         addresses = [ip4_address, ip6_address]
2884         uuids = []
2885         for address in addresses:
2886             if address:
2887                 try:
2888                     ports = self.network_api.list_ports(
2889                         context, fixed_ips='ip_address_substr=' + address,
2890                         fields=['device_id'])['ports']
2891                     for port in ports:
2892                         uuids.append(port['device_id'])
2893                 except Exception as e:
2894                     LOG.error('An error occurred while listing ports '
2895                               'with an ip_address filter value of "%s". '
2896                               'Error: %s',
2897                               address, six.text_type(e))
2898         return uuids
2899 
2900     def update_instance(self, context, instance, updates):
2901         """Updates a single Instance object with some updates dict.
2902 
2903         Returns the updated instance.
2904         """
2905 
2906         # NOTE(sbauza): Given we only persist the Instance object after we
2907         # create the BuildRequest, we are sure that if the Instance object
2908         # has an ID field set, then it was persisted in the right Cell DB.
2909         if instance.obj_attr_is_set('id'):
2910             instance.update(updates)
2911             instance.save()
2912         else:
2913             # Instance is not yet mapped to a cell, so we need to update
2914             # BuildRequest instead
2915             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2916             # could be deleted because of either a concurrent instance delete
2917             # or because the scheduler just returned a destination right
2918             # after we called the instance in the API.
2919             try:
2920                 build_req = objects.BuildRequest.get_by_instance_uuid(
2921                     context, instance.uuid)
2922                 instance = build_req.instance
2923                 instance.update(updates)
2924                 # FIXME(sbauza): Here we are updating the current
2925                 # thread-related BuildRequest object. Given that another worker
2926                 # could have looking up at that BuildRequest in the API, it
2927                 # means that it could pass it down to the conductor without
2928                 # making sure that it's not updated, we could have some race
2929                 # condition where it would missing the updated fields, but
2930                 # that's something we could discuss once the instance record
2931                 # is persisted by the conductor.
2932                 build_req.save()
2933             except exception.BuildRequestNotFound:
2934                 # Instance was mapped and the BuildRequest was deleted
2935                 # while fetching (and possibly the instance could have been
2936                 # deleted as well). We need to lookup again the Instance object
2937                 # in order to correctly update it.
2938                 # TODO(sbauza): Figure out a good way to know the expected
2939                 # attributes by checking which fields are set or not.
2940                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2941                                   'tags', 'metadata', 'system_metadata',
2942                                   'security_groups', 'info_cache']
2943                 inst_map = self._get_instance_map_or_none(context,
2944                                                           instance.uuid)
2945                 if inst_map and (inst_map.cell_mapping is not None):
2946                     with nova_context.target_cell(
2947                             context,
2948                             inst_map.cell_mapping) as cctxt:
2949                         instance = objects.Instance.get_by_uuid(
2950                             cctxt, instance.uuid,
2951                             expected_attrs=expected_attrs)
2952                         instance.update(updates)
2953                         instance.save()
2954                 else:
2955                     # Conductor doesn't delete the BuildRequest until after the
2956                     # InstanceMapping record is created, so if we didn't get
2957                     # that and the BuildRequest doesn't exist, then the
2958                     # instance is already gone and we need to just error out.
2959                     raise exception.InstanceNotFound(instance_id=instance.uuid)
2960         return instance
2961 
2962     # NOTE(melwitt): We don't check instance lock for backup because lock is
2963     #                intended to prevent accidental change/delete of instances
2964     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2965                                     vm_states.PAUSED, vm_states.SUSPENDED])
2966     def backup(self, context, instance, name, backup_type, rotation,
2967                extra_properties=None):
2968         """Backup the given instance
2969 
2970         :param instance: nova.objects.instance.Instance object
2971         :param name: name of the backup
2972         :param backup_type: 'daily' or 'weekly'
2973         :param rotation: int representing how many backups to keep around;
2974             None if rotation shouldn't be used (as in the case of snapshots)
2975         :param extra_properties: dict of extra image properties to include
2976                                  when creating the image.
2977         :returns: A dict containing image metadata
2978         """
2979         props_copy = dict(extra_properties, backup_type=backup_type)
2980 
2981         if compute_utils.is_volume_backed_instance(context, instance):
2982             LOG.info("It's not supported to backup volume backed "
2983                      "instance.", instance=instance)
2984             raise exception.InvalidRequest(
2985                 _('Backup is not supported for volume-backed instances.'))
2986         else:
2987             image_meta = compute_utils.create_image(
2988                 context, instance, name, 'backup', self.image_api,
2989                 extra_properties=props_copy)
2990 
2991         instance.task_state = task_states.IMAGE_BACKUP
2992         instance.save(expected_task_state=[None])
2993 
2994         self._record_action_start(context, instance,
2995                                   instance_actions.BACKUP)
2996 
2997         self.compute_rpcapi.backup_instance(context, instance,
2998                                             image_meta['id'],
2999                                             backup_type,
3000                                             rotation)
3001         return image_meta
3002 
3003     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3004     #                intended to prevent accidental change/delete of instances
3005     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3006                                     vm_states.PAUSED, vm_states.SUSPENDED])
3007     def snapshot(self, context, instance, name, extra_properties=None):
3008         """Snapshot the given instance.
3009 
3010         :param instance: nova.objects.instance.Instance object
3011         :param name: name of the snapshot
3012         :param extra_properties: dict of extra image properties to include
3013                                  when creating the image.
3014         :returns: A dict containing image metadata
3015         """
3016         image_meta = compute_utils.create_image(
3017             context, instance, name, 'snapshot', self.image_api,
3018             extra_properties=extra_properties)
3019 
3020         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
3021         try:
3022             instance.save(expected_task_state=[None])
3023         except (exception.InstanceNotFound,
3024                 exception.UnexpectedDeletingTaskStateError) as ex:
3025             # Changing the instance task state to use in raising the
3026             # InstanceInvalidException below
3027             LOG.debug('Instance disappeared during snapshot.',
3028                       instance=instance)
3029             try:
3030                 image_id = image_meta['id']
3031                 self.image_api.delete(context, image_id)
3032                 LOG.info('Image %s deleted because instance '
3033                          'deleted before snapshot started.',
3034                          image_id, instance=instance)
3035             except exception.ImageNotFound:
3036                 pass
3037             except Exception as exc:
3038                 LOG.warning("Error while trying to clean up image %(img_id)s: "
3039                             "%(error_msg)s",
3040                             {"img_id": image_meta['id'],
3041                              "error_msg": six.text_type(exc)})
3042             attr = 'task_state'
3043             state = task_states.DELETING
3044             if type(ex) == exception.InstanceNotFound:
3045                 attr = 'vm_state'
3046                 state = vm_states.DELETED
3047             raise exception.InstanceInvalidState(attr=attr,
3048                                            instance_uuid=instance.uuid,
3049                                            state=state,
3050                                            method='snapshot')
3051 
3052         self._record_action_start(context, instance,
3053                                   instance_actions.CREATE_IMAGE)
3054 
3055         self.compute_rpcapi.snapshot_instance(context, instance,
3056                                               image_meta['id'])
3057 
3058         return image_meta
3059 
3060     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3061     #                intended to prevent accidental change/delete of instances
3062     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3063                                     vm_states.SUSPENDED])
3064     def snapshot_volume_backed(self, context, instance, name,
3065                                extra_properties=None):
3066         """Snapshot the given volume-backed instance.
3067 
3068         :param instance: nova.objects.instance.Instance object
3069         :param name: name of the backup or snapshot
3070         :param extra_properties: dict of extra image properties to include
3071 
3072         :returns: the new image metadata
3073         """
3074         image_meta = compute_utils.initialize_instance_snapshot_metadata(
3075             context, instance, name, extra_properties)
3076         # the new image is simply a bucket of properties (particularly the
3077         # block device mapping, kernel and ramdisk IDs) with no image data,
3078         # hence the zero size
3079         image_meta['size'] = 0
3080         for attr in ('container_format', 'disk_format'):
3081             image_meta.pop(attr, None)
3082         properties = image_meta['properties']
3083         # clean properties before filling
3084         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
3085             properties.pop(key, None)
3086         if instance.root_device_name:
3087             properties['root_device_name'] = instance.root_device_name
3088 
3089         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3090                 context, instance.uuid)
3091 
3092         mapping = []  # list of BDM dicts that can go into the image properties
3093         # Do some up-front filtering of the list of BDMs from
3094         # which we are going to create snapshots.
3095         volume_bdms = []
3096         for bdm in bdms:
3097             if bdm.no_device:
3098                 continue
3099             if bdm.is_volume:
3100                 # These will be handled below.
3101                 volume_bdms.append(bdm)
3102             else:
3103                 mapping.append(bdm.get_image_mapping())
3104 
3105         # Check limits in Cinder before creating snapshots to avoid going over
3106         # quota in the middle of a list of volumes. This is a best-effort check
3107         # but concurrently running snapshot requests from the same project
3108         # could still fail to create volume snapshots if they go over limit.
3109         if volume_bdms:
3110             limits = self.volume_api.get_absolute_limits(context)
3111             total_snapshots_used = limits['totalSnapshotsUsed']
3112             max_snapshots = limits['maxTotalSnapshots']
3113             # -1 means there is unlimited quota for snapshots
3114             if (max_snapshots > -1 and
3115                     len(volume_bdms) + total_snapshots_used > max_snapshots):
3116                 LOG.debug('Unable to create volume snapshots for instance. '
3117                           'Currently has %s snapshots, requesting %s new '
3118                           'snapshots, with a limit of %s.',
3119                           total_snapshots_used, len(volume_bdms),
3120                           max_snapshots, instance=instance)
3121                 raise exception.OverQuota(overs='snapshots')
3122 
3123         quiesced = False
3124         if instance.vm_state == vm_states.ACTIVE:
3125             try:
3126                 LOG.info("Attempting to quiesce instance before volume "
3127                          "snapshot.", instance=instance)
3128                 self.compute_rpcapi.quiesce_instance(context, instance)
3129                 quiesced = True
3130             except (exception.InstanceQuiesceNotSupported,
3131                     exception.QemuGuestAgentNotEnabled,
3132                     exception.NovaException, NotImplementedError) as err:
3133                 if strutils.bool_from_string(instance.system_metadata.get(
3134                         'image_os_require_quiesce')):
3135                     raise
3136 
3137                 if isinstance(err, exception.NovaException):
3138                     LOG.info('Skipping quiescing instance: %(reason)s.',
3139                              {'reason': err.format_message()},
3140                              instance=instance)
3141                 else:
3142                     LOG.info('Skipping quiescing instance because the '
3143                              'operation is not supported by the underlying '
3144                              'compute driver.', instance=instance)
3145             # NOTE(tasker): discovered that an uncaught exception could occur
3146             #               after the instance has been frozen. catch and thaw.
3147             except Exception as ex:
3148                 with excutils.save_and_reraise_exception():
3149                     LOG.error("An error occurred during quiesce of instance. "
3150                               "Unquiescing to ensure instance is thawed. "
3151                               "Error: %s", six.text_type(ex),
3152                               instance=instance)
3153                     self.compute_rpcapi.unquiesce_instance(context, instance,
3154                                                            mapping=None)
3155 
3156         @wrap_instance_event(prefix='api')
3157         def snapshot_instance(self, context, instance, bdms):
3158             try:
3159                 for bdm in volume_bdms:
3160                     # create snapshot based on volume_id
3161                     volume = self.volume_api.get(context, bdm.volume_id)
3162                     # NOTE(yamahata): Should we wait for snapshot creation?
3163                     #                 Linux LVM snapshot creation completes in
3164                     #                 short time, it doesn't matter for now.
3165                     name = _('snapshot for %s') % image_meta['name']
3166                     LOG.debug('Creating snapshot from volume %s.',
3167                               volume['id'], instance=instance)
3168                     snapshot = self.volume_api.create_snapshot_force(
3169                         context, volume['id'],
3170                         name, volume['display_description'])
3171                     mapping_dict = block_device.snapshot_from_bdm(
3172                         snapshot['id'], bdm)
3173                     mapping_dict = mapping_dict.get_image_mapping()
3174                     mapping.append(mapping_dict)
3175                 return mapping
3176             # NOTE(tasker): No error handling is done in the above for loop.
3177             # This means that if the snapshot fails and throws an exception
3178             # the traceback will skip right over the unquiesce needed below.
3179             # Here, catch any exception, unquiesce the instance, and raise the
3180             # error so that the calling function can do what it needs to in
3181             # order to properly treat a failed snap.
3182             except Exception:
3183                 with excutils.save_and_reraise_exception():
3184                     if quiesced:
3185                         LOG.info("Unquiescing instance after volume snapshot "
3186                                  "failure.", instance=instance)
3187                         self.compute_rpcapi.unquiesce_instance(
3188                             context, instance, mapping)
3189 
3190         self._record_action_start(context, instance,
3191                                   instance_actions.CREATE_IMAGE)
3192         mapping = snapshot_instance(self, context, instance, bdms)
3193 
3194         if quiesced:
3195             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
3196 
3197         if mapping:
3198             properties['block_device_mapping'] = mapping
3199             properties['bdm_v2'] = True
3200 
3201         return self.image_api.create(context, image_meta)
3202 
3203     @check_instance_lock
3204     def reboot(self, context, instance, reboot_type):
3205         """Reboot the given instance."""
3206         if reboot_type == 'SOFT':
3207             self._soft_reboot(context, instance)
3208         else:
3209             self._hard_reboot(context, instance)
3210 
3211     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
3212                           task_state=[None])
3213     def _soft_reboot(self, context, instance):
3214         expected_task_state = [None]
3215         instance.task_state = task_states.REBOOTING
3216         instance.save(expected_task_state=expected_task_state)
3217 
3218         self._record_action_start(context, instance, instance_actions.REBOOT)
3219 
3220         self.compute_rpcapi.reboot_instance(context, instance=instance,
3221                                             block_device_info=None,
3222                                             reboot_type='SOFT')
3223 
3224     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
3225                           task_state=task_states.ALLOW_REBOOT)
3226     def _hard_reboot(self, context, instance):
3227         instance.task_state = task_states.REBOOTING_HARD
3228         instance.save(expected_task_state=task_states.ALLOW_REBOOT)
3229 
3230         self._record_action_start(context, instance, instance_actions.REBOOT)
3231 
3232         self.compute_rpcapi.reboot_instance(context, instance=instance,
3233                                             block_device_info=None,
3234                                             reboot_type='HARD')
3235 
3236     # TODO(stephenfin): We should expand kwargs out to named args
3237     @check_instance_lock
3238     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3239                                     vm_states.ERROR])
3240     def rebuild(self, context, instance, image_href, admin_password,
3241                 files_to_inject=None, **kwargs):
3242         """Rebuild the given instance with the provided attributes."""
3243         files_to_inject = files_to_inject or []
3244         metadata = kwargs.get('metadata', {})
3245         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
3246         auto_disk_config = kwargs.get('auto_disk_config')
3247 
3248         if 'key_name' in kwargs:
3249             key_name = kwargs.pop('key_name')
3250             if key_name:
3251                 # NOTE(liuyulong): we are intentionally using the user_id from
3252                 # the request context rather than the instance.user_id because
3253                 # users own keys but instances are owned by projects, and
3254                 # another user in the same project can rebuild an instance
3255                 # even if they didn't create it.
3256                 key_pair = objects.KeyPair.get_by_name(context,
3257                                                        context.user_id,
3258                                                        key_name)
3259                 instance.key_name = key_pair.name
3260                 instance.key_data = key_pair.public_key
3261                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
3262             else:
3263                 instance.key_name = None
3264                 instance.key_data = None
3265                 instance.keypairs = objects.KeyPairList(objects=[])
3266 
3267         # Use trusted_certs value from kwargs to create TrustedCerts object
3268         trusted_certs = None
3269         if 'trusted_certs' in kwargs:
3270             # Note that the user can set, change, or unset / reset trusted
3271             # certs. If they are explicitly specifying
3272             # trusted_image_certificates=None, that means we'll either unset
3273             # them on the instance *or* reset to use the defaults (if defaults
3274             # are configured).
3275             trusted_certs = kwargs.pop('trusted_certs')
3276             instance.trusted_certs = self._retrieve_trusted_certs_object(
3277                 context, trusted_certs, rebuild=True)
3278 
3279         image_id, image = self._get_image(context, image_href)
3280         self._check_auto_disk_config(image=image, **kwargs)
3281 
3282         flavor = instance.get_flavor()
3283         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3284             context, instance.uuid)
3285         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
3286 
3287         # Check to see if the image is changing and we have a volume-backed
3288         # server. The compute doesn't support changing the image in the
3289         # root disk of a volume-backed server, so we need to just fail fast.
3290         is_volume_backed = compute_utils.is_volume_backed_instance(
3291             context, instance, bdms)
3292         if is_volume_backed:
3293             if trusted_certs:
3294                 # The only way we can get here is if the user tried to set
3295                 # trusted certs or specified trusted_image_certificates=None
3296                 # and default_trusted_certificate_ids is configured.
3297                 msg = _("Image certificate validation is not supported "
3298                         "for volume-backed servers.")
3299                 raise exception.CertificateValidationFailed(message=msg)
3300 
3301             # For boot from volume, instance.image_ref is empty, so we need to
3302             # query the image from the volume.
3303             if root_bdm is None:
3304                 # This shouldn't happen and is an error, we need to fail. This
3305                 # is not the users fault, it's an internal error. Without a
3306                 # root BDM we have no way of knowing the backing volume (or
3307                 # image in that volume) for this instance.
3308                 raise exception.NovaException(
3309                     _('Unable to find root block device mapping for '
3310                       'volume-backed instance.'))
3311 
3312             volume = self.volume_api.get(context, root_bdm.volume_id)
3313             volume_image_metadata = volume.get('volume_image_metadata', {})
3314             orig_image_ref = volume_image_metadata.get('image_id')
3315 
3316             if orig_image_ref != image_href:
3317                 # Leave a breadcrumb.
3318                 LOG.debug('Requested to rebuild instance with a new image %s '
3319                           'for a volume-backed server with image %s in its '
3320                           'root volume which is not supported.', image_href,
3321                           orig_image_ref, instance=instance)
3322                 msg = _('Unable to rebuild with a different image for a '
3323                         'volume-backed server.')
3324                 raise exception.ImageUnacceptable(
3325                     image_id=image_href, reason=msg)
3326         else:
3327             orig_image_ref = instance.image_ref
3328 
3329         request_spec = objects.RequestSpec.get_by_instance_uuid(
3330             context, instance.uuid)
3331 
3332         self._checks_for_create_and_rebuild(context, image_id, image,
3333                 flavor, metadata, files_to_inject, root_bdm)
3334 
3335         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
3336                 context, None, None, image)
3337 
3338         def _reset_image_metadata():
3339             """Remove old image properties that we're storing as instance
3340             system metadata.  These properties start with 'image_'.
3341             Then add the properties for the new image.
3342             """
3343             # FIXME(comstud): There's a race condition here in that if
3344             # the system_metadata for this instance is updated after
3345             # we do the previous save() and before we update.. those
3346             # other updates will be lost. Since this problem exists in
3347             # a lot of other places, I think it should be addressed in
3348             # a DB layer overhaul.
3349 
3350             orig_sys_metadata = dict(instance.system_metadata)
3351             # Remove the old keys
3352             for key in list(instance.system_metadata.keys()):
3353                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
3354                     del instance.system_metadata[key]
3355 
3356             # Add the new ones
3357             new_sys_metadata = utils.get_system_metadata_from_image(
3358                 image, flavor)
3359 
3360             instance.system_metadata.update(new_sys_metadata)
3361             instance.save()
3362             return orig_sys_metadata
3363 
3364         # Since image might have changed, we may have new values for
3365         # os_type, vm_mode, etc
3366         options_from_image = self._inherit_properties_from_image(
3367                 image, auto_disk_config)
3368         instance.update(options_from_image)
3369 
3370         instance.task_state = task_states.REBUILDING
3371         # An empty instance.image_ref is currently used as an indication
3372         # of BFV.  Preserve that over a rebuild to not break users.
3373         if not is_volume_backed:
3374             instance.image_ref = image_href
3375         instance.kernel_id = kernel_id or ""
3376         instance.ramdisk_id = ramdisk_id or ""
3377         instance.progress = 0
3378         instance.update(kwargs)
3379         instance.save(expected_task_state=[None])
3380 
3381         # On a rebuild, since we're potentially changing images, we need to
3382         # wipe out the old image properties that we're storing as instance
3383         # system metadata... and copy in the properties for the new image.
3384         orig_sys_metadata = _reset_image_metadata()
3385 
3386         self._record_action_start(context, instance, instance_actions.REBUILD)
3387 
3388         # NOTE(sbauza): The migration script we provided in Newton should make
3389         # sure that all our instances are currently migrated to have an
3390         # attached RequestSpec object but let's consider that the operator only
3391         # half migrated all their instances in the meantime.
3392         host = instance.host
3393         # If a new image is provided on rebuild, we will need to run
3394         # through the scheduler again, but we want the instance to be
3395         # rebuilt on the same host it's already on.
3396         if orig_image_ref != image_href:
3397             # We have to modify the request spec that goes to the scheduler
3398             # to contain the new image. We persist this since we've already
3399             # changed the instance.image_ref above so we're being
3400             # consistent.
3401             request_spec.image = objects.ImageMeta.from_dict(image)
3402             request_spec.save()
3403             if 'scheduler_hints' not in request_spec:
3404                 request_spec.scheduler_hints = {}
3405             # Nuke the id on this so we can't accidentally save
3406             # this hint hack later
3407             del request_spec.id
3408 
3409             # NOTE(danms): Passing host=None tells conductor to
3410             # call the scheduler. The _nova_check_type hint
3411             # requires that the scheduler returns only the same
3412             # host that we are currently on and only checks
3413             # rebuild-related filters.
3414             request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3415             request_spec.force_hosts = [instance.host]
3416             request_spec.force_nodes = [instance.node]
3417             host = None
3418 
3419         self.compute_task_api.rebuild_instance(context, instance=instance,
3420                 new_pass=admin_password, injected_files=files_to_inject,
3421                 image_ref=image_href, orig_image_ref=orig_image_ref,
3422                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3423                 preserve_ephemeral=preserve_ephemeral, host=host,
3424                 request_spec=request_spec)
3425 
3426     @staticmethod
3427     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3428         project_id, user_id = quotas_obj.ids_from_instance(context,
3429                                                            instance)
3430         # Deltas will be empty if the resize is not an upsize.
3431         deltas = compute_utils.upsize_quota_delta(new_flavor,
3432                                                   current_flavor)
3433         if deltas:
3434             try:
3435                 res_deltas = {'cores': deltas.get('cores', 0),
3436                               'ram': deltas.get('ram', 0)}
3437                 objects.Quotas.check_deltas(context, res_deltas,
3438                                             project_id, user_id=user_id,
3439                                             check_project_id=project_id,
3440                                             check_user_id=user_id)
3441             except exception.OverQuota as exc:
3442                 quotas = exc.kwargs['quotas']
3443                 overs = exc.kwargs['overs']
3444                 usages = exc.kwargs['usages']
3445                 headroom = compute_utils.get_headroom(quotas, usages,
3446                                                       deltas)
3447                 (overs, reqs, total_alloweds,
3448                  useds) = compute_utils.get_over_quota_detail(headroom,
3449                                                               overs,
3450                                                               quotas,
3451                                                               deltas)
3452                 LOG.info("%(overs)s quota exceeded for %(pid)s,"
3453                          " tried to resize instance.",
3454                          {'overs': overs, 'pid': context.project_id})
3455                 raise exception.TooManyInstances(overs=overs,
3456                                                  req=reqs,
3457                                                  used=useds,
3458                                                  allowed=total_alloweds)
3459 
3460     @check_instance_lock
3461     @check_instance_state(vm_state=[vm_states.RESIZED])
3462     def revert_resize(self, context, instance):
3463         """Reverts a resize or cold migration, deleting the 'new' instance in
3464         the process.
3465         """
3466         elevated = context.elevated()
3467         migration = objects.Migration.get_by_instance_and_status(
3468             elevated, instance.uuid, 'finished')
3469 
3470         # If this is a resize down, a revert might go over quota.
3471         self._check_quota_for_upsize(context, instance, instance.flavor,
3472                                      instance.old_flavor)
3473 
3474         # The AZ for the server may have changed when it was migrated so while
3475         # we are in the API and have access to the API DB, update the
3476         # instance.availability_zone before casting off to the compute service.
3477         # Note that we do this in the API to avoid an "up-call" from the
3478         # compute service to the API DB. This is not great in case something
3479         # fails during revert before the instance.host is updated to the
3480         # original source host, but it is good enough for now. Long-term we
3481         # could consider passing the AZ down to compute so it can set it when
3482         # the instance.host value is set in finish_revert_resize.
3483         instance.availability_zone = (
3484             availability_zones.get_host_availability_zone(
3485                 context, migration.source_compute))
3486 
3487         # Conductor updated the RequestSpec.flavor during the initial resize
3488         # operation to point at the new flavor, so we need to update the
3489         # RequestSpec to point back at the original flavor, otherwise
3490         # subsequent move operations through the scheduler will be using the
3491         # wrong flavor.
3492         reqspec = objects.RequestSpec.get_by_instance_uuid(
3493             context, instance.uuid)
3494         reqspec.flavor = instance.old_flavor
3495         reqspec.save()
3496 
3497         # NOTE(gibi): This is a performance optimization. If the network info
3498         # cache does not have ports with allocations in the binding profile
3499         # then we can skip reading port resource request from neutron below.
3500         # If a port has resource request then that would have already caused
3501         # that the finish_resize call put allocation in the binding profile
3502         # during the resize.
3503         if instance.get_network_info().has_port_with_allocation():
3504             # TODO(gibi): do not directly overwrite the
3505             # RequestSpec.requested_resources as others like cyborg might added
3506             # to things there already
3507             # NOTE(gibi): We need to collect the requested resource again as it
3508             # is intentionally not persisted in nova. Note that this needs to
3509             # be done here as the nova API code directly calls revert on the
3510             # dest compute service skipping the conductor.
3511             port_res_req = (
3512                 self.network_api.get_requested_resource_for_instance(
3513                     context, instance.uuid))
3514             reqspec.requested_resources = port_res_req
3515 
3516         instance.task_state = task_states.RESIZE_REVERTING
3517         instance.save(expected_task_state=[None])
3518 
3519         migration.status = 'reverting'
3520         migration.save()
3521 
3522         self._record_action_start(context, instance,
3523                                   instance_actions.REVERT_RESIZE)
3524 
3525         # TODO(melwitt): We're not rechecking for strict quota here to guard
3526         # against going over quota during a race at this time because the
3527         # resource consumption for this operation is written to the database
3528         # by compute.
3529         self.compute_rpcapi.revert_resize(context, instance,
3530                                           migration,
3531                                           migration.dest_compute,
3532                                           reqspec)
3533 
3534     @check_instance_lock
3535     @check_instance_state(vm_state=[vm_states.RESIZED])
3536     def confirm_resize(self, context, instance, migration=None):
3537         """Confirms a migration/resize and deletes the 'old' instance."""
3538         elevated = context.elevated()
3539         # NOTE(melwitt): We're not checking quota here because there isn't a
3540         # change in resource usage when confirming a resize. Resource
3541         # consumption for resizes are written to the database by compute, so
3542         # a confirm resize is just a clean up of the migration objects and a
3543         # state change in compute.
3544         if migration is None:
3545             migration = objects.Migration.get_by_instance_and_status(
3546                 elevated, instance.uuid, 'finished')
3547 
3548         migration.status = 'confirming'
3549         migration.save()
3550 
3551         self._record_action_start(context, instance,
3552                                   instance_actions.CONFIRM_RESIZE)
3553 
3554         self.compute_rpcapi.confirm_resize(context,
3555                                            instance,
3556                                            migration,
3557                                            migration.source_compute)
3558 
3559     # TODO(mriedem): It looks like for resize (not cold migrate) the only
3560     # possible kwarg here is auto_disk_config. Drop this dumb **kwargs and make
3561     # it explicitly an auto_disk_config param
3562     @check_instance_lock
3563     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3564     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3565                host_name=None, **extra_instance_updates):
3566         """Resize (ie, migrate) a running instance.
3567 
3568         If flavor_id is None, the process is considered a migration, keeping
3569         the original flavor_id. If flavor_id is not None, the instance should
3570         be migrated to a new host and resized to the new flavor_id.
3571         host_name is always None in the resize case.
3572         host_name can be set in the cold migration case only.
3573         """
3574         if host_name is not None:
3575             # Cannot migrate to the host where the instance exists
3576             # because it is useless.
3577             if host_name == instance.host:
3578                 raise exception.CannotMigrateToSameHost()
3579 
3580             # Check whether host exists or not.
3581             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3582                 context, host_name, use_slave=True)
3583 
3584         self._check_auto_disk_config(instance, **extra_instance_updates)
3585 
3586         current_instance_type = instance.get_flavor()
3587 
3588         # If flavor_id is not provided, only migrate the instance.
3589         volume_backed = None
3590         if not flavor_id:
3591             LOG.debug("flavor_id is None. Assuming migration.",
3592                       instance=instance)
3593             new_instance_type = current_instance_type
3594         else:
3595             new_instance_type = flavors.get_flavor_by_flavor_id(
3596                     flavor_id, read_deleted="no")
3597             # Check to see if we're resizing to a zero-disk flavor which is
3598             # only supported with volume-backed servers.
3599             if (new_instance_type.get('root_gb') == 0 and
3600                     current_instance_type.get('root_gb') != 0):
3601                 volume_backed = compute_utils.is_volume_backed_instance(
3602                         context, instance)
3603                 if not volume_backed:
3604                     reason = _('Resize to zero disk flavor is not allowed.')
3605                     raise exception.CannotResizeDisk(reason=reason)
3606 
3607         current_instance_type_name = current_instance_type['name']
3608         new_instance_type_name = new_instance_type['name']
3609         LOG.debug("Old instance type %(current_instance_type_name)s, "
3610                   "new instance type %(new_instance_type_name)s",
3611                   {'current_instance_type_name': current_instance_type_name,
3612                    'new_instance_type_name': new_instance_type_name},
3613                   instance=instance)
3614 
3615         same_instance_type = (current_instance_type['id'] ==
3616                               new_instance_type['id'])
3617 
3618         # NOTE(sirp): We don't want to force a customer to change their flavor
3619         # when Ops is migrating off of a failed host.
3620         if not same_instance_type and new_instance_type.get('disabled'):
3621             raise exception.FlavorNotFound(flavor_id=flavor_id)
3622 
3623         if same_instance_type and flavor_id:
3624             raise exception.CannotResizeToSameFlavor()
3625 
3626         # ensure there is sufficient headroom for upsizes
3627         if flavor_id:
3628             self._check_quota_for_upsize(context, instance,
3629                                          current_instance_type,
3630                                          new_instance_type)
3631 
3632         if not same_instance_type:
3633             image = utils.get_image_from_system_metadata(
3634                 instance.system_metadata)
3635             # Figure out if the instance is volume-backed but only if we didn't
3636             # already figure that out above (avoid the extra db hit).
3637             if volume_backed is None:
3638                 volume_backed = compute_utils.is_volume_backed_instance(
3639                     context, instance)
3640             # If the server is volume-backed, we still want to validate numa
3641             # and pci information in the new flavor, but we don't call
3642             # _validate_flavor_image_nostatus because how it handles checking
3643             # disk size validation was not intended for a volume-backed
3644             # resize case.
3645             if volume_backed:
3646                 self._validate_flavor_image_numa_pci(
3647                     image, new_instance_type, validate_pci=True)
3648             else:
3649                 self._validate_flavor_image_nostatus(
3650                     context, image, new_instance_type, root_bdm=None,
3651                     validate_pci=True)
3652 
3653         filter_properties = {'ignore_hosts': []}
3654 
3655         if not CONF.allow_resize_to_same_host:
3656             filter_properties['ignore_hosts'].append(instance.host)
3657 
3658         request_spec = objects.RequestSpec.get_by_instance_uuid(
3659             context, instance.uuid)
3660         request_spec.ignore_hosts = filter_properties['ignore_hosts']
3661 
3662         instance.task_state = task_states.RESIZE_PREP
3663         instance.progress = 0
3664         instance.update(extra_instance_updates)
3665         instance.save(expected_task_state=[None])
3666 
3667         if not flavor_id:
3668             self._record_action_start(context, instance,
3669                                       instance_actions.MIGRATE)
3670         else:
3671             self._record_action_start(context, instance,
3672                                       instance_actions.RESIZE)
3673 
3674         # TODO(melwitt): We're not rechecking for strict quota here to guard
3675         # against going over quota during a race at this time because the
3676         # resource consumption for this operation is written to the database
3677         # by compute.
3678         scheduler_hint = {'filter_properties': filter_properties}
3679 
3680         if host_name is None:
3681             # If 'host_name' is not specified,
3682             # clear the 'requested_destination' field of the RequestSpec.
3683             request_spec.requested_destination = None
3684         else:
3685             # Set the host and the node so that the scheduler will
3686             # validate them.
3687             request_spec.requested_destination = objects.Destination(
3688                 host=node.host, node=node.hypervisor_hostname)
3689 
3690         self.compute_task_api.resize_instance(context, instance,
3691             scheduler_hint=scheduler_hint,
3692             flavor=new_instance_type,
3693             clean_shutdown=clean_shutdown,
3694             request_spec=request_spec)
3695 
3696     @check_instance_lock
3697     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3698                                     vm_states.PAUSED, vm_states.SUSPENDED])
3699     def shelve(self, context, instance, clean_shutdown=True):
3700         """Shelve an instance.
3701 
3702         Shuts down an instance and frees it up to be removed from the
3703         hypervisor.
3704         """
3705         instance.task_state = task_states.SHELVING
3706         instance.save(expected_task_state=[None])
3707 
3708         self._record_action_start(context, instance, instance_actions.SHELVE)
3709 
3710         if not compute_utils.is_volume_backed_instance(context, instance):
3711             name = '%s-shelved' % instance.display_name
3712             image_meta = compute_utils.create_image(
3713                 context, instance, name, 'snapshot', self.image_api)
3714             image_id = image_meta['id']
3715             self.compute_rpcapi.shelve_instance(context, instance=instance,
3716                     image_id=image_id, clean_shutdown=clean_shutdown)
3717         else:
3718             self.compute_rpcapi.shelve_offload_instance(context,
3719                     instance=instance, clean_shutdown=clean_shutdown)
3720 
3721     @check_instance_lock
3722     @check_instance_state(vm_state=[vm_states.SHELVED])
3723     def shelve_offload(self, context, instance, clean_shutdown=True):
3724         """Remove a shelved instance from the hypervisor."""
3725         instance.task_state = task_states.SHELVING_OFFLOADING
3726         instance.save(expected_task_state=[None])
3727 
3728         self._record_action_start(context, instance,
3729                                   instance_actions.SHELVE_OFFLOAD)
3730 
3731         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3732             clean_shutdown=clean_shutdown)
3733 
3734     def _validate_unshelve_az(self, context, instance, availability_zone):
3735         """Verify the specified availability_zone during unshelve.
3736 
3737         Verifies that the server is shelved offloaded, the AZ exists and
3738         if [cinder]/cross_az_attach=False, that any attached volumes are in
3739         the same AZ.
3740 
3741         :param context: nova auth RequestContext for the unshelve action
3742         :param instance: Instance object for the server being unshelved
3743         :param availability_zone: The user-requested availability zone in
3744             which to unshelve the server.
3745         :raises: UnshelveInstanceInvalidState if the server is not shelved
3746             offloaded
3747         :raises: InvalidRequest if the requested AZ does not exist
3748         :raises: MismatchVolumeAZException if [cinder]/cross_az_attach=False
3749             and any attached volumes are not in the requested AZ
3750         """
3751         if instance.vm_state != vm_states.SHELVED_OFFLOADED:
3752             # NOTE(brinzhang): If the server status is 'SHELVED', it still
3753             # belongs to a host, the availability_zone has not changed.
3754             # Unshelving a shelved offloaded server will go through the
3755             # scheduler to find a new host.
3756             raise exception.UnshelveInstanceInvalidState(
3757                 state=instance.vm_state, instance_uuid=instance.uuid)
3758 
3759         available_zones = availability_zones.get_availability_zones(
3760             context, self.host_api, get_only_available=True)
3761         if availability_zone not in available_zones:
3762             msg = _('The requested availability zone is not available')
3763             raise exception.InvalidRequest(msg)
3764 
3765         # NOTE(brinzhang): When specifying a availability zone to unshelve
3766         # a shelved offloaded server, and conf cross_az_attach=False, need
3767         # to determine if attached volume AZ matches the user-specified AZ.
3768         if not CONF.cinder.cross_az_attach:
3769             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3770                 context, instance.uuid)
3771             for bdm in bdms:
3772                 if bdm.is_volume and bdm.volume_id:
3773                     volume = self.volume_api.get(context, bdm.volume_id)
3774                     if availability_zone != volume['availability_zone']:
3775                         msg = _("The specified availability zone does not "
3776                                 "match the volume %(vol_id)s attached to the "
3777                                 "server. Specified availability zone is "
3778                                 "%(az)s. Volume is in %(vol_zone)s.") % {
3779                             "vol_id": volume['id'],
3780                             "az": availability_zone,
3781                             "vol_zone": volume['availability_zone']}
3782                         raise exception.MismatchVolumeAZException(reason=msg)
3783 
3784     @check_instance_lock
3785     @check_instance_state(vm_state=[vm_states.SHELVED,
3786         vm_states.SHELVED_OFFLOADED])
3787     def unshelve(self, context, instance, new_az=None):
3788         """Restore a shelved instance."""
3789         request_spec = objects.RequestSpec.get_by_instance_uuid(
3790             context, instance.uuid)
3791 
3792         if new_az:
3793             self._validate_unshelve_az(context, instance, new_az)
3794             LOG.debug("Replace the old AZ %(old_az)s in RequestSpec "
3795                       "with a new AZ %(new_az)s of the instance.",
3796                       {"old_az": request_spec.availability_zone,
3797                        "new_az": new_az}, instance=instance)
3798             # Unshelving a shelved offloaded server will go through the
3799             # scheduler to pick a new host, so we update the
3800             # RequestSpec.availability_zone here. Note that if scheduling
3801             # fails the RequestSpec will remain updated, which is not great,
3802             # but if we want to change that we need to defer updating the
3803             # RequestSpec until conductor which probably means RPC changes to
3804             # pass the new_az variable to conductor. This is likely low
3805             # priority since the RequestSpec.availability_zone on a shelved
3806             # offloaded server does not mean much anyway and clearly the user
3807             # is trying to put the server in the target AZ.
3808             request_spec.availability_zone = new_az
3809             request_spec.save()
3810 
3811         instance.task_state = task_states.UNSHELVING
3812         instance.save(expected_task_state=[None])
3813 
3814         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3815 
3816         self.compute_task_api.unshelve_instance(context, instance,
3817                                                 request_spec)
3818 
3819     @check_instance_lock
3820     def add_fixed_ip(self, context, instance, network_id):
3821         """Add fixed_ip from specified network to given instance."""
3822         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3823                 instance=instance, network_id=network_id)
3824 
3825     @check_instance_lock
3826     def remove_fixed_ip(self, context, instance, address):
3827         """Remove fixed_ip from specified network to given instance."""
3828         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3829                 instance=instance, address=address)
3830 
3831     @check_instance_lock
3832     @check_instance_state(vm_state=[vm_states.ACTIVE])
3833     def pause(self, context, instance):
3834         """Pause the given instance."""
3835         instance.task_state = task_states.PAUSING
3836         instance.save(expected_task_state=[None])
3837         self._record_action_start(context, instance, instance_actions.PAUSE)
3838         self.compute_rpcapi.pause_instance(context, instance)
3839 
3840     @check_instance_lock
3841     @check_instance_state(vm_state=[vm_states.PAUSED])
3842     def unpause(self, context, instance):
3843         """Unpause the given instance."""
3844         instance.task_state = task_states.UNPAUSING
3845         instance.save(expected_task_state=[None])
3846         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3847         self.compute_rpcapi.unpause_instance(context, instance)
3848 
3849     @check_instance_host
3850     def get_diagnostics(self, context, instance):
3851         """Retrieve diagnostics for the given instance."""
3852         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3853 
3854     @check_instance_host
3855     def get_instance_diagnostics(self, context, instance):
3856         """Retrieve diagnostics for the given instance."""
3857         return self.compute_rpcapi.get_instance_diagnostics(context,
3858                                                             instance=instance)
3859 
3860     @reject_sev_instances(instance_actions.SUSPEND)
3861     @check_instance_lock
3862     @check_instance_state(vm_state=[vm_states.ACTIVE])
3863     def suspend(self, context, instance):
3864         """Suspend the given instance."""
3865         instance.task_state = task_states.SUSPENDING
3866         instance.save(expected_task_state=[None])
3867         self._record_action_start(context, instance, instance_actions.SUSPEND)
3868         self.compute_rpcapi.suspend_instance(context, instance)
3869 
3870     @check_instance_lock
3871     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3872     def resume(self, context, instance):
3873         """Resume the given instance."""
3874         instance.task_state = task_states.RESUMING
3875         instance.save(expected_task_state=[None])
3876         self._record_action_start(context, instance, instance_actions.RESUME)
3877         self.compute_rpcapi.resume_instance(context, instance)
3878 
3879     @check_instance_lock
3880     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3881                                     vm_states.ERROR])
3882     def rescue(self, context, instance, rescue_password=None,
3883                rescue_image_ref=None, clean_shutdown=True):
3884         """Rescue the given instance."""
3885 
3886         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3887                     context, instance.uuid)
3888         for bdm in bdms:
3889             if bdm.volume_id:
3890                 vol = self.volume_api.get(context, bdm.volume_id)
3891                 self.volume_api.check_attached(context, vol)
3892         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3893             reason = _("Cannot rescue a volume-backed instance")
3894             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3895                                                  reason=reason)
3896 
3897         instance.task_state = task_states.RESCUING
3898         instance.save(expected_task_state=[None])
3899 
3900         self._record_action_start(context, instance, instance_actions.RESCUE)
3901 
3902         self.compute_rpcapi.rescue_instance(context, instance=instance,
3903             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3904             clean_shutdown=clean_shutdown)
3905 
3906     @check_instance_lock
3907     @check_instance_state(vm_state=[vm_states.RESCUED])
3908     def unrescue(self, context, instance):
3909         """Unrescue the given instance."""
3910         instance.task_state = task_states.UNRESCUING
3911         instance.save(expected_task_state=[None])
3912 
3913         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3914 
3915         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3916 
3917     @check_instance_lock
3918     @check_instance_state(vm_state=[vm_states.ACTIVE])
3919     def set_admin_password(self, context, instance, password=None):
3920         """Set the root/admin password for the given instance.
3921 
3922         @param context: Nova auth context.
3923         @param instance: Nova instance object.
3924         @param password: The admin password for the instance.
3925         """
3926         instance.task_state = task_states.UPDATING_PASSWORD
3927         instance.save(expected_task_state=[None])
3928 
3929         self._record_action_start(context, instance,
3930                                   instance_actions.CHANGE_PASSWORD)
3931 
3932         self.compute_rpcapi.set_admin_password(context,
3933                                                instance=instance,
3934                                                new_pass=password)
3935 
3936     @check_instance_host
3937     @reject_instance_state(
3938         task_state=[task_states.DELETING, task_states.MIGRATING])
3939     def get_vnc_console(self, context, instance, console_type):
3940         """Get a url to an instance Console."""
3941         connect_info = self.compute_rpcapi.get_vnc_console(context,
3942                 instance=instance, console_type=console_type)
3943         return {'url': connect_info['access_url']}
3944 
3945     @check_instance_host
3946     @reject_instance_state(
3947         task_state=[task_states.DELETING, task_states.MIGRATING])
3948     def get_spice_console(self, context, instance, console_type):
3949         """Get a url to an instance Console."""
3950         connect_info = self.compute_rpcapi.get_spice_console(context,
3951                 instance=instance, console_type=console_type)
3952         return {'url': connect_info['access_url']}
3953 
3954     @check_instance_host
3955     @reject_instance_state(
3956         task_state=[task_states.DELETING, task_states.MIGRATING])
3957     def get_rdp_console(self, context, instance, console_type):
3958         """Get a url to an instance Console."""
3959         connect_info = self.compute_rpcapi.get_rdp_console(context,
3960                 instance=instance, console_type=console_type)
3961         return {'url': connect_info['access_url']}
3962 
3963     @check_instance_host
3964     @reject_instance_state(
3965         task_state=[task_states.DELETING, task_states.MIGRATING])
3966     def get_serial_console(self, context, instance, console_type):
3967         """Get a url to a serial console."""
3968         connect_info = self.compute_rpcapi.get_serial_console(context,
3969                 instance=instance, console_type=console_type)
3970         return {'url': connect_info['access_url']}
3971 
3972     @check_instance_host
3973     @reject_instance_state(
3974         task_state=[task_states.DELETING, task_states.MIGRATING])
3975     def get_mks_console(self, context, instance, console_type):
3976         """Get a url to a MKS console."""
3977         connect_info = self.compute_rpcapi.get_mks_console(context,
3978                 instance=instance, console_type=console_type)
3979         return {'url': connect_info['access_url']}
3980 
3981     @check_instance_host
3982     def get_console_output(self, context, instance, tail_length=None):
3983         """Get console output for an instance."""
3984         return self.compute_rpcapi.get_console_output(context,
3985                 instance=instance, tail_length=tail_length)
3986 
3987     def lock(self, context, instance, reason=None):
3988         """Lock the given instance."""
3989         # Only update the lock if we are an admin (non-owner)
3990         is_owner = instance.project_id == context.project_id
3991         if instance.locked and is_owner:
3992             return
3993 
3994         context = context.elevated()
3995         self._record_action_start(context, instance,
3996                                   instance_actions.LOCK)
3997 
3998         @wrap_instance_event(prefix='api')
3999         def lock(self, context, instance, reason=None):
4000             LOG.debug('Locking', instance=instance)
4001             instance.locked = True
4002             instance.locked_by = 'owner' if is_owner else 'admin'
4003             if reason:
4004                 instance.system_metadata['locked_reason'] = reason
4005             instance.save()
4006 
4007         lock(self, context, instance, reason=reason)
4008         compute_utils.notify_about_instance_action(
4009             context, instance, CONF.host,
4010             action=fields_obj.NotificationAction.LOCK,
4011             source=fields_obj.NotificationSource.API)
4012 
4013     def is_expected_locked_by(self, context, instance):
4014         is_owner = instance.project_id == context.project_id
4015         expect_locked_by = 'owner' if is_owner else 'admin'
4016         locked_by = instance.locked_by
4017         if locked_by and locked_by != expect_locked_by:
4018             return False
4019         return True
4020 
4021     def unlock(self, context, instance):
4022         """Unlock the given instance."""
4023         context = context.elevated()
4024         self._record_action_start(context, instance,
4025                                   instance_actions.UNLOCK)
4026 
4027         @wrap_instance_event(prefix='api')
4028         def unlock(self, context, instance):
4029             LOG.debug('Unlocking', instance=instance)
4030             instance.locked = False
4031             instance.locked_by = None
4032             instance.system_metadata.pop('locked_reason', None)
4033             instance.save()
4034 
4035         unlock(self, context, instance)
4036         compute_utils.notify_about_instance_action(
4037             context, instance, CONF.host,
4038             action=fields_obj.NotificationAction.UNLOCK,
4039             source=fields_obj.NotificationSource.API)
4040 
4041     @check_instance_lock
4042     def reset_network(self, context, instance):
4043         """Reset networking on the instance."""
4044         self.compute_rpcapi.reset_network(context, instance=instance)
4045 
4046     @check_instance_lock
4047     def inject_network_info(self, context, instance):
4048         """Inject network info for the instance."""
4049         self.compute_rpcapi.inject_network_info(context, instance=instance)
4050 
4051     def _create_volume_bdm(self, context, instance, device, volume,
4052                            disk_bus, device_type, is_local_creation=False,
4053                            tag=None, delete_on_termination=False):
4054         volume_id = volume['id']
4055         if is_local_creation:
4056             # when the creation is done locally we can't specify the device
4057             # name as we do not have a way to check that the name specified is
4058             # a valid one.
4059             # We leave the setting of that value when the actual attach
4060             # happens on the compute manager
4061             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
4062             # support device tagging because we have no way to call the compute
4063             # manager to check that it supports device tagging. In fact, we
4064             # don't even know which computer manager the instance will
4065             # eventually end up on when it's unshelved.
4066             volume_bdm = objects.BlockDeviceMapping(
4067                 context=context,
4068                 source_type='volume', destination_type='volume',
4069                 instance_uuid=instance.uuid, boot_index=None,
4070                 volume_id=volume_id,
4071                 device_name=None, guest_format=None,
4072                 disk_bus=disk_bus, device_type=device_type,
4073                 delete_on_termination=delete_on_termination)
4074             volume_bdm.create()
4075         else:
4076             # NOTE(vish): This is done on the compute host because we want
4077             #             to avoid a race where two devices are requested at
4078             #             the same time. When db access is removed from
4079             #             compute, the bdm will be created here and we will
4080             #             have to make sure that they are assigned atomically.
4081             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
4082                 context, instance, device, volume_id, disk_bus=disk_bus,
4083                 device_type=device_type, tag=tag,
4084                 multiattach=volume['multiattach'])
4085             volume_bdm.delete_on_termination = delete_on_termination
4086             volume_bdm.save()
4087         return volume_bdm
4088 
4089     def _check_volume_already_attached_to_instance(self, context, instance,
4090                                                    volume_id):
4091         """Avoid attaching the same volume to the same instance twice.
4092 
4093            As the new Cinder flow (microversion 3.44) is handling the checks
4094            differently and allows to attach the same volume to the same
4095            instance twice to enable live_migrate we are checking whether the
4096            BDM already exists for this combination for the new flow and fail
4097            if it does.
4098         """
4099 
4100         try:
4101             objects.BlockDeviceMapping.get_by_volume_and_instance(
4102                 context, volume_id, instance.uuid)
4103 
4104             msg = _("volume %s already attached") % volume_id
4105             raise exception.InvalidVolume(reason=msg)
4106         except exception.VolumeBDMNotFound:
4107             pass
4108 
4109     def _check_attach_and_reserve_volume(self, context, volume, instance,
4110                                          bdm, supports_multiattach=False):
4111         volume_id = volume['id']
4112         self.volume_api.check_availability_zone(context, volume,
4113                                                 instance=instance)
4114         # If volume.multiattach=True and the microversion to
4115         # support multiattach is not used, fail the request.
4116         if volume['multiattach'] and not supports_multiattach:
4117             raise exception.MultiattachNotSupportedOldMicroversion()
4118 
4119         attachment_id = self.volume_api.attachment_create(
4120             context, volume_id, instance.uuid)['id']
4121         bdm.attachment_id = attachment_id
4122         # NOTE(ildikov): In case of boot from volume the BDM at this
4123         # point is not yet created in a cell database, so we can't
4124         # call save().  When attaching a volume to an existing
4125         # instance, the instance is already in a cell and the BDM has
4126         # been created in that same cell so updating here in that case
4127         # is "ok".
4128         if bdm.obj_attr_is_set('id'):
4129             bdm.save()
4130 
4131     # TODO(stephenfin): Fold this back in now that cells v1 no longer needs to
4132     # override it.
4133     def _attach_volume(self, context, instance, volume, device,
4134                        disk_bus, device_type, tag=None,
4135                        supports_multiattach=False,
4136                        delete_on_termination=False):
4137         """Attach an existing volume to an existing instance.
4138 
4139         This method is separated to make it possible for cells version
4140         to override it.
4141         """
4142         volume_bdm = self._create_volume_bdm(
4143             context, instance, device, volume, disk_bus=disk_bus,
4144             device_type=device_type, tag=tag,
4145             delete_on_termination=delete_on_termination)
4146         try:
4147             self._check_attach_and_reserve_volume(context, volume, instance,
4148                                                   volume_bdm,
4149                                                   supports_multiattach)
4150             self._record_action_start(
4151                 context, instance, instance_actions.ATTACH_VOLUME)
4152             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
4153         except Exception:
4154             with excutils.save_and_reraise_exception():
4155                 volume_bdm.destroy()
4156 
4157         return volume_bdm.device_name
4158 
4159     def _attach_volume_shelved_offloaded(self, context, instance, volume,
4160                                          device, disk_bus, device_type,
4161                                          delete_on_termination):
4162         """Attach an existing volume to an instance in shelved offloaded state.
4163 
4164         Attaching a volume for an instance in shelved offloaded state requires
4165         to perform the regular check to see if we can attach and reserve the
4166         volume then we need to call the attach method on the volume API
4167         to mark the volume as 'in-use'.
4168         The instance at this stage is not managed by a compute manager
4169         therefore the actual attachment will be performed once the
4170         instance will be unshelved.
4171         """
4172         volume_id = volume['id']
4173 
4174         @wrap_instance_event(prefix='api')
4175         def attach_volume(self, context, v_id, instance, dev, attachment_id):
4176             if attachment_id:
4177                 # Normally we wouldn't complete an attachment without a host
4178                 # connector, but we do this to make the volume status change
4179                 # to "in-use" to maintain the API semantics with the old flow.
4180                 # When unshelving the instance, the compute service will deal
4181                 # with this disconnected attachment.
4182                 self.volume_api.attachment_complete(context, attachment_id)
4183             else:
4184                 self.volume_api.attach(context,
4185                                        v_id,
4186                                        instance.uuid,
4187                                        dev)
4188 
4189         volume_bdm = self._create_volume_bdm(
4190             context, instance, device, volume, disk_bus=disk_bus,
4191             device_type=device_type, is_local_creation=True,
4192             delete_on_termination=delete_on_termination)
4193         try:
4194             self._check_attach_and_reserve_volume(context, volume, instance,
4195                                                   volume_bdm)
4196             self._record_action_start(
4197                 context, instance,
4198                 instance_actions.ATTACH_VOLUME)
4199             attach_volume(self, context, volume_id, instance, device,
4200                           volume_bdm.attachment_id)
4201         except Exception:
4202             with excutils.save_and_reraise_exception():
4203                 volume_bdm.destroy()
4204 
4205         return volume_bdm.device_name
4206 
4207     @check_instance_lock
4208     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4209                                     vm_states.STOPPED, vm_states.RESIZED,
4210                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4211                                     vm_states.SHELVED_OFFLOADED])
4212     def attach_volume(self, context, instance, volume_id, device=None,
4213                       disk_bus=None, device_type=None, tag=None,
4214                       supports_multiattach=False,
4215                       delete_on_termination=False):
4216         """Attach an existing volume to an existing instance."""
4217         # NOTE(vish): Fail fast if the device is not going to pass. This
4218         #             will need to be removed along with the test if we
4219         #             change the logic in the manager for what constitutes
4220         #             a valid device.
4221         if device and not block_device.match_device(device):
4222             raise exception.InvalidDevicePath(path=device)
4223 
4224         # Make sure the volume isn't already attached to this instance
4225         # because we'll use the v3.44 attachment flow in
4226         # _check_attach_and_reserve_volume and Cinder will allow multiple
4227         # attachments between the same volume and instance but the old flow
4228         # API semantics don't allow that so we enforce it here.
4229         self._check_volume_already_attached_to_instance(context,
4230                                                         instance,
4231                                                         volume_id)
4232 
4233         volume = self.volume_api.get(context, volume_id)
4234         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
4235         if is_shelved_offloaded:
4236             if tag:
4237                 # NOTE(artom) Local attach (to a shelved-offload instance)
4238                 # cannot support device tagging because we have no way to call
4239                 # the compute manager to check that it supports device tagging.
4240                 # In fact, we don't even know which computer manager the
4241                 # instance will eventually end up on when it's unshelved.
4242                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
4243             if volume['multiattach']:
4244                 # NOTE(mriedem): Similar to tagged attach, we don't support
4245                 # attaching a multiattach volume to shelved offloaded instances
4246                 # because we can't tell if the compute host (since there isn't
4247                 # one) supports it. This could possibly be supported in the
4248                 # future if the scheduler was made aware of which computes
4249                 # support multiattach volumes.
4250                 raise exception.MultiattachToShelvedNotSupported()
4251             return self._attach_volume_shelved_offloaded(context,
4252                                                          instance,
4253                                                          volume,
4254                                                          device,
4255                                                          disk_bus,
4256                                                          device_type,
4257                                                          delete_on_termination)
4258 
4259         return self._attach_volume(context, instance, volume, device,
4260                                    disk_bus, device_type, tag=tag,
4261                                    supports_multiattach=supports_multiattach,
4262                                    delete_on_termination=delete_on_termination)
4263 
4264     # TODO(stephenfin): Fold this back in now that cells v1 no longer needs to
4265     # override it.
4266     def _detach_volume(self, context, instance, volume):
4267         """Detach volume from instance.
4268 
4269         This method is separated to make it easier for cells version
4270         to override.
4271         """
4272         try:
4273             self.volume_api.begin_detaching(context, volume['id'])
4274         except exception.InvalidInput as exc:
4275             raise exception.InvalidVolume(reason=exc.format_message())
4276         attachments = volume.get('attachments', {})
4277         attachment_id = None
4278         if attachments and instance.uuid in attachments:
4279             attachment_id = attachments[instance.uuid]['attachment_id']
4280         self._record_action_start(
4281             context, instance, instance_actions.DETACH_VOLUME)
4282         self.compute_rpcapi.detach_volume(context, instance=instance,
4283                 volume_id=volume['id'], attachment_id=attachment_id)
4284 
4285     def _detach_volume_shelved_offloaded(self, context, instance, volume):
4286         """Detach a volume from an instance in shelved offloaded state.
4287 
4288         If the instance is shelved offloaded we just need to cleanup volume
4289         calling the volume api detach, the volume api terminate_connection
4290         and delete the bdm record.
4291         If the volume has delete_on_termination option set then we call the
4292         volume api delete as well.
4293         """
4294         @wrap_instance_event(prefix='api')
4295         def detach_volume(self, context, instance, bdms):
4296             self._local_cleanup_bdm_volumes(bdms, instance, context)
4297 
4298         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
4299                 context, volume['id'], instance.uuid)]
4300         # The begin_detaching() call only works with in-use volumes,
4301         # which will not be the case for volumes attached to a shelved
4302         # offloaded server via the attachments API since those volumes
4303         # will have `reserved` status.
4304         if not bdms[0].attachment_id:
4305             try:
4306                 self.volume_api.begin_detaching(context, volume['id'])
4307             except exception.InvalidInput as exc:
4308                 raise exception.InvalidVolume(reason=exc.format_message())
4309         self._record_action_start(
4310             context, instance,
4311             instance_actions.DETACH_VOLUME)
4312         detach_volume(self, context, instance, bdms)
4313 
4314     @check_instance_lock
4315     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4316                                     vm_states.STOPPED, vm_states.RESIZED,
4317                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4318                                     vm_states.SHELVED_OFFLOADED])
4319     def detach_volume(self, context, instance, volume):
4320         """Detach a volume from an instance."""
4321         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4322             self._detach_volume_shelved_offloaded(context, instance, volume)
4323         else:
4324             self._detach_volume(context, instance, volume)
4325 
4326     def _count_attachments_for_swap(self, ctxt, volume):
4327         """Counts the number of attachments for a swap-related volume.
4328 
4329         Attempts to only count read/write attachments if the volume attachment
4330         records exist, otherwise simply just counts the number of attachments
4331         regardless of attach mode.
4332 
4333         :param ctxt: nova.context.RequestContext - user request context
4334         :param volume: nova-translated volume dict from nova.volume.cinder.
4335         :returns: count of attachments for the volume
4336         """
4337         # This is a dict, keyed by server ID, to a dict of attachment_id and
4338         # mountpoint.
4339         attachments = volume.get('attachments', {})
4340         # Multiattach volumes can have more than one attachment, so if there
4341         # is more than one attachment, attempt to count the read/write
4342         # attachments.
4343         if len(attachments) > 1:
4344             count = 0
4345             for attachment in attachments.values():
4346                 attachment_id = attachment['attachment_id']
4347                 # Get the attachment record for this attachment so we can
4348                 # get the attach_mode.
4349                 # TODO(mriedem): This could be optimized if we had
4350                 # GET /attachments/detail?volume_id=volume['id'] in Cinder.
4351                 try:
4352                     attachment_record = self.volume_api.attachment_get(
4353                         ctxt, attachment_id)
4354                     # Note that the attachment record from Cinder has
4355                     # attach_mode in the top-level of the resource but the
4356                     # nova.volume.cinder code translates it and puts the
4357                     # attach_mode in the connection_info for some legacy
4358                     # reason...
4359                     if attachment_record['attach_mode'] == 'rw':
4360                         count += 1
4361                 except exception.VolumeAttachmentNotFound:
4362                     # attachments are read/write by default so count it
4363                     count += 1
4364         else:
4365             count = len(attachments)
4366 
4367         return count
4368 
4369     @check_instance_lock
4370     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4371                                     vm_states.RESIZED])
4372     def swap_volume(self, context, instance, old_volume, new_volume):
4373         """Swap volume attached to an instance."""
4374         # The caller likely got the instance from volume['attachments']
4375         # in the first place, but let's sanity check.
4376         if not old_volume.get('attachments', {}).get(instance.uuid):
4377             msg = _("Old volume is attached to a different instance.")
4378             raise exception.InvalidVolume(reason=msg)
4379         if new_volume['attach_status'] == 'attached':
4380             msg = _("New volume must be detached in order to swap.")
4381             raise exception.InvalidVolume(reason=msg)
4382         if int(new_volume['size']) < int(old_volume['size']):
4383             msg = _("New volume must be the same size or larger.")
4384             raise exception.InvalidVolume(reason=msg)
4385         self.volume_api.check_availability_zone(context, new_volume,
4386                                                 instance=instance)
4387         try:
4388             self.volume_api.begin_detaching(context, old_volume['id'])
4389         except exception.InvalidInput as exc:
4390             raise exception.InvalidVolume(reason=exc.format_message())
4391 
4392         # Disallow swapping from multiattach volumes that have more than one
4393         # read/write attachment. We know the old_volume has at least one
4394         # attachment since it's attached to this server. The new_volume
4395         # can't have any attachments because of the attach_status check above.
4396         # We do this count after calling "begin_detaching" to lock against
4397         # concurrent attachments being made while we're counting.
4398         try:
4399             if self._count_attachments_for_swap(context, old_volume) > 1:
4400                 raise exception.MultiattachSwapVolumeNotSupported()
4401         except Exception:  # This is generic to handle failures while counting
4402             # We need to reset the detaching status before raising.
4403             with excutils.save_and_reraise_exception():
4404                 self.volume_api.roll_detaching(context, old_volume['id'])
4405 
4406         # Get the BDM for the attached (old) volume so we can tell if it was
4407         # attached with the new-style Cinder 3.44 API.
4408         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4409             context, old_volume['id'], instance.uuid)
4410         new_attachment_id = None
4411         if bdm.attachment_id is None:
4412             # This is an old-style attachment so reserve the new volume before
4413             # we cast to the compute host.
4414             self.volume_api.reserve_volume(context, new_volume['id'])
4415         else:
4416             try:
4417                 self._check_volume_already_attached_to_instance(
4418                     context, instance, new_volume['id'])
4419             except exception.InvalidVolume:
4420                 with excutils.save_and_reraise_exception():
4421                     self.volume_api.roll_detaching(context, old_volume['id'])
4422 
4423             # This is a new-style attachment so for the volume that we are
4424             # going to swap to, create a new volume attachment.
4425             new_attachment_id = self.volume_api.attachment_create(
4426                 context, new_volume['id'], instance.uuid)['id']
4427 
4428         self._record_action_start(
4429             context, instance, instance_actions.SWAP_VOLUME)
4430 
4431         try:
4432             self.compute_rpcapi.swap_volume(
4433                     context, instance=instance,
4434                     old_volume_id=old_volume['id'],
4435                     new_volume_id=new_volume['id'],
4436                     new_attachment_id=new_attachment_id)
4437         except Exception:
4438             with excutils.save_and_reraise_exception():
4439                 self.volume_api.roll_detaching(context, old_volume['id'])
4440                 if new_attachment_id is None:
4441                     self.volume_api.unreserve_volume(context, new_volume['id'])
4442                 else:
4443                     self.volume_api.attachment_delete(
4444                         context, new_attachment_id)
4445 
4446     @check_instance_lock
4447     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4448                                     vm_states.STOPPED],
4449                           task_state=[None])
4450     def attach_interface(self, context, instance, network_id, port_id,
4451                          requested_ip, tag=None):
4452         """Use hotplug to add an network adapter to an instance."""
4453         self._record_action_start(
4454             context, instance, instance_actions.ATTACH_INTERFACE)
4455 
4456         # NOTE(gibi): Checking if the requested port has resource request as
4457         # such ports are currently not supported as they would at least
4458         # need resource allocation manipulation in placement but might also
4459         # need a new scheduling if resource on this host is not available.
4460         if port_id:
4461             port = self.network_api.show_port(context, port_id)
4462             if port['port'].get(constants.RESOURCE_REQUEST):
4463                 raise exception.AttachInterfaceWithQoSPolicyNotSupported(
4464                     instance_uuid=instance.uuid)
4465 
4466         return self.compute_rpcapi.attach_interface(context,
4467             instance=instance, network_id=network_id, port_id=port_id,
4468             requested_ip=requested_ip, tag=tag)
4469 
4470     @check_instance_lock
4471     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4472                                     vm_states.STOPPED],
4473                           task_state=[None])
4474     def detach_interface(self, context, instance, port_id):
4475         """Detach an network adapter from an instance."""
4476         self._record_action_start(
4477             context, instance, instance_actions.DETACH_INTERFACE)
4478         self.compute_rpcapi.detach_interface(context, instance=instance,
4479             port_id=port_id)
4480 
4481     def get_instance_metadata(self, context, instance):
4482         """Get all metadata associated with an instance."""
4483         return self.db.instance_metadata_get(context, instance.uuid)
4484 
4485     @check_instance_lock
4486     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4487                                     vm_states.SUSPENDED, vm_states.STOPPED],
4488                           task_state=None)
4489     def delete_instance_metadata(self, context, instance, key):
4490         """Delete the given metadata item from an instance."""
4491         instance.delete_metadata_key(key)
4492         self.compute_rpcapi.change_instance_metadata(context,
4493                                                      instance=instance,
4494                                                      diff={key: ['-']})
4495 
4496     @check_instance_lock
4497     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4498                                     vm_states.SUSPENDED, vm_states.STOPPED],
4499                           task_state=None)
4500     def update_instance_metadata(self, context, instance,
4501                                  metadata, delete=False):
4502         """Updates or creates instance metadata.
4503 
4504         If delete is True, metadata items that are not specified in the
4505         `metadata` argument will be deleted.
4506 
4507         """
4508         orig = dict(instance.metadata)
4509         if delete:
4510             _metadata = metadata
4511         else:
4512             _metadata = dict(instance.metadata)
4513             _metadata.update(metadata)
4514 
4515         self._check_metadata_properties_quota(context, _metadata)
4516         instance.metadata = _metadata
4517         instance.save()
4518         diff = _diff_dict(orig, instance.metadata)
4519         self.compute_rpcapi.change_instance_metadata(context,
4520                                                      instance=instance,
4521                                                      diff=diff)
4522         return _metadata
4523 
4524     @reject_sev_instances(instance_actions.LIVE_MIGRATION)
4525     @check_instance_lock
4526     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
4527     def live_migrate(self, context, instance, block_migration,
4528                      disk_over_commit, host_name, force=None, async_=False):
4529         """Migrate a server lively to a new host."""
4530         LOG.debug("Going to try to live migrate instance to %s",
4531                   host_name or "another host", instance=instance)
4532 
4533         if host_name:
4534             # Validate the specified host before changing the instance task
4535             # state.
4536             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
4537 
4538         request_spec = objects.RequestSpec.get_by_instance_uuid(
4539             context, instance.uuid)
4540 
4541         instance.task_state = task_states.MIGRATING
4542         instance.save(expected_task_state=[None])
4543 
4544         self._record_action_start(context, instance,
4545                                   instance_actions.LIVE_MIGRATION)
4546 
4547         # NOTE(sbauza): Force is a boolean by the new related API version
4548         if force is False and host_name:
4549             # Unset the host to make sure we call the scheduler
4550             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
4551             # to behavior in conductor and not great.
4552             host_name = None
4553             # FIXME(sbauza): Since only Ironic driver uses more than one
4554             # compute per service but doesn't support live migrations,
4555             # let's provide the first one.
4556             target = nodes[0]
4557             destination = objects.Destination(
4558                 host=target.host,
4559                 node=target.hypervisor_hostname
4560             )
4561             # This is essentially a hint to the scheduler to only consider
4562             # the specified host but still run it through the filters.
4563             request_spec.requested_destination = destination
4564 
4565         try:
4566             self.compute_task_api.live_migrate_instance(context, instance,
4567                 host_name, block_migration=block_migration,
4568                 disk_over_commit=disk_over_commit,
4569                 request_spec=request_spec, async_=async_)
4570         except oslo_exceptions.MessagingTimeout as messaging_timeout:
4571             with excutils.save_and_reraise_exception():
4572                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
4573                 # occurs, but LM will still be in progress, so write
4574                 # instance fault to database
4575                 compute_utils.add_instance_fault_from_exc(context,
4576                                                           instance,
4577                                                           messaging_timeout)
4578 
4579     @check_instance_lock
4580     @check_instance_state(vm_state=[vm_states.ACTIVE],
4581                           task_state=[task_states.MIGRATING])
4582     def live_migrate_force_complete(self, context, instance, migration_id):
4583         """Force live migration to complete.
4584 
4585         :param context: Security context
4586         :param instance: The instance that is being migrated
4587         :param migration_id: ID of ongoing migration
4588 
4589         """
4590         LOG.debug("Going to try to force live migration to complete",
4591                   instance=instance)
4592 
4593         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
4594         # live migration for particular instance. Also pass migration id to
4595         # compute to double check and avoid possible race condition.
4596         migration = objects.Migration.get_by_id_and_instance(
4597             context, migration_id, instance.uuid)
4598         if migration.status != 'running':
4599             raise exception.InvalidMigrationState(migration_id=migration_id,
4600                                                   instance_uuid=instance.uuid,
4601                                                   state=migration.status,
4602                                                   method='force complete')
4603 
4604         self._record_action_start(
4605             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
4606 
4607         self.compute_rpcapi.live_migration_force_complete(
4608             context, instance, migration)
4609 
4610     @check_instance_lock
4611     @check_instance_state(task_state=[task_states.MIGRATING])
4612     def live_migrate_abort(self, context, instance, migration_id,
4613                            support_abort_in_queue=False):
4614         """Abort an in-progress live migration.
4615 
4616         :param context: Security context
4617         :param instance: The instance that is being migrated
4618         :param migration_id: ID of in-progress live migration
4619         :param support_abort_in_queue: Flag indicating whether we can support
4620             abort migrations in "queued" or "preparing" status.
4621 
4622         """
4623         migration = objects.Migration.get_by_id_and_instance(context,
4624                     migration_id, instance.uuid)
4625         LOG.debug("Going to cancel live migration %s",
4626                   migration.id, instance=instance)
4627 
4628         # If the microversion does not support abort migration in queue,
4629         # we are only be able to abort migrations with `running` status;
4630         # if it is supported, we are able to also abort migrations in
4631         # `queued` and `preparing` status.
4632         allowed_states = ['running']
4633         queued_states = ['queued', 'preparing']
4634         if support_abort_in_queue:
4635             # The user requested a microversion that supports aborting a queued
4636             # or preparing live migration. But we need to check that the
4637             # compute service hosting the instance is new enough to support
4638             # aborting a queued/preparing live migration, so we check the
4639             # service version here.
4640             # TODO(Kevin_Zheng): This service version check can be removed in
4641             # Stein (at the earliest) when the API only supports Rocky or
4642             # newer computes.
4643             if migration.status in queued_states:
4644                 service = objects.Service.get_by_compute_host(
4645                     context, instance.host)
4646                 if service.version < MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION:
4647                     raise exception.AbortQueuedLiveMigrationNotYetSupported(
4648                         migration_id=migration_id, status=migration.status)
4649             allowed_states.extend(queued_states)
4650 
4651         if migration.status not in allowed_states:
4652             raise exception.InvalidMigrationState(migration_id=migration_id,
4653                     instance_uuid=instance.uuid,
4654                     state=migration.status,
4655                     method='abort live migration')
4656         self._record_action_start(context, instance,
4657                                   instance_actions.LIVE_MIGRATION_CANCEL)
4658 
4659         self.compute_rpcapi.live_migration_abort(context,
4660                 instance, migration.id)
4661 
4662     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4663                                     vm_states.ERROR])
4664     def evacuate(self, context, instance, host, on_shared_storage,
4665                  admin_password=None, force=None):
4666         """Running evacuate to target host.
4667 
4668         Checking vm compute host state, if the host not in expected_state,
4669         raising an exception.
4670 
4671         :param instance: The instance to evacuate
4672         :param host: Target host. if not set, the scheduler will pick up one
4673         :param on_shared_storage: True if instance files on shared storage
4674         :param admin_password: password to set on rebuilt instance
4675         :param force: Force the evacuation to the specific host target
4676 
4677         """
4678         LOG.debug('vm evacuation scheduled', instance=instance)
4679         inst_host = instance.host
4680         service = objects.Service.get_by_compute_host(context, inst_host)
4681         if self.servicegroup_api.service_is_up(service):
4682             LOG.error('Instance compute service state on %s '
4683                       'expected to be down, but it was up.', inst_host)
4684             raise exception.ComputeServiceInUse(host=inst_host)
4685 
4686         request_spec = objects.RequestSpec.get_by_instance_uuid(
4687             context, instance.uuid)
4688 
4689         instance.task_state = task_states.REBUILDING
4690         instance.save(expected_task_state=[None])
4691         self._record_action_start(context, instance, instance_actions.EVACUATE)
4692 
4693         # NOTE(danms): Create this as a tombstone for the source compute
4694         # to find and cleanup. No need to pass it anywhere else.
4695         migration = objects.Migration(context,
4696                                       source_compute=instance.host,
4697                                       source_node=instance.node,
4698                                       instance_uuid=instance.uuid,
4699                                       status='accepted',
4700                                       migration_type='evacuation')
4701         if host:
4702             migration.dest_compute = host
4703         migration.create()
4704 
4705         compute_utils.notify_about_instance_usage(
4706             self.notifier, context, instance, "evacuate")
4707         compute_utils.notify_about_instance_action(
4708             context, instance, CONF.host,
4709             action=fields_obj.NotificationAction.EVACUATE,
4710             source=fields_obj.NotificationSource.API)
4711 
4712         # NOTE(sbauza): Force is a boolean by the new related API version
4713         # TODO(stephenfin): Any reason we can't use 'not force' here to handle
4714         # the pre-v2.29 API microversion, which wouldn't set force
4715         if force is False and host:
4716             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
4717             # NOTE(sbauza): Unset the host to make sure we call the scheduler
4718             host = None
4719             # FIXME(sbauza): Since only Ironic driver uses more than one
4720             # compute per service but doesn't support evacuations,
4721             # let's provide the first one.
4722             target = nodes[0]
4723             destination = objects.Destination(
4724                 host=target.host,
4725                 node=target.hypervisor_hostname
4726             )
4727             request_spec.requested_destination = destination
4728 
4729         return self.compute_task_api.rebuild_instance(context,
4730                        instance=instance,
4731                        new_pass=admin_password,
4732                        injected_files=None,
4733                        image_ref=None,
4734                        orig_image_ref=None,
4735                        orig_sys_metadata=None,
4736                        bdms=None,
4737                        recreate=True,
4738                        on_shared_storage=on_shared_storage,
4739                        host=host,
4740                        request_spec=request_spec,
4741                        )
4742 
4743     def get_migrations(self, context, filters):
4744         """Get all migrations for the given filters."""
4745         load_cells()
4746 
4747         migrations = []
4748         for cell in CELLS:
4749             if cell.uuid == objects.CellMapping.CELL0_UUID:
4750                 continue
4751             with nova_context.target_cell(context, cell) as cctxt:
4752                 migrations.extend(objects.MigrationList.get_by_filters(
4753                     cctxt, filters).objects)
4754         return objects.MigrationList(objects=migrations)
4755 
4756     def get_migrations_sorted(self, context, filters, sort_dirs=None,
4757                               sort_keys=None, limit=None, marker=None):
4758         """Get all migrations for the given parameters."""
4759         mig_objs = migration_list.get_migration_objects_sorted(
4760             context, filters, limit, marker, sort_keys, sort_dirs)
4761         return mig_objs
4762 
4763     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
4764                                                migration_type=None):
4765         """Get all migrations of an instance in progress."""
4766         return objects.MigrationList.get_in_progress_by_instance(
4767                 context, instance_uuid, migration_type)
4768 
4769     def get_migration_by_id_and_instance(self, context,
4770                                          migration_id, instance_uuid):
4771         """Get the migration of an instance by id."""
4772         return objects.Migration.get_by_id_and_instance(
4773                 context, migration_id, instance_uuid)
4774 
4775     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
4776         """Retrieve a BDM without knowing its cell.
4777 
4778         .. note:: The context will be targeted to the cell in which the
4779             BDM is found, if any.
4780 
4781         :param context: The API request context.
4782         :param volume_id: The ID of the volume.
4783         :param expected_attrs: list of any additional attributes that should
4784             be joined when the BDM is loaded from the database.
4785         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
4786         """
4787         load_cells()
4788         for cell in CELLS:
4789             nova_context.set_target_cell(context, cell)
4790             try:
4791                 return objects.BlockDeviceMapping.get_by_volume(
4792                     context, volume_id, expected_attrs=expected_attrs)
4793             except exception.NotFound:
4794                 continue
4795         raise exception.VolumeBDMNotFound(volume_id=volume_id)
4796 
4797     def volume_snapshot_create(self, context, volume_id, create_info):
4798         bdm = self._get_bdm_by_volume_id(
4799             context, volume_id, expected_attrs=['instance'])
4800 
4801         # We allow creating the snapshot in any vm_state as long as there is
4802         # no task being performed on the instance and it has a host.
4803         @check_instance_host
4804         @check_instance_state(vm_state=None)
4805         def do_volume_snapshot_create(self, context, instance):
4806             self.compute_rpcapi.volume_snapshot_create(context, instance,
4807                     volume_id, create_info)
4808             snapshot = {
4809                 'snapshot': {
4810                     'id': create_info.get('id'),
4811                     'volumeId': volume_id
4812                 }
4813             }
4814             return snapshot
4815 
4816         return do_volume_snapshot_create(self, context, bdm.instance)
4817 
4818     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4819                                delete_info):
4820         bdm = self._get_bdm_by_volume_id(
4821             context, volume_id, expected_attrs=['instance'])
4822 
4823         # We allow deleting the snapshot in any vm_state as long as there is
4824         # no task being performed on the instance and it has a host.
4825         @check_instance_host
4826         @check_instance_state(vm_state=None)
4827         def do_volume_snapshot_delete(self, context, instance):
4828             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4829                     volume_id, snapshot_id, delete_info)
4830 
4831         do_volume_snapshot_delete(self, context, bdm.instance)
4832 
4833     def external_instance_event(self, api_context, instances, events):
4834         # NOTE(danms): The external API consumer just provides events,
4835         # but doesn't know where they go. We need to collate lists
4836         # by the host the affected instance is on and dispatch them
4837         # according to host
4838         instances_by_host = collections.defaultdict(list)
4839         events_by_host = collections.defaultdict(list)
4840         hosts_by_instance = collections.defaultdict(list)
4841         cell_contexts_by_host = {}
4842         for instance in instances:
4843             # instance._context is used here since it's already targeted to
4844             # the cell that the instance lives in, and we need to use that
4845             # cell context to lookup any migrations associated to the instance.
4846             for host in self._get_relevant_hosts(instance._context, instance):
4847                 # NOTE(danms): All instances on a host must have the same
4848                 # mapping, so just use that
4849                 # NOTE(mdbooth): We don't currently support migrations between
4850                 # cells, and given that the Migration record is hosted in the
4851                 # cell _get_relevant_hosts will likely have to change before we
4852                 # do. Consequently we can currently assume that the context for
4853                 # both the source and destination hosts of a migration is the
4854                 # same.
4855                 if host not in cell_contexts_by_host:
4856                     cell_contexts_by_host[host] = instance._context
4857 
4858                 instances_by_host[host].append(instance)
4859                 hosts_by_instance[instance.uuid].append(host)
4860 
4861         for event in events:
4862             if event.name == 'volume-extended':
4863                 # Volume extend is a user-initiated operation starting in the
4864                 # Block Storage service API. We record an instance action so
4865                 # the user can monitor the operation to completion.
4866                 host = hosts_by_instance[event.instance_uuid][0]
4867                 cell_context = cell_contexts_by_host[host]
4868                 objects.InstanceAction.action_start(
4869                     cell_context, event.instance_uuid,
4870                     instance_actions.EXTEND_VOLUME, want_result=False)
4871             elif event.name == 'power-update':
4872                 host = hosts_by_instance[event.instance_uuid][0]
4873                 cell_context = cell_contexts_by_host[host]
4874                 if event.tag == external_event_obj.POWER_ON:
4875                     inst_action = instance_actions.START
4876                 elif event.tag == external_event_obj.POWER_OFF:
4877                     inst_action = instance_actions.STOP
4878                 else:
4879                     LOG.warning("Invalid power state %s. Cannot process "
4880                                 "the event %s. Skipping it.", event.tag,
4881                                 event)
4882                     continue
4883                 objects.InstanceAction.action_start(
4884                     cell_context, event.instance_uuid, inst_action,
4885                     want_result=False)
4886 
4887             for host in hosts_by_instance[event.instance_uuid]:
4888                 events_by_host[host].append(event)
4889 
4890         for host in instances_by_host:
4891             cell_context = cell_contexts_by_host[host]
4892 
4893             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4894             # in order to ensure that a failure in processing events on a host
4895             # will not prevent processing events on other hosts
4896             self.compute_rpcapi.external_instance_event(
4897                 cell_context, instances_by_host[host], events_by_host[host],
4898                 host=host)
4899 
4900     def _get_relevant_hosts(self, context, instance):
4901         hosts = set()
4902         hosts.add(instance.host)
4903         if instance.migration_context is not None:
4904             migration_id = instance.migration_context.migration_id
4905             migration = objects.Migration.get_by_id(context, migration_id)
4906             hosts.add(migration.dest_compute)
4907             hosts.add(migration.source_compute)
4908             LOG.debug('Instance %(instance)s is migrating, '
4909                       'copying events to all relevant hosts: '
4910                       '%(hosts)s', {'instance': instance.uuid,
4911                                     'hosts': hosts})
4912         return hosts
4913 
4914     def get_instance_host_status(self, instance):
4915         if instance.host:
4916             try:
4917                 service = [service for service in instance.services if
4918                            service.binary == 'nova-compute'][0]
4919                 if service.forced_down:
4920                     host_status = fields_obj.HostStatus.DOWN
4921                 elif service.disabled:
4922                     host_status = fields_obj.HostStatus.MAINTENANCE
4923                 else:
4924                     alive = self.servicegroup_api.service_is_up(service)
4925                     host_status = ((alive and fields_obj.HostStatus.UP) or
4926                                    fields_obj.HostStatus.UNKNOWN)
4927             except IndexError:
4928                 host_status = fields_obj.HostStatus.NONE
4929         else:
4930             host_status = fields_obj.HostStatus.NONE
4931         return host_status
4932 
4933     def get_instances_host_statuses(self, instance_list):
4934         host_status_dict = dict()
4935         host_statuses = dict()
4936         for instance in instance_list:
4937             if instance.host:
4938                 if instance.host not in host_status_dict:
4939                     host_status = self.get_instance_host_status(instance)
4940                     host_status_dict[instance.host] = host_status
4941                 else:
4942                     host_status = host_status_dict[instance.host]
4943             else:
4944                 host_status = fields_obj.HostStatus.NONE
4945             host_statuses[instance.uuid] = host_status
4946         return host_statuses
4947 
4948 
4949 def target_host_cell(fn):
4950     """Target a host-based function to a cell.
4951 
4952     Expects to wrap a function of signature:
4953 
4954        func(self, context, host, ...)
4955     """
4956 
4957     @functools.wraps(fn)
4958     def targeted(self, context, host, *args, **kwargs):
4959         mapping = objects.HostMapping.get_by_host(context, host)
4960         nova_context.set_target_cell(context, mapping.cell_mapping)
4961         return fn(self, context, host, *args, **kwargs)
4962     return targeted
4963 
4964 
4965 def _find_service_in_cell(context, service_id=None, service_host=None):
4966     """Find a service by id or hostname by searching all cells.
4967 
4968     If one matching service is found, return it. If none or multiple
4969     are found, raise an exception.
4970 
4971     :param context: A context.RequestContext
4972     :param service_id: If not none, the DB ID of the service to find
4973     :param service_host: If not None, the hostname of the service to find
4974     :returns: An objects.Service
4975     :raises: ServiceNotUnique if multiple matching IDs are found
4976     :raises: NotFound if no matches are found
4977     :raises: NovaException if called with neither search option
4978     """
4979 
4980     load_cells()
4981     service = None
4982     found_in_cell = None
4983 
4984     is_uuid = False
4985     if service_id is not None:
4986         is_uuid = uuidutils.is_uuid_like(service_id)
4987         if is_uuid:
4988             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
4989         else:
4990             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
4991     elif service_host is not None:
4992         lookup_fn = lambda c: (
4993             objects.Service.get_by_compute_host(c, service_host))
4994     else:
4995         LOG.exception('_find_service_in_cell called with no search parameters')
4996         # This is intentionally cryptic so we don't leak implementation details
4997         # out of the API.
4998         raise exception.NovaException()
4999 
5000     for cell in CELLS:
5001         # NOTE(danms): Services can be in cell0, so don't skip it here
5002         try:
5003             with nova_context.target_cell(context, cell) as cctxt:
5004                 cell_service = lookup_fn(cctxt)
5005         except exception.NotFound:
5006             # NOTE(danms): Keep looking in other cells
5007             continue
5008         if service and cell_service:
5009             raise exception.ServiceNotUnique()
5010         service = cell_service
5011         found_in_cell = cell
5012         if service and is_uuid:
5013             break
5014 
5015     if service:
5016         # NOTE(danms): Set the cell on the context so it remains
5017         # when we return to our caller
5018         nova_context.set_target_cell(context, found_in_cell)
5019         return service
5020     else:
5021         raise exception.NotFound()
5022 
5023 
5024 class HostAPI(base.Base):
5025     """Sub-set of the Compute Manager API for managing host operations."""
5026 
5027     def __init__(self, rpcapi=None, servicegroup_api=None):
5028         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
5029         self.servicegroup_api = servicegroup_api or servicegroup.API()
5030         super(HostAPI, self).__init__()
5031 
5032     def _assert_host_exists(self, context, host_name, must_be_up=False):
5033         """Raise HostNotFound if compute host doesn't exist."""
5034         service = objects.Service.get_by_compute_host(context, host_name)
5035         if not service:
5036             raise exception.HostNotFound(host=host_name)
5037         if must_be_up and not self.servicegroup_api.service_is_up(service):
5038             raise exception.ComputeServiceUnavailable(host=host_name)
5039         return service['host']
5040 
5041     @wrap_exception()
5042     @target_host_cell
5043     def set_host_enabled(self, context, host_name, enabled):
5044         """Sets the specified host's ability to accept new instances."""
5045         host_name = self._assert_host_exists(context, host_name)
5046         payload = {'host_name': host_name, 'enabled': enabled}
5047         compute_utils.notify_about_host_update(context,
5048                                                'set_enabled.start',
5049                                                payload)
5050         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
5051                 host=host_name)
5052         compute_utils.notify_about_host_update(context,
5053                                                'set_enabled.end',
5054                                                payload)
5055         return result
5056 
5057     @target_host_cell
5058     def get_host_uptime(self, context, host_name):
5059         """Returns the result of calling "uptime" on the target host."""
5060         host_name = self._assert_host_exists(context, host_name,
5061                          must_be_up=True)
5062         return self.rpcapi.get_host_uptime(context, host=host_name)
5063 
5064     @wrap_exception()
5065     @target_host_cell
5066     def host_power_action(self, context, host_name, action):
5067         """Reboots, shuts down or powers up the host."""
5068         host_name = self._assert_host_exists(context, host_name)
5069         payload = {'host_name': host_name, 'action': action}
5070         compute_utils.notify_about_host_update(context,
5071                                                'power_action.start',
5072                                                payload)
5073         result = self.rpcapi.host_power_action(context, action=action,
5074                 host=host_name)
5075         compute_utils.notify_about_host_update(context,
5076                                                'power_action.end',
5077                                                payload)
5078         return result
5079 
5080     @wrap_exception()
5081     @target_host_cell
5082     def set_host_maintenance(self, context, host_name, mode):
5083         """Start/Stop host maintenance window. On start, it triggers
5084         guest VMs evacuation.
5085         """
5086         host_name = self._assert_host_exists(context, host_name)
5087         payload = {'host_name': host_name, 'mode': mode}
5088         compute_utils.notify_about_host_update(context,
5089                                                'set_maintenance.start',
5090                                                payload)
5091         result = self.rpcapi.host_maintenance_mode(context,
5092                 host_param=host_name, mode=mode, host=host_name)
5093         compute_utils.notify_about_host_update(context,
5094                                                'set_maintenance.end',
5095                                                payload)
5096         return result
5097 
5098     def service_get_all(self, context, filters=None, set_zones=False,
5099                         all_cells=False, cell_down_support=False):
5100         """Returns a list of services, optionally filtering the results.
5101 
5102         If specified, 'filters' should be a dictionary containing services
5103         attributes and matching values.  Ie, to get a list of services for
5104         the 'compute' topic, use filters={'topic': 'compute'}.
5105 
5106         If all_cells=True, then scan all cells and merge the results.
5107 
5108         If cell_down_support=True then return minimal service records
5109         for cells that do not respond based on what we have in the
5110         host mappings. These will have only 'binary' and 'host' set.
5111         """
5112         if filters is None:
5113             filters = {}
5114         disabled = filters.pop('disabled', None)
5115         if 'availability_zone' in filters:
5116             set_zones = True
5117 
5118         # NOTE(danms): Eventually this all_cells nonsense should go away
5119         # and we should always iterate over the cells. However, certain
5120         # callers need the legacy behavior for now.
5121         if all_cells:
5122             services = []
5123             service_dict = nova_context.scatter_gather_all_cells(context,
5124                 objects.ServiceList.get_all, disabled, set_zones=set_zones)
5125             for cell_uuid, service in service_dict.items():
5126                 if not nova_context.is_cell_failure_sentinel(service):
5127                     services.extend(service)
5128                 elif cell_down_support:
5129                     unavailable_services = objects.ServiceList()
5130                     cid = [cm.id for cm in nova_context.CELLS
5131                            if cm.uuid == cell_uuid]
5132                     # We know cid[0] is in the list because we are using the
5133                     # same list that scatter_gather_all_cells used
5134                     hms = objects.HostMappingList.get_by_cell_id(context,
5135                                                                  cid[0])
5136                     for hm in hms:
5137                         unavailable_services.objects.append(objects.Service(
5138                             binary='nova-compute', host=hm.host))
5139                     LOG.warning("Cell %s is not responding and hence only "
5140                                 "partial results are available from this "
5141                                 "cell.", cell_uuid)
5142                     services.extend(unavailable_services)
5143                 else:
5144                     LOG.warning("Cell %s is not responding and hence skipped "
5145                                 "from the results.", cell_uuid)
5146         else:
5147             services = objects.ServiceList.get_all(context, disabled,
5148                                                    set_zones=set_zones)
5149         ret_services = []
5150         for service in services:
5151             for key, val in filters.items():
5152                 if service[key] != val:
5153                     break
5154             else:
5155                 # All filters matched.
5156                 ret_services.append(service)
5157         return ret_services
5158 
5159     def service_get_by_id(self, context, service_id):
5160         """Get service entry for the given service id or uuid."""
5161         try:
5162             return _find_service_in_cell(context, service_id=service_id)
5163         except exception.NotFound:
5164             raise exception.ServiceNotFound(service_id=service_id)
5165 
5166     @target_host_cell
5167     def service_get_by_compute_host(self, context, host_name):
5168         """Get service entry for the given compute hostname."""
5169         return objects.Service.get_by_compute_host(context, host_name)
5170 
5171     def _update_compute_provider_status(self, context, service):
5172         """Calls the compute service to sync the COMPUTE_STATUS_DISABLED trait.
5173 
5174         There are two cases where the API will not call the compute service:
5175 
5176         * The compute service is down. In this case the trait is synchronized
5177           when the compute service is restarted.
5178         * The compute service is old. In this case the trait is synchronized
5179           when the compute service is upgraded and restarted.
5180 
5181         :param context: nova auth RequestContext
5182         :param service: nova.objects.Service object which has been enabled
5183             or disabled (see ``service_update``).
5184         """
5185         # Make sure the service is up so we can make the RPC call.
5186         if not self.servicegroup_api.service_is_up(service):
5187             LOG.info('Compute service on host %s is down. The '
5188                      'COMPUTE_STATUS_DISABLED trait will be synchronized '
5189                      'when the service is restarted.', service.host)
5190             return
5191 
5192         # Make sure the compute service is new enough for the trait sync
5193         # behavior.
5194         # TODO(mriedem): Remove this compat check in the U release.
5195         if service.version < MIN_COMPUTE_SYNC_COMPUTE_STATUS_DISABLED:
5196             LOG.info('Compute service on host %s is too old to sync the '
5197                      'COMPUTE_STATUS_DISABLED trait in Placement. The '
5198                      'trait will be synchronized when the service is '
5199                      'upgraded and restarted.', service.host)
5200             return
5201 
5202         enabled = not service.disabled
5203         # Avoid leaking errors out of the API.
5204         try:
5205             LOG.debug('Calling the compute service on host %s to sync the '
5206                       'COMPUTE_STATUS_DISABLED trait.', service.host)
5207             self.rpcapi.set_host_enabled(context, service.host, enabled)
5208         except Exception:
5209             LOG.exception('An error occurred while updating the '
5210                           'COMPUTE_STATUS_DISABLED trait on compute node '
5211                           'resource providers managed by host %s. The trait '
5212                           'will be synchronized automatically by the compute '
5213                           'service when the update_available_resource '
5214                           'periodic task runs.', service.host)
5215 
5216     def service_update(self, context, service):
5217         """Performs the actual service update operation.
5218 
5219         If the "disabled" field is changed, potentially calls the compute
5220         service to sync the COMPUTE_STATUS_DISABLED trait on the compute node
5221         resource providers managed by this compute service.
5222 
5223         :param context: nova auth RequestContext
5224         :param service: nova.objects.Service object with changes already
5225             set on the object
5226         """
5227         # Before persisting changes and resetting the changed fields on the
5228         # Service object, determine if the disabled field changed.
5229         update_placement = 'disabled' in service.obj_what_changed()
5230         # Persist the Service object changes to the database.
5231         service.save()
5232         # If the disabled field changed, potentially call the compute service
5233         # to sync the COMPUTE_STATUS_DISABLED trait.
5234         if update_placement:
5235             self._update_compute_provider_status(context, service)
5236         return service
5237 
5238     @target_host_cell
5239     def service_update_by_host_and_binary(self, context, host_name, binary,
5240                                           params_to_update):
5241         """Enable / Disable a service.
5242 
5243         Determines the cell that the service is in using the HostMapping.
5244 
5245         For compute services, this stops new builds and migrations going to
5246         the host.
5247 
5248         See also ``service_update``.
5249 
5250         :param context: nova auth RequestContext
5251         :param host_name: hostname of the service
5252         :param binary: service binary (really only supports "nova-compute")
5253         :param params_to_update: dict of changes to make to the Service object
5254         :raises: HostMappingNotFound if the host is not mapped to a cell
5255         :raises: HostBinaryNotFound if a services table record is not found
5256             with the given host_name and binary
5257         """
5258         # TODO(mriedem): Service.get_by_args is deprecated; we should use
5259         # get_by_compute_host here (remember to update the "raises" docstring).
5260         service = objects.Service.get_by_args(context, host_name, binary)
5261         service.update(params_to_update)
5262         return self.service_update(context, service)
5263 
5264     def _service_delete(self, context, service_id):
5265         """Performs the actual Service deletion operation."""
5266         try:
5267             service = _find_service_in_cell(context, service_id=service_id)
5268         except exception.NotFound:
5269             raise exception.ServiceNotFound(service_id=service_id)
5270         service.destroy()
5271 
5272     # TODO(mriedem): Nothing outside of tests is using this now so we should
5273     # be able to remove it.
5274     def service_delete(self, context, service_id):
5275         """Deletes the specified service found via id or uuid."""
5276         self._service_delete(context, service_id)
5277 
5278     @target_host_cell
5279     def instance_get_all_by_host(self, context, host_name):
5280         """Return all instances on the given host."""
5281         return objects.InstanceList.get_by_host(context, host_name)
5282 
5283     def task_log_get_all(self, context, task_name, period_beginning,
5284                          period_ending, host=None, state=None):
5285         """Return the task logs within a given range, optionally
5286         filtering by host and/or state.
5287         """
5288         return self.db.task_log_get_all(context, task_name,
5289                                         period_beginning,
5290                                         period_ending,
5291                                         host=host,
5292                                         state=state)
5293 
5294     def compute_node_get(self, context, compute_id):
5295         """Return compute node entry for particular integer ID or UUID."""
5296         load_cells()
5297 
5298         # NOTE(danms): Unfortunately this API exposes database identifiers
5299         # which means we really can't do something efficient here
5300         is_uuid = uuidutils.is_uuid_like(compute_id)
5301         for cell in CELLS:
5302             if cell.uuid == objects.CellMapping.CELL0_UUID:
5303                 continue
5304             with nova_context.target_cell(context, cell) as cctxt:
5305                 try:
5306                     if is_uuid:
5307                         return objects.ComputeNode.get_by_uuid(cctxt,
5308                                                                compute_id)
5309                     return objects.ComputeNode.get_by_id(cctxt,
5310                                                          int(compute_id))
5311                 except exception.ComputeHostNotFound:
5312                     # NOTE(danms): Keep looking in other cells
5313                     continue
5314 
5315         raise exception.ComputeHostNotFound(host=compute_id)
5316 
5317     def compute_node_get_all(self, context, limit=None, marker=None):
5318         load_cells()
5319 
5320         computes = []
5321         uuid_marker = marker and uuidutils.is_uuid_like(marker)
5322         for cell in CELLS:
5323             if cell.uuid == objects.CellMapping.CELL0_UUID:
5324                 continue
5325             with nova_context.target_cell(context, cell) as cctxt:
5326 
5327                 # If we have a marker and it's a uuid, see if the compute node
5328                 # is in this cell.
5329                 if marker and uuid_marker:
5330                     try:
5331                         compute_marker = objects.ComputeNode.get_by_uuid(
5332                             cctxt, marker)
5333                         # we found the marker compute node, so use it's id
5334                         # for the actual marker for paging in this cell's db
5335                         marker = compute_marker.id
5336                     except exception.ComputeHostNotFound:
5337                         # The marker node isn't in this cell so keep looking.
5338                         continue
5339 
5340                 try:
5341                     cell_computes = objects.ComputeNodeList.get_by_pagination(
5342                         cctxt, limit=limit, marker=marker)
5343                 except exception.MarkerNotFound:
5344                     # NOTE(danms): Keep looking through cells
5345                     continue
5346                 computes.extend(cell_computes)
5347                 # NOTE(danms): We must have found the marker, so continue on
5348                 # without one
5349                 marker = None
5350                 if limit:
5351                     limit -= len(cell_computes)
5352                     if limit <= 0:
5353                         break
5354 
5355         if marker is not None and len(computes) == 0:
5356             # NOTE(danms): If we did not find the marker in any cell,
5357             # mimic the db_api behavior here.
5358             raise exception.MarkerNotFound(marker=marker)
5359 
5360         return objects.ComputeNodeList(objects=computes)
5361 
5362     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
5363         load_cells()
5364 
5365         computes = []
5366         for cell in CELLS:
5367             if cell.uuid == objects.CellMapping.CELL0_UUID:
5368                 continue
5369             with nova_context.target_cell(context, cell) as cctxt:
5370                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
5371                     cctxt, hypervisor_match)
5372             computes.extend(cell_computes)
5373         return objects.ComputeNodeList(objects=computes)
5374 
5375     def compute_node_statistics(self, context):
5376         load_cells()
5377 
5378         cell_stats = []
5379         for cell in CELLS:
5380             if cell.uuid == objects.CellMapping.CELL0_UUID:
5381                 continue
5382             with nova_context.target_cell(context, cell) as cctxt:
5383                 cell_stats.append(self.db.compute_node_statistics(cctxt))
5384 
5385         if cell_stats:
5386             keys = cell_stats[0].keys()
5387             return {k: sum(stats[k] for stats in cell_stats)
5388                     for k in keys}
5389         else:
5390             return {}
5391 
5392 
5393 class InstanceActionAPI(base.Base):
5394     """Sub-set of the Compute Manager API for managing instance actions."""
5395 
5396     def actions_get(self, context, instance, limit=None, marker=None,
5397                     filters=None):
5398         return objects.InstanceActionList.get_by_instance_uuid(
5399             context, instance.uuid, limit, marker, filters)
5400 
5401     def action_get_by_request_id(self, context, instance, request_id):
5402         return objects.InstanceAction.get_by_request_id(
5403             context, instance.uuid, request_id)
5404 
5405     def action_events_get(self, context, instance, action_id):
5406         return objects.InstanceActionEventList.get_by_action(
5407             context, action_id)
5408 
5409 
5410 class AggregateAPI(base.Base):
5411     """Sub-set of the Compute Manager API for managing host aggregates."""
5412     def __init__(self, **kwargs):
5413         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5414         self.query_client = query.SchedulerQueryClient()
5415         self._placement_client = None  # Lazy-load on first access.
5416         super(AggregateAPI, self).__init__(**kwargs)
5417 
5418     @property
5419     def placement_client(self):
5420         if self._placement_client is None:
5421             self._placement_client = report.SchedulerReportClient()
5422         return self._placement_client
5423 
5424     @wrap_exception()
5425     def create_aggregate(self, context, aggregate_name, availability_zone):
5426         """Creates the model for the aggregate."""
5427 
5428         aggregate = objects.Aggregate(context=context)
5429         aggregate.name = aggregate_name
5430         if availability_zone:
5431             aggregate.metadata = {'availability_zone': availability_zone}
5432         aggregate.create()
5433         self.query_client.update_aggregates(context, [aggregate])
5434         return aggregate
5435 
5436     def get_aggregate(self, context, aggregate_id):
5437         """Get an aggregate by id."""
5438         return objects.Aggregate.get_by_id(context, aggregate_id)
5439 
5440     def get_aggregate_list(self, context):
5441         """Get all the aggregates."""
5442         return objects.AggregateList.get_all(context)
5443 
5444     def get_aggregates_by_host(self, context, compute_host):
5445         """Get all the aggregates where the given host is presented."""
5446         return objects.AggregateList.get_by_host(context, compute_host)
5447 
5448     @wrap_exception()
5449     def update_aggregate(self, context, aggregate_id, values):
5450         """Update the properties of an aggregate."""
5451         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5452         if 'name' in values:
5453             aggregate.name = values.pop('name')
5454             aggregate.save()
5455         self.is_safe_to_update_az(context, values, aggregate=aggregate,
5456                                   action_name=AGGREGATE_ACTION_UPDATE,
5457                                   check_no_instances_in_az=True)
5458         if values:
5459             aggregate.update_metadata(values)
5460             aggregate.updated_at = timeutils.utcnow()
5461         self.query_client.update_aggregates(context, [aggregate])
5462         # If updated values include availability_zones, then the cache
5463         # which stored availability_zones and host need to be reset
5464         if values.get('availability_zone'):
5465             availability_zones.reset_cache()
5466         return aggregate
5467 
5468     @wrap_exception()
5469     def update_aggregate_metadata(self, context, aggregate_id, metadata):
5470         """Updates the aggregate metadata."""
5471         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5472         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
5473                                   action_name=AGGREGATE_ACTION_UPDATE_META,
5474                                   check_no_instances_in_az=True)
5475         aggregate.update_metadata(metadata)
5476         self.query_client.update_aggregates(context, [aggregate])
5477         # If updated metadata include availability_zones, then the cache
5478         # which stored availability_zones and host need to be reset
5479         if metadata and metadata.get('availability_zone'):
5480             availability_zones.reset_cache()
5481         aggregate.updated_at = timeutils.utcnow()
5482         return aggregate
5483 
5484     @wrap_exception()
5485     def delete_aggregate(self, context, aggregate_id):
5486         """Deletes the aggregate."""
5487         aggregate_payload = {'aggregate_id': aggregate_id}
5488         compute_utils.notify_about_aggregate_update(context,
5489                                                     "delete.start",
5490                                                     aggregate_payload)
5491         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5492 
5493         compute_utils.notify_about_aggregate_action(
5494             context=context,
5495             aggregate=aggregate,
5496             action=fields_obj.NotificationAction.DELETE,
5497             phase=fields_obj.NotificationPhase.START)
5498 
5499         if len(aggregate.hosts) > 0:
5500             msg = _("Host aggregate is not empty")
5501             raise exception.InvalidAggregateActionDelete(
5502                 aggregate_id=aggregate_id, reason=msg)
5503         aggregate.destroy()
5504         self.query_client.delete_aggregate(context, aggregate)
5505         compute_utils.notify_about_aggregate_update(context,
5506                                                     "delete.end",
5507                                                     aggregate_payload)
5508         compute_utils.notify_about_aggregate_action(
5509             context=context,
5510             aggregate=aggregate,
5511             action=fields_obj.NotificationAction.DELETE,
5512             phase=fields_obj.NotificationPhase.END)
5513 
5514     def is_safe_to_update_az(self, context, metadata, aggregate,
5515                              hosts=None,
5516                              action_name=AGGREGATE_ACTION_ADD,
5517                              check_no_instances_in_az=False):
5518         """Determine if updates alter an aggregate's availability zone.
5519 
5520             :param context: local context
5521             :param metadata: Target metadata for updating aggregate
5522             :param aggregate: Aggregate to update
5523             :param hosts: Hosts to check. If None, aggregate.hosts is used
5524             :type hosts: list
5525             :param action_name: Calling method for logging purposes
5526             :param check_no_instances_in_az: if True, it checks
5527                 there is no instances on any hosts of the aggregate
5528 
5529         """
5530         if 'availability_zone' in metadata:
5531             if not metadata['availability_zone']:
5532                 msg = _("Aggregate %s does not support empty named "
5533                         "availability zone") % aggregate.name
5534                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5535                                                   msg)
5536             _hosts = hosts or aggregate.hosts
5537             host_aggregates = objects.AggregateList.get_by_metadata_key(
5538                 context, 'availability_zone', hosts=_hosts)
5539             conflicting_azs = [
5540                 agg.availability_zone for agg in host_aggregates
5541                 if agg.availability_zone != metadata['availability_zone'] and
5542                 agg.id != aggregate.id]
5543             if conflicting_azs:
5544                 msg = _("One or more hosts already in availability zone(s) "
5545                         "%s") % conflicting_azs
5546                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5547                                                   msg)
5548             same_az_name = (aggregate.availability_zone ==
5549                             metadata['availability_zone'])
5550             if check_no_instances_in_az and not same_az_name:
5551                 instance_count_by_cell = (
5552                     nova_context.scatter_gather_skip_cell0(
5553                         context,
5554                         objects.InstanceList.get_count_by_hosts,
5555                         _hosts))
5556                 if any(cnt for cnt in instance_count_by_cell.values()):
5557                     msg = _("One or more hosts contain instances in this zone")
5558                     self._raise_invalid_aggregate_exc(
5559                         action_name, aggregate.id, msg)
5560 
5561     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
5562         if action_name == AGGREGATE_ACTION_ADD:
5563             raise exception.InvalidAggregateActionAdd(
5564                 aggregate_id=aggregate_id, reason=reason)
5565         elif action_name == AGGREGATE_ACTION_UPDATE:
5566             raise exception.InvalidAggregateActionUpdate(
5567                 aggregate_id=aggregate_id, reason=reason)
5568         elif action_name == AGGREGATE_ACTION_UPDATE_META:
5569             raise exception.InvalidAggregateActionUpdateMeta(
5570                 aggregate_id=aggregate_id, reason=reason)
5571         elif action_name == AGGREGATE_ACTION_DELETE:
5572             raise exception.InvalidAggregateActionDelete(
5573                 aggregate_id=aggregate_id, reason=reason)
5574 
5575         raise exception.NovaException(
5576             _("Unexpected aggregate action %s") % action_name)
5577 
5578     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
5579         # Update the availability_zone cache to avoid getting wrong
5580         # availability_zone in cache retention time when add/remove
5581         # host to/from aggregate.
5582         if aggregate_meta and aggregate_meta.get('availability_zone'):
5583             availability_zones.update_host_availability_zone_cache(context,
5584                                                                    host_name)
5585 
5586     @wrap_exception()
5587     def add_host_to_aggregate(self, context, aggregate_id, host_name):
5588         """Adds the host to an aggregate."""
5589         aggregate_payload = {'aggregate_id': aggregate_id,
5590                              'host_name': host_name}
5591         compute_utils.notify_about_aggregate_update(context,
5592                                                     "addhost.start",
5593                                                     aggregate_payload)
5594         # validates the host; HostMappingNotFound or ComputeHostNotFound
5595         # is raised if invalid
5596         try:
5597             mapping = objects.HostMapping.get_by_host(context, host_name)
5598             nova_context.set_target_cell(context, mapping.cell_mapping)
5599             service = objects.Service.get_by_compute_host(context, host_name)
5600         except exception.HostMappingNotFound:
5601             try:
5602                 # NOTE(danms): This targets our cell
5603                 service = _find_service_in_cell(context,
5604                                                 service_host=host_name)
5605             except exception.NotFound:
5606                 raise exception.ComputeHostNotFound(host=host_name)
5607 
5608         if service.host != host_name:
5609             # NOTE(danms): If we found a service but it is not an
5610             # exact match, we may have a case-insensitive backend
5611             # database (like mysql) which will end up with us
5612             # adding the host-aggregate mapping with a
5613             # non-matching hostname.
5614             raise exception.ComputeHostNotFound(host=host_name)
5615 
5616         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5617 
5618         compute_utils.notify_about_aggregate_action(
5619             context=context,
5620             aggregate=aggregate,
5621             action=fields_obj.NotificationAction.ADD_HOST,
5622             phase=fields_obj.NotificationPhase.START)
5623 
5624         self.is_safe_to_update_az(context, aggregate.metadata,
5625                                   hosts=[host_name], aggregate=aggregate)
5626 
5627         aggregate.add_host(host_name)
5628         self.query_client.update_aggregates(context, [aggregate])
5629         try:
5630             self.placement_client.aggregate_add_host(
5631                 context, aggregate.uuid, host_name=host_name)
5632         except exception.PlacementAPIConnectFailure:
5633             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5634             # service not communicating with the Placement API, so just log a
5635             # warning here.
5636             # TODO(jaypipes): Remove this in Stein, when placement must be able
5637             # to be contacted from the nova-api service.
5638             LOG.warning("Failed to associate %s with a placement "
5639                         "aggregate: %s. There was a failure to communicate "
5640                         "with the placement service.",
5641                         host_name, aggregate.uuid)
5642         except (exception.ResourceProviderNotFound,
5643                 exception.ResourceProviderAggregateRetrievalFailed,
5644                 exception.ResourceProviderUpdateFailed,
5645                 exception.ResourceProviderUpdateConflict) as err:
5646             # NOTE(jaypipes): We don't want a failure perform the mirroring
5647             # action in the placement service to be returned to the user (they
5648             # probably don't know anything about the placement service and
5649             # would just be confused). So, we just log a warning here, noting
5650             # that on the next run of nova-manage placement sync_aggregates
5651             # things will go back to normal
5652             LOG.warning("Failed to associate %s with a placement "
5653                         "aggregate: %s. This may be corrected after running "
5654                         "nova-manage placement sync_aggregates.",
5655                         host_name, err)
5656         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5657         # NOTE(jogo): Send message to host to support resource pools
5658         self.compute_rpcapi.add_aggregate_host(context,
5659                 aggregate=aggregate, host_param=host_name, host=host_name)
5660         aggregate_payload.update({'name': aggregate.name})
5661         compute_utils.notify_about_aggregate_update(context,
5662                                                     "addhost.end",
5663                                                     aggregate_payload)
5664         compute_utils.notify_about_aggregate_action(
5665             context=context,
5666             aggregate=aggregate,
5667             action=fields_obj.NotificationAction.ADD_HOST,
5668             phase=fields_obj.NotificationPhase.END)
5669 
5670         return aggregate
5671 
5672     @wrap_exception()
5673     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
5674         """Removes host from the aggregate."""
5675         aggregate_payload = {'aggregate_id': aggregate_id,
5676                              'host_name': host_name}
5677         compute_utils.notify_about_aggregate_update(context,
5678                                                     "removehost.start",
5679                                                     aggregate_payload)
5680         # validates the host; HostMappingNotFound or ComputeHostNotFound
5681         # is raised if invalid
5682         mapping = objects.HostMapping.get_by_host(context, host_name)
5683         nova_context.set_target_cell(context, mapping.cell_mapping)
5684         objects.Service.get_by_compute_host(context, host_name)
5685         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5686 
5687         compute_utils.notify_about_aggregate_action(
5688             context=context,
5689             aggregate=aggregate,
5690             action=fields_obj.NotificationAction.REMOVE_HOST,
5691             phase=fields_obj.NotificationPhase.START)
5692 
5693         aggregate.delete_host(host_name)
5694         self.query_client.update_aggregates(context, [aggregate])
5695         try:
5696             self.placement_client.aggregate_remove_host(
5697                 context, aggregate.uuid, host_name)
5698         except exception.PlacementAPIConnectFailure:
5699             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5700             # service not communicating with the Placement API, so just log a
5701             # warning here.
5702             # TODO(jaypipes): Remove this in Stein, when placement must be able
5703             # to be contacted from the nova-api service.
5704             LOG.warning("Failed to remove association of %s with a placement "
5705                         "aggregate: %s. There was a failure to communicate "
5706                         "with the placement service.",
5707                         host_name, aggregate.uuid)
5708         except (exception.ResourceProviderNotFound,
5709                 exception.ResourceProviderAggregateRetrievalFailed,
5710                 exception.ResourceProviderUpdateFailed,
5711                 exception.ResourceProviderUpdateConflict) as err:
5712             # NOTE(jaypipes): We don't want a failure perform the mirroring
5713             # action in the placement service to be returned to the user (they
5714             # probably don't know anything about the placement service and
5715             # would just be confused). So, we just log a warning here, noting
5716             # that on the next run of nova-manage placement sync_aggregates
5717             # things will go back to normal
5718             LOG.warning("Failed to remove association of %s with a placement "
5719                         "aggregate: %s. This may be corrected after running "
5720                         "nova-manage placement sync_aggregates.",
5721                         host_name, err)
5722         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5723         self.compute_rpcapi.remove_aggregate_host(context,
5724                 aggregate=aggregate, host_param=host_name, host=host_name)
5725         compute_utils.notify_about_aggregate_update(context,
5726                                                     "removehost.end",
5727                                                     aggregate_payload)
5728         compute_utils.notify_about_aggregate_action(
5729             context=context,
5730             aggregate=aggregate,
5731             action=fields_obj.NotificationAction.REMOVE_HOST,
5732             phase=fields_obj.NotificationPhase.END)
5733         return aggregate
5734 
5735 
5736 class KeypairAPI(base.Base):
5737     """Subset of the Compute Manager API for managing key pairs."""
5738 
5739     get_notifier = functools.partial(rpc.get_notifier, service='api')
5740     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
5741                                        get_notifier=get_notifier,
5742                                        binary='nova-api')
5743 
5744     def _notify(self, context, event_suffix, keypair_name):
5745         payload = {
5746             'tenant_id': context.project_id,
5747             'user_id': context.user_id,
5748             'key_name': keypair_name,
5749         }
5750         notify = self.get_notifier()
5751         notify.info(context, 'keypair.%s' % event_suffix, payload)
5752 
5753     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
5754         safe_chars = "_- " + string.digits + string.ascii_letters
5755         clean_value = "".join(x for x in key_name if x in safe_chars)
5756         if clean_value != key_name:
5757             raise exception.InvalidKeypair(
5758                 reason=_("Keypair name contains unsafe characters"))
5759 
5760         try:
5761             utils.check_string_length(key_name, min_length=1, max_length=255)
5762         except exception.InvalidInput:
5763             raise exception.InvalidKeypair(
5764                 reason=_('Keypair name must be string and between '
5765                          '1 and 255 characters long'))
5766         try:
5767             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
5768         except exception.OverQuota:
5769             raise exception.KeypairLimitExceeded()
5770 
5771     @wrap_exception()
5772     def import_key_pair(self, context, user_id, key_name, public_key,
5773                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5774         """Import a key pair using an existing public key."""
5775         self._validate_new_key_pair(context, user_id, key_name, key_type)
5776 
5777         self._notify(context, 'import.start', key_name)
5778 
5779         keypair = objects.KeyPair(context)
5780         keypair.user_id = user_id
5781         keypair.name = key_name
5782         keypair.type = key_type
5783         keypair.fingerprint = None
5784         keypair.public_key = public_key
5785 
5786         compute_utils.notify_about_keypair_action(
5787             context=context,
5788             keypair=keypair,
5789             action=fields_obj.NotificationAction.IMPORT,
5790             phase=fields_obj.NotificationPhase.START)
5791 
5792         fingerprint = self._generate_fingerprint(public_key, key_type)
5793 
5794         keypair.fingerprint = fingerprint
5795         keypair.create()
5796 
5797         compute_utils.notify_about_keypair_action(
5798             context=context,
5799             keypair=keypair,
5800             action=fields_obj.NotificationAction.IMPORT,
5801             phase=fields_obj.NotificationPhase.END)
5802         self._notify(context, 'import.end', key_name)
5803 
5804         return keypair
5805 
5806     @wrap_exception()
5807     def create_key_pair(self, context, user_id, key_name,
5808                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5809         """Create a new key pair."""
5810         self._validate_new_key_pair(context, user_id, key_name, key_type)
5811 
5812         keypair = objects.KeyPair(context)
5813         keypair.user_id = user_id
5814         keypair.name = key_name
5815         keypair.type = key_type
5816         keypair.fingerprint = None
5817         keypair.public_key = None
5818 
5819         self._notify(context, 'create.start', key_name)
5820         compute_utils.notify_about_keypair_action(
5821             context=context,
5822             keypair=keypair,
5823             action=fields_obj.NotificationAction.CREATE,
5824             phase=fields_obj.NotificationPhase.START)
5825 
5826         private_key, public_key, fingerprint = self._generate_key_pair(
5827             user_id, key_type)
5828 
5829         keypair.fingerprint = fingerprint
5830         keypair.public_key = public_key
5831         keypair.create()
5832 
5833         # NOTE(melwitt): We recheck the quota after creating the object to
5834         # prevent users from allocating more resources than their allowed quota
5835         # in the event of a race. This is configurable because it can be
5836         # expensive if strict quota limits are not required in a deployment.
5837         if CONF.quota.recheck_quota:
5838             try:
5839                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
5840             except exception.OverQuota:
5841                 keypair.destroy()
5842                 raise exception.KeypairLimitExceeded()
5843 
5844         compute_utils.notify_about_keypair_action(
5845             context=context,
5846             keypair=keypair,
5847             action=fields_obj.NotificationAction.CREATE,
5848             phase=fields_obj.NotificationPhase.END)
5849 
5850         self._notify(context, 'create.end', key_name)
5851 
5852         return keypair, private_key
5853 
5854     def _generate_fingerprint(self, public_key, key_type):
5855         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5856             return crypto.generate_fingerprint(public_key)
5857         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5858             return crypto.generate_x509_fingerprint(public_key)
5859 
5860     def _generate_key_pair(self, user_id, key_type):
5861         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5862             return crypto.generate_key_pair()
5863         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5864             return crypto.generate_winrm_x509_cert(user_id)
5865 
5866     @wrap_exception()
5867     def delete_key_pair(self, context, user_id, key_name):
5868         """Delete a keypair by name."""
5869         self._notify(context, 'delete.start', key_name)
5870         keypair = self.get_key_pair(context, user_id, key_name)
5871         compute_utils.notify_about_keypair_action(
5872             context=context,
5873             keypair=keypair,
5874             action=fields_obj.NotificationAction.DELETE,
5875             phase=fields_obj.NotificationPhase.START)
5876         objects.KeyPair.destroy_by_name(context, user_id, key_name)
5877         compute_utils.notify_about_keypair_action(
5878             context=context,
5879             keypair=keypair,
5880             action=fields_obj.NotificationAction.DELETE,
5881             phase=fields_obj.NotificationPhase.END)
5882         self._notify(context, 'delete.end', key_name)
5883 
5884     def get_key_pairs(self, context, user_id, limit=None, marker=None):
5885         """List key pairs."""
5886         return objects.KeyPairList.get_by_user(
5887             context, user_id, limit=limit, marker=marker)
5888 
5889     def get_key_pair(self, context, user_id, key_name):
5890         """Get a keypair by name."""
5891         return objects.KeyPair.get_by_name(context, user_id, key_name)
5892 
5893 
5894 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
5895     """Sub-set of the Compute API related to managing security groups
5896     and security group rules
5897     """
5898 
5899     # The nova security group api does not use a uuid for the id.
5900     id_is_uuid = False
5901 
5902     def __init__(self, **kwargs):
5903         super(SecurityGroupAPI, self).__init__(**kwargs)
5904         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5905 
5906     def validate_property(self, value, property, allowed):
5907         """Validate given security group property.
5908 
5909         :param value:          the value to validate, as a string or unicode
5910         :param property:       the property, either 'name' or 'description'
5911         :param allowed:        the range of characters allowed
5912         """
5913 
5914         try:
5915             val = value.strip()
5916         except AttributeError:
5917             msg = _("Security group %s is not a string or unicode") % property
5918             self.raise_invalid_property(msg)
5919         utils.check_string_length(val, name=property, min_length=1,
5920                                   max_length=255)
5921 
5922         if allowed and not re.match(allowed, val):
5923             # Some validation to ensure that values match API spec.
5924             # - Alphanumeric characters, spaces, dashes, and underscores.
5925             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
5926             #  probably create a param validator that can be used elsewhere.
5927             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
5928                      "invalid. Content limited to '%(allowed)s'.") %
5929                    {'value': value, 'allowed': allowed,
5930                     'property': property.capitalize()})
5931             self.raise_invalid_property(msg)
5932 
5933     def ensure_default(self, context):
5934         """Ensure that a context has a security group.
5935 
5936         Creates a security group for the security context if it does not
5937         already exist.
5938 
5939         :param context: the security context
5940         """
5941         self.db.security_group_ensure_default(context)
5942 
5943     def create_security_group(self, context, name, description):
5944         try:
5945             objects.Quotas.check_deltas(context, {'security_groups': 1},
5946                                         context.project_id,
5947                                         user_id=context.user_id)
5948         except exception.OverQuota:
5949             msg = _("Quota exceeded, too many security groups.")
5950             self.raise_over_quota(msg)
5951 
5952         LOG.info("Create Security Group %s", name)
5953 
5954         self.ensure_default(context)
5955 
5956         group = {'user_id': context.user_id,
5957                  'project_id': context.project_id,
5958                  'name': name,
5959                  'description': description}
5960         try:
5961             group_ref = self.db.security_group_create(context, group)
5962         except exception.SecurityGroupExists:
5963             msg = _('Security group %s already exists') % name
5964             self.raise_group_already_exists(msg)
5965 
5966         # NOTE(melwitt): We recheck the quota after creating the object to
5967         # prevent users from allocating more resources than their allowed quota
5968         # in the event of a race. This is configurable because it can be
5969         # expensive if strict quota limits are not required in a deployment.
5970         if CONF.quota.recheck_quota:
5971             try:
5972                 objects.Quotas.check_deltas(context, {'security_groups': 0},
5973                                             context.project_id,
5974                                             user_id=context.user_id)
5975             except exception.OverQuota:
5976                 self.db.security_group_destroy(context, group_ref['id'])
5977                 msg = _("Quota exceeded, too many security groups.")
5978                 self.raise_over_quota(msg)
5979 
5980         return group_ref
5981 
5982     def update_security_group(self, context, security_group,
5983                                 name, description):
5984         if security_group['name'] in RO_SECURITY_GROUPS:
5985             msg = (_("Unable to update system group '%s'") %
5986                     security_group['name'])
5987             self.raise_invalid_group(msg)
5988 
5989         group = {'name': name,
5990                  'description': description}
5991 
5992         columns_to_join = ['rules.grantee_group']
5993         group_ref = self.db.security_group_update(context,
5994                 security_group['id'],
5995                 group,
5996                 columns_to_join=columns_to_join)
5997         return group_ref
5998 
5999     def get(self, context, name=None, id=None, map_exception=False):
6000         self.ensure_default(context)
6001         cols = ['rules']
6002         try:
6003             if name:
6004                 return self.db.security_group_get_by_name(context,
6005                                                           context.project_id,
6006                                                           name,
6007                                                           columns_to_join=cols)
6008             elif id:
6009                 return self.db.security_group_get(context, id,
6010                                                   columns_to_join=cols)
6011         except exception.NotFound as exp:
6012             if map_exception:
6013                 msg = exp.format_message()
6014                 self.raise_not_found(msg)
6015             else:
6016                 raise
6017 
6018     def list(self, context, names=None, ids=None, project=None,
6019              search_opts=None):
6020         self.ensure_default(context)
6021 
6022         groups = []
6023         if names or ids:
6024             if names:
6025                 for name in names:
6026                     groups.append(self.db.security_group_get_by_name(context,
6027                                                                      project,
6028                                                                      name))
6029             if ids:
6030                 for id in ids:
6031                     groups.append(self.db.security_group_get(context, id))
6032 
6033         elif context.is_admin:
6034             # TODO(eglynn): support a wider set of search options than just
6035             # all_tenants, at least include the standard filters defined for
6036             # the EC2 DescribeSecurityGroups API for the non-admin case also
6037             if (search_opts and 'all_tenants' in search_opts):
6038                 groups = self.db.security_group_get_all(context)
6039             else:
6040                 groups = self.db.security_group_get_by_project(context,
6041                                                                project)
6042 
6043         elif project:
6044             groups = self.db.security_group_get_by_project(context, project)
6045 
6046         return groups
6047 
6048     def destroy(self, context, security_group):
6049         if security_group['name'] in RO_SECURITY_GROUPS:
6050             msg = _("Unable to delete system group '%s'") % \
6051                     security_group['name']
6052             self.raise_invalid_group(msg)
6053 
6054         if self.db.security_group_in_use(context, security_group['id']):
6055             msg = _("Security group is still in use")
6056             self.raise_invalid_group(msg)
6057 
6058         LOG.info("Delete security group %s", security_group['name'])
6059         self.db.security_group_destroy(context, security_group['id'])
6060 
6061     def is_associated_with_server(self, security_group, instance_uuid):
6062         """Check if the security group is already associated
6063            with the instance. If Yes, return True.
6064         """
6065 
6066         if not security_group:
6067             return False
6068 
6069         instances = security_group.get('instances')
6070         if not instances:
6071             return False
6072 
6073         for inst in instances:
6074             if (instance_uuid == inst['uuid']):
6075                 return True
6076 
6077         return False
6078 
6079     def add_to_instance(self, context, instance, security_group_name):
6080         """Add security group to the instance."""
6081         security_group = self.db.security_group_get_by_name(context,
6082                 context.project_id,
6083                 security_group_name)
6084 
6085         instance_uuid = instance.uuid
6086 
6087         # check if the security group is associated with the server
6088         if self.is_associated_with_server(security_group, instance_uuid):
6089             raise exception.SecurityGroupExistsForInstance(
6090                                         security_group_id=security_group['id'],
6091                                         instance_id=instance_uuid)
6092 
6093         self.db.instance_add_security_group(context.elevated(),
6094                                             instance_uuid,
6095                                             security_group['id'])
6096         if instance.host:
6097             self.compute_rpcapi.refresh_instance_security_rules(
6098                     context, instance, instance.host)
6099 
6100     def remove_from_instance(self, context, instance, security_group_name):
6101         """Remove the security group associated with the instance."""
6102         security_group = self.db.security_group_get_by_name(context,
6103                 context.project_id,
6104                 security_group_name)
6105 
6106         instance_uuid = instance.uuid
6107 
6108         # check if the security group is associated with the server
6109         if not self.is_associated_with_server(security_group, instance_uuid):
6110             raise exception.SecurityGroupNotExistsForInstance(
6111                                     security_group_id=security_group['id'],
6112                                     instance_id=instance_uuid)
6113 
6114         self.db.instance_remove_security_group(context.elevated(),
6115                                                instance_uuid,
6116                                                security_group['id'])
6117         if instance.host:
6118             self.compute_rpcapi.refresh_instance_security_rules(
6119                     context, instance, instance.host)
6120 
6121     def get_rule(self, context, id):
6122         self.ensure_default(context)
6123         try:
6124             return self.db.security_group_rule_get(context, id)
6125         except exception.NotFound:
6126             msg = _("Rule (%s) not found") % id
6127             self.raise_not_found(msg)
6128 
6129     def add_rules(self, context, id, name, vals):
6130         """Add security group rule(s) to security group.
6131 
6132         Note: the Nova security group API doesn't support adding multiple
6133         security group rules at once but the EC2 one does. Therefore,
6134         this function is written to support both.
6135         """
6136 
6137         try:
6138             objects.Quotas.check_deltas(context,
6139                                         {'security_group_rules': len(vals)},
6140                                         id)
6141         except exception.OverQuota:
6142             msg = _("Quota exceeded, too many security group rules.")
6143             self.raise_over_quota(msg)
6144 
6145         msg = ("Security group %(name)s added %(protocol)s ingress "
6146                "(%(from_port)s:%(to_port)s)")
6147         rules = []
6148         for v in vals:
6149             rule = self.db.security_group_rule_create(context, v)
6150 
6151             # NOTE(melwitt): We recheck the quota after creating the object to
6152             # prevent users from allocating more resources than their allowed
6153             # quota in the event of a race. This is configurable because it can
6154             # be expensive if strict quota limits are not required in a
6155             # deployment.
6156             if CONF.quota.recheck_quota:
6157                 try:
6158                     objects.Quotas.check_deltas(context,
6159                                                 {'security_group_rules': 0},
6160                                                 id)
6161                 except exception.OverQuota:
6162                     self.db.security_group_rule_destroy(context, rule['id'])
6163                     msg = _("Quota exceeded, too many security group rules.")
6164                     self.raise_over_quota(msg)
6165 
6166             rules.append(rule)
6167             LOG.info(msg, {'name': name,
6168                            'protocol': rule.protocol,
6169                            'from_port': rule.from_port,
6170                            'to_port': rule.to_port})
6171 
6172         self.trigger_rules_refresh(context, id=id)
6173         return rules
6174 
6175     def remove_rules(self, context, security_group, rule_ids):
6176         msg = ("Security group %(name)s removed %(protocol)s ingress "
6177                "(%(from_port)s:%(to_port)s)")
6178         for rule_id in rule_ids:
6179             rule = self.get_rule(context, rule_id)
6180             LOG.info(msg, {'name': security_group['name'],
6181                            'protocol': rule.protocol,
6182                            'from_port': rule.from_port,
6183                            'to_port': rule.to_port})
6184 
6185             self.db.security_group_rule_destroy(context, rule_id)
6186 
6187         # NOTE(vish): we removed some rules, so refresh
6188         self.trigger_rules_refresh(context, id=security_group['id'])
6189 
6190     def remove_default_rules(self, context, rule_ids):
6191         for rule_id in rule_ids:
6192             self.db.security_group_default_rule_destroy(context, rule_id)
6193 
6194     def add_default_rules(self, context, vals):
6195         rules = [self.db.security_group_default_rule_create(context, v)
6196                  for v in vals]
6197         return rules
6198 
6199     def default_rule_exists(self, context, values):
6200         """Indicates whether the specified rule values are already
6201            defined in the default security group rules.
6202         """
6203         for rule in self.db.security_group_default_rule_list(context):
6204             keys = ('cidr', 'from_port', 'to_port', 'protocol')
6205             for key in keys:
6206                 if rule.get(key) != values.get(key):
6207                     break
6208             else:
6209                 return rule.get('id') or True
6210         return False
6211 
6212     def get_all_default_rules(self, context):
6213         try:
6214             rules = self.db.security_group_default_rule_list(context)
6215         except Exception:
6216             msg = 'cannot get default security group rules'
6217             raise exception.SecurityGroupDefaultRuleNotFound(msg)
6218 
6219         return rules
6220 
6221     def get_default_rule(self, context, id):
6222         return self.db.security_group_default_rule_get(context, id)
6223 
6224     def validate_id(self, id):
6225         try:
6226             return int(id)
6227         except ValueError:
6228             msg = _("Security group id should be integer")
6229             self.raise_invalid_property(msg)
6230 
6231     def _refresh_instance_security_rules(self, context, instances):
6232         for instance in instances:
6233             if instance.host is not None:
6234                 self.compute_rpcapi.refresh_instance_security_rules(
6235                         context, instance, instance.host)
6236 
6237     def trigger_rules_refresh(self, context, id):
6238         """Called when a rule is added to or removed from a security_group."""
6239         instances = objects.InstanceList.get_by_security_group_id(context, id)
6240         self._refresh_instance_security_rules(context, instances)
6241 
6242     def trigger_members_refresh(self, context, group_ids):
6243         """Called when a security group gains a new or loses a member.
6244 
6245         Sends an update request to each compute node for each instance for
6246         which this is relevant.
6247         """
6248         instances = objects.InstanceList.get_by_grantee_security_group_ids(
6249             context, group_ids)
6250         self._refresh_instance_security_rules(context, instances)
6251 
6252     def get_instance_security_groups(self, context, instance, detailed=False):
6253         if detailed:
6254             return self.db.security_group_get_by_instance(context,
6255                                                           instance.uuid)
6256         return [{'name': group.name} for group in instance.security_groups]
