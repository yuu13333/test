Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import functools
32 import inspect
33 import sys
34 import time
35 import traceback
36 
37 from cinderclient import exceptions as cinder_exception
38 import eventlet.event
39 from eventlet import greenthread
40 import eventlet.semaphore
41 import eventlet.timeout
42 from keystoneauth1 import exceptions as keystone_exception
43 from oslo_log import log as logging
44 import oslo_messaging as messaging
45 from oslo_serialization import jsonutils
46 from oslo_service import loopingcall
47 from oslo_service import periodic_task
48 from oslo_utils import excutils
49 from oslo_utils import strutils
50 from oslo_utils import timeutils
51 from oslo_utils import uuidutils
52 import six
53 from six.moves import range
54 
55 from nova import block_device
56 from nova.cells import rpcapi as cells_rpcapi
57 from nova.cloudpipe import pipelib
58 from nova import compute
59 from nova.compute import build_results
60 from nova.compute import claims
61 from nova.compute import power_state
62 from nova.compute import resource_tracker
63 from nova.compute import rpcapi as compute_rpcapi
64 from nova.compute import task_states
65 from nova.compute import utils as compute_utils
66 from nova.compute.utils import wrap_instance_event
67 from nova.compute import vm_states
68 from nova import conductor
69 import nova.conf
70 from nova import consoleauth
71 import nova.context
72 from nova import exception
73 from nova import exception_wrapper
74 from nova import hooks
75 from nova.i18n import _
76 from nova.i18n import _LE
77 from nova.i18n import _LI
78 from nova.i18n import _LW
79 from nova import image
80 from nova.image import glance
81 from nova import manager
82 from nova import network
83 from nova.network import base_api as base_net_api
84 from nova.network import model as network_model
85 from nova.network.security_group import openstack_driver
86 from nova import objects
87 from nova.objects import base as obj_base
88 from nova.objects import fields
89 from nova.objects import instance as obj_instance
90 from nova.objects import migrate_data as migrate_data_obj
91 from nova.pci import whitelist
92 from nova import rpc
93 from nova import safe_utils
94 from nova.scheduler import client as scheduler_client
95 from nova import utils
96 from nova.virt import block_device as driver_block_device
97 from nova.virt import configdrive
98 from nova.virt import driver
99 from nova.virt import event as virtevent
100 from nova.virt import storage_users
101 from nova.virt import virtapi
102 from nova.volume import cinder
103 from nova.volume import encryptors
104 
105 CONF = nova.conf.CONF
106 
107 LOG = logging.getLogger(__name__)
108 
109 get_notifier = functools.partial(rpc.get_notifier, service='compute')
110 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
111                                    get_notifier=get_notifier,
112                                    binary='nova-compute')
113 
114 
115 @utils.expects_func_args('migration')
116 def errors_out_migration(function):
117     """Decorator to error out migration on failure."""
118 
119     @functools.wraps(function)
120     def decorated_function(self, context, *args, **kwargs):
121         try:
122             return function(self, context, *args, **kwargs)
123         except Exception as ex:
124             with excutils.save_and_reraise_exception():
125                 wrapped_func = safe_utils.get_wrapped_function(function)
126                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
127                                                  *args, **kwargs)
128                 migration = keyed_args['migration']
129 
130                 # NOTE(rajesht): If InstanceNotFound error is thrown from
131                 # decorated function, migration status should be set to
132                 # 'error', without checking current migration status.
133                 if not isinstance(ex, exception.InstanceNotFound):
134                     status = migration.status
135                     if status not in ['migrating', 'post-migrating']:
136                         return
137 
138                 migration.status = 'error'
139                 try:
140                     with migration.obj_as_admin():
141                         migration.save()
142                 except Exception:
143                     LOG.debug('Error setting migration status '
144                               'for instance %s.',
145                               migration.instance_uuid, exc_info=True)
146 
147     return decorated_function
148 
149 
150 @utils.expects_func_args('instance')
151 def reverts_task_state(function):
152     """Decorator to revert task_state on failure."""
153 
154     @functools.wraps(function)
155     def decorated_function(self, context, *args, **kwargs):
156         try:
157             return function(self, context, *args, **kwargs)
158         except exception.UnexpectedTaskStateError as e:
159             # Note(maoy): unexpected task state means the current
160             # task is preempted. Do not clear task state in this
161             # case.
162             with excutils.save_and_reraise_exception():
163                 LOG.info(_LI("Task possibly preempted: %s"),
164                          e.format_message())
165         except Exception:
166             with excutils.save_and_reraise_exception():
167                 wrapped_func = safe_utils.get_wrapped_function(function)
168                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
169                                                  *args, **kwargs)
170                 # NOTE(mriedem): 'instance' must be in keyed_args because we
171                 # have utils.expects_func_args('instance') decorating this
172                 # method.
173                 instance = keyed_args['instance']
174                 original_task_state = instance.task_state
175                 try:
176                     self._instance_update(context, instance, task_state=None)
177                     LOG.info(_LI("Successfully reverted task state from %s on "
178                                  "failure for instance."), original_task_state,
179                                                            instance=instance)
180                 except exception.InstanceNotFound:
181                     # We might delete an instance that failed to build shortly
182                     # after it errored out this is an expected case and we
183                     # should not trace on it.
184                     pass
185                 except Exception as e:
186                     msg = _LW("Failed to revert task state for instance. "
187                               "Error: %s")
188                     LOG.warning(msg, e, instance=instance)
189 
190     return decorated_function
191 
192 
193 @utils.expects_func_args('instance')
194 def wrap_instance_fault(function):
195     """Wraps a method to catch exceptions related to instances.
196 
197     This decorator wraps a method to catch any exceptions having to do with
198     an instance that may get thrown. It then logs an instance fault in the db.
199     """
200 
201     @functools.wraps(function)
202     def decorated_function(self, context, *args, **kwargs):
203         try:
204             return function(self, context, *args, **kwargs)
205         except exception.InstanceNotFound:
206             raise
207         except Exception as e:
208             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
209             # we will get a KeyError exception which will cover up the real
210             # exception. So, we update kwargs with the values from args first.
211             # then, we can get 'instance' from kwargs easily.
212             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
213 
214             with excutils.save_and_reraise_exception():
215                 compute_utils.add_instance_fault_from_exc(context,
216                         kwargs['instance'], e, sys.exc_info())
217 
218     return decorated_function
219 
220 
221 @utils.expects_func_args('image_id', 'instance')
222 def delete_image_on_error(function):
223     """Used for snapshot related method to ensure the image created in
224     compute.api is deleted when an error occurs.
225     """
226 
227     @functools.wraps(function)
228     def decorated_function(self, context, image_id, instance,
229                            *args, **kwargs):
230         try:
231             return function(self, context, image_id, instance,
232                             *args, **kwargs)
233         except Exception:
234             with excutils.save_and_reraise_exception():
235                 LOG.debug("Cleaning up image %s", image_id,
236                           exc_info=True, instance=instance)
237                 try:
238                     self.image_api.delete(context, image_id)
239                 except exception.ImageNotFound:
240                     # Since we're trying to cleanup an image, we don't care if
241                     # if it's already gone.
242                     pass
243                 except Exception:
244                     LOG.exception(_LE("Error while trying to clean up "
245                                       "image %s"), image_id,
246                                   instance=instance)
247 
248     return decorated_function
249 
250 
251 # TODO(danms): Remove me after Icehouse
252 # TODO(alaski): Actually remove this after Newton, assuming a major RPC bump
253 # NOTE(mikal): if the method being decorated has more than one decorator, then
254 # put this one first. Otherwise the various exception handling decorators do
255 # not function correctly.
256 def object_compat(function):
257     """Wraps a method that expects a new-world instance
258 
259     This provides compatibility for callers passing old-style dict
260     instances.
261     """
262 
263     @functools.wraps(function)
264     def decorated_function(self, context, *args, **kwargs):
265         def _load_instance(instance_or_dict):
266             if isinstance(instance_or_dict, dict):
267                 # try to get metadata and system_metadata for most cases but
268                 # only attempt to load those if the db instance already has
269                 # those fields joined
270                 metas = [meta for meta in ('metadata', 'system_metadata')
271                          if meta in instance_or_dict]
272                 instance = objects.Instance._from_db_object(
273                     context, objects.Instance(), instance_or_dict,
274                     expected_attrs=metas)
275                 instance._context = context
276                 return instance
277             return instance_or_dict
278 
279         try:
280             kwargs['instance'] = _load_instance(kwargs['instance'])
281         except KeyError:
282             args = (_load_instance(args[0]),) + args[1:]
283 
284         migration = kwargs.get('migration')
285         if isinstance(migration, dict):
286             migration = objects.Migration._from_db_object(
287                     context.elevated(), objects.Migration(),
288                     migration)
289             kwargs['migration'] = migration
290 
291         return function(self, context, *args, **kwargs)
292 
293     return decorated_function
294 
295 
296 class InstanceEvents(object):
297     def __init__(self):
298         self._events = {}
299 
300     @staticmethod
301     def _lock_name(instance):
302         return '%s-%s' % (instance.uuid, 'events')
303 
304     def prepare_for_instance_event(self, instance, event_name):
305         """Prepare to receive an event for an instance.
306 
307         This will register an event for the given instance that we will
308         wait on later. This should be called before initiating whatever
309         action will trigger the event. The resulting eventlet.event.Event
310         object should be wait()'d on to ensure completion.
311 
312         :param instance: the instance for which the event will be generated
313         :param event_name: the name of the event we're expecting
314         :returns: an event object that should be wait()'d on
315         """
316         if self._events is None:
317             # NOTE(danms): We really should have a more specific error
318             # here, but this is what we use for our default error case
319             raise exception.NovaException('In shutdown, no new events '
320                                           'can be scheduled')
321 
322         @utils.synchronized(self._lock_name(instance))
323         def _create_or_get_event():
324             instance_events = self._events.setdefault(instance.uuid, {})
325             return instance_events.setdefault(event_name,
326                                               eventlet.event.Event())
327         LOG.debug('Preparing to wait for external event %(event)s',
328                   {'event': event_name}, instance=instance)
329         return _create_or_get_event()
330 
331     def pop_instance_event(self, instance, event):
332         """Remove a pending event from the wait list.
333 
334         This will remove a pending event from the wait list so that it
335         can be used to signal the waiters to wake up.
336 
337         :param instance: the instance for which the event was generated
338         :param event: the nova.objects.external_event.InstanceExternalEvent
339                       that describes the event
340         :returns: the eventlet.event.Event object on which the waiters
341                   are blocked
342         """
343         no_events_sentinel = object()
344         no_matching_event_sentinel = object()
345 
346         @utils.synchronized(self._lock_name(instance))
347         def _pop_event():
348             if not self._events:
349                 LOG.debug('Unexpected attempt to pop events during shutdown',
350                           instance=instance)
351                 return no_events_sentinel
352             events = self._events.get(instance.uuid)
353             if not events:
354                 return no_events_sentinel
355             _event = events.pop(event.key, None)
356             if not events:
357                 del self._events[instance.uuid]
358             if _event is None:
359                 return no_matching_event_sentinel
360             return _event
361 
362         result = _pop_event()
363         if result is no_events_sentinel:
364             LOG.debug('No waiting events found dispatching %(event)s',
365                       {'event': event.key},
366                       instance=instance)
367             return None
368         elif result is no_matching_event_sentinel:
369             LOG.debug('No event matching %(event)s in %(events)s',
370                       {'event': event.key,
371                        'events': self._events.get(instance.uuid, {}).keys()},
372                       instance=instance)
373             return None
374         else:
375             return result
376 
377     def clear_events_for_instance(self, instance):
378         """Remove all pending events for an instance.
379 
380         This will remove all events currently pending for an instance
381         and return them (indexed by event name).
382 
383         :param instance: the instance for which events should be purged
384         :returns: a dictionary of {event_name: eventlet.event.Event}
385         """
386         @utils.synchronized(self._lock_name(instance))
387         def _clear_events():
388             if self._events is None:
389                 LOG.debug('Unexpected attempt to clear events during shutdown',
390                           instance=instance)
391                 return dict()
392             return self._events.pop(instance.uuid, {})
393         return _clear_events()
394 
395     def cancel_all_events(self):
396         if self._events is None:
397             LOG.debug('Unexpected attempt to cancel events during shutdown.')
398             return
399         our_events = self._events
400         # NOTE(danms): Block new events
401         self._events = None
402 
403         for instance_uuid, events in our_events.items():
404             for event_name, eventlet_event in events.items():
405                 LOG.debug('Canceling in-flight event %(event)s for '
406                           'instance %(instance_uuid)s',
407                           {'event': event_name,
408                            'instance_uuid': instance_uuid})
409                 name, tag = event_name.rsplit('-', 1)
410                 event = objects.InstanceExternalEvent(
411                     instance_uuid=instance_uuid,
412                     name=name, status='failed',
413                     tag=tag, data={})
414                 eventlet_event.send(event)
415 
416 
417 class ComputeVirtAPI(virtapi.VirtAPI):
418     def __init__(self, compute):
419         super(ComputeVirtAPI, self).__init__()
420         self._compute = compute
421 
422     def _default_error_callback(self, event_name, instance):
423         raise exception.NovaException(_('Instance event failed'))
424 
425     @contextlib.contextmanager
426     def wait_for_instance_event(self, instance, event_names, deadline=300,
427                                 error_callback=None):
428         """Plan to wait for some events, run some code, then wait.
429 
430         This context manager will first create plans to wait for the
431         provided event_names, yield, and then wait for all the scheduled
432         events to complete.
433 
434         Note that this uses an eventlet.timeout.Timeout to bound the
435         operation, so callers should be prepared to catch that
436         failure and handle that situation appropriately.
437 
438         If the event is not received by the specified timeout deadline,
439         eventlet.timeout.Timeout is raised.
440 
441         If the event is received but did not have a 'completed'
442         status, a NovaException is raised.  If an error_callback is
443         provided, instead of raising an exception as detailed above
444         for the failure case, the callback will be called with the
445         event_name and instance, and can return True to continue
446         waiting for the rest of the events, False to stop processing,
447         or raise an exception which will bubble up to the waiter.
448 
449         :param instance: The instance for which an event is expected
450         :param event_names: A list of event names. Each element can be a
451                             string event name or tuple of strings to
452                             indicate (name, tag).
453         :param deadline: Maximum number of seconds we should wait for all
454                          of the specified events to arrive.
455         :param error_callback: A function to be called if an event arrives
456 
457         """
458 
459         if error_callback is None:
460             error_callback = self._default_error_callback
461         events = {}
462         for event_name in event_names:
463             if isinstance(event_name, tuple):
464                 name, tag = event_name
465                 event_name = objects.InstanceExternalEvent.make_key(
466                     name, tag)
467             try:
468                 events[event_name] = (
469                     self._compute.instance_events.prepare_for_instance_event(
470                         instance, event_name))
471             except exception.NovaException:
472                 error_callback(event_name, instance)
473                 # NOTE(danms): Don't wait for any of the events. They
474                 # should all be canceled and fired immediately below,
475                 # but don't stick around if not.
476                 deadline = 0
477         yield
478         with eventlet.timeout.Timeout(deadline):
479             for event_name, event in events.items():
480                 actual_event = event.wait()
481                 if actual_event.status == 'completed':
482                     continue
483                 decision = error_callback(event_name, instance)
484                 if decision is False:
485                     break
486 
487 
488 class ComputeManager(manager.Manager):
489     """Manages the running instances from creation to destruction."""
490 
491     target = messaging.Target(version='4.13')
492 
493     # How long to wait in seconds before re-issuing a shutdown
494     # signal to an instance during power off.  The overall
495     # time to wait is set by CONF.shutdown_timeout.
496     SHUTDOWN_RETRY_INTERVAL = 10
497 
498     def __init__(self, compute_driver=None, *args, **kwargs):
499         """Load configuration options and connect to the hypervisor."""
500         self.virtapi = ComputeVirtAPI(self)
501         self.network_api = network.API()
502         self.volume_api = cinder.API()
503         self.image_api = image.API()
504         self._last_host_check = 0
505         self._last_bw_usage_poll = 0
506         self._bw_usage_supported = True
507         self._last_bw_usage_cell_update = 0
508         self.compute_api = compute.API()
509         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
510         self.conductor_api = conductor.API()
511         self.compute_task_api = conductor.ComputeTaskAPI()
512         self.is_neutron_security_groups = (
513             openstack_driver.is_neutron_security_groups())
514         self.consoleauth_rpcapi = consoleauth.rpcapi.ConsoleAuthAPI()
515         self.cells_rpcapi = cells_rpcapi.CellsAPI()
516         self.scheduler_client = scheduler_client.SchedulerClient()
517         self._resource_tracker = None
518         self.instance_events = InstanceEvents()
519         self._sync_power_pool = eventlet.GreenPool(
520             size=CONF.sync_power_state_pool_size)
521         self._syncs_in_progress = {}
522         self.send_instance_updates = (
523             CONF.filter_scheduler.track_instance_changes)
524         if CONF.max_concurrent_builds != 0:
525             self._build_semaphore = eventlet.semaphore.Semaphore(
526                 CONF.max_concurrent_builds)
527         else:
528             self._build_semaphore = compute_utils.UnlimitedSemaphore()
529         if max(CONF.max_concurrent_live_migrations, 0) != 0:
530             self._live_migration_semaphore = eventlet.semaphore.Semaphore(
531                 CONF.max_concurrent_live_migrations)
532         else:
533             self._live_migration_semaphore = compute_utils.UnlimitedSemaphore()
534 
535         super(ComputeManager, self).__init__(service_name="compute",
536                                              *args, **kwargs)
537 
538         # NOTE(russellb) Load the driver last.  It may call back into the
539         # compute manager via the virtapi, so we want it to be fully
540         # initialized before that happens.
541         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
542         self.use_legacy_block_device_info = \
543                             self.driver.need_legacy_block_device_info
544 
545     def reset(self):
546         LOG.info(_LI('Reloading compute RPC API'))
547         compute_rpcapi.LAST_VERSION = None
548         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
549 
550     def _get_resource_tracker(self):
551         if not self._resource_tracker:
552             rt = resource_tracker.ResourceTracker(self.host, self.driver)
553             self._resource_tracker = rt
554         return self._resource_tracker
555 
556     def _update_resource_tracker(self, context, instance):
557         """Let the resource tracker know that an instance has changed state."""
558 
559         if instance.host == self.host:
560             rt = self._get_resource_tracker()
561             rt.update_usage(context, instance, instance.node)
562 
563     def _instance_update(self, context, instance, **kwargs):
564         """Update an instance in the database using kwargs as value."""
565 
566         for k, v in kwargs.items():
567             setattr(instance, k, v)
568         instance.save()
569         self._update_resource_tracker(context, instance)
570 
571     def _nil_out_instance_obj_host_and_node(self, instance):
572         # NOTE(jwcroppe): We don't do instance.save() here for performance
573         # reasons; a call to this is expected to be immediately followed by
574         # another call that does instance.save(), thus avoiding two writes
575         # to the database layer.
576         instance.host = None
577         instance.node = None
578 
579     def _set_instance_obj_error_state(self, context, instance,
580                                       clean_task_state=False):
581         try:
582             instance.vm_state = vm_states.ERROR
583             if clean_task_state:
584                 instance.task_state = None
585             instance.save()
586         except exception.InstanceNotFound:
587             LOG.debug('Instance has been destroyed from under us while '
588                       'trying to set it to ERROR', instance=instance)
589 
590     def _get_instances_on_driver(self, context, filters=None):
591         """Return a list of instance records for the instances found
592         on the hypervisor which satisfy the specified filters. If filters=None
593         return a list of instance records for all the instances found on the
594         hypervisor.
595         """
596         if not filters:
597             filters = {}
598         try:
599             driver_uuids = self.driver.list_instance_uuids()
600             if len(driver_uuids) == 0:
601                 # Short circuit, don't waste a DB call
602                 return objects.InstanceList()
603             filters['uuid'] = driver_uuids
604             local_instances = objects.InstanceList.get_by_filters(
605                 context, filters, use_slave=True)
606             return local_instances
607         except NotImplementedError:
608             pass
609 
610         # The driver doesn't support uuids listing, so we'll have
611         # to brute force.
612         driver_instances = self.driver.list_instances()
613         instances = objects.InstanceList.get_by_filters(context, filters,
614                                                         use_slave=True)
615         name_map = {instance.name: instance for instance in instances}
616         local_instances = []
617         for driver_instance in driver_instances:
618             instance = name_map.get(driver_instance)
619             if not instance:
620                 continue
621             local_instances.append(instance)
622         return local_instances
623 
624     def _destroy_evacuated_instances(self, context):
625         """Destroys evacuated instances.
626 
627         While nova-compute was down, the instances running on it could be
628         evacuated to another host. Check that the instances reported
629         by the driver are still associated with this host.  If they are
630         not, destroy them, with the exception of instances which are in
631         the MIGRATING, RESIZE_MIGRATING, RESIZE_MIGRATED, RESIZE_FINISH
632         task state or RESIZED vm state.
633         """
634         filters = {
635             'source_compute': self.host,
636             'status': ['accepted', 'done'],
637             'migration_type': 'evacuation',
638         }
639         evacuations = objects.MigrationList.get_by_filters(context, filters)
640         if not evacuations:
641             return
642         evacuations = {mig.instance_uuid: mig for mig in evacuations}
643 
644         filters = {'deleted': False}
645         local_instances = self._get_instances_on_driver(context, filters)
646         evacuated = [inst for inst in local_instances
647                      if inst.uuid in evacuations]
648         for instance in evacuated:
649             migration = evacuations[instance.uuid]
650             LOG.info(_LI('Deleting instance as it has been evacuated from '
651                          'this host'), instance=instance)
652             try:
653                 network_info = self.network_api.get_instance_nw_info(
654                     context, instance)
655                 bdi = self._get_instance_block_device_info(context,
656                                                            instance)
657                 destroy_disks = not (self._is_instance_storage_shared(
658                     context, instance))
659             except exception.InstanceNotFound:
660                 network_info = network_model.NetworkInfo()
661                 bdi = {}
662                 LOG.info(_LI('Instance has been marked deleted already, '
663                              'removing it from the hypervisor.'),
664                          instance=instance)
665                 # always destroy disks if the instance was deleted
666                 destroy_disks = True
667             self.driver.destroy(context, instance,
668                                 network_info,
669                                 bdi, destroy_disks)
670             migration.status = 'completed'
671             migration.save()
672 
673     def _is_instance_storage_shared(self, context, instance, host=None):
674         shared_storage = True
675         data = None
676         try:
677             data = self.driver.check_instance_shared_storage_local(context,
678                                                        instance)
679             if data:
680                 shared_storage = (self.compute_rpcapi.
681                                   check_instance_shared_storage(context,
682                                   instance, data, host=host))
683         except NotImplementedError:
684             LOG.debug('Hypervisor driver does not support '
685                       'instance shared storage check, '
686                       'assuming it\'s not on shared storage',
687                       instance=instance)
688             shared_storage = False
689         except Exception:
690             LOG.exception(_LE('Failed to check if instance shared'),
691                       instance=instance)
692         finally:
693             if data:
694                 self.driver.check_instance_shared_storage_cleanup(context,
695                                                                   data)
696         return shared_storage
697 
698     def _complete_partial_deletion(self, context, instance):
699         """Complete deletion for instances in DELETED status but not marked as
700         deleted in the DB
701         """
702         system_meta = instance.system_metadata
703         instance.destroy()
704         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
705                 context, instance.uuid)
706         quotas = objects.Quotas(context=context)
707         project_id, user_id = objects.quotas.ids_from_instance(context,
708                                                                instance)
709         quotas.reserve(project_id=project_id, user_id=user_id, instances=-1,
710                        cores=-instance.flavor.vcpus,
711                        ram=-instance.flavor.memory_mb)
712         self._complete_deletion(context,
713                                 instance,
714                                 bdms,
715                                 quotas,
716                                 system_meta)
717 
718     def _complete_deletion(self, context, instance, bdms,
719                            quotas, system_meta):
720         if quotas:
721             quotas.commit()
722 
723         # ensure block device mappings are not leaked
724         for bdm in bdms:
725             bdm.destroy()
726 
727         self._update_resource_tracker(context, instance)
728         self._notify_about_instance_usage(context, instance, "delete.end",
729                 system_metadata=system_meta)
730         compute_utils.notify_about_instance_action(context, instance,
731                 self.host, action=fields.NotificationAction.DELETE,
732                 phase=fields.NotificationPhase.END)
733         self._clean_instance_console_tokens(context, instance)
734         self._delete_scheduler_instance_info(context, instance.uuid)
735 
736     def _create_reservations(self, context, instance, project_id, user_id):
737         vcpus = instance.flavor.vcpus
738         mem_mb = instance.flavor.memory_mb
739 
740         quotas = objects.Quotas(context=context)
741         quotas.reserve(project_id=project_id,
742                        user_id=user_id,
743                        instances=-1,
744                        cores=-vcpus,
745                        ram=-mem_mb)
746         return quotas
747 
748     def _init_instance(self, context, instance):
749         '''Initialize this instance during service init.'''
750 
751         # NOTE(danms): If the instance appears to not be owned by this
752         # host, it may have been evacuated away, but skipped by the
753         # evacuation cleanup code due to configuration. Thus, if that
754         # is a possibility, don't touch the instance in any way, but
755         # log the concern. This will help avoid potential issues on
756         # startup due to misconfiguration.
757         if instance.host != self.host:
758             LOG.warning(_LW('Instance %(uuid)s appears to not be owned '
759                             'by this host, but by %(host)s. Startup '
760                             'processing is being skipped.'),
761                         {'uuid': instance.uuid,
762                          'host': instance.host})
763             return
764 
765         # Instances that are shut down, or in an error state can not be
766         # initialized and are not attempted to be recovered. The exception
767         # to this are instances that are in RESIZE_MIGRATING or DELETING,
768         # which are dealt with further down.
769         if (instance.vm_state == vm_states.SOFT_DELETED or
770             (instance.vm_state == vm_states.ERROR and
771             instance.task_state not in
772             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
773             LOG.debug("Instance is in %s state.",
774                       instance.vm_state, instance=instance)
775             return
776 
777         if instance.vm_state == vm_states.DELETED:
778             try:
779                 self._complete_partial_deletion(context, instance)
780             except Exception:
781                 # we don't want that an exception blocks the init_host
782                 msg = _LE('Failed to complete a deletion')
783                 LOG.exception(msg, instance=instance)
784             return
785 
786         if (instance.vm_state == vm_states.BUILDING or
787             instance.task_state in [task_states.SCHEDULING,
788                                     task_states.BLOCK_DEVICE_MAPPING,
789                                     task_states.NETWORKING,
790                                     task_states.SPAWNING]):
791             # NOTE(dave-mcnally) compute stopped before instance was fully
792             # spawned so set to ERROR state. This is safe to do as the state
793             # may be set by the api but the host is not so if we get here the
794             # instance has already been scheduled to this particular host.
795             LOG.debug("Instance failed to spawn correctly, "
796                       "setting to ERROR state", instance=instance)
797             instance.task_state = None
798             instance.vm_state = vm_states.ERROR
799             instance.save()
800             return
801 
802         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
803             instance.task_state in [task_states.REBUILDING,
804                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
805                                     task_states.REBUILD_SPAWNING]):
806             # NOTE(jichenjc) compute stopped before instance was fully
807             # spawned so set to ERROR state. This is consistent to BUILD
808             LOG.debug("Instance failed to rebuild correctly, "
809                       "setting to ERROR state", instance=instance)
810             instance.task_state = None
811             instance.vm_state = vm_states.ERROR
812             instance.save()
813             return
814 
815         if (instance.vm_state != vm_states.ERROR and
816             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
817                                     task_states.IMAGE_PENDING_UPLOAD,
818                                     task_states.IMAGE_UPLOADING,
819                                     task_states.IMAGE_SNAPSHOT]):
820             LOG.debug("Instance in transitional state %s at start-up "
821                       "clearing task state",
822                       instance.task_state, instance=instance)
823             try:
824                 self._post_interrupted_snapshot_cleanup(context, instance)
825             except Exception:
826                 # we don't want that an exception blocks the init_host
827                 msg = _LE('Failed to cleanup snapshot.')
828                 LOG.exception(msg, instance=instance)
829             instance.task_state = None
830             instance.save()
831 
832         if (instance.vm_state != vm_states.ERROR and
833             instance.task_state in [task_states.RESIZE_PREP]):
834             LOG.debug("Instance in transitional state %s at start-up "
835                       "clearing task state",
836                       instance['task_state'], instance=instance)
837             instance.task_state = None
838             instance.save()
839 
840         if instance.task_state == task_states.DELETING:
841             try:
842                 LOG.info(_LI('Service started deleting the instance during '
843                              'the previous run, but did not finish. Restarting'
844                              ' the deletion now.'), instance=instance)
845                 instance.obj_load_attr('metadata')
846                 instance.obj_load_attr('system_metadata')
847                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
848                         context, instance.uuid)
849                 project_id, user_id = objects.quotas.ids_from_instance(
850                     context, instance)
851                 quotas = self._create_reservations(context, instance,
852                                                    project_id, user_id)
853 
854                 self._delete_instance(context, instance, bdms, quotas)
855             except Exception:
856                 # we don't want that an exception blocks the init_host
857                 msg = _LE('Failed to complete a deletion')
858                 LOG.exception(msg, instance=instance)
859                 self._set_instance_obj_error_state(context, instance)
860             return
861 
862         current_power_state = self._get_power_state(context, instance)
863         try_reboot, reboot_type = self._retry_reboot(context, instance,
864                                                      current_power_state)
865 
866         if try_reboot:
867             LOG.debug("Instance in transitional state (%(task_state)s) at "
868                       "start-up and power state is (%(power_state)s), "
869                       "triggering reboot",
870                       {'task_state': instance.task_state,
871                        'power_state': current_power_state},
872                       instance=instance)
873 
874             # NOTE(mikal): if the instance was doing a soft reboot that got as
875             # far as shutting down the instance but not as far as starting it
876             # again, then we've just become a hard reboot. That means the
877             # task state for the instance needs to change so that we're in one
878             # of the expected task states for a hard reboot.
879             soft_types = [task_states.REBOOT_STARTED,
880                           task_states.REBOOT_PENDING,
881                           task_states.REBOOTING]
882             if instance.task_state in soft_types and reboot_type == 'HARD':
883                 instance.task_state = task_states.REBOOT_PENDING_HARD
884                 instance.save()
885 
886             self.reboot_instance(context, instance, block_device_info=None,
887                                  reboot_type=reboot_type)
888             return
889 
890         elif (current_power_state == power_state.RUNNING and
891               instance.task_state in [task_states.REBOOT_STARTED,
892                                       task_states.REBOOT_STARTED_HARD,
893                                       task_states.PAUSING,
894                                       task_states.UNPAUSING]):
895             LOG.warning(_LW("Instance in transitional state "
896                             "(%(task_state)s) at start-up and power state "
897                             "is (%(power_state)s), clearing task state"),
898                         {'task_state': instance.task_state,
899                          'power_state': current_power_state},
900                         instance=instance)
901             instance.task_state = None
902             instance.vm_state = vm_states.ACTIVE
903             instance.save()
904         elif (current_power_state == power_state.PAUSED and
905               instance.task_state == task_states.UNPAUSING):
906             LOG.warning(_LW("Instance in transitional state "
907                             "(%(task_state)s) at start-up and power state "
908                             "is (%(power_state)s), clearing task state "
909                             "and unpausing the instance"),
910                         {'task_state': instance.task_state,
911                          'power_state': current_power_state},
912                         instance=instance)
913             try:
914                 self.unpause_instance(context, instance)
915             except NotImplementedError:
916                 # Some virt driver didn't support pause and unpause
917                 pass
918             except Exception:
919                 LOG.exception(_LE('Failed to unpause instance'),
920                               instance=instance)
921             return
922 
923         if instance.task_state == task_states.POWERING_OFF:
924             try:
925                 LOG.debug("Instance in transitional state %s at start-up "
926                           "retrying stop request",
927                           instance.task_state, instance=instance)
928                 self.stop_instance(context, instance, True)
929             except Exception:
930                 # we don't want that an exception blocks the init_host
931                 msg = _LE('Failed to stop instance')
932                 LOG.exception(msg, instance=instance)
933             return
934 
935         if instance.task_state == task_states.POWERING_ON:
936             try:
937                 LOG.debug("Instance in transitional state %s at start-up "
938                           "retrying start request",
939                           instance.task_state, instance=instance)
940                 self.start_instance(context, instance)
941             except Exception:
942                 # we don't want that an exception blocks the init_host
943                 msg = _LE('Failed to start instance')
944                 LOG.exception(msg, instance=instance)
945             return
946 
947         net_info = compute_utils.get_nw_info_for_instance(instance)
948         try:
949             self.driver.plug_vifs(instance, net_info)
950         except NotImplementedError as e:
951             LOG.debug(e, instance=instance)
952         except exception.VirtualInterfacePlugException:
953             # we don't want an exception to block the init_host
954             LOG.exception(_LE("Vifs plug failed"), instance=instance)
955             self._set_instance_obj_error_state(context, instance)
956             return
957 
958         if instance.task_state == task_states.RESIZE_MIGRATING:
959             # We crashed during resize/migration, so roll back for safety
960             try:
961                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
962                 # not in system_metadata we default to True for backwards
963                 # compatibility
964                 power_on = (instance.system_metadata.get('old_vm_state') !=
965                             vm_states.STOPPED)
966 
967                 block_dev_info = self._get_instance_block_device_info(context,
968                                                                       instance)
969 
970                 self.driver.finish_revert_migration(context,
971                     instance, net_info, block_dev_info, power_on)
972 
973             except Exception:
974                 LOG.exception(_LE('Failed to revert crashed migration'),
975                               instance=instance)
976             finally:
977                 LOG.info(_LI('Instance found in migrating state during '
978                              'startup. Resetting task_state'),
979                          instance=instance)
980                 instance.task_state = None
981                 instance.save()
982         if instance.task_state == task_states.MIGRATING:
983             # Live migration did not complete, but instance is on this
984             # host, so reset the state.
985             instance.task_state = None
986             instance.save(expected_task_state=[task_states.MIGRATING])
987 
988         db_state = instance.power_state
989         drv_state = self._get_power_state(context, instance)
990         expect_running = (db_state == power_state.RUNNING and
991                           drv_state != db_state)
992 
993         LOG.debug('Current state is %(drv_state)s, state in DB is '
994                   '%(db_state)s.',
995                   {'drv_state': drv_state, 'db_state': db_state},
996                   instance=instance)
997 
998         if expect_running and CONF.resume_guests_state_on_host_boot:
999             LOG.info(_LI('Rebooting instance after nova-compute restart.'),
1000                      instance=instance)
1001 
1002             block_device_info = \
1003                 self._get_instance_block_device_info(context, instance)
1004 
1005             try:
1006                 self.driver.resume_state_on_host_boot(
1007                     context, instance, net_info, block_device_info)
1008             except NotImplementedError:
1009                 LOG.warning(_LW('Hypervisor driver does not support '
1010                                 'resume guests'), instance=instance)
1011             except Exception:
1012                 # NOTE(vish): The instance failed to resume, so we set the
1013                 #             instance to error and attempt to continue.
1014                 LOG.warning(_LW('Failed to resume instance'),
1015                             instance=instance)
1016                 self._set_instance_obj_error_state(context, instance)
1017 
1018         elif drv_state == power_state.RUNNING:
1019             # VMwareAPI drivers will raise an exception
1020             try:
1021                 self.driver.ensure_filtering_rules_for_instance(
1022                                        instance, net_info)
1023             except NotImplementedError:
1024                 LOG.debug('Hypervisor driver does not support '
1025                           'firewall rules', instance=instance)
1026 
1027     def _retry_reboot(self, context, instance, current_power_state):
1028         current_task_state = instance.task_state
1029         retry_reboot = False
1030         reboot_type = compute_utils.get_reboot_type(current_task_state,
1031                                                     current_power_state)
1032 
1033         pending_soft = (current_task_state == task_states.REBOOT_PENDING and
1034                         instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1035         pending_hard = (current_task_state == task_states.REBOOT_PENDING_HARD
1036                         and instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1037         started_not_running = (current_task_state in
1038                                [task_states.REBOOT_STARTED,
1039                                 task_states.REBOOT_STARTED_HARD] and
1040                                current_power_state != power_state.RUNNING)
1041 
1042         if pending_soft or pending_hard or started_not_running:
1043             retry_reboot = True
1044 
1045         return retry_reboot, reboot_type
1046 
1047     def handle_lifecycle_event(self, event):
1048         LOG.info(_LI("VM %(state)s (Lifecycle Event)"),
1049                  {'state': event.get_name()},
1050                  instance_uuid=event.get_instance_uuid())
1051         context = nova.context.get_admin_context(read_deleted='yes')
1052         instance = objects.Instance.get_by_uuid(context,
1053                                                 event.get_instance_uuid(),
1054                                                 expected_attrs=[])
1055         vm_power_state = None
1056         if event.get_transition() == virtevent.EVENT_LIFECYCLE_STOPPED:
1057             vm_power_state = power_state.SHUTDOWN
1058         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_STARTED:
1059             vm_power_state = power_state.RUNNING
1060         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_PAUSED:
1061             vm_power_state = power_state.PAUSED
1062         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_RESUMED:
1063             vm_power_state = power_state.RUNNING
1064         elif event.get_transition() == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1065             vm_power_state = power_state.SUSPENDED
1066         else:
1067             LOG.warning(_LW("Unexpected power state %d"),
1068                         event.get_transition())
1069 
1070         # Note(lpetrut): The event may be delayed, thus not reflecting
1071         # the current instance power state. In that case, ignore the event.
1072         current_power_state = self._get_power_state(context, instance)
1073         if current_power_state == vm_power_state:
1074             LOG.debug('Synchronizing instance power state after lifecycle '
1075                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1076                       'current task_state: %(task_state)s, current DB '
1077                       'power_state: %(db_power_state)s, VM power_state: '
1078                       '%(vm_power_state)s',
1079                       {'event': event.get_name(),
1080                        'vm_state': instance.vm_state,
1081                        'task_state': instance.task_state,
1082                        'db_power_state': instance.power_state,
1083                        'vm_power_state': vm_power_state},
1084                       instance_uuid=instance.uuid)
1085             self._sync_instance_power_state(context,
1086                                             instance,
1087                                             vm_power_state)
1088 
1089     def handle_events(self, event):
1090         if isinstance(event, virtevent.LifecycleEvent):
1091             try:
1092                 self.handle_lifecycle_event(event)
1093             except exception.InstanceNotFound:
1094                 LOG.debug("Event %s arrived for non-existent instance. The "
1095                           "instance was probably deleted.", event)
1096         else:
1097             LOG.debug("Ignoring event %s", event)
1098 
1099     def init_virt_events(self):
1100         if CONF.workarounds.handle_virt_lifecycle_events:
1101             self.driver.register_event_listener(self.handle_events)
1102         else:
1103             # NOTE(mriedem): If the _sync_power_states periodic task is
1104             # disabled we should emit a warning in the logs.
1105             if CONF.sync_power_state_interval < 0:
1106                 LOG.warning(_LW('Instance lifecycle events from the compute '
1107                              'driver have been disabled. Note that lifecycle '
1108                              'changes to an instance outside of the compute '
1109                              'service will not be synchronized '
1110                              'automatically since the _sync_power_states '
1111                              'periodic task is also disabled.'))
1112             else:
1113                 LOG.info(_LI('Instance lifecycle events from the compute '
1114                              'driver have been disabled. Note that lifecycle '
1115                              'changes to an instance outside of the compute '
1116                              'service will only be synchronized by the '
1117                              '_sync_power_states periodic task.'))
1118 
1119     def init_host(self):
1120         """Initialization for a standalone compute service."""
1121 
1122         if CONF.pci.passthrough_whitelist:
1123             # Simply loading the PCI passthrough whitelist will do a bunch of
1124             # validation that would otherwise wait until the PciDevTracker is
1125             # constructed when updating available resources for the compute
1126             # node(s) in the resource tracker, effectively killing that task.
1127             # So load up the whitelist when starting the compute service to
1128             # flush any invalid configuration early so we can kill the service
1129             # if the configuration is wrong.
1130             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1131 
1132         self.driver.init_host(host=self.host)
1133         context = nova.context.get_admin_context()
1134         instances = objects.InstanceList.get_by_host(
1135             context, self.host, expected_attrs=['info_cache', 'metadata'])
1136 
1137         if CONF.defer_iptables_apply:
1138             self.driver.filter_defer_apply_on()
1139 
1140         # NOTE(sbauza): We want the compute node to hard fail if it can't be
1141         # able to provide its resources to the placement API, or it would not
1142         # be able to be eligible as a destination.
1143         if CONF.placement.password is None:
1144                 raise exception.PlacementNotConfigured()
1145 
1146         self.init_virt_events()
1147 
1148         try:
1149             # checking that instance was not already evacuated to other host
1150             self._destroy_evacuated_instances(context)
1151             for instance in instances:
1152                 self._init_instance(context, instance)
1153         finally:
1154             if CONF.defer_iptables_apply:
1155                 self.driver.filter_defer_apply_off()
1156             self._update_scheduler_instance_info(context, instances)
1157 
1158     def cleanup_host(self):
1159         self.driver.register_event_listener(None)
1160         self.instance_events.cancel_all_events()
1161         self.driver.cleanup_host(host=self.host)
1162 
1163     def pre_start_hook(self):
1164         """After the service is initialized, but before we fully bring
1165         the service up by listening on RPC queues, make sure to update
1166         our available resources (and indirectly our available nodes).
1167         """
1168         self.update_available_resource(nova.context.get_admin_context())
1169 
1170     def _get_power_state(self, context, instance):
1171         """Retrieve the power state for the given instance."""
1172         LOG.debug('Checking state', instance=instance)
1173         try:
1174             return self.driver.get_info(instance).state
1175         except exception.InstanceNotFound:
1176             return power_state.NOSTATE
1177 
1178     def get_console_topic(self, context):
1179         """Retrieves the console host for a project on this host.
1180 
1181         Currently this is just set in the flags for each compute host.
1182 
1183         """
1184         # TODO(mdragon): perhaps make this variable by console_type?
1185         return '%s.%s' % (CONF.console_topic, CONF.console_host)
1186 
1187     @wrap_exception()
1188     def get_console_pool_info(self, context, console_type):
1189         return self.driver.get_console_pool_info(console_type)
1190 
1191     # NOTE(hanlind): This and the virt method it calls can be removed in
1192     # version 5.0 of the RPC API
1193     @wrap_exception()
1194     def refresh_security_group_rules(self, context, security_group_id):
1195         """Tell the virtualization driver to refresh security group rules.
1196 
1197         Passes straight through to the virtualization driver.
1198 
1199         """
1200         return self.driver.refresh_security_group_rules(security_group_id)
1201 
1202     # TODO(alaski): Remove object_compat for RPC version 5.0
1203     @object_compat
1204     @wrap_exception()
1205     def refresh_instance_security_rules(self, context, instance):
1206         """Tell the virtualization driver to refresh security rules for
1207         an instance.
1208 
1209         Passes straight through to the virtualization driver.
1210 
1211         Synchronize the call because we may still be in the middle of
1212         creating the instance.
1213         """
1214         @utils.synchronized(instance.uuid)
1215         def _sync_refresh():
1216             try:
1217                 return self.driver.refresh_instance_security_rules(instance)
1218             except NotImplementedError:
1219                 LOG.debug('Hypervisor driver does not support '
1220                           'security groups.', instance=instance)
1221 
1222         return _sync_refresh()
1223 
1224     def _await_block_device_map_created(self, context, vol_id):
1225         # TODO(yamahata): creating volume simultaneously
1226         #                 reduces creation time?
1227         # TODO(yamahata): eliminate dumb polling
1228         start = time.time()
1229         retries = CONF.block_device_allocate_retries
1230         if retries < 0:
1231             LOG.warning(_LW("Treating negative config value (%(retries)s) for "
1232                             "'block_device_retries' as 0."),
1233                         {'retries': retries})
1234         # (1) treat  negative config value as 0
1235         # (2) the configured value is 0, one attempt should be made
1236         # (3) the configured value is > 0, then the total number attempts
1237         #      is (retries + 1)
1238         attempts = 1
1239         if retries >= 1:
1240             attempts = retries + 1
1241         for attempt in range(1, attempts + 1):
1242             volume = self.volume_api.get(context, vol_id)
1243             volume_status = volume['status']
1244             if volume_status not in ['creating', 'downloading']:
1245                 if volume_status == 'available':
1246                     return attempt
1247                 LOG.warning(_LW("Volume id: %(vol_id)s finished being "
1248                                 "created but its status is %(vol_status)s."),
1249                             {'vol_id': vol_id,
1250                              'vol_status': volume_status})
1251                 break
1252             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1253         raise exception.VolumeNotCreated(volume_id=vol_id,
1254                                          seconds=int(time.time() - start),
1255                                          attempts=attempt,
1256                                          volume_status=volume_status)
1257 
1258     def _decode_files(self, injected_files):
1259         """Base64 decode the list of files to inject."""
1260         if not injected_files:
1261             return []
1262 
1263         def _decode(f):
1264             path, contents = f
1265             # Py3 raises binascii.Error instead of TypeError as in Py27
1266             try:
1267                 decoded = base64.b64decode(contents)
1268                 return path, decoded
1269             except (TypeError, binascii.Error):
1270                 raise exception.Base64Exception(path=path)
1271 
1272         return [_decode(f) for f in injected_files]
1273 
1274     def _validate_instance_group_policy(self, context, instance,
1275             filter_properties):
1276         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1277         # However, there is a race condition with the enforcement of
1278         # the policy.  Since more than one instance may be scheduled at the
1279         # same time, it's possible that more than one instance with an
1280         # anti-affinity policy may end up here.  It's also possible that
1281         # multiple instances with an affinity policy could end up on different
1282         # hosts.  This is a validation step to make sure that starting the
1283         # instance here doesn't violate the policy.
1284 
1285         scheduler_hints = filter_properties.get('scheduler_hints') or {}
1286         group_hint = scheduler_hints.get('group')
1287         if not group_hint:
1288             return
1289 
1290         @utils.synchronized(group_hint)
1291         def _do_validation(context, instance, group_hint):
1292             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1293             if 'anti-affinity' in group.policies:
1294                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1295                 if self.host in group_hosts:
1296                     msg = _("Anti-affinity instance group policy "
1297                             "was violated.")
1298                     raise exception.RescheduledException(
1299                             instance_uuid=instance.uuid,
1300                             reason=msg)
1301             elif 'affinity' in group.policies:
1302                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1303                 if group_hosts and self.host not in group_hosts:
1304                     msg = _("Affinity instance group policy was violated.")
1305                     raise exception.RescheduledException(
1306                             instance_uuid=instance.uuid,
1307                             reason=msg)
1308 
1309         _do_validation(context, instance, group_hint)
1310 
1311     def _log_original_error(self, exc_info, instance_uuid):
1312         LOG.error(_LE('Error: %s'), exc_info[1], instance_uuid=instance_uuid,
1313                   exc_info=exc_info)
1314 
1315     def _reschedule(self, context, request_spec, filter_properties,
1316             instance, reschedule_method, method_args, task_state,
1317             exc_info=None):
1318         """Attempt to re-schedule a compute operation."""
1319 
1320         instance_uuid = instance.uuid
1321         retry = filter_properties.get('retry')
1322         if not retry:
1323             # no retry information, do not reschedule.
1324             LOG.debug("Retry info not present, will not reschedule",
1325                       instance_uuid=instance_uuid)
1326             return
1327 
1328         if not request_spec:
1329             LOG.debug("No request spec, will not reschedule",
1330                       instance_uuid=instance_uuid)
1331             return
1332 
1333         LOG.debug("Re-scheduling %(method)s: attempt %(num)d",
1334                   {'method': reschedule_method.__name__,
1335                    'num': retry['num_attempts']}, instance_uuid=instance_uuid)
1336 
1337         # reset the task state:
1338         self._instance_update(context, instance, task_state=task_state)
1339 
1340         if exc_info:
1341             # stringify to avoid circular ref problem in json serialization:
1342             retry['exc'] = traceback.format_exception_only(exc_info[0],
1343                                     exc_info[1])
1344 
1345         reschedule_method(context, *method_args)
1346         return True
1347 
1348     @periodic_task.periodic_task
1349     def _check_instance_build_time(self, context):
1350         """Ensure that instances are not stuck in build."""
1351         timeout = CONF.instance_build_timeout
1352         if timeout == 0:
1353             return
1354 
1355         filters = {'vm_state': vm_states.BUILDING,
1356                    'host': self.host}
1357 
1358         building_insts = objects.InstanceList.get_by_filters(context,
1359                            filters, expected_attrs=[], use_slave=True)
1360 
1361         for instance in building_insts:
1362             if timeutils.is_older_than(instance.created_at, timeout):
1363                 self._set_instance_obj_error_state(context, instance)
1364                 LOG.warning(_LW("Instance build timed out. Set to error "
1365                                 "state."), instance=instance)
1366 
1367     def _check_instance_exists(self, context, instance):
1368         """Ensure an instance with the same name is not already present."""
1369         if self.driver.instance_exists(instance):
1370             raise exception.InstanceExists(name=instance.name)
1371 
1372     def _allocate_network_async(self, context, instance, requested_networks,
1373                                 macs, security_groups, is_vpn, dhcp_options):
1374         """Method used to allocate networks in the background.
1375 
1376         Broken out for testing.
1377         """
1378         # First check to see if we're specifically not supposed to allocate
1379         # networks because if so, we can exit early.
1380         if requested_networks and requested_networks.no_allocate:
1381             LOG.debug("Not allocating networking since 'none' was specified.",
1382                       instance=instance)
1383             return network_model.NetworkInfo([])
1384 
1385         LOG.debug("Allocating IP information in the background.",
1386                   instance=instance)
1387         retries = CONF.network_allocate_retries
1388         attempts = retries + 1
1389         retry_time = 1
1390         bind_host_id = self.driver.network_binding_host_id(context, instance)
1391         for attempt in range(1, attempts + 1):
1392             try:
1393                 nwinfo = self.network_api.allocate_for_instance(
1394                         context, instance, vpn=is_vpn,
1395                         requested_networks=requested_networks,
1396                         macs=macs,
1397                         security_groups=security_groups,
1398                         dhcp_options=dhcp_options,
1399                         bind_host_id=bind_host_id)
1400                 LOG.debug('Instance network_info: |%s|', nwinfo,
1401                           instance=instance)
1402                 instance.system_metadata['network_allocated'] = 'True'
1403                 # NOTE(JoshNang) do not save the instance here, as it can cause
1404                 # races. The caller shares a reference to instance and waits
1405                 # for this async greenthread to finish before calling
1406                 # instance.save().
1407                 return nwinfo
1408             except Exception:
1409                 exc_info = sys.exc_info()
1410                 log_info = {'attempt': attempt,
1411                             'attempts': attempts}
1412                 if attempt == attempts:
1413                     LOG.exception(_LE('Instance failed network setup '
1414                                       'after %(attempts)d attempt(s)'),
1415                                   log_info)
1416                     six.reraise(*exc_info)
1417                 LOG.warning(_LW('Instance failed network setup '
1418                                 '(attempt %(attempt)d of %(attempts)d)'),
1419                             log_info, instance=instance)
1420                 time.sleep(retry_time)
1421                 retry_time *= 2
1422                 if retry_time > 30:
1423                     retry_time = 30
1424         # Not reached.
1425 
1426     def _build_networks_for_instance(self, context, instance,
1427             requested_networks, security_groups):
1428 
1429         # If we're here from a reschedule the network may already be allocated.
1430         if strutils.bool_from_string(
1431                 instance.system_metadata.get('network_allocated', 'False')):
1432             # NOTE(alex_xu): The network_allocated is True means the network
1433             # resource already allocated at previous scheduling, and the
1434             # network setup is cleanup at previous. After rescheduling, the
1435             # network resource need setup on the new host.
1436             self.network_api.setup_instance_network_on_host(
1437                 context, instance, instance.host)
1438             return self.network_api.get_instance_nw_info(context, instance)
1439 
1440         if not self.is_neutron_security_groups:
1441             security_groups = []
1442 
1443         macs = self.driver.macs_for_instance(instance)
1444         dhcp_options = self.driver.dhcp_options_for_instance(instance)
1445         network_info = self._allocate_network(context, instance,
1446                 requested_networks, macs, security_groups, dhcp_options)
1447 
1448         return network_info
1449 
1450     def _allocate_network(self, context, instance, requested_networks, macs,
1451                           security_groups, dhcp_options):
1452         """Start network allocation asynchronously.  Return an instance
1453         of NetworkInfoAsyncWrapper that can be used to retrieve the
1454         allocated networks when the operation has finished.
1455         """
1456         # NOTE(comstud): Since we're allocating networks asynchronously,
1457         # this task state has little meaning, as we won't be in this
1458         # state for very long.
1459         instance.vm_state = vm_states.BUILDING
1460         instance.task_state = task_states.NETWORKING
1461         instance.save(expected_task_state=[None])
1462         self._update_resource_tracker(context, instance)
1463 
1464         is_vpn = pipelib.is_vpn_image(instance.image_ref)
1465         return network_model.NetworkInfoAsyncWrapper(
1466                 self._allocate_network_async, context, instance,
1467                 requested_networks, macs, security_groups, is_vpn,
1468                 dhcp_options)
1469 
1470     def _default_root_device_name(self, instance, image_meta, root_bdm):
1471         try:
1472             return self.driver.default_root_device_name(instance,
1473                                                         image_meta,
1474                                                         root_bdm)
1475         except NotImplementedError:
1476             return compute_utils.get_next_device_name(instance, [])
1477 
1478     def _default_device_names_for_instance(self, instance,
1479                                            root_device_name,
1480                                            *block_device_lists):
1481         try:
1482             self.driver.default_device_names_for_instance(instance,
1483                                                           root_device_name,
1484                                                           *block_device_lists)
1485         except NotImplementedError:
1486             compute_utils.default_device_names_for_instance(
1487                 instance, root_device_name, *block_device_lists)
1488 
1489     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1490         # NOTE(ndipanov): Copy obj to avoid changing the original
1491         block_device_obj = block_device_obj.obj_clone()
1492         try:
1493             return self.driver.get_device_name_for_instance(
1494                 instance, bdms, block_device_obj)
1495         except NotImplementedError:
1496             return compute_utils.get_device_name_for_instance(
1497                 instance, bdms, block_device_obj.get("device_name"))
1498 
1499     def _default_block_device_names(self, instance, image_meta, block_devices):
1500         """Verify that all the devices have the device_name set. If not,
1501         provide a default name.
1502 
1503         It also ensures that there is a root_device_name and is set to the
1504         first block device in the boot sequence (boot_index=0).
1505         """
1506         root_bdm = block_device.get_root_bdm(block_devices)
1507         if not root_bdm:
1508             return
1509 
1510         # Get the root_device_name from the root BDM or the instance
1511         root_device_name = None
1512         update_root_bdm = False
1513 
1514         if root_bdm.device_name:
1515             root_device_name = root_bdm.device_name
1516             instance.root_device_name = root_device_name
1517         elif instance.root_device_name:
1518             root_device_name = instance.root_device_name
1519             root_bdm.device_name = root_device_name
1520             update_root_bdm = True
1521         else:
1522             root_device_name = self._default_root_device_name(instance,
1523                                                               image_meta,
1524                                                               root_bdm)
1525 
1526             instance.root_device_name = root_device_name
1527             root_bdm.device_name = root_device_name
1528             update_root_bdm = True
1529 
1530         if update_root_bdm:
1531             root_bdm.save()
1532 
1533         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1534                             block_devices))
1535         swap = list(filter(block_device.new_format_is_swap,
1536                       block_devices))
1537         block_device_mapping = list(filter(
1538               driver_block_device.is_block_device_mapping, block_devices))
1539 
1540         self._default_device_names_for_instance(instance,
1541                                                 root_device_name,
1542                                                 ephemerals,
1543                                                 swap,
1544                                                 block_device_mapping)
1545 
1546     def _block_device_info_to_legacy(self, block_device_info):
1547         """Convert BDI to the old format for drivers that need it."""
1548 
1549         if self.use_legacy_block_device_info:
1550             ephemerals = driver_block_device.legacy_block_devices(
1551                 driver.block_device_info_get_ephemerals(block_device_info))
1552             mapping = driver_block_device.legacy_block_devices(
1553                 driver.block_device_info_get_mapping(block_device_info))
1554             swap = block_device_info['swap']
1555             if swap:
1556                 swap = swap.legacy()
1557 
1558             block_device_info.update({
1559                 'ephemerals': ephemerals,
1560                 'swap': swap,
1561                 'block_device_mapping': mapping})
1562 
1563     def _add_missing_dev_names(self, bdms, instance):
1564         for bdm in bdms:
1565             if bdm.device_name is not None:
1566                 continue
1567 
1568             device_name = self._get_device_name_for_instance(instance,
1569                                                              bdms, bdm)
1570             values = {'device_name': device_name}
1571             bdm.update(values)
1572             bdm.save()
1573 
1574     def _prep_block_device(self, context, instance, bdms,
1575                            do_check_attach=True):
1576         """Set up the block device for an instance with error logging."""
1577         try:
1578             self._add_missing_dev_names(bdms, instance)
1579             block_device_info = driver.get_block_device_info(instance, bdms)
1580             mapping = driver.block_device_info_get_mapping(block_device_info)
1581             driver_block_device.attach_block_devices(
1582                 mapping, context, instance, self.volume_api, self.driver,
1583                 do_check_attach=do_check_attach,
1584                 wait_func=self._await_block_device_map_created)
1585 
1586             self._block_device_info_to_legacy(block_device_info)
1587             return block_device_info
1588 
1589         except exception.OverQuota:
1590             msg = _LW('Failed to create block device for instance due to '
1591                       'being over volume resource quota')
1592             LOG.warning(msg, instance=instance)
1593             raise exception.VolumeLimitExceeded()
1594 
1595         except Exception:
1596             LOG.exception(_LE('Instance failed block device setup'),
1597                           instance=instance)
1598             raise exception.InvalidBDM()
1599 
1600     def _update_instance_after_spawn(self, context, instance):
1601         instance.power_state = self._get_power_state(context, instance)
1602         instance.vm_state = vm_states.ACTIVE
1603         instance.task_state = None
1604         instance.launched_at = timeutils.utcnow()
1605         configdrive.update_instance(instance)
1606 
1607     def _update_scheduler_instance_info(self, context, instance):
1608         """Sends an InstanceList with created or updated Instance objects to
1609         the Scheduler client.
1610 
1611         In the case of init_host, the value passed will already be an
1612         InstanceList. Other calls will send individual Instance objects that
1613         have been created or resized. In this case, we create an InstanceList
1614         object containing that Instance.
1615         """
1616         if not self.send_instance_updates:
1617             return
1618         if isinstance(instance, obj_instance.Instance):
1619             instance = objects.InstanceList(objects=[instance])
1620         context = context.elevated()
1621         self.scheduler_client.update_instance_info(context, self.host,
1622                                                    instance)
1623 
1624     def _delete_scheduler_instance_info(self, context, instance_uuid):
1625         """Sends the uuid of the deleted Instance to the Scheduler client."""
1626         if not self.send_instance_updates:
1627             return
1628         context = context.elevated()
1629         self.scheduler_client.delete_instance_info(context, self.host,
1630                                                    instance_uuid)
1631 
1632     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
1633     def _sync_scheduler_instance_info(self, context):
1634         if not self.send_instance_updates:
1635             return
1636         context = context.elevated()
1637         instances = objects.InstanceList.get_by_host(context, self.host,
1638                                                      expected_attrs=[],
1639                                                      use_slave=True)
1640         uuids = [instance.uuid for instance in instances]
1641         self.scheduler_client.sync_instance_info(context, self.host, uuids)
1642 
1643     def _notify_about_instance_usage(self, context, instance, event_suffix,
1644                                      network_info=None, system_metadata=None,
1645                                      extra_usage_info=None, fault=None):
1646         compute_utils.notify_about_instance_usage(
1647             self.notifier, context, instance, event_suffix,
1648             network_info=network_info,
1649             system_metadata=system_metadata,
1650             extra_usage_info=extra_usage_info, fault=fault)
1651 
1652     def _deallocate_network(self, context, instance,
1653                             requested_networks=None):
1654         # If we were told not to allocate networks let's save ourselves
1655         # the trouble of calling the network API.
1656         if requested_networks and requested_networks.no_allocate:
1657             LOG.debug("Skipping network deallocation for instance since "
1658                       "networking was not requested.", instance=instance)
1659             return
1660 
1661         LOG.debug('Deallocating network for instance', instance=instance)
1662         with timeutils.StopWatch() as timer:
1663             self.network_api.deallocate_for_instance(
1664                 context, instance, requested_networks=requested_networks)
1665         # nova-network does an rpc call so we're OK tracking time spent here
1666         LOG.info(_LI('Took %0.2f seconds to deallocate network for instance.'),
1667                  timer.elapsed(), instance=instance)
1668 
1669     def _get_instance_block_device_info(self, context, instance,
1670                                         refresh_conn_info=False,
1671                                         bdms=None):
1672         """Transform block devices to the driver block_device format."""
1673 
1674         if not bdms:
1675             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1676                     context, instance.uuid)
1677         block_device_info = driver.get_block_device_info(instance, bdms)
1678 
1679         if not refresh_conn_info:
1680             # if the block_device_mapping has no value in connection_info
1681             # (returned as None), don't include in the mapping
1682             block_device_info['block_device_mapping'] = [
1683                 bdm for bdm in driver.block_device_info_get_mapping(
1684                                     block_device_info)
1685                 if bdm.get('connection_info')]
1686         else:
1687             driver_block_device.refresh_conn_infos(
1688                 driver.block_device_info_get_mapping(block_device_info),
1689                 context, instance, self.volume_api, self.driver)
1690 
1691         self._block_device_info_to_legacy(block_device_info)
1692 
1693         return block_device_info
1694 
1695     @wrap_exception()
1696     @reverts_task_state
1697     @wrap_instance_fault
1698     def build_and_run_instance(self, context, instance, image, request_spec,
1699                      filter_properties, admin_password=None,
1700                      injected_files=None, requested_networks=None,
1701                      security_groups=None, block_device_mapping=None,
1702                      node=None, limits=None):
1703 
1704         @utils.synchronized(instance.uuid)
1705         def _locked_do_build_and_run_instance(*args, **kwargs):
1706             # NOTE(danms): We grab the semaphore with the instance uuid
1707             # locked because we could wait in line to build this instance
1708             # for a while and we want to make sure that nothing else tries
1709             # to do anything with this instance while we wait.
1710             with self._build_semaphore:
1711                 self._do_build_and_run_instance(*args, **kwargs)
1712 
1713         # NOTE(danms): We spawn here to return the RPC worker thread back to
1714         # the pool. Since what follows could take a really long time, we don't
1715         # want to tie up RPC workers.
1716         utils.spawn_n(_locked_do_build_and_run_instance,
1717                       context, instance, image, request_spec,
1718                       filter_properties, admin_password, injected_files,
1719                       requested_networks, security_groups,
1720                       block_device_mapping, node, limits)
1721 
1722     def _check_device_tagging(self, requested_networks, block_device_mapping):
1723         tagging_requested = False
1724         if requested_networks:
1725             for net in requested_networks:
1726                 if 'tag' in net and net.tag is not None:
1727                     tagging_requested = True
1728                     break
1729         if block_device_mapping and not tagging_requested:
1730             for bdm in block_device_mapping:
1731                 if 'tag' in bdm and bdm.tag is not None:
1732                     tagging_requested = True
1733                     break
1734         if (tagging_requested and
1735                 not self.driver.capabilities.get('supports_device_tagging')):
1736             raise exception.BuildAbortException('Attempt to boot guest with '
1737                                                 'tagged devices on host that '
1738                                                 'does not support tagging.')
1739 
1740     @hooks.add_hook('build_instance')
1741     @wrap_exception()
1742     @reverts_task_state
1743     @wrap_instance_event(prefix='compute')
1744     @wrap_instance_fault
1745     def _do_build_and_run_instance(self, context, instance, image,
1746             request_spec, filter_properties, admin_password, injected_files,
1747             requested_networks, security_groups, block_device_mapping,
1748             node=None, limits=None):
1749 
1750         try:
1751             LOG.debug('Starting instance...', instance=instance)
1752             instance.vm_state = vm_states.BUILDING
1753             instance.task_state = None
1754             instance.save(expected_task_state=
1755                     (task_states.SCHEDULING, None))
1756         except exception.InstanceNotFound:
1757             msg = 'Instance disappeared before build.'
1758             LOG.debug(msg, instance=instance)
1759             return build_results.FAILED
1760         except exception.UnexpectedTaskStateError as e:
1761             LOG.debug(e.format_message(), instance=instance)
1762             return build_results.FAILED
1763 
1764         # b64 decode the files to inject:
1765         decoded_files = self._decode_files(injected_files)
1766 
1767         if limits is None:
1768             limits = {}
1769 
1770         if node is None:
1771             node = self.driver.get_available_nodes(refresh=True)[0]
1772             LOG.debug('No node specified, defaulting to %s', node,
1773                       instance=instance)
1774 
1775         try:
1776             with timeutils.StopWatch() as timer:
1777                 self._build_and_run_instance(context, instance, image,
1778                         decoded_files, admin_password, requested_networks,
1779                         security_groups, block_device_mapping, node, limits,
1780                         filter_properties)
1781             LOG.info(_LI('Took %0.2f seconds to build instance.'),
1782                      timer.elapsed(), instance=instance)
1783             return build_results.ACTIVE
1784         except exception.RescheduledException as e:
1785             retry = filter_properties.get('retry')
1786             if not retry:
1787                 # no retry information, do not reschedule.
1788                 LOG.debug("Retry info not present, will not reschedule",
1789                     instance=instance)
1790                 self._cleanup_allocated_networks(context, instance,
1791                     requested_networks)
1792                 compute_utils.add_instance_fault_from_exc(context,
1793                         instance, e, sys.exc_info(),
1794                         fault_message=e.kwargs['reason'])
1795                 self._nil_out_instance_obj_host_and_node(instance)
1796                 self._set_instance_obj_error_state(context, instance,
1797                                                    clean_task_state=True)
1798                 return build_results.FAILED
1799             LOG.debug(e.format_message(), instance=instance)
1800             # This will be used for logging the exception
1801             retry['exc'] = traceback.format_exception(*sys.exc_info())
1802             # This will be used for setting the instance fault message
1803             retry['exc_reason'] = e.kwargs['reason']
1804             # NOTE(comstud): Deallocate networks if the driver wants
1805             # us to do so.
1806             # NOTE(vladikr): SR-IOV ports should be deallocated to
1807             # allow new sriov pci devices to be allocated on a new host.
1808             # Otherwise, if devices with pci addresses are already allocated
1809             # on the destination host, the instance will fail to spawn.
1810             # info_cache.network_info should be present at this stage.
1811             if (self.driver.deallocate_networks_on_reschedule(instance) or
1812                 self.deallocate_sriov_ports_on_reschedule(instance)):
1813                 self._cleanup_allocated_networks(context, instance,
1814                         requested_networks)
1815             else:
1816                 # NOTE(alex_xu): Network already allocated and we don't
1817                 # want to deallocate them before rescheduling. But we need
1818                 # to cleanup those network resources setup on this host before
1819                 # rescheduling.
1820                 self.network_api.cleanup_instance_network_on_host(
1821                     context, instance, self.host)
1822 
1823             self._nil_out_instance_obj_host_and_node(instance)
1824             instance.task_state = task_states.SCHEDULING
1825             instance.save()
1826 
1827             self.compute_task_api.build_instances(context, [instance],
1828                     image, filter_properties, admin_password,
1829                     injected_files, requested_networks, security_groups,
1830                     block_device_mapping)
1831             return build_results.RESCHEDULED
1832         except (exception.InstanceNotFound,
1833                 exception.UnexpectedDeletingTaskStateError):
1834             msg = 'Instance disappeared during build.'
1835             LOG.debug(msg, instance=instance)
1836             self._cleanup_allocated_networks(context, instance,
1837                     requested_networks)
1838             return build_results.FAILED
1839         except exception.BuildAbortException as e:
1840             LOG.exception(e.format_message(), instance=instance)
1841             self._cleanup_allocated_networks(context, instance,
1842                     requested_networks)
1843             self._cleanup_volumes(context, instance.uuid,
1844                     block_device_mapping, raise_exc=False)
1845             compute_utils.add_instance_fault_from_exc(context, instance,
1846                     e, sys.exc_info())
1847             self._nil_out_instance_obj_host_and_node(instance)
1848             self._set_instance_obj_error_state(context, instance,
1849                                                clean_task_state=True)
1850             return build_results.FAILED
1851         except Exception as e:
1852             # Should not reach here.
1853             msg = _LE('Unexpected build failure, not rescheduling build.')
1854             LOG.exception(msg, instance=instance)
1855             self._cleanup_allocated_networks(context, instance,
1856                     requested_networks)
1857             self._cleanup_volumes(context, instance.uuid,
1858                     block_device_mapping, raise_exc=False)
1859             compute_utils.add_instance_fault_from_exc(context, instance,
1860                     e, sys.exc_info())
1861             self._nil_out_instance_obj_host_and_node(instance)
1862             self._set_instance_obj_error_state(context, instance,
1863                                                clean_task_state=True)
1864             return build_results.FAILED
1865 
1866     def deallocate_sriov_ports_on_reschedule(self, instance):
1867         """Determine if networks are needed to be deallocated before reschedule
1868 
1869         Check the cached network info for any assigned SR-IOV ports.
1870         SR-IOV ports should be deallocated prior to rescheduling
1871         in order to allow new sriov pci devices to be allocated on a new host.
1872         """
1873         info_cache = instance.info_cache
1874 
1875         def _has_sriov_port(vif):
1876             return vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV
1877 
1878         if (info_cache and info_cache.network_info):
1879             for vif in info_cache.network_info:
1880                 if _has_sriov_port(vif):
1881                     return True
1882         return False
1883 
1884     def _build_and_run_instance(self, context, instance, image, injected_files,
1885             admin_password, requested_networks, security_groups,
1886             block_device_mapping, node, limits, filter_properties):
1887 
1888         image_name = image.get('name')
1889         self._notify_about_instance_usage(context, instance, 'create.start',
1890                 extra_usage_info={'image_name': image_name})
1891         compute_utils.notify_about_instance_action(
1892             context, instance, self.host,
1893             action=fields.NotificationAction.CREATE,
1894             phase=fields.NotificationPhase.START)
1895 
1896         self._check_device_tagging(requested_networks, block_device_mapping)
1897 
1898         try:
1899             rt = self._get_resource_tracker()
1900             with rt.instance_claim(context, instance, node, limits):
1901                 # NOTE(russellb) It's important that this validation be done
1902                 # *after* the resource tracker instance claim, as that is where
1903                 # the host is set on the instance.
1904                 self._validate_instance_group_policy(context, instance,
1905                         filter_properties)
1906                 image_meta = objects.ImageMeta.from_dict(image)
1907                 with self._build_resources(context, instance,
1908                         requested_networks, security_groups, image_meta,
1909                         block_device_mapping) as resources:
1910                     instance.vm_state = vm_states.BUILDING
1911                     instance.task_state = task_states.SPAWNING
1912                     # NOTE(JoshNang) This also saves the changes to the
1913                     # instance from _allocate_network_async, as they aren't
1914                     # saved in that function to prevent races.
1915                     instance.save(expected_task_state=
1916                             task_states.BLOCK_DEVICE_MAPPING)
1917                     block_device_info = resources['block_device_info']
1918                     network_info = resources['network_info']
1919                     LOG.debug('Start spawning the instance on the hypervisor.',
1920                               instance=instance)
1921                     with timeutils.StopWatch() as timer:
1922                         self.driver.spawn(context, instance, image_meta,
1923                                           injected_files, admin_password,
1924                                           network_info=network_info,
1925                                           block_device_info=block_device_info)
1926                     LOG.info(_LI('Took %0.2f seconds to spawn the instance on '
1927                                  'the hypervisor.'), timer.elapsed(),
1928                              instance=instance)
1929         except (exception.InstanceNotFound,
1930                 exception.UnexpectedDeletingTaskStateError) as e:
1931             with excutils.save_and_reraise_exception():
1932                 self._notify_about_instance_usage(context, instance,
1933                     'create.error', fault=e)
1934                 compute_utils.notify_about_instance_action(
1935                     context, instance, self.host,
1936                     action=fields.NotificationAction.CREATE,
1937                     phase=fields.NotificationPhase.ERROR, exception=e)
1938         except exception.ComputeResourcesUnavailable as e:
1939             LOG.debug(e.format_message(), instance=instance)
1940             self._notify_about_instance_usage(context, instance,
1941                     'create.error', fault=e)
1942             compute_utils.notify_about_instance_action(
1943                     context, instance, self.host,
1944                     action=fields.NotificationAction.CREATE,
1945                     phase=fields.NotificationPhase.ERROR, exception=e)
1946             raise exception.RescheduledException(
1947                     instance_uuid=instance.uuid, reason=e.format_message())
1948         except exception.BuildAbortException as e:
1949             with excutils.save_and_reraise_exception():
1950                 LOG.debug(e.format_message(), instance=instance)
1951                 self._notify_about_instance_usage(context, instance,
1952                     'create.error', fault=e)
1953                 compute_utils.notify_about_instance_action(
1954                     context, instance, self.host,
1955                     action=fields.NotificationAction.CREATE,
1956                     phase=fields.NotificationPhase.ERROR, exception=e)
1957         except (exception.FixedIpLimitExceeded,
1958                 exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:
1959             LOG.warning(_LW('No more network or fixed IP to be allocated'),
1960                         instance=instance)
1961             self._notify_about_instance_usage(context, instance,
1962                     'create.error', fault=e)
1963             compute_utils.notify_about_instance_action(
1964                     context, instance, self.host,
1965                     action=fields.NotificationAction.CREATE,
1966                     phase=fields.NotificationPhase.ERROR, exception=e)
1967             msg = _('Failed to allocate the network(s) with error %s, '
1968                     'not rescheduling.') % e.format_message()
1969             raise exception.BuildAbortException(instance_uuid=instance.uuid,
1970                     reason=msg)
1971         except (exception.VirtualInterfaceCreateException,
1972                 exception.VirtualInterfaceMacAddressException,
1973                 exception.FixedIpInvalidOnHost,
1974                 exception.UnableToAutoAllocateNetwork) as e:
1975             LOG.exception(_LE('Failed to allocate network(s)'),
1976                           instance=instance)
1977             self._notify_about_instance_usage(context, instance,
1978                     'create.error', fault=e)
1979             compute_utils.notify_about_instance_action(
1980                     context, instance, self.host,
1981                     action=fields.NotificationAction.CREATE,
1982                     phase=fields.NotificationPhase.ERROR, exception=e)
1983             msg = _('Failed to allocate the network(s), not rescheduling.')
1984             raise exception.BuildAbortException(instance_uuid=instance.uuid,
1985                     reason=msg)
1986         except (exception.FlavorDiskTooSmall,
1987                 exception.FlavorMemoryTooSmall,
1988                 exception.ImageNotActive,
1989                 exception.ImageUnacceptable,
1990                 exception.InvalidDiskInfo,
1991                 exception.InvalidDiskFormat,
1992                 exception.SignatureVerificationError,
1993                 exception.VolumeEncryptionNotSupported) as e:
1994             self._notify_about_instance_usage(context, instance,
1995                     'create.error', fault=e)
1996             compute_utils.notify_about_instance_action(
1997                     context, instance, self.host,
1998                     action=fields.NotificationAction.CREATE,
1999                     phase=fields.NotificationPhase.ERROR, exception=e)
2000             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2001                     reason=e.format_message())
2002         except Exception as e:
2003             self._notify_about_instance_usage(context, instance,
2004                     'create.error', fault=e)
2005             compute_utils.notify_about_instance_action(
2006                     context, instance, self.host,
2007                     action=fields.NotificationAction.CREATE,
2008                     phase=fields.NotificationPhase.ERROR, exception=e)
2009             raise exception.RescheduledException(
2010                     instance_uuid=instance.uuid, reason=six.text_type(e))
2011 
2012         # NOTE(alaski): This is only useful during reschedules, remove it now.
2013         instance.system_metadata.pop('network_allocated', None)
2014 
2015         # If CONF.default_access_ip_network_name is set, grab the
2016         # corresponding network and set the access ip values accordingly.
2017         network_name = CONF.default_access_ip_network_name
2018         if (network_name and not instance.access_ip_v4 and
2019                 not instance.access_ip_v6):
2020             # Note that when there are multiple ips to choose from, an
2021             # arbitrary one will be chosen.
2022             for vif in network_info:
2023                 if vif['network']['label'] == network_name:
2024                     for ip in vif.fixed_ips():
2025                         if not instance.access_ip_v4 and ip['version'] == 4:
2026                             instance.access_ip_v4 = ip['address']
2027                         if not instance.access_ip_v6 and ip['version'] == 6:
2028                             instance.access_ip_v6 = ip['address']
2029                     break
2030 
2031         self._update_instance_after_spawn(context, instance)
2032 
2033         try:
2034             instance.save(expected_task_state=task_states.SPAWNING)
2035         except (exception.InstanceNotFound,
2036                 exception.UnexpectedDeletingTaskStateError) as e:
2037             with excutils.save_and_reraise_exception():
2038                 self._notify_about_instance_usage(context, instance,
2039                     'create.error', fault=e)
2040                 compute_utils.notify_about_instance_action(
2041                     context, instance, self.host,
2042                     action=fields.NotificationAction.CREATE,
2043                     phase=fields.NotificationPhase.ERROR, exception=e)
2044 
2045         self._update_scheduler_instance_info(context, instance)
2046         self._notify_about_instance_usage(context, instance, 'create.end',
2047                 extra_usage_info={'message': _('Success')},
2048                 network_info=network_info)
2049         compute_utils.notify_about_instance_action(context, instance,
2050                 self.host, action=fields.NotificationAction.CREATE,
2051                 phase=fields.NotificationPhase.END)
2052 
2053     @contextlib.contextmanager
2054     def _build_resources(self, context, instance, requested_networks,
2055                          security_groups, image_meta, block_device_mapping):
2056         resources = {}
2057         network_info = None
2058         try:
2059             LOG.debug('Start building networks asynchronously for instance.',
2060                       instance=instance)
2061             network_info = self._build_networks_for_instance(context, instance,
2062                     requested_networks, security_groups)
2063             resources['network_info'] = network_info
2064         except (exception.InstanceNotFound,
2065                 exception.UnexpectedDeletingTaskStateError):
2066             raise
2067         except exception.UnexpectedTaskStateError as e:
2068             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2069                     reason=e.format_message())
2070         except Exception:
2071             # Because this allocation is async any failures are likely to occur
2072             # when the driver accesses network_info during spawn().
2073             LOG.exception(_LE('Failed to allocate network(s)'),
2074                           instance=instance)
2075             msg = _('Failed to allocate the network(s), not rescheduling.')
2076             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2077                     reason=msg)
2078 
2079         try:
2080             # Verify that all the BDMs have a device_name set and assign a
2081             # default to the ones missing it with the help of the driver.
2082             self._default_block_device_names(instance, image_meta,
2083                                              block_device_mapping)
2084 
2085             LOG.debug('Start building block device mappings for instance.',
2086                       instance=instance)
2087             instance.vm_state = vm_states.BUILDING
2088             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2089             instance.save()
2090 
2091             block_device_info = self._prep_block_device(context, instance,
2092                     block_device_mapping)
2093             resources['block_device_info'] = block_device_info
2094         except (exception.InstanceNotFound,
2095                 exception.UnexpectedDeletingTaskStateError):
2096             with excutils.save_and_reraise_exception():
2097                 # Make sure the async call finishes
2098                 if network_info is not None:
2099                     network_info.wait(do_raise=False)
2100         except (exception.UnexpectedTaskStateError,
2101                 exception.VolumeLimitExceeded,
2102                 exception.InvalidBDM) as e:
2103             # Make sure the async call finishes
2104             if network_info is not None:
2105                 network_info.wait(do_raise=False)
2106             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2107                     reason=e.format_message())
2108         except Exception:
2109             LOG.exception(_LE('Failure prepping block device'),
2110                     instance=instance)
2111             # Make sure the async call finishes
2112             if network_info is not None:
2113                 network_info.wait(do_raise=False)
2114             msg = _('Failure prepping block device.')
2115             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2116                     reason=msg)
2117 
2118         try:
2119             yield resources
2120         except Exception as exc:
2121             with excutils.save_and_reraise_exception() as ctxt:
2122                 if not isinstance(exc, (
2123                         exception.InstanceNotFound,
2124                         exception.UnexpectedDeletingTaskStateError)):
2125                     LOG.exception(_LE('Instance failed to spawn'),
2126                                   instance=instance)
2127                 # Make sure the async call finishes
2128                 if network_info is not None:
2129                     network_info.wait(do_raise=False)
2130                 # if network_info is empty we're likely here because of
2131                 # network allocation failure. Since nothing can be reused on
2132                 # rescheduling it's better to deallocate network to eliminate
2133                 # the chance of orphaned ports in neutron
2134                 deallocate_networks = False if network_info else True
2135                 try:
2136                     self._shutdown_instance(context, instance,
2137                             block_device_mapping, requested_networks,
2138                             try_deallocate_networks=deallocate_networks)
2139                 except Exception as exc2:
2140                     ctxt.reraise = False
2141                     LOG.warning(_LW('Could not clean up failed build,'
2142                                     ' not rescheduling. Error: %s'),
2143                                 six.text_type(exc2))
2144                     raise exception.BuildAbortException(
2145                             instance_uuid=instance.uuid,
2146                             reason=six.text_type(exc))
2147 
2148     def _cleanup_allocated_networks(self, context, instance,
2149             requested_networks):
2150         try:
2151             self._deallocate_network(context, instance, requested_networks)
2152         except Exception:
2153             msg = _LE('Failed to deallocate networks')
2154             LOG.exception(msg, instance=instance)
2155             return
2156 
2157         instance.system_metadata['network_allocated'] = 'False'
2158         try:
2159             instance.save()
2160         except exception.InstanceNotFound:
2161             # NOTE(alaski): It's possible that we're cleaning up the networks
2162             # because the instance was deleted.  If that's the case then this
2163             # exception will be raised by instance.save()
2164             pass
2165 
2166     def _try_deallocate_network(self, context, instance,
2167                                 requested_networks=None):
2168         try:
2169             # tear down allocated network structure
2170             self._deallocate_network(context, instance, requested_networks)
2171         except Exception as ex:
2172             with excutils.save_and_reraise_exception():
2173                 LOG.error(_LE('Failed to deallocate network for instance. '
2174                               'Error: %s'), ex,
2175                           instance=instance)
2176                 self._set_instance_obj_error_state(context, instance)
2177 
2178     def _get_power_off_values(self, context, instance, clean_shutdown):
2179         """Get the timing configuration for powering down this instance."""
2180         if clean_shutdown:
2181             timeout = compute_utils.get_value_from_system_metadata(instance,
2182                           key='image_os_shutdown_timeout', type=int,
2183                           default=CONF.shutdown_timeout)
2184             retry_interval = self.SHUTDOWN_RETRY_INTERVAL
2185         else:
2186             timeout = 0
2187             retry_interval = 0
2188 
2189         return timeout, retry_interval
2190 
2191     def _power_off_instance(self, context, instance, clean_shutdown=True):
2192         """Power off an instance on this host."""
2193         timeout, retry_interval = self._get_power_off_values(context,
2194                                         instance, clean_shutdown)
2195         self.driver.power_off(instance, timeout, retry_interval)
2196 
2197     def _shutdown_instance(self, context, instance,
2198                            bdms, requested_networks=None, notify=True,
2199                            try_deallocate_networks=True):
2200         """Shutdown an instance on this host.
2201 
2202         :param:context: security context
2203         :param:instance: a nova.objects.Instance object
2204         :param:bdms: the block devices for the instance to be torn
2205                      down
2206         :param:requested_networks: the networks on which the instance
2207                                    has ports
2208         :param:notify: true if a final usage notification should be
2209                        emitted
2210         :param:try_deallocate_networks: false if we should avoid
2211                                         trying to teardown networking
2212         """
2213         context = context.elevated()
2214         LOG.info(_LI('Terminating instance'), instance=instance)
2215 
2216         if notify:
2217             self._notify_about_instance_usage(context, instance,
2218                                               "shutdown.start")
2219             compute_utils.notify_about_instance_action(context, instance,
2220                     self.host, action=fields.NotificationAction.SHUTDOWN,
2221                     phase=fields.NotificationPhase.START)
2222 
2223         network_info = compute_utils.get_nw_info_for_instance(instance)
2224 
2225         # NOTE(vish) get bdms before destroying the instance
2226         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2227         block_device_info = self._get_instance_block_device_info(
2228             context, instance, bdms=bdms)
2229 
2230         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2231         #                want to keep ip allocated for certain failures
2232         timer = timeutils.StopWatch()
2233         try:
2234             LOG.debug('Start destroying the instance on the hypervisor.',
2235                       instance=instance)
2236             timer.start()
2237             self.driver.destroy(context, instance, network_info,
2238                     block_device_info)
2239             LOG.info(_LI('Took %0.2f seconds to destroy the instance on the '
2240                          'hypervisor.'), timer.elapsed(), instance=instance)
2241         except exception.InstancePowerOffFailure:
2242             # if the instance can't power off, don't release the ip
2243             with excutils.save_and_reraise_exception():
2244                 pass
2245         except Exception:
2246             with excutils.save_and_reraise_exception():
2247                 # deallocate ip and fail without proceeding to
2248                 # volume api calls, preserving current behavior
2249                 if try_deallocate_networks:
2250                     self._try_deallocate_network(context, instance,
2251                                                  requested_networks)
2252 
2253         if try_deallocate_networks:
2254             self._try_deallocate_network(context, instance, requested_networks)
2255 
2256         timer.restart()
2257         for bdm in vol_bdms:
2258             try:
2259                 # NOTE(vish): actual driver detach done in driver.destroy, so
2260                 #             just tell cinder that we are done with it.
2261                 connector = self.driver.get_volume_connector(instance)
2262                 self.volume_api.terminate_connection(context,
2263                                                      bdm.volume_id,
2264                                                      connector)
2265                 self.volume_api.detach(context, bdm.volume_id, instance.uuid)
2266             except exception.DiskNotFound as exc:
2267                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2268                           instance=instance)
2269             except exception.VolumeNotFound as exc:
2270                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2271                           instance=instance)
2272             except (cinder_exception.EndpointNotFound,
2273                     keystone_exception.EndpointNotFound) as exc:
2274                 LOG.warning(_LW('Ignoring EndpointNotFound for '
2275                                 'volume %(volume_id)s: %(exc)s'),
2276                             {'exc': exc, 'volume_id': bdm.volume_id},
2277                             instance=instance)
2278             except cinder_exception.ClientException as exc:
2279                 LOG.warning(_LW('Ignoring unknown cinder exception for '
2280                                 'volume %(volume_id)s: %(exc)s'),
2281                             {'exc': exc, 'volume_id': bdm.volume_id},
2282                             instance=instance)
2283             except Exception as exc:
2284                 LOG.warning(_LW('Ignoring unknown exception for '
2285                                 'volume %(volume_id)s: %(exc)s'),
2286                             {'exc': exc, 'volume_id': bdm.volume_id},
2287                             instance=instance)
2288         if vol_bdms:
2289             LOG.info(_LI('Took %(time).2f seconds to detach %(num)s volumes '
2290                          'for instance.'),
2291                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2292                      instance=instance)
2293 
2294         if notify:
2295             self._notify_about_instance_usage(context, instance,
2296                                               "shutdown.end")
2297             compute_utils.notify_about_instance_action(context, instance,
2298                     self.host, action=fields.NotificationAction.SHUTDOWN,
2299                     phase=fields.NotificationPhase.END)
2300 
2301     def _cleanup_volumes(self, context, instance_uuid, bdms, raise_exc=True):
2302         exc_info = None
2303 
2304         for bdm in bdms:
2305             LOG.debug("terminating bdm %s", bdm,
2306                       instance_uuid=instance_uuid)
2307             if bdm.volume_id and bdm.delete_on_termination:
2308                 try:
2309                     self.volume_api.delete(context, bdm.volume_id)
2310                 except Exception as exc:
2311                     exc_info = sys.exc_info()
2312                     LOG.warning(_LW('Failed to delete volume: %(volume_id)s '
2313                                     'due to %(exc)s'),
2314                                 {'volume_id': bdm.volume_id, 'exc': exc})
2315         if exc_info is not None and raise_exc:
2316             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2317 
2318     @hooks.add_hook("delete_instance")
2319     def _delete_instance(self, context, instance, bdms, quotas):
2320         """Delete an instance on this host.  Commit or rollback quotas
2321         as necessary.
2322 
2323         :param context: nova request context
2324         :param instance: nova.objects.instance.Instance object
2325         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2326         :param quotas: nova.objects.quotas.Quotas object
2327         """
2328         was_soft_deleted = instance.vm_state == vm_states.SOFT_DELETED
2329         if was_soft_deleted:
2330             # Instances in SOFT_DELETED vm_state have already had quotas
2331             # decremented.
2332             try:
2333                 quotas.rollback()
2334             except Exception:
2335                 pass
2336 
2337         try:
2338             events = self.instance_events.clear_events_for_instance(instance)
2339             if events:
2340                 LOG.debug('Events pending at deletion: %(events)s',
2341                           {'events': ','.join(events.keys())},
2342                           instance=instance)
2343             self._notify_about_instance_usage(context, instance,
2344                                               "delete.start")
2345             compute_utils.notify_about_instance_action(context, instance,
2346                     self.host, action=fields.NotificationAction.DELETE,
2347                     phase=fields.NotificationPhase.START)
2348 
2349             self._shutdown_instance(context, instance, bdms)
2350             # NOTE(dims): instance.info_cache.delete() should be called after
2351             # _shutdown_instance in the compute manager as shutdown calls
2352             # deallocate_for_instance so the info_cache is still needed
2353             # at this point.
2354             if instance.info_cache is not None:
2355                 instance.info_cache.delete()
2356             else:
2357                 # NOTE(yoshimatsu): Avoid AttributeError if instance.info_cache
2358                 # is None. When the root cause that instance.info_cache becomes
2359                 # None is fixed, the log level should be reconsidered.
2360                 LOG.warning(_LW("Info cache for instance could not be found. "
2361                                 "Ignore."), instance=instance)
2362 
2363             # NOTE(vish): We have already deleted the instance, so we have
2364             #             to ignore problems cleaning up the volumes. It
2365             #             would be nice to let the user know somehow that
2366             #             the volume deletion failed, but it is not
2367             #             acceptable to have an instance that can not be
2368             #             deleted. Perhaps this could be reworked in the
2369             #             future to set an instance fault the first time
2370             #             and to only ignore the failure if the instance
2371             #             is already in ERROR.
2372             self._cleanup_volumes(context, instance.uuid, bdms,
2373                     raise_exc=False)
2374             # if a delete task succeeded, always update vm state and task
2375             # state without expecting task state to be DELETING
2376             instance.vm_state = vm_states.DELETED
2377             instance.task_state = None
2378             instance.power_state = power_state.NOSTATE
2379             instance.terminated_at = timeutils.utcnow()
2380             instance.save()
2381             system_meta = instance.system_metadata
2382             instance.destroy()
2383         except Exception:
2384             with excutils.save_and_reraise_exception():
2385                 quotas.rollback()
2386 
2387         self._complete_deletion(context,
2388                                 instance,
2389                                 bdms,
2390                                 quotas,
2391                                 system_meta)
2392 
2393     @wrap_exception()
2394     @reverts_task_state
2395     @wrap_instance_event(prefix='compute')
2396     @wrap_instance_fault
2397     def terminate_instance(self, context, instance, bdms, reservations):
2398         """Terminate an instance on this host."""
2399         quotas = objects.Quotas.from_reservations(context,
2400                                                   reservations,
2401                                                   instance=instance)
2402 
2403         @utils.synchronized(instance.uuid)
2404         def do_terminate_instance(instance, bdms):
2405             # NOTE(mriedem): If we are deleting the instance while it was
2406             # booting from volume, we could be racing with a database update of
2407             # the BDM volume_id. Since the compute API passes the BDMs over RPC
2408             # to compute here, the BDMs may be stale at this point. So check
2409             # for any volume BDMs that don't have volume_id set and if we
2410             # detect that, we need to refresh the BDM list before proceeding.
2411             # TODO(mriedem): Move this into _delete_instance and make the bdms
2412             # parameter optional.
2413             for bdm in list(bdms):
2414                 if bdm.is_volume and not bdm.volume_id:
2415                     LOG.debug('There are potentially stale BDMs during '
2416                               'delete, refreshing the BlockDeviceMappingList.',
2417                               instance=instance)
2418                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2419                         context, instance.uuid)
2420                     break
2421             try:
2422                 self._delete_instance(context, instance, bdms, quotas)
2423             except exception.InstanceNotFound:
2424                 LOG.info(_LI("Instance disappeared during terminate"),
2425                          instance=instance)
2426             except Exception:
2427                 # As we're trying to delete always go to Error if something
2428                 # goes wrong that _delete_instance can't handle.
2429                 with excutils.save_and_reraise_exception():
2430                     LOG.exception(_LE('Setting instance vm_state to ERROR'),
2431                                   instance=instance)
2432                     self._set_instance_obj_error_state(context, instance)
2433 
2434         do_terminate_instance(instance, bdms)
2435 
2436     # NOTE(johannes): This is probably better named power_off_instance
2437     # so it matches the driver method, but because of other issues, we
2438     # can't use that name in grizzly.
2439     @wrap_exception()
2440     @reverts_task_state
2441     @wrap_instance_event(prefix='compute')
2442     @wrap_instance_fault
2443     def stop_instance(self, context, instance, clean_shutdown):
2444         """Stopping an instance on this host."""
2445 
2446         @utils.synchronized(instance.uuid)
2447         def do_stop_instance():
2448             current_power_state = self._get_power_state(context, instance)
2449             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
2450                       'current task_state: %(task_state)s, current DB '
2451                       'power_state: %(db_power_state)s, current VM '
2452                       'power_state: %(current_power_state)s',
2453                       {'vm_state': instance.vm_state,
2454                        'task_state': instance.task_state,
2455                        'db_power_state': instance.power_state,
2456                        'current_power_state': current_power_state},
2457                       instance_uuid=instance.uuid)
2458 
2459             # NOTE(mriedem): If the instance is already powered off, we are
2460             # possibly tearing down and racing with other operations, so we can
2461             # expect the task_state to be None if something else updates the
2462             # instance and we're not locking it.
2463             expected_task_state = [task_states.POWERING_OFF]
2464             # The list of power states is from _sync_instance_power_state.
2465             if current_power_state in (power_state.NOSTATE,
2466                                        power_state.SHUTDOWN,
2467                                        power_state.CRASHED):
2468                 LOG.info(_LI('Instance is already powered off in the '
2469                              'hypervisor when stop is called.'),
2470                          instance=instance)
2471                 expected_task_state.append(None)
2472 
2473             self._notify_about_instance_usage(context, instance,
2474                                               "power_off.start")
2475 
2476             compute_utils.notify_about_instance_action(context, instance,
2477                         self.host, action=fields.NotificationAction.POWER_OFF,
2478                         phase=fields.NotificationPhase.START)
2479 
2480             self._power_off_instance(context, instance, clean_shutdown)
2481             instance.power_state = self._get_power_state(context, instance)
2482             instance.vm_state = vm_states.STOPPED
2483             instance.task_state = None
2484             instance.save(expected_task_state=expected_task_state)
2485             self._notify_about_instance_usage(context, instance,
2486                                               "power_off.end")
2487 
2488             compute_utils.notify_about_instance_action(context, instance,
2489                         self.host, action=fields.NotificationAction.POWER_OFF,
2490                         phase=fields.NotificationPhase.END)
2491 
2492         do_stop_instance()
2493 
2494     def _power_on(self, context, instance):
2495         network_info = self.network_api.get_instance_nw_info(context, instance)
2496         block_device_info = self._get_instance_block_device_info(context,
2497                                                                  instance)
2498         self.driver.power_on(context, instance,
2499                              network_info,
2500                              block_device_info)
2501 
2502     def _delete_snapshot_of_shelved_instance(self, context, instance,
2503                                              snapshot_id):
2504         """Delete snapshot of shelved instance."""
2505         try:
2506             self.image_api.delete(context, snapshot_id)
2507         except (exception.ImageNotFound,
2508                 exception.ImageNotAuthorized) as exc:
2509             LOG.warning(_LW("Failed to delete snapshot "
2510                             "from shelved instance (%s)."),
2511                         exc.format_message(), instance=instance)
2512         except Exception:
2513             LOG.exception(_LE("Something wrong happened when trying to "
2514                               "delete snapshot from shelved instance."),
2515                           instance=instance)
2516 
2517     # NOTE(johannes): This is probably better named power_on_instance
2518     # so it matches the driver method, but because of other issues, we
2519     # can't use that name in grizzly.
2520     @wrap_exception()
2521     @reverts_task_state
2522     @wrap_instance_event(prefix='compute')
2523     @wrap_instance_fault
2524     def start_instance(self, context, instance):
2525         """Starting an instance on this host."""
2526         self._notify_about_instance_usage(context, instance, "power_on.start")
2527         compute_utils.notify_about_instance_action(context, instance,
2528             self.host, action=fields.NotificationAction.POWER_ON,
2529             phase=fields.NotificationPhase.START)
2530         self._power_on(context, instance)
2531         instance.power_state = self._get_power_state(context, instance)
2532         instance.vm_state = vm_states.ACTIVE
2533         instance.task_state = None
2534 
2535         # Delete an image(VM snapshot) for a shelved instance
2536         snapshot_id = instance.system_metadata.get('shelved_image_id')
2537         if snapshot_id:
2538             self._delete_snapshot_of_shelved_instance(context, instance,
2539                                                       snapshot_id)
2540 
2541         # Delete system_metadata for a shelved instance
2542         compute_utils.remove_shelved_keys_from_system_metadata(instance)
2543 
2544         instance.save(expected_task_state=task_states.POWERING_ON)
2545         self._notify_about_instance_usage(context, instance, "power_on.end")
2546         compute_utils.notify_about_instance_action(context, instance,
2547             self.host, action=fields.NotificationAction.POWER_ON,
2548             phase=fields.NotificationPhase.END)
2549 
2550     @messaging.expected_exceptions(NotImplementedError,
2551                                    exception.TriggerCrashDumpNotSupported,
2552                                    exception.InstanceNotRunning)
2553     @wrap_exception()
2554     @wrap_instance_event(prefix='compute')
2555     @wrap_instance_fault
2556     def trigger_crash_dump(self, context, instance):
2557         """Trigger crash dump in an instance."""
2558 
2559         self._notify_about_instance_usage(context, instance,
2560                                           "trigger_crash_dump.start")
2561 
2562         # This method does not change task_state and power_state because the
2563         # effect of a trigger depends on user's configuration.
2564         self.driver.trigger_crash_dump(instance)
2565 
2566         self._notify_about_instance_usage(context, instance,
2567                                           "trigger_crash_dump.end")
2568 
2569     @wrap_exception()
2570     @reverts_task_state
2571     @wrap_instance_event(prefix='compute')
2572     @wrap_instance_fault
2573     def soft_delete_instance(self, context, instance, reservations):
2574         """Soft delete an instance on this host."""
2575 
2576         quotas = objects.Quotas.from_reservations(context,
2577                                                   reservations,
2578                                                   instance=instance)
2579         try:
2580             self._notify_about_instance_usage(context, instance,
2581                                               "soft_delete.start")
2582             try:
2583                 self.driver.soft_delete(instance)
2584             except NotImplementedError:
2585                 # Fallback to just powering off the instance if the
2586                 # hypervisor doesn't implement the soft_delete method
2587                 self.driver.power_off(instance)
2588             instance.power_state = self._get_power_state(context, instance)
2589             instance.vm_state = vm_states.SOFT_DELETED
2590             instance.task_state = None
2591             instance.save(expected_task_state=[task_states.SOFT_DELETING])
2592         except Exception:
2593             with excutils.save_and_reraise_exception():
2594                 quotas.rollback()
2595         quotas.commit()
2596         self._notify_about_instance_usage(context, instance, "soft_delete.end")
2597 
2598     @wrap_exception()
2599     @reverts_task_state
2600     @wrap_instance_event(prefix='compute')
2601     @wrap_instance_fault
2602     def restore_instance(self, context, instance):
2603         """Restore a soft-deleted instance on this host."""
2604         self._notify_about_instance_usage(context, instance, "restore.start")
2605         compute_utils.notify_about_instance_action(context, instance,
2606             self.host, action=fields.NotificationAction.RESTORE,
2607             phase=fields.NotificationPhase.START)
2608         try:
2609             self.driver.restore(instance)
2610         except NotImplementedError:
2611             # Fallback to just powering on the instance if the hypervisor
2612             # doesn't implement the restore method
2613             self._power_on(context, instance)
2614         instance.power_state = self._get_power_state(context, instance)
2615         instance.vm_state = vm_states.ACTIVE
2616         instance.task_state = None
2617         instance.save(expected_task_state=task_states.RESTORING)
2618         self._notify_about_instance_usage(context, instance, "restore.end")
2619         compute_utils.notify_about_instance_action(context, instance,
2620             self.host, action=fields.NotificationAction.RESTORE,
2621             phase=fields.NotificationPhase.END)
2622 
2623     @staticmethod
2624     def _set_migration_status(migration, status):
2625         """Set the status, and guard against a None being passed in.
2626 
2627         This is useful as some of the compute RPC calls will not pass
2628         a migration object in older versions. The check can be removed when
2629         we move past 4.x major version of the RPC API.
2630         """
2631         if migration:
2632             migration.status = status
2633             migration.save()
2634 
2635     def _rebuild_default_impl(self, context, instance, image_meta,
2636                               injected_files, admin_password, bdms,
2637                               detach_block_devices, attach_block_devices,
2638                               network_info=None,
2639                               recreate=False, block_device_info=None,
2640                               preserve_ephemeral=False):
2641         if preserve_ephemeral:
2642             # The default code path does not support preserving ephemeral
2643             # partitions.
2644             raise exception.PreserveEphemeralNotSupported()
2645 
2646         if recreate:
2647             detach_block_devices(context, bdms)
2648         else:
2649             self._power_off_instance(context, instance, clean_shutdown=True)
2650             detach_block_devices(context, bdms)
2651             self.driver.destroy(context, instance,
2652                                 network_info=network_info,
2653                                 block_device_info=block_device_info)
2654 
2655         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
2656         instance.save(expected_task_state=[task_states.REBUILDING])
2657 
2658         new_block_device_info = attach_block_devices(context, instance, bdms)
2659 
2660         instance.task_state = task_states.REBUILD_SPAWNING
2661         instance.save(
2662             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
2663 
2664         with instance.mutated_migration_context():
2665             self.driver.spawn(context, instance, image_meta, injected_files,
2666                               admin_password, network_info=network_info,
2667                               block_device_info=new_block_device_info)
2668 
2669     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
2670     @wrap_exception()
2671     @reverts_task_state
2672     @wrap_instance_event(prefix='compute')
2673     @wrap_instance_fault
2674     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
2675                          injected_files, new_pass, orig_sys_metadata,
2676                          bdms, recreate, on_shared_storage=None,
2677                          preserve_ephemeral=False, migration=None,
2678                          scheduled_node=None, limits=None):
2679         """Destroy and re-make this instance.
2680 
2681         A 'rebuild' effectively purges all existing data from the system and
2682         remakes the VM with given 'metadata' and 'personalities'.
2683 
2684         :param context: `nova.RequestContext` object
2685         :param instance: Instance object
2686         :param orig_image_ref: Original image_ref before rebuild
2687         :param image_ref: New image_ref for rebuild
2688         :param injected_files: Files to inject
2689         :param new_pass: password to set on rebuilt instance
2690         :param orig_sys_metadata: instance system metadata from pre-rebuild
2691         :param bdms: block-device-mappings to use for rebuild
2692         :param recreate: True if the instance is being recreated (e.g. the
2693             hypervisor it was on failed) - cleanup of old state will be
2694             skipped.
2695         :param on_shared_storage: True if instance files on shared storage.
2696                                   If not provided then information from the
2697                                   driver will be used to decide if the instance
2698                                   files are available or not on the target host
2699         :param preserve_ephemeral: True if the default ephemeral storage
2700                                    partition must be preserved on rebuild
2701         :param migration: a Migration object if one was created for this
2702                           rebuild operation (if it's a part of evacuate)
2703         :param scheduled_node: A node of the host chosen by the scheduler. If a
2704                                host was specified by the user, this will be
2705                                None
2706         :param limits: Overcommit limits set by the scheduler. If a host was
2707                        specified by the user, this will be None
2708         """
2709         context = context.elevated()
2710 
2711         LOG.info(_LI("Rebuilding instance"), instance=instance)
2712         if scheduled_node is not None:
2713             rt = self._get_resource_tracker()
2714             rebuild_claim = rt.rebuild_claim
2715         else:
2716             rebuild_claim = claims.NopClaim
2717 
2718         image_meta = {}
2719         if image_ref:
2720             image_meta = self.image_api.get(context, image_ref)
2721 
2722         # NOTE(mriedem): On a recreate (evacuate), we need to update
2723         # the instance's host and node properties to reflect it's
2724         # destination node for the recreate.
2725         if not scheduled_node:
2726             if recreate:
2727                 try:
2728                     compute_node = self._get_compute_info(context, self.host)
2729                     scheduled_node = compute_node.hypervisor_hostname
2730                 except exception.ComputeHostNotFound:
2731                     LOG.exception(_LE('Failed to get compute_info for %s'),
2732                                   self.host)
2733             else:
2734                 scheduled_node = instance.node
2735 
2736         with self._error_out_instance_on_exception(context, instance):
2737             try:
2738                 claim_ctxt = rebuild_claim(
2739                     context, instance, scheduled_node,
2740                     limits=limits, image_meta=image_meta,
2741                     migration=migration)
2742                 self._do_rebuild_instance_with_claim(
2743                     claim_ctxt, context, instance, orig_image_ref,
2744                     image_ref, injected_files, new_pass, orig_sys_metadata,
2745                     bdms, recreate, on_shared_storage, preserve_ephemeral)
2746             except exception.ComputeResourcesUnavailable as e:
2747                 LOG.debug("Could not rebuild instance on this host, not "
2748                           "enough resources available.", instance=instance)
2749 
2750                 # NOTE(ndipanov): We just abort the build for now and leave a
2751                 # migration record for potential cleanup later
2752                 self._set_migration_status(migration, 'failed')
2753 
2754                 self._notify_about_instance_usage(context, instance,
2755                         'rebuild.error', fault=e)
2756                 raise exception.BuildAbortException(
2757                     instance_uuid=instance.uuid, reason=e.format_message())
2758             except (exception.InstanceNotFound,
2759                     exception.UnexpectedDeletingTaskStateError) as e:
2760                 LOG.debug('Instance was deleted while rebuilding',
2761                           instance=instance)
2762                 self._set_migration_status(migration, 'failed')
2763                 self._notify_about_instance_usage(context, instance,
2764                         'rebuild.error', fault=e)
2765             except Exception as e:
2766                 self._set_migration_status(migration, 'failed')
2767                 self._notify_about_instance_usage(context, instance,
2768                         'rebuild.error', fault=e)
2769                 raise
2770             else:
2771                 instance.apply_migration_context()
2772                 # NOTE (ndipanov): This save will now update the host and node
2773                 # attributes making sure that next RT pass is consistent since
2774                 # it will be based on the instance and not the migration DB
2775                 # entry.
2776                 instance.host = self.host
2777                 instance.node = scheduled_node
2778                 instance.save()
2779                 instance.drop_migration_context()
2780 
2781                 # NOTE (ndipanov): Mark the migration as done only after we
2782                 # mark the instance as belonging to this host.
2783                 self._set_migration_status(migration, 'done')
2784 
2785     def _do_rebuild_instance_with_claim(self, claim_context, *args, **kwargs):
2786         """Helper to avoid deep nesting in the top-level method."""
2787 
2788         with claim_context:
2789             self._do_rebuild_instance(*args, **kwargs)
2790 
2791     @staticmethod
2792     def _get_image_name(image_meta):
2793         if image_meta.obj_attr_is_set("name"):
2794             return image_meta.name
2795         else:
2796             return ''
2797 
2798     def _do_rebuild_instance(self, context, instance, orig_image_ref,
2799                              image_ref, injected_files, new_pass,
2800                              orig_sys_metadata, bdms, recreate,
2801                              on_shared_storage, preserve_ephemeral):
2802         orig_vm_state = instance.vm_state
2803 
2804         if recreate:
2805             if not self.driver.capabilities["supports_recreate"]:
2806                 raise exception.InstanceRecreateNotSupported
2807 
2808             self._check_instance_exists(context, instance)
2809 
2810             if on_shared_storage is None:
2811                 LOG.debug('on_shared_storage is not provided, using driver'
2812                             'information to decide if the instance needs to'
2813                             'be recreated')
2814                 on_shared_storage = self.driver.instance_on_disk(instance)
2815 
2816             elif (on_shared_storage !=
2817                     self.driver.instance_on_disk(instance)):
2818                 # To cover case when admin expects that instance files are
2819                 # on shared storage, but not accessible and vice versa
2820                 raise exception.InvalidSharedStorage(
2821                         _("Invalid state of instance files on shared"
2822                             " storage"))
2823 
2824             if on_shared_storage:
2825                 LOG.info(_LI('disk on shared storage, recreating using'
2826                                 ' existing disk'))
2827             else:
2828                 image_ref = orig_image_ref = instance.image_ref
2829                 LOG.info(_LI("disk not on shared storage, rebuilding from:"
2830                                 " '%s'"), str(image_ref))
2831 
2832         if image_ref:
2833             image_meta = objects.ImageMeta.from_image_ref(
2834                 context, self.image_api, image_ref)
2835         else:
2836             image_meta = instance.image_meta
2837 
2838         # This instance.exists message should contain the original
2839         # image_ref, not the new one.  Since the DB has been updated
2840         # to point to the new one... we have to override it.
2841         # TODO(jaypipes): Move generate_image_url() into the nova.image.api
2842         orig_image_ref_url = glance.generate_image_url(orig_image_ref)
2843         extra_usage_info = {'image_ref_url': orig_image_ref_url}
2844         compute_utils.notify_usage_exists(
2845                 self.notifier, context, instance,
2846                 current_period=True, system_metadata=orig_sys_metadata,
2847                 extra_usage_info=extra_usage_info)
2848 
2849         # This message should contain the new image_ref
2850         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
2851         self._notify_about_instance_usage(context, instance,
2852                 "rebuild.start", extra_usage_info=extra_usage_info)
2853 
2854         instance.power_state = self._get_power_state(context, instance)
2855         instance.task_state = task_states.REBUILDING
2856         instance.save(expected_task_state=[task_states.REBUILDING])
2857 
2858         if recreate:
2859             self.network_api.setup_networks_on_host(
2860                     context, instance, self.host)
2861             # For nova-network this is needed to move floating IPs
2862             # For neutron this updates the host in the port binding
2863             # TODO(cfriesen): this network_api call and the one above
2864             # are so similar, we should really try to unify them.
2865             self.network_api.setup_instance_network_on_host(
2866                     context, instance, self.host)
2867 
2868         network_info = compute_utils.get_nw_info_for_instance(instance)
2869         if bdms is None:
2870             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2871                     context, instance.uuid)
2872 
2873         block_device_info = \
2874             self._get_instance_block_device_info(
2875                     context, instance, bdms=bdms)
2876 
2877         def detach_block_devices(context, bdms):
2878             for bdm in bdms:
2879                 if bdm.is_volume:
2880                     self._detach_volume(context, bdm.volume_id, instance,
2881                                         destroy_bdm=False)
2882 
2883         files = self._decode_files(injected_files)
2884 
2885         kwargs = dict(
2886             context=context,
2887             instance=instance,
2888             image_meta=image_meta,
2889             injected_files=files,
2890             admin_password=new_pass,
2891             bdms=bdms,
2892             detach_block_devices=detach_block_devices,
2893             attach_block_devices=self._prep_block_device,
2894             block_device_info=block_device_info,
2895             network_info=network_info,
2896             preserve_ephemeral=preserve_ephemeral,
2897             recreate=recreate)
2898         try:
2899             with instance.mutated_migration_context():
2900                 self.driver.rebuild(**kwargs)
2901         except NotImplementedError:
2902             # NOTE(rpodolyaka): driver doesn't provide specialized version
2903             # of rebuild, fall back to the default implementation
2904             self._rebuild_default_impl(**kwargs)
2905         self._update_instance_after_spawn(context, instance)
2906         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
2907 
2908         if orig_vm_state == vm_states.STOPPED:
2909             LOG.info(_LI("bringing vm to original state: '%s'"),
2910                         orig_vm_state, instance=instance)
2911             instance.vm_state = vm_states.ACTIVE
2912             instance.task_state = task_states.POWERING_OFF
2913             instance.progress = 0
2914             instance.save()
2915             self.stop_instance(context, instance, False)
2916         self._update_scheduler_instance_info(context, instance)
2917         self._notify_about_instance_usage(
2918                 context, instance, "rebuild.end",
2919                 network_info=network_info,
2920                 extra_usage_info=extra_usage_info)
2921 
2922     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
2923                                      block_device_info):
2924         """Handle cases where the virt-layer had to detach non-working volumes
2925         in order to complete an operation.
2926         """
2927         for bdm in block_device_info['block_device_mapping']:
2928             if bdm.get('mount_device') in bad_devices:
2929                 try:
2930                     volume_id = bdm['connection_info']['data']['volume_id']
2931                 except KeyError:
2932                     continue
2933 
2934                 # NOTE(sirp): ideally we'd just call
2935                 # `compute_api.detach_volume` here but since that hits the
2936                 # DB directly, that's off limits from within the
2937                 # compute-manager.
2938                 #
2939                 # API-detach
2940                 LOG.info(_LI("Detaching from volume api: %s"), volume_id)
2941                 volume = self.volume_api.get(context, volume_id)
2942                 self.volume_api.check_detach(context, volume)
2943                 self.volume_api.begin_detaching(context, volume_id)
2944 
2945                 # Manager-detach
2946                 self.detach_volume(context, volume_id, instance)
2947 
2948     @wrap_exception()
2949     @reverts_task_state
2950     @wrap_instance_event(prefix='compute')
2951     @wrap_instance_fault
2952     def reboot_instance(self, context, instance, block_device_info,
2953                         reboot_type):
2954         """Reboot an instance on this host."""
2955         # acknowledge the request made it to the manager
2956         if reboot_type == "SOFT":
2957             instance.task_state = task_states.REBOOT_PENDING
2958             expected_states = (task_states.REBOOTING,
2959                                task_states.REBOOT_PENDING,
2960                                task_states.REBOOT_STARTED)
2961         else:
2962             instance.task_state = task_states.REBOOT_PENDING_HARD
2963             expected_states = (task_states.REBOOTING_HARD,
2964                                task_states.REBOOT_PENDING_HARD,
2965                                task_states.REBOOT_STARTED_HARD)
2966         context = context.elevated()
2967         LOG.info(_LI("Rebooting instance"), instance=instance)
2968 
2969         block_device_info = self._get_instance_block_device_info(context,
2970                                                                  instance)
2971 
2972         network_info = self.network_api.get_instance_nw_info(context, instance)
2973 
2974         self._notify_about_instance_usage(context, instance, "reboot.start")
2975 
2976         instance.power_state = self._get_power_state(context, instance)
2977         instance.save(expected_task_state=expected_states)
2978 
2979         if instance.power_state != power_state.RUNNING:
2980             state = instance.power_state
2981             running = power_state.RUNNING
2982             LOG.warning(_LW('trying to reboot a non-running instance:'
2983                             ' (state: %(state)s expected: %(running)s)'),
2984                         {'state': state, 'running': running},
2985                         instance=instance)
2986 
2987         def bad_volumes_callback(bad_devices):
2988             self._handle_bad_volumes_detached(
2989                     context, instance, bad_devices, block_device_info)
2990 
2991         try:
2992             # Don't change it out of rescue mode
2993             if instance.vm_state == vm_states.RESCUED:
2994                 new_vm_state = vm_states.RESCUED
2995             else:
2996                 new_vm_state = vm_states.ACTIVE
2997             new_power_state = None
2998             if reboot_type == "SOFT":
2999                 instance.task_state = task_states.REBOOT_STARTED
3000                 expected_state = task_states.REBOOT_PENDING
3001             else:
3002                 instance.task_state = task_states.REBOOT_STARTED_HARD
3003                 expected_state = task_states.REBOOT_PENDING_HARD
3004             instance.save(expected_task_state=expected_state)
3005             self.driver.reboot(context, instance,
3006                                network_info,
3007                                reboot_type,
3008                                block_device_info=block_device_info,
3009                                bad_volumes_callback=bad_volumes_callback)
3010 
3011         except Exception as error:
3012             with excutils.save_and_reraise_exception() as ctxt:
3013                 exc_info = sys.exc_info()
3014                 # if the reboot failed but the VM is running don't
3015                 # put it into an error state
3016                 new_power_state = self._get_power_state(context, instance)
3017                 if new_power_state == power_state.RUNNING:
3018                     LOG.warning(_LW('Reboot failed but instance is running'),
3019                                 instance=instance)
3020                     compute_utils.add_instance_fault_from_exc(context,
3021                             instance, error, exc_info)
3022                     self._notify_about_instance_usage(context, instance,
3023                             'reboot.error', fault=error)
3024                     ctxt.reraise = False
3025                 else:
3026                     LOG.error(_LE('Cannot reboot instance: %s'), error,
3027                               instance=instance)
3028                     self._set_instance_obj_error_state(context, instance)
3029 
3030         if not new_power_state:
3031             new_power_state = self._get_power_state(context, instance)
3032         try:
3033             instance.power_state = new_power_state
3034             instance.vm_state = new_vm_state
3035             instance.task_state = None
3036             instance.save()
3037         except exception.InstanceNotFound:
3038             LOG.warning(_LW("Instance disappeared during reboot"),
3039                         instance=instance)
3040 
3041         self._notify_about_instance_usage(context, instance, "reboot.end")
3042 
3043     @delete_image_on_error
3044     def _do_snapshot_instance(self, context, image_id, instance):
3045         self._snapshot_instance(context, image_id, instance,
3046                                 task_states.IMAGE_BACKUP)
3047 
3048     @wrap_exception()
3049     @reverts_task_state
3050     @wrap_instance_fault
3051     def backup_instance(self, context, image_id, instance, backup_type,
3052                         rotation):
3053         """Backup an instance on this host.
3054 
3055         :param backup_type: daily | weekly
3056         :param rotation: int representing how many backups to keep around
3057         """
3058         self._do_snapshot_instance(context, image_id, instance)
3059         self._rotate_backups(context, instance, backup_type, rotation)
3060 
3061     @wrap_exception()
3062     @reverts_task_state
3063     @wrap_instance_fault
3064     @delete_image_on_error
3065     def snapshot_instance(self, context, image_id, instance):
3066         """Snapshot an instance on this host.
3067 
3068         :param context: security context
3069         :param image_id: glance.db.sqlalchemy.models.Image.Id
3070         :param instance: a nova.objects.instance.Instance object
3071         """
3072         # NOTE(dave-mcnally) the task state will already be set by the api
3073         # but if the compute manager has crashed/been restarted prior to the
3074         # request getting here the task state may have been cleared so we set
3075         # it again and things continue normally
3076         try:
3077             instance.task_state = task_states.IMAGE_SNAPSHOT
3078             instance.save(
3079                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3080         except exception.InstanceNotFound:
3081             # possibility instance no longer exists, no point in continuing
3082             LOG.debug("Instance not found, could not set state %s "
3083                       "for instance.",
3084                       task_states.IMAGE_SNAPSHOT, instance=instance)
3085             return
3086 
3087         except exception.UnexpectedDeletingTaskStateError:
3088             LOG.debug("Instance being deleted, snapshot cannot continue",
3089                       instance=instance)
3090             return
3091 
3092         self._snapshot_instance(context, image_id, instance,
3093                                 task_states.IMAGE_SNAPSHOT)
3094 
3095     def _snapshot_instance(self, context, image_id, instance,
3096                            expected_task_state):
3097         context = context.elevated()
3098 
3099         instance.power_state = self._get_power_state(context, instance)
3100         try:
3101             instance.save()
3102 
3103             LOG.info(_LI('instance snapshotting'), instance=instance)
3104 
3105             if instance.power_state != power_state.RUNNING:
3106                 state = instance.power_state
3107                 running = power_state.RUNNING
3108                 LOG.warning(_LW('trying to snapshot a non-running instance: '
3109                                 '(state: %(state)s expected: %(running)s)'),
3110                             {'state': state, 'running': running},
3111                             instance=instance)
3112 
3113             self._notify_about_instance_usage(
3114                 context, instance, "snapshot.start")
3115             compute_utils.notify_about_instance_action(context, instance,
3116                 self.host, action=fields.NotificationAction.SNAPSHOT,
3117                 phase=fields.NotificationPhase.START)
3118 
3119             def update_task_state(task_state,
3120                                   expected_state=expected_task_state):
3121                 instance.task_state = task_state
3122                 instance.save(expected_task_state=expected_state)
3123 
3124             self.driver.snapshot(context, instance, image_id,
3125                                  update_task_state)
3126 
3127             instance.task_state = None
3128             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3129 
3130             self._notify_about_instance_usage(context, instance,
3131                                               "snapshot.end")
3132             compute_utils.notify_about_instance_action(context, instance,
3133                 self.host, action=fields.NotificationAction.SNAPSHOT,
3134                 phase=fields.NotificationPhase.END)
3135         except (exception.InstanceNotFound,
3136                 exception.UnexpectedDeletingTaskStateError):
3137             # the instance got deleted during the snapshot
3138             # Quickly bail out of here
3139             msg = 'Instance disappeared during snapshot'
3140             LOG.debug(msg, instance=instance)
3141             try:
3142                 image_service = glance.get_default_image_service()
3143                 image = image_service.show(context, image_id)
3144                 if image['status'] != 'active':
3145                     image_service.delete(context, image_id)
3146             except Exception:
3147                 LOG.warning(_LW("Error while trying to clean up image %s"),
3148                             image_id, instance=instance)
3149         except exception.ImageNotFound:
3150             instance.task_state = None
3151             instance.save()
3152             msg = _LW("Image not found during snapshot")
3153             LOG.warning(msg, instance=instance)
3154 
3155     def _post_interrupted_snapshot_cleanup(self, context, instance):
3156         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3157 
3158     @messaging.expected_exceptions(NotImplementedError)
3159     @wrap_exception()
3160     def volume_snapshot_create(self, context, instance, volume_id,
3161                                create_info):
3162         self.driver.volume_snapshot_create(context, instance, volume_id,
3163                                            create_info)
3164 
3165     @messaging.expected_exceptions(NotImplementedError)
3166     @wrap_exception()
3167     def volume_snapshot_delete(self, context, instance, volume_id,
3168                                snapshot_id, delete_info):
3169         self.driver.volume_snapshot_delete(context, instance, volume_id,
3170                                            snapshot_id, delete_info)
3171 
3172     @wrap_instance_fault
3173     def _rotate_backups(self, context, instance, backup_type, rotation):
3174         """Delete excess backups associated to an instance.
3175 
3176         Instances are allowed a fixed number of backups (the rotation number);
3177         this method deletes the oldest backups that exceed the rotation
3178         threshold.
3179 
3180         :param context: security context
3181         :param instance: Instance dict
3182         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3183         :param rotation: int representing how many backups to keep around;
3184             None if rotation shouldn't be used (as in the case of snapshots)
3185         """
3186         filters = {'property-image_type': 'backup',
3187                    'property-backup_type': backup_type,
3188                    'property-instance_uuid': instance.uuid}
3189 
3190         images = self.image_api.get_all(context, filters=filters,
3191                                         sort_key='created_at', sort_dir='desc')
3192         num_images = len(images)
3193         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3194                   {'num_images': num_images, 'rotation': rotation},
3195                   instance=instance)
3196 
3197         if num_images > rotation:
3198             # NOTE(sirp): this deletes all backups that exceed the rotation
3199             # limit
3200             excess = len(images) - rotation
3201             LOG.debug("Rotating out %d backups", excess,
3202                       instance=instance)
3203             for i in range(excess):
3204                 image = images.pop()
3205                 image_id = image['id']
3206                 LOG.debug("Deleting image %s", image_id,
3207                           instance=instance)
3208                 try:
3209                     self.image_api.delete(context, image_id)
3210                 except exception.ImageNotFound:
3211                     LOG.info(_LI("Failed to find image %(image_id)s to "
3212                                  "delete"), {'image_id': image_id},
3213                              instance=instance)
3214 
3215     @wrap_exception()
3216     @reverts_task_state
3217     @wrap_instance_event(prefix='compute')
3218     @wrap_instance_fault
3219     def set_admin_password(self, context, instance, new_pass):
3220         """Set the root/admin password for an instance on this host.
3221 
3222         This is generally only called by API password resets after an
3223         image has been built.
3224 
3225         @param context: Nova auth context.
3226         @param instance: Nova instance object.
3227         @param new_pass: The admin password for the instance.
3228         """
3229 
3230         context = context.elevated()
3231         if new_pass is None:
3232             # Generate a random password
3233             new_pass = utils.generate_password()
3234 
3235         current_power_state = self._get_power_state(context, instance)
3236         expected_state = power_state.RUNNING
3237 
3238         if current_power_state != expected_state:
3239             instance.task_state = None
3240             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
3241             _msg = _('instance %s is not running') % instance.uuid
3242             raise exception.InstancePasswordSetFailed(
3243                 instance=instance.uuid, reason=_msg)
3244 
3245         try:
3246             self.driver.set_admin_password(instance, new_pass)
3247             LOG.info(_LI("Root password set"), instance=instance)
3248             instance.task_state = None
3249             instance.save(
3250                 expected_task_state=task_states.UPDATING_PASSWORD)
3251         except exception.InstanceAgentNotEnabled:
3252             with excutils.save_and_reraise_exception():
3253                 LOG.debug('Guest agent is not enabled for the instance.',
3254                           instance=instance)
3255                 instance.task_state = None
3256                 instance.save(
3257                     expected_task_state=task_states.UPDATING_PASSWORD)
3258         except exception.SetAdminPasswdNotSupported:
3259             with excutils.save_and_reraise_exception():
3260                 LOG.info(_LI('set_admin_password is not supported '
3261                                 'by this driver or guest instance.'),
3262                             instance=instance)
3263                 instance.task_state = None
3264                 instance.save(
3265                     expected_task_state=task_states.UPDATING_PASSWORD)
3266         except NotImplementedError:
3267             LOG.warning(_LW('set_admin_password is not implemented '
3268                             'by this driver or guest instance.'),
3269                         instance=instance)
3270             instance.task_state = None
3271             instance.save(
3272                 expected_task_state=task_states.UPDATING_PASSWORD)
3273             raise NotImplementedError(_('set_admin_password is not '
3274                                         'implemented by this driver or guest '
3275                                         'instance.'))
3276         except exception.UnexpectedTaskStateError:
3277             # interrupted by another (most likely delete) task
3278             # do not retry
3279             raise
3280         except Exception:
3281             # Catch all here because this could be anything.
3282             LOG.exception(_LE('set_admin_password failed'),
3283                           instance=instance)
3284             self._set_instance_obj_error_state(context, instance)
3285             # We create a new exception here so that we won't
3286             # potentially reveal password information to the
3287             # API caller.  The real exception is logged above
3288             _msg = _('error setting admin password')
3289             raise exception.InstancePasswordSetFailed(
3290                 instance=instance.uuid, reason=_msg)
3291 
3292     @wrap_exception()
3293     @reverts_task_state
3294     @wrap_instance_fault
3295     def inject_file(self, context, path, file_contents, instance):
3296         """Write a file to the specified path in an instance on this host."""
3297         # NOTE(russellb) Remove this method, as well as the underlying virt
3298         # driver methods, when the compute rpc interface is bumped to 4.x
3299         # as it is no longer used.
3300         context = context.elevated()
3301         current_power_state = self._get_power_state(context, instance)
3302         expected_state = power_state.RUNNING
3303         if current_power_state != expected_state:
3304             LOG.warning(_LW('trying to inject a file into a non-running '
3305                             '(state: %(current_state)s expected: '
3306                             '%(expected_state)s)'),
3307                         {'current_state': current_power_state,
3308                          'expected_state': expected_state},
3309                         instance=instance)
3310         LOG.info(_LI('injecting file to %s'), path,
3311                     instance=instance)
3312         self.driver.inject_file(instance, path, file_contents)
3313 
3314     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
3315         """Determine what image should be used to boot the rescue VM."""
3316         # 1. If rescue_image_ref is passed in, use that for rescue.
3317         # 2. Else, use the base image associated with instance's current image.
3318         #       The idea here is to provide the customer with a rescue
3319         #       environment which they are familiar with.
3320         #       So, if they built their instance off of a Debian image,
3321         #       their rescue VM will also be Debian.
3322         # 3. As a last resort, use instance's current image.
3323         if not rescue_image_ref:
3324             system_meta = utils.instance_sys_meta(instance)
3325             rescue_image_ref = system_meta.get('image_base_image_ref')
3326 
3327         if not rescue_image_ref:
3328             LOG.warning(_LW('Unable to find a different image to use for '
3329                             'rescue VM, using instance\'s current image'),
3330                         instance=instance)
3331             rescue_image_ref = instance.image_ref
3332 
3333         return objects.ImageMeta.from_image_ref(
3334             context, self.image_api, rescue_image_ref)
3335 
3336     @wrap_exception()
3337     @reverts_task_state
3338     @wrap_instance_event(prefix='compute')
3339     @wrap_instance_fault
3340     def rescue_instance(self, context, instance, rescue_password,
3341                         rescue_image_ref, clean_shutdown):
3342         context = context.elevated()
3343         LOG.info(_LI('Rescuing'), instance=instance)
3344 
3345         admin_password = (rescue_password if rescue_password else
3346                       utils.generate_password())
3347 
3348         network_info = self.network_api.get_instance_nw_info(context, instance)
3349 
3350         rescue_image_meta = self._get_rescue_image(context, instance,
3351                                                    rescue_image_ref)
3352 
3353         extra_usage_info = {'rescue_image_name':
3354                             self._get_image_name(rescue_image_meta)}
3355         self._notify_about_instance_usage(context, instance,
3356                 "rescue.start", extra_usage_info=extra_usage_info,
3357                 network_info=network_info)
3358 
3359         try:
3360             self._power_off_instance(context, instance, clean_shutdown)
3361 
3362             self.driver.rescue(context, instance,
3363                                network_info,
3364                                rescue_image_meta, admin_password)
3365         except Exception as e:
3366             LOG.exception(_LE("Error trying to Rescue Instance"),
3367                           instance=instance)
3368             self._set_instance_obj_error_state(context, instance)
3369             raise exception.InstanceNotRescuable(
3370                 instance_id=instance.uuid,
3371                 reason=_("Driver Error: %s") % e)
3372 
3373         compute_utils.notify_usage_exists(self.notifier, context, instance,
3374                                           current_period=True)
3375 
3376         instance.vm_state = vm_states.RESCUED
3377         instance.task_state = None
3378         instance.power_state = self._get_power_state(context, instance)
3379         instance.launched_at = timeutils.utcnow()
3380         instance.save(expected_task_state=task_states.RESCUING)
3381 
3382         self._notify_about_instance_usage(context, instance,
3383                 "rescue.end", extra_usage_info=extra_usage_info,
3384                 network_info=network_info)
3385 
3386     @wrap_exception()
3387     @reverts_task_state
3388     @wrap_instance_event(prefix='compute')
3389     @wrap_instance_fault
3390     def unrescue_instance(self, context, instance):
3391         context = context.elevated()
3392         LOG.info(_LI('Unrescuing'), instance=instance)
3393 
3394         network_info = self.network_api.get_instance_nw_info(context, instance)
3395         self._notify_about_instance_usage(context, instance,
3396                 "unrescue.start", network_info=network_info)
3397         with self._error_out_instance_on_exception(context, instance):
3398             self.driver.unrescue(instance,
3399                                  network_info)
3400 
3401         instance.vm_state = vm_states.ACTIVE
3402         instance.task_state = None
3403         instance.power_state = self._get_power_state(context, instance)
3404         instance.save(expected_task_state=task_states.UNRESCUING)
3405 
3406         self._notify_about_instance_usage(context,
3407                                           instance,
3408                                           "unrescue.end",
3409                                           network_info=network_info)
3410 
3411     @wrap_exception()
3412     @wrap_instance_fault
3413     def change_instance_metadata(self, context, diff, instance):
3414         """Update the metadata published to the instance."""
3415         LOG.debug("Changing instance metadata according to %r",
3416                   diff, instance=instance)
3417         self.driver.change_instance_metadata(context, instance, diff)
3418 
3419     @wrap_exception()
3420     @wrap_instance_event(prefix='compute')
3421     @wrap_instance_fault
3422     def confirm_resize(self, context, instance, reservations, migration):
3423 
3424         quotas = objects.Quotas.from_reservations(context,
3425                                                   reservations,
3426                                                   instance=instance)
3427 
3428         @utils.synchronized(instance.uuid)
3429         def do_confirm_resize(context, instance, migration_id):
3430             # NOTE(wangpan): Get the migration status from db, if it has been
3431             #                confirmed, we do nothing and return here
3432             LOG.debug("Going to confirm migration %s", migration_id,
3433                       instance=instance)
3434             try:
3435                 # TODO(russellb) Why are we sending the migration object just
3436                 # to turn around and look it up from the db again?
3437                 migration = objects.Migration.get_by_id(
3438                                     context.elevated(), migration_id)
3439             except exception.MigrationNotFound:
3440                 LOG.error(_LE("Migration %s is not found during confirmation"),
3441                           migration_id, instance=instance)
3442                 quotas.rollback()
3443                 return
3444 
3445             if migration.status == 'confirmed':
3446                 LOG.info(_LI("Migration %s is already confirmed"),
3447                          migration_id, instance=instance)
3448                 quotas.rollback()
3449                 return
3450             elif migration.status not in ('finished', 'confirming'):
3451                 LOG.warning(_LW("Unexpected confirmation status '%(status)s' "
3452                                 "of migration %(id)s, exit confirmation "
3453                                 "process"),
3454                             {"status": migration.status, "id": migration_id},
3455                             instance=instance)
3456                 quotas.rollback()
3457                 return
3458 
3459             # NOTE(wangpan): Get the instance from db, if it has been
3460             #                deleted, we do nothing and return here
3461             expected_attrs = ['metadata', 'system_metadata', 'flavor']
3462             try:
3463                 instance = objects.Instance.get_by_uuid(
3464                         context, instance.uuid,
3465                         expected_attrs=expected_attrs)
3466             except exception.InstanceNotFound:
3467                 LOG.info(_LI("Instance is not found during confirmation"),
3468                          instance=instance)
3469                 quotas.rollback()
3470                 return
3471 
3472             self._confirm_resize(context, instance, quotas,
3473                                  migration=migration)
3474 
3475         do_confirm_resize(context, instance, migration.id)
3476 
3477     def _confirm_resize(self, context, instance, quotas,
3478                         migration=None):
3479         """Destroys the source instance."""
3480         self._notify_about_instance_usage(context, instance,
3481                                           "resize.confirm.start")
3482 
3483         with self._error_out_instance_on_exception(context, instance,
3484                                                    quotas=quotas):
3485             # NOTE(danms): delete stashed migration information
3486             old_instance_type = instance.old_flavor
3487             instance.old_flavor = None
3488             instance.new_flavor = None
3489             instance.system_metadata.pop('old_vm_state', None)
3490             instance.save()
3491 
3492             # NOTE(tr3buchet): tear down networks on source host
3493             self.network_api.setup_networks_on_host(context, instance,
3494                                migration.source_compute, teardown=True)
3495 
3496             network_info = self.network_api.get_instance_nw_info(context,
3497                                                                  instance)
3498             self.driver.confirm_migration(context, migration, instance,
3499                                           network_info)
3500 
3501             migration.status = 'confirmed'
3502             with migration.obj_as_admin():
3503                 migration.save()
3504 
3505             rt = self._get_resource_tracker()
3506             rt.drop_move_claim(context, instance, migration.source_node,
3507                                old_instance_type, prefix='old_')
3508             instance.drop_migration_context()
3509 
3510             # NOTE(mriedem): The old_vm_state could be STOPPED but the user
3511             # might have manually powered up the instance to confirm the
3512             # resize/migrate, so we need to check the current power state
3513             # on the instance and set the vm_state appropriately. We default
3514             # to ACTIVE because if the power state is not SHUTDOWN, we
3515             # assume _sync_instance_power_state will clean it up.
3516             p_state = instance.power_state
3517             vm_state = None
3518             if p_state == power_state.SHUTDOWN:
3519                 vm_state = vm_states.STOPPED
3520                 LOG.debug("Resized/migrated instance is powered off. "
3521                           "Setting vm_state to '%s'.", vm_state,
3522                           instance=instance)
3523             else:
3524                 vm_state = vm_states.ACTIVE
3525 
3526             instance.vm_state = vm_state
3527             instance.task_state = None
3528             instance.save(expected_task_state=[None, task_states.DELETING])
3529 
3530             self._notify_about_instance_usage(
3531                 context, instance, "resize.confirm.end",
3532                 network_info=network_info)
3533 
3534             quotas.commit()
3535 
3536     @wrap_exception()
3537     @reverts_task_state
3538     @wrap_instance_event(prefix='compute')
3539     @errors_out_migration
3540     @wrap_instance_fault
3541     def revert_resize(self, context, instance, migration, reservations):
3542         """Destroys the new instance on the destination machine.
3543 
3544         Reverts the model changes, and powers on the old instance on the
3545         source machine.
3546 
3547         """
3548 
3549         quotas = objects.Quotas.from_reservations(context,
3550                                                   reservations,
3551                                                   instance=instance)
3552 
3553         # NOTE(comstud): A revert_resize is essentially a resize back to
3554         # the old size, so we need to send a usage event here.
3555         compute_utils.notify_usage_exists(self.notifier, context, instance,
3556                                           current_period=True)
3557 
3558         with self._error_out_instance_on_exception(context, instance,
3559                                                    quotas=quotas):
3560             # NOTE(tr3buchet): tear down networks on destination host
3561             self.network_api.setup_networks_on_host(context, instance,
3562                                                     teardown=True)
3563 
3564             migration_p = obj_base.obj_to_primitive(migration)
3565             self.network_api.migrate_instance_start(context,
3566                                                     instance,
3567                                                     migration_p)
3568 
3569             network_info = self.network_api.get_instance_nw_info(context,
3570                                                                  instance)
3571             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3572                     context, instance.uuid)
3573             block_device_info = self._get_instance_block_device_info(
3574                                 context, instance, bdms=bdms)
3575 
3576             destroy_disks = not self._is_instance_storage_shared(
3577                 context, instance, host=migration.source_compute)
3578             self.driver.destroy(context, instance, network_info,
3579                                 block_device_info, destroy_disks)
3580 
3581             self._terminate_volume_connections(context, instance, bdms)
3582 
3583             migration.status = 'reverted'
3584             with migration.obj_as_admin():
3585                 migration.save()
3586 
3587             # NOTE(ndipanov): We need to do this here because dropping the
3588             # claim means we lose the migration_context data. We really should
3589             # fix this by moving the drop_move_claim call to the
3590             # finish_revert_resize method as this is racy (revert is dropped,
3591             # but instance resources will be tracked with the new flavor until
3592             # it gets rolled back in finish_revert_resize, which is
3593             # potentially wrong for a period of time).
3594             instance.revert_migration_context()
3595             instance.save()
3596 
3597             rt = self._get_resource_tracker()
3598             rt.drop_move_claim(context, instance, instance.node)
3599 
3600             self.compute_rpcapi.finish_revert_resize(context, instance,
3601                     migration, migration.source_compute,
3602                     quotas.reservations)
3603 
3604     @wrap_exception()
3605     @reverts_task_state
3606     @wrap_instance_event(prefix='compute')
3607     @errors_out_migration
3608     @wrap_instance_fault
3609     def finish_revert_resize(self, context, instance, reservations, migration):
3610         """Finishes the second half of reverting a resize.
3611 
3612         Bring the original source instance state back (active/shutoff) and
3613         revert the resized attributes in the database.
3614 
3615         """
3616 
3617         quotas = objects.Quotas.from_reservations(context,
3618                                                   reservations,
3619                                                   instance=instance)
3620 
3621         with self._error_out_instance_on_exception(context, instance,
3622                                                    quotas=quotas):
3623             self._notify_about_instance_usage(
3624                     context, instance, "resize.revert.start")
3625 
3626             # NOTE(mriedem): delete stashed old_vm_state information; we
3627             # default to ACTIVE for backwards compatibility if old_vm_state
3628             # is not set
3629             old_vm_state = instance.system_metadata.pop('old_vm_state',
3630                                                         vm_states.ACTIVE)
3631 
3632             self._set_instance_info(instance, instance.old_flavor)
3633             instance.old_flavor = None
3634             instance.new_flavor = None
3635             instance.host = migration.source_compute
3636             instance.node = migration.source_node
3637             instance.save()
3638 
3639             self.network_api.setup_networks_on_host(context, instance,
3640                                                     migration.source_compute)
3641             migration_p = obj_base.obj_to_primitive(migration)
3642             # NOTE(hanrong): we need to change migration_p['dest_compute'] to
3643             # source host temporarily. "network_api.migrate_instance_finish"
3644             # will setup the network for the instance on the destination host.
3645             # For revert resize, the instance will back to the source host, the
3646             # setup of the network for instance should be on the source host.
3647             # So set the migration_p['dest_compute'] to source host at here.
3648             migration_p['dest_compute'] = migration.source_compute
3649             self.network_api.migrate_instance_finish(context,
3650                                                      instance,
3651                                                      migration_p)
3652             network_info = self.network_api.get_instance_nw_info(context,
3653                                                                  instance)
3654 
3655             block_device_info = self._get_instance_block_device_info(
3656                     context, instance, refresh_conn_info=True)
3657 
3658             power_on = old_vm_state != vm_states.STOPPED
3659             self.driver.finish_revert_migration(context, instance,
3660                                        network_info,
3661                                        block_device_info, power_on)
3662 
3663             instance.drop_migration_context()
3664             instance.launched_at = timeutils.utcnow()
3665             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
3666 
3667             # if the original vm state was STOPPED, set it back to STOPPED
3668             LOG.info(_LI("Updating instance to original state: '%s'"),
3669                      old_vm_state, instance=instance)
3670             if power_on:
3671                 instance.vm_state = vm_states.ACTIVE
3672                 instance.task_state = None
3673                 instance.save()
3674             else:
3675                 instance.task_state = task_states.POWERING_OFF
3676                 instance.save()
3677                 self.stop_instance(context, instance=instance,
3678                                    clean_shutdown=True)
3679 
3680             self._notify_about_instance_usage(
3681                     context, instance, "resize.revert.end")
3682             quotas.commit()
3683 
3684     def _prep_resize(self, context, image, instance, instance_type,
3685             quotas, request_spec, filter_properties, node,
3686             clean_shutdown=True):
3687 
3688         if not filter_properties:
3689             filter_properties = {}
3690 
3691         if not instance.host:
3692             self._set_instance_obj_error_state(context, instance)
3693             msg = _('Instance has no source host')
3694             raise exception.MigrationError(reason=msg)
3695 
3696         same_host = instance.host == self.host
3697         # if the flavor IDs match, it's migrate; otherwise resize
3698         if same_host and instance_type.id == instance['instance_type_id']:
3699             # check driver whether support migrate to same host
3700             if not self.driver.capabilities['supports_migrate_to_same_host']:
3701                 raise exception.UnableToMigrateToSelf(
3702                     instance_id=instance.uuid, host=self.host)
3703 
3704         # NOTE(danms): Stash the new instance_type to avoid having to
3705         # look it up in the database later
3706         instance.new_flavor = instance_type
3707         # NOTE(mriedem): Stash the old vm_state so we can set the
3708         # resized/reverted instance back to the same state later.
3709         vm_state = instance.vm_state
3710         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
3711         instance.system_metadata['old_vm_state'] = vm_state
3712         instance.save()
3713 
3714         limits = filter_properties.get('limits', {})
3715         rt = self._get_resource_tracker()
3716         with rt.resize_claim(context, instance, instance_type, node,
3717                              image_meta=image, limits=limits) as claim:
3718             LOG.info(_LI('Migrating'), instance=instance)
3719             self.compute_rpcapi.resize_instance(
3720                     context, instance, claim.migration, image,
3721                     instance_type, quotas.reservations,
3722                     clean_shutdown)
3723 
3724     @wrap_exception()
3725     @reverts_task_state
3726     @wrap_instance_event(prefix='compute')
3727     @wrap_instance_fault
3728     def prep_resize(self, context, image, instance, instance_type,
3729                     reservations, request_spec, filter_properties, node,
3730                     clean_shutdown):
3731         """Initiates the process of moving a running instance to another host.
3732 
3733         Possibly changes the RAM and disk size in the process.
3734 
3735         """
3736         if node is None:
3737             node = self.driver.get_available_nodes(refresh=True)[0]
3738             LOG.debug("No node specified, defaulting to %s", node,
3739                       instance=instance)
3740 
3741         # NOTE(melwitt): Remove this in version 5.0 of the RPC API
3742         # Code downstream may expect extra_specs to be populated since it
3743         # is receiving an object, so lookup the flavor to ensure this.
3744         if not isinstance(instance_type, objects.Flavor):
3745             instance_type = objects.Flavor.get_by_id(context,
3746                                                      instance_type['id'])
3747 
3748         quotas = objects.Quotas.from_reservations(context,
3749                                                   reservations,
3750                                                   instance=instance)
3751         with self._error_out_instance_on_exception(context, instance,
3752                                                    quotas=quotas):
3753             compute_utils.notify_usage_exists(self.notifier, context, instance,
3754                                               current_period=True)
3755             self._notify_about_instance_usage(
3756                     context, instance, "resize.prep.start")
3757             try:
3758                 self._prep_resize(context, image, instance,
3759                                   instance_type, quotas,
3760                                   request_spec, filter_properties,
3761                                   node, clean_shutdown)
3762             # NOTE(dgenin): This is thrown in LibvirtDriver when the
3763             #               instance to be migrated is backed by LVM.
3764             #               Remove when LVM migration is implemented.
3765             except exception.MigrationPreCheckError:
3766                 raise
3767             except Exception:
3768                 # try to re-schedule the resize elsewhere:
3769                 exc_info = sys.exc_info()
3770                 self._reschedule_resize_or_reraise(context, image, instance,
3771                         exc_info, instance_type, quotas, request_spec,
3772                         filter_properties)
3773             finally:
3774                 extra_usage_info = dict(
3775                         new_instance_type=instance_type.name,
3776                         new_instance_type_id=instance_type.id)
3777 
3778                 self._notify_about_instance_usage(
3779                     context, instance, "resize.prep.end",
3780                     extra_usage_info=extra_usage_info)
3781 
3782     def _reschedule_resize_or_reraise(self, context, image, instance, exc_info,
3783             instance_type, quotas, request_spec, filter_properties):
3784         """Try to re-schedule the resize or re-raise the original error to
3785         error out the instance.
3786         """
3787         if not request_spec:
3788             request_spec = {}
3789         if not filter_properties:
3790             filter_properties = {}
3791 
3792         rescheduled = False
3793         instance_uuid = instance.uuid
3794 
3795         try:
3796             reschedule_method = self.compute_task_api.resize_instance
3797             scheduler_hint = dict(filter_properties=filter_properties)
3798             method_args = (instance, None, scheduler_hint, instance_type,
3799                            quotas.reservations)
3800             task_state = task_states.RESIZE_PREP
3801 
3802             rescheduled = self._reschedule(context, request_spec,
3803                     filter_properties, instance, reschedule_method,
3804                     method_args, task_state, exc_info)
3805         except Exception as error:
3806             rescheduled = False
3807             LOG.exception(_LE("Error trying to reschedule"),
3808                           instance_uuid=instance_uuid)
3809             compute_utils.add_instance_fault_from_exc(context,
3810                     instance, error,
3811                     exc_info=sys.exc_info())
3812             self._notify_about_instance_usage(context, instance,
3813                     'resize.error', fault=error)
3814 
3815         if rescheduled:
3816             self._log_original_error(exc_info, instance_uuid)
3817             compute_utils.add_instance_fault_from_exc(context,
3818                     instance, exc_info[1], exc_info=exc_info)
3819             self._notify_about_instance_usage(context, instance,
3820                     'resize.error', fault=exc_info[1])
3821         else:
3822             # not re-scheduling
3823             six.reraise(*exc_info)
3824 
3825     @wrap_exception()
3826     @reverts_task_state
3827     @wrap_instance_event(prefix='compute')
3828     @errors_out_migration
3829     @wrap_instance_fault
3830     def resize_instance(self, context, instance, image,
3831                         reservations, migration, instance_type,
3832                         clean_shutdown):
3833         """Starts the migration of a running instance to another host."""
3834 
3835         quotas = objects.Quotas.from_reservations(context,
3836                                                   reservations,
3837                                                   instance=instance)
3838         with self._error_out_instance_on_exception(context, instance,
3839                                                    quotas=quotas):
3840             # TODO(chaochin) Remove this until v5 RPC API
3841             # Code downstream may expect extra_specs to be populated since it
3842             # is receiving an object, so lookup the flavor to ensure this.
3843             if (not instance_type or
3844                 not isinstance(instance_type, objects.Flavor)):
3845                 instance_type = objects.Flavor.get_by_id(
3846                     context, migration['new_instance_type_id'])
3847 
3848             network_info = self.network_api.get_instance_nw_info(context,
3849                                                                  instance)
3850 
3851             migration.status = 'migrating'
3852             with migration.obj_as_admin():
3853                 migration.save()
3854 
3855             instance.task_state = task_states.RESIZE_MIGRATING
3856             instance.save(expected_task_state=task_states.RESIZE_PREP)
3857 
3858             self._notify_about_instance_usage(
3859                 context, instance, "resize.start", network_info=network_info)
3860 
3861             compute_utils.notify_about_instance_action(context, instance,
3862                    self.host, action=fields.NotificationAction.RESIZE,
3863                    phase=fields.NotificationPhase.START)
3864 
3865             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3866                     context, instance.uuid)
3867             block_device_info = self._get_instance_block_device_info(
3868                                 context, instance, bdms=bdms)
3869 
3870             timeout, retry_interval = self._get_power_off_values(context,
3871                                             instance, clean_shutdown)
3872             disk_info = self.driver.migrate_disk_and_power_off(
3873                     context, instance, migration.dest_host,
3874                     instance_type, network_info,
3875                     block_device_info,
3876                     timeout, retry_interval)
3877 
3878             self._terminate_volume_connections(context, instance, bdms)
3879 
3880             migration_p = obj_base.obj_to_primitive(migration)
3881             self.network_api.migrate_instance_start(context,
3882                                                     instance,
3883                                                     migration_p)
3884 
3885             migration.status = 'post-migrating'
3886             with migration.obj_as_admin():
3887                 migration.save()
3888 
3889             instance.host = migration.dest_compute
3890             instance.node = migration.dest_node
3891             instance.task_state = task_states.RESIZE_MIGRATED
3892             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
3893 
3894             self.compute_rpcapi.finish_resize(context, instance,
3895                     migration, image, disk_info,
3896                     migration.dest_compute, reservations=quotas.reservations)
3897 
3898             self._notify_about_instance_usage(context, instance, "resize.end",
3899                                               network_info=network_info)
3900 
3901             compute_utils.notify_about_instance_action(context, instance,
3902                    self.host, action=fields.NotificationAction.RESIZE,
3903                    phase=fields.NotificationPhase.END)
3904             self.instance_events.clear_events_for_instance(instance)
3905 
3906     def _terminate_volume_connections(self, context, instance, bdms):
3907         connector = self.driver.get_volume_connector(instance)
3908         for bdm in bdms:
3909             if bdm.is_volume:
3910                 self.volume_api.terminate_connection(context, bdm.volume_id,
3911                                                      connector)
3912 
3913     @staticmethod
3914     def _set_instance_info(instance, instance_type):
3915         instance.instance_type_id = instance_type.id
3916         # NOTE(danms): These are purely for any legacy code that still
3917         # looks at them.
3918         instance.memory_mb = instance_type.memory_mb
3919         instance.vcpus = instance_type.vcpus
3920         instance.root_gb = instance_type.root_gb
3921         instance.ephemeral_gb = instance_type.ephemeral_gb
3922         instance.flavor = instance_type
3923 
3924     def _finish_resize(self, context, instance, migration, disk_info,
3925                        image_meta):
3926         resize_instance = False
3927         old_instance_type_id = migration['old_instance_type_id']
3928         new_instance_type_id = migration['new_instance_type_id']
3929         old_instance_type = instance.get_flavor()
3930         # NOTE(mriedem): Get the old_vm_state so we know if we should
3931         # power on the instance. If old_vm_state is not set we need to default
3932         # to ACTIVE for backwards compatibility
3933         old_vm_state = instance.system_metadata.get('old_vm_state',
3934                                                     vm_states.ACTIVE)
3935         instance.old_flavor = old_instance_type
3936 
3937         if old_instance_type_id != new_instance_type_id:
3938             instance_type = instance.get_flavor('new')
3939             self._set_instance_info(instance, instance_type)
3940             for key in ('root_gb', 'swap', 'ephemeral_gb'):
3941                 if old_instance_type[key] != instance_type[key]:
3942                     resize_instance = True
3943                     break
3944         instance.apply_migration_context()
3945 
3946         # NOTE(tr3buchet): setup networks on destination host
3947         self.network_api.setup_networks_on_host(context, instance,
3948                                                 migration['dest_compute'])
3949 
3950         migration_p = obj_base.obj_to_primitive(migration)
3951         self.network_api.migrate_instance_finish(context,
3952                                                  instance,
3953                                                  migration_p)
3954 
3955         network_info = self.network_api.get_instance_nw_info(context, instance)
3956 
3957         instance.task_state = task_states.RESIZE_FINISH
3958         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
3959 
3960         self._notify_about_instance_usage(
3961             context, instance, "finish_resize.start",
3962             network_info=network_info)
3963         compute_utils.notify_about_instance_action(context, instance,
3964                self.host, action=fields.NotificationAction.RESIZE_FINISH,
3965                phase=fields.NotificationPhase.START)
3966 
3967         block_device_info = self._get_instance_block_device_info(
3968                             context, instance, refresh_conn_info=True)
3969 
3970         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
3971         # automatically power on the instance after it's migrated
3972         power_on = old_vm_state != vm_states.STOPPED
3973 
3974         try:
3975             self.driver.finish_migration(context, migration, instance,
3976                                          disk_info,
3977                                          network_info,
3978                                          image_meta, resize_instance,
3979                                          block_device_info, power_on)
3980         except Exception:
3981             with excutils.save_and_reraise_exception():
3982                 if old_instance_type_id != new_instance_type_id:
3983                     self._set_instance_info(instance,
3984                                             old_instance_type)
3985 
3986         migration.status = 'finished'
3987         with migration.obj_as_admin():
3988             migration.save()
3989 
3990         instance.vm_state = vm_states.RESIZED
3991         instance.task_state = None
3992         instance.launched_at = timeutils.utcnow()
3993         instance.save(expected_task_state=task_states.RESIZE_FINISH)
3994 
3995         self._update_scheduler_instance_info(context, instance)
3996         self._notify_about_instance_usage(
3997             context, instance, "finish_resize.end",
3998             network_info=network_info)
3999         compute_utils.notify_about_instance_action(context, instance,
4000                self.host, action=fields.NotificationAction.RESIZE_FINISH,
4001                phase=fields.NotificationPhase.END)
4002 
4003     @wrap_exception()
4004     @reverts_task_state
4005     @wrap_instance_event(prefix='compute')
4006     @errors_out_migration
4007     @wrap_instance_fault
4008     def finish_resize(self, context, disk_info, image, instance,
4009                       reservations, migration):
4010         """Completes the migration process.
4011 
4012         Sets up the newly transferred disk and turns on the instance at its
4013         new host machine.
4014 
4015         """
4016         quotas = objects.Quotas.from_reservations(context,
4017                                                   reservations,
4018                                                   instance=instance)
4019         try:
4020             image_meta = objects.ImageMeta.from_dict(image)
4021             self._finish_resize(context, instance, migration,
4022                                 disk_info, image_meta)
4023             quotas.commit()
4024         except Exception:
4025             LOG.exception(_LE('Setting instance vm_state to ERROR'),
4026                           instance=instance)
4027             with excutils.save_and_reraise_exception():
4028                 try:
4029                     quotas.rollback()
4030                 except Exception:
4031                     LOG.exception(_LE("Failed to rollback quota for failed "
4032                                       "finish_resize"),
4033                                   instance=instance)
4034                 self._set_instance_obj_error_state(context, instance)
4035 
4036     @wrap_exception()
4037     @wrap_instance_fault
4038     def add_fixed_ip_to_instance(self, context, network_id, instance):
4039         """Calls network_api to add new fixed_ip to instance
4040         then injects the new network info and resets instance networking.
4041 
4042         """
4043         self._notify_about_instance_usage(
4044                 context, instance, "create_ip.start")
4045 
4046         network_info = self.network_api.add_fixed_ip_to_instance(context,
4047                                                                  instance,
4048                                                                  network_id)
4049         self._inject_network_info(context, instance, network_info)
4050         self.reset_network(context, instance)
4051 
4052         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4053         instance.updated_at = timeutils.utcnow()
4054         instance.save()
4055 
4056         self._notify_about_instance_usage(
4057             context, instance, "create_ip.end", network_info=network_info)
4058 
4059     @wrap_exception()
4060     @wrap_instance_fault
4061     def remove_fixed_ip_from_instance(self, context, address, instance):
4062         """Calls network_api to remove existing fixed_ip from instance
4063         by injecting the altered network info and resetting
4064         instance networking.
4065         """
4066         self._notify_about_instance_usage(
4067                 context, instance, "delete_ip.start")
4068 
4069         network_info = self.network_api.remove_fixed_ip_from_instance(context,
4070                                                                       instance,
4071                                                                       address)
4072         self._inject_network_info(context, instance, network_info)
4073         self.reset_network(context, instance)
4074 
4075         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4076         instance.updated_at = timeutils.utcnow()
4077         instance.save()
4078 
4079         self._notify_about_instance_usage(
4080             context, instance, "delete_ip.end", network_info=network_info)
4081 
4082     @wrap_exception()
4083     @reverts_task_state
4084     @wrap_instance_event(prefix='compute')
4085     @wrap_instance_fault
4086     def pause_instance(self, context, instance):
4087         """Pause an instance on this host."""
4088         context = context.elevated()
4089         LOG.info(_LI('Pausing'), instance=instance)
4090         self._notify_about_instance_usage(context, instance, 'pause.start')
4091         compute_utils.notify_about_instance_action(context, instance,
4092                self.host, action=fields.NotificationAction.PAUSE,
4093                phase=fields.NotificationPhase.START)
4094         self.driver.pause(instance)
4095         instance.power_state = self._get_power_state(context, instance)
4096         instance.vm_state = vm_states.PAUSED
4097         instance.task_state = None
4098         instance.save(expected_task_state=task_states.PAUSING)
4099         self._notify_about_instance_usage(context, instance, 'pause.end')
4100         compute_utils.notify_about_instance_action(context, instance,
4101                self.host, action=fields.NotificationAction.PAUSE,
4102                phase=fields.NotificationPhase.END)
4103 
4104     @wrap_exception()
4105     @reverts_task_state
4106     @wrap_instance_event(prefix='compute')
4107     @wrap_instance_fault
4108     def unpause_instance(self, context, instance):
4109         """Unpause a paused instance on this host."""
4110         context = context.elevated()
4111         LOG.info(_LI('Unpausing'), instance=instance)
4112         self._notify_about_instance_usage(context, instance, 'unpause.start')
4113         compute_utils.notify_about_instance_action(context, instance,
4114             self.host, action=fields.NotificationAction.UNPAUSE,
4115             phase=fields.NotificationPhase.START)
4116         self.driver.unpause(instance)
4117         instance.power_state = self._get_power_state(context, instance)
4118         instance.vm_state = vm_states.ACTIVE
4119         instance.task_state = None
4120         instance.save(expected_task_state=task_states.UNPAUSING)
4121         self._notify_about_instance_usage(context, instance, 'unpause.end')
4122         compute_utils.notify_about_instance_action(context, instance,
4123             self.host, action=fields.NotificationAction.UNPAUSE,
4124             phase=fields.NotificationPhase.END)
4125 
4126     @wrap_exception()
4127     def host_power_action(self, context, action):
4128         """Reboots, shuts down or powers up the host."""
4129         return self.driver.host_power_action(action)
4130 
4131     @wrap_exception()
4132     def host_maintenance_mode(self, context, host, mode):
4133         """Start/Stop host maintenance window. On start, it triggers
4134         guest VMs evacuation.
4135         """
4136         return self.driver.host_maintenance_mode(host, mode)
4137 
4138     @wrap_exception()
4139     def set_host_enabled(self, context, enabled):
4140         """Sets the specified host's ability to accept new instances."""
4141         return self.driver.set_host_enabled(enabled)
4142 
4143     @wrap_exception()
4144     def get_host_uptime(self, context):
4145         """Returns the result of calling "uptime" on the target host."""
4146         return self.driver.get_host_uptime()
4147 
4148     @wrap_exception()
4149     @wrap_instance_fault
4150     def get_diagnostics(self, context, instance):
4151         """Retrieve diagnostics for an instance on this host."""
4152         current_power_state = self._get_power_state(context, instance)
4153         if current_power_state == power_state.RUNNING:
4154             LOG.info(_LI("Retrieving diagnostics"), instance=instance)
4155             return self.driver.get_diagnostics(instance)
4156         else:
4157             raise exception.InstanceInvalidState(
4158                 attr='power state',
4159                 instance_uuid=instance.uuid,
4160                 state=power_state.STATE_MAP[instance.power_state],
4161                 method='get_diagnostics')
4162 
4163     # TODO(alaski): Remove object_compat for RPC version 5.0
4164     @object_compat
4165     @wrap_exception()
4166     @wrap_instance_fault
4167     def get_instance_diagnostics(self, context, instance):
4168         """Retrieve diagnostics for an instance on this host."""
4169         current_power_state = self._get_power_state(context, instance)
4170         if current_power_state == power_state.RUNNING:
4171             LOG.info(_LI("Retrieving diagnostics"), instance=instance)
4172             diags = self.driver.get_instance_diagnostics(instance)
4173             return diags.serialize()
4174         else:
4175             raise exception.InstanceInvalidState(
4176                 attr='power state',
4177                 instance_uuid=instance.uuid,
4178                 state=power_state.STATE_MAP[instance.power_state],
4179                 method='get_diagnostics')
4180 
4181     @wrap_exception()
4182     @reverts_task_state
4183     @wrap_instance_event(prefix='compute')
4184     @wrap_instance_fault
4185     def suspend_instance(self, context, instance):
4186         """Suspend the given instance."""
4187         context = context.elevated()
4188 
4189         # Store the old state
4190         instance.system_metadata['old_vm_state'] = instance.vm_state
4191         self._notify_about_instance_usage(context, instance, 'suspend.start')
4192         compute_utils.notify_about_instance_action(context, instance,
4193                 self.host, action=fields.NotificationAction.SUSPEND,
4194                 phase=fields.NotificationPhase.START)
4195         with self._error_out_instance_on_exception(context, instance,
4196              instance_state=instance.vm_state):
4197             self.driver.suspend(context, instance)
4198         instance.power_state = self._get_power_state(context, instance)
4199         instance.vm_state = vm_states.SUSPENDED
4200         instance.task_state = None
4201         instance.save(expected_task_state=task_states.SUSPENDING)
4202         self._notify_about_instance_usage(context, instance, 'suspend.end')
4203         compute_utils.notify_about_instance_action(context, instance,
4204                 self.host, action=fields.NotificationAction.SUSPEND,
4205                 phase=fields.NotificationPhase.END)
4206 
4207     @wrap_exception()
4208     @reverts_task_state
4209     @wrap_instance_event(prefix='compute')
4210     @wrap_instance_fault
4211     def resume_instance(self, context, instance):
4212         """Resume the given suspended instance."""
4213         context = context.elevated()
4214         LOG.info(_LI('Resuming'), instance=instance)
4215 
4216         self._notify_about_instance_usage(context, instance, 'resume.start')
4217         compute_utils.notify_about_instance_action(context, instance,
4218             self.host, action=fields.NotificationAction.RESUME,
4219             phase=fields.NotificationPhase.START)
4220 
4221         network_info = self.network_api.get_instance_nw_info(context, instance)
4222         block_device_info = self._get_instance_block_device_info(
4223                             context, instance)
4224 
4225         with self._error_out_instance_on_exception(context, instance,
4226              instance_state=instance.vm_state):
4227             self.driver.resume(context, instance, network_info,
4228                                block_device_info)
4229 
4230         instance.power_state = self._get_power_state(context, instance)
4231 
4232         # We default to the ACTIVE state for backwards compatibility
4233         instance.vm_state = instance.system_metadata.pop('old_vm_state',
4234                                                          vm_states.ACTIVE)
4235 
4236         instance.task_state = None
4237         instance.save(expected_task_state=task_states.RESUMING)
4238         self._notify_about_instance_usage(context, instance, 'resume.end')
4239         compute_utils.notify_about_instance_action(context, instance,
4240             self.host, action=fields.NotificationAction.RESUME,
4241             phase=fields.NotificationPhase.END)
4242 
4243     @wrap_exception()
4244     @reverts_task_state
4245     @wrap_instance_event(prefix='compute')
4246     @wrap_instance_fault
4247     def shelve_instance(self, context, instance, image_id,
4248                         clean_shutdown):
4249         """Shelve an instance.
4250 
4251         This should be used when you want to take a snapshot of the instance.
4252         It also adds system_metadata that can be used by a periodic task to
4253         offload the shelved instance after a period of time.
4254 
4255         :param context: request context
4256         :param instance: an Instance object
4257         :param image_id: an image id to snapshot to.
4258         :param clean_shutdown: give the GuestOS a chance to stop
4259         """
4260 
4261         @utils.synchronized(instance.uuid)
4262         def do_shelve_instance():
4263             self._shelve_instance(context, instance, image_id, clean_shutdown)
4264         do_shelve_instance()
4265 
4266     def _shelve_instance(self, context, instance, image_id,
4267                          clean_shutdown):
4268         LOG.info(_LI('Shelving'), instance=instance)
4269         compute_utils.notify_usage_exists(self.notifier, context, instance,
4270                                           current_period=True)
4271         self._notify_about_instance_usage(context, instance, 'shelve.start')
4272         compute_utils.notify_about_instance_action(context, instance,
4273                 self.host, action=fields.NotificationAction.SHELVE,
4274                 phase=fields.NotificationPhase.START)
4275 
4276         def update_task_state(task_state, expected_state=task_states.SHELVING):
4277             shelving_state_map = {
4278                     task_states.IMAGE_PENDING_UPLOAD:
4279                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
4280                     task_states.IMAGE_UPLOADING:
4281                         task_states.SHELVING_IMAGE_UPLOADING,
4282                     task_states.SHELVING: task_states.SHELVING}
4283             task_state = shelving_state_map[task_state]
4284             expected_state = shelving_state_map[expected_state]
4285             instance.task_state = task_state
4286             instance.save(expected_task_state=expected_state)
4287 
4288         self._power_off_instance(context, instance, clean_shutdown)
4289         self.driver.snapshot(context, instance, image_id, update_task_state)
4290 
4291         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
4292         instance.system_metadata['shelved_image_id'] = image_id
4293         instance.system_metadata['shelved_host'] = self.host
4294         instance.vm_state = vm_states.SHELVED
4295         instance.task_state = None
4296         if CONF.shelved_offload_time == 0:
4297             instance.task_state = task_states.SHELVING_OFFLOADING
4298         instance.power_state = self._get_power_state(context, instance)
4299         instance.save(expected_task_state=[
4300                 task_states.SHELVING,
4301                 task_states.SHELVING_IMAGE_UPLOADING])
4302 
4303         self._notify_about_instance_usage(context, instance, 'shelve.end')
4304         compute_utils.notify_about_instance_action(context, instance,
4305                 self.host, action=fields.NotificationAction.SHELVE,
4306                 phase=fields.NotificationPhase.END)
4307 
4308         if CONF.shelved_offload_time == 0:
4309             self._shelve_offload_instance(context, instance,
4310                                           clean_shutdown=False)
4311 
4312     @wrap_exception()
4313     @reverts_task_state
4314     @wrap_instance_fault
4315     def shelve_offload_instance(self, context, instance, clean_shutdown):
4316         """Remove a shelved instance from the hypervisor.
4317 
4318         This frees up those resources for use by other instances, but may lead
4319         to slower unshelve times for this instance.  This method is used by
4320         volume backed instances since restoring them doesn't involve the
4321         potentially large download of an image.
4322 
4323         :param context: request context
4324         :param instance: nova.objects.instance.Instance
4325         :param clean_shutdown: give the GuestOS a chance to stop
4326         """
4327 
4328         @utils.synchronized(instance.uuid)
4329         def do_shelve_offload_instance():
4330             self._shelve_offload_instance(context, instance, clean_shutdown)
4331         do_shelve_offload_instance()
4332 
4333     def _shelve_offload_instance(self, context, instance, clean_shutdown):
4334         LOG.info(_LI('Shelve offloading'), instance=instance)
4335         self._notify_about_instance_usage(context, instance,
4336                 'shelve_offload.start')
4337         compute_utils.notify_about_instance_action(context, instance,
4338                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
4339                 phase=fields.NotificationPhase.START)
4340 
4341         self._power_off_instance(context, instance, clean_shutdown)
4342         current_power_state = self._get_power_state(context, instance)
4343 
4344         self.network_api.cleanup_instance_network_on_host(context, instance,
4345                                                           instance.host)
4346         network_info = self.network_api.get_instance_nw_info(context, instance)
4347         block_device_info = self._get_instance_block_device_info(context,
4348                                                                  instance)
4349         self.driver.destroy(context, instance, network_info,
4350                 block_device_info)
4351 
4352         instance.power_state = current_power_state
4353         # NOTE(mriedem): The vm_state has to be set before updating the
4354         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
4355         # values cannot be nulled out until after updating the resource tracker
4356         # though.
4357         instance.vm_state = vm_states.SHELVED_OFFLOADED
4358         instance.task_state = None
4359         instance.save(expected_task_state=[task_states.SHELVING,
4360                                            task_states.SHELVING_OFFLOADING])
4361 
4362         # NOTE(ndipanov): Free resources from the resource tracker
4363         self._update_resource_tracker(context, instance)
4364 
4365         # NOTE(sfinucan): RPC calls should no longer be attempted against this
4366         # instance, so ensure any calls result in errors
4367         self._nil_out_instance_obj_host_and_node(instance)
4368         instance.save(expected_task_state=None)
4369 
4370         self._delete_scheduler_instance_info(context, instance.uuid)
4371         self._notify_about_instance_usage(context, instance,
4372                 'shelve_offload.end')
4373         compute_utils.notify_about_instance_action(context, instance,
4374                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
4375                 phase=fields.NotificationPhase.END)
4376 
4377     @wrap_exception()
4378     @reverts_task_state
4379     @wrap_instance_event(prefix='compute')
4380     @wrap_instance_fault
4381     def unshelve_instance(self, context, instance, image,
4382                           filter_properties, node):
4383         """Unshelve the instance.
4384 
4385         :param context: request context
4386         :param instance: a nova.objects.instance.Instance object
4387         :param image: an image to build from.  If None we assume a
4388             volume backed instance.
4389         :param filter_properties: dict containing limits, retry info etc.
4390         :param node: target compute node
4391         """
4392         if filter_properties is None:
4393             filter_properties = {}
4394 
4395         @utils.synchronized(instance.uuid)
4396         def do_unshelve_instance():
4397             self._unshelve_instance(context, instance, image,
4398                                     filter_properties, node)
4399         do_unshelve_instance()
4400 
4401     def _unshelve_instance_key_scrub(self, instance):
4402         """Remove data from the instance that may cause side effects."""
4403         cleaned_keys = dict(
4404                 key_data=instance.key_data,
4405                 auto_disk_config=instance.auto_disk_config)
4406         instance.key_data = None
4407         instance.auto_disk_config = False
4408         return cleaned_keys
4409 
4410     def _unshelve_instance_key_restore(self, instance, keys):
4411         """Restore previously scrubbed keys before saving the instance."""
4412         instance.update(keys)
4413 
4414     def _unshelve_instance(self, context, instance, image, filter_properties,
4415                            node):
4416         LOG.info(_LI('Unshelving'), instance=instance)
4417         self._notify_about_instance_usage(context, instance, 'unshelve.start')
4418         compute_utils.notify_about_instance_action(context, instance,
4419                 self.host, action=fields.NotificationAction.UNSHELVE,
4420                 phase=fields.NotificationPhase.START)
4421 
4422         instance.task_state = task_states.SPAWNING
4423         instance.save()
4424 
4425         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4426                 context, instance.uuid)
4427         block_device_info = self._prep_block_device(context, instance, bdms,
4428                                                     do_check_attach=False)
4429         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
4430 
4431         if node is None:
4432             node = self.driver.get_available_nodes()[0]
4433             LOG.debug('No node specified, defaulting to %s', node,
4434                       instance=instance)
4435 
4436         rt = self._get_resource_tracker()
4437         limits = filter_properties.get('limits', {})
4438 
4439         shelved_image_ref = instance.image_ref
4440         if image:
4441             instance.image_ref = image['id']
4442             image_meta = objects.ImageMeta.from_dict(image)
4443         else:
4444             image_meta = objects.ImageMeta.from_dict(
4445                 utils.get_image_from_system_metadata(
4446                     instance.system_metadata))
4447 
4448         self.network_api.setup_instance_network_on_host(context, instance,
4449                                                         self.host)
4450         network_info = self.network_api.get_instance_nw_info(context, instance)
4451         try:
4452             with rt.instance_claim(context, instance, node, limits):
4453                 self.driver.spawn(context, instance, image_meta,
4454                                   injected_files=[],
4455                                   admin_password=None,
4456                                   network_info=network_info,
4457                                   block_device_info=block_device_info)
4458         except Exception:
4459             with excutils.save_and_reraise_exception():
4460                 LOG.exception(_LE('Instance failed to spawn'),
4461                               instance=instance)
4462 
4463         if image:
4464             instance.image_ref = shelved_image_ref
4465             self._delete_snapshot_of_shelved_instance(context, instance,
4466                                                       image['id'])
4467 
4468         self._unshelve_instance_key_restore(instance, scrubbed_keys)
4469         self._update_instance_after_spawn(context, instance)
4470         # Delete system_metadata for a shelved instance
4471         compute_utils.remove_shelved_keys_from_system_metadata(instance)
4472 
4473         instance.save(expected_task_state=task_states.SPAWNING)
4474         self._update_scheduler_instance_info(context, instance)
4475         self._notify_about_instance_usage(context, instance, 'unshelve.end')
4476         compute_utils.notify_about_instance_action(context, instance,
4477                 self.host, action=fields.NotificationAction.UNSHELVE,
4478                 phase=fields.NotificationPhase.END)
4479 
4480     @messaging.expected_exceptions(NotImplementedError)
4481     @wrap_instance_fault
4482     def reset_network(self, context, instance):
4483         """Reset networking on the given instance."""
4484         LOG.debug('Reset network', instance=instance)
4485         self.driver.reset_network(instance)
4486 
4487     def _inject_network_info(self, context, instance, network_info):
4488         """Inject network info for the given instance."""
4489         LOG.debug('Inject network info', instance=instance)
4490         LOG.debug('network_info to inject: |%s|', network_info,
4491                   instance=instance)
4492 
4493         self.driver.inject_network_info(instance,
4494                                         network_info)
4495 
4496     @wrap_instance_fault
4497     def inject_network_info(self, context, instance):
4498         """Inject network info, but don't return the info."""
4499         network_info = self.network_api.get_instance_nw_info(context, instance)
4500         self._inject_network_info(context, instance, network_info)
4501 
4502     @messaging.expected_exceptions(NotImplementedError,
4503                                    exception.ConsoleNotAvailable,
4504                                    exception.InstanceNotFound)
4505     @wrap_exception()
4506     @wrap_instance_fault
4507     def get_console_output(self, context, instance, tail_length):
4508         """Send the console output for the given instance."""
4509         context = context.elevated()
4510         LOG.info(_LI("Get console output"), instance=instance)
4511         output = self.driver.get_console_output(context, instance)
4512 
4513         if type(output) is six.text_type:
4514             output = six.b(output)
4515 
4516         if tail_length is not None:
4517             output = self._tail_log(output, tail_length)
4518 
4519         return output.decode('ascii', 'replace')
4520 
4521     def _tail_log(self, log, length):
4522         try:
4523             length = int(length)
4524         except ValueError:
4525             length = 0
4526 
4527         if length == 0:
4528             return b''
4529         else:
4530             return b'\n'.join(log.split(b'\n')[-int(length):])
4531 
4532     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4533                                    exception.InstanceNotReady,
4534                                    exception.InstanceNotFound,
4535                                    exception.ConsoleTypeUnavailable,
4536                                    NotImplementedError)
4537     @wrap_exception()
4538     @wrap_instance_fault
4539     def get_vnc_console(self, context, console_type, instance):
4540         """Return connection information for a vnc console."""
4541         context = context.elevated()
4542         LOG.debug("Getting vnc console", instance=instance)
4543         token = uuidutils.generate_uuid()
4544 
4545         if not CONF.vnc.enabled:
4546             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4547 
4548         if console_type == 'novnc':
4549             # For essex, novncproxy_base_url must include the full path
4550             # including the html file (like http://myhost/vnc_auto.html)
4551             access_url = '%s?token=%s' % (CONF.vnc.novncproxy_base_url, token)
4552         elif console_type == 'xvpvnc':
4553             access_url = '%s?token=%s' % (CONF.vnc.xvpvncproxy_base_url, token)
4554         else:
4555             raise exception.ConsoleTypeInvalid(console_type=console_type)
4556 
4557         try:
4558             # Retrieve connect info from driver, and then decorate with our
4559             # access info token
4560             console = self.driver.get_vnc_console(context, instance)
4561             connect_info = console.get_connection_info(token, access_url)
4562         except exception.InstanceNotFound:
4563             if instance.vm_state != vm_states.BUILDING:
4564                 raise
4565             raise exception.InstanceNotReady(instance_id=instance.uuid)
4566 
4567         return connect_info
4568 
4569     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4570                                    exception.InstanceNotReady,
4571                                    exception.InstanceNotFound,
4572                                    exception.ConsoleTypeUnavailable,
4573                                    NotImplementedError)
4574     @wrap_exception()
4575     @wrap_instance_fault
4576     def get_spice_console(self, context, console_type, instance):
4577         """Return connection information for a spice console."""
4578         context = context.elevated()
4579         LOG.debug("Getting spice console", instance=instance)
4580         token = uuidutils.generate_uuid()
4581 
4582         if not CONF.spice.enabled:
4583             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4584 
4585         if console_type == 'spice-html5':
4586             # For essex, spicehtml5proxy_base_url must include the full path
4587             # including the html file (like http://myhost/spice_auto.html)
4588             access_url = '%s?token=%s' % (CONF.spice.html5proxy_base_url,
4589                                           token)
4590         else:
4591             raise exception.ConsoleTypeInvalid(console_type=console_type)
4592 
4593         try:
4594             # Retrieve connect info from driver, and then decorate with our
4595             # access info token
4596             console = self.driver.get_spice_console(context, instance)
4597             connect_info = console.get_connection_info(token, access_url)
4598         except exception.InstanceNotFound:
4599             if instance.vm_state != vm_states.BUILDING:
4600                 raise
4601             raise exception.InstanceNotReady(instance_id=instance.uuid)
4602 
4603         return connect_info
4604 
4605     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4606                                    exception.InstanceNotReady,
4607                                    exception.InstanceNotFound,
4608                                    exception.ConsoleTypeUnavailable,
4609                                    NotImplementedError)
4610     @wrap_exception()
4611     @wrap_instance_fault
4612     def get_rdp_console(self, context, console_type, instance):
4613         """Return connection information for a RDP console."""
4614         context = context.elevated()
4615         LOG.debug("Getting RDP console", instance=instance)
4616         token = uuidutils.generate_uuid()
4617 
4618         if not CONF.rdp.enabled:
4619             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4620 
4621         if console_type == 'rdp-html5':
4622             access_url = '%s?token=%s' % (CONF.rdp.html5_proxy_base_url,
4623                                           token)
4624         else:
4625             raise exception.ConsoleTypeInvalid(console_type=console_type)
4626 
4627         try:
4628             # Retrieve connect info from driver, and then decorate with our
4629             # access info token
4630             console = self.driver.get_rdp_console(context, instance)
4631             connect_info = console.get_connection_info(token, access_url)
4632         except exception.InstanceNotFound:
4633             if instance.vm_state != vm_states.BUILDING:
4634                 raise
4635             raise exception.InstanceNotReady(instance_id=instance.uuid)
4636 
4637         return connect_info
4638 
4639     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4640                                    exception.InstanceNotReady,
4641                                    exception.InstanceNotFound,
4642                                    exception.ConsoleTypeUnavailable,
4643                                    NotImplementedError)
4644     @wrap_exception()
4645     @wrap_instance_fault
4646     def get_mks_console(self, context, console_type, instance):
4647         """Return connection information for a MKS console."""
4648         context = context.elevated()
4649         LOG.debug("Getting MKS console", instance=instance)
4650         token = uuidutils.generate_uuid()
4651 
4652         if not CONF.mks.enabled:
4653             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4654 
4655         if console_type == 'webmks':
4656             access_url = '%s?token=%s' % (CONF.mks.mksproxy_base_url,
4657                                           token)
4658         else:
4659             raise exception.ConsoleTypeInvalid(console_type=console_type)
4660 
4661         try:
4662             # Retrieve connect info from driver, and then decorate with our
4663             # access info token
4664             console = self.driver.get_mks_console(context, instance)
4665             connect_info = console.get_connection_info(token, access_url)
4666         except exception.InstanceNotFound:
4667             if instance.vm_state != vm_states.BUILDING:
4668                 raise
4669             raise exception.InstanceNotReady(instance_id=instance.uuid)
4670 
4671         return connect_info
4672 
4673     @messaging.expected_exceptions(
4674         exception.ConsoleTypeInvalid,
4675         exception.InstanceNotReady,
4676         exception.InstanceNotFound,
4677         exception.ConsoleTypeUnavailable,
4678         exception.SocketPortRangeExhaustedException,
4679         exception.ImageSerialPortNumberInvalid,
4680         exception.ImageSerialPortNumberExceedFlavorValue,
4681         NotImplementedError)
4682     @wrap_exception()
4683     @wrap_instance_fault
4684     def get_serial_console(self, context, console_type, instance):
4685         """Returns connection information for a serial console."""
4686 
4687         LOG.debug("Getting serial console", instance=instance)
4688 
4689         if not CONF.serial_console.enabled:
4690             raise exception.ConsoleTypeUnavailable(console_type=console_type)
4691 
4692         context = context.elevated()
4693 
4694         token = uuidutils.generate_uuid()
4695         access_url = '%s?token=%s' % (CONF.serial_console.base_url, token)
4696 
4697         try:
4698             # Retrieve connect info from driver, and then decorate with our
4699             # access info token
4700             console = self.driver.get_serial_console(context, instance)
4701             connect_info = console.get_connection_info(token, access_url)
4702         except exception.InstanceNotFound:
4703             if instance.vm_state != vm_states.BUILDING:
4704                 raise
4705             raise exception.InstanceNotReady(instance_id=instance.uuid)
4706 
4707         return connect_info
4708 
4709     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
4710                                    exception.InstanceNotReady,
4711                                    exception.InstanceNotFound)
4712     @wrap_exception()
4713     @wrap_instance_fault
4714     def validate_console_port(self, ctxt, instance, port, console_type):
4715         if console_type == "spice-html5":
4716             console_info = self.driver.get_spice_console(ctxt, instance)
4717         elif console_type == "rdp-html5":
4718             console_info = self.driver.get_rdp_console(ctxt, instance)
4719         elif console_type == "serial":
4720             console_info = self.driver.get_serial_console(ctxt, instance)
4721         elif console_type == "webmks":
4722             console_info = self.driver.get_mks_console(ctxt, instance)
4723         else:
4724             console_info = self.driver.get_vnc_console(ctxt, instance)
4725 
4726         return console_info.port == port
4727 
4728     @wrap_exception()
4729     @reverts_task_state
4730     @wrap_instance_fault
4731     def reserve_block_device_name(self, context, instance, device,
4732                                   volume_id, disk_bus, device_type):
4733         @utils.synchronized(instance.uuid)
4734         def do_reserve():
4735             bdms = (
4736                 objects.BlockDeviceMappingList.get_by_instance_uuid(
4737                     context, instance.uuid))
4738 
4739             # NOTE(ndipanov): We need to explicitly set all the fields on the
4740             #                 object so that obj_load_attr does not fail
4741             new_bdm = objects.BlockDeviceMapping(
4742                     context=context,
4743                     source_type='volume', destination_type='volume',
4744                     instance_uuid=instance.uuid, boot_index=None,
4745                     volume_id=volume_id,
4746                     device_name=device, guest_format=None,
4747                     disk_bus=disk_bus, device_type=device_type)
4748 
4749             new_bdm.device_name = self._get_device_name_for_instance(
4750                     instance, bdms, new_bdm)
4751 
4752             # NOTE(vish): create bdm here to avoid race condition
4753             new_bdm.create()
4754             return new_bdm
4755 
4756         return do_reserve()
4757 
4758     @wrap_exception()
4759     @wrap_instance_fault
4760     def attach_volume(self, context, instance, bdm):
4761         """Attach a volume to an instance."""
4762         driver_bdm = driver_block_device.convert_volume(bdm)
4763 
4764         @utils.synchronized(instance.uuid)
4765         def do_attach_volume(context, instance, driver_bdm):
4766             try:
4767                 return self._attach_volume(context, instance, driver_bdm)
4768             except Exception:
4769                 with excutils.save_and_reraise_exception():
4770                     bdm.destroy()
4771 
4772         do_attach_volume(context, instance, driver_bdm)
4773 
4774     def _attach_volume(self, context, instance, bdm):
4775         context = context.elevated()
4776         LOG.info(_LI('Attaching volume %(volume_id)s to %(mountpoint)s'),
4777                   {'volume_id': bdm.volume_id,
4778                   'mountpoint': bdm['mount_device']},
4779                  instance=instance)
4780         try:
4781             bdm.attach(context, instance, self.volume_api, self.driver,
4782                        do_check_attach=False, do_driver_attach=True)
4783         except Exception:
4784             with excutils.save_and_reraise_exception():
4785                 LOG.exception(_LE("Failed to attach %(volume_id)s "
4786                                   "at %(mountpoint)s"),
4787                               {'volume_id': bdm.volume_id,
4788                                'mountpoint': bdm['mount_device']},
4789                               instance=instance)
4790                 self.volume_api.unreserve_volume(context, bdm.volume_id)
4791 
4792         info = {'volume_id': bdm.volume_id}
4793         self._notify_about_instance_usage(
4794             context, instance, "volume.attach", extra_usage_info=info)
4795 
4796     def _driver_detach_volume(self, context, instance, bdm, connection_info):
4797         """Do the actual driver detach using block device mapping."""
4798         mp = bdm.device_name
4799         volume_id = bdm.volume_id
4800 
4801         LOG.info(_LI('Detach volume %(volume_id)s from mountpoint %(mp)s'),
4802                   {'volume_id': volume_id, 'mp': mp},
4803                   instance=instance)
4804 
4805         try:
4806             if not self.driver.instance_exists(instance):
4807                 LOG.warning(_LW('Detaching volume from unknown instance'),
4808                             instance=instance)
4809 
4810             encryption = encryptors.get_encryption_metadata(
4811                 context, self.volume_api, volume_id, connection_info)
4812 
4813             self.driver.detach_volume(connection_info,
4814                                       instance,
4815                                       mp,
4816                                       encryption=encryption)
4817         except exception.DiskNotFound as err:
4818             LOG.warning(_LW('Ignoring DiskNotFound exception while detaching '
4819                             'volume %(volume_id)s from %(mp)s: %(err)s'),
4820                         {'volume_id': volume_id, 'mp': mp, 'err': err},
4821                         instance=instance)
4822         except Exception:
4823             with excutils.save_and_reraise_exception():
4824                 LOG.exception(_LE('Failed to detach volume %(volume_id)s '
4825                                   'from %(mp)s'),
4826                               {'volume_id': volume_id, 'mp': mp},
4827                               instance=instance)
4828                 self.volume_api.roll_detaching(context, volume_id)
4829 
4830     def _detach_volume(self, context, volume_id, instance, destroy_bdm=True,
4831                        attachment_id=None):
4832         """Detach a volume from an instance.
4833 
4834         :param context: security context
4835         :param volume_id: the volume id
4836         :param instance: the Instance object to detach the volume from
4837         :param destroy_bdm: if True, the corresponding BDM entry will be marked
4838                             as deleted. Disabling this is useful for operations
4839                             like rebuild, when we don't want to destroy BDM
4840 
4841         """
4842 
4843         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4844                 context, volume_id, instance.uuid)
4845         if CONF.volume_usage_poll_interval > 0:
4846             vol_stats = []
4847             mp = bdm.device_name
4848             # Handle bootable volumes which will not contain /dev/
4849             if '/dev/' in mp:
4850                 mp = mp[5:]
4851             try:
4852                 vol_stats = self.driver.block_stats(instance, mp)
4853             except NotImplementedError:
4854                 pass
4855 
4856             if vol_stats:
4857                 LOG.debug("Updating volume usage cache with totals",
4858                           instance=instance)
4859                 rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
4860                 vol_usage = objects.VolumeUsage(context)
4861                 vol_usage.volume_id = volume_id
4862                 vol_usage.instance_uuid = instance.uuid
4863                 vol_usage.project_id = instance.project_id
4864                 vol_usage.user_id = instance.user_id
4865                 vol_usage.availability_zone = instance.availability_zone
4866                 vol_usage.curr_reads = rd_req
4867                 vol_usage.curr_read_bytes = rd_bytes
4868                 vol_usage.curr_writes = wr_req
4869                 vol_usage.curr_write_bytes = wr_bytes
4870                 vol_usage.save(update_totals=True)
4871                 self.notifier.info(context, 'volume.usage',
4872                                    compute_utils.usage_volume_info(vol_usage))
4873 
4874         connection_info = jsonutils.loads(bdm.connection_info)
4875         connector = self.driver.get_volume_connector(instance)
4876         if CONF.host == instance.host:
4877             # Only attempt to detach and disconnect from the volume if the
4878             # instance is currently associated with the local compute host.
4879             self._driver_detach_volume(context, instance, bdm, connection_info)
4880         elif not destroy_bdm:
4881             LOG.debug("Skipping _driver_detach_volume during remote rebuild.",
4882                       instance=instance)
4883         elif destroy_bdm:
4884             LOG.error(_LE("Unable to call for a driver detach of volume "
4885                           "%(vol_id)s due to the instance being registered to "
4886                           "the remote host %(inst_host)s."),
4887                       {'vol_id': volume_id, 'inst_host': instance.host},
4888                       instance=instance)
4889 
4890         if connection_info and not destroy_bdm and (
4891            connector.get('host') != instance.host):
4892             # If the volume is attached to another host (evacuate) then
4893             # this connector is for the wrong host. Use the connector that
4894             # was stored in connection_info instead (if we have one, and it
4895             # is for the expected host).
4896             stashed_connector = connection_info.get('connector')
4897             if not stashed_connector:
4898                 # Volume was attached before we began stashing connectors
4899                 LOG.warning(_LW("Host mismatch detected, but stashed "
4900                                 "volume connector not found. Instance host is "
4901                                 "%(ihost)s, but volume connector host is "
4902                                 "%(chost)s."),
4903                             {'ihost': instance.host,
4904                              'chost': connector.get('host')})
4905             elif stashed_connector.get('host') != instance.host:
4906                 # Unexpected error. The stashed connector is also not matching
4907                 # the needed instance host.
4908                 LOG.error(_LE("Host mismatch detected in stashed volume "
4909                               "connector. Will use local volume connector. "
4910                               "Instance host is %(ihost)s. Local volume "
4911                               "connector host is %(chost)s. Stashed volume "
4912                               "connector host is %(schost)s."),
4913                           {'ihost': instance.host,
4914                            'chost': connector.get('host'),
4915                            'schost': stashed_connector.get('host')})
4916             else:
4917                 # Fix found. Use stashed connector.
4918                 LOG.debug("Host mismatch detected. Found usable stashed "
4919                           "volume connector. Instance host is %(ihost)s. "
4920                           "Local volume connector host was %(chost)s. "
4921                           "Stashed volume connector host is %(schost)s.",
4922                           {'ihost': instance.host,
4923                            'chost': connector.get('host'),
4924                            'schost': stashed_connector.get('host')})
4925                 connector = stashed_connector
4926 
4927         self.volume_api.terminate_connection(context, volume_id, connector)
4928 
4929         if destroy_bdm:
4930             bdm.destroy()
4931 
4932         info = dict(volume_id=volume_id)
4933         self._notify_about_instance_usage(
4934             context, instance, "volume.detach", extra_usage_info=info)
4935         self.volume_api.detach(context.elevated(), volume_id, instance.uuid,
4936                                attachment_id)
4937 
4938     @wrap_exception()
4939     @wrap_instance_fault
4940     def detach_volume(self, context, volume_id, instance, attachment_id=None):
4941         """Detach a volume from an instance."""
4942 
4943         self._detach_volume(context, volume_id, instance,
4944                             attachment_id=attachment_id)
4945 
4946     def _init_volume_connection(self, context, new_volume_id,
4947                                 old_volume_id, connector, instance, bdm):
4948 
4949         new_cinfo = self.volume_api.initialize_connection(context,
4950                                                           new_volume_id,
4951                                                           connector)
4952         old_cinfo = jsonutils.loads(bdm['connection_info'])
4953         if old_cinfo and 'serial' not in old_cinfo:
4954             old_cinfo['serial'] = old_volume_id
4955         new_cinfo['serial'] = old_cinfo['serial']
4956         return (old_cinfo, new_cinfo)
4957 
4958     def _swap_volume(self, context, instance, bdm, connector,
4959                      old_volume_id, new_volume_id, resize_to):
4960         mountpoint = bdm['device_name']
4961         failed = False
4962         new_cinfo = None
4963         try:
4964             old_cinfo, new_cinfo = self._init_volume_connection(context,
4965                                                                 new_volume_id,
4966                                                                 old_volume_id,
4967                                                                 connector,
4968                                                                 instance,
4969                                                                 bdm)
4970             LOG.debug("swap_volume: Calling driver volume swap with "
4971                       "connection infos: new: %(new_cinfo)s; "
4972                       "old: %(old_cinfo)s",
4973                       {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo},
4974                       instance=instance)
4975             self.driver.swap_volume(old_cinfo, new_cinfo, instance, mountpoint,
4976                                     resize_to)
4977         except Exception as ex:
4978             failed = True
4979             with excutils.save_and_reraise_exception():
4980                 compute_utils.notify_about_volume_swap(
4981                     context, instance, self.host,
4982                     fields.NotificationAction.VOLUME_SWAP,
4983                     fields.NotificationPhase.ERROR,
4984                     old_volume_id, new_volume_id, ex)
4985                 if new_cinfo:
4986                     msg = _LE("Failed to swap volume %(old_volume_id)s "
4987                               "for %(new_volume_id)s")
4988                     LOG.exception(msg, {'old_volume_id': old_volume_id,
4989                                         'new_volume_id': new_volume_id},
4990                                   instance=instance)
4991                 else:
4992                     msg = _LE("Failed to connect to volume %(volume_id)s "
4993                               "with volume at %(mountpoint)s")
4994                     LOG.exception(msg, {'volume_id': new_volume_id,
4995                                         'mountpoint': bdm['device_name']},
4996                                   instance=instance)
4997                 self.volume_api.roll_detaching(context, old_volume_id)
4998                 self.volume_api.unreserve_volume(context, new_volume_id)
4999         finally:
5000             conn_volume = new_volume_id if failed else old_volume_id
5001             if new_cinfo:
5002                 LOG.debug("swap_volume: calling Cinder terminate_connection "
5003                           "for %(volume)s", {'volume': conn_volume},
5004                           instance=instance)
5005                 self.volume_api.terminate_connection(context,
5006                                                      conn_volume,
5007                                                      connector)
5008             # If Cinder initiated the swap, it will keep
5009             # the original ID
5010             comp_ret = self.volume_api.migrate_volume_completion(
5011                                                       context,
5012                                                       old_volume_id,
5013                                                       new_volume_id,
5014                                                       error=failed)
5015             LOG.debug("swap_volume: Cinder migrate_volume_completion "
5016                       "returned: %(comp_ret)s", {'comp_ret': comp_ret},
5017                       instance=instance)
5018 
5019         return (comp_ret, new_cinfo)
5020 
5021     @wrap_exception()
5022     @reverts_task_state
5023     @wrap_instance_fault
5024     def swap_volume(self, context, old_volume_id, new_volume_id, instance):
5025         """Swap volume for an instance."""
5026         context = context.elevated()
5027 
5028         compute_utils.notify_about_volume_swap(
5029             context, instance, self.host,
5030             fields.NotificationAction.VOLUME_SWAP,
5031             fields.NotificationPhase.START,
5032             old_volume_id, new_volume_id)
5033 
5034         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5035                 context, old_volume_id, instance.uuid)
5036         connector = self.driver.get_volume_connector(instance)
5037 
5038         resize_to = 0
5039         old_vol_size = self.volume_api.get(context, old_volume_id)['size']
5040         new_vol_size = self.volume_api.get(context, new_volume_id)['size']
5041         if new_vol_size > old_vol_size:
5042             resize_to = new_vol_size
5043 
5044         LOG.info(_LI('Swapping volume %(old_volume)s for %(new_volume)s'),
5045                   {'old_volume': old_volume_id, 'new_volume': new_volume_id},
5046                  instance=instance)
5047         comp_ret, new_cinfo = self._swap_volume(context, instance,
5048                                                          bdm,
5049                                                          connector,
5050                                                          old_volume_id,
5051                                                          new_volume_id,
5052                                                          resize_to)
5053 
5054         save_volume_id = comp_ret['save_volume_id']
5055 
5056         # Update bdm
5057         values = {
5058             'connection_info': jsonutils.dumps(new_cinfo),
5059             'source_type': 'volume',
5060             'destination_type': 'volume',
5061             'snapshot_id': None,
5062             'volume_id': save_volume_id,
5063             'no_device': None}
5064 
5065         if resize_to:
5066             values['volume_size'] = resize_to
5067 
5068         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
5069                   "%(updates)s", {'volume_id': bdm.volume_id,
5070                                   'updates': values},
5071                   instance=instance)
5072         bdm.update(values)
5073         bdm.save()
5074 
5075         compute_utils.notify_about_volume_swap(
5076             context, instance, self.host,
5077             fields.NotificationAction.VOLUME_SWAP,
5078             fields.NotificationPhase.END,
5079             old_volume_id, new_volume_id)
5080 
5081     @wrap_exception()
5082     def remove_volume_connection(self, context, volume_id, instance):
5083         """Remove a volume connection using the volume api."""
5084         # NOTE(vish): We don't want to actually mark the volume
5085         #             detached, or delete the bdm, just remove the
5086         #             connection from this host.
5087 
5088         try:
5089             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5090                     context, volume_id, instance.uuid)
5091             connection_info = jsonutils.loads(bdm.connection_info)
5092             self._driver_detach_volume(context, instance, bdm, connection_info)
5093             connector = self.driver.get_volume_connector(instance)
5094             self.volume_api.terminate_connection(context, volume_id, connector)
5095         except exception.NotFound:
5096             pass
5097 
5098     @wrap_exception()
5099     @wrap_instance_fault
5100     def attach_interface(self, context, instance, network_id, port_id,
5101                          requested_ip):
5102         """Use hotplug to add an network adapter to an instance."""
5103         if not self.driver.capabilities['supports_attach_interface']:
5104             raise exception.AttachInterfaceNotSupported(
5105                 instance_id=instance.uuid)
5106         bind_host_id = self.driver.network_binding_host_id(context, instance)
5107         network_info = self.network_api.allocate_port_for_instance(
5108             context, instance, port_id, network_id, requested_ip,
5109             bind_host_id=bind_host_id)
5110         if len(network_info) != 1:
5111             LOG.error(_LE('allocate_port_for_instance returned %(ports)s '
5112                           'ports'), {'ports': len(network_info)})
5113             raise exception.InterfaceAttachFailed(
5114                     instance_uuid=instance.uuid)
5115         image_meta = objects.ImageMeta.from_instance(instance)
5116 
5117         try:
5118             self.driver.attach_interface(context, instance, image_meta,
5119                                          network_info[0])
5120         except exception.NovaException as ex:
5121             port_id = network_info[0].get('id')
5122             LOG.warning(_LW("attach interface failed , try to deallocate "
5123                          "port %(port_id)s, reason: %(msg)s"),
5124                      {'port_id': port_id, 'msg': ex},
5125                      instance=instance)
5126             try:
5127                 self.network_api.deallocate_port_for_instance(
5128                     context, instance, port_id)
5129             except Exception:
5130                 LOG.warning(_LW("deallocate port %(port_id)s failed"),
5131                              {'port_id': port_id}, instance=instance)
5132             raise exception.InterfaceAttachFailed(
5133                 instance_uuid=instance.uuid)
5134 
5135         return network_info[0]
5136 
5137     @wrap_exception()
5138     @wrap_instance_fault
5139     def detach_interface(self, context, instance, port_id):
5140         """Detach a network adapter from an instance."""
5141         network_info = instance.info_cache.network_info
5142         condemned = None
5143         for vif in network_info:
5144             if vif['id'] == port_id:
5145                 condemned = vif
5146                 break
5147         if condemned is None:
5148             raise exception.PortNotFound(_("Port %s is not "
5149                                            "attached") % port_id)
5150         try:
5151             self.driver.detach_interface(context, instance, condemned)
5152         except exception.NovaException as ex:
5153             LOG.warning(_LW("Detach interface failed, port_id=%(port_id)s,"
5154                             " reason: %(msg)s"),
5155                         {'port_id': port_id, 'msg': ex}, instance=instance)
5156             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
5157         else:
5158             try:
5159                 self.network_api.deallocate_port_for_instance(
5160                     context, instance, port_id)
5161             except Exception as ex:
5162                 with excutils.save_and_reraise_exception():
5163                     # Since this is a cast operation, log the failure for
5164                     # triage.
5165                     LOG.warning(_LW('Failed to deallocate port %(port_id)s '
5166                                     'for instance. Error: %(error)s'),
5167                                 {'port_id': port_id, 'error': ex},
5168                                 instance=instance)
5169 
5170     def _get_compute_info(self, context, host):
5171         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
5172             context, host)
5173 
5174     @wrap_exception()
5175     def check_instance_shared_storage(self, ctxt, instance, data):
5176         """Check if the instance files are shared
5177 
5178         :param ctxt: security context
5179         :param instance: dict of instance data
5180         :param data: result of driver.check_instance_shared_storage_local
5181 
5182         Returns True if instance disks located on shared storage and
5183         False otherwise.
5184         """
5185         return self.driver.check_instance_shared_storage_remote(ctxt, data)
5186 
5187     @wrap_exception()
5188     @wrap_instance_event(prefix='compute')
5189     @wrap_instance_fault
5190     def check_can_live_migrate_destination(self, ctxt, instance,
5191                                            block_migration, disk_over_commit):
5192         """Check if it is possible to execute live migration.
5193 
5194         This runs checks on the destination host, and then calls
5195         back to the source host to check the results.
5196 
5197         :param context: security context
5198         :param instance: dict of instance data
5199         :param block_migration: if true, prepare for block migration
5200                                 if None, calculate it in driver
5201         :param disk_over_commit: if true, allow disk over commit
5202                                  if None, ignore disk usage checking
5203         :returns: a dict containing migration info
5204         """
5205         return self._do_check_can_live_migrate_destination(ctxt, instance,
5206                                                             block_migration,
5207                                                             disk_over_commit)
5208 
5209     def _do_check_can_live_migrate_destination(self, ctxt, instance,
5210                                                block_migration,
5211                                                disk_over_commit):
5212         src_compute_info = obj_base.obj_to_primitive(
5213             self._get_compute_info(ctxt, instance.host))
5214         dst_compute_info = obj_base.obj_to_primitive(
5215             self._get_compute_info(ctxt, CONF.host))
5216         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
5217             instance, src_compute_info, dst_compute_info,
5218             block_migration, disk_over_commit)
5219         LOG.debug('destination check data is %s', dest_check_data)
5220         try:
5221             migrate_data = self.compute_rpcapi.\
5222                                 check_can_live_migrate_source(ctxt, instance,
5223                                                               dest_check_data)
5224         finally:
5225             self.driver.cleanup_live_migration_destination_check(ctxt,
5226                     dest_check_data)
5227         return migrate_data
5228 
5229     @wrap_exception()
5230     @wrap_instance_event(prefix='compute')
5231     @wrap_instance_fault
5232     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
5233         """Check if it is possible to execute live migration.
5234 
5235         This checks if the live migration can succeed, based on the
5236         results from check_can_live_migrate_destination.
5237 
5238         :param ctxt: security context
5239         :param instance: dict of instance data
5240         :param dest_check_data: result of check_can_live_migrate_destination
5241         :returns: a dict containing migration info
5242         """
5243         is_volume_backed = compute_utils.is_volume_backed_instance(ctxt,
5244                                                                       instance)
5245         # TODO(tdurakov): remove dict to object conversion once RPC API version
5246         # is bumped to 5.x
5247         got_migrate_data_object = isinstance(dest_check_data,
5248                                              migrate_data_obj.LiveMigrateData)
5249         if not got_migrate_data_object:
5250             dest_check_data = \
5251                 migrate_data_obj.LiveMigrateData.detect_implementation(
5252                     dest_check_data)
5253         dest_check_data.is_volume_backed = is_volume_backed
5254         block_device_info = self._get_instance_block_device_info(
5255                             ctxt, instance, refresh_conn_info=False)
5256         result = self.driver.check_can_live_migrate_source(ctxt, instance,
5257                                                            dest_check_data,
5258                                                            block_device_info)
5259         if not got_migrate_data_object:
5260             result = result.to_legacy_dict()
5261         LOG.debug('source check data is %s', result)
5262         return result
5263 
5264     @wrap_exception()
5265     @wrap_instance_event(prefix='compute')
5266     @wrap_instance_fault
5267     def pre_live_migration(self, context, instance, block_migration, disk,
5268                            migrate_data):
5269         """Preparations for live migration at dest host.
5270 
5271         :param context: security context
5272         :param instance: dict of instance data
5273         :param block_migration: if true, prepare for block migration
5274         :param migrate_data: A dict or LiveMigrateData object holding data
5275                              required for live migration without shared
5276                              storage.
5277 
5278         """
5279         LOG.debug('pre_live_migration data is %s', migrate_data)
5280         # TODO(tdurakov): remove dict to object conversion once RPC API version
5281         # is bumped to 5.x
5282         got_migrate_data_object = isinstance(migrate_data,
5283                                              migrate_data_obj.LiveMigrateData)
5284         if not got_migrate_data_object:
5285             migrate_data = \
5286                 migrate_data_obj.LiveMigrateData.detect_implementation(
5287                     migrate_data)
5288         block_device_info = self._get_instance_block_device_info(
5289                             context, instance, refresh_conn_info=True)
5290 
5291         network_info = self.network_api.get_instance_nw_info(context, instance)
5292         self._notify_about_instance_usage(
5293                      context, instance, "live_migration.pre.start",
5294                      network_info=network_info)
5295 
5296         migrate_data = self.driver.pre_live_migration(context,
5297                                        instance,
5298                                        block_device_info,
5299                                        network_info,
5300                                        disk,
5301                                        migrate_data)
5302         LOG.debug('driver pre_live_migration data is %s', migrate_data)
5303 
5304         # NOTE(tr3buchet): setup networks on destination host
5305         self.network_api.setup_networks_on_host(context, instance,
5306                                                          self.host)
5307 
5308         # Creating filters to hypervisors and firewalls.
5309         # An example is that nova-instance-instance-xxx,
5310         # which is written to libvirt.xml(Check "virsh nwfilter-list")
5311         # This nwfilter is necessary on the destination host.
5312         # In addition, this method is creating filtering rule
5313         # onto destination host.
5314         self.driver.ensure_filtering_rules_for_instance(instance,
5315                                             network_info)
5316 
5317         self._notify_about_instance_usage(
5318                      context, instance, "live_migration.pre.end",
5319                      network_info=network_info)
5320         # TODO(tdurakov): remove dict to object conversion once RPC API version
5321         # is bumped to 5.x
5322         if not got_migrate_data_object and migrate_data:
5323             migrate_data = migrate_data.to_legacy_dict(
5324                 pre_migration_result=True)
5325             migrate_data = migrate_data['pre_live_migration_result']
5326         LOG.debug('pre_live_migration result data is %s', migrate_data)
5327         return migrate_data
5328 
5329     def _do_live_migration(self, context, dest, instance, block_migration,
5330                            migration, migrate_data):
5331         # NOTE(danms): We should enhance the RT to account for migrations
5332         # and use the status field to denote when the accounting has been
5333         # done on source/destination. For now, this is just here for status
5334         # reporting
5335         self._set_migration_status(migration, 'preparing')
5336 
5337         got_migrate_data_object = isinstance(migrate_data,
5338                                              migrate_data_obj.LiveMigrateData)
5339         if not got_migrate_data_object:
5340             migrate_data = \
5341                 migrate_data_obj.LiveMigrateData.detect_implementation(
5342                     migrate_data)
5343 
5344         try:
5345             if ('block_migration' in migrate_data and
5346                     migrate_data.block_migration):
5347                 block_device_info = self._get_instance_block_device_info(
5348                     context, instance)
5349                 disk = self.driver.get_instance_disk_info(
5350                     instance, block_device_info=block_device_info)
5351             else:
5352                 disk = None
5353 
5354             migrate_data = self.compute_rpcapi.pre_live_migration(
5355                 context, instance,
5356                 block_migration, disk, dest, migrate_data)
5357         except Exception:
5358             with excutils.save_and_reraise_exception():
5359                 LOG.exception(_LE('Pre live migration failed at %s'),
5360                               dest, instance=instance)
5361                 self._set_migration_status(migration, 'error')
5362                 self._rollback_live_migration(context, instance, dest,
5363                                               migrate_data)
5364 
5365         self._set_migration_status(migration, 'running')
5366 
5367         if migrate_data:
5368             migrate_data.migration = migration
5369         LOG.debug('live_migration data is %s', migrate_data)
5370         try:
5371             self.driver.live_migration(context, instance, dest,
5372                                        self._post_live_migration,
5373                                        self._rollback_live_migration,
5374                                        block_migration, migrate_data)
5375         except Exception:
5376             # Executing live migration
5377             # live_migration might raises exceptions, but
5378             # nothing must be recovered in this version.
5379             LOG.exception(_LE('Live migration failed.'), instance=instance)
5380             with excutils.save_and_reraise_exception():
5381                 self._set_migration_status(migration, 'error')
5382 
5383     @wrap_exception()
5384     @wrap_instance_event(prefix='compute')
5385     @wrap_instance_fault
5386     def live_migration(self, context, dest, instance, block_migration,
5387                        migration, migrate_data):
5388         """Executing live migration.
5389 
5390         :param context: security context
5391         :param dest: destination host
5392         :param instance: a nova.objects.instance.Instance object
5393         :param block_migration: if true, prepare for block migration
5394         :param migration: an nova.objects.Migration object
5395         :param migrate_data: implementation specific params
5396 
5397         """
5398         self._set_migration_status(migration, 'queued')
5399 
5400         def dispatch_live_migration(*args, **kwargs):
5401             with self._live_migration_semaphore:
5402                 self._do_live_migration(*args, **kwargs)
5403 
5404         # NOTE(danms): We spawn here to return the RPC worker thread back to
5405         # the pool. Since what follows could take a really long time, we don't
5406         # want to tie up RPC workers.
5407         utils.spawn_n(dispatch_live_migration,
5408                       context, dest, instance,
5409                       block_migration, migration,
5410                       migrate_data)
5411 
5412     # TODO(tdurakov): migration_id is used since 4.12 rpc api version
5413     # remove migration_id parameter when the compute RPC version
5414     # is bumped to 5.x.
5415     @wrap_exception()
5416     @wrap_instance_event(prefix='compute')
5417     @wrap_instance_fault
5418     def live_migration_force_complete(self, context, instance,
5419                                       migration_id=None):
5420         """Force live migration to complete.
5421 
5422         :param context: Security context
5423         :param instance: The instance that is being migrated
5424         :param migration_id: ID of ongoing migration; is currently not used,
5425         and isn't removed for backward compatibility
5426         """
5427 
5428         self._notify_about_instance_usage(
5429             context, instance, 'live.migration.force.complete.start')
5430         self.driver.live_migration_force_complete(instance)
5431         self._notify_about_instance_usage(
5432             context, instance, 'live.migration.force.complete.end')
5433 
5434     @wrap_exception()
5435     @wrap_instance_event(prefix='compute')
5436     @wrap_instance_fault
5437     def live_migration_abort(self, context, instance, migration_id):
5438         """Abort an in-progress live migration.
5439 
5440         :param context: Security context
5441         :param instance: The instance that is being migrated
5442         :param migration_id: ID of in-progress live migration
5443 
5444         """
5445         migration = objects.Migration.get_by_id(context, migration_id)
5446         if migration.status != 'running':
5447             raise exception.InvalidMigrationState(migration_id=migration_id,
5448                     instance_uuid=instance.uuid,
5449                     state=migration.status,
5450                     method='abort live migration')
5451 
5452         self._notify_about_instance_usage(
5453             context, instance, 'live.migration.abort.start')
5454         self.driver.live_migration_abort(instance)
5455         self._notify_about_instance_usage(
5456             context, instance, 'live.migration.abort.end')
5457 
5458     def _live_migration_cleanup_flags(self, migrate_data):
5459         """Determine whether disks or instance path need to be cleaned up after
5460         live migration (at source on success, at destination on rollback)
5461 
5462         Block migration needs empty image at destination host before migration
5463         starts, so if any failure occurs, any empty images has to be deleted.
5464 
5465         Also Volume backed live migration w/o shared storage needs to delete
5466         newly created instance-xxx dir on the destination as a part of its
5467         rollback process
5468 
5469         :param migrate_data: implementation specific data
5470         :returns: (bool, bool) -- do_cleanup, destroy_disks
5471         """
5472         # NOTE(pkoniszewski): block migration specific params are set inside
5473         # migrate_data objects for drivers that expose block live migration
5474         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
5475         # cleanup is not needed.
5476         is_shared_block_storage = True
5477         is_shared_instance_path = True
5478         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
5479             is_shared_block_storage = migrate_data.is_shared_block_storage
5480             is_shared_instance_path = migrate_data.is_shared_instance_path
5481         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
5482             is_shared_block_storage = not migrate_data.block_migration
5483             is_shared_instance_path = not migrate_data.block_migration
5484         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
5485             is_shared_instance_path = migrate_data.is_shared_instance_path
5486             is_shared_block_storage = migrate_data.is_shared_instance_path
5487 
5488         # No instance booting at source host, but instance dir
5489         # must be deleted for preparing next block migration
5490         # must be deleted for preparing next live migration w/o shared storage
5491         do_cleanup = not is_shared_instance_path
5492         destroy_disks = not is_shared_block_storage
5493 
5494         return (do_cleanup, destroy_disks)
5495 
5496     @wrap_exception()
5497     @wrap_instance_fault
5498     def _post_live_migration(self, ctxt, instance,
5499                             dest, block_migration=False, migrate_data=None):
5500         """Post operations for live migration.
5501 
5502         This method is called from live_migration
5503         and mainly updating database record.
5504 
5505         :param ctxt: security context
5506         :param instance: instance dict
5507         :param dest: destination host
5508         :param block_migration: if true, prepare for block migration
5509         :param migrate_data: if not None, it is a dict which has data
5510         required for live migration without shared storage
5511 
5512         """
5513         LOG.info(_LI('_post_live_migration() is started..'),
5514                  instance=instance)
5515 
5516         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5517                 ctxt, instance.uuid)
5518 
5519         # Cleanup source host post live-migration
5520         block_device_info = self._get_instance_block_device_info(
5521                             ctxt, instance, bdms=bdms)
5522         self.driver.post_live_migration(ctxt, instance, block_device_info,
5523                                         migrate_data)
5524 
5525         # Detaching volumes.
5526         connector = self.driver.get_volume_connector(instance)
5527         for bdm in bdms:
5528             # NOTE(vish): We don't want to actually mark the volume
5529             #             detached, or delete the bdm, just remove the
5530             #             connection from this host.
5531 
5532             # remove the volume connection without detaching from hypervisor
5533             # because the instance is not running anymore on the current host
5534             if bdm.is_volume:
5535                 self.volume_api.terminate_connection(ctxt, bdm.volume_id,
5536                                                      connector)
5537 
5538         # Releasing vlan.
5539         # (not necessary in current implementation?)
5540 
5541         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5542 
5543         self._notify_about_instance_usage(ctxt, instance,
5544                                           "live_migration._post.start",
5545                                           network_info=network_info)
5546         # Releasing security group ingress rule.
5547         LOG.debug('Calling driver.unfilter_instance from _post_live_migration',
5548                   instance=instance)
5549         self.driver.unfilter_instance(instance,
5550                                       network_info)
5551 
5552         migration = {'source_compute': self.host,
5553                      'dest_compute': dest, }
5554         self.network_api.migrate_instance_start(ctxt,
5555                                                 instance,
5556                                                 migration)
5557 
5558         destroy_vifs = False
5559         try:
5560             self.driver.post_live_migration_at_source(ctxt, instance,
5561                                                       network_info)
5562         except NotImplementedError as ex:
5563             LOG.debug(ex, instance=instance)
5564             # For all hypervisors other than libvirt, there is a possibility
5565             # they are unplugging networks from source node in the cleanup
5566             # method
5567             destroy_vifs = True
5568 
5569         # Define domain at destination host, without doing it,
5570         # pause/suspend/terminate do not work.
5571         try:
5572             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
5573                     instance, block_migration, dest)
5574         except Exception as error:
5575             # We don't want to break _post_live_migration() if
5576             # post_live_migration_at_destination() fails as it should never
5577             # affect cleaning up source node.
5578             LOG.exception(_LE("Post live migration at destination %s failed"),
5579                     dest, instance=instance, error=error)
5580 
5581         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
5582                 migrate_data)
5583 
5584         if do_cleanup:
5585             LOG.debug('Calling driver.cleanup from _post_live_migration',
5586                       instance=instance)
5587             self.driver.cleanup(ctxt, instance, network_info,
5588                                 destroy_disks=destroy_disks,
5589                                 migrate_data=migrate_data,
5590                                 destroy_vifs=destroy_vifs)
5591 
5592         self.instance_events.clear_events_for_instance(instance)
5593 
5594         # NOTE(timello): make sure we update available resources on source
5595         # host even before next periodic task.
5596         self.update_available_resource(ctxt)
5597 
5598         self._update_scheduler_instance_info(ctxt, instance)
5599         self._notify_about_instance_usage(ctxt, instance,
5600                                           "live_migration._post.end",
5601                                           network_info=network_info)
5602         LOG.info(_LI('Migrating instance to %s finished successfully.'),
5603                  dest, instance=instance)
5604         LOG.info(_LI("You may see the error \"libvirt: QEMU error: "
5605                      "Domain not found: no domain with matching name.\" "
5606                      "This error can be safely ignored."),
5607                  instance=instance)
5608 
5609         self._clean_instance_console_tokens(ctxt, instance)
5610         if migrate_data and migrate_data.obj_attr_is_set('migration'):
5611             migrate_data.migration.status = 'completed'
5612             migrate_data.migration.save()
5613 
5614     def _consoles_enabled(self):
5615         """Returns whether a console is enable."""
5616         return (CONF.vnc.enabled or CONF.spice.enabled or
5617                 CONF.rdp.enabled or CONF.serial_console.enabled or
5618                 CONF.mks.enabled)
5619 
5620     def _clean_instance_console_tokens(self, ctxt, instance):
5621         """Clean console tokens stored for an instance."""
5622         if self._consoles_enabled():
5623             if CONF.cells.enable:
5624                 self.cells_rpcapi.consoleauth_delete_tokens(
5625                     ctxt, instance.uuid)
5626             else:
5627                 self.consoleauth_rpcapi.delete_tokens_for_instance(
5628                     ctxt, instance.uuid)
5629 
5630     @wrap_exception()
5631     @wrap_instance_event(prefix='compute')
5632     @wrap_instance_fault
5633     def post_live_migration_at_destination(self, context, instance,
5634                                            block_migration):
5635         """Post operations for live migration .
5636 
5637         :param context: security context
5638         :param instance: Instance dict
5639         :param block_migration: if true, prepare for block migration
5640 
5641         """
5642         LOG.info(_LI('Post operation of migration started'),
5643                  instance=instance)
5644 
5645         # NOTE(tr3buchet): setup networks on destination host
5646         #                  this is called a second time because
5647         #                  multi_host does not create the bridge in
5648         #                  plug_vifs
5649         self.network_api.setup_networks_on_host(context, instance,
5650                                                          self.host)
5651         migration = {'source_compute': instance.host,
5652                      'dest_compute': self.host, }
5653         self.network_api.migrate_instance_finish(context,
5654                                                  instance,
5655                                                  migration)
5656 
5657         network_info = self.network_api.get_instance_nw_info(context, instance)
5658         self._notify_about_instance_usage(
5659                      context, instance, "live_migration.post.dest.start",
5660                      network_info=network_info)
5661         block_device_info = self._get_instance_block_device_info(context,
5662                                                                  instance)
5663 
5664         try:
5665             self.driver.post_live_migration_at_destination(
5666                 context, instance, network_info, block_migration,
5667                 block_device_info)
5668         except Exception:
5669             with excutils.save_and_reraise_exception():
5670                 instance.vm_state = vm_states.ERROR
5671                 LOG.error(_LE('Unexpected error during post live migration at '
5672                               'destination host.'), instance=instance)
5673         finally:
5674             # Restore instance state and update host
5675             current_power_state = self._get_power_state(context, instance)
5676             node_name = None
5677             prev_host = instance.host
5678             try:
5679                 compute_node = self._get_compute_info(context, self.host)
5680                 node_name = compute_node.hypervisor_hostname
5681             except exception.ComputeHostNotFound:
5682                 LOG.exception(_LE('Failed to get compute_info for %s'),
5683                               self.host)
5684             finally:
5685                 instance.host = self.host
5686                 instance.power_state = current_power_state
5687                 instance.task_state = None
5688                 instance.node = node_name
5689                 instance.progress = 0
5690                 instance.save(expected_task_state=task_states.MIGRATING)
5691 
5692         # NOTE(tr3buchet): tear down networks on source host
5693         self.network_api.setup_networks_on_host(context, instance,
5694                                                 prev_host, teardown=True)
5695         # NOTE(vish): this is necessary to update dhcp
5696         self.network_api.setup_networks_on_host(context, instance, self.host)
5697         self._notify_about_instance_usage(
5698                      context, instance, "live_migration.post.dest.end",
5699                      network_info=network_info)
5700 
5701     @wrap_exception()
5702     @wrap_instance_fault
5703     def _rollback_live_migration(self, context, instance,
5704                                  dest, migrate_data=None,
5705                                  migration_status='error'):
5706         """Recovers Instance/volume state from migrating -> running.
5707 
5708         :param context: security context
5709         :param instance: nova.objects.instance.Instance object
5710         :param dest:
5711             This method is called from live migration src host.
5712             This param specifies destination host.
5713         :param migrate_data:
5714             if not none, contains implementation specific data.
5715         :param migration_status:
5716             Contains the status we want to set for the migration object
5717 
5718         """
5719         instance.task_state = None
5720         instance.progress = 0
5721         instance.save(expected_task_state=[task_states.MIGRATING])
5722 
5723         # TODO(tdurakov): remove dict to object conversion once RPC API version
5724         # is bumped to 5.x
5725         if isinstance(migrate_data, dict):
5726             migration = migrate_data.pop('migration', None)
5727             migrate_data = \
5728                 migrate_data_obj.LiveMigrateData.detect_implementation(
5729                     migrate_data)
5730         elif (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
5731               migrate_data.obj_attr_is_set('migration')):
5732             migration = migrate_data.migration
5733         else:
5734             migration = None
5735 
5736         # NOTE(tr3buchet): setup networks on source host (really it's re-setup)
5737         self.network_api.setup_networks_on_host(context, instance, self.host)
5738 
5739         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5740                 context, instance.uuid)
5741         for bdm in bdms:
5742             if bdm.is_volume:
5743                 self.compute_rpcapi.remove_volume_connection(
5744                         context, instance, bdm.volume_id, dest)
5745 
5746         self._notify_about_instance_usage(context, instance,
5747                                           "live_migration._rollback.start")
5748 
5749         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
5750                 migrate_data)
5751 
5752         if do_cleanup:
5753             self.compute_rpcapi.rollback_live_migration_at_destination(
5754                     context, instance, dest, destroy_disks=destroy_disks,
5755                     migrate_data=migrate_data)
5756 
5757         self._notify_about_instance_usage(context, instance,
5758                                           "live_migration._rollback.end")
5759 
5760         self._set_migration_status(migration, migration_status)
5761 
5762     @wrap_exception()
5763     @wrap_instance_event(prefix='compute')
5764     @wrap_instance_fault
5765     def rollback_live_migration_at_destination(self, context, instance,
5766                                                destroy_disks,
5767                                                migrate_data):
5768         """Cleaning up image directory that is created pre_live_migration.
5769 
5770         :param context: security context
5771         :param instance: a nova.objects.instance.Instance object sent over rpc
5772         """
5773         network_info = self.network_api.get_instance_nw_info(context, instance)
5774         self._notify_about_instance_usage(
5775                       context, instance, "live_migration.rollback.dest.start",
5776                       network_info=network_info)
5777         try:
5778             # NOTE(tr3buchet): tear down networks on destination host
5779             self.network_api.setup_networks_on_host(context, instance,
5780                                                     self.host, teardown=True)
5781         except Exception:
5782             with excutils.save_and_reraise_exception():
5783                 # NOTE(tdurakov): even if teardown networks fails driver
5784                 # should try to rollback live migration on destination.
5785                 LOG.exception(
5786                     _LE('An error occurred while deallocating network.'),
5787                     instance=instance)
5788         finally:
5789             # always run this even if setup_networks_on_host fails
5790             # NOTE(vish): The mapping is passed in so the driver can disconnect
5791             #             from remote volumes if necessary
5792             block_device_info = self._get_instance_block_device_info(context,
5793                                                                      instance)
5794             # TODO(tdurakov): remove dict to object conversion once RPC API
5795             # version is bumped to 5.x
5796             if isinstance(migrate_data, dict):
5797                 migrate_data = \
5798                     migrate_data_obj.LiveMigrateData.detect_implementation(
5799                         migrate_data)
5800             self.driver.rollback_live_migration_at_destination(
5801                 context, instance, network_info, block_device_info,
5802                 destroy_disks=destroy_disks, migrate_data=migrate_data)
5803 
5804         self._notify_about_instance_usage(
5805                         context, instance, "live_migration.rollback.dest.end",
5806                         network_info=network_info)
5807 
5808     @periodic_task.periodic_task(
5809         spacing=CONF.heal_instance_info_cache_interval)
5810     def _heal_instance_info_cache(self, context):
5811         """Called periodically.  On every call, try to update the
5812         info_cache's network information for another instance by
5813         calling to the network manager.
5814 
5815         This is implemented by keeping a cache of uuids of instances
5816         that live on this host.  On each call, we pop one off of a
5817         list, pull the DB record, and try the call to the network API.
5818         If anything errors don't fail, as it's possible the instance
5819         has been deleted, etc.
5820         """
5821         heal_interval = CONF.heal_instance_info_cache_interval
5822         if not heal_interval:
5823             return
5824 
5825         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
5826         instance = None
5827 
5828         LOG.debug('Starting heal instance info cache')
5829 
5830         if not instance_uuids:
5831             # The list of instances to heal is empty so rebuild it
5832             LOG.debug('Rebuilding the list of instances to heal')
5833             db_instances = objects.InstanceList.get_by_host(
5834                 context, self.host, expected_attrs=[], use_slave=True)
5835             for inst in db_instances:
5836                 # We don't want to refresh the cache for instances
5837                 # which are building or deleting so don't put them
5838                 # in the list. If they are building they will get
5839                 # added to the list next time we build it.
5840                 if (inst.vm_state == vm_states.BUILDING):
5841                     LOG.debug('Skipping network cache update for instance '
5842                               'because it is Building.', instance=inst)
5843                     continue
5844                 if (inst.task_state == task_states.DELETING):
5845                     LOG.debug('Skipping network cache update for instance '
5846                               'because it is being deleted.', instance=inst)
5847                     continue
5848 
5849                 if not instance:
5850                     # Save the first one we find so we don't
5851                     # have to get it again
5852                     instance = inst
5853                 else:
5854                     instance_uuids.append(inst['uuid'])
5855 
5856             self._instance_uuids_to_heal = instance_uuids
5857         else:
5858             # Find the next valid instance on the list
5859             while instance_uuids:
5860                 try:
5861                     inst = objects.Instance.get_by_uuid(
5862                             context, instance_uuids.pop(0),
5863                             expected_attrs=['system_metadata', 'info_cache',
5864                                             'flavor'],
5865                             use_slave=True)
5866                 except exception.InstanceNotFound:
5867                     # Instance is gone.  Try to grab another.
5868                     continue
5869 
5870                 # Check the instance hasn't been migrated
5871                 if inst.host != self.host:
5872                     LOG.debug('Skipping network cache update for instance '
5873                               'because it has been migrated to another '
5874                               'host.', instance=inst)
5875                 # Check the instance isn't being deleting
5876                 elif inst.task_state == task_states.DELETING:
5877                     LOG.debug('Skipping network cache update for instance '
5878                               'because it is being deleted.', instance=inst)
5879                 else:
5880                     instance = inst
5881                     break
5882 
5883         if instance:
5884             # We have an instance now to refresh
5885             try:
5886                 # Call to network API to get instance info.. this will
5887                 # force an update to the instance's info_cache
5888                 self.network_api.get_instance_nw_info(context, instance)
5889                 LOG.debug('Updated the network info_cache for instance',
5890                           instance=instance)
5891             except exception.InstanceNotFound:
5892                 # Instance is gone.
5893                 LOG.debug('Instance no longer exists. Unable to refresh',
5894                           instance=instance)
5895                 return
5896             except exception.InstanceInfoCacheNotFound:
5897                 # InstanceInfoCache is gone.
5898                 LOG.debug('InstanceInfoCache no longer exists. '
5899                           'Unable to refresh', instance=instance)
5900             except Exception:
5901                 LOG.error(_LE('An error occurred while refreshing the network '
5902                               'cache.'), instance=instance, exc_info=True)
5903         else:
5904             LOG.debug("Didn't find any instances for network info cache "
5905                       "update.")
5906 
5907     @periodic_task.periodic_task
5908     def _poll_rebooting_instances(self, context):
5909         if CONF.reboot_timeout > 0:
5910             filters = {'task_state':
5911                        [task_states.REBOOTING,
5912                         task_states.REBOOT_STARTED,
5913                         task_states.REBOOT_PENDING],
5914                        'host': self.host}
5915             rebooting = objects.InstanceList.get_by_filters(
5916                 context, filters, expected_attrs=[], use_slave=True)
5917 
5918             to_poll = []
5919             for instance in rebooting:
5920                 if timeutils.is_older_than(instance.updated_at,
5921                                            CONF.reboot_timeout):
5922                     to_poll.append(instance)
5923 
5924             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
5925 
5926     @periodic_task.periodic_task
5927     def _poll_rescued_instances(self, context):
5928         if CONF.rescue_timeout > 0:
5929             filters = {'vm_state': vm_states.RESCUED,
5930                        'host': self.host}
5931             rescued_instances = objects.InstanceList.get_by_filters(
5932                 context, filters, expected_attrs=["system_metadata"],
5933                 use_slave=True)
5934 
5935             to_unrescue = []
5936             for instance in rescued_instances:
5937                 if timeutils.is_older_than(instance.launched_at,
5938                                            CONF.rescue_timeout):
5939                     to_unrescue.append(instance)
5940 
5941             for instance in to_unrescue:
5942                 self.compute_api.unrescue(context, instance)
5943 
5944     @periodic_task.periodic_task
5945     def _poll_unconfirmed_resizes(self, context):
5946         if CONF.resize_confirm_window == 0:
5947             return
5948 
5949         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
5950                 context, CONF.resize_confirm_window, self.host,
5951                 use_slave=True)
5952 
5953         migrations_info = dict(migration_count=len(migrations),
5954                 confirm_window=CONF.resize_confirm_window)
5955 
5956         if migrations_info["migration_count"] > 0:
5957             LOG.info(_LI("Found %(migration_count)d unconfirmed migrations "
5958                          "older than %(confirm_window)d seconds"),
5959                      migrations_info)
5960 
5961         def _set_migration_to_error(migration, reason, **kwargs):
5962             LOG.warning(_LW("Setting migration %(migration_id)s to error: "
5963                          "%(reason)s"),
5964                      {'migration_id': migration['id'], 'reason': reason},
5965                      **kwargs)
5966             migration.status = 'error'
5967             with migration.obj_as_admin():
5968                 migration.save()
5969 
5970         for migration in migrations:
5971             instance_uuid = migration.instance_uuid
5972             LOG.info(_LI("Automatically confirming migration "
5973                          "%(migration_id)s for instance %(instance_uuid)s"),
5974                      {'migration_id': migration.id,
5975                       'instance_uuid': instance_uuid})
5976             expected_attrs = ['metadata', 'system_metadata']
5977             try:
5978                 instance = objects.Instance.get_by_uuid(context,
5979                             instance_uuid, expected_attrs=expected_attrs,
5980                             use_slave=True)
5981             except exception.InstanceNotFound:
5982                 reason = (_("Instance %s not found") %
5983                           instance_uuid)
5984                 _set_migration_to_error(migration, reason)
5985                 continue
5986             if instance.vm_state == vm_states.ERROR:
5987                 reason = _("In ERROR state")
5988                 _set_migration_to_error(migration, reason,
5989                                         instance=instance)
5990                 continue
5991             # race condition: The instance in DELETING state should not be
5992             # set the migration state to error, otherwise the instance in
5993             # to be deleted which is in RESIZED state
5994             # will not be able to confirm resize
5995             if instance.task_state in [task_states.DELETING,
5996                                        task_states.SOFT_DELETING]:
5997                 msg = ("Instance being deleted or soft deleted during resize "
5998                        "confirmation. Skipping.")
5999                 LOG.debug(msg, instance=instance)
6000                 continue
6001 
6002             # race condition: This condition is hit when this method is
6003             # called between the save of the migration record with a status of
6004             # finished and the save of the instance object with a state of
6005             # RESIZED. The migration record should not be set to error.
6006             if instance.task_state == task_states.RESIZE_FINISH:
6007                 msg = ("Instance still resizing during resize "
6008                        "confirmation. Skipping.")
6009                 LOG.debug(msg, instance=instance)
6010                 continue
6011 
6012             vm_state = instance.vm_state
6013             task_state = instance.task_state
6014             if vm_state != vm_states.RESIZED or task_state is not None:
6015                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
6016                            "RESIZED/None") %
6017                           {'vm_state': vm_state,
6018                            'task_state': task_state})
6019                 _set_migration_to_error(migration, reason,
6020                                         instance=instance)
6021                 continue
6022             try:
6023                 self.compute_api.confirm_resize(context, instance,
6024                                                 migration=migration)
6025             except Exception as e:
6026                 LOG.info(_LI("Error auto-confirming resize: %s. "
6027                              "Will retry later."),
6028                          e, instance=instance)
6029 
6030     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
6031     def _poll_shelved_instances(self, context):
6032 
6033         if CONF.shelved_offload_time <= 0:
6034             return
6035 
6036         filters = {'vm_state': vm_states.SHELVED,
6037                    'task_state': None,
6038                    'host': self.host}
6039         shelved_instances = objects.InstanceList.get_by_filters(
6040             context, filters=filters, expected_attrs=['system_metadata'],
6041             use_slave=True)
6042 
6043         to_gc = []
6044         for instance in shelved_instances:
6045             sys_meta = instance.system_metadata
6046             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
6047             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
6048                 to_gc.append(instance)
6049 
6050         for instance in to_gc:
6051             try:
6052                 instance.task_state = task_states.SHELVING_OFFLOADING
6053                 instance.save(expected_task_state=(None,))
6054                 self.shelve_offload_instance(context, instance,
6055                                              clean_shutdown=False)
6056             except Exception:
6057                 LOG.exception(_LE('Periodic task failed to offload instance.'),
6058                         instance=instance)
6059 
6060     @periodic_task.periodic_task
6061     def _instance_usage_audit(self, context):
6062         if not CONF.instance_usage_audit:
6063             return
6064 
6065         begin, end = utils.last_completed_audit_period()
6066         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
6067                                self.host):
6068             return
6069 
6070         instances = objects.InstanceList.get_active_by_window_joined(
6071             context, begin, end, host=self.host,
6072             expected_attrs=['system_metadata', 'info_cache', 'metadata',
6073                             'flavor'],
6074             use_slave=True)
6075         num_instances = len(instances)
6076         errors = 0
6077         successes = 0
6078         LOG.info(_LI("Running instance usage audit for"
6079                      " host %(host)s from %(begin_time)s to "
6080                      "%(end_time)s. %(number_instances)s"
6081                      " instances."),
6082                  {'host': self.host,
6083                   'begin_time': begin,
6084                   'end_time': end,
6085                   'number_instances': num_instances})
6086         start_time = time.time()
6087         task_log = objects.TaskLog(context)
6088         task_log.task_name = 'instance_usage_audit'
6089         task_log.period_beginning = begin
6090         task_log.period_ending = end
6091         task_log.host = self.host
6092         task_log.task_items = num_instances
6093         task_log.message = 'Instance usage audit started...'
6094         task_log.begin_task()
6095         for instance in instances:
6096             try:
6097                 compute_utils.notify_usage_exists(
6098                     self.notifier, context, instance,
6099                     ignore_missing_network_data=False)
6100                 successes += 1
6101             except Exception:
6102                 LOG.exception(_LE('Failed to generate usage '
6103                                   'audit for instance '
6104                                   'on host %s'), self.host,
6105                               instance=instance)
6106                 errors += 1
6107         task_log.errors = errors
6108         task_log.message = (
6109             'Instance usage audit ran for host %s, %s instances in %s seconds.'
6110             % (self.host, num_instances, time.time() - start_time))
6111         task_log.end_task()
6112 
6113     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
6114     def _poll_bandwidth_usage(self, context):
6115 
6116         if not self._bw_usage_supported:
6117             return
6118 
6119         prev_time, start_time = utils.last_completed_audit_period()
6120 
6121         curr_time = time.time()
6122         if (curr_time - self._last_bw_usage_poll >
6123                 CONF.bandwidth_poll_interval):
6124             self._last_bw_usage_poll = curr_time
6125             LOG.info(_LI("Updating bandwidth usage cache"))
6126             cells_update_interval = CONF.cells.bandwidth_update_interval
6127             if (cells_update_interval > 0 and
6128                    curr_time - self._last_bw_usage_cell_update >
6129                            cells_update_interval):
6130                 self._last_bw_usage_cell_update = curr_time
6131                 update_cells = True
6132             else:
6133                 update_cells = False
6134 
6135             instances = objects.InstanceList.get_by_host(context,
6136                                                               self.host,
6137                                                               use_slave=True)
6138             try:
6139                 bw_counters = self.driver.get_all_bw_counters(instances)
6140             except NotImplementedError:
6141                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
6142                 # implemented yet.  If they don't it doesn't break anything,
6143                 # they just don't get the info in the usage events.
6144                 # NOTE(PhilDay): Record that its not supported so we can
6145                 # skip fast on future calls rather than waste effort getting
6146                 # the list of instances.
6147                 LOG.info(_LI("Bandwidth usage not supported by "
6148                              "hypervisor."))
6149                 self._bw_usage_supported = False
6150                 return
6151 
6152             refreshed = timeutils.utcnow()
6153             for bw_ctr in bw_counters:
6154                 # Allow switching of greenthreads between queries.
6155                 greenthread.sleep(0)
6156                 bw_in = 0
6157                 bw_out = 0
6158                 last_ctr_in = None
6159                 last_ctr_out = None
6160                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
6161                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
6162                     start_period=start_time, use_slave=True)
6163                 if usage:
6164                     bw_in = usage.bw_in
6165                     bw_out = usage.bw_out
6166                     last_ctr_in = usage.last_ctr_in
6167                     last_ctr_out = usage.last_ctr_out
6168                 else:
6169                     usage = (objects.BandwidthUsage.
6170                              get_by_instance_uuid_and_mac(
6171                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
6172                         start_period=prev_time, use_slave=True))
6173                     if usage:
6174                         last_ctr_in = usage.last_ctr_in
6175                         last_ctr_out = usage.last_ctr_out
6176 
6177                 if last_ctr_in is not None:
6178                     if bw_ctr['bw_in'] < last_ctr_in:
6179                         # counter rollover
6180                         bw_in += bw_ctr['bw_in']
6181                     else:
6182                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
6183 
6184                 if last_ctr_out is not None:
6185                     if bw_ctr['bw_out'] < last_ctr_out:
6186                         # counter rollover
6187                         bw_out += bw_ctr['bw_out']
6188                     else:
6189                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
6190 
6191                 objects.BandwidthUsage(context=context).create(
6192                                               bw_ctr['uuid'],
6193                                               bw_ctr['mac_address'],
6194                                               bw_in,
6195                                               bw_out,
6196                                               bw_ctr['bw_in'],
6197                                               bw_ctr['bw_out'],
6198                                               start_period=start_time,
6199                                               last_refreshed=refreshed,
6200                                               update_cells=update_cells)
6201 
6202     def _get_host_volume_bdms(self, context, use_slave=False):
6203         """Return all block device mappings on a compute host."""
6204         compute_host_bdms = []
6205         instances = objects.InstanceList.get_by_host(context, self.host,
6206             use_slave=use_slave)
6207         for instance in instances:
6208             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6209                     context, instance.uuid, use_slave=use_slave)
6210             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
6211             compute_host_bdms.append(dict(instance=instance,
6212                                           instance_bdms=instance_bdms))
6213 
6214         return compute_host_bdms
6215 
6216     def _update_volume_usage_cache(self, context, vol_usages):
6217         """Updates the volume usage cache table with a list of stats."""
6218         for usage in vol_usages:
6219             # Allow switching of greenthreads between queries.
6220             greenthread.sleep(0)
6221             vol_usage = objects.VolumeUsage(context)
6222             vol_usage.volume_id = usage['volume']
6223             vol_usage.instance_uuid = usage['instance'].uuid
6224             vol_usage.project_id = usage['instance'].project_id
6225             vol_usage.user_id = usage['instance'].user_id
6226             vol_usage.availability_zone = usage['instance'].availability_zone
6227             vol_usage.curr_reads = usage['rd_req']
6228             vol_usage.curr_read_bytes = usage['rd_bytes']
6229             vol_usage.curr_writes = usage['wr_req']
6230             vol_usage.curr_write_bytes = usage['wr_bytes']
6231             vol_usage.save()
6232             self.notifier.info(context, 'volume.usage',
6233                                compute_utils.usage_volume_info(vol_usage))
6234 
6235     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
6236     def _poll_volume_usage(self, context):
6237         if CONF.volume_usage_poll_interval == 0:
6238             return
6239 
6240         compute_host_bdms = self._get_host_volume_bdms(context,
6241                                                        use_slave=True)
6242         if not compute_host_bdms:
6243             return
6244 
6245         LOG.debug("Updating volume usage cache")
6246         try:
6247             vol_usages = self.driver.get_all_volume_usage(context,
6248                                                           compute_host_bdms)
6249         except NotImplementedError:
6250             return
6251 
6252         self._update_volume_usage_cache(context, vol_usages)
6253 
6254     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
6255                                  run_immediately=True)
6256     def _sync_power_states(self, context):
6257         """Align power states between the database and the hypervisor.
6258 
6259         To sync power state data we make a DB call to get the number of
6260         virtual machines known by the hypervisor and if the number matches the
6261         number of virtual machines known by the database, we proceed in a lazy
6262         loop, one database record at a time, checking if the hypervisor has the
6263         same power state as is in the database.
6264         """
6265         db_instances = objects.InstanceList.get_by_host(context, self.host,
6266                                                         expected_attrs=[],
6267                                                         use_slave=True)
6268 
6269         num_vm_instances = self.driver.get_num_instances()
6270         num_db_instances = len(db_instances)
6271 
6272         if num_vm_instances != num_db_instances:
6273             LOG.warning(_LW("While synchronizing instance power states, found "
6274                             "%(num_db_instances)s instances in the database "
6275                             "and %(num_vm_instances)s instances on the "
6276                             "hypervisor."),
6277                         {'num_db_instances': num_db_instances,
6278                          'num_vm_instances': num_vm_instances})
6279 
6280         def _sync(db_instance):
6281             # NOTE(melwitt): This must be synchronized as we query state from
6282             #                two separate sources, the driver and the database.
6283             #                They are set (in stop_instance) and read, in sync.
6284             @utils.synchronized(db_instance.uuid)
6285             def query_driver_power_state_and_sync():
6286                 self._query_driver_power_state_and_sync(context, db_instance)
6287 
6288             try:
6289                 query_driver_power_state_and_sync()
6290             except Exception:
6291                 LOG.exception(_LE("Periodic sync_power_state task had an "
6292                                   "error while processing an instance."),
6293                               instance=db_instance)
6294 
6295             self._syncs_in_progress.pop(db_instance.uuid)
6296 
6297         for db_instance in db_instances:
6298             # process syncs asynchronously - don't want instance locking to
6299             # block entire periodic task thread
6300             uuid = db_instance.uuid
6301             if uuid in self._syncs_in_progress:
6302                 LOG.debug('Sync already in progress for %s', uuid)
6303             else:
6304                 LOG.debug('Triggering sync for uuid %s', uuid)
6305                 self._syncs_in_progress[uuid] = True
6306                 self._sync_power_pool.spawn_n(_sync, db_instance)
6307 
6308     def _query_driver_power_state_and_sync(self, context, db_instance):
6309         if db_instance.task_state is not None:
6310             LOG.info(_LI("During sync_power_state the instance has a "
6311                          "pending task (%(task)s). Skip."),
6312                      {'task': db_instance.task_state}, instance=db_instance)
6313             return
6314         # No pending tasks. Now try to figure out the real vm_power_state.
6315         try:
6316             vm_instance = self.driver.get_info(db_instance)
6317             vm_power_state = vm_instance.state
6318         except exception.InstanceNotFound:
6319             vm_power_state = power_state.NOSTATE
6320         # Note(maoy): the above get_info call might take a long time,
6321         # for example, because of a broken libvirt driver.
6322         try:
6323             self._sync_instance_power_state(context,
6324                                             db_instance,
6325                                             vm_power_state,
6326                                             use_slave=True)
6327         except exception.InstanceNotFound:
6328             # NOTE(hanlind): If the instance gets deleted during sync,
6329             # silently ignore.
6330             pass
6331 
6332     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
6333                                    use_slave=False):
6334         """Align instance power state between the database and hypervisor.
6335 
6336         If the instance is not found on the hypervisor, but is in the database,
6337         then a stop() API will be called on the instance.
6338         """
6339 
6340         # We re-query the DB to get the latest instance info to minimize
6341         # (not eliminate) race condition.
6342         db_instance.refresh(use_slave=use_slave)
6343         db_power_state = db_instance.power_state
6344         vm_state = db_instance.vm_state
6345 
6346         if self.host != db_instance.host:
6347             # on the sending end of nova-compute _sync_power_state
6348             # may have yielded to the greenthread performing a live
6349             # migration; this in turn has changed the resident-host
6350             # for the VM; However, the instance is still active, it
6351             # is just in the process of migrating to another host.
6352             # This implies that the compute source must relinquish
6353             # control to the compute destination.
6354             LOG.info(_LI("During the sync_power process the "
6355                          "instance has moved from "
6356                          "host %(src)s to host %(dst)s"),
6357                      {'src': db_instance.host,
6358                       'dst': self.host},
6359                      instance=db_instance)
6360             return
6361         elif db_instance.task_state is not None:
6362             # on the receiving end of nova-compute, it could happen
6363             # that the DB instance already report the new resident
6364             # but the actual VM has not showed up on the hypervisor
6365             # yet. In this case, let's allow the loop to continue
6366             # and run the state sync in a later round
6367             LOG.info(_LI("During sync_power_state the instance has a "
6368                          "pending task (%(task)s). Skip."),
6369                      {'task': db_instance.task_state},
6370                      instance=db_instance)
6371             return
6372 
6373         orig_db_power_state = db_power_state
6374         if vm_power_state != db_power_state:
6375             LOG.info(_LI('During _sync_instance_power_state the DB '
6376                          'power_state (%(db_power_state)s) does not match '
6377                          'the vm_power_state from the hypervisor '
6378                          '(%(vm_power_state)s). Updating power_state in the '
6379                          'DB to match the hypervisor.'),
6380                      {'db_power_state': db_power_state,
6381                       'vm_power_state': vm_power_state},
6382                      instance=db_instance)
6383             # power_state is always updated from hypervisor to db
6384             db_instance.power_state = vm_power_state
6385             db_instance.save()
6386             db_power_state = vm_power_state
6387 
6388         # Note(maoy): Now resolve the discrepancy between vm_state and
6389         # vm_power_state. We go through all possible vm_states.
6390         if vm_state in (vm_states.BUILDING,
6391                         vm_states.RESCUED,
6392                         vm_states.RESIZED,
6393                         vm_states.SUSPENDED,
6394                         vm_states.ERROR):
6395             # TODO(maoy): we ignore these vm_state for now.
6396             pass
6397         elif vm_state == vm_states.ACTIVE:
6398             # The only rational power state should be RUNNING
6399             if vm_power_state in (power_state.SHUTDOWN,
6400                                   power_state.CRASHED):
6401                 LOG.warning(_LW("Instance shutdown by itself. Calling the "
6402                                 "stop API. Current vm_state: %(vm_state)s, "
6403                                 "current task_state: %(task_state)s, "
6404                                 "original DB power_state: %(db_power_state)s, "
6405                                 "current VM power_state: %(vm_power_state)s"),
6406                             {'vm_state': vm_state,
6407                              'task_state': db_instance.task_state,
6408                              'db_power_state': orig_db_power_state,
6409                              'vm_power_state': vm_power_state},
6410                             instance=db_instance)
6411                 try:
6412                     # Note(maoy): here we call the API instead of
6413                     # brutally updating the vm_state in the database
6414                     # to allow all the hooks and checks to be performed.
6415                     if db_instance.shutdown_terminate:
6416                         self.compute_api.delete(context, db_instance)
6417                     else:
6418                         self.compute_api.stop(context, db_instance)
6419                 except Exception:
6420                     # Note(maoy): there is no need to propagate the error
6421                     # because the same power_state will be retrieved next
6422                     # time and retried.
6423                     # For example, there might be another task scheduled.
6424                     LOG.exception(_LE("error during stop() in "
6425                                       "sync_power_state."),
6426                                   instance=db_instance)
6427             elif vm_power_state == power_state.SUSPENDED:
6428                 LOG.warning(_LW("Instance is suspended unexpectedly. Calling "
6429                                 "the stop API."), instance=db_instance)
6430                 try:
6431                     self.compute_api.stop(context, db_instance)
6432                 except Exception:
6433                     LOG.exception(_LE("error during stop() in "
6434                                       "sync_power_state."),
6435                                   instance=db_instance)
6436             elif vm_power_state == power_state.PAUSED:
6437                 # Note(maoy): a VM may get into the paused state not only
6438                 # because the user request via API calls, but also
6439                 # due to (temporary) external instrumentations.
6440                 # Before the virt layer can reliably report the reason,
6441                 # we simply ignore the state discrepancy. In many cases,
6442                 # the VM state will go back to running after the external
6443                 # instrumentation is done. See bug 1097806 for details.
6444                 LOG.warning(_LW("Instance is paused unexpectedly. Ignore."),
6445                             instance=db_instance)
6446             elif vm_power_state == power_state.NOSTATE:
6447                 # Occasionally, depending on the status of the hypervisor,
6448                 # which could be restarting for example, an instance may
6449                 # not be found.  Therefore just log the condition.
6450                 LOG.warning(_LW("Instance is unexpectedly not found. Ignore."),
6451                             instance=db_instance)
6452         elif vm_state == vm_states.STOPPED:
6453             if vm_power_state not in (power_state.NOSTATE,
6454                                       power_state.SHUTDOWN,
6455                                       power_state.CRASHED):
6456                 LOG.warning(_LW("Instance is not stopped. Calling "
6457                                 "the stop API. Current vm_state: %(vm_state)s,"
6458                                 " current task_state: %(task_state)s, "
6459                                 "original DB power_state: %(db_power_state)s, "
6460                                 "current VM power_state: %(vm_power_state)s"),
6461                             {'vm_state': vm_state,
6462                              'task_state': db_instance.task_state,
6463                              'db_power_state': orig_db_power_state,
6464                              'vm_power_state': vm_power_state},
6465                             instance=db_instance)
6466                 try:
6467                     # NOTE(russellb) Force the stop, because normally the
6468                     # compute API would not allow an attempt to stop a stopped
6469                     # instance.
6470                     self.compute_api.force_stop(context, db_instance)
6471                 except Exception:
6472                     LOG.exception(_LE("error during stop() in "
6473                                       "sync_power_state."),
6474                                   instance=db_instance)
6475         elif vm_state == vm_states.PAUSED:
6476             if vm_power_state in (power_state.SHUTDOWN,
6477                                   power_state.CRASHED):
6478                 LOG.warning(_LW("Paused instance shutdown by itself. Calling "
6479                                 "the stop API."), instance=db_instance)
6480                 try:
6481                     self.compute_api.force_stop(context, db_instance)
6482                 except Exception:
6483                     LOG.exception(_LE("error during stop() in "
6484                                       "sync_power_state."),
6485                                   instance=db_instance)
6486         elif vm_state in (vm_states.SOFT_DELETED,
6487                           vm_states.DELETED):
6488             if vm_power_state not in (power_state.NOSTATE,
6489                                       power_state.SHUTDOWN):
6490                 # Note(maoy): this should be taken care of periodically in
6491                 # _cleanup_running_deleted_instances().
6492                 LOG.warning(_LW("Instance is not (soft-)deleted."),
6493                             instance=db_instance)
6494 
6495     @periodic_task.periodic_task
6496     def _reclaim_queued_deletes(self, context):
6497         """Reclaim instances that are queued for deletion."""
6498         interval = CONF.reclaim_instance_interval
6499         if interval <= 0:
6500             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
6501             return
6502 
6503         # TODO(comstud, jichenjc): Dummy quota object for now See bug 1296414.
6504         # The only case that the quota might be inconsistent is
6505         # the compute node died between set instance state to SOFT_DELETED
6506         # and quota commit to DB. When compute node starts again
6507         # it will have no idea the reservation is committed or not or even
6508         # expired, since it's a rare case, so marked as todo.
6509         quotas = objects.Quotas.from_reservations(context, None)
6510 
6511         filters = {'vm_state': vm_states.SOFT_DELETED,
6512                    'task_state': None,
6513                    'host': self.host}
6514         instances = objects.InstanceList.get_by_filters(
6515             context, filters,
6516             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
6517             use_slave=True)
6518         for instance in instances:
6519             if self._deleted_old_enough(instance, interval):
6520                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6521                         context, instance.uuid)
6522                 LOG.info(_LI('Reclaiming deleted instance'), instance=instance)
6523                 try:
6524                     self._delete_instance(context, instance, bdms, quotas)
6525                 except Exception as e:
6526                     LOG.warning(_LW("Periodic reclaim failed to delete "
6527                                     "instance: %s"),
6528                                 e, instance=instance)
6529 
6530     def update_available_resource_for_node(self, context, nodename):
6531 
6532         rt = self._get_resource_tracker()
6533         try:
6534             rt.update_available_resource(context, nodename)
6535         except exception.ComputeHostNotFound:
6536             # NOTE(comstud): We can get to this case if a node was
6537             # marked 'deleted' in the DB and then re-added with a
6538             # different auto-increment id. The cached resource
6539             # tracker tried to update a deleted record and failed.
6540             # Don't add this resource tracker to the new dict, so
6541             # that this will resolve itself on the next run.
6542             LOG.info(_LI("Compute node '%s' not found in "
6543                          "update_available_resource."), nodename)
6544             # TODO(jaypipes): Yes, this is inefficient to throw away all of the
6545             # compute nodes to force a rebuild, but this is only temporary
6546             # until Ironic baremetal node resource providers are tracked
6547             # properly in the report client and this is a tiny edge case
6548             # anyway.
6549             self._resource_tracker = None
6550             return
6551         except Exception:
6552             LOG.exception(_LE("Error updating resources for node "
6553                           "%(node)s."), {'node': nodename})
6554 
6555     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
6556     def update_available_resource(self, context):
6557         """See driver.get_available_resource()
6558 
6559         Periodic process that keeps that the compute host's understanding of
6560         resource availability and usage in sync with the underlying hypervisor.
6561 
6562         :param context: security context
6563         """
6564 
6565         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
6566                                                             use_slave=True)
6567         nodenames = set(self.driver.get_available_nodes())
6568         for nodename in nodenames:
6569             self.update_available_resource_for_node(context, nodename)
6570 
6571         # Delete orphan compute node not reported by driver but still in db
6572         for cn in compute_nodes_in_db:
6573             if cn.hypervisor_hostname not in nodenames:
6574                 LOG.info(_LI("Deleting orphan compute node %(id)s "
6575                              "hypervisor host is %(hh)s, "
6576                              "nodes are %(nodes)s"),
6577                              {'id': cn.id, 'hh': cn.hypervisor_hostname,
6578                               'nodes': nodenames})
6579                 cn.destroy()
6580 
6581     def _get_compute_nodes_in_db(self, context, use_slave=False):
6582         try:
6583             return objects.ComputeNodeList.get_all_by_host(context, self.host,
6584                                                            use_slave=use_slave)
6585         except exception.NotFound:
6586             LOG.error(_LE("No compute node record for host %s"), self.host)
6587             return []
6588 
6589     @periodic_task.periodic_task(
6590         spacing=CONF.running_deleted_instance_poll_interval)
6591     def _cleanup_running_deleted_instances(self, context):
6592         """Cleanup any instances which are erroneously still running after
6593         having been deleted.
6594 
6595         Valid actions to take are:
6596 
6597             1. noop - do nothing
6598             2. log - log which instances are erroneously running
6599             3. reap - shutdown and cleanup any erroneously running instances
6600             4. shutdown - power off *and disable* any erroneously running
6601                           instances
6602 
6603         The use-case for this cleanup task is: for various reasons, it may be
6604         possible for the database to show an instance as deleted but for that
6605         instance to still be running on a host machine (see bug
6606         https://bugs.launchpad.net/nova/+bug/911366).
6607 
6608         This cleanup task is a cross-hypervisor utility for finding these
6609         zombied instances and either logging the discrepancy (likely what you
6610         should do in production), or automatically reaping the instances (more
6611         appropriate for dev environments).
6612         """
6613         action = CONF.running_deleted_instance_action
6614 
6615         if action == "noop":
6616             return
6617 
6618         # NOTE(sirp): admin contexts don't ordinarily return deleted records
6619         with utils.temporary_mutation(context, read_deleted="yes"):
6620             for instance in self._running_deleted_instances(context):
6621                 if action == "log":
6622                     LOG.warning(_LW("Detected instance with name label "
6623                                     "'%s' which is marked as "
6624                                     "DELETED but still present on host."),
6625                                 instance.name, instance=instance)
6626 
6627                 elif action == 'shutdown':
6628                     LOG.info(_LI("Powering off instance with name label "
6629                                  "'%s' which is marked as "
6630                                  "DELETED but still present on host."),
6631                              instance.name, instance=instance)
6632                     try:
6633                         try:
6634                             # disable starting the instance
6635                             self.driver.set_bootable(instance, False)
6636                         except NotImplementedError:
6637                             LOG.debug("set_bootable is not implemented "
6638                                       "for the current driver")
6639                         # and power it off
6640                         self.driver.power_off(instance)
6641                     except Exception:
6642                         msg = _LW("Failed to power off instance")
6643                         LOG.warning(msg, instance=instance, exc_info=True)
6644 
6645                 elif action == 'reap':
6646                     LOG.info(_LI("Destroying instance with name label "
6647                                  "'%s' which is marked as "
6648                                  "DELETED but still present on host."),
6649                              instance.name, instance=instance)
6650                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6651                         context, instance.uuid, use_slave=True)
6652                     self.instance_events.clear_events_for_instance(instance)
6653                     try:
6654                         self._shutdown_instance(context, instance, bdms,
6655                                                 notify=False)
6656                         self._cleanup_volumes(context, instance.uuid, bdms)
6657                     except Exception as e:
6658                         LOG.warning(_LW("Periodic cleanup failed to delete "
6659                                         "instance: %s"),
6660                                     e, instance=instance)
6661                 else:
6662                     raise Exception(_("Unrecognized value '%s'"
6663                                       " for CONF.running_deleted_"
6664                                       "instance_action") % action)
6665 
6666     def _running_deleted_instances(self, context):
6667         """Returns a list of instances nova thinks is deleted,
6668         but the hypervisor thinks is still running.
6669         """
6670         timeout = CONF.running_deleted_instance_timeout
6671         filters = {'deleted': True,
6672                    'soft_deleted': False,
6673                    'host': self.host}
6674         instances = self._get_instances_on_driver(context, filters)
6675         return [i for i in instances if self._deleted_old_enough(i, timeout)]
6676 
6677     def _deleted_old_enough(self, instance, timeout):
6678         deleted_at = instance.deleted_at
6679         if deleted_at:
6680             deleted_at = deleted_at.replace(tzinfo=None)
6681         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
6682 
6683     @contextlib.contextmanager
6684     def _error_out_instance_on_exception(self, context, instance,
6685                                          quotas=None,
6686                                          instance_state=vm_states.ACTIVE):
6687         instance_uuid = instance.uuid
6688         try:
6689             yield
6690         except NotImplementedError as error:
6691             with excutils.save_and_reraise_exception():
6692                 if quotas:
6693                     quotas.rollback()
6694                 LOG.info(_LI("Setting instance back to %(state)s after: "
6695                              "%(error)s"),
6696                          {'state': instance_state, 'error': error},
6697                          instance_uuid=instance_uuid)
6698                 self._instance_update(context, instance,
6699                                       vm_state=instance_state,
6700                                       task_state=None)
6701         except exception.InstanceFaultRollback as error:
6702             if quotas:
6703                 quotas.rollback()
6704             LOG.info(_LI("Setting instance back to ACTIVE after: %s"),
6705                      error, instance_uuid=instance_uuid)
6706             self._instance_update(context, instance,
6707                                   vm_state=vm_states.ACTIVE,
6708                                   task_state=None)
6709             raise error.inner_exception
6710         except Exception:
6711             LOG.exception(_LE('Setting instance vm_state to ERROR'),
6712                           instance_uuid=instance_uuid)
6713             with excutils.save_and_reraise_exception():
6714                 if quotas:
6715                     quotas.rollback()
6716                 self._set_instance_obj_error_state(context, instance)
6717 
6718     @wrap_exception()
6719     def add_aggregate_host(self, context, aggregate, host, slave_info):
6720         """Notify hypervisor of change (for hypervisor pools)."""
6721         try:
6722             self.driver.add_to_aggregate(context, aggregate, host,
6723                                          slave_info=slave_info)
6724         except NotImplementedError:
6725             LOG.debug('Hypervisor driver does not support '
6726                       'add_aggregate_host')
6727         except exception.AggregateError:
6728             with excutils.save_and_reraise_exception():
6729                 self.driver.undo_aggregate_operation(
6730                                     context,
6731                                     aggregate.delete_host,
6732                                     aggregate, host)
6733 
6734     @wrap_exception()
6735     def remove_aggregate_host(self, context, host, slave_info, aggregate):
6736         """Removes a host from a physical hypervisor pool."""
6737         try:
6738             self.driver.remove_from_aggregate(context, aggregate, host,
6739                                               slave_info=slave_info)
6740         except NotImplementedError:
6741             LOG.debug('Hypervisor driver does not support '
6742                       'remove_aggregate_host')
6743         except (exception.AggregateError,
6744                 exception.InvalidAggregateAction) as e:
6745             with excutils.save_and_reraise_exception():
6746                 self.driver.undo_aggregate_operation(
6747                                     context,
6748                                     aggregate.add_host,
6749                                     aggregate, host,
6750                                     isinstance(e, exception.AggregateError))
6751 
6752     def _process_instance_event(self, instance, event):
6753         _event = self.instance_events.pop_instance_event(instance, event)
6754         if _event:
6755             LOG.debug('Processing event %(event)s',
6756                       {'event': event.key}, instance=instance)
6757             _event.send(event)
6758         else:
6759             LOG.warning(_LW('Received unexpected event %(event)s for '
6760                             'instance'),
6761                         {'event': event.key}, instance=instance)
6762 
6763     def _process_instance_vif_deleted_event(self, context, instance,
6764                                             deleted_vif_id):
6765         # If an attached port is deleted by neutron, it needs to
6766         # be detached from the instance.
6767         # And info cache needs to be updated.
6768         network_info = instance.info_cache.network_info
6769         for index, vif in enumerate(network_info):
6770             if vif['id'] == deleted_vif_id:
6771                 LOG.info(_LI('Neutron deleted interface %(intf)s; '
6772                              'detaching it from the instance and '
6773                              'deleting it from the info cache'),
6774                          {'intf': vif['id']},
6775                          instance=instance)
6776                 del network_info[index]
6777                 base_net_api.update_instance_cache_with_nw_info(
6778                                  self.network_api, context,
6779                                  instance,
6780                                  nw_info=network_info)
6781                 try:
6782                     self.driver.detach_interface(context, instance, vif)
6783                 except exception.NovaException as ex:
6784                     LOG.warning(_LW("Detach interface failed, "
6785                                     "port_id=%(port_id)s, reason: %(msg)s"),
6786                                 {'port_id': deleted_vif_id, 'msg': ex},
6787                                 instance=instance)
6788                 break
6789 
6790     @wrap_exception()
6791     def external_instance_event(self, context, instances, events):
6792         # NOTE(danms): Some event types are handled by the manager, such
6793         # as when we're asked to update the instance's info_cache. If it's
6794         # not one of those, look for some thread(s) waiting for the event and
6795         # unblock them if so.
6796         for event in events:
6797             instance = [inst for inst in instances
6798                         if inst.uuid == event.instance_uuid][0]
6799             LOG.debug('Received event %(event)s',
6800                       {'event': event.key},
6801                       instance=instance)
6802             if event.name == 'network-changed':
6803                 try:
6804                     self.network_api.get_instance_nw_info(context, instance)
6805                 except exception.NotFound as e:
6806                     LOG.info(_LI('Failed to process external instance event '
6807                                  '%(event)s due to: %(error)s'),
6808                              {'event': event.key, 'error': six.text_type(e)},
6809                              instance=instance)
6810             elif event.name == 'network-vif-deleted':
6811                 self._process_instance_vif_deleted_event(context,
6812                                                          instance,
6813                                                          event.tag)
6814             else:
6815                 self._process_instance_event(instance, event)
6816 
6817     @periodic_task.periodic_task(spacing=CONF.image_cache_manager_interval,
6818                                  external_process_ok=True)
6819     def _run_image_cache_manager_pass(self, context):
6820         """Run a single pass of the image cache manager."""
6821 
6822         if not self.driver.capabilities["has_imagecache"]:
6823             return
6824 
6825         # Determine what other nodes use this storage
6826         storage_users.register_storage_use(CONF.instances_path, CONF.host)
6827         nodes = storage_users.get_storage_users(CONF.instances_path)
6828 
6829         # Filter all_instances to only include those nodes which share this
6830         # storage path.
6831         # TODO(mikal): this should be further refactored so that the cache
6832         # cleanup code doesn't know what those instances are, just a remote
6833         # count, and then this logic should be pushed up the stack.
6834         filters = {'deleted': False,
6835                    'soft_deleted': True,
6836                    'host': nodes}
6837         filtered_instances = objects.InstanceList.get_by_filters(context,
6838                                  filters, expected_attrs=[], use_slave=True)
6839 
6840         self.driver.manage_image_cache(context, filtered_instances)
6841 
6842     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
6843     def _run_pending_deletes(self, context):
6844         """Retry any pending instance file deletes."""
6845         LOG.debug('Cleaning up deleted instances')
6846         filters = {'deleted': True,
6847                    'soft_deleted': False,
6848                    'host': CONF.host,
6849                    'cleaned': False}
6850         attrs = ['system_metadata']
6851         with utils.temporary_mutation(context, read_deleted='yes'):
6852             instances = objects.InstanceList.get_by_filters(
6853                 context, filters, expected_attrs=attrs, use_slave=True)
6854         LOG.debug('There are %d instances to clean', len(instances))
6855 
6856         # TODO(raj_singh): Remove this if condition when min value is
6857         # introduced to "maximum_instance_delete_attempts" cfg option.
6858         if CONF.maximum_instance_delete_attempts < 1:
6859             LOG.warning(_LW('Future versions of Nova will restrict the '
6860                             '"maximum_instance_delete_attempts" config option '
6861                             'to values >=1. Update your configuration file to '
6862                             'mitigate future upgrade issues.'))
6863 
6864         for instance in instances:
6865             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
6866             LOG.debug('Instance has had %(attempts)s of %(max)s '
6867                       'cleanup attempts',
6868                       {'attempts': attempts,
6869                        'max': CONF.maximum_instance_delete_attempts},
6870                       instance=instance)
6871             if attempts < CONF.maximum_instance_delete_attempts:
6872                 success = self.driver.delete_instance_files(instance)
6873 
6874                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
6875                 if success:
6876                     instance.cleaned = True
6877                 with utils.temporary_mutation(context, read_deleted='yes'):
6878                     instance.save()
6879 
6880     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
6881     def _cleanup_incomplete_migrations(self, context):
6882         """Delete instance files on failed resize/revert-resize operation
6883 
6884         During resize/revert-resize operation, if that instance gets deleted
6885         in-between then instance files might remain either on source or
6886         destination compute node because of race condition.
6887         """
6888         LOG.debug('Cleaning up deleted instances with incomplete migration ')
6889         migration_filters = {'host': CONF.host,
6890                              'status': 'error'}
6891         migrations = objects.MigrationList.get_by_filters(context,
6892                                                           migration_filters)
6893 
6894         if not migrations:
6895             return
6896 
6897         inst_uuid_from_migrations = set([migration.instance_uuid for migration
6898                                          in migrations])
6899 
6900         inst_filters = {'deleted': True, 'soft_deleted': False,
6901                         'uuid': inst_uuid_from_migrations}
6902         attrs = ['info_cache', 'security_groups', 'system_metadata']
6903         with utils.temporary_mutation(context, read_deleted='yes'):
6904             instances = objects.InstanceList.get_by_filters(
6905                 context, inst_filters, expected_attrs=attrs, use_slave=True)
6906 
6907         for instance in instances:
6908             if instance.host != CONF.host:
6909                 for migration in migrations:
6910                     if instance.uuid == migration.instance_uuid:
6911                         # Delete instance files if not cleanup properly either
6912                         # from the source or destination compute nodes when
6913                         # the instance is deleted during resizing.
6914                         self.driver.delete_instance_files(instance)
6915                         try:
6916                             migration.status = 'failed'
6917                             with migration.obj_as_admin():
6918                                 migration.save()
6919                         except exception.MigrationNotFound:
6920                             LOG.warning(_LW("Migration %s is not found."),
6921                                         migration.id,
6922                                         instance=instance)
6923                         break
6924 
6925     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
6926                                    exception.QemuGuestAgentNotEnabled,
6927                                    exception.NovaException,
6928                                    NotImplementedError)
6929     @wrap_exception()
6930     def quiesce_instance(self, context, instance):
6931         """Quiesce an instance on this host."""
6932         context = context.elevated()
6933         image_meta = objects.ImageMeta.from_instance(instance)
6934         self.driver.quiesce(context, instance, image_meta)
6935 
6936     def _wait_for_snapshots_completion(self, context, mapping):
6937         for mapping_dict in mapping:
6938             if mapping_dict.get('source_type') == 'snapshot':
6939 
6940                 def _wait_snapshot():
6941                     snapshot = self.volume_api.get_snapshot(
6942                         context, mapping_dict['snapshot_id'])
6943                     if snapshot.get('status') != 'creating':
6944                         raise loopingcall.LoopingCallDone()
6945 
6946                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
6947                 timer.start(interval=0.5).wait()
6948 
6949     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
6950                                    exception.QemuGuestAgentNotEnabled,
6951                                    exception.NovaException,
6952                                    NotImplementedError)
6953     @wrap_exception()
6954     def unquiesce_instance(self, context, instance, mapping=None):
6955         """Unquiesce an instance on this host.
6956 
6957         If snapshots' image mapping is provided, it waits until snapshots are
6958         completed before unqueiscing.
6959         """
6960         context = context.elevated()
6961         if mapping:
6962             try:
6963                 self._wait_for_snapshots_completion(context, mapping)
6964             except Exception as error:
6965                 LOG.exception(_LE("Exception while waiting completion of "
6966                                   "volume snapshots: %s"),
6967                               error, instance=instance)
6968         image_meta = objects.ImageMeta.from_instance(instance)
6969         self.driver.unquiesce(context, instance, image_meta)
