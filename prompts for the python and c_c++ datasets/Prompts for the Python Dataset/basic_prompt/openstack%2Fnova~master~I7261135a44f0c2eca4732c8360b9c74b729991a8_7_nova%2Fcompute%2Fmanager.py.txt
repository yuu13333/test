Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import functools
32 import inspect
33 import sys
34 import time
35 import traceback
36 
37 from cinderclient import exceptions as cinder_exception
38 from cursive import exception as cursive_exception
39 import eventlet.event
40 from eventlet import greenthread
41 import eventlet.semaphore
42 import eventlet.timeout
43 import futurist
44 from keystoneauth1 import exceptions as keystone_exception
45 from oslo_log import log as logging
46 import oslo_messaging as messaging
47 from oslo_serialization import jsonutils
48 from oslo_service import loopingcall
49 from oslo_service import periodic_task
50 from oslo_utils import excutils
51 from oslo_utils import strutils
52 from oslo_utils import timeutils
53 import six
54 from six.moves import range
55 
56 from nova import block_device
57 from nova.cells import rpcapi as cells_rpcapi
58 from nova import compute
59 from nova.compute import build_results
60 from nova.compute import claims
61 from nova.compute import power_state
62 from nova.compute import resource_tracker
63 from nova.compute import rpcapi as compute_rpcapi
64 from nova.compute import task_states
65 from nova.compute import utils as compute_utils
66 from nova.compute.utils import wrap_instance_event
67 from nova.compute import vm_states
68 from nova import conductor
69 import nova.conf
70 from nova.console import rpcapi as console_rpcapi
71 import nova.context
72 from nova import exception
73 from nova import exception_wrapper
74 from nova import hooks
75 from nova.i18n import _
76 from nova import image
77 from nova import manager
78 from nova import network
79 from nova.network import base_api as base_net_api
80 from nova.network import model as network_model
81 from nova.network.security_group import openstack_driver
82 from nova import objects
83 from nova.objects import base as obj_base
84 from nova.objects import fields
85 from nova.objects import instance as obj_instance
86 from nova.objects import migrate_data as migrate_data_obj
87 from nova.pci import whitelist
88 from nova import rpc
89 from nova import safe_utils
90 from nova.scheduler.client import query
91 from nova import utils
92 from nova.virt import block_device as driver_block_device
93 from nova.virt import configdrive
94 from nova.virt import driver
95 from nova.virt import event as virtevent
96 from nova.virt import storage_users
97 from nova.virt import virtapi
98 from nova.volume import cinder
99 
100 CONF = nova.conf.CONF
101 
102 LOG = logging.getLogger(__name__)
103 
104 get_notifier = functools.partial(rpc.get_notifier, service='compute')
105 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
106                                    get_notifier=get_notifier,
107                                    binary='nova-compute')
108 
109 
110 @contextlib.contextmanager
111 def errors_out_migration_ctxt(migration):
112     """Context manager to error out migration on failure."""
113 
114     try:
115         yield
116     except Exception:
117         with excutils.save_and_reraise_exception():
118             if migration:
119                 # We may have been passed None for our migration if we're
120                 # receiving from an older client. The migration will be
121                 # errored via the legacy path.
122                 migration.status = 'error'
123                 try:
124                     with migration.obj_as_admin():
125                         migration.save()
126                 except Exception:
127                     LOG.debug(
128                         'Error setting migration status for instance %s.',
129                         migration.instance_uuid, exc_info=True)
130 
131 
132 @utils.expects_func_args('migration')
133 def errors_out_migration(function):
134     """Decorator to error out migration on failure."""
135 
136     @functools.wraps(function)
137     def decorated_function(self, context, *args, **kwargs):
138         wrapped_func = safe_utils.get_wrapped_function(function)
139         keyed_args = inspect.getcallargs(wrapped_func, self, context,
140                                          *args, **kwargs)
141         migration = keyed_args['migration']
142         with errors_out_migration_ctxt(migration):
143             return function(self, context, *args, **kwargs)
144 
145     return decorated_function
146 
147 
148 @utils.expects_func_args('instance')
149 def reverts_task_state(function):
150     """Decorator to revert task_state on failure."""
151 
152     @functools.wraps(function)
153     def decorated_function(self, context, *args, **kwargs):
154         try:
155             return function(self, context, *args, **kwargs)
156         except exception.UnexpectedTaskStateError as e:
157             # Note(maoy): unexpected task state means the current
158             # task is preempted. Do not clear task state in this
159             # case.
160             with excutils.save_and_reraise_exception():
161                 LOG.info("Task possibly preempted: %s",
162                          e.format_message())
163         except Exception:
164             with excutils.save_and_reraise_exception():
165                 wrapped_func = safe_utils.get_wrapped_function(function)
166                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
167                                                  *args, **kwargs)
168                 # NOTE(mriedem): 'instance' must be in keyed_args because we
169                 # have utils.expects_func_args('instance') decorating this
170                 # method.
171                 instance = keyed_args['instance']
172                 original_task_state = instance.task_state
173                 try:
174                     self._instance_update(context, instance, task_state=None)
175                     LOG.info("Successfully reverted task state from %s on "
176                              "failure for instance.",
177                              original_task_state, instance=instance)
178                 except exception.InstanceNotFound:
179                     # We might delete an instance that failed to build shortly
180                     # after it errored out this is an expected case and we
181                     # should not trace on it.
182                     pass
183                 except Exception as e:
184                     LOG.warning("Failed to revert task state for instance. "
185                                 "Error: %s", e, instance=instance)
186 
187     return decorated_function
188 
189 
190 @utils.expects_func_args('instance')
191 def wrap_instance_fault(function):
192     """Wraps a method to catch exceptions related to instances.
193 
194     This decorator wraps a method to catch any exceptions having to do with
195     an instance that may get thrown. It then logs an instance fault in the db.
196     """
197 
198     @functools.wraps(function)
199     def decorated_function(self, context, *args, **kwargs):
200         try:
201             return function(self, context, *args, **kwargs)
202         except exception.InstanceNotFound:
203             raise
204         except Exception as e:
205             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
206             # we will get a KeyError exception which will cover up the real
207             # exception. So, we update kwargs with the values from args first.
208             # then, we can get 'instance' from kwargs easily.
209             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
210 
211             with excutils.save_and_reraise_exception():
212                 compute_utils.add_instance_fault_from_exc(context,
213                         kwargs['instance'], e, sys.exc_info())
214 
215     return decorated_function
216 
217 
218 @utils.expects_func_args('image_id', 'instance')
219 def delete_image_on_error(function):
220     """Used for snapshot related method to ensure the image created in
221     compute.api is deleted when an error occurs.
222     """
223 
224     @functools.wraps(function)
225     def decorated_function(self, context, image_id, instance,
226                            *args, **kwargs):
227         try:
228             return function(self, context, image_id, instance,
229                             *args, **kwargs)
230         except Exception:
231             with excutils.save_and_reraise_exception():
232                 LOG.debug("Cleaning up image %s", image_id,
233                           exc_info=True, instance=instance)
234                 try:
235                     self.image_api.delete(context, image_id)
236                 except exception.ImageNotFound:
237                     # Since we're trying to cleanup an image, we don't care if
238                     # if it's already gone.
239                     pass
240                 except Exception:
241                     LOG.exception("Error while trying to clean up image %s",
242                                   image_id, instance=instance)
243 
244     return decorated_function
245 
246 
247 # TODO(danms): Remove me after Icehouse
248 # TODO(alaski): Actually remove this after Newton, assuming a major RPC bump
249 # NOTE(mikal): if the method being decorated has more than one decorator, then
250 # put this one first. Otherwise the various exception handling decorators do
251 # not function correctly.
252 def object_compat(function):
253     """Wraps a method that expects a new-world instance
254 
255     This provides compatibility for callers passing old-style dict
256     instances.
257     """
258 
259     @functools.wraps(function)
260     def decorated_function(self, context, *args, **kwargs):
261         def _load_instance(instance_or_dict):
262             if isinstance(instance_or_dict, dict):
263                 # try to get metadata and system_metadata for most cases but
264                 # only attempt to load those if the db instance already has
265                 # those fields joined
266                 metas = [meta for meta in ('metadata', 'system_metadata')
267                          if meta in instance_or_dict]
268                 instance = objects.Instance._from_db_object(
269                     context, objects.Instance(), instance_or_dict,
270                     expected_attrs=metas)
271                 instance._context = context
272                 return instance
273             return instance_or_dict
274 
275         try:
276             kwargs['instance'] = _load_instance(kwargs['instance'])
277         except KeyError:
278             args = (_load_instance(args[0]),) + args[1:]
279 
280         migration = kwargs.get('migration')
281         if isinstance(migration, dict):
282             migration = objects.Migration._from_db_object(
283                     context.elevated(), objects.Migration(),
284                     migration)
285             kwargs['migration'] = migration
286 
287         return function(self, context, *args, **kwargs)
288 
289     return decorated_function
290 
291 
292 class InstanceEvents(object):
293     def __init__(self):
294         self._events = {}
295 
296     @staticmethod
297     def _lock_name(instance):
298         return '%s-%s' % (instance.uuid, 'events')
299 
300     def prepare_for_instance_event(self, instance, name, tag):
301         """Prepare to receive an event for an instance.
302 
303         This will register an event for the given instance that we will
304         wait on later. This should be called before initiating whatever
305         action will trigger the event. The resulting eventlet.event.Event
306         object should be wait()'d on to ensure completion.
307 
308         :param instance: the instance for which the event will be generated
309         :param name: the name of the event we're expecting
310         :param tag: the tag associated with the event we're expecting
311         :returns: an event object that should be wait()'d on
312         """
313         if self._events is None:
314             # NOTE(danms): We really should have a more specific error
315             # here, but this is what we use for our default error case
316             raise exception.NovaException('In shutdown, no new events '
317                                           'can be scheduled')
318 
319         @utils.synchronized(self._lock_name(instance))
320         def _create_or_get_event():
321             instance_events = self._events.setdefault(instance.uuid, {})
322             return instance_events.setdefault((name, tag),
323                                               eventlet.event.Event())
324         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
325                   {'name': name, 'tag': tag}, instance=instance)
326         return _create_or_get_event()
327 
328     def pop_instance_event(self, instance, event):
329         """Remove a pending event from the wait list.
330 
331         This will remove a pending event from the wait list so that it
332         can be used to signal the waiters to wake up.
333 
334         :param instance: the instance for which the event was generated
335         :param event: the nova.objects.external_event.InstanceExternalEvent
336                       that describes the event
337         :returns: the eventlet.event.Event object on which the waiters
338                   are blocked
339         """
340         no_events_sentinel = object()
341         no_matching_event_sentinel = object()
342 
343         @utils.synchronized(self._lock_name(instance))
344         def _pop_event():
345             if self._events is None:
346                 LOG.debug('Unexpected attempt to pop events during shutdown',
347                           instance=instance)
348                 return no_events_sentinel
349             events = self._events.get(instance.uuid)
350             if not events:
351                 return no_events_sentinel
352             _event = events.pop((event.name, event.tag), None)
353             if not events:
354                 del self._events[instance.uuid]
355             if _event is None:
356                 return no_matching_event_sentinel
357             return _event
358 
359         result = _pop_event()
360         if result is no_events_sentinel:
361             LOG.debug('No waiting events found dispatching %(event)s',
362                       {'event': event.key},
363                       instance=instance)
364             return None
365         elif result is no_matching_event_sentinel:
366             LOG.debug('No event matching %(event)s in %(events)s',
367                       {'event': event.key,
368                        'events': self._events.get(instance.uuid, {}).keys()},
369                       instance=instance)
370             return None
371         else:
372             return result
373 
374     def clear_events_for_instance(self, instance):
375         """Remove all pending events for an instance.
376 
377         This will remove all events currently pending for an instance
378         and return them (indexed by event name).
379 
380         :param instance: the instance for which events should be purged
381         :returns: a dictionary of {event_name: eventlet.event.Event}
382         """
383         @utils.synchronized(self._lock_name(instance))
384         def _clear_events():
385             if self._events is None:
386                 LOG.debug('Unexpected attempt to clear events during shutdown',
387                           instance=instance)
388                 return dict()
389             # NOTE(danms): We have historically returned the raw internal
390             # format here, which is {event.key: [events, ...])} so just
391             # trivially convert it here.
392             return {'%s-%s' % k: e
393                     for k, e in self._events.pop(instance.uuid, {}).items()}
394         return _clear_events()
395 
396     def cancel_all_events(self):
397         if self._events is None:
398             LOG.debug('Unexpected attempt to cancel events during shutdown.')
399             return
400         our_events = self._events
401         # NOTE(danms): Block new events
402         self._events = None
403 
404         for instance_uuid, events in our_events.items():
405             for (name, tag), eventlet_event in events.items():
406                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
407                           'instance %(instance_uuid)s',
408                           {'name': name,
409                            'tag': tag,
410                            'instance_uuid': instance_uuid})
411                 event = objects.InstanceExternalEvent(
412                     instance_uuid=instance_uuid,
413                     name=name, status='failed',
414                     tag=tag, data={})
415                 eventlet_event.send(event)
416 
417 
418 class ComputeVirtAPI(virtapi.VirtAPI):
419     def __init__(self, compute):
420         super(ComputeVirtAPI, self).__init__()
421         self._compute = compute
422 
423     def _default_error_callback(self, event_name, instance):
424         raise exception.NovaException(_('Instance event failed'))
425 
426     @contextlib.contextmanager
427     def wait_for_instance_event(self, instance, event_names, deadline=300,
428                                 error_callback=None):
429         """Plan to wait for some events, run some code, then wait.
430 
431         This context manager will first create plans to wait for the
432         provided event_names, yield, and then wait for all the scheduled
433         events to complete.
434 
435         Note that this uses an eventlet.timeout.Timeout to bound the
436         operation, so callers should be prepared to catch that
437         failure and handle that situation appropriately.
438 
439         If the event is not received by the specified timeout deadline,
440         eventlet.timeout.Timeout is raised.
441 
442         If the event is received but did not have a 'completed'
443         status, a NovaException is raised.  If an error_callback is
444         provided, instead of raising an exception as detailed above
445         for the failure case, the callback will be called with the
446         event_name and instance, and can return True to continue
447         waiting for the rest of the events, False to stop processing,
448         or raise an exception which will bubble up to the waiter.
449 
450         :param instance: The instance for which an event is expected
451         :param event_names: A list of event names. Each element is a
452                             tuple of strings to indicate (name, tag),
453                             where name is required, but tag may be None.
454         :param deadline: Maximum number of seconds we should wait for all
455                          of the specified events to arrive.
456         :param error_callback: A function to be called if an event arrives
457 
458         """
459 
460         if error_callback is None:
461             error_callback = self._default_error_callback
462         events = {}
463         for event_name in event_names:
464             name, tag = event_name
465             event_name = objects.InstanceExternalEvent.make_key(name, tag)
466             try:
467                 events[event_name] = (
468                     self._compute.instance_events.prepare_for_instance_event(
469                         instance, name, tag))
470             except exception.NovaException:
471                 error_callback(event_name, instance)
472                 # NOTE(danms): Don't wait for any of the events. They
473                 # should all be canceled and fired immediately below,
474                 # but don't stick around if not.
475                 deadline = 0
476         yield
477         with eventlet.timeout.Timeout(deadline):
478             for event_name, event in events.items():
479                 actual_event = event.wait()
480                 if actual_event.status == 'completed':
481                     continue
482                 decision = error_callback(event_name, instance)
483                 if decision is False:
484                     break
485 
486 
487 class ComputeManager(manager.Manager):
488     """Manages the running instances from creation to destruction."""
489 
490     target = messaging.Target(version='5.1')
491 
492     def __init__(self, compute_driver=None, *args, **kwargs):
493         """Load configuration options and connect to the hypervisor."""
494         self.virtapi = ComputeVirtAPI(self)
495         self.network_api = network.API()
496         self.volume_api = cinder.API()
497         self.image_api = image.API()
498         self._last_host_check = 0
499         self._last_bw_usage_poll = 0
500         self._bw_usage_supported = True
501         self._last_bw_usage_cell_update = 0
502         self.compute_api = compute.API()
503         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
504         self.conductor_api = conductor.API()
505         self.compute_task_api = conductor.ComputeTaskAPI()
506         self.is_neutron_security_groups = (
507             openstack_driver.is_neutron_security_groups())
508         self.cells_rpcapi = cells_rpcapi.CellsAPI()
509         self.query_client = query.SchedulerQueryClient()
510         self.instance_events = InstanceEvents()
511         self._sync_power_pool = eventlet.GreenPool(
512             size=CONF.sync_power_state_pool_size)
513         self._syncs_in_progress = {}
514         self.send_instance_updates = (
515             CONF.filter_scheduler.track_instance_changes)
516         if CONF.max_concurrent_builds != 0:
517             self._build_semaphore = eventlet.semaphore.Semaphore(
518                 CONF.max_concurrent_builds)
519         else:
520             self._build_semaphore = compute_utils.UnlimitedSemaphore()
521         if max(CONF.max_concurrent_live_migrations, 0) != 0:
522             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
523                 max_workers=CONF.max_concurrent_live_migrations)
524         else:
525             if CONF.max_concurrent_live_migrations < 0:
526                 LOG.warning('The value of the max_concurrent_live_migrations '
527                             'config option is less than 0. '
528                             'It is treated as 0 and will raise ValueError '
529                             'in a future release.')
530             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
531         # This is a dict, keyed by instance uuid, to a two-item tuple of
532         # migration object and Future for the queued live migration.
533         self._waiting_live_migrations = {}
534 
535         super(ComputeManager, self).__init__(service_name="compute",
536                                              *args, **kwargs)
537 
538         # NOTE(russellb) Load the driver last.  It may call back into the
539         # compute manager via the virtapi, so we want it to be fully
540         # initialized before that happens.
541         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
542         self.use_legacy_block_device_info = \
543                             self.driver.need_legacy_block_device_info
544         self.rt = resource_tracker.ResourceTracker(self.host, self.driver)
545         self.reportclient = self.rt.reportclient
546 
547     def reset(self):
548         LOG.info('Reloading compute RPC API')
549         compute_rpcapi.LAST_VERSION = None
550         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
551         self.reportclient.clear_provider_cache()
552 
553     def _update_resource_tracker(self, context, instance):
554         """Let the resource tracker know that an instance has changed state."""
555 
556         if instance.host == self.host:
557             self.rt.update_usage(context, instance, instance.node)
558 
559     def _instance_update(self, context, instance, **kwargs):
560         """Update an instance in the database using kwargs as value."""
561 
562         for k, v in kwargs.items():
563             setattr(instance, k, v)
564         instance.save()
565         self._update_resource_tracker(context, instance)
566 
567     def _nil_out_instance_obj_host_and_node(self, instance):
568         # NOTE(jwcroppe): We don't do instance.save() here for performance
569         # reasons; a call to this is expected to be immediately followed by
570         # another call that does instance.save(), thus avoiding two writes
571         # to the database layer.
572         instance.host = None
573         instance.node = None
574         # If the instance is not on a host, it's not in an aggregate and
575         # therefore is not in an availability zone.
576         instance.availability_zone = None
577 
578     def _set_instance_obj_error_state(self, context, instance,
579                                       clean_task_state=False):
580         try:
581             instance.vm_state = vm_states.ERROR
582             if clean_task_state:
583                 instance.task_state = None
584             instance.save()
585         except exception.InstanceNotFound:
586             LOG.debug('Instance has been destroyed from under us while '
587                       'trying to set it to ERROR', instance=instance)
588 
589     def _get_instances_on_driver(self, context, filters=None):
590         """Return a list of instance records for the instances found
591         on the hypervisor which satisfy the specified filters. If filters=None
592         return a list of instance records for all the instances found on the
593         hypervisor.
594         """
595         if not filters:
596             filters = {}
597         try:
598             driver_uuids = self.driver.list_instance_uuids()
599             if len(driver_uuids) == 0:
600                 # Short circuit, don't waste a DB call
601                 return objects.InstanceList()
602             filters['uuid'] = driver_uuids
603             local_instances = objects.InstanceList.get_by_filters(
604                 context, filters, use_slave=True)
605             return local_instances
606         except NotImplementedError:
607             pass
608 
609         # The driver doesn't support uuids listing, so we'll have
610         # to brute force.
611         driver_instances = self.driver.list_instances()
612         # NOTE(mjozefcz): In this case we need to apply host filter.
613         # Without this all instance data would be fetched from db.
614         filters['host'] = self.host
615         instances = objects.InstanceList.get_by_filters(context, filters,
616                                                         use_slave=True)
617         name_map = {instance.name: instance for instance in instances}
618         local_instances = []
619         for driver_instance in driver_instances:
620             instance = name_map.get(driver_instance)
621             if not instance:
622                 continue
623             local_instances.append(instance)
624         return local_instances
625 
626     def _destroy_evacuated_instances(self, context):
627         """Destroys evacuated instances.
628 
629         While nova-compute was down, the instances running on it could be
630         evacuated to another host. This method looks for evacuation migration
631         records where this is the source host and which were either started
632         (accepted), in-progress (pre-migrating) or migrated (done). From those
633         migration records, local instances reported by the hypervisor are
634         compared to the instances for the migration records and those local
635         guests are destroyed, along with instance allocation records in
636         Placement for this node.
637         """
638         filters = {
639             'source_compute': self.host,
640             # NOTE(mriedem): Migration records that have been accepted are
641             # included in case the source node comes back up while instances
642             # are being evacuated to another host. We don't want the same
643             # instance being reported from multiple hosts.
644             # NOTE(lyarwood): pre-migrating is also included here as the
645             # source compute can come back online shortly after the RT
646             # claims on the destination that in-turn moves the migration to
647             # pre-migrating. If the evacuate fails on the destination host,
648             # the user can rebuild the instance (in ERROR state) on the source
649             # host.
650             'status': ['accepted', 'pre-migrating', 'done'],
651             'migration_type': 'evacuation',
652         }
653         with utils.temporary_mutation(context, read_deleted='yes'):
654             evacuations = objects.MigrationList.get_by_filters(context,
655                                                                filters)
656         if not evacuations:
657             return
658         evacuations = {mig.instance_uuid: mig for mig in evacuations}
659 
660         # TODO(mriedem): We could optimize by pre-loading the joined fields
661         # we know we'll use, like info_cache and flavor.
662         local_instances = self._get_instances_on_driver(context)
663         evacuated = [inst for inst in local_instances
664                      if inst.uuid in evacuations]
665 
666         # NOTE(gibi): We are called from init_host and at this point the
667         # compute_nodes of the resource tracker has not been populated yet so
668         # we cannot rely on the resource tracker here.
669         compute_nodes = {}
670 
671         for instance in evacuated:
672             migration = evacuations[instance.uuid]
673             LOG.info('Deleting instance as it has been evacuated from '
674                      'this host', instance=instance)
675             try:
676                 network_info = self.network_api.get_instance_nw_info(
677                     context, instance)
678                 bdi = self._get_instance_block_device_info(context,
679                                                            instance)
680                 destroy_disks = not (self._is_instance_storage_shared(
681                     context, instance))
682             except exception.InstanceNotFound:
683                 network_info = network_model.NetworkInfo()
684                 bdi = {}
685                 LOG.info('Instance has been marked deleted already, '
686                          'removing it from the hypervisor.',
687                          instance=instance)
688                 # always destroy disks if the instance was deleted
689                 destroy_disks = True
690             self.driver.destroy(context, instance,
691                                 network_info,
692                                 bdi, destroy_disks)
693 
694             # delete the allocation of the evacuated instance from this host
695             if migration.source_node not in compute_nodes:
696                 try:
697                     cn_uuid = objects.ComputeNode.get_by_host_and_nodename(
698                         context, self.host, migration.source_node).uuid
699                     compute_nodes[migration.source_node] = cn_uuid
700                 except exception.ComputeHostNotFound:
701                     LOG.error("Failed to clean allocation of evacuated "
702                               "instance as the source node %s is not found",
703                               migration.source_node, instance=instance)
704                     continue
705             cn_uuid = compute_nodes[migration.source_node]
706 
707             # If the instance was deleted in the interim, assume its
708             # allocations were properly cleaned up (either by its hosting
709             # compute service or the API).
710             if (not instance.deleted and
711                     not self.reportclient.
712                         remove_provider_tree_from_instance_allocation(
713                             context, instance.uuid, cn_uuid)):
714                 LOG.error("Failed to clean allocation of evacuated instance "
715                           "on the source node %s",
716                           cn_uuid, instance=instance)
717 
718             migration.status = 'completed'
719             migration.save()
720         return evacuations
721 
722     def _is_instance_storage_shared(self, context, instance, host=None):
723         shared_storage = True
724         data = None
725         try:
726             data = self.driver.check_instance_shared_storage_local(context,
727                                                        instance)
728             if data:
729                 shared_storage = (self.compute_rpcapi.
730                                   check_instance_shared_storage(context,
731                                   instance, data, host=host))
732         except NotImplementedError:
733             LOG.debug('Hypervisor driver does not support '
734                       'instance shared storage check, '
735                       'assuming it\'s not on shared storage',
736                       instance=instance)
737             shared_storage = False
738         except Exception:
739             LOG.exception('Failed to check if instance shared',
740                           instance=instance)
741         finally:
742             if data:
743                 self.driver.check_instance_shared_storage_cleanup(context,
744                                                                   data)
745         return shared_storage
746 
747     def _complete_partial_deletion(self, context, instance):
748         """Complete deletion for instances in DELETED status but not marked as
749         deleted in the DB
750         """
751         instance.destroy()
752         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
753                 context, instance.uuid)
754         self._complete_deletion(context,
755                                 instance)
756         self._notify_about_instance_usage(context, instance, "delete.end")
757         compute_utils.notify_about_instance_action(context, instance,
758                 self.host, action=fields.NotificationAction.DELETE,
759                 phase=fields.NotificationPhase.END, bdms=bdms)
760 
761     def _complete_deletion(self, context, instance):
762         self._update_resource_tracker(context, instance)
763 
764         self.reportclient.delete_allocation_for_instance(context,
765                                                          instance.uuid)
766 
767         self._clean_instance_console_tokens(context, instance)
768         self._delete_scheduler_instance_info(context, instance.uuid)
769 
770     def _init_instance(self, context, instance):
771         """Initialize this instance during service init."""
772 
773         # NOTE(danms): If the instance appears to not be owned by this
774         # host, it may have been evacuated away, but skipped by the
775         # evacuation cleanup code due to configuration. Thus, if that
776         # is a possibility, don't touch the instance in any way, but
777         # log the concern. This will help avoid potential issues on
778         # startup due to misconfiguration.
779         if instance.host != self.host:
780             LOG.warning('Instance %(uuid)s appears to not be owned '
781                         'by this host, but by %(host)s. Startup '
782                         'processing is being skipped.',
783                         {'uuid': instance.uuid,
784                          'host': instance.host})
785             return
786 
787         # Instances that are shut down, or in an error state can not be
788         # initialized and are not attempted to be recovered. The exception
789         # to this are instances that are in RESIZE_MIGRATING or DELETING,
790         # which are dealt with further down.
791         if (instance.vm_state == vm_states.SOFT_DELETED or
792             (instance.vm_state == vm_states.ERROR and
793             instance.task_state not in
794             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
795             LOG.debug("Instance is in %s state.",
796                       instance.vm_state, instance=instance)
797             return
798 
799         if instance.vm_state == vm_states.DELETED:
800             try:
801                 self._complete_partial_deletion(context, instance)
802             except Exception:
803                 # we don't want that an exception blocks the init_host
804                 LOG.exception('Failed to complete a deletion',
805                               instance=instance)
806             return
807 
808         if (instance.vm_state == vm_states.BUILDING or
809             instance.task_state in [task_states.SCHEDULING,
810                                     task_states.BLOCK_DEVICE_MAPPING,
811                                     task_states.NETWORKING,
812                                     task_states.SPAWNING]):
813             # NOTE(dave-mcnally) compute stopped before instance was fully
814             # spawned so set to ERROR state. This is safe to do as the state
815             # may be set by the api but the host is not so if we get here the
816             # instance has already been scheduled to this particular host.
817             LOG.debug("Instance failed to spawn correctly, "
818                       "setting to ERROR state", instance=instance)
819             instance.task_state = None
820             instance.vm_state = vm_states.ERROR
821             instance.save()
822             return
823 
824         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
825             instance.task_state in [task_states.REBUILDING,
826                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
827                                     task_states.REBUILD_SPAWNING]):
828             # NOTE(jichenjc) compute stopped before instance was fully
829             # spawned so set to ERROR state. This is consistent to BUILD
830             LOG.debug("Instance failed to rebuild correctly, "
831                       "setting to ERROR state", instance=instance)
832             instance.task_state = None
833             instance.vm_state = vm_states.ERROR
834             instance.save()
835             return
836 
837         if (instance.vm_state != vm_states.ERROR and
838             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
839                                     task_states.IMAGE_PENDING_UPLOAD,
840                                     task_states.IMAGE_UPLOADING,
841                                     task_states.IMAGE_SNAPSHOT]):
842             LOG.debug("Instance in transitional state %s at start-up "
843                       "clearing task state",
844                       instance.task_state, instance=instance)
845             try:
846                 self._post_interrupted_snapshot_cleanup(context, instance)
847             except Exception:
848                 # we don't want that an exception blocks the init_host
849                 LOG.exception('Failed to cleanup snapshot.', instance=instance)
850             instance.task_state = None
851             instance.save()
852 
853         if (instance.vm_state != vm_states.ERROR and
854             instance.task_state in [task_states.RESIZE_PREP]):
855             LOG.debug("Instance in transitional state %s at start-up "
856                       "clearing task state",
857                       instance['task_state'], instance=instance)
858             instance.task_state = None
859             instance.save()
860 
861         if instance.task_state == task_states.DELETING:
862             try:
863                 LOG.info('Service started deleting the instance during '
864                          'the previous run, but did not finish. Restarting'
865                          ' the deletion now.', instance=instance)
866                 instance.obj_load_attr('metadata')
867                 instance.obj_load_attr('system_metadata')
868                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
869                         context, instance.uuid)
870                 self._delete_instance(context, instance, bdms)
871             except Exception:
872                 # we don't want that an exception blocks the init_host
873                 LOG.exception('Failed to complete a deletion',
874                               instance=instance)
875                 self._set_instance_obj_error_state(context, instance)
876             return
877 
878         current_power_state = self._get_power_state(context, instance)
879         try_reboot, reboot_type = self._retry_reboot(context, instance,
880                                                      current_power_state)
881 
882         if try_reboot:
883             LOG.debug("Instance in transitional state (%(task_state)s) at "
884                       "start-up and power state is (%(power_state)s), "
885                       "triggering reboot",
886                       {'task_state': instance.task_state,
887                        'power_state': current_power_state},
888                       instance=instance)
889 
890             # NOTE(mikal): if the instance was doing a soft reboot that got as
891             # far as shutting down the instance but not as far as starting it
892             # again, then we've just become a hard reboot. That means the
893             # task state for the instance needs to change so that we're in one
894             # of the expected task states for a hard reboot.
895             if (instance.task_state in task_states.soft_reboot_states and
896                 reboot_type == 'HARD'):
897                 instance.task_state = task_states.REBOOT_PENDING_HARD
898                 instance.save()
899 
900             self.reboot_instance(context, instance, block_device_info=None,
901                                  reboot_type=reboot_type)
902             return
903 
904         elif (current_power_state == power_state.RUNNING and
905               instance.task_state in [task_states.REBOOT_STARTED,
906                                       task_states.REBOOT_STARTED_HARD,
907                                       task_states.PAUSING,
908                                       task_states.UNPAUSING]):
909             LOG.warning("Instance in transitional state "
910                         "(%(task_state)s) at start-up and power state "
911                         "is (%(power_state)s), clearing task state",
912                         {'task_state': instance.task_state,
913                          'power_state': current_power_state},
914                         instance=instance)
915             instance.task_state = None
916             instance.vm_state = vm_states.ACTIVE
917             instance.save()
918         elif (current_power_state == power_state.PAUSED and
919               instance.task_state == task_states.UNPAUSING):
920             LOG.warning("Instance in transitional state "
921                         "(%(task_state)s) at start-up and power state "
922                         "is (%(power_state)s), clearing task state "
923                         "and unpausing the instance",
924                         {'task_state': instance.task_state,
925                          'power_state': current_power_state},
926                         instance=instance)
927             try:
928                 self.unpause_instance(context, instance)
929             except NotImplementedError:
930                 # Some virt driver didn't support pause and unpause
931                 pass
932             except Exception:
933                 LOG.exception('Failed to unpause instance', instance=instance)
934             return
935 
936         if instance.task_state == task_states.POWERING_OFF:
937             try:
938                 LOG.debug("Instance in transitional state %s at start-up "
939                           "retrying stop request",
940                           instance.task_state, instance=instance)
941                 self.stop_instance(context, instance, True)
942             except Exception:
943                 # we don't want that an exception blocks the init_host
944                 LOG.exception('Failed to stop instance', instance=instance)
945             return
946 
947         if instance.task_state == task_states.POWERING_ON:
948             try:
949                 LOG.debug("Instance in transitional state %s at start-up "
950                           "retrying start request",
951                           instance.task_state, instance=instance)
952                 self.start_instance(context, instance)
953             except Exception:
954                 # we don't want that an exception blocks the init_host
955                 LOG.exception('Failed to start instance', instance=instance)
956             return
957 
958         net_info = instance.get_network_info()
959         try:
960             self.driver.plug_vifs(instance, net_info)
961         except NotImplementedError as e:
962             LOG.debug(e, instance=instance)
963         except exception.VirtualInterfacePlugException:
964             # NOTE(mriedem): If we get here, it could be because the vif_type
965             # in the cache is "binding_failed" or "unbound".  The only way to
966             # fix this is to try and bind the ports again, which would be
967             # expensive here on host startup. We could add a check to
968             # _heal_instance_info_cache to handle this, but probably only if
969             # the instance task_state is None.
970             LOG.exception('Virtual interface plugging failed for instance. '
971                           'The port binding:host_id may need to be manually '
972                           'updated.', instance=instance)
973             self._set_instance_obj_error_state(context, instance)
974             return
975 
976         if instance.task_state == task_states.RESIZE_MIGRATING:
977             # We crashed during resize/migration, so roll back for safety
978             try:
979                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
980                 # not in system_metadata we default to True for backwards
981                 # compatibility
982                 power_on = (instance.system_metadata.get('old_vm_state') !=
983                             vm_states.STOPPED)
984 
985                 block_dev_info = self._get_instance_block_device_info(context,
986                                                                       instance)
987 
988                 self.driver.finish_revert_migration(context,
989                     instance, net_info, block_dev_info, power_on)
990 
991             except Exception:
992                 LOG.exception('Failed to revert crashed migration',
993                               instance=instance)
994             finally:
995                 LOG.info('Instance found in migrating state during '
996                          'startup. Resetting task_state',
997                          instance=instance)
998                 instance.task_state = None
999                 instance.save()
1000         if instance.task_state == task_states.MIGRATING:
1001             # Live migration did not complete, but instance is on this
1002             # host, so reset the state.
1003             instance.task_state = None
1004             instance.save(expected_task_state=[task_states.MIGRATING])
1005 
1006         db_state = instance.power_state
1007         drv_state = self._get_power_state(context, instance)
1008         expect_running = (db_state == power_state.RUNNING and
1009                           drv_state != db_state)
1010 
1011         LOG.debug('Current state is %(drv_state)s, state in DB is '
1012                   '%(db_state)s.',
1013                   {'drv_state': drv_state, 'db_state': db_state},
1014                   instance=instance)
1015 
1016         if expect_running and CONF.resume_guests_state_on_host_boot:
1017             self._resume_guests_state(context, instance, net_info)
1018         elif drv_state == power_state.RUNNING:
1019             # VMwareAPI drivers will raise an exception
1020             try:
1021                 self.driver.ensure_filtering_rules_for_instance(
1022                                        instance, net_info)
1023             except NotImplementedError:
1024                 LOG.debug('Hypervisor driver does not support '
1025                           'firewall rules', instance=instance)
1026 
1027     def _resume_guests_state(self, context, instance, net_info):
1028         LOG.info('Rebooting instance after nova-compute restart.',
1029                  instance=instance)
1030         block_device_info = \
1031             self._get_instance_block_device_info(context, instance)
1032 
1033         try:
1034             self.driver.resume_state_on_host_boot(
1035                 context, instance, net_info, block_device_info)
1036         except NotImplementedError:
1037             LOG.warning('Hypervisor driver does not support '
1038                         'resume guests', instance=instance)
1039         except Exception:
1040             # NOTE(vish): The instance failed to resume, so we set the
1041             #             instance to error and attempt to continue.
1042             LOG.warning('Failed to resume instance',
1043                         instance=instance)
1044             self._set_instance_obj_error_state(context, instance)
1045 
1046     def _retry_reboot(self, context, instance, current_power_state):
1047         current_task_state = instance.task_state
1048         retry_reboot = False
1049         reboot_type = compute_utils.get_reboot_type(current_task_state,
1050                                                     current_power_state)
1051 
1052         pending_soft = (current_task_state == task_states.REBOOT_PENDING and
1053                         instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1054         pending_hard = (current_task_state == task_states.REBOOT_PENDING_HARD
1055                         and instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1056         started_not_running = (current_task_state in
1057                                [task_states.REBOOT_STARTED,
1058                                 task_states.REBOOT_STARTED_HARD] and
1059                                current_power_state != power_state.RUNNING)
1060 
1061         if pending_soft or pending_hard or started_not_running:
1062             retry_reboot = True
1063 
1064         return retry_reboot, reboot_type
1065 
1066     def handle_lifecycle_event(self, event):
1067         LOG.info("VM %(state)s (Lifecycle Event)",
1068                  {'state': event.get_name()},
1069                  instance_uuid=event.get_instance_uuid())
1070         context = nova.context.get_admin_context(read_deleted='yes')
1071         vm_power_state = None
1072         event_transition = event.get_transition()
1073         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1074             vm_power_state = power_state.SHUTDOWN
1075         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1076             vm_power_state = power_state.RUNNING
1077         elif event_transition in (
1078                 virtevent.EVENT_LIFECYCLE_PAUSED,
1079                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1080                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1081             vm_power_state = power_state.PAUSED
1082         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1083             vm_power_state = power_state.RUNNING
1084         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1085             vm_power_state = power_state.SUSPENDED
1086         else:
1087             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1088 
1089         migrate_finish_statuses = {
1090             # This happens on the source node and indicates live migration
1091             # entered post-copy mode.
1092             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1093             # Suspended for offline migration.
1094             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1095         }
1096 
1097         expected_attrs = []
1098         if event_transition in migrate_finish_statuses:
1099             # Join on info_cache since that's needed in migrate_instance_start.
1100             expected_attrs.append('info_cache')
1101         instance = objects.Instance.get_by_uuid(context,
1102                                                 event.get_instance_uuid(),
1103                                                 expected_attrs=expected_attrs)
1104 
1105         # Note(lpetrut): The event may be delayed, thus not reflecting
1106         # the current instance power state. In that case, ignore the event.
1107         current_power_state = self._get_power_state(context, instance)
1108         if current_power_state == vm_power_state:
1109             LOG.debug('Synchronizing instance power state after lifecycle '
1110                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1111                       'current task_state: %(task_state)s, current DB '
1112                       'power_state: %(db_power_state)s, VM power_state: '
1113                       '%(vm_power_state)s',
1114                       {'event': event.get_name(),
1115                        'vm_state': instance.vm_state,
1116                        'task_state': instance.task_state,
1117                        'db_power_state': instance.power_state,
1118                        'vm_power_state': vm_power_state},
1119                       instance_uuid=instance.uuid)
1120             self._sync_instance_power_state(context,
1121                                             instance,
1122                                             vm_power_state)
1123 
1124         # The following checks are for live migration. We want to activate
1125         # the port binding for the destination host before the live migration
1126         # is resumed on the destination host in order to reduce network
1127         # downtime. Otherwise the ports are bound to the destination host
1128         # in post_live_migration_at_destination.
1129         # TODO(danms): Explore options for using a different live migration
1130         # specific callback for this instead of piggy-backing on the
1131         # handle_lifecycle_event callback.
1132         if (instance.task_state == task_states.MIGRATING and
1133                 event_transition in migrate_finish_statuses):
1134             status = migrate_finish_statuses[event_transition]
1135             try:
1136                 migration = objects.Migration.get_by_instance_and_status(
1137                             context, instance.uuid, status)
1138                 LOG.debug('Binding ports to destination host: %s',
1139                           migration.dest_compute, instance=instance)
1140                 # For neutron, migrate_instance_start will activate the
1141                 # destination host port bindings, if there are any created by
1142                 # conductor before live migration started.
1143                 self.network_api.migrate_instance_start(
1144                     context, instance, migration)
1145             except exception.MigrationNotFoundByStatus:
1146                 LOG.warning("Unable to find migration record with status "
1147                             "'%s' for instance. Port binding will happen in "
1148                             "post live migration.", status, instance=instance)
1149 
1150     def handle_events(self, event):
1151         if isinstance(event, virtevent.LifecycleEvent):
1152             try:
1153                 self.handle_lifecycle_event(event)
1154             except exception.InstanceNotFound:
1155                 LOG.debug("Event %s arrived for non-existent instance. The "
1156                           "instance was probably deleted.", event)
1157         else:
1158             LOG.debug("Ignoring event %s", event)
1159 
1160     def init_virt_events(self):
1161         if CONF.workarounds.handle_virt_lifecycle_events:
1162             self.driver.register_event_listener(self.handle_events)
1163         else:
1164             # NOTE(mriedem): If the _sync_power_states periodic task is
1165             # disabled we should emit a warning in the logs.
1166             if CONF.sync_power_state_interval < 0:
1167                 LOG.warning('Instance lifecycle events from the compute '
1168                             'driver have been disabled. Note that lifecycle '
1169                             'changes to an instance outside of the compute '
1170                             'service will not be synchronized '
1171                             'automatically since the _sync_power_states '
1172                             'periodic task is also disabled.')
1173             else:
1174                 LOG.info('Instance lifecycle events from the compute '
1175                          'driver have been disabled. Note that lifecycle '
1176                          'changes to an instance outside of the compute '
1177                          'service will only be synchronized by the '
1178                          '_sync_power_states periodic task.')
1179 
1180     def init_host(self):
1181         """Initialization for a standalone compute service."""
1182 
1183         if CONF.pci.passthrough_whitelist:
1184             # Simply loading the PCI passthrough whitelist will do a bunch of
1185             # validation that would otherwise wait until the PciDevTracker is
1186             # constructed when updating available resources for the compute
1187             # node(s) in the resource tracker, effectively killing that task.
1188             # So load up the whitelist when starting the compute service to
1189             # flush any invalid configuration early so we can kill the service
1190             # if the configuration is wrong.
1191             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1192 
1193         nova.conf.neutron.register_dynamic_opts(CONF)
1194 
1195         # Override the number of concurrent disk operations allowed if the
1196         # user has specified a limit.
1197         if CONF.compute.max_concurrent_disk_ops != 0:
1198             compute_utils.disk_ops_semaphore = \
1199                 eventlet.semaphore.BoundedSemaphore(
1200                     CONF.compute.max_concurrent_disk_ops)
1201 
1202         self.driver.init_host(host=self.host)
1203         context = nova.context.get_admin_context()
1204         instances = objects.InstanceList.get_by_host(
1205             context, self.host, expected_attrs=['info_cache', 'metadata'])
1206 
1207         if CONF.defer_iptables_apply:
1208             self.driver.filter_defer_apply_on()
1209 
1210         self.init_virt_events()
1211 
1212         try:
1213             # checking that instance was not already evacuated to other host
1214             evacuated_instances = self._destroy_evacuated_instances(context)
1215 
1216             # Initialise instances on the host that are not evacuating
1217             for instance in instances:
1218                 if (not evacuated_instances or
1219                         instance.uuid not in evacuated_instances):
1220                     self._init_instance(context, instance)
1221 
1222         finally:
1223             if CONF.defer_iptables_apply:
1224                 self.driver.filter_defer_apply_off()
1225             if instances:
1226                 # We only send the instance info to the scheduler on startup
1227                 # if there is anything to send, otherwise this host might
1228                 # not be mapped yet in a cell and the scheduler may have
1229                 # issues dealing with the information. Later changes to
1230                 # instances on this host will update the scheduler, or the
1231                 # _sync_scheduler_instance_info periodic task will.
1232                 self._update_scheduler_instance_info(context, instances)
1233 
1234     def cleanup_host(self):
1235         self.driver.register_event_listener(None)
1236         self.instance_events.cancel_all_events()
1237         self.driver.cleanup_host(host=self.host)
1238         self._cleanup_live_migrations_in_pool()
1239 
1240     def _cleanup_live_migrations_in_pool(self):
1241         # Shutdown the pool so we don't get new requests.
1242         self._live_migration_executor.shutdown(wait=False)
1243         # For any queued migrations, cancel the migration and update
1244         # its status.
1245         for migration, future in self._waiting_live_migrations.values():
1246             # If we got here before the Future was submitted then we need
1247             # to move on since there isn't anything we can do.
1248             if future is None:
1249                 continue
1250             if future.cancel():
1251                 self._set_migration_status(migration, 'cancelled')
1252                 LOG.info('Successfully cancelled queued live migration.',
1253                          instance_uuid=migration.instance_uuid)
1254             else:
1255                 LOG.warning('Unable to cancel live migration.',
1256                             instance_uuid=migration.instance_uuid)
1257         self._waiting_live_migrations.clear()
1258 
1259     def pre_start_hook(self):
1260         """After the service is initialized, but before we fully bring
1261         the service up by listening on RPC queues, make sure to update
1262         our available resources (and indirectly our available nodes).
1263         """
1264         self.update_available_resource(nova.context.get_admin_context(),
1265                                        startup=True)
1266 
1267     def _get_power_state(self, context, instance):
1268         """Retrieve the power state for the given instance."""
1269         LOG.debug('Checking state', instance=instance)
1270         try:
1271             return self.driver.get_info(instance).state
1272         except exception.InstanceNotFound:
1273             return power_state.NOSTATE
1274 
1275     def get_console_topic(self, context):
1276         """Retrieves the console host for a project on this host.
1277 
1278         Currently this is just set in the flags for each compute host.
1279 
1280         """
1281         # TODO(mdragon): perhaps make this variable by console_type?
1282         return '%s.%s' % (console_rpcapi.RPC_TOPIC, CONF.console_host)
1283 
1284     @wrap_exception()
1285     def get_console_pool_info(self, context, console_type):
1286         return self.driver.get_console_pool_info(console_type)
1287 
1288     @wrap_exception()
1289     def refresh_instance_security_rules(self, context, instance):
1290         """Tell the virtualization driver to refresh security rules for
1291         an instance.
1292 
1293         Passes straight through to the virtualization driver.
1294 
1295         Synchronize the call because we may still be in the middle of
1296         creating the instance.
1297         """
1298         @utils.synchronized(instance.uuid)
1299         def _sync_refresh():
1300             try:
1301                 return self.driver.refresh_instance_security_rules(instance)
1302             except NotImplementedError:
1303                 LOG.debug('Hypervisor driver does not support '
1304                           'security groups.', instance=instance)
1305 
1306         return _sync_refresh()
1307 
1308     def _await_block_device_map_created(self, context, vol_id):
1309         # TODO(yamahata): creating volume simultaneously
1310         #                 reduces creation time?
1311         # TODO(yamahata): eliminate dumb polling
1312         start = time.time()
1313         retries = CONF.block_device_allocate_retries
1314         if retries < 0:
1315             LOG.warning("Treating negative config value (%(retries)s) for "
1316                         "'block_device_retries' as 0.",
1317                         {'retries': retries})
1318         # (1) treat  negative config value as 0
1319         # (2) the configured value is 0, one attempt should be made
1320         # (3) the configured value is > 0, then the total number attempts
1321         #      is (retries + 1)
1322         attempts = 1
1323         if retries >= 1:
1324             attempts = retries + 1
1325         for attempt in range(1, attempts + 1):
1326             volume = self.volume_api.get(context, vol_id)
1327             volume_status = volume['status']
1328             if volume_status not in ['creating', 'downloading']:
1329                 if volume_status == 'available':
1330                     return attempt
1331                 LOG.warning("Volume id: %(vol_id)s finished being "
1332                             "created but its status is %(vol_status)s.",
1333                             {'vol_id': vol_id,
1334                              'vol_status': volume_status})
1335                 break
1336             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1337         raise exception.VolumeNotCreated(volume_id=vol_id,
1338                                          seconds=int(time.time() - start),
1339                                          attempts=attempt,
1340                                          volume_status=volume_status)
1341 
1342     def _decode_files(self, injected_files):
1343         """Base64 decode the list of files to inject."""
1344         if not injected_files:
1345             return []
1346 
1347         def _decode(f):
1348             path, contents = f
1349             # Py3 raises binascii.Error instead of TypeError as in Py27
1350             try:
1351                 decoded = base64.b64decode(contents)
1352                 return path, decoded
1353             except (TypeError, binascii.Error):
1354                 raise exception.Base64Exception(path=path)
1355 
1356         return [_decode(f) for f in injected_files]
1357 
1358     def _validate_instance_group_policy(self, context, instance,
1359                                         scheduler_hints):
1360         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1361         # However, there is a race condition with the enforcement of
1362         # the policy.  Since more than one instance may be scheduled at the
1363         # same time, it's possible that more than one instance with an
1364         # anti-affinity policy may end up here.  It's also possible that
1365         # multiple instances with an affinity policy could end up on different
1366         # hosts.  This is a validation step to make sure that starting the
1367         # instance here doesn't violate the policy.
1368         group_hint = scheduler_hints.get('group')
1369         if not group_hint:
1370             return
1371 
1372         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1373         # to check the type on the value and pull the single entry out. The
1374         # API request schema validates that the 'group' hint is a single value.
1375         if isinstance(group_hint, list):
1376             group_hint = group_hint[0]
1377 
1378         @utils.synchronized(group_hint)
1379         def _do_validation(context, instance, group_hint):
1380             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1381             if group.policy and 'anti-affinity' == group.policy:
1382                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1383                     context, self.host)
1384                 ins_on_host = set(instances_uuids)
1385                 members = set(group.members)
1386                 # Determine the set of instance group members on this host
1387                 # which are not the instance in question. This is used to
1388                 # determine how many other members from the same anti-affinity
1389                 # group can be on this host.
1390                 members_on_host = ins_on_host & members - set([instance.uuid])
1391                 rules = group.rules
1392                 if rules and 'max_server_per_host' in rules:
1393                     max_server = rules['max_server_per_host']
1394                 else:
1395                     max_server = 1
1396                 if len(members_on_host) >= max_server:
1397                     msg = _("Anti-affinity instance group policy "
1398                             "was violated.")
1399                     raise exception.RescheduledException(
1400                             instance_uuid=instance.uuid,
1401                             reason=msg)
1402             elif group.policy and 'affinity' == group.policy:
1403                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1404                 if group_hosts and self.host not in group_hosts:
1405                     msg = _("Affinity instance group policy was violated.")
1406                     raise exception.RescheduledException(
1407                             instance_uuid=instance.uuid,
1408                             reason=msg)
1409 
1410         if not CONF.workarounds.disable_group_policy_check_upcall:
1411             _do_validation(context, instance, group_hint)
1412 
1413     def _log_original_error(self, exc_info, instance_uuid):
1414         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1415                   exc_info=exc_info)
1416 
1417     # TODO(mriedem): This method is confusing and only ever used for resize
1418     # reschedules; remove it and merge into _reschedule_resize_or_reraise.
1419     def _reschedule(self, context, request_spec, filter_properties,
1420             instance, reschedule_method, method_args, task_state,
1421             exc_info=None, host_list=None):
1422         """Attempt to re-schedule a compute operation."""
1423 
1424         instance_uuid = instance.uuid
1425         retry = filter_properties.get('retry')
1426         if not retry:
1427             # no retry information, do not reschedule.
1428             LOG.debug("Retry info not present, will not reschedule",
1429                       instance_uuid=instance_uuid)
1430             return
1431 
1432         LOG.debug("Re-scheduling %(method)s: attempt %(num)d",
1433                   {'method': reschedule_method.__name__,
1434                    'num': retry['num_attempts']}, instance_uuid=instance_uuid)
1435 
1436         # reset the task state:
1437         self._instance_update(context, instance, task_state=task_state)
1438 
1439         if exc_info:
1440             # stringify to avoid circular ref problem in json serialization:
1441             retry['exc'] = traceback.format_exception_only(exc_info[0],
1442                                     exc_info[1])
1443 
1444         reschedule_method(context, *method_args, request_spec=request_spec,
1445                           host_list=host_list)
1446         return True
1447 
1448     @periodic_task.periodic_task
1449     def _check_instance_build_time(self, context):
1450         """Ensure that instances are not stuck in build."""
1451         timeout = CONF.instance_build_timeout
1452         if timeout == 0:
1453             return
1454 
1455         filters = {'vm_state': vm_states.BUILDING,
1456                    'host': self.host}
1457 
1458         building_insts = objects.InstanceList.get_by_filters(context,
1459                            filters, expected_attrs=[], use_slave=True)
1460 
1461         for instance in building_insts:
1462             if timeutils.is_older_than(instance.created_at, timeout):
1463                 self._set_instance_obj_error_state(context, instance)
1464                 LOG.warning("Instance build timed out. Set to error "
1465                             "state.", instance=instance)
1466 
1467     def _check_instance_exists(self, context, instance):
1468         """Ensure an instance with the same name is not already present."""
1469         if self.driver.instance_exists(instance):
1470             raise exception.InstanceExists(name=instance.name)
1471 
1472     def _allocate_network_async(self, context, instance, requested_networks,
1473                                 macs, security_groups, is_vpn):
1474         """Method used to allocate networks in the background.
1475 
1476         Broken out for testing.
1477         """
1478         # First check to see if we're specifically not supposed to allocate
1479         # networks because if so, we can exit early.
1480         if requested_networks and requested_networks.no_allocate:
1481             LOG.debug("Not allocating networking since 'none' was specified.",
1482                       instance=instance)
1483             return network_model.NetworkInfo([])
1484 
1485         LOG.debug("Allocating IP information in the background.",
1486                   instance=instance)
1487         retries = CONF.network_allocate_retries
1488         attempts = retries + 1
1489         retry_time = 1
1490         bind_host_id = self.driver.network_binding_host_id(context, instance)
1491         for attempt in range(1, attempts + 1):
1492             try:
1493                 nwinfo = self.network_api.allocate_for_instance(
1494                         context, instance, vpn=is_vpn,
1495                         requested_networks=requested_networks,
1496                         macs=macs,
1497                         security_groups=security_groups,
1498                         bind_host_id=bind_host_id)
1499                 LOG.debug('Instance network_info: |%s|', nwinfo,
1500                           instance=instance)
1501                 instance.system_metadata['network_allocated'] = 'True'
1502                 # NOTE(JoshNang) do not save the instance here, as it can cause
1503                 # races. The caller shares a reference to instance and waits
1504                 # for this async greenthread to finish before calling
1505                 # instance.save().
1506                 return nwinfo
1507             except Exception:
1508                 exc_info = sys.exc_info()
1509                 log_info = {'attempt': attempt,
1510                             'attempts': attempts}
1511                 if attempt == attempts:
1512                     LOG.exception('Instance failed network setup '
1513                                   'after %(attempts)d attempt(s)',
1514                                   log_info)
1515                     six.reraise(*exc_info)
1516                 LOG.warning('Instance failed network setup '
1517                             '(attempt %(attempt)d of %(attempts)d)',
1518                             log_info, instance=instance)
1519                 time.sleep(retry_time)
1520                 retry_time *= 2
1521                 if retry_time > 30:
1522                     retry_time = 30
1523         # Not reached.
1524 
1525     def _build_networks_for_instance(self, context, instance,
1526             requested_networks, security_groups):
1527 
1528         # If we're here from a reschedule the network may already be allocated.
1529         if strutils.bool_from_string(
1530                 instance.system_metadata.get('network_allocated', 'False')):
1531             # NOTE(alex_xu): The network_allocated is True means the network
1532             # resource already allocated at previous scheduling, and the
1533             # network setup is cleanup at previous. After rescheduling, the
1534             # network resource need setup on the new host.
1535             self.network_api.setup_instance_network_on_host(
1536                 context, instance, instance.host)
1537             return self.network_api.get_instance_nw_info(context, instance)
1538 
1539         if not self.is_neutron_security_groups:
1540             security_groups = []
1541 
1542         macs = self.driver.macs_for_instance(instance)
1543         network_info = self._allocate_network(context, instance,
1544                 requested_networks, macs, security_groups)
1545 
1546         return network_info
1547 
1548     def _allocate_network(self, context, instance, requested_networks, macs,
1549                           security_groups):
1550         """Start network allocation asynchronously.  Return an instance
1551         of NetworkInfoAsyncWrapper that can be used to retrieve the
1552         allocated networks when the operation has finished.
1553         """
1554         # NOTE(comstud): Since we're allocating networks asynchronously,
1555         # this task state has little meaning, as we won't be in this
1556         # state for very long.
1557         instance.vm_state = vm_states.BUILDING
1558         instance.task_state = task_states.NETWORKING
1559         instance.save(expected_task_state=[None])
1560 
1561         is_vpn = False
1562         return network_model.NetworkInfoAsyncWrapper(
1563                 self._allocate_network_async, context, instance,
1564                 requested_networks, macs, security_groups, is_vpn)
1565 
1566     def _default_root_device_name(self, instance, image_meta, root_bdm):
1567         """Gets a default root device name from the driver.
1568 
1569         :param nova.objects.Instance instance:
1570             The instance for which to get the root device name.
1571         :param nova.objects.ImageMeta image_meta:
1572             The metadata of the image of the instance.
1573         :param nova.objects.BlockDeviceMapping root_bdm:
1574             The description of the root device.
1575         :returns: str -- The default root device name.
1576         :raises: InternalError, TooManyDiskDevices
1577         """
1578         try:
1579             return self.driver.default_root_device_name(instance,
1580                                                         image_meta,
1581                                                         root_bdm)
1582         except NotImplementedError:
1583             return compute_utils.get_next_device_name(instance, [])
1584 
1585     def _default_device_names_for_instance(self, instance,
1586                                            root_device_name,
1587                                            *block_device_lists):
1588         """Default the missing device names in the BDM from the driver.
1589 
1590         :param nova.objects.Instance instance:
1591             The instance for which to get default device names.
1592         :param str root_device_name: The root device name.
1593         :param list block_device_lists: List of block device mappings.
1594         :returns: None
1595         :raises: InternalError, TooManyDiskDevices
1596         """
1597         try:
1598             self.driver.default_device_names_for_instance(instance,
1599                                                           root_device_name,
1600                                                           *block_device_lists)
1601         except NotImplementedError:
1602             compute_utils.default_device_names_for_instance(
1603                 instance, root_device_name, *block_device_lists)
1604 
1605     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1606         """Get the next device name from the driver, based on the BDM.
1607 
1608         :param nova.objects.Instance instance:
1609             The instance whose volume is requesting a device name.
1610         :param nova.objects.BlockDeviceMappingList bdms:
1611             The block device mappings for the instance.
1612         :param nova.objects.BlockDeviceMapping block_device_obj:
1613             A block device mapping containing info about the requested block
1614             device.
1615         :returns: The next device name.
1616         :raises: InternalError, TooManyDiskDevices
1617         """
1618         # NOTE(ndipanov): Copy obj to avoid changing the original
1619         block_device_obj = block_device_obj.obj_clone()
1620         try:
1621             return self.driver.get_device_name_for_instance(
1622                 instance, bdms, block_device_obj)
1623         except NotImplementedError:
1624             return compute_utils.get_device_name_for_instance(
1625                 instance, bdms, block_device_obj.get("device_name"))
1626 
1627     def _default_block_device_names(self, instance, image_meta, block_devices):
1628         """Verify that all the devices have the device_name set. If not,
1629         provide a default name.
1630 
1631         It also ensures that there is a root_device_name and is set to the
1632         first block device in the boot sequence (boot_index=0).
1633         """
1634         root_bdm = block_device.get_root_bdm(block_devices)
1635         if not root_bdm:
1636             return
1637 
1638         # Get the root_device_name from the root BDM or the instance
1639         root_device_name = None
1640         update_root_bdm = False
1641 
1642         if root_bdm.device_name:
1643             root_device_name = root_bdm.device_name
1644             instance.root_device_name = root_device_name
1645         elif instance.root_device_name:
1646             root_device_name = instance.root_device_name
1647             root_bdm.device_name = root_device_name
1648             update_root_bdm = True
1649         else:
1650             root_device_name = self._default_root_device_name(instance,
1651                                                               image_meta,
1652                                                               root_bdm)
1653 
1654             instance.root_device_name = root_device_name
1655             root_bdm.device_name = root_device_name
1656             update_root_bdm = True
1657 
1658         if update_root_bdm:
1659             root_bdm.save()
1660 
1661         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1662                             block_devices))
1663         swap = list(filter(block_device.new_format_is_swap,
1664                       block_devices))
1665         block_device_mapping = list(filter(
1666               driver_block_device.is_block_device_mapping, block_devices))
1667 
1668         self._default_device_names_for_instance(instance,
1669                                                 root_device_name,
1670                                                 ephemerals,
1671                                                 swap,
1672                                                 block_device_mapping)
1673 
1674     def _block_device_info_to_legacy(self, block_device_info):
1675         """Convert BDI to the old format for drivers that need it."""
1676 
1677         if self.use_legacy_block_device_info:
1678             ephemerals = driver_block_device.legacy_block_devices(
1679                 driver.block_device_info_get_ephemerals(block_device_info))
1680             mapping = driver_block_device.legacy_block_devices(
1681                 driver.block_device_info_get_mapping(block_device_info))
1682             swap = block_device_info['swap']
1683             if swap:
1684                 swap = swap.legacy()
1685 
1686             block_device_info.update({
1687                 'ephemerals': ephemerals,
1688                 'swap': swap,
1689                 'block_device_mapping': mapping})
1690 
1691     def _add_missing_dev_names(self, bdms, instance):
1692         for bdm in bdms:
1693             if bdm.device_name is not None:
1694                 continue
1695 
1696             device_name = self._get_device_name_for_instance(instance,
1697                                                              bdms, bdm)
1698             values = {'device_name': device_name}
1699             bdm.update(values)
1700             bdm.save()
1701 
1702     def _prep_block_device(self, context, instance, bdms):
1703         """Set up the block device for an instance with error logging."""
1704         try:
1705             self._add_missing_dev_names(bdms, instance)
1706             block_device_info = driver.get_block_device_info(instance, bdms)
1707             mapping = driver.block_device_info_get_mapping(block_device_info)
1708             driver_block_device.attach_block_devices(
1709                 mapping, context, instance, self.volume_api, self.driver,
1710                 wait_func=self._await_block_device_map_created)
1711 
1712             self._block_device_info_to_legacy(block_device_info)
1713             return block_device_info
1714 
1715         except exception.OverQuota as e:
1716             LOG.warning('Failed to create block device for instance due'
1717                         ' to exceeding volume related resource quota.'
1718                         ' Error: %s', e.message, instance=instance)
1719             raise
1720 
1721         except Exception as ex:
1722             LOG.exception('Instance failed block device setup',
1723                           instance=instance)
1724             # InvalidBDM will eventually result in a BuildAbortException when
1725             # booting from volume, and will be recorded as an instance fault.
1726             # Maintain the original exception message which most likely has
1727             # useful details which the standard InvalidBDM error message lacks.
1728             raise exception.InvalidBDM(six.text_type(ex))
1729 
1730     def _update_instance_after_spawn(self, context, instance):
1731         instance.power_state = self._get_power_state(context, instance)
1732         instance.vm_state = vm_states.ACTIVE
1733         instance.task_state = None
1734         instance.launched_at = timeutils.utcnow()
1735         configdrive.update_instance(instance)
1736 
1737     def _update_scheduler_instance_info(self, context, instance):
1738         """Sends an InstanceList with created or updated Instance objects to
1739         the Scheduler client.
1740 
1741         In the case of init_host, the value passed will already be an
1742         InstanceList. Other calls will send individual Instance objects that
1743         have been created or resized. In this case, we create an InstanceList
1744         object containing that Instance.
1745         """
1746         if not self.send_instance_updates:
1747             return
1748         if isinstance(instance, obj_instance.Instance):
1749             instance = objects.InstanceList(objects=[instance])
1750         context = context.elevated()
1751         self.query_client.update_instance_info(context, self.host,
1752                                                instance)
1753 
1754     def _delete_scheduler_instance_info(self, context, instance_uuid):
1755         """Sends the uuid of the deleted Instance to the Scheduler client."""
1756         if not self.send_instance_updates:
1757             return
1758         context = context.elevated()
1759         self.query_client.delete_instance_info(context, self.host,
1760                                                instance_uuid)
1761 
1762     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
1763     def _sync_scheduler_instance_info(self, context):
1764         if not self.send_instance_updates:
1765             return
1766         context = context.elevated()
1767         instances = objects.InstanceList.get_by_host(context, self.host,
1768                                                      expected_attrs=[],
1769                                                      use_slave=True)
1770         uuids = [instance.uuid for instance in instances]
1771         self.query_client.sync_instance_info(context, self.host, uuids)
1772 
1773     def _notify_about_instance_usage(self, context, instance, event_suffix,
1774                                      network_info=None, extra_usage_info=None,
1775                                      fault=None):
1776         compute_utils.notify_about_instance_usage(
1777             self.notifier, context, instance, event_suffix,
1778             network_info=network_info,
1779             extra_usage_info=extra_usage_info, fault=fault)
1780 
1781     def _deallocate_network(self, context, instance,
1782                             requested_networks=None):
1783         # If we were told not to allocate networks let's save ourselves
1784         # the trouble of calling the network API.
1785         if requested_networks and requested_networks.no_allocate:
1786             LOG.debug("Skipping network deallocation for instance since "
1787                       "networking was not requested.", instance=instance)
1788             return
1789 
1790         LOG.debug('Deallocating network for instance', instance=instance)
1791         with timeutils.StopWatch() as timer:
1792             self.network_api.deallocate_for_instance(
1793                 context, instance, requested_networks=requested_networks)
1794         # nova-network does an rpc call so we're OK tracking time spent here
1795         LOG.info('Took %0.2f seconds to deallocate network for instance.',
1796                  timer.elapsed(), instance=instance)
1797 
1798     def _get_instance_block_device_info(self, context, instance,
1799                                         refresh_conn_info=False,
1800                                         bdms=None):
1801         """Transform block devices to the driver block_device format."""
1802 
1803         if bdms is None:
1804             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1805                     context, instance.uuid)
1806         block_device_info = driver.get_block_device_info(instance, bdms)
1807 
1808         if not refresh_conn_info:
1809             # if the block_device_mapping has no value in connection_info
1810             # (returned as None), don't include in the mapping
1811             block_device_info['block_device_mapping'] = [
1812                 bdm for bdm in driver.block_device_info_get_mapping(
1813                                     block_device_info)
1814                 if bdm.get('connection_info')]
1815         else:
1816             driver_block_device.refresh_conn_infos(
1817                 driver.block_device_info_get_mapping(block_device_info),
1818                 context, instance, self.volume_api, self.driver)
1819 
1820         self._block_device_info_to_legacy(block_device_info)
1821 
1822         return block_device_info
1823 
1824     def _build_failed(self, node):
1825         if CONF.compute.consecutive_build_service_disable_threshold:
1826             # NOTE(danms): Update our counter, but wait for the next
1827             # update_available_resource() periodic to flush it to the DB
1828             self.rt.build_failed(node)
1829 
1830     def _build_succeeded(self, node):
1831         self.rt.build_succeeded(node)
1832 
1833     @wrap_exception()
1834     @reverts_task_state
1835     @wrap_instance_fault
1836     def build_and_run_instance(self, context, instance, image, request_spec,
1837                      filter_properties, admin_password=None,
1838                      injected_files=None, requested_networks=None,
1839                      security_groups=None, block_device_mapping=None,
1840                      node=None, limits=None, host_list=None):
1841 
1842         @utils.synchronized(instance.uuid)
1843         def _locked_do_build_and_run_instance(*args, **kwargs):
1844             # NOTE(danms): We grab the semaphore with the instance uuid
1845             # locked because we could wait in line to build this instance
1846             # for a while and we want to make sure that nothing else tries
1847             # to do anything with this instance while we wait.
1848             with self._build_semaphore:
1849                 try:
1850                     result = self._do_build_and_run_instance(*args, **kwargs)
1851                 except Exception:
1852                     # NOTE(mriedem): This should really only happen if
1853                     # _decode_files in _do_build_and_run_instance fails, and
1854                     # that's before a guest is spawned so it's OK to remove
1855                     # allocations for the instance for this node from Placement
1856                     # below as there is no guest consuming resources anyway.
1857                     # The _decode_files case could be handled more specifically
1858                     # but that's left for another day.
1859                     result = build_results.FAILED
1860                     raise
1861                 finally:
1862                     if result == build_results.FAILED:
1863                         # Remove the allocation records from Placement for the
1864                         # instance if the build failed. The instance.host is
1865                         # likely set to None in _do_build_and_run_instance
1866                         # which means if the user deletes the instance, it
1867                         # will be deleted in the API, not the compute service.
1868                         # Setting the instance.host to None in
1869                         # _do_build_and_run_instance means that the
1870                         # ResourceTracker will no longer consider this instance
1871                         # to be claiming resources against it, so we want to
1872                         # reflect that same thing in Placement.  No need to
1873                         # call this for a reschedule, as the allocations will
1874                         # have already been removed in
1875                         # self._do_build_and_run_instance().
1876                         self.reportclient.delete_allocation_for_instance(
1877                             context, instance.uuid)
1878 
1879                     if result in (build_results.FAILED,
1880                                   build_results.RESCHEDULED):
1881                         self._build_failed(node)
1882                     else:
1883                         self._build_succeeded(node)
1884 
1885         # NOTE(danms): We spawn here to return the RPC worker thread back to
1886         # the pool. Since what follows could take a really long time, we don't
1887         # want to tie up RPC workers.
1888         utils.spawn_n(_locked_do_build_and_run_instance,
1889                       context, instance, image, request_spec,
1890                       filter_properties, admin_password, injected_files,
1891                       requested_networks, security_groups,
1892                       block_device_mapping, node, limits, host_list)
1893 
1894     def _check_device_tagging(self, requested_networks, block_device_mapping):
1895         tagging_requested = False
1896         if requested_networks:
1897             for net in requested_networks:
1898                 if 'tag' in net and net.tag is not None:
1899                     tagging_requested = True
1900                     break
1901         if block_device_mapping and not tagging_requested:
1902             for bdm in block_device_mapping:
1903                 if 'tag' in bdm and bdm.tag is not None:
1904                     tagging_requested = True
1905                     break
1906         if (tagging_requested and
1907                 not self.driver.capabilities.get('supports_device_tagging',
1908                                                  False)):
1909             raise exception.BuildAbortException('Attempt to boot guest with '
1910                                                 'tagged devices on host that '
1911                                                 'does not support tagging.')
1912 
1913     def _check_trusted_certs(self, instance):
1914         if (instance.trusted_certs and
1915                 not self.driver.capabilities.get('supports_trusted_certs',
1916                                                  False)):
1917             raise exception.BuildAbortException(
1918                 'Trusted image certificates provided on host that does not '
1919                 'support certificate validation.')
1920 
1921     @hooks.add_hook('build_instance')
1922     @wrap_exception()
1923     @reverts_task_state
1924     @wrap_instance_event(prefix='compute')
1925     @wrap_instance_fault
1926     def _do_build_and_run_instance(self, context, instance, image,
1927             request_spec, filter_properties, admin_password, injected_files,
1928             requested_networks, security_groups, block_device_mapping,
1929             node=None, limits=None, host_list=None):
1930 
1931         try:
1932             LOG.debug('Starting instance...', instance=instance)
1933             instance.vm_state = vm_states.BUILDING
1934             instance.task_state = None
1935             instance.save(expected_task_state=
1936                     (task_states.SCHEDULING, None))
1937         except exception.InstanceNotFound:
1938             msg = 'Instance disappeared before build.'
1939             LOG.debug(msg, instance=instance)
1940             return build_results.FAILED
1941         except exception.UnexpectedTaskStateError as e:
1942             LOG.debug(e.format_message(), instance=instance)
1943             return build_results.FAILED
1944 
1945         # b64 decode the files to inject:
1946         decoded_files = self._decode_files(injected_files)
1947 
1948         if limits is None:
1949             limits = {}
1950 
1951         if node is None:
1952             node = self._get_nodename(instance, refresh=True)
1953 
1954         try:
1955             with timeutils.StopWatch() as timer:
1956                 self._build_and_run_instance(context, instance, image,
1957                         decoded_files, admin_password, requested_networks,
1958                         security_groups, block_device_mapping, node, limits,
1959                         filter_properties, request_spec)
1960             LOG.info('Took %0.2f seconds to build instance.',
1961                      timer.elapsed(), instance=instance)
1962             return build_results.ACTIVE
1963         except exception.RescheduledException as e:
1964             retry = filter_properties.get('retry')
1965             if not retry:
1966                 # no retry information, do not reschedule.
1967                 LOG.debug("Retry info not present, will not reschedule",
1968                     instance=instance)
1969                 self._cleanup_allocated_networks(context, instance,
1970                     requested_networks)
1971                 self._cleanup_volumes(context, instance,
1972                     block_device_mapping, raise_exc=False)
1973                 compute_utils.add_instance_fault_from_exc(context,
1974                         instance, e, sys.exc_info(),
1975                         fault_message=e.kwargs['reason'])
1976                 self._nil_out_instance_obj_host_and_node(instance)
1977                 self._set_instance_obj_error_state(context, instance,
1978                                                    clean_task_state=True)
1979                 return build_results.FAILED
1980             LOG.debug(e.format_message(), instance=instance)
1981             # This will be used for logging the exception
1982             retry['exc'] = traceback.format_exception(*sys.exc_info())
1983             # This will be used for setting the instance fault message
1984             retry['exc_reason'] = e.kwargs['reason']
1985             # NOTE(comstud): Deallocate networks if the driver wants
1986             # us to do so.
1987             # NOTE(mriedem): Always deallocate networking when using Neutron.
1988             # This is to unbind any ports that the user supplied in the server
1989             # create request, or delete any ports that nova created which were
1990             # meant to be bound to this host. This check intentionally bypasses
1991             # the result of deallocate_networks_on_reschedule because the
1992             # default value in the driver is False, but that method was really
1993             # only meant for Ironic and should be removed when nova-network is
1994             # removed (since is_neutron() will then always be True).
1995             # NOTE(vladikr): SR-IOV ports should be deallocated to
1996             # allow new sriov pci devices to be allocated on a new host.
1997             # Otherwise, if devices with pci addresses are already allocated
1998             # on the destination host, the instance will fail to spawn.
1999             # info_cache.network_info should be present at this stage.
2000             if (self.driver.deallocate_networks_on_reschedule(instance) or
2001                 utils.is_neutron() or
2002                 self.deallocate_sriov_ports_on_reschedule(instance)):
2003                 self._cleanup_allocated_networks(context, instance,
2004                         requested_networks)
2005             else:
2006                 # NOTE(alex_xu): Network already allocated and we don't
2007                 # want to deallocate them before rescheduling. But we need
2008                 # to cleanup those network resources setup on this host before
2009                 # rescheduling.
2010                 self.network_api.cleanup_instance_network_on_host(
2011                     context, instance, self.host)
2012 
2013             self._nil_out_instance_obj_host_and_node(instance)
2014             instance.task_state = task_states.SCHEDULING
2015             instance.save()
2016             # The instance will have already claimed resources from this host
2017             # before this build was attempted. Now that it has failed, we need
2018             # to unclaim those resources before casting to the conductor, so
2019             # that if there are alternate hosts available for a retry, it can
2020             # claim resources on that new host for the instance.
2021             self.reportclient.delete_allocation_for_instance(context,
2022                                                              instance.uuid)
2023 
2024             self.compute_task_api.build_instances(context, [instance],
2025                     image, filter_properties, admin_password,
2026                     injected_files, requested_networks, security_groups,
2027                     block_device_mapping, request_spec=request_spec,
2028                     host_lists=[host_list])
2029             return build_results.RESCHEDULED
2030         except (exception.InstanceNotFound,
2031                 exception.UnexpectedDeletingTaskStateError):
2032             msg = 'Instance disappeared during build.'
2033             LOG.debug(msg, instance=instance)
2034             self._cleanup_allocated_networks(context, instance,
2035                     requested_networks)
2036             return build_results.FAILED
2037         except Exception as e:
2038             if isinstance(e, exception.BuildAbortException):
2039                 LOG.error(e.format_message(), instance=instance)
2040             else:
2041                 # Should not reach here.
2042                 LOG.exception('Unexpected build failure, not rescheduling '
2043                               'build.', instance=instance)
2044             self._cleanup_allocated_networks(context, instance,
2045                     requested_networks)
2046             self._cleanup_volumes(context, instance,
2047                     block_device_mapping, raise_exc=False)
2048             compute_utils.add_instance_fault_from_exc(context, instance,
2049                     e, sys.exc_info())
2050             self._nil_out_instance_obj_host_and_node(instance)
2051             self._set_instance_obj_error_state(context, instance,
2052                                                clean_task_state=True)
2053             return build_results.FAILED
2054 
2055     def deallocate_sriov_ports_on_reschedule(self, instance):
2056         """Determine if networks are needed to be deallocated before reschedule
2057 
2058         Check the cached network info for any assigned SR-IOV ports.
2059         SR-IOV ports should be deallocated prior to rescheduling
2060         in order to allow new sriov pci devices to be allocated on a new host.
2061         """
2062         info_cache = instance.info_cache
2063 
2064         def _has_sriov_port(vif):
2065             return vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV
2066 
2067         if (info_cache and info_cache.network_info):
2068             for vif in info_cache.network_info:
2069                 if _has_sriov_port(vif):
2070                     return True
2071         return False
2072 
2073     @staticmethod
2074     def _get_scheduler_hints(filter_properties, request_spec=None):
2075         """Helper method to get scheduler hints.
2076 
2077         This method prefers to get the hints out of the request spec, but that
2078         might not be provided. Conductor will pass request_spec down to the
2079         first compute chosen for a build but older computes will not pass
2080         the request_spec to conductor's build_instances method for a
2081         a reschedule, so if we're on a host via a retry, request_spec may not
2082         be provided so we need to fallback to use the filter_properties
2083         to get scheduler hints.
2084         """
2085         hints = {}
2086         if request_spec is not None and 'scheduler_hints' in request_spec:
2087             hints = request_spec.scheduler_hints
2088         if not hints:
2089             hints = filter_properties.get('scheduler_hints') or {}
2090         return hints
2091 
2092     def _build_and_run_instance(self, context, instance, image, injected_files,
2093             admin_password, requested_networks, security_groups,
2094             block_device_mapping, node, limits, filter_properties,
2095             request_spec=None):
2096 
2097         image_name = image.get('name')
2098         self._notify_about_instance_usage(context, instance, 'create.start',
2099                 extra_usage_info={'image_name': image_name})
2100         compute_utils.notify_about_instance_create(
2101             context, instance, self.host,
2102             phase=fields.NotificationPhase.START,
2103             bdms=block_device_mapping)
2104 
2105         # NOTE(mikal): cache the keystone roles associated with the instance
2106         # at boot time for later reference
2107         instance.system_metadata.update(
2108             {'boot_roles': ','.join(context.roles)})
2109 
2110         self._check_device_tagging(requested_networks, block_device_mapping)
2111         self._check_trusted_certs(instance)
2112 
2113         try:
2114             scheduler_hints = self._get_scheduler_hints(filter_properties,
2115                                                         request_spec)
2116             with self.rt.instance_claim(context, instance, node, limits):
2117                 # NOTE(russellb) It's important that this validation be done
2118                 # *after* the resource tracker instance claim, as that is where
2119                 # the host is set on the instance.
2120                 self._validate_instance_group_policy(context, instance,
2121                                                      scheduler_hints)
2122                 image_meta = objects.ImageMeta.from_dict(image)
2123                 with self._build_resources(context, instance,
2124                         requested_networks, security_groups, image_meta,
2125                         block_device_mapping) as resources:
2126                     instance.vm_state = vm_states.BUILDING
2127                     instance.task_state = task_states.SPAWNING
2128                     # NOTE(JoshNang) This also saves the changes to the
2129                     # instance from _allocate_network_async, as they aren't
2130                     # saved in that function to prevent races.
2131                     instance.save(expected_task_state=
2132                             task_states.BLOCK_DEVICE_MAPPING)
2133                     block_device_info = resources['block_device_info']
2134                     network_info = resources['network_info']
2135                     allocs = resources['allocations']
2136                     LOG.debug('Start spawning the instance on the hypervisor.',
2137                               instance=instance)
2138                     with timeutils.StopWatch() as timer:
2139                         self.driver.spawn(context, instance, image_meta,
2140                                           injected_files, admin_password,
2141                                           allocs, network_info=network_info,
2142                                           block_device_info=block_device_info)
2143                     LOG.info('Took %0.2f seconds to spawn the instance on '
2144                              'the hypervisor.', timer.elapsed(),
2145                              instance=instance)
2146         except (exception.InstanceNotFound,
2147                 exception.UnexpectedDeletingTaskStateError) as e:
2148             with excutils.save_and_reraise_exception():
2149                 self._notify_about_instance_usage(context, instance,
2150                     'create.error', fault=e)
2151                 tb = traceback.format_exc()
2152                 compute_utils.notify_about_instance_create(
2153                     context, instance, self.host,
2154                     phase=fields.NotificationPhase.ERROR, exception=e,
2155                     bdms=block_device_mapping, tb=tb)
2156         except exception.ComputeResourcesUnavailable as e:
2157             LOG.debug(e.format_message(), instance=instance)
2158             self._notify_about_instance_usage(context, instance,
2159                     'create.error', fault=e)
2160             tb = traceback.format_exc()
2161             compute_utils.notify_about_instance_create(
2162                     context, instance, self.host,
2163                     phase=fields.NotificationPhase.ERROR, exception=e,
2164                     bdms=block_device_mapping, tb=tb)
2165             raise exception.RescheduledException(
2166                     instance_uuid=instance.uuid, reason=e.format_message())
2167         except exception.BuildAbortException as e:
2168             with excutils.save_and_reraise_exception():
2169                 LOG.debug(e.format_message(), instance=instance)
2170                 self._notify_about_instance_usage(context, instance,
2171                     'create.error', fault=e)
2172                 tb = traceback.format_exc()
2173                 compute_utils.notify_about_instance_create(
2174                     context, instance, self.host,
2175                     phase=fields.NotificationPhase.ERROR, exception=e,
2176                     bdms=block_device_mapping, tb=tb)
2177         except (exception.FixedIpLimitExceeded,
2178                 exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:
2179             LOG.warning('No more network or fixed IP to be allocated',
2180                         instance=instance)
2181             self._notify_about_instance_usage(context, instance,
2182                     'create.error', fault=e)
2183             tb = traceback.format_exc()
2184             compute_utils.notify_about_instance_create(
2185                     context, instance, self.host,
2186                     phase=fields.NotificationPhase.ERROR, exception=e,
2187                     bdms=block_device_mapping, tb=tb)
2188             msg = _('Failed to allocate the network(s) with error %s, '
2189                     'not rescheduling.') % e.format_message()
2190             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2191                     reason=msg)
2192         except (exception.VirtualInterfaceCreateException,
2193                 exception.VirtualInterfaceMacAddressException,
2194                 exception.FixedIpInvalidOnHost,
2195                 exception.UnableToAutoAllocateNetwork,
2196                 exception.NetworksWithQoSPolicyNotSupported) as e:
2197             LOG.exception('Failed to allocate network(s)',
2198                           instance=instance)
2199             self._notify_about_instance_usage(context, instance,
2200                     'create.error', fault=e)
2201             tb = traceback.format_exc()
2202             compute_utils.notify_about_instance_create(
2203                     context, instance, self.host,
2204                     phase=fields.NotificationPhase.ERROR, exception=e,
2205                     bdms=block_device_mapping, tb=tb)
2206             msg = _('Failed to allocate the network(s), not rescheduling.')
2207             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2208                     reason=msg)
2209         except (exception.FlavorDiskTooSmall,
2210                 exception.FlavorMemoryTooSmall,
2211                 exception.ImageNotActive,
2212                 exception.ImageUnacceptable,
2213                 exception.InvalidDiskInfo,
2214                 exception.InvalidDiskFormat,
2215                 cursive_exception.SignatureVerificationError,
2216                 exception.CertificateValidationFailed,
2217                 exception.VolumeEncryptionNotSupported,
2218                 exception.InvalidInput,
2219                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2220                 # in the API during server create and rebuild.
2221                 exception.RequestedVRamTooHigh) as e:
2222             self._notify_about_instance_usage(context, instance,
2223                     'create.error', fault=e)
2224             tb = traceback.format_exc()
2225             compute_utils.notify_about_instance_create(
2226                     context, instance, self.host,
2227                     phase=fields.NotificationPhase.ERROR, exception=e,
2228                     bdms=block_device_mapping, tb=tb)
2229             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2230                     reason=e.format_message())
2231         except Exception as e:
2232             self._notify_about_instance_usage(context, instance,
2233                     'create.error', fault=e)
2234             tb = traceback.format_exc()
2235             compute_utils.notify_about_instance_create(
2236                     context, instance, self.host,
2237                     phase=fields.NotificationPhase.ERROR, exception=e,
2238                     bdms=block_device_mapping, tb=tb)
2239             raise exception.RescheduledException(
2240                     instance_uuid=instance.uuid, reason=six.text_type(e))
2241 
2242         # NOTE(alaski): This is only useful during reschedules, remove it now.
2243         instance.system_metadata.pop('network_allocated', None)
2244 
2245         # If CONF.default_access_ip_network_name is set, grab the
2246         # corresponding network and set the access ip values accordingly.
2247         network_name = CONF.default_access_ip_network_name
2248         if (network_name and not instance.access_ip_v4 and
2249                 not instance.access_ip_v6):
2250             # Note that when there are multiple ips to choose from, an
2251             # arbitrary one will be chosen.
2252             for vif in network_info:
2253                 if vif['network']['label'] == network_name:
2254                     for ip in vif.fixed_ips():
2255                         if not instance.access_ip_v4 and ip['version'] == 4:
2256                             instance.access_ip_v4 = ip['address']
2257                         if not instance.access_ip_v6 and ip['version'] == 6:
2258                             instance.access_ip_v6 = ip['address']
2259                     break
2260 
2261         self._update_instance_after_spawn(context, instance)
2262 
2263         try:
2264             instance.save(expected_task_state=task_states.SPAWNING)
2265         except (exception.InstanceNotFound,
2266                 exception.UnexpectedDeletingTaskStateError) as e:
2267             with excutils.save_and_reraise_exception():
2268                 self._notify_about_instance_usage(context, instance,
2269                     'create.error', fault=e)
2270                 tb = traceback.format_exc()
2271                 compute_utils.notify_about_instance_create(
2272                     context, instance, self.host,
2273                     phase=fields.NotificationPhase.ERROR, exception=e,
2274                     bdms=block_device_mapping, tb=tb)
2275 
2276         self._update_scheduler_instance_info(context, instance)
2277         self._notify_about_instance_usage(context, instance, 'create.end',
2278                 extra_usage_info={'message': _('Success')},
2279                 network_info=network_info)
2280         compute_utils.notify_about_instance_create(context, instance,
2281                 self.host, phase=fields.NotificationPhase.END,
2282                 bdms=block_device_mapping)
2283 
2284     @contextlib.contextmanager
2285     def _build_resources(self, context, instance, requested_networks,
2286                          security_groups, image_meta, block_device_mapping):
2287         resources = {}
2288         network_info = None
2289         try:
2290             LOG.debug('Start building networks asynchronously for instance.',
2291                       instance=instance)
2292             network_info = self._build_networks_for_instance(context, instance,
2293                     requested_networks, security_groups)
2294             resources['network_info'] = network_info
2295         except (exception.InstanceNotFound,
2296                 exception.UnexpectedDeletingTaskStateError):
2297             raise
2298         except exception.UnexpectedTaskStateError as e:
2299             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2300                     reason=e.format_message())
2301         except Exception:
2302             # Because this allocation is async any failures are likely to occur
2303             # when the driver accesses network_info during spawn().
2304             LOG.exception('Failed to allocate network(s)',
2305                           instance=instance)
2306             msg = _('Failed to allocate the network(s), not rescheduling.')
2307             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2308                     reason=msg)
2309 
2310         try:
2311             # Perform any driver preparation work for the driver.
2312             self.driver.prepare_for_spawn(instance)
2313 
2314             # Depending on a virt driver, some network configuration is
2315             # necessary before preparing block devices.
2316             self.driver.prepare_networks_before_block_device_mapping(
2317                 instance, network_info)
2318 
2319             # Verify that all the BDMs have a device_name set and assign a
2320             # default to the ones missing it with the help of the driver.
2321             self._default_block_device_names(instance, image_meta,
2322                                              block_device_mapping)
2323 
2324             LOG.debug('Start building block device mappings for instance.',
2325                       instance=instance)
2326             instance.vm_state = vm_states.BUILDING
2327             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2328             instance.save()
2329 
2330             block_device_info = self._prep_block_device(context, instance,
2331                     block_device_mapping)
2332             resources['block_device_info'] = block_device_info
2333         except (exception.InstanceNotFound,
2334                 exception.UnexpectedDeletingTaskStateError):
2335             with excutils.save_and_reraise_exception():
2336                 # Make sure the async call finishes
2337                 if network_info is not None:
2338                     network_info.wait(do_raise=False)
2339                     self.driver.clean_networks_preparation(instance,
2340                                                            network_info)
2341                 self.driver.failed_spawn_cleanup(instance)
2342         except (exception.UnexpectedTaskStateError,
2343                 exception.OverQuota, exception.InvalidBDM) as e:
2344             # Make sure the async call finishes
2345             if network_info is not None:
2346                 network_info.wait(do_raise=False)
2347                 self.driver.clean_networks_preparation(instance, network_info)
2348             self.driver.failed_spawn_cleanup(instance)
2349             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2350                     reason=e.format_message())
2351         except Exception:
2352             LOG.exception('Failure prepping block device',
2353                           instance=instance)
2354             # Make sure the async call finishes
2355             if network_info is not None:
2356                 network_info.wait(do_raise=False)
2357                 self.driver.clean_networks_preparation(instance, network_info)
2358             self.driver.failed_spawn_cleanup(instance)
2359             msg = _('Failure prepping block device.')
2360             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2361                     reason=msg)
2362 
2363         try:
2364             resources['allocations'] = (
2365                 self.reportclient.get_allocations_for_consumer(context,
2366                                                                instance.uuid))
2367         except Exception:
2368             LOG.exception('Failure retrieving placement allocations',
2369                           instance=instance)
2370             # Make sure the async call finishes
2371             if network_info is not None:
2372                 network_info.wait(do_raise=False)
2373             self.driver.failed_spawn_cleanup(instance)
2374             msg = _('Failure retrieving placement allocations')
2375             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2376                                                 reason=msg)
2377 
2378         try:
2379             yield resources
2380         except Exception as exc:
2381             with excutils.save_and_reraise_exception() as ctxt:
2382                 if not isinstance(exc, (
2383                         exception.InstanceNotFound,
2384                         exception.UnexpectedDeletingTaskStateError)):
2385                     LOG.exception('Instance failed to spawn',
2386                                   instance=instance)
2387                 # Make sure the async call finishes
2388                 if network_info is not None:
2389                     network_info.wait(do_raise=False)
2390                 # if network_info is empty we're likely here because of
2391                 # network allocation failure. Since nothing can be reused on
2392                 # rescheduling it's better to deallocate network to eliminate
2393                 # the chance of orphaned ports in neutron
2394                 deallocate_networks = False if network_info else True
2395                 try:
2396                     self._shutdown_instance(context, instance,
2397                             block_device_mapping, requested_networks,
2398                             try_deallocate_networks=deallocate_networks)
2399                 except Exception as exc2:
2400                     ctxt.reraise = False
2401                     LOG.warning('Could not clean up failed build,'
2402                                 ' not rescheduling. Error: %s',
2403                                 six.text_type(exc2))
2404                     raise exception.BuildAbortException(
2405                             instance_uuid=instance.uuid,
2406                             reason=six.text_type(exc))
2407 
2408     def _cleanup_allocated_networks(self, context, instance,
2409             requested_networks):
2410         try:
2411             self._deallocate_network(context, instance, requested_networks)
2412         except Exception:
2413             LOG.exception('Failed to deallocate networks', instance=instance)
2414             return
2415 
2416         instance.system_metadata['network_allocated'] = 'False'
2417         try:
2418             instance.save()
2419         except exception.InstanceNotFound:
2420             # NOTE(alaski): It's possible that we're cleaning up the networks
2421             # because the instance was deleted.  If that's the case then this
2422             # exception will be raised by instance.save()
2423             pass
2424 
2425     def _try_deallocate_network(self, context, instance,
2426                                 requested_networks=None):
2427 
2428         # During auto-scale cleanup, we could be deleting a large number
2429         # of servers at the same time and overloading parts of the system,
2430         # so we retry a few times in case of connection failures to the
2431         # networking service.
2432         @loopingcall.RetryDecorator(
2433             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2434             exceptions=(keystone_exception.connection.ConnectFailure,))
2435         def _deallocate_network_with_retries():
2436             try:
2437                 self._deallocate_network(
2438                     context, instance, requested_networks)
2439             except keystone_exception.connection.ConnectFailure as e:
2440                 # Provide a warning that something is amiss.
2441                 with excutils.save_and_reraise_exception():
2442                     LOG.warning('Failed to deallocate network for instance; '
2443                                 'retrying. Error: %s', six.text_type(e),
2444                                 instance=instance)
2445 
2446         try:
2447             # tear down allocated network structure
2448             _deallocate_network_with_retries()
2449         except Exception as ex:
2450             with excutils.save_and_reraise_exception():
2451                 LOG.error('Failed to deallocate network for instance. '
2452                           'Error: %s', ex, instance=instance)
2453                 self._set_instance_obj_error_state(context, instance)
2454 
2455     def _get_power_off_values(self, context, instance, clean_shutdown):
2456         """Get the timing configuration for powering down this instance."""
2457         if clean_shutdown:
2458             timeout = compute_utils.get_value_from_system_metadata(instance,
2459                           key='image_os_shutdown_timeout', type=int,
2460                           default=CONF.shutdown_timeout)
2461             retry_interval = CONF.compute.shutdown_retry_interval
2462         else:
2463             timeout = 0
2464             retry_interval = 0
2465 
2466         return timeout, retry_interval
2467 
2468     def _power_off_instance(self, context, instance, clean_shutdown=True):
2469         """Power off an instance on this host."""
2470         timeout, retry_interval = self._get_power_off_values(context,
2471                                         instance, clean_shutdown)
2472         self.driver.power_off(instance, timeout, retry_interval)
2473 
2474     def _shutdown_instance(self, context, instance,
2475                            bdms, requested_networks=None, notify=True,
2476                            try_deallocate_networks=True):
2477         """Shutdown an instance on this host.
2478 
2479         :param:context: security context
2480         :param:instance: a nova.objects.Instance object
2481         :param:bdms: the block devices for the instance to be torn
2482                      down
2483         :param:requested_networks: the networks on which the instance
2484                                    has ports
2485         :param:notify: true if a final usage notification should be
2486                        emitted
2487         :param:try_deallocate_networks: false if we should avoid
2488                                         trying to teardown networking
2489         """
2490         context = context.elevated()
2491         LOG.info('Terminating instance', instance=instance)
2492 
2493         if notify:
2494             self._notify_about_instance_usage(context, instance,
2495                                               "shutdown.start")
2496             compute_utils.notify_about_instance_action(context, instance,
2497                     self.host, action=fields.NotificationAction.SHUTDOWN,
2498                     phase=fields.NotificationPhase.START, bdms=bdms)
2499 
2500         network_info = instance.get_network_info()
2501 
2502         # NOTE(vish) get bdms before destroying the instance
2503         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2504         block_device_info = self._get_instance_block_device_info(
2505             context, instance, bdms=bdms)
2506 
2507         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2508         #                want to keep ip allocated for certain failures
2509         try:
2510             LOG.debug('Start destroying the instance on the hypervisor.',
2511                       instance=instance)
2512             with timeutils.StopWatch() as timer:
2513                 self.driver.destroy(context, instance, network_info,
2514                                     block_device_info)
2515             LOG.info('Took %0.2f seconds to destroy the instance on the '
2516                      'hypervisor.', timer.elapsed(), instance=instance)
2517         except exception.InstancePowerOffFailure:
2518             # if the instance can't power off, don't release the ip
2519             with excutils.save_and_reraise_exception():
2520                 pass
2521         except Exception:
2522             with excutils.save_and_reraise_exception():
2523                 # deallocate ip and fail without proceeding to
2524                 # volume api calls, preserving current behavior
2525                 if try_deallocate_networks:
2526                     self._try_deallocate_network(context, instance,
2527                                                  requested_networks)
2528 
2529         if try_deallocate_networks:
2530             self._try_deallocate_network(context, instance, requested_networks)
2531 
2532         timer.restart()
2533         for bdm in vol_bdms:
2534             try:
2535                 if bdm.attachment_id:
2536                     self.volume_api.attachment_delete(context,
2537                                                       bdm.attachment_id)
2538                 else:
2539                     # NOTE(vish): actual driver detach done in driver.destroy,
2540                     #             so just tell cinder that we are done with it.
2541                     connector = self.driver.get_volume_connector(instance)
2542                     self.volume_api.terminate_connection(context,
2543                                                          bdm.volume_id,
2544                                                          connector)
2545                     self.volume_api.detach(context, bdm.volume_id,
2546                                            instance.uuid)
2547 
2548             except exception.VolumeAttachmentNotFound as exc:
2549                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2550                           instance=instance)
2551             except exception.DiskNotFound as exc:
2552                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2553                           instance=instance)
2554             except exception.VolumeNotFound as exc:
2555                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2556                           instance=instance)
2557             except (cinder_exception.EndpointNotFound,
2558                     keystone_exception.EndpointNotFound) as exc:
2559                 LOG.warning('Ignoring EndpointNotFound for '
2560                             'volume %(volume_id)s: %(exc)s',
2561                             {'exc': exc, 'volume_id': bdm.volume_id},
2562                             instance=instance)
2563             except cinder_exception.ClientException as exc:
2564                 LOG.warning('Ignoring unknown cinder exception for '
2565                             'volume %(volume_id)s: %(exc)s',
2566                             {'exc': exc, 'volume_id': bdm.volume_id},
2567                             instance=instance)
2568             except Exception as exc:
2569                 LOG.warning('Ignoring unknown exception for '
2570                             'volume %(volume_id)s: %(exc)s',
2571                             {'exc': exc, 'volume_id': bdm.volume_id},
2572                             instance=instance)
2573         if vol_bdms:
2574             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2575                      'for instance.',
2576                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2577                      instance=instance)
2578 
2579         if notify:
2580             self._notify_about_instance_usage(context, instance,
2581                                               "shutdown.end")
2582             compute_utils.notify_about_instance_action(context, instance,
2583                     self.host, action=fields.NotificationAction.SHUTDOWN,
2584                     phase=fields.NotificationPhase.END, bdms=bdms)
2585 
2586     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2587                          detach=True):
2588         exc_info = None
2589         for bdm in bdms:
2590             if detach and bdm.volume_id:
2591                 try:
2592                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2593                               instance_uuid=instance.uuid)
2594                     destroy = bdm.delete_on_termination
2595                     self._detach_volume(context, bdm, instance,
2596                                         destroy_bdm=destroy)
2597                 except Exception as exc:
2598                     exc_info = sys.exc_info()
2599                     LOG.warning('Failed to detach volume: %(volume_id)s '
2600                                 'due to %(exc)s',
2601                                 {'volume_id': bdm.volume_id, 'exc': exc})
2602 
2603             if bdm.volume_id and bdm.delete_on_termination:
2604                 try:
2605                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2606                               instance_uuid=instance.uuid)
2607                     self.volume_api.delete(context, bdm.volume_id)
2608                 except Exception as exc:
2609                     exc_info = sys.exc_info()
2610                     LOG.warning('Failed to delete volume: %(volume_id)s '
2611                                 'due to %(exc)s',
2612                                 {'volume_id': bdm.volume_id, 'exc': exc})
2613         if exc_info is not None and raise_exc:
2614             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2615 
2616     @hooks.add_hook("delete_instance")
2617     def _delete_instance(self, context, instance, bdms):
2618         """Delete an instance on this host.
2619 
2620         :param context: nova request context
2621         :param instance: nova.objects.instance.Instance object
2622         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2623         """
2624         events = self.instance_events.clear_events_for_instance(instance)
2625         if events:
2626             LOG.debug('Events pending at deletion: %(events)s',
2627                       {'events': ','.join(events.keys())},
2628                       instance=instance)
2629         self._notify_about_instance_usage(context, instance,
2630                                           "delete.start")
2631         compute_utils.notify_about_instance_action(context, instance,
2632                 self.host, action=fields.NotificationAction.DELETE,
2633                 phase=fields.NotificationPhase.START, bdms=bdms)
2634 
2635         self._shutdown_instance(context, instance, bdms)
2636 
2637         # NOTE(vish): We have already deleted the instance, so we have
2638         #             to ignore problems cleaning up the volumes. It
2639         #             would be nice to let the user know somehow that
2640         #             the volume deletion failed, but it is not
2641         #             acceptable to have an instance that can not be
2642         #             deleted. Perhaps this could be reworked in the
2643         #             future to set an instance fault the first time
2644         #             and to only ignore the failure if the instance
2645         #             is already in ERROR.
2646 
2647         # NOTE(ameeda): The volumes already detached during the above
2648         #               _shutdown_instance() call and this is why
2649         #               detach is not requested from _cleanup_volumes()
2650         #               in this case
2651 
2652         self._cleanup_volumes(context, instance, bdms,
2653                 raise_exc=False, detach=False)
2654         # if a delete task succeeded, always update vm state and task
2655         # state without expecting task state to be DELETING
2656         instance.vm_state = vm_states.DELETED
2657         instance.task_state = None
2658         instance.power_state = power_state.NOSTATE
2659         instance.terminated_at = timeutils.utcnow()
2660         instance.save()
2661 
2662         self._complete_deletion(context, instance)
2663         # only destroy the instance in the db if the _complete_deletion
2664         # doesn't raise and therefore allocation is successfully
2665         # deleted in placement
2666         instance.destroy()
2667 
2668         self._notify_about_instance_usage(context, instance, "delete.end")
2669         compute_utils.notify_about_instance_action(context, instance,
2670                 self.host, action=fields.NotificationAction.DELETE,
2671                 phase=fields.NotificationPhase.END, bdms=bdms)
2672 
2673     @wrap_exception()
2674     @reverts_task_state
2675     @wrap_instance_event(prefix='compute')
2676     @wrap_instance_fault
2677     def terminate_instance(self, context, instance, bdms):
2678         """Terminate an instance on this host."""
2679         @utils.synchronized(instance.uuid)
2680         def do_terminate_instance(instance, bdms):
2681             # NOTE(mriedem): If we are deleting the instance while it was
2682             # booting from volume, we could be racing with a database update of
2683             # the BDM volume_id. Since the compute API passes the BDMs over RPC
2684             # to compute here, the BDMs may be stale at this point. So check
2685             # for any volume BDMs that don't have volume_id set and if we
2686             # detect that, we need to refresh the BDM list before proceeding.
2687             # TODO(mriedem): Move this into _delete_instance and make the bdms
2688             # parameter optional.
2689             for bdm in list(bdms):
2690                 if bdm.is_volume and not bdm.volume_id:
2691                     LOG.debug('There are potentially stale BDMs during '
2692                               'delete, refreshing the BlockDeviceMappingList.',
2693                               instance=instance)
2694                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2695                         context, instance.uuid)
2696                     break
2697             try:
2698                 self._delete_instance(context, instance, bdms)
2699             except exception.InstanceNotFound:
2700                 LOG.info("Instance disappeared during terminate",
2701                          instance=instance)
2702             except Exception:
2703                 # As we're trying to delete always go to Error if something
2704                 # goes wrong that _delete_instance can't handle.
2705                 with excutils.save_and_reraise_exception():
2706                     LOG.exception('Setting instance vm_state to ERROR',
2707                                   instance=instance)
2708                     self._set_instance_obj_error_state(context, instance)
2709 
2710         do_terminate_instance(instance, bdms)
2711 
2712     # NOTE(johannes): This is probably better named power_off_instance
2713     # so it matches the driver method, but because of other issues, we
2714     # can't use that name in grizzly.
2715     @wrap_exception()
2716     @reverts_task_state
2717     @wrap_instance_event(prefix='compute')
2718     @wrap_instance_fault
2719     def stop_instance(self, context, instance, clean_shutdown):
2720         """Stopping an instance on this host."""
2721 
2722         @utils.synchronized(instance.uuid)
2723         def do_stop_instance():
2724             current_power_state = self._get_power_state(context, instance)
2725             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
2726                       'current task_state: %(task_state)s, current DB '
2727                       'power_state: %(db_power_state)s, current VM '
2728                       'power_state: %(current_power_state)s',
2729                       {'vm_state': instance.vm_state,
2730                        'task_state': instance.task_state,
2731                        'db_power_state': instance.power_state,
2732                        'current_power_state': current_power_state},
2733                       instance_uuid=instance.uuid)
2734 
2735             # NOTE(mriedem): If the instance is already powered off, we are
2736             # possibly tearing down and racing with other operations, so we can
2737             # expect the task_state to be None if something else updates the
2738             # instance and we're not locking it.
2739             expected_task_state = [task_states.POWERING_OFF]
2740             # The list of power states is from _sync_instance_power_state.
2741             if current_power_state in (power_state.NOSTATE,
2742                                        power_state.SHUTDOWN,
2743                                        power_state.CRASHED):
2744                 LOG.info('Instance is already powered off in the '
2745                          'hypervisor when stop is called.',
2746                          instance=instance)
2747                 expected_task_state.append(None)
2748 
2749             self._notify_about_instance_usage(context, instance,
2750                                               "power_off.start")
2751 
2752             compute_utils.notify_about_instance_action(context, instance,
2753                         self.host, action=fields.NotificationAction.POWER_OFF,
2754                         phase=fields.NotificationPhase.START)
2755 
2756             self._power_off_instance(context, instance, clean_shutdown)
2757             instance.power_state = self._get_power_state(context, instance)
2758             instance.vm_state = vm_states.STOPPED
2759             instance.task_state = None
2760             instance.save(expected_task_state=expected_task_state)
2761             self._notify_about_instance_usage(context, instance,
2762                                               "power_off.end")
2763 
2764             compute_utils.notify_about_instance_action(context, instance,
2765                         self.host, action=fields.NotificationAction.POWER_OFF,
2766                         phase=fields.NotificationPhase.END)
2767 
2768         do_stop_instance()
2769 
2770     def _power_on(self, context, instance):
2771         network_info = self.network_api.get_instance_nw_info(context, instance)
2772         block_device_info = self._get_instance_block_device_info(context,
2773                                                                  instance)
2774         self.driver.power_on(context, instance,
2775                              network_info,
2776                              block_device_info)
2777 
2778     def _delete_snapshot_of_shelved_instance(self, context, instance,
2779                                              snapshot_id):
2780         """Delete snapshot of shelved instance."""
2781         try:
2782             self.image_api.delete(context, snapshot_id)
2783         except (exception.ImageNotFound,
2784                 exception.ImageNotAuthorized) as exc:
2785             LOG.warning("Failed to delete snapshot "
2786                         "from shelved instance (%s).",
2787                         exc.format_message(), instance=instance)
2788         except Exception:
2789             LOG.exception("Something wrong happened when trying to "
2790                           "delete snapshot from shelved instance.",
2791                           instance=instance)
2792 
2793     # NOTE(johannes): This is probably better named power_on_instance
2794     # so it matches the driver method, but because of other issues, we
2795     # can't use that name in grizzly.
2796     @wrap_exception()
2797     @reverts_task_state
2798     @wrap_instance_event(prefix='compute')
2799     @wrap_instance_fault
2800     def start_instance(self, context, instance):
2801         """Starting an instance on this host."""
2802         self._notify_about_instance_usage(context, instance, "power_on.start")
2803         compute_utils.notify_about_instance_action(context, instance,
2804             self.host, action=fields.NotificationAction.POWER_ON,
2805             phase=fields.NotificationPhase.START)
2806         self._power_on(context, instance)
2807         instance.power_state = self._get_power_state(context, instance)
2808         instance.vm_state = vm_states.ACTIVE
2809         instance.task_state = None
2810 
2811         # Delete an image(VM snapshot) for a shelved instance
2812         snapshot_id = instance.system_metadata.get('shelved_image_id')
2813         if snapshot_id:
2814             self._delete_snapshot_of_shelved_instance(context, instance,
2815                                                       snapshot_id)
2816 
2817         # Delete system_metadata for a shelved instance
2818         compute_utils.remove_shelved_keys_from_system_metadata(instance)
2819 
2820         instance.save(expected_task_state=task_states.POWERING_ON)
2821         self._notify_about_instance_usage(context, instance, "power_on.end")
2822         compute_utils.notify_about_instance_action(context, instance,
2823             self.host, action=fields.NotificationAction.POWER_ON,
2824             phase=fields.NotificationPhase.END)
2825 
2826     @messaging.expected_exceptions(NotImplementedError,
2827                                    exception.TriggerCrashDumpNotSupported,
2828                                    exception.InstanceNotRunning)
2829     @wrap_exception()
2830     @wrap_instance_event(prefix='compute')
2831     @wrap_instance_fault
2832     def trigger_crash_dump(self, context, instance):
2833         """Trigger crash dump in an instance."""
2834 
2835         self._notify_about_instance_usage(context, instance,
2836                                           "trigger_crash_dump.start")
2837         compute_utils.notify_about_instance_action(context, instance,
2838                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
2839                 phase=fields.NotificationPhase.START)
2840 
2841         # This method does not change task_state and power_state because the
2842         # effect of a trigger depends on user's configuration.
2843         self.driver.trigger_crash_dump(instance)
2844 
2845         self._notify_about_instance_usage(context, instance,
2846                                           "trigger_crash_dump.end")
2847         compute_utils.notify_about_instance_action(context, instance,
2848                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
2849                 phase=fields.NotificationPhase.END)
2850 
2851     @wrap_exception()
2852     @reverts_task_state
2853     @wrap_instance_event(prefix='compute')
2854     @wrap_instance_fault
2855     def soft_delete_instance(self, context, instance):
2856         """Soft delete an instance on this host."""
2857         with compute_utils.notify_about_instance_delete(
2858                 self.notifier, context, instance, 'soft_delete',
2859                 source=fields.NotificationSource.COMPUTE):
2860             try:
2861                 self.driver.soft_delete(instance)
2862             except NotImplementedError:
2863                 # Fallback to just powering off the instance if the
2864                 # hypervisor doesn't implement the soft_delete method
2865                 self.driver.power_off(instance)
2866             instance.power_state = self._get_power_state(context, instance)
2867             instance.vm_state = vm_states.SOFT_DELETED
2868             instance.task_state = None
2869             instance.save(expected_task_state=[task_states.SOFT_DELETING])
2870 
2871     @wrap_exception()
2872     @reverts_task_state
2873     @wrap_instance_event(prefix='compute')
2874     @wrap_instance_fault
2875     def restore_instance(self, context, instance):
2876         """Restore a soft-deleted instance on this host."""
2877         self._notify_about_instance_usage(context, instance, "restore.start")
2878         compute_utils.notify_about_instance_action(context, instance,
2879             self.host, action=fields.NotificationAction.RESTORE,
2880             phase=fields.NotificationPhase.START)
2881         try:
2882             self.driver.restore(instance)
2883         except NotImplementedError:
2884             # Fallback to just powering on the instance if the hypervisor
2885             # doesn't implement the restore method
2886             self._power_on(context, instance)
2887         instance.power_state = self._get_power_state(context, instance)
2888         instance.vm_state = vm_states.ACTIVE
2889         instance.task_state = None
2890         instance.save(expected_task_state=task_states.RESTORING)
2891         self._notify_about_instance_usage(context, instance, "restore.end")
2892         compute_utils.notify_about_instance_action(context, instance,
2893             self.host, action=fields.NotificationAction.RESTORE,
2894             phase=fields.NotificationPhase.END)
2895 
2896     @staticmethod
2897     def _set_migration_status(migration, status):
2898         """Set the status, and guard against a None being passed in.
2899 
2900         This is useful as some of the compute RPC calls will not pass
2901         a migration object in older versions. The check can be removed when
2902         we move past 4.x major version of the RPC API.
2903         """
2904         if migration:
2905             migration.status = status
2906             migration.save()
2907 
2908     def _rebuild_default_impl(self, context, instance, image_meta,
2909                               injected_files, admin_password, allocations,
2910                               bdms, detach_block_devices, attach_block_devices,
2911                               network_info=None,
2912                               evacuate=False, block_device_info=None,
2913                               preserve_ephemeral=False):
2914         if preserve_ephemeral:
2915             # The default code path does not support preserving ephemeral
2916             # partitions.
2917             raise exception.PreserveEphemeralNotSupported()
2918 
2919         if evacuate:
2920             detach_block_devices(context, bdms)
2921         else:
2922             self._power_off_instance(context, instance, clean_shutdown=True)
2923             detach_block_devices(context, bdms)
2924             self.driver.destroy(context, instance,
2925                                 network_info=network_info,
2926                                 block_device_info=block_device_info)
2927 
2928         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
2929         instance.save(expected_task_state=[task_states.REBUILDING])
2930 
2931         new_block_device_info = attach_block_devices(context, instance, bdms)
2932 
2933         instance.task_state = task_states.REBUILD_SPAWNING
2934         instance.save(
2935             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
2936 
2937         with instance.mutated_migration_context():
2938             self.driver.spawn(context, instance, image_meta, injected_files,
2939                               admin_password, allocations,
2940                               network_info=network_info,
2941                               block_device_info=new_block_device_info)
2942 
2943     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
2944         tb = traceback.format_exc()
2945         self._notify_about_instance_usage(context, instance,
2946                                           'rebuild.error', fault=error)
2947         compute_utils.notify_about_instance_rebuild(
2948             context, instance, self.host,
2949             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms,
2950             tb=tb)
2951 
2952     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
2953     @wrap_exception()
2954     @reverts_task_state
2955     @wrap_instance_event(prefix='compute')
2956     @wrap_instance_fault
2957     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
2958                          injected_files, new_pass, orig_sys_metadata,
2959                          bdms, recreate, on_shared_storage,
2960                          preserve_ephemeral, migration,
2961                          scheduled_node, limits, request_spec):
2962         """Destroy and re-make this instance.
2963 
2964         A 'rebuild' effectively purges all existing data from the system and
2965         remakes the VM with given 'metadata' and 'personalities'.
2966 
2967         :param context: `nova.RequestContext` object
2968         :param instance: Instance object
2969         :param orig_image_ref: Original image_ref before rebuild
2970         :param image_ref: New image_ref for rebuild
2971         :param injected_files: Files to inject
2972         :param new_pass: password to set on rebuilt instance
2973         :param orig_sys_metadata: instance system metadata from pre-rebuild
2974         :param bdms: block-device-mappings to use for rebuild
2975         :param recreate: True if the instance is being recreated (e.g. the
2976             hypervisor it was on failed) - cleanup of old state will be
2977             skipped.
2978         :param on_shared_storage: True if instance files on shared storage.
2979                                   If not provided then information from the
2980                                   driver will be used to decide if the instance
2981                                   files are available or not on the target host
2982         :param preserve_ephemeral: True if the default ephemeral storage
2983                                    partition must be preserved on rebuild
2984         :param migration: a Migration object if one was created for this
2985                           rebuild operation (if it's a part of evacuate)
2986         :param scheduled_node: A node of the host chosen by the scheduler. If a
2987                                host was specified by the user, this will be
2988                                None
2989         :param limits: Overcommit limits set by the scheduler. If a host was
2990                        specified by the user, this will be None
2991         :param request_spec: a RequestSpec object used to schedule the instance
2992 
2993         """
2994         # recreate=True means the instance is being evacuated from a failed
2995         # host to a new destination host (this host). The 'recreate' variable
2996         # name is confusing, so rename it to evacuate here at the top, which
2997         # is simpler than renaming a parameter in an RPC versioned method.
2998         evacuate = recreate
2999         context = context.elevated()
3000 
3001         if evacuate:
3002             LOG.info("Evacuating instance", instance=instance)
3003         else:
3004             LOG.info("Rebuilding instance", instance=instance)
3005 
3006         if evacuate:
3007             # This is an evacuation to a new host, so we need to perform a
3008             # resource claim.
3009             rebuild_claim = self.rt.rebuild_claim
3010         else:
3011             # This is a rebuild to the same host, so we don't need to make
3012             # a claim since the instance is already on this host.
3013             rebuild_claim = claims.NopClaim
3014 
3015         if image_ref:
3016             image_meta = objects.ImageMeta.from_image_ref(
3017                 context, self.image_api, image_ref)
3018         elif evacuate:
3019             # For evacuate the API does not send down the image_ref since the
3020             # image does not change so just get it from what was stashed in
3021             # the instance system_metadata when the instance was created (or
3022             # last rebuilt). This also works for volume-backed instances.
3023             image_meta = instance.image_meta
3024         else:
3025             image_meta = objects.ImageMeta()
3026 
3027         # NOTE(mriedem): On an evacuate, we need to update
3028         # the instance's host and node properties to reflect it's
3029         # destination node for the evacuate.
3030         if not scheduled_node:
3031             if evacuate:
3032                 try:
3033                     compute_node = self._get_compute_info(context, self.host)
3034                     scheduled_node = compute_node.hypervisor_hostname
3035                 except exception.ComputeHostNotFound:
3036                     LOG.exception('Failed to get compute_info for %s',
3037                                   self.host)
3038             else:
3039                 scheduled_node = instance.node
3040 
3041         with self._error_out_instance_on_exception(context, instance):
3042             try:
3043                 claim_ctxt = rebuild_claim(
3044                     context, instance, scheduled_node,
3045                     limits=limits, image_meta=image_meta,
3046                     migration=migration)
3047                 self._do_rebuild_instance_with_claim(
3048                     claim_ctxt, context, instance, orig_image_ref,
3049                     image_meta, injected_files, new_pass, orig_sys_metadata,
3050                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3051                     migration, request_spec)
3052             except (exception.ComputeResourcesUnavailable,
3053                     exception.RescheduledException) as e:
3054                 if isinstance(e, exception.ComputeResourcesUnavailable):
3055                     LOG.debug("Could not rebuild instance on this host, not "
3056                               "enough resources available.", instance=instance)
3057                 else:
3058                     # RescheduledException is raised by the late server group
3059                     # policy check during evacuation if a parallel scheduling
3060                     # violated the policy.
3061                     # We catch the RescheduledException here but we don't have
3062                     # the plumbing to do an actual reschedule so we abort the
3063                     # operation.
3064                     LOG.debug("Could not rebuild instance on this host, "
3065                               "late server group check failed.",
3066                               instance=instance)
3067                 # NOTE(ndipanov): We just abort the build for now and leave a
3068                 # migration record for potential cleanup later
3069                 self._set_migration_status(migration, 'failed')
3070                 # Since the claim failed, we need to remove the allocation
3071                 # created against the destination node. Note that we can only
3072                 # get here when evacuating to a destination node. Rebuilding
3073                 # on the same host (not evacuate) uses the NopClaim which will
3074                 # not raise ComputeResourcesUnavailable.
3075                 self.rt.delete_allocation_for_evacuated_instance(
3076                     context, instance, scheduled_node, node_type='destination')
3077                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3078                 raise exception.BuildAbortException(
3079                     instance_uuid=instance.uuid, reason=e.format_message())
3080             except (exception.InstanceNotFound,
3081                     exception.UnexpectedDeletingTaskStateError) as e:
3082                 LOG.debug('Instance was deleted while rebuilding',
3083                           instance=instance)
3084                 self._set_migration_status(migration, 'failed')
3085                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3086             except Exception as e:
3087                 self._set_migration_status(migration, 'failed')
3088                 if evacuate or scheduled_node is not None:
3089                     self.rt.delete_allocation_for_evacuated_instance(
3090                         context, instance, scheduled_node,
3091                         node_type='destination')
3092                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3093                 raise
3094             else:
3095                 instance.apply_migration_context()
3096                 # NOTE (ndipanov): This save will now update the host and node
3097                 # attributes making sure that next RT pass is consistent since
3098                 # it will be based on the instance and not the migration DB
3099                 # entry.
3100                 instance.host = self.host
3101                 instance.node = scheduled_node
3102                 instance.save()
3103                 instance.drop_migration_context()
3104 
3105                 # NOTE (ndipanov): Mark the migration as done only after we
3106                 # mark the instance as belonging to this host.
3107                 self._set_migration_status(migration, 'done')
3108 
3109     def _do_rebuild_instance_with_claim(self, claim_context, *args, **kwargs):
3110         """Helper to avoid deep nesting in the top-level method."""
3111 
3112         with claim_context:
3113             self._do_rebuild_instance(*args, **kwargs)
3114 
3115     @staticmethod
3116     def _get_image_name(image_meta):
3117         if image_meta.obj_attr_is_set("name"):
3118             return image_meta.name
3119         else:
3120             return ''
3121 
3122     def _do_rebuild_instance(self, context, instance, orig_image_ref,
3123                              image_meta, injected_files, new_pass,
3124                              orig_sys_metadata, bdms, evacuate,
3125                              on_shared_storage, preserve_ephemeral,
3126                              migration, request_spec):
3127         orig_vm_state = instance.vm_state
3128 
3129         if evacuate:
3130             if request_spec:
3131                 # NOTE(gibi): Do a late check of server group policy as
3132                 # parallel scheduling could violate such policy. This will
3133                 # cause the evacuate to fail as rebuild does not implement
3134                 # reschedule.
3135                 hints = self._get_scheduler_hints({}, request_spec)
3136                 self._validate_instance_group_policy(context, instance, hints)
3137 
3138             if not self.driver.capabilities.get("supports_evacuate", False):
3139                 raise exception.InstanceEvacuateNotSupported
3140 
3141             self._check_instance_exists(context, instance)
3142 
3143             if on_shared_storage is None:
3144                 LOG.debug('on_shared_storage is not provided, using driver '
3145                           'information to decide if the instance needs to '
3146                           'be evacuated')
3147                 on_shared_storage = self.driver.instance_on_disk(instance)
3148 
3149             elif (on_shared_storage !=
3150                     self.driver.instance_on_disk(instance)):
3151                 # To cover case when admin expects that instance files are
3152                 # on shared storage, but not accessible and vice versa
3153                 raise exception.InvalidSharedStorage(
3154                         _("Invalid state of instance files on shared"
3155                             " storage"))
3156 
3157             if on_shared_storage:
3158                 LOG.info('disk on shared storage, evacuating using'
3159                          ' existing disk')
3160             elif instance.image_ref:
3161                 orig_image_ref = instance.image_ref
3162                 LOG.info("disk not on shared storage, evacuating from "
3163                          "image: '%s'", str(orig_image_ref))
3164             else:
3165                 LOG.info('disk on volume, evacuating using existing '
3166                          'volume')
3167 
3168         # We check trusted certs capabilities for both evacuate (rebuild on
3169         # another host) and rebuild (rebuild on the same host) because for
3170         # evacuate we need to make sure an instance with trusted certs can
3171         # have the image verified with those certs during rebuild, and for
3172         # rebuild we could be rebuilding a server that started out with no
3173         # trusted certs on this host, and then was rebuilt with trusted certs
3174         # for a new image, in which case we need to validate that new image
3175         # with the trusted certs during the rebuild.
3176         self._check_trusted_certs(instance)
3177 
3178         # This instance.exists message should contain the original
3179         # image_ref, not the new one.  Since the DB has been updated
3180         # to point to the new one... we have to override it.
3181         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3182                                                                context)
3183         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3184         compute_utils.notify_usage_exists(
3185                 self.notifier, context, instance, self.host,
3186                 current_period=True, system_metadata=orig_sys_metadata,
3187                 extra_usage_info=extra_usage_info)
3188 
3189         # This message should contain the new image_ref
3190         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3191         self._notify_about_instance_usage(context, instance,
3192                 "rebuild.start", extra_usage_info=extra_usage_info)
3193         # NOTE: image_name is not included in the versioned notification
3194         # because we already provide the image_uuid in the notification
3195         # payload and the image details can be looked up via the uuid.
3196         compute_utils.notify_about_instance_rebuild(
3197             context, instance, self.host,
3198             phase=fields.NotificationPhase.START,
3199             bdms=bdms)
3200 
3201         instance.power_state = self._get_power_state(context, instance)
3202         instance.task_state = task_states.REBUILDING
3203         instance.save(expected_task_state=[task_states.REBUILDING])
3204 
3205         if evacuate:
3206             self.network_api.setup_networks_on_host(
3207                     context, instance, self.host)
3208             # For nova-network this is needed to move floating IPs
3209             # For neutron this updates the host in the port binding
3210             # TODO(cfriesen): this network_api call and the one above
3211             # are so similar, we should really try to unify them.
3212             self.network_api.setup_instance_network_on_host(
3213                     context, instance, self.host, migration)
3214             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3215             # with @base_api.refresh_cache and then we wouldn't need this
3216             # explicit call to get_instance_nw_info.
3217             network_info = self.network_api.get_instance_nw_info(context,
3218                                                                  instance)
3219         else:
3220             network_info = instance.get_network_info()
3221 
3222         allocations = self.reportclient.get_allocations_for_consumer(
3223             context, instance.uuid)
3224 
3225         if bdms is None:
3226             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3227                     context, instance.uuid)
3228 
3229         block_device_info = \
3230             self._get_instance_block_device_info(
3231                     context, instance, bdms=bdms)
3232 
3233         def detach_block_devices(context, bdms):
3234             for bdm in bdms:
3235                 if bdm.is_volume:
3236                     # NOTE (ildikov): Having the attachment_id set in the BDM
3237                     # means that it's the new Cinder attach/detach flow
3238                     # (available from v3.44). In that case we explicitly
3239                     # attach and detach the volumes through attachment level
3240                     # operations. In this scenario _detach_volume will delete
3241                     # the existing attachment which would make the volume
3242                     # status change to 'available' if we don't pre-create
3243                     # another empty attachment before deleting the old one.
3244                     attachment_id = None
3245                     if bdm.attachment_id:
3246                         attachment_id = self.volume_api.attachment_create(
3247                             context, bdm['volume_id'], instance.uuid)['id']
3248                     self._detach_volume(context, bdm, instance,
3249                                         destroy_bdm=False)
3250                     if attachment_id:
3251                         bdm.attachment_id = attachment_id
3252                         bdm.save()
3253 
3254         files = self._decode_files(injected_files)
3255 
3256         kwargs = dict(
3257             context=context,
3258             instance=instance,
3259             image_meta=image_meta,
3260             injected_files=files,
3261             admin_password=new_pass,
3262             allocations=allocations,
3263             bdms=bdms,
3264             detach_block_devices=detach_block_devices,
3265             attach_block_devices=self._prep_block_device,
3266             block_device_info=block_device_info,
3267             network_info=network_info,
3268             preserve_ephemeral=preserve_ephemeral,
3269             evacuate=evacuate)
3270         try:
3271             with instance.mutated_migration_context():
3272                 self.driver.rebuild(**kwargs)
3273         except NotImplementedError:
3274             # NOTE(rpodolyaka): driver doesn't provide specialized version
3275             # of rebuild, fall back to the default implementation
3276             self._rebuild_default_impl(**kwargs)
3277         self._update_instance_after_spawn(context, instance)
3278         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3279 
3280         if orig_vm_state == vm_states.STOPPED:
3281             LOG.info("bringing vm to original state: '%s'",
3282                      orig_vm_state, instance=instance)
3283             instance.vm_state = vm_states.ACTIVE
3284             instance.task_state = task_states.POWERING_OFF
3285             instance.progress = 0
3286             instance.save()
3287             self.stop_instance(context, instance, False)
3288         # TODO(melwitt): We should clean up instance console tokens here in the
3289         # case of evacuate. The instance is on a new host and will need to
3290         # establish a new console connection.
3291         self._update_scheduler_instance_info(context, instance)
3292         self._notify_about_instance_usage(
3293                 context, instance, "rebuild.end",
3294                 network_info=network_info,
3295                 extra_usage_info=extra_usage_info)
3296         compute_utils.notify_about_instance_rebuild(
3297             context, instance, self.host,
3298             phase=fields.NotificationPhase.END,
3299             bdms=bdms)
3300 
3301     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3302                                      block_device_info):
3303         """Handle cases where the virt-layer had to detach non-working volumes
3304         in order to complete an operation.
3305         """
3306         for bdm in block_device_info['block_device_mapping']:
3307             if bdm.get('mount_device') in bad_devices:
3308                 try:
3309                     volume_id = bdm['connection_info']['data']['volume_id']
3310                 except KeyError:
3311                     continue
3312 
3313                 # NOTE(sirp): ideally we'd just call
3314                 # `compute_api.detach_volume` here but since that hits the
3315                 # DB directly, that's off limits from within the
3316                 # compute-manager.
3317                 #
3318                 # API-detach
3319                 LOG.info("Detaching from volume api: %s", volume_id)
3320                 self.volume_api.begin_detaching(context, volume_id)
3321 
3322                 # Manager-detach
3323                 self.detach_volume(context, volume_id, instance)
3324 
3325     @wrap_exception()
3326     @reverts_task_state
3327     @wrap_instance_event(prefix='compute')
3328     @wrap_instance_fault
3329     def reboot_instance(self, context, instance, block_device_info,
3330                         reboot_type):
3331         """Reboot an instance on this host."""
3332         # acknowledge the request made it to the manager
3333         if reboot_type == "SOFT":
3334             instance.task_state = task_states.REBOOT_PENDING
3335             expected_states = task_states.soft_reboot_states
3336         else:
3337             instance.task_state = task_states.REBOOT_PENDING_HARD
3338             expected_states = task_states.hard_reboot_states
3339 
3340         context = context.elevated()
3341         LOG.info("Rebooting instance", instance=instance)
3342 
3343         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3344             context, instance.uuid)
3345         block_device_info = self._get_instance_block_device_info(
3346             context, instance, bdms=bdms)
3347 
3348         network_info = self.network_api.get_instance_nw_info(context, instance)
3349 
3350         self._notify_about_instance_usage(context, instance, "reboot.start")
3351         compute_utils.notify_about_instance_action(
3352             context, instance, self.host,
3353             action=fields.NotificationAction.REBOOT,
3354             phase=fields.NotificationPhase.START,
3355             bdms=bdms
3356         )
3357 
3358         instance.power_state = self._get_power_state(context, instance)
3359         instance.save(expected_task_state=expected_states)
3360 
3361         if instance.power_state != power_state.RUNNING:
3362             state = instance.power_state
3363             running = power_state.RUNNING
3364             LOG.warning('trying to reboot a non-running instance:'
3365                         ' (state: %(state)s expected: %(running)s)',
3366                         {'state': state, 'running': running},
3367                         instance=instance)
3368 
3369         def bad_volumes_callback(bad_devices):
3370             self._handle_bad_volumes_detached(
3371                     context, instance, bad_devices, block_device_info)
3372 
3373         try:
3374             # Don't change it out of rescue mode
3375             if instance.vm_state == vm_states.RESCUED:
3376                 new_vm_state = vm_states.RESCUED
3377             else:
3378                 new_vm_state = vm_states.ACTIVE
3379             new_power_state = None
3380             if reboot_type == "SOFT":
3381                 instance.task_state = task_states.REBOOT_STARTED
3382                 expected_state = task_states.REBOOT_PENDING
3383             else:
3384                 instance.task_state = task_states.REBOOT_STARTED_HARD
3385                 expected_state = task_states.REBOOT_PENDING_HARD
3386             instance.save(expected_task_state=expected_state)
3387             self.driver.reboot(context, instance,
3388                                network_info,
3389                                reboot_type,
3390                                block_device_info=block_device_info,
3391                                bad_volumes_callback=bad_volumes_callback)
3392 
3393         except Exception as error:
3394             with excutils.save_and_reraise_exception() as ctxt:
3395                 exc_info = sys.exc_info()
3396                 # if the reboot failed but the VM is running don't
3397                 # put it into an error state
3398                 new_power_state = self._get_power_state(context, instance)
3399                 if new_power_state == power_state.RUNNING:
3400                     LOG.warning('Reboot failed but instance is running',
3401                                 instance=instance)
3402                     compute_utils.add_instance_fault_from_exc(context,
3403                             instance, error, exc_info)
3404                     self._notify_about_instance_usage(context, instance,
3405                             'reboot.error', fault=error)
3406                     tb = traceback.format_exc()
3407                     compute_utils.notify_about_instance_action(
3408                         context, instance, self.host,
3409                         action=fields.NotificationAction.REBOOT,
3410                         phase=fields.NotificationPhase.ERROR,
3411                         exception=error, bdms=bdms, tb=tb
3412                     )
3413                     ctxt.reraise = False
3414                 else:
3415                     LOG.error('Cannot reboot instance: %s', error,
3416                               instance=instance)
3417                     self._set_instance_obj_error_state(context, instance)
3418 
3419         if not new_power_state:
3420             new_power_state = self._get_power_state(context, instance)
3421         try:
3422             instance.power_state = new_power_state
3423             instance.vm_state = new_vm_state
3424             instance.task_state = None
3425             instance.save()
3426         except exception.InstanceNotFound:
3427             LOG.warning("Instance disappeared during reboot",
3428                         instance=instance)
3429 
3430         self._notify_about_instance_usage(context, instance, "reboot.end")
3431         compute_utils.notify_about_instance_action(
3432             context, instance, self.host,
3433             action=fields.NotificationAction.REBOOT,
3434             phase=fields.NotificationPhase.END,
3435             bdms=bdms
3436         )
3437 
3438     @delete_image_on_error
3439     def _do_snapshot_instance(self, context, image_id, instance):
3440         self._snapshot_instance(context, image_id, instance,
3441                                 task_states.IMAGE_BACKUP)
3442 
3443     @wrap_exception()
3444     @reverts_task_state
3445     @wrap_instance_event(prefix='compute')
3446     @wrap_instance_fault
3447     def backup_instance(self, context, image_id, instance, backup_type,
3448                         rotation):
3449         """Backup an instance on this host.
3450 
3451         :param backup_type: daily | weekly
3452         :param rotation: int representing how many backups to keep around
3453         """
3454         self._do_snapshot_instance(context, image_id, instance)
3455         self._rotate_backups(context, instance, backup_type, rotation)
3456 
3457     @wrap_exception()
3458     @reverts_task_state
3459     @wrap_instance_event(prefix='compute')
3460     @wrap_instance_fault
3461     @delete_image_on_error
3462     def snapshot_instance(self, context, image_id, instance):
3463         """Snapshot an instance on this host.
3464 
3465         :param context: security context
3466         :param image_id: glance.db.sqlalchemy.models.Image.Id
3467         :param instance: a nova.objects.instance.Instance object
3468         """
3469         # NOTE(dave-mcnally) the task state will already be set by the api
3470         # but if the compute manager has crashed/been restarted prior to the
3471         # request getting here the task state may have been cleared so we set
3472         # it again and things continue normally
3473         try:
3474             instance.task_state = task_states.IMAGE_SNAPSHOT
3475             instance.save(
3476                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3477         except exception.InstanceNotFound:
3478             # possibility instance no longer exists, no point in continuing
3479             LOG.debug("Instance not found, could not set state %s "
3480                       "for instance.",
3481                       task_states.IMAGE_SNAPSHOT, instance=instance)
3482             return
3483 
3484         except exception.UnexpectedDeletingTaskStateError:
3485             LOG.debug("Instance being deleted, snapshot cannot continue",
3486                       instance=instance)
3487             return
3488 
3489         self._snapshot_instance(context, image_id, instance,
3490                                 task_states.IMAGE_SNAPSHOT)
3491 
3492     def _snapshot_instance(self, context, image_id, instance,
3493                            expected_task_state):
3494         context = context.elevated()
3495 
3496         instance.power_state = self._get_power_state(context, instance)
3497         try:
3498             instance.save()
3499 
3500             LOG.info('instance snapshotting', instance=instance)
3501 
3502             if instance.power_state != power_state.RUNNING:
3503                 state = instance.power_state
3504                 running = power_state.RUNNING
3505                 LOG.warning('trying to snapshot a non-running instance: '
3506                             '(state: %(state)s expected: %(running)s)',
3507                             {'state': state, 'running': running},
3508                             instance=instance)
3509 
3510             self._notify_about_instance_usage(
3511                 context, instance, "snapshot.start")
3512             compute_utils.notify_about_instance_snapshot(context, instance,
3513                 self.host, phase=fields.NotificationPhase.START,
3514                 snapshot_image_id=image_id)
3515 
3516             def update_task_state(task_state,
3517                                   expected_state=expected_task_state):
3518                 instance.task_state = task_state
3519                 instance.save(expected_task_state=expected_state)
3520 
3521             with timeutils.StopWatch() as timer:
3522                 self.driver.snapshot(context, instance, image_id,
3523                                      update_task_state)
3524             LOG.info('Took %0.2f seconds to snapshot the instance on '
3525                      'the hypervisor.', timer.elapsed(), instance=instance)
3526 
3527             instance.task_state = None
3528             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3529 
3530             self._notify_about_instance_usage(context, instance,
3531                                               "snapshot.end")
3532             compute_utils.notify_about_instance_snapshot(context, instance,
3533                 self.host, phase=fields.NotificationPhase.END,
3534                 snapshot_image_id=image_id)
3535         except (exception.InstanceNotFound,
3536                 exception.UnexpectedDeletingTaskStateError):
3537             # the instance got deleted during the snapshot
3538             # Quickly bail out of here
3539             msg = 'Instance disappeared during snapshot'
3540             LOG.debug(msg, instance=instance)
3541             try:
3542                 image = self.image_api.get(context, image_id)
3543                 if image['status'] != 'active':
3544                     self.image_api.delete(context, image_id)
3545             except exception.ImageNotFound:
3546                 LOG.debug('Image not found during clean up %s', image_id)
3547             except Exception:
3548                 LOG.warning("Error while trying to clean up image %s",
3549                             image_id, instance=instance)
3550         except exception.ImageNotFound:
3551             instance.task_state = None
3552             instance.save()
3553             LOG.warning("Image not found during snapshot", instance=instance)
3554 
3555     def _post_interrupted_snapshot_cleanup(self, context, instance):
3556         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3557 
3558     @messaging.expected_exceptions(NotImplementedError)
3559     @wrap_exception()
3560     def volume_snapshot_create(self, context, instance, volume_id,
3561                                create_info):
3562         self.driver.volume_snapshot_create(context, instance, volume_id,
3563                                            create_info)
3564 
3565     @messaging.expected_exceptions(NotImplementedError)
3566     @wrap_exception()
3567     def volume_snapshot_delete(self, context, instance, volume_id,
3568                                snapshot_id, delete_info):
3569         self.driver.volume_snapshot_delete(context, instance, volume_id,
3570                                            snapshot_id, delete_info)
3571 
3572     @wrap_instance_fault
3573     def _rotate_backups(self, context, instance, backup_type, rotation):
3574         """Delete excess backups associated to an instance.
3575 
3576         Instances are allowed a fixed number of backups (the rotation number);
3577         this method deletes the oldest backups that exceed the rotation
3578         threshold.
3579 
3580         :param context: security context
3581         :param instance: Instance dict
3582         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3583         :param rotation: int representing how many backups to keep around;
3584             None if rotation shouldn't be used (as in the case of snapshots)
3585         """
3586         filters = {'property-image_type': 'backup',
3587                    'property-backup_type': backup_type,
3588                    'property-instance_uuid': instance.uuid}
3589 
3590         images = self.image_api.get_all(context, filters=filters,
3591                                         sort_key='created_at', sort_dir='desc')
3592         num_images = len(images)
3593         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3594                   {'num_images': num_images, 'rotation': rotation},
3595                   instance=instance)
3596 
3597         if num_images > rotation:
3598             # NOTE(sirp): this deletes all backups that exceed the rotation
3599             # limit
3600             excess = len(images) - rotation
3601             LOG.debug("Rotating out %d backups", excess,
3602                       instance=instance)
3603             for i in range(excess):
3604                 image = images.pop()
3605                 image_id = image['id']
3606                 LOG.debug("Deleting image %s", image_id,
3607                           instance=instance)
3608                 try:
3609                     self.image_api.delete(context, image_id)
3610                 except exception.ImageNotFound:
3611                     LOG.info("Failed to find image %(image_id)s to "
3612                              "delete", {'image_id': image_id},
3613                              instance=instance)
3614                 except (exception.ImageDeleteConflict, Exception) as exc:
3615                     LOG.info("Failed to delete image %(image_id)s during "
3616                              "deleting excess backups. "
3617                              "Continuing for next image.. %(exc)s",
3618                              {'image_id': image_id, 'exc': exc},
3619                              instance=instance)
3620 
3621     @wrap_exception()
3622     @reverts_task_state
3623     @wrap_instance_event(prefix='compute')
3624     @wrap_instance_fault
3625     def set_admin_password(self, context, instance, new_pass):
3626         """Set the root/admin password for an instance on this host.
3627 
3628         This is generally only called by API password resets after an
3629         image has been built.
3630 
3631         @param context: Nova auth context.
3632         @param instance: Nova instance object.
3633         @param new_pass: The admin password for the instance.
3634         """
3635 
3636         context = context.elevated()
3637         if new_pass is None:
3638             # Generate a random password
3639             new_pass = utils.generate_password()
3640 
3641         current_power_state = self._get_power_state(context, instance)
3642         expected_state = power_state.RUNNING
3643 
3644         if current_power_state != expected_state:
3645             instance.task_state = None
3646             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
3647             _msg = _('instance %s is not running') % instance.uuid
3648             raise exception.InstancePasswordSetFailed(
3649                 instance=instance.uuid, reason=_msg)
3650 
3651         try:
3652             self.driver.set_admin_password(instance, new_pass)
3653             LOG.info("Admin password set", instance=instance)
3654             instance.task_state = None
3655             instance.save(
3656                 expected_task_state=task_states.UPDATING_PASSWORD)
3657         except exception.InstanceAgentNotEnabled:
3658             with excutils.save_and_reraise_exception():
3659                 LOG.debug('Guest agent is not enabled for the instance.',
3660                           instance=instance)
3661                 instance.task_state = None
3662                 instance.save(
3663                     expected_task_state=task_states.UPDATING_PASSWORD)
3664         except exception.SetAdminPasswdNotSupported:
3665             with excutils.save_and_reraise_exception():
3666                 LOG.info('set_admin_password is not supported '
3667                          'by this driver or guest instance.',
3668                          instance=instance)
3669                 instance.task_state = None
3670                 instance.save(
3671                     expected_task_state=task_states.UPDATING_PASSWORD)
3672         except NotImplementedError:
3673             LOG.warning('set_admin_password is not implemented '
3674                         'by this driver or guest instance.',
3675                         instance=instance)
3676             instance.task_state = None
3677             instance.save(
3678                 expected_task_state=task_states.UPDATING_PASSWORD)
3679             raise NotImplementedError(_('set_admin_password is not '
3680                                         'implemented by this driver or guest '
3681                                         'instance.'))
3682         except exception.UnexpectedTaskStateError:
3683             # interrupted by another (most likely delete) task
3684             # do not retry
3685             raise
3686         except Exception:
3687             # Catch all here because this could be anything.
3688             LOG.exception('set_admin_password failed', instance=instance)
3689             # We create a new exception here so that we won't
3690             # potentially reveal password information to the
3691             # API caller.  The real exception is logged above
3692             _msg = _('error setting admin password')
3693             raise exception.InstancePasswordSetFailed(
3694                 instance=instance.uuid, reason=_msg)
3695 
3696     @wrap_exception()
3697     @reverts_task_state
3698     @wrap_instance_fault
3699     def inject_file(self, context, path, file_contents, instance):
3700         """Write a file to the specified path in an instance on this host."""
3701         # NOTE(russellb) Remove this method, as well as the underlying virt
3702         # driver methods, when the compute rpc interface is bumped to 4.x
3703         # as it is no longer used.
3704         context = context.elevated()
3705         current_power_state = self._get_power_state(context, instance)
3706         expected_state = power_state.RUNNING
3707         if current_power_state != expected_state:
3708             LOG.warning('trying to inject a file into a non-running '
3709                         '(state: %(current_state)s expected: '
3710                         '%(expected_state)s)',
3711                         {'current_state': current_power_state,
3712                          'expected_state': expected_state},
3713                         instance=instance)
3714         LOG.info('injecting file to %s', path, instance=instance)
3715         self.driver.inject_file(instance, path, file_contents)
3716 
3717     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
3718         """Determine what image should be used to boot the rescue VM."""
3719         # 1. If rescue_image_ref is passed in, use that for rescue.
3720         # 2. Else, use the base image associated with instance's current image.
3721         #       The idea here is to provide the customer with a rescue
3722         #       environment which they are familiar with.
3723         #       So, if they built their instance off of a Debian image,
3724         #       their rescue VM will also be Debian.
3725         # 3. As a last resort, use instance's current image.
3726         if not rescue_image_ref:
3727             system_meta = utils.instance_sys_meta(instance)
3728             rescue_image_ref = system_meta.get('image_base_image_ref')
3729 
3730         if not rescue_image_ref:
3731             LOG.warning('Unable to find a different image to use for '
3732                         'rescue VM, using instance\'s current image',
3733                         instance=instance)
3734             rescue_image_ref = instance.image_ref
3735 
3736         return objects.ImageMeta.from_image_ref(
3737             context, self.image_api, rescue_image_ref)
3738 
3739     @wrap_exception()
3740     @reverts_task_state
3741     @wrap_instance_event(prefix='compute')
3742     @wrap_instance_fault
3743     def rescue_instance(self, context, instance, rescue_password,
3744                         rescue_image_ref, clean_shutdown):
3745         context = context.elevated()
3746         LOG.info('Rescuing', instance=instance)
3747 
3748         admin_password = (rescue_password if rescue_password else
3749                       utils.generate_password())
3750 
3751         network_info = self.network_api.get_instance_nw_info(context, instance)
3752 
3753         rescue_image_meta = self._get_rescue_image(context, instance,
3754                                                    rescue_image_ref)
3755 
3756         extra_usage_info = {'rescue_image_name':
3757                             self._get_image_name(rescue_image_meta)}
3758         self._notify_about_instance_usage(context, instance,
3759                 "rescue.start", extra_usage_info=extra_usage_info,
3760                 network_info=network_info)
3761         compute_utils.notify_about_instance_rescue_action(
3762             context, instance, self.host, rescue_image_ref,
3763             phase=fields.NotificationPhase.START)
3764 
3765         try:
3766             self._power_off_instance(context, instance, clean_shutdown)
3767 
3768             self.driver.rescue(context, instance,
3769                                network_info,
3770                                rescue_image_meta, admin_password)
3771         except Exception as e:
3772             LOG.exception("Error trying to Rescue Instance",
3773                           instance=instance)
3774             self._set_instance_obj_error_state(context, instance)
3775             raise exception.InstanceNotRescuable(
3776                 instance_id=instance.uuid,
3777                 reason=_("Driver Error: %s") % e)
3778 
3779         compute_utils.notify_usage_exists(self.notifier, context, instance,
3780                                           self.host, current_period=True)
3781 
3782         instance.vm_state = vm_states.RESCUED
3783         instance.task_state = None
3784         instance.power_state = self._get_power_state(context, instance)
3785         instance.launched_at = timeutils.utcnow()
3786         instance.save(expected_task_state=task_states.RESCUING)
3787 
3788         self._notify_about_instance_usage(context, instance,
3789                 "rescue.end", extra_usage_info=extra_usage_info,
3790                 network_info=network_info)
3791         compute_utils.notify_about_instance_rescue_action(
3792             context, instance, self.host, rescue_image_ref,
3793             phase=fields.NotificationPhase.END)
3794 
3795     @wrap_exception()
3796     @reverts_task_state
3797     @wrap_instance_event(prefix='compute')
3798     @wrap_instance_fault
3799     def unrescue_instance(self, context, instance):
3800         context = context.elevated()
3801         LOG.info('Unrescuing', instance=instance)
3802 
3803         network_info = self.network_api.get_instance_nw_info(context, instance)
3804         self._notify_about_instance_usage(context, instance,
3805                 "unrescue.start", network_info=network_info)
3806         compute_utils.notify_about_instance_action(context, instance,
3807             self.host, action=fields.NotificationAction.UNRESCUE,
3808             phase=fields.NotificationPhase.START)
3809 
3810         with self._error_out_instance_on_exception(context, instance):
3811             self.driver.unrescue(instance,
3812                                  network_info)
3813 
3814         instance.vm_state = vm_states.ACTIVE
3815         instance.task_state = None
3816         instance.power_state = self._get_power_state(context, instance)
3817         instance.save(expected_task_state=task_states.UNRESCUING)
3818 
3819         self._notify_about_instance_usage(context,
3820                                           instance,
3821                                           "unrescue.end",
3822                                           network_info=network_info)
3823         compute_utils.notify_about_instance_action(context, instance,
3824             self.host, action=fields.NotificationAction.UNRESCUE,
3825             phase=fields.NotificationPhase.END)
3826 
3827     @wrap_exception()
3828     @wrap_instance_fault
3829     def change_instance_metadata(self, context, diff, instance):
3830         """Update the metadata published to the instance."""
3831         LOG.debug("Changing instance metadata according to %r",
3832                   diff, instance=instance)
3833         self.driver.change_instance_metadata(context, instance, diff)
3834 
3835     @wrap_exception()
3836     @wrap_instance_event(prefix='compute')
3837     @wrap_instance_fault
3838     def confirm_resize(self, context, instance, migration):
3839         """Confirms a migration/resize and deletes the 'old' instance.
3840 
3841         This is called from the API and runs on the source host.
3842 
3843         Nothing needs to happen on the destination host at this point since
3844         the instance is already running there. This routine just cleans up the
3845         source host.
3846         """
3847         @utils.synchronized(instance.uuid)
3848         def do_confirm_resize(context, instance, migration_id):
3849             # NOTE(wangpan): Get the migration status from db, if it has been
3850             #                confirmed, we do nothing and return here
3851             LOG.debug("Going to confirm migration %s", migration_id,
3852                       instance=instance)
3853             try:
3854                 # TODO(russellb) Why are we sending the migration object just
3855                 # to turn around and look it up from the db again?
3856                 migration = objects.Migration.get_by_id(
3857                                     context.elevated(), migration_id)
3858             except exception.MigrationNotFound:
3859                 LOG.error("Migration %s is not found during confirmation",
3860                           migration_id, instance=instance)
3861                 return
3862 
3863             if migration.status == 'confirmed':
3864                 LOG.info("Migration %s is already confirmed",
3865                          migration_id, instance=instance)
3866                 return
3867             elif migration.status not in ('finished', 'confirming'):
3868                 LOG.warning("Unexpected confirmation status '%(status)s' "
3869                             "of migration %(id)s, exit confirmation process",
3870                             {"status": migration.status, "id": migration_id},
3871                             instance=instance)
3872                 return
3873 
3874             # NOTE(wangpan): Get the instance from db, if it has been
3875             #                deleted, we do nothing and return here
3876             expected_attrs = ['metadata', 'system_metadata', 'flavor']
3877             try:
3878                 instance = objects.Instance.get_by_uuid(
3879                         context, instance.uuid,
3880                         expected_attrs=expected_attrs)
3881             except exception.InstanceNotFound:
3882                 LOG.info("Instance is not found during confirmation",
3883                          instance=instance)
3884                 return
3885 
3886             self._confirm_resize(context, instance, migration=migration)
3887 
3888         do_confirm_resize(context, instance, migration.id)
3889 
3890     def _confirm_resize(self, context, instance, migration=None):
3891         """Destroys the source instance."""
3892         self._notify_about_instance_usage(context, instance,
3893                                           "resize.confirm.start")
3894         compute_utils.notify_about_instance_action(context, instance,
3895             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
3896             phase=fields.NotificationPhase.START)
3897 
3898         with self._error_out_instance_on_exception(context, instance):
3899             # NOTE(danms): delete stashed migration information
3900             old_instance_type = instance.old_flavor
3901             instance.old_flavor = None
3902             instance.new_flavor = None
3903             instance.system_metadata.pop('old_vm_state', None)
3904             instance.save()
3905 
3906             # NOTE(tr3buchet): tear down networks on source host
3907             self.network_api.setup_networks_on_host(context, instance,
3908                                migration.source_compute, teardown=True)
3909 
3910             network_info = self.network_api.get_instance_nw_info(context,
3911                                                                  instance)
3912             # TODO(mriedem): Get BDMs here and pass them to the driver.
3913             self.driver.confirm_migration(context, migration, instance,
3914                                           network_info)
3915 
3916             migration.status = 'confirmed'
3917             with migration.obj_as_admin():
3918                 migration.save()
3919 
3920             self.rt.drop_move_claim(context, instance, migration.source_node,
3921                                     old_instance_type, prefix='old_')
3922             self._delete_allocation_after_move(context, instance, migration)
3923             instance.drop_migration_context()
3924 
3925             # NOTE(mriedem): The old_vm_state could be STOPPED but the user
3926             # might have manually powered up the instance to confirm the
3927             # resize/migrate, so we need to check the current power state
3928             # on the instance and set the vm_state appropriately. We default
3929             # to ACTIVE because if the power state is not SHUTDOWN, we
3930             # assume _sync_instance_power_state will clean it up.
3931             p_state = instance.power_state
3932             vm_state = None
3933             if p_state == power_state.SHUTDOWN:
3934                 vm_state = vm_states.STOPPED
3935                 LOG.debug("Resized/migrated instance is powered off. "
3936                           "Setting vm_state to '%s'.", vm_state,
3937                           instance=instance)
3938             else:
3939                 vm_state = vm_states.ACTIVE
3940 
3941             instance.vm_state = vm_state
3942             instance.task_state = None
3943             instance.save(expected_task_state=[None, task_states.DELETING,
3944                                                task_states.SOFT_DELETING])
3945 
3946             self._notify_about_instance_usage(
3947                 context, instance, "resize.confirm.end",
3948                 network_info=network_info)
3949             compute_utils.notify_about_instance_action(context, instance,
3950                    self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
3951                    phase=fields.NotificationPhase.END)
3952 
3953     def _delete_allocation_after_move(self, context, instance, migration):
3954         """Deletes resource allocations held by the migration record against
3955         the source compute node resource provider after a confirmed cold /
3956         successful live migration.
3957         """
3958         try:
3959             # NOTE(danms): We're finishing on the source node, so try
3960             # to delete the allocation based on the migration uuid
3961             self.reportclient.delete_allocation_for_instance(
3962                 context, migration.uuid)
3963         except exception.AllocationDeleteFailed:
3964             LOG.error('Deleting allocation in placement for migration '
3965                       '%(migration_uuid)s failed. The instance '
3966                       '%(instance_uuid)s will be put to ERROR state '
3967                       'but the allocation held by the migration is '
3968                       'leaked.',
3969                       {'instance_uuid': instance.uuid,
3970                        'migration_uuid': migration.uuid})
3971             raise
3972 
3973     @wrap_exception()
3974     @reverts_task_state
3975     @wrap_instance_event(prefix='compute')
3976     @errors_out_migration
3977     @wrap_instance_fault
3978     def revert_resize(self, context, instance, migration):
3979         """Destroys the new instance on the destination machine.
3980 
3981         Reverts the model changes, and powers on the old instance on the
3982         source machine.
3983 
3984         """
3985         # NOTE(comstud): A revert_resize is essentially a resize back to
3986         # the old size, so we need to send a usage event here.
3987         compute_utils.notify_usage_exists(self.notifier, context, instance,
3988                                           self.host, current_period=True)
3989 
3990         with self._error_out_instance_on_exception(context, instance):
3991             # NOTE(tr3buchet): tear down networks on destination host
3992             self.network_api.setup_networks_on_host(context, instance,
3993                                                     teardown=True)
3994 
3995             migration_p = obj_base.obj_to_primitive(migration)
3996             self.network_api.migrate_instance_start(context,
3997                                                     instance,
3998                                                     migration_p)
3999 
4000             network_info = self.network_api.get_instance_nw_info(context,
4001                                                                  instance)
4002             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4003                     context, instance.uuid)
4004             block_device_info = self._get_instance_block_device_info(
4005                                 context, instance, bdms=bdms)
4006 
4007             destroy_disks = not self._is_instance_storage_shared(
4008                 context, instance, host=migration.source_compute)
4009             self.driver.destroy(context, instance, network_info,
4010                                 block_device_info, destroy_disks)
4011 
4012             self._terminate_volume_connections(context, instance, bdms)
4013 
4014             migration.status = 'reverted'
4015             with migration.obj_as_admin():
4016                 migration.save()
4017 
4018             # NOTE(ndipanov): We need to do this here because dropping the
4019             # claim means we lose the migration_context data. We really should
4020             # fix this by moving the drop_move_claim call to the
4021             # finish_revert_resize method as this is racy (revert is dropped,
4022             # but instance resources will be tracked with the new flavor until
4023             # it gets rolled back in finish_revert_resize, which is
4024             # potentially wrong for a period of time).
4025             instance.revert_migration_context()
4026             instance.save()
4027 
4028             self.rt.drop_move_claim(context, instance, instance.node)
4029 
4030             # RPC cast back to the source host to finish the revert there.
4031             self.compute_rpcapi.finish_revert_resize(context, instance,
4032                     migration, migration.source_compute)
4033 
4034     @wrap_exception()
4035     @reverts_task_state
4036     @wrap_instance_event(prefix='compute')
4037     @errors_out_migration
4038     @wrap_instance_fault
4039     def finish_revert_resize(self, context, instance, migration):
4040         """Finishes the second half of reverting a resize on the source host.
4041 
4042         Bring the original source instance state back (active/shutoff) and
4043         revert the resized attributes in the database.
4044 
4045         """
4046         with self._error_out_instance_on_exception(context, instance):
4047             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4048                 context, instance.uuid)
4049             self._notify_about_instance_usage(
4050                     context, instance, "resize.revert.start")
4051             compute_utils.notify_about_instance_action(context, instance,
4052                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4053                     phase=fields.NotificationPhase.START, bdms=bdms)
4054 
4055             # NOTE(mriedem): delete stashed old_vm_state information; we
4056             # default to ACTIVE for backwards compatibility if old_vm_state
4057             # is not set
4058             old_vm_state = instance.system_metadata.pop('old_vm_state',
4059                                                         vm_states.ACTIVE)
4060 
4061             self._set_instance_info(instance, instance.old_flavor)
4062             instance.old_flavor = None
4063             instance.new_flavor = None
4064             instance.host = migration.source_compute
4065             instance.node = migration.source_node
4066             instance.save()
4067 
4068             try:
4069                 self._revert_allocation(context, instance, migration)
4070             except exception.AllocationMoveFailed:
4071                 LOG.error('Reverting allocation in placement for migration '
4072                           '%(migration_uuid)s failed. The instance '
4073                           '%(instance_uuid)s will be put into ERROR state but '
4074                           'the allocation held by the migration is leaked.',
4075                           {'instance_uuid': instance.uuid,
4076                            'migration_uuid': migration.uuid})
4077                 raise
4078 
4079             self.network_api.setup_networks_on_host(context, instance,
4080                                                     migration.source_compute)
4081             migration_p = obj_base.obj_to_primitive(migration)
4082             # NOTE(hanrong): we need to change migration_p['dest_compute'] to
4083             # source host temporarily. "network_api.migrate_instance_finish"
4084             # will setup the network for the instance on the destination host.
4085             # For revert resize, the instance will back to the source host, the
4086             # setup of the network for instance should be on the source host.
4087             # So set the migration_p['dest_compute'] to source host at here.
4088             migration_p['dest_compute'] = migration.source_compute
4089             self.network_api.migrate_instance_finish(context,
4090                                                      instance,
4091                                                      migration_p)
4092             network_info = self.network_api.get_instance_nw_info(context,
4093                                                                  instance)
4094 
4095             # revert_resize deleted any volume attachments for the instance
4096             # and created new ones to be used on this host, but we
4097             # have to update those attachments with the host connector so the
4098             # BDM.connection_info will get set in the call to
4099             # _get_instance_block_device_info below with refresh_conn_info=True
4100             # and then the volumes can be re-connected via the driver on this
4101             # host.
4102             self._update_volume_attachments(context, instance, bdms)
4103 
4104             block_device_info = self._get_instance_block_device_info(
4105                     context, instance, refresh_conn_info=True, bdms=bdms)
4106 
4107             power_on = old_vm_state != vm_states.STOPPED
4108             self.driver.finish_revert_migration(context, instance,
4109                                        network_info,
4110                                        block_device_info, power_on)
4111 
4112             instance.drop_migration_context()
4113             instance.launched_at = timeutils.utcnow()
4114             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4115 
4116             # Complete any volume attachments so the volumes are in-use.
4117             self._complete_volume_attachments(context, bdms)
4118 
4119             # if the original vm state was STOPPED, set it back to STOPPED
4120             LOG.info("Updating instance to original state: '%s'",
4121                      old_vm_state, instance=instance)
4122             if power_on:
4123                 instance.vm_state = vm_states.ACTIVE
4124                 instance.task_state = None
4125                 instance.save()
4126             else:
4127                 instance.task_state = task_states.POWERING_OFF
4128                 instance.save()
4129                 self.stop_instance(context, instance=instance,
4130                                    clean_shutdown=True)
4131 
4132             self._notify_about_instance_usage(
4133                     context, instance, "resize.revert.end")
4134             compute_utils.notify_about_instance_action(context, instance,
4135                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4136                     phase=fields.NotificationPhase.END, bdms=bdms)
4137 
4138     def _revert_allocation(self, context, instance, migration):
4139         """Revert an allocation that is held by migration to our instance."""
4140 
4141         # Fetch the original allocation that the instance had on the source
4142         # node, which are now held by the migration
4143         orig_alloc = self.reportclient.get_allocations_for_consumer(
4144             context, migration.uuid)
4145         if not orig_alloc:
4146             LOG.error('Did not find resource allocations for migration '
4147                       '%s on source node %s. Unable to revert source node '
4148                       'allocations back to the instance.',
4149                       migration.uuid, migration.source_node, instance=instance)
4150             return False
4151 
4152         if len(orig_alloc) > 1:
4153             # NOTE(danms): This may change later if we have other allocations
4154             # against other providers that need to be held by the migration
4155             # as well. Perhaps something like shared storage resources that
4156             # will actually be duplicated during a resize type operation.
4157             LOG.error('Migration %(mig)s has allocations against '
4158                       'more than one provider %(rps)s. This should not be '
4159                       'possible, but reverting it anyway.',
4160                       {'mig': migration.uuid,
4161                        'rps': ','.join(orig_alloc.keys())},
4162                       instance=instance)
4163 
4164         # We only have a claim against one provider, it is the source node
4165         cn_uuid = list(orig_alloc.keys())[0]
4166 
4167         # FIXME(danms): This method is flawed in that it asssumes allocations
4168         # against only one provider. So, this may overwite allocations against
4169         # a shared provider, if we had one.
4170         LOG.info('Swapping old allocation on %(node)s held by migration '
4171                  '%(mig)s for instance',
4172                  {'node': cn_uuid, 'mig': migration.uuid},
4173                  instance=instance)
4174         # TODO(cdent): Should we be doing anything with return values here?
4175         self.reportclient.move_allocations(context, migration.uuid,
4176                                            instance.uuid)
4177         return True
4178 
4179     def _prep_resize(self, context, image, instance, instance_type,
4180                      filter_properties, node, migration, clean_shutdown=True):
4181 
4182         if not filter_properties:
4183             filter_properties = {}
4184 
4185         if not instance.host:
4186             self._set_instance_obj_error_state(context, instance)
4187             msg = _('Instance has no source host')
4188             raise exception.MigrationError(reason=msg)
4189 
4190         same_host = instance.host == self.host
4191         # if the flavor IDs match, it's migrate; otherwise resize
4192         if same_host and instance_type.id == instance['instance_type_id']:
4193             # check driver whether support migrate to same host
4194             if not self.driver.capabilities.get(
4195                     'supports_migrate_to_same_host', False):
4196                 raise exception.UnableToMigrateToSelf(
4197                     instance_id=instance.uuid, host=self.host)
4198 
4199         # NOTE(danms): Stash the new instance_type to avoid having to
4200         # look it up in the database later
4201         instance.new_flavor = instance_type
4202         # NOTE(mriedem): Stash the old vm_state so we can set the
4203         # resized/reverted instance back to the same state later.
4204         vm_state = instance.vm_state
4205         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
4206         instance.system_metadata['old_vm_state'] = vm_state
4207         instance.save()
4208 
4209         limits = filter_properties.get('limits', {})
4210         with self.rt.resize_claim(context, instance, instance_type, node,
4211                                   migration, image_meta=image,
4212                                   limits=limits) as claim:
4213             LOG.info('Migrating', instance=instance)
4214             # RPC cast to the source host to start the actual resize/migration.
4215             self.compute_rpcapi.resize_instance(
4216                     context, instance, claim.migration, image,
4217                     instance_type, clean_shutdown)
4218 
4219     def _send_prep_resize_notifications(
4220             self, context, instance, phase, flavor):
4221         """Send "resize.prep.*" notifications.
4222 
4223         :param context: nova auth request context
4224         :param instance: The instance being resized
4225         :param phase: The phase of the action (NotificationPhase enum)
4226         :param flavor: The (new) flavor for the resize (same as existing
4227             instance.flavor for a cold migration)
4228         """
4229         # Only send notify_usage_exists if it's the "start" phase.
4230         if phase == fields.NotificationPhase.START:
4231             compute_utils.notify_usage_exists(
4232                 self.notifier, context, instance, self.host,
4233                 current_period=True)
4234 
4235         # Send extra usage info about the flavor if it's the "end" phase for
4236         # the legacy unversioned notification.
4237         extra_usage_info = None
4238         if phase == fields.NotificationPhase.END:
4239             extra_usage_info = dict(
4240                 new_instance_type=flavor.name,
4241                 new_instance_type_id=flavor.id)
4242         self._notify_about_instance_usage(
4243             context, instance, "resize.prep.%s" % phase,
4244             extra_usage_info=extra_usage_info)
4245 
4246         # Send the versioned notification.
4247         compute_utils.notify_about_resize_prep_instance(
4248             context, instance, self.host, phase, flavor)
4249 
4250     @wrap_exception()
4251     @reverts_task_state
4252     @wrap_instance_event(prefix='compute')
4253     @wrap_instance_fault
4254     def prep_resize(self, context, image, instance, instance_type,
4255                     request_spec, filter_properties, node,
4256                     clean_shutdown, migration, host_list):
4257         """Initiates the process of moving a running instance to another host.
4258 
4259         Possibly changes the VCPU, RAM and disk size in the process.
4260 
4261         This is initiated from conductor and runs on the destination host.
4262 
4263         The main purpose of this method is performing some checks on the
4264         destination host and making a claim for resources. If the claim fails
4265         then a reschedule to another host may be attempted which involves
4266         calling back to conductor to start the process over again.
4267         """
4268         if node is None:
4269             node = self._get_nodename(instance, refresh=True)
4270 
4271         with self._error_out_instance_on_exception(context, instance), \
4272                  errors_out_migration_ctxt(migration):
4273             self._send_prep_resize_notifications(
4274                 context, instance, fields.NotificationPhase.START,
4275                 instance_type)
4276             try:
4277                 self._prep_resize(context, image, instance,
4278                                   instance_type, filter_properties,
4279                                   node, migration, clean_shutdown)
4280             except Exception:
4281                 # Since we hit a failure, we're either rescheduling or dead
4282                 # and either way we need to cleanup any allocations created
4283                 # by the scheduler for the destination node.
4284                 self._revert_allocation(context, instance, migration)
4285                 # try to re-schedule the resize elsewhere:
4286                 exc_info = sys.exc_info()
4287                 self._reschedule_resize_or_reraise(context, instance,
4288                         exc_info, instance_type, request_spec,
4289                         filter_properties, host_list)
4290             finally:
4291                 self._send_prep_resize_notifications(
4292                     context, instance, fields.NotificationPhase.END,
4293                     instance_type)
4294 
4295     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
4296             instance_type, request_spec, filter_properties, host_list):
4297         """Try to re-schedule the resize or re-raise the original error to
4298         error out the instance.
4299         """
4300         if not filter_properties:
4301             filter_properties = {}
4302 
4303         rescheduled = False
4304         instance_uuid = instance.uuid
4305 
4306         try:
4307             reschedule_method = self.compute_task_api.resize_instance
4308             scheduler_hint = dict(filter_properties=filter_properties)
4309             method_args = (instance, None, scheduler_hint, instance_type)
4310             task_state = task_states.RESIZE_PREP
4311 
4312             rescheduled = self._reschedule(context, request_spec,
4313                     filter_properties, instance, reschedule_method,
4314                     method_args, task_state, exc_info, host_list=host_list)
4315         except Exception as error:
4316             rescheduled = False
4317             LOG.exception("Error trying to reschedule",
4318                           instance_uuid=instance_uuid)
4319             compute_utils.add_instance_fault_from_exc(context,
4320                     instance, error,
4321                     exc_info=sys.exc_info())
4322             self._notify_about_instance_usage(context, instance,
4323                     'resize.error', fault=error)
4324             compute_utils.notify_about_instance_action(
4325                 context, instance, self.host,
4326                 action=fields.NotificationAction.RESIZE,
4327                 phase=fields.NotificationPhase.ERROR,
4328                 exception=error,
4329                 tb=','.join(traceback.format_exception(*exc_info)))
4330         if rescheduled:
4331             self._log_original_error(exc_info, instance_uuid)
4332             compute_utils.add_instance_fault_from_exc(context,
4333                     instance, exc_info[1], exc_info=exc_info)
4334             self._notify_about_instance_usage(context, instance,
4335                     'resize.error', fault=exc_info[1])
4336             compute_utils.notify_about_instance_action(
4337                 context, instance, self.host,
4338                 action=fields.NotificationAction.RESIZE,
4339                 phase=fields.NotificationPhase.ERROR,
4340                 exception=exc_info[1],
4341                 tb=','.join(traceback.format_exception(*exc_info)))
4342         else:
4343             # not re-scheduling
4344             six.reraise(*exc_info)
4345 
4346     @wrap_exception()
4347     @reverts_task_state
4348     @wrap_instance_event(prefix='compute')
4349     @wrap_instance_fault
4350     def resize_instance(self, context, instance, image,
4351                         migration, instance_type, clean_shutdown):
4352         """Starts the migration of a running instance to another host.
4353 
4354         This is initiated from the destination host's ``prep_resize`` routine
4355         and runs on the source host.
4356         """
4357         try:
4358             self._resize_instance(context, instance, image, migration,
4359                                   instance_type, clean_shutdown)
4360         except Exception:
4361             with excutils.save_and_reraise_exception():
4362                 self._revert_allocation(context, instance, migration)
4363 
4364     def _resize_instance(self, context, instance, image,
4365                          migration, instance_type, clean_shutdown):
4366         with self._error_out_instance_on_exception(context, instance), \
4367              errors_out_migration_ctxt(migration):
4368             network_info = self.network_api.get_instance_nw_info(context,
4369                                                                  instance)
4370 
4371             migration.status = 'migrating'
4372             with migration.obj_as_admin():
4373                 migration.save()
4374 
4375             instance.task_state = task_states.RESIZE_MIGRATING
4376             instance.save(expected_task_state=task_states.RESIZE_PREP)
4377 
4378             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4379                     context, instance.uuid)
4380             self._send_resize_instance_notifications(
4381                 context, instance, bdms, network_info,
4382                 fields.NotificationPhase.START)
4383 
4384             block_device_info = self._get_instance_block_device_info(
4385                                 context, instance, bdms=bdms)
4386 
4387             timeout, retry_interval = self._get_power_off_values(context,
4388                                             instance, clean_shutdown)
4389             disk_info = self.driver.migrate_disk_and_power_off(
4390                     context, instance, migration.dest_host,
4391                     instance_type, network_info,
4392                     block_device_info,
4393                     timeout, retry_interval)
4394 
4395             self._terminate_volume_connections(context, instance, bdms)
4396 
4397             migration_p = obj_base.obj_to_primitive(migration)
4398             self.network_api.migrate_instance_start(context,
4399                                                     instance,
4400                                                     migration_p)
4401 
4402             migration.status = 'post-migrating'
4403             with migration.obj_as_admin():
4404                 migration.save()
4405 
4406             instance.host = migration.dest_compute
4407             instance.node = migration.dest_node
4408             instance.task_state = task_states.RESIZE_MIGRATED
4409             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
4410 
4411             # RPC cast to the destination host to finish the resize/migration.
4412             self.compute_rpcapi.finish_resize(context, instance,
4413                     migration, image, disk_info, migration.dest_compute)
4414 
4415         self._send_resize_instance_notifications(
4416             context, instance, bdms, network_info,
4417             fields.NotificationPhase.END)
4418         self.instance_events.clear_events_for_instance(instance)
4419 
4420     def _send_resize_instance_notifications(
4421             self, context, instance, bdms, network_info, phase):
4422         """Send "resize.(start|end)" notifications.
4423 
4424         :param context: nova auth request context
4425         :param instance: The instance being resized
4426         :param bdms: BlockDeviceMappingList for the BDMs associated with the
4427             instance
4428         :param network_info: NetworkInfo for the instance info cache of ports
4429         :param phase: The phase of the action (NotificationPhase enum, either
4430             ``start`` or ``end``)
4431         """
4432         action = fields.NotificationAction.RESIZE
4433         # Send the legacy unversioned notification.
4434         self._notify_about_instance_usage(
4435             context, instance, "%s.%s" % (action, phase),
4436             network_info=network_info)
4437         # Send the versioned notification.
4438         compute_utils.notify_about_instance_action(
4439             context, instance, self.host, action=action, phase=phase,
4440             bdms=bdms)
4441 
4442     def _terminate_volume_connections(self, context, instance, bdms):
4443         connector = None
4444         for bdm in bdms:
4445             if bdm.is_volume:
4446                 if bdm.attachment_id:
4447                     # NOTE(jdg): So here's the thing, the idea behind the new
4448                     # attach API's was to have a new code fork/path that we
4449                     # followed, we're not going to do that so we have to do
4450                     # some extra work in here to make it *behave* just like the
4451                     # old code. Cinder doesn't allow disconnect/reconnect (you
4452                     # just delete the attachment and get a new one)
4453                     # attachments in the new attach code so we have to do
4454                     # a delete and create without a connector (reserve),
4455                     # in other words, beware
4456                     attachment_id = self.volume_api.attachment_create(
4457                         context, bdm.volume_id, instance.uuid)['id']
4458                     self.volume_api.attachment_delete(context,
4459                                                       bdm.attachment_id)
4460                     bdm.attachment_id = attachment_id
4461                     bdm.save()
4462 
4463                 else:
4464                     if connector is None:
4465                         connector = self.driver.get_volume_connector(instance)
4466                     self.volume_api.terminate_connection(context,
4467                                                          bdm.volume_id,
4468                                                          connector)
4469 
4470     @staticmethod
4471     def _set_instance_info(instance, instance_type):
4472         instance.instance_type_id = instance_type.id
4473         instance.memory_mb = instance_type.memory_mb
4474         instance.vcpus = instance_type.vcpus
4475         instance.root_gb = instance_type.root_gb
4476         instance.ephemeral_gb = instance_type.ephemeral_gb
4477         instance.flavor = instance_type
4478 
4479     def _update_volume_attachments(self, context, instance, bdms):
4480         """Updates volume attachments using the virt driver host connector.
4481 
4482         :param context: nova.context.RequestContext - user request context
4483         :param instance: nova.objects.Instance
4484         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
4485                      device mappings for the given instance
4486         """
4487         if bdms:
4488             connector = None
4489             for bdm in bdms:
4490                 if bdm.is_volume and bdm.attachment_id:
4491                     if connector is None:
4492                         connector = self.driver.get_volume_connector(instance)
4493                     self.volume_api.attachment_update(
4494                         context, bdm.attachment_id, connector, bdm.device_name)
4495 
4496     def _complete_volume_attachments(self, context, bdms):
4497         """Completes volume attachments for the instance
4498 
4499         :param context: nova.context.RequestContext - user request context
4500         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
4501                      device mappings for the given instance
4502         """
4503         if bdms:
4504             for bdm in bdms:
4505                 if bdm.is_volume and bdm.attachment_id:
4506                     self.volume_api.attachment_complete(
4507                         context, bdm.attachment_id)
4508 
4509     def _finish_resize(self, context, instance, migration, disk_info,
4510                        image_meta, bdms):
4511         resize_instance = False
4512         old_instance_type_id = migration['old_instance_type_id']
4513         new_instance_type_id = migration['new_instance_type_id']
4514         old_instance_type = instance.get_flavor()
4515         # NOTE(mriedem): Get the old_vm_state so we know if we should
4516         # power on the instance. If old_vm_state is not set we need to default
4517         # to ACTIVE for backwards compatibility
4518         old_vm_state = instance.system_metadata.get('old_vm_state',
4519                                                     vm_states.ACTIVE)
4520         instance.old_flavor = old_instance_type
4521 
4522         if old_instance_type_id != new_instance_type_id:
4523             instance_type = instance.get_flavor('new')
4524             self._set_instance_info(instance, instance_type)
4525             for key in ('root_gb', 'swap', 'ephemeral_gb'):
4526                 if old_instance_type[key] != instance_type[key]:
4527                     resize_instance = True
4528                     break
4529         instance.apply_migration_context()
4530 
4531         # NOTE(tr3buchet): setup networks on destination host
4532         self.network_api.setup_networks_on_host(context, instance,
4533                                                 migration['dest_compute'])
4534 
4535         migration_p = obj_base.obj_to_primitive(migration)
4536         self.network_api.migrate_instance_finish(context,
4537                                                  instance,
4538                                                  migration_p)
4539 
4540         network_info = self.network_api.get_instance_nw_info(context, instance)
4541 
4542         instance.task_state = task_states.RESIZE_FINISH
4543         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
4544 
4545         self._notify_about_instance_usage(
4546             context, instance, "finish_resize.start",
4547             network_info=network_info)
4548         compute_utils.notify_about_instance_action(context, instance,
4549                self.host, action=fields.NotificationAction.RESIZE_FINISH,
4550                phase=fields.NotificationPhase.START, bdms=bdms)
4551 
4552         # We need to update any volume attachments using the destination
4553         # host connector so that we can update the BDM.connection_info
4554         # before calling driver.finish_migration otherwise the driver
4555         # won't know how to connect the volumes to this host.
4556         # Note that _get_instance_block_device_info with
4557         # refresh_conn_info=True will update the BDM.connection_info value
4558         # in the database so we must do this before calling that method.
4559         self._update_volume_attachments(context, instance, bdms)
4560 
4561         block_device_info = self._get_instance_block_device_info(
4562             context, instance, refresh_conn_info=True, bdms=bdms)
4563 
4564         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
4565         # automatically power on the instance after it's migrated
4566         power_on = old_vm_state != vm_states.STOPPED
4567 
4568         try:
4569             self.driver.finish_migration(context, migration, instance,
4570                                          disk_info,
4571                                          network_info,
4572                                          image_meta, resize_instance,
4573                                          block_device_info, power_on)
4574         except Exception:
4575             with excutils.save_and_reraise_exception():
4576                 if old_instance_type_id != new_instance_type_id:
4577                     self._set_instance_info(instance,
4578                                             old_instance_type)
4579 
4580         # Now complete any volume attachments that were previously updated.
4581         self._complete_volume_attachments(context, bdms)
4582 
4583         migration.status = 'finished'
4584         with migration.obj_as_admin():
4585             migration.save()
4586 
4587         instance.vm_state = vm_states.RESIZED
4588         instance.task_state = None
4589         instance.launched_at = timeutils.utcnow()
4590         instance.save(expected_task_state=task_states.RESIZE_FINISH)
4591 
4592         return network_info
4593 
4594     @wrap_exception()
4595     @reverts_task_state
4596     @wrap_instance_event(prefix='compute')
4597     @wrap_instance_fault
4598     def finish_resize(self, context, disk_info, image, instance,
4599                       migration):
4600         """Completes the migration process.
4601 
4602         Sets up the newly transferred disk and turns on the instance at its
4603         new host machine.
4604 
4605         """
4606         try:
4607             self._finish_resize_helper(context, disk_info, image, instance,
4608                                        migration)
4609         except Exception:
4610             with excutils.save_and_reraise_exception():
4611                 self._revert_allocation(context, instance, migration)
4612 
4613     def _finish_resize_helper(self, context, disk_info, image, instance,
4614                               migration):
4615         """Completes the migration process.
4616 
4617         The caller must revert the instance's allocations if the migration
4618         process failed.
4619         """
4620         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4621             context, instance.uuid)
4622 
4623         with self._error_out_instance_on_exception(context, instance), \
4624              errors_out_migration_ctxt(migration):
4625             image_meta = objects.ImageMeta.from_dict(image)
4626             network_info = self._finish_resize(context, instance, migration,
4627                                                disk_info, image_meta, bdms)
4628 
4629         # TODO(melwitt): We should clean up instance console tokens here. The
4630         # instance is on a new host and will need to establish a new console
4631         # connection.
4632         self._update_scheduler_instance_info(context, instance)
4633         self._notify_about_instance_usage(
4634             context, instance, "finish_resize.end",
4635             network_info=network_info)
4636         compute_utils.notify_about_instance_action(context, instance,
4637                self.host, action=fields.NotificationAction.RESIZE_FINISH,
4638                phase=fields.NotificationPhase.END, bdms=bdms)
4639 
4640     @wrap_exception()
4641     @wrap_instance_fault
4642     def add_fixed_ip_to_instance(self, context, network_id, instance):
4643         """Calls network_api to add new fixed_ip to instance
4644         then injects the new network info and resets instance networking.
4645 
4646         """
4647         self._notify_about_instance_usage(
4648                 context, instance, "create_ip.start")
4649 
4650         network_info = self.network_api.add_fixed_ip_to_instance(context,
4651                                                                  instance,
4652                                                                  network_id)
4653         self._inject_network_info(context, instance, network_info)
4654         self.reset_network(context, instance)
4655 
4656         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4657         instance.updated_at = timeutils.utcnow()
4658         instance.save()
4659 
4660         self._notify_about_instance_usage(
4661             context, instance, "create_ip.end", network_info=network_info)
4662 
4663     @wrap_exception()
4664     @wrap_instance_fault
4665     def remove_fixed_ip_from_instance(self, context, address, instance):
4666         """Calls network_api to remove existing fixed_ip from instance
4667         by injecting the altered network info and resetting
4668         instance networking.
4669         """
4670         self._notify_about_instance_usage(
4671                 context, instance, "delete_ip.start")
4672 
4673         network_info = self.network_api.remove_fixed_ip_from_instance(context,
4674                                                                       instance,
4675                                                                       address)
4676         self._inject_network_info(context, instance, network_info)
4677         self.reset_network(context, instance)
4678 
4679         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
4680         instance.updated_at = timeutils.utcnow()
4681         instance.save()
4682 
4683         self._notify_about_instance_usage(
4684             context, instance, "delete_ip.end", network_info=network_info)
4685 
4686     @wrap_exception()
4687     @reverts_task_state
4688     @wrap_instance_event(prefix='compute')
4689     @wrap_instance_fault
4690     def pause_instance(self, context, instance):
4691         """Pause an instance on this host."""
4692         context = context.elevated()
4693         LOG.info('Pausing', instance=instance)
4694         self._notify_about_instance_usage(context, instance, 'pause.start')
4695         compute_utils.notify_about_instance_action(context, instance,
4696                self.host, action=fields.NotificationAction.PAUSE,
4697                phase=fields.NotificationPhase.START)
4698         self.driver.pause(instance)
4699         instance.power_state = self._get_power_state(context, instance)
4700         instance.vm_state = vm_states.PAUSED
4701         instance.task_state = None
4702         instance.save(expected_task_state=task_states.PAUSING)
4703         self._notify_about_instance_usage(context, instance, 'pause.end')
4704         compute_utils.notify_about_instance_action(context, instance,
4705                self.host, action=fields.NotificationAction.PAUSE,
4706                phase=fields.NotificationPhase.END)
4707 
4708     @wrap_exception()
4709     @reverts_task_state
4710     @wrap_instance_event(prefix='compute')
4711     @wrap_instance_fault
4712     def unpause_instance(self, context, instance):
4713         """Unpause a paused instance on this host."""
4714         context = context.elevated()
4715         LOG.info('Unpausing', instance=instance)
4716         self._notify_about_instance_usage(context, instance, 'unpause.start')
4717         compute_utils.notify_about_instance_action(context, instance,
4718             self.host, action=fields.NotificationAction.UNPAUSE,
4719             phase=fields.NotificationPhase.START)
4720         self.driver.unpause(instance)
4721         instance.power_state = self._get_power_state(context, instance)
4722         instance.vm_state = vm_states.ACTIVE
4723         instance.task_state = None
4724         instance.save(expected_task_state=task_states.UNPAUSING)
4725         self._notify_about_instance_usage(context, instance, 'unpause.end')
4726         compute_utils.notify_about_instance_action(context, instance,
4727             self.host, action=fields.NotificationAction.UNPAUSE,
4728             phase=fields.NotificationPhase.END)
4729 
4730     @wrap_exception()
4731     def host_power_action(self, context, action):
4732         """Reboots, shuts down or powers up the host."""
4733         return self.driver.host_power_action(action)
4734 
4735     @wrap_exception()
4736     def host_maintenance_mode(self, context, host, mode):
4737         """Start/Stop host maintenance window. On start, it triggers
4738         guest VMs evacuation.
4739         """
4740         return self.driver.host_maintenance_mode(host, mode)
4741 
4742     @wrap_exception()
4743     def set_host_enabled(self, context, enabled):
4744         """Sets the specified host's ability to accept new instances."""
4745         return self.driver.set_host_enabled(enabled)
4746 
4747     @wrap_exception()
4748     def get_host_uptime(self, context):
4749         """Returns the result of calling "uptime" on the target host."""
4750         return self.driver.get_host_uptime()
4751 
4752     @wrap_exception()
4753     @wrap_instance_fault
4754     def get_diagnostics(self, context, instance):
4755         """Retrieve diagnostics for an instance on this host."""
4756         current_power_state = self._get_power_state(context, instance)
4757         if current_power_state == power_state.RUNNING:
4758             LOG.info("Retrieving diagnostics", instance=instance)
4759             return self.driver.get_diagnostics(instance)
4760         else:
4761             raise exception.InstanceInvalidState(
4762                 attr='power state',
4763                 instance_uuid=instance.uuid,
4764                 state=power_state.STATE_MAP[instance.power_state],
4765                 method='get_diagnostics')
4766 
4767     @wrap_exception()
4768     @wrap_instance_fault
4769     def get_instance_diagnostics(self, context, instance):
4770         """Retrieve diagnostics for an instance on this host."""
4771         current_power_state = self._get_power_state(context, instance)
4772         if current_power_state == power_state.RUNNING:
4773             LOG.info("Retrieving diagnostics", instance=instance)
4774             return self.driver.get_instance_diagnostics(instance)
4775         else:
4776             raise exception.InstanceInvalidState(
4777                 attr='power state',
4778                 instance_uuid=instance.uuid,
4779                 state=power_state.STATE_MAP[instance.power_state],
4780                 method='get_diagnostics')
4781 
4782     @wrap_exception()
4783     @reverts_task_state
4784     @wrap_instance_event(prefix='compute')
4785     @wrap_instance_fault
4786     def suspend_instance(self, context, instance):
4787         """Suspend the given instance."""
4788         context = context.elevated()
4789 
4790         # Store the old state
4791         instance.system_metadata['old_vm_state'] = instance.vm_state
4792         self._notify_about_instance_usage(context, instance, 'suspend.start')
4793         compute_utils.notify_about_instance_action(context, instance,
4794                 self.host, action=fields.NotificationAction.SUSPEND,
4795                 phase=fields.NotificationPhase.START)
4796         with self._error_out_instance_on_exception(context, instance,
4797              instance_state=instance.vm_state):
4798             self.driver.suspend(context, instance)
4799         instance.power_state = self._get_power_state(context, instance)
4800         instance.vm_state = vm_states.SUSPENDED
4801         instance.task_state = None
4802         instance.save(expected_task_state=task_states.SUSPENDING)
4803         self._notify_about_instance_usage(context, instance, 'suspend.end')
4804         compute_utils.notify_about_instance_action(context, instance,
4805                 self.host, action=fields.NotificationAction.SUSPEND,
4806                 phase=fields.NotificationPhase.END)
4807 
4808     @wrap_exception()
4809     @reverts_task_state
4810     @wrap_instance_event(prefix='compute')
4811     @wrap_instance_fault
4812     def resume_instance(self, context, instance):
4813         """Resume the given suspended instance."""
4814         context = context.elevated()
4815         LOG.info('Resuming', instance=instance)
4816 
4817         self._notify_about_instance_usage(context, instance, 'resume.start')
4818 
4819         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4820             context, instance.uuid)
4821         block_device_info = self._get_instance_block_device_info(
4822             context, instance, bdms=bdms)
4823 
4824         compute_utils.notify_about_instance_action(context, instance,
4825             self.host, action=fields.NotificationAction.RESUME,
4826             phase=fields.NotificationPhase.START, bdms=bdms)
4827 
4828         network_info = self.network_api.get_instance_nw_info(context, instance)
4829 
4830         with self._error_out_instance_on_exception(context, instance,
4831              instance_state=instance.vm_state):
4832             self.driver.resume(context, instance, network_info,
4833                                block_device_info)
4834 
4835         instance.power_state = self._get_power_state(context, instance)
4836 
4837         # We default to the ACTIVE state for backwards compatibility
4838         instance.vm_state = instance.system_metadata.pop('old_vm_state',
4839                                                          vm_states.ACTIVE)
4840 
4841         instance.task_state = None
4842         instance.save(expected_task_state=task_states.RESUMING)
4843         self._notify_about_instance_usage(context, instance, 'resume.end')
4844         compute_utils.notify_about_instance_action(context, instance,
4845             self.host, action=fields.NotificationAction.RESUME,
4846             phase=fields.NotificationPhase.END, bdms=bdms)
4847 
4848     @wrap_exception()
4849     @reverts_task_state
4850     @wrap_instance_event(prefix='compute')
4851     @wrap_instance_fault
4852     def shelve_instance(self, context, instance, image_id,
4853                         clean_shutdown):
4854         """Shelve an instance.
4855 
4856         This should be used when you want to take a snapshot of the instance.
4857         It also adds system_metadata that can be used by a periodic task to
4858         offload the shelved instance after a period of time.
4859 
4860         :param context: request context
4861         :param instance: an Instance object
4862         :param image_id: an image id to snapshot to.
4863         :param clean_shutdown: give the GuestOS a chance to stop
4864         """
4865 
4866         @utils.synchronized(instance.uuid)
4867         def do_shelve_instance():
4868             self._shelve_instance(context, instance, image_id, clean_shutdown)
4869         do_shelve_instance()
4870 
4871     def _shelve_instance(self, context, instance, image_id,
4872                          clean_shutdown):
4873         LOG.info('Shelving', instance=instance)
4874         offload = CONF.shelved_offload_time == 0
4875         if offload:
4876             # Get the BDMs early so we can pass them into versioned
4877             # notifications since _shelve_offload_instance needs the
4878             # BDMs anyway.
4879             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4880                 context, instance.uuid)
4881         else:
4882             bdms = None
4883         compute_utils.notify_usage_exists(self.notifier, context, instance,
4884                                           self.host, current_period=True)
4885         self._notify_about_instance_usage(context, instance, 'shelve.start')
4886         compute_utils.notify_about_instance_action(context, instance,
4887                 self.host, action=fields.NotificationAction.SHELVE,
4888                 phase=fields.NotificationPhase.START, bdms=bdms)
4889 
4890         def update_task_state(task_state, expected_state=task_states.SHELVING):
4891             shelving_state_map = {
4892                     task_states.IMAGE_PENDING_UPLOAD:
4893                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
4894                     task_states.IMAGE_UPLOADING:
4895                         task_states.SHELVING_IMAGE_UPLOADING,
4896                     task_states.SHELVING: task_states.SHELVING}
4897             task_state = shelving_state_map[task_state]
4898             expected_state = shelving_state_map[expected_state]
4899             instance.task_state = task_state
4900             instance.save(expected_task_state=expected_state)
4901         # Do not attempt a clean shutdown of a paused guest since some
4902         # hypervisors will fail the clean shutdown if the guest is not
4903         # running.
4904         if instance.power_state == power_state.PAUSED:
4905             clean_shutdown = False
4906         self._power_off_instance(context, instance, clean_shutdown)
4907         self.driver.snapshot(context, instance, image_id, update_task_state)
4908 
4909         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
4910         instance.system_metadata['shelved_image_id'] = image_id
4911         instance.system_metadata['shelved_host'] = self.host
4912         instance.vm_state = vm_states.SHELVED
4913         instance.task_state = None
4914         if CONF.shelved_offload_time == 0:
4915             instance.task_state = task_states.SHELVING_OFFLOADING
4916         instance.power_state = self._get_power_state(context, instance)
4917         instance.save(expected_task_state=[
4918                 task_states.SHELVING,
4919                 task_states.SHELVING_IMAGE_UPLOADING])
4920 
4921         self._notify_about_instance_usage(context, instance, 'shelve.end')
4922         compute_utils.notify_about_instance_action(context, instance,
4923                 self.host, action=fields.NotificationAction.SHELVE,
4924                 phase=fields.NotificationPhase.END, bdms=bdms)
4925 
4926         if offload:
4927             self._shelve_offload_instance(context, instance,
4928                                           clean_shutdown=False, bdms=bdms)
4929 
4930     @wrap_exception()
4931     @reverts_task_state
4932     @wrap_instance_event(prefix='compute')
4933     @wrap_instance_fault
4934     def shelve_offload_instance(self, context, instance, clean_shutdown):
4935         """Remove a shelved instance from the hypervisor.
4936 
4937         This frees up those resources for use by other instances, but may lead
4938         to slower unshelve times for this instance.  This method is used by
4939         volume backed instances since restoring them doesn't involve the
4940         potentially large download of an image.
4941 
4942         :param context: request context
4943         :param instance: nova.objects.instance.Instance
4944         :param clean_shutdown: give the GuestOS a chance to stop
4945         """
4946 
4947         @utils.synchronized(instance.uuid)
4948         def do_shelve_offload_instance():
4949             self._shelve_offload_instance(context, instance, clean_shutdown)
4950         do_shelve_offload_instance()
4951 
4952     def _shelve_offload_instance(self, context, instance, clean_shutdown,
4953                                  bdms=None):
4954         LOG.info('Shelve offloading', instance=instance)
4955         if bdms is None:
4956             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4957                 context, instance.uuid)
4958         self._notify_about_instance_usage(context, instance,
4959                 'shelve_offload.start')
4960         compute_utils.notify_about_instance_action(context, instance,
4961                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
4962                 phase=fields.NotificationPhase.START, bdms=bdms)
4963 
4964         self._power_off_instance(context, instance, clean_shutdown)
4965         current_power_state = self._get_power_state(context, instance)
4966 
4967         self.network_api.cleanup_instance_network_on_host(context, instance,
4968                                                           instance.host)
4969         network_info = self.network_api.get_instance_nw_info(context, instance)
4970 
4971         block_device_info = self._get_instance_block_device_info(context,
4972                                                                  instance,
4973                                                                  bdms=bdms)
4974         self.driver.destroy(context, instance, network_info,
4975                 block_device_info)
4976 
4977         # the instance is going to be removed from the host so we want to
4978         # terminate all the connections with the volume server and the host
4979         self._terminate_volume_connections(context, instance, bdms)
4980 
4981         # Free up the resource allocations in the placement service.
4982         # This should happen *before* the vm_state is changed to
4983         # SHELVED_OFFLOADED in case client-side code is polling the API to
4984         # schedule more instances (or unshelve) once this server is offloaded.
4985         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
4986                                                                 instance)
4987 
4988         instance.power_state = current_power_state
4989         # NOTE(mriedem): The vm_state has to be set before updating the
4990         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
4991         # values cannot be nulled out until after updating the resource tracker
4992         # though.
4993         instance.vm_state = vm_states.SHELVED_OFFLOADED
4994         instance.task_state = None
4995         instance.save(expected_task_state=[task_states.SHELVING,
4996                                            task_states.SHELVING_OFFLOADING])
4997 
4998         # NOTE(ndipanov): Free resources from the resource tracker
4999         self._update_resource_tracker(context, instance)
5000 
5001         # NOTE(sfinucan): RPC calls should no longer be attempted against this
5002         # instance, so ensure any calls result in errors
5003         self._nil_out_instance_obj_host_and_node(instance)
5004         instance.save(expected_task_state=None)
5005 
5006         # TODO(melwitt): We should clean up instance console tokens here. The
5007         # instance has no host at this point and will need to establish a new
5008         # console connection in the future after it is unshelved.
5009         self._delete_scheduler_instance_info(context, instance.uuid)
5010         self._notify_about_instance_usage(context, instance,
5011                 'shelve_offload.end')
5012         compute_utils.notify_about_instance_action(context, instance,
5013                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
5014                 phase=fields.NotificationPhase.END, bdms=bdms)
5015 
5016     @wrap_exception()
5017     @reverts_task_state
5018     @wrap_instance_event(prefix='compute')
5019     @wrap_instance_fault
5020     def unshelve_instance(self, context, instance, image,
5021                           filter_properties, node):
5022         """Unshelve the instance.
5023 
5024         :param context: request context
5025         :param instance: a nova.objects.instance.Instance object
5026         :param image: an image to build from.  If None we assume a
5027             volume backed instance.
5028         :param filter_properties: dict containing limits, retry info etc.
5029         :param node: target compute node
5030         """
5031         if filter_properties is None:
5032             filter_properties = {}
5033 
5034         @utils.synchronized(instance.uuid)
5035         def do_unshelve_instance():
5036             self._unshelve_instance(context, instance, image,
5037                                     filter_properties, node)
5038         do_unshelve_instance()
5039 
5040     def _unshelve_instance_key_scrub(self, instance):
5041         """Remove data from the instance that may cause side effects."""
5042         cleaned_keys = dict(
5043                 key_data=instance.key_data,
5044                 auto_disk_config=instance.auto_disk_config)
5045         instance.key_data = None
5046         instance.auto_disk_config = False
5047         return cleaned_keys
5048 
5049     def _unshelve_instance_key_restore(self, instance, keys):
5050         """Restore previously scrubbed keys before saving the instance."""
5051         instance.update(keys)
5052 
5053     def _unshelve_instance(self, context, instance, image, filter_properties,
5054                            node):
5055         LOG.info('Unshelving', instance=instance)
5056         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5057                 context, instance.uuid)
5058 
5059         self._notify_about_instance_usage(context, instance, 'unshelve.start')
5060         compute_utils.notify_about_instance_action(context, instance,
5061                 self.host, action=fields.NotificationAction.UNSHELVE,
5062                 phase=fields.NotificationPhase.START, bdms=bdms)
5063 
5064         instance.task_state = task_states.SPAWNING
5065         instance.save()
5066 
5067         block_device_info = self._prep_block_device(context, instance, bdms)
5068         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
5069 
5070         if node is None:
5071             node = self._get_nodename(instance)
5072 
5073         limits = filter_properties.get('limits', {})
5074 
5075         allocations = self.reportclient.get_allocations_for_consumer(
5076             context, instance.uuid)
5077 
5078         shelved_image_ref = instance.image_ref
5079         if image:
5080             instance.image_ref = image['id']
5081             image_meta = objects.ImageMeta.from_dict(image)
5082         else:
5083             image_meta = objects.ImageMeta.from_dict(
5084                 utils.get_image_from_system_metadata(
5085                     instance.system_metadata))
5086 
5087         self.network_api.setup_instance_network_on_host(context, instance,
5088                                                         self.host)
5089         network_info = self.network_api.get_instance_nw_info(context, instance)
5090         try:
5091             with self.rt.instance_claim(context, instance, node, limits):
5092                 self.driver.spawn(context, instance, image_meta,
5093                                   injected_files=[],
5094                                   admin_password=None,
5095                                   allocations=allocations,
5096                                   network_info=network_info,
5097                                   block_device_info=block_device_info)
5098         except Exception:
5099             with excutils.save_and_reraise_exception(logger=LOG):
5100                 LOG.exception('Instance failed to spawn',
5101                               instance=instance)
5102                 # Cleanup allocations created by the scheduler on this host
5103                 # since we failed to spawn the instance. We do this both if
5104                 # the instance claim failed with ComputeResourcesUnavailable
5105                 # or if we did claim but the spawn failed, because aborting the
5106                 # instance claim will not remove the allocations.
5107                 self.reportclient.delete_allocation_for_instance(context,
5108                                                                  instance.uuid)
5109                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
5110                 self._terminate_volume_connections(context, instance, bdms)
5111                 # The reverts_task_state decorator on unshelve_instance will
5112                 # eventually save these updates.
5113                 self._nil_out_instance_obj_host_and_node(instance)
5114 
5115         if image:
5116             instance.image_ref = shelved_image_ref
5117             self._delete_snapshot_of_shelved_instance(context, instance,
5118                                                       image['id'])
5119 
5120         self._unshelve_instance_key_restore(instance, scrubbed_keys)
5121         self._update_instance_after_spawn(context, instance)
5122         # Delete system_metadata for a shelved instance
5123         compute_utils.remove_shelved_keys_from_system_metadata(instance)
5124 
5125         instance.save(expected_task_state=task_states.SPAWNING)
5126         self._update_scheduler_instance_info(context, instance)
5127         self._notify_about_instance_usage(context, instance, 'unshelve.end')
5128         compute_utils.notify_about_instance_action(context, instance,
5129                 self.host, action=fields.NotificationAction.UNSHELVE,
5130                 phase=fields.NotificationPhase.END, bdms=bdms)
5131 
5132     @messaging.expected_exceptions(NotImplementedError)
5133     @wrap_instance_fault
5134     def reset_network(self, context, instance):
5135         """Reset networking on the given instance."""
5136         LOG.debug('Reset network', instance=instance)
5137         self.driver.reset_network(instance)
5138 
5139     def _inject_network_info(self, context, instance, network_info):
5140         """Inject network info for the given instance."""
5141         LOG.debug('Inject network info', instance=instance)
5142         LOG.debug('network_info to inject: |%s|', network_info,
5143                   instance=instance)
5144 
5145         self.driver.inject_network_info(instance,
5146                                         network_info)
5147 
5148     @wrap_instance_fault
5149     def inject_network_info(self, context, instance):
5150         """Inject network info, but don't return the info."""
5151         network_info = self.network_api.get_instance_nw_info(context, instance)
5152         self._inject_network_info(context, instance, network_info)
5153 
5154     @messaging.expected_exceptions(NotImplementedError,
5155                                    exception.ConsoleNotAvailable,
5156                                    exception.InstanceNotFound)
5157     @wrap_exception()
5158     @wrap_instance_fault
5159     def get_console_output(self, context, instance, tail_length):
5160         """Send the console output for the given instance."""
5161         context = context.elevated()
5162         LOG.info("Get console output", instance=instance)
5163         output = self.driver.get_console_output(context, instance)
5164 
5165         if type(output) is six.text_type:
5166             output = six.b(output)
5167 
5168         if tail_length is not None:
5169             output = self._tail_log(output, tail_length)
5170 
5171         return output.decode('ascii', 'replace')
5172 
5173     def _tail_log(self, log, length):
5174         try:
5175             length = int(length)
5176         except ValueError:
5177             length = 0
5178 
5179         if length == 0:
5180             return b''
5181         else:
5182             return b'\n'.join(log.split(b'\n')[-int(length):])
5183 
5184     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5185                                    exception.InstanceNotReady,
5186                                    exception.InstanceNotFound,
5187                                    exception.ConsoleTypeUnavailable,
5188                                    NotImplementedError)
5189     @wrap_exception()
5190     @wrap_instance_fault
5191     def get_vnc_console(self, context, console_type, instance):
5192         """Return connection information for a vnc console."""
5193         context = context.elevated()
5194         LOG.debug("Getting vnc console", instance=instance)
5195 
5196         if not CONF.vnc.enabled:
5197             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5198 
5199         if console_type == 'novnc':
5200             # For essex, novncproxy_base_url must include the full path
5201             # including the html file (like http://myhost/vnc_auto.html)
5202             access_url_base = CONF.vnc.novncproxy_base_url
5203         elif console_type == 'xvpvnc':
5204             access_url_base = CONF.vnc.xvpvncproxy_base_url
5205         else:
5206             raise exception.ConsoleTypeInvalid(console_type=console_type)
5207 
5208         try:
5209             # Retrieve connect info from driver, and then decorate with our
5210             # access info token
5211             console = self.driver.get_vnc_console(context, instance)
5212             console_auth = objects.ConsoleAuthToken(
5213                 context=context,
5214                 console_type=console_type,
5215                 host=console.host,
5216                 port=console.port,
5217                 internal_access_path=console.internal_access_path,
5218                 instance_uuid=instance.uuid,
5219                 access_url_base=access_url_base,
5220             )
5221             console_auth.authorize(CONF.consoleauth.token_ttl)
5222             connect_info = console.get_connection_info(
5223                 console_auth.token, console_auth.access_url)
5224 
5225         except exception.InstanceNotFound:
5226             if instance.vm_state != vm_states.BUILDING:
5227                 raise
5228             raise exception.InstanceNotReady(instance_id=instance.uuid)
5229 
5230         return connect_info
5231 
5232     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5233                                    exception.InstanceNotReady,
5234                                    exception.InstanceNotFound,
5235                                    exception.ConsoleTypeUnavailable,
5236                                    NotImplementedError)
5237     @wrap_exception()
5238     @wrap_instance_fault
5239     def get_spice_console(self, context, console_type, instance):
5240         """Return connection information for a spice console."""
5241         context = context.elevated()
5242         LOG.debug("Getting spice console", instance=instance)
5243 
5244         if not CONF.spice.enabled:
5245             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5246 
5247         if console_type != 'spice-html5':
5248             raise exception.ConsoleTypeInvalid(console_type=console_type)
5249 
5250         try:
5251             # Retrieve connect info from driver, and then decorate with our
5252             # access info token
5253             console = self.driver.get_spice_console(context, instance)
5254             console_auth = objects.ConsoleAuthToken(
5255                 context=context,
5256                 console_type=console_type,
5257                 host=console.host,
5258                 port=console.port,
5259                 internal_access_path=console.internal_access_path,
5260                 instance_uuid=instance.uuid,
5261                 access_url_base=CONF.spice.html5proxy_base_url,
5262             )
5263             console_auth.authorize(CONF.consoleauth.token_ttl)
5264             connect_info = console.get_connection_info(
5265                 console_auth.token, console_auth.access_url)
5266 
5267         except exception.InstanceNotFound:
5268             if instance.vm_state != vm_states.BUILDING:
5269                 raise
5270             raise exception.InstanceNotReady(instance_id=instance.uuid)
5271 
5272         return connect_info
5273 
5274     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5275                                    exception.InstanceNotReady,
5276                                    exception.InstanceNotFound,
5277                                    exception.ConsoleTypeUnavailable,
5278                                    NotImplementedError)
5279     @wrap_exception()
5280     @wrap_instance_fault
5281     def get_rdp_console(self, context, console_type, instance):
5282         """Return connection information for a RDP console."""
5283         context = context.elevated()
5284         LOG.debug("Getting RDP console", instance=instance)
5285 
5286         if not CONF.rdp.enabled:
5287             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5288 
5289         if console_type != 'rdp-html5':
5290             raise exception.ConsoleTypeInvalid(console_type=console_type)
5291 
5292         try:
5293             # Retrieve connect info from driver, and then decorate with our
5294             # access info token
5295             console = self.driver.get_rdp_console(context, instance)
5296             console_auth = objects.ConsoleAuthToken(
5297                 context=context,
5298                 console_type=console_type,
5299                 host=console.host,
5300                 port=console.port,
5301                 internal_access_path=console.internal_access_path,
5302                 instance_uuid=instance.uuid,
5303                 access_url_base=CONF.rdp.html5_proxy_base_url,
5304             )
5305             console_auth.authorize(CONF.consoleauth.token_ttl)
5306             connect_info = console.get_connection_info(
5307                 console_auth.token, console_auth.access_url)
5308 
5309         except exception.InstanceNotFound:
5310             if instance.vm_state != vm_states.BUILDING:
5311                 raise
5312             raise exception.InstanceNotReady(instance_id=instance.uuid)
5313 
5314         return connect_info
5315 
5316     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5317                                    exception.InstanceNotReady,
5318                                    exception.InstanceNotFound,
5319                                    exception.ConsoleTypeUnavailable,
5320                                    NotImplementedError)
5321     @wrap_exception()
5322     @wrap_instance_fault
5323     def get_mks_console(self, context, console_type, instance):
5324         """Return connection information for a MKS console."""
5325         context = context.elevated()
5326         LOG.debug("Getting MKS console", instance=instance)
5327 
5328         if not CONF.mks.enabled:
5329             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5330 
5331         if console_type != 'webmks':
5332             raise exception.ConsoleTypeInvalid(console_type=console_type)
5333 
5334         try:
5335             # Retrieve connect info from driver, and then decorate with our
5336             # access info token
5337             console = self.driver.get_mks_console(context, instance)
5338             console_auth = objects.ConsoleAuthToken(
5339                 context=context,
5340                 console_type=console_type,
5341                 host=console.host,
5342                 port=console.port,
5343                 internal_access_path=console.internal_access_path,
5344                 instance_uuid=instance.uuid,
5345                 access_url_base=CONF.mks.mksproxy_base_url,
5346             )
5347             console_auth.authorize(CONF.consoleauth.token_ttl)
5348             connect_info = console.get_connection_info(
5349                 console_auth.token, console_auth.access_url)
5350 
5351         except exception.InstanceNotFound:
5352             if instance.vm_state != vm_states.BUILDING:
5353                 raise
5354             raise exception.InstanceNotReady(instance_id=instance.uuid)
5355 
5356         return connect_info
5357 
5358     @messaging.expected_exceptions(
5359         exception.ConsoleTypeInvalid,
5360         exception.InstanceNotReady,
5361         exception.InstanceNotFound,
5362         exception.ConsoleTypeUnavailable,
5363         exception.SocketPortRangeExhaustedException,
5364         exception.ImageSerialPortNumberInvalid,
5365         exception.ImageSerialPortNumberExceedFlavorValue,
5366         NotImplementedError)
5367     @wrap_exception()
5368     @wrap_instance_fault
5369     def get_serial_console(self, context, console_type, instance):
5370         """Returns connection information for a serial console."""
5371 
5372         LOG.debug("Getting serial console", instance=instance)
5373 
5374         if not CONF.serial_console.enabled:
5375             raise exception.ConsoleTypeUnavailable(console_type=console_type)
5376 
5377         context = context.elevated()
5378 
5379         try:
5380             # Retrieve connect info from driver, and then decorate with our
5381             # access info token
5382             console = self.driver.get_serial_console(context, instance)
5383             console_auth = objects.ConsoleAuthToken(
5384                 context=context,
5385                 console_type=console_type,
5386                 host=console.host,
5387                 port=console.port,
5388                 internal_access_path=console.internal_access_path,
5389                 instance_uuid=instance.uuid,
5390                 access_url_base=CONF.serial_console.base_url,
5391             )
5392             console_auth.authorize(CONF.consoleauth.token_ttl)
5393             connect_info = console.get_connection_info(
5394                 console_auth.token, console_auth.access_url)
5395 
5396         except exception.InstanceNotFound:
5397             if instance.vm_state != vm_states.BUILDING:
5398                 raise
5399             raise exception.InstanceNotReady(instance_id=instance.uuid)
5400 
5401         return connect_info
5402 
5403     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
5404                                    exception.InstanceNotReady,
5405                                    exception.InstanceNotFound)
5406     @wrap_exception()
5407     @wrap_instance_fault
5408     def validate_console_port(self, ctxt, instance, port, console_type):
5409         if console_type == "spice-html5":
5410             console_info = self.driver.get_spice_console(ctxt, instance)
5411         elif console_type == "rdp-html5":
5412             console_info = self.driver.get_rdp_console(ctxt, instance)
5413         elif console_type == "serial":
5414             console_info = self.driver.get_serial_console(ctxt, instance)
5415         elif console_type == "webmks":
5416             console_info = self.driver.get_mks_console(ctxt, instance)
5417         else:
5418             console_info = self.driver.get_vnc_console(ctxt, instance)
5419 
5420         # Some drivers may return an int on console_info.port but the port
5421         # variable in this method is a string, so cast to be sure we are
5422         # comparing the correct types.
5423         return str(console_info.port) == port
5424 
5425     @wrap_exception()
5426     @reverts_task_state
5427     @wrap_instance_fault
5428     def reserve_block_device_name(self, context, instance, device,
5429                                   volume_id, disk_bus, device_type, tag,
5430                                   multiattach):
5431         if (tag and not
5432                 self.driver.capabilities.get('supports_tagged_attach_volume',
5433                                              False)):
5434             raise exception.VolumeTaggedAttachNotSupported()
5435 
5436         if (multiattach and not
5437                 self.driver.capabilities.get('supports_multiattach', False)):
5438             raise exception.MultiattachNotSupportedByVirtDriver(
5439                 volume_id=volume_id)
5440 
5441         @utils.synchronized(instance.uuid)
5442         def do_reserve():
5443             bdms = (
5444                 objects.BlockDeviceMappingList.get_by_instance_uuid(
5445                     context, instance.uuid))
5446 
5447             # NOTE(ndipanov): We need to explicitly set all the fields on the
5448             #                 object so that obj_load_attr does not fail
5449             new_bdm = objects.BlockDeviceMapping(
5450                     context=context,
5451                     source_type='volume', destination_type='volume',
5452                     instance_uuid=instance.uuid, boot_index=None,
5453                     volume_id=volume_id,
5454                     device_name=device, guest_format=None,
5455                     disk_bus=disk_bus, device_type=device_type, tag=tag)
5456 
5457             new_bdm.device_name = self._get_device_name_for_instance(
5458                     instance, bdms, new_bdm)
5459 
5460             # NOTE(vish): create bdm here to avoid race condition
5461             new_bdm.create()
5462             return new_bdm
5463 
5464         return do_reserve()
5465 
5466     @wrap_exception()
5467     @wrap_instance_event(prefix='compute')
5468     @wrap_instance_fault
5469     def attach_volume(self, context, instance, bdm):
5470         """Attach a volume to an instance."""
5471         driver_bdm = driver_block_device.convert_volume(bdm)
5472 
5473         @utils.synchronized(instance.uuid)
5474         def do_attach_volume(context, instance, driver_bdm):
5475             try:
5476                 return self._attach_volume(context, instance, driver_bdm)
5477             except Exception:
5478                 with excutils.save_and_reraise_exception():
5479                     bdm.destroy()
5480 
5481         do_attach_volume(context, instance, driver_bdm)
5482 
5483     def _attach_volume(self, context, instance, bdm):
5484         context = context.elevated()
5485         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
5486                  {'volume_id': bdm.volume_id,
5487                   'mountpoint': bdm['mount_device']},
5488                  instance=instance)
5489         compute_utils.notify_about_volume_attach_detach(
5490             context, instance, self.host,
5491             action=fields.NotificationAction.VOLUME_ATTACH,
5492             phase=fields.NotificationPhase.START,
5493             volume_id=bdm.volume_id)
5494         try:
5495             bdm.attach(context, instance, self.volume_api, self.driver,
5496                        do_driver_attach=True)
5497         except Exception as e:
5498             with excutils.save_and_reraise_exception():
5499                 LOG.exception("Failed to attach %(volume_id)s "
5500                               "at %(mountpoint)s",
5501                               {'volume_id': bdm.volume_id,
5502                                'mountpoint': bdm['mount_device']},
5503                               instance=instance)
5504                 if bdm['attachment_id']:
5505                     # Try to delete the attachment to make the volume
5506                     # available again. Note that DriverVolumeBlockDevice
5507                     # may have already deleted the attachment so ignore
5508                     # VolumeAttachmentNotFound.
5509                     try:
5510                         self.volume_api.attachment_delete(
5511                             context, bdm['attachment_id'])
5512                     except exception.VolumeAttachmentNotFound as exc:
5513                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
5514                                   exc, instance=instance)
5515                 else:
5516                     self.volume_api.unreserve_volume(context, bdm.volume_id)
5517                 tb = traceback.format_exc()
5518                 compute_utils.notify_about_volume_attach_detach(
5519                     context, instance, self.host,
5520                     action=fields.NotificationAction.VOLUME_ATTACH,
5521                     phase=fields.NotificationPhase.ERROR,
5522                     exception=e,
5523                     volume_id=bdm.volume_id, tb=tb)
5524 
5525         info = {'volume_id': bdm.volume_id}
5526         self._notify_about_instance_usage(
5527             context, instance, "volume.attach", extra_usage_info=info)
5528         compute_utils.notify_about_volume_attach_detach(
5529             context, instance, self.host,
5530             action=fields.NotificationAction.VOLUME_ATTACH,
5531             phase=fields.NotificationPhase.END,
5532             volume_id=bdm.volume_id)
5533 
5534     def _notify_volume_usage_detach(self, context, instance, bdm):
5535         if CONF.volume_usage_poll_interval <= 0:
5536             return
5537 
5538         mp = bdm.device_name
5539         # Handle bootable volumes which will not contain /dev/
5540         if '/dev/' in mp:
5541             mp = mp[5:]
5542         try:
5543             vol_stats = self.driver.block_stats(instance, mp)
5544             if vol_stats is None:
5545                 return
5546         except NotImplementedError:
5547             return
5548 
5549         LOG.debug("Updating volume usage cache with totals", instance=instance)
5550         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
5551         vol_usage = objects.VolumeUsage(context)
5552         vol_usage.volume_id = bdm.volume_id
5553         vol_usage.instance_uuid = instance.uuid
5554         vol_usage.project_id = instance.project_id
5555         vol_usage.user_id = instance.user_id
5556         vol_usage.availability_zone = instance.availability_zone
5557         vol_usage.curr_reads = rd_req
5558         vol_usage.curr_read_bytes = rd_bytes
5559         vol_usage.curr_writes = wr_req
5560         vol_usage.curr_write_bytes = wr_bytes
5561         vol_usage.save(update_totals=True)
5562         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
5563         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
5564 
5565     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
5566                        attachment_id=None):
5567         """Detach a volume from an instance.
5568 
5569         :param context: security context
5570         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
5571         :param instance: the Instance object to detach the volume from
5572         :param destroy_bdm: if True, the corresponding BDM entry will be marked
5573                             as deleted. Disabling this is useful for operations
5574                             like rebuild, when we don't want to destroy BDM
5575         :param attachment_id: The volume attachment_id for the given instance
5576                               and volume.
5577         """
5578         volume_id = bdm.volume_id
5579         compute_utils.notify_about_volume_attach_detach(
5580             context, instance, self.host,
5581             action=fields.NotificationAction.VOLUME_DETACH,
5582             phase=fields.NotificationPhase.START,
5583             volume_id=volume_id)
5584 
5585         self._notify_volume_usage_detach(context, instance, bdm)
5586 
5587         LOG.info('Detaching volume %(volume_id)s',
5588                  {'volume_id': volume_id}, instance=instance)
5589 
5590         driver_bdm = driver_block_device.convert_volume(bdm)
5591         driver_bdm.detach(context, instance, self.volume_api, self.driver,
5592                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
5593 
5594         info = dict(volume_id=volume_id)
5595         self._notify_about_instance_usage(
5596             context, instance, "volume.detach", extra_usage_info=info)
5597         compute_utils.notify_about_volume_attach_detach(
5598             context, instance, self.host,
5599             action=fields.NotificationAction.VOLUME_DETACH,
5600             phase=fields.NotificationPhase.END,
5601             volume_id=volume_id)
5602 
5603         if 'tag' in bdm and bdm.tag:
5604             self._delete_disk_metadata(instance, bdm)
5605         if destroy_bdm:
5606             bdm.destroy()
5607 
5608     def _delete_disk_metadata(self, instance, bdm):
5609         for device in instance.device_metadata.devices:
5610             if isinstance(device, objects.DiskMetadata):
5611                 if 'serial' in device:
5612                     if device.serial == bdm.volume_id:
5613                         instance.device_metadata.devices.remove(device)
5614                         instance.save()
5615                         break
5616                 else:
5617                     # NOTE(artom) We log the entire device object because all
5618                     # fields are nullable and may not be set
5619                     LOG.warning('Unable to determine whether to clean up '
5620                                 'device metadata for disk %s', device,
5621                                 instance=instance)
5622 
5623     @wrap_exception()
5624     @wrap_instance_event(prefix='compute')
5625     @wrap_instance_fault
5626     def detach_volume(self, context, volume_id, instance, attachment_id):
5627         """Detach a volume from an instance.
5628 
5629         :param context: security context
5630         :param volume_id: the volume id
5631         :param instance: the Instance object to detach the volume from
5632         :param attachment_id: The volume attachment_id for the given instance
5633                               and volume.
5634 
5635         """
5636         @utils.synchronized(instance.uuid)
5637         def do_detach_volume(context, volume_id, instance, attachment_id):
5638             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5639                     context, volume_id, instance.uuid)
5640             self._detach_volume(context, bdm, instance,
5641                                 attachment_id=attachment_id)
5642 
5643         do_detach_volume(context, volume_id, instance, attachment_id)
5644 
5645     def _init_volume_connection(self, context, new_volume,
5646                                 old_volume_id, connector, bdm,
5647                                 new_attachment_id, mountpoint):
5648         new_volume_id = new_volume['id']
5649         if new_attachment_id is None:
5650             # We're dealing with an old-style attachment so initialize the
5651             # connection so we can get the connection_info.
5652             new_cinfo = self.volume_api.initialize_connection(context,
5653                                                               new_volume_id,
5654                                                               connector)
5655         else:
5656             # Check for multiattach on the new volume and if True, check to
5657             # see if the virt driver supports multiattach.
5658             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
5659             # and should be consolidated into some common code at some point.
5660             vol_multiattach = new_volume.get('multiattach', False)
5661             virt_multiattach = self.driver.capabilities.get(
5662                 'supports_multiattach', False)
5663             if vol_multiattach and not virt_multiattach:
5664                 raise exception.MultiattachNotSupportedByVirtDriver(
5665                     volume_id=new_volume_id)
5666 
5667             # This is a new style attachment and the API created the new
5668             # volume attachment and passed the id to the compute over RPC.
5669             # At this point we need to update the new volume attachment with
5670             # the host connector, which will give us back the new attachment
5671             # connection_info.
5672             new_cinfo = self.volume_api.attachment_update(
5673                 context, new_attachment_id, connector,
5674                 mountpoint)['connection_info']
5675 
5676             if vol_multiattach:
5677                 # This will be used by the volume driver to determine the
5678                 # proper disk configuration.
5679                 new_cinfo['multiattach'] = True
5680 
5681         old_cinfo = jsonutils.loads(bdm['connection_info'])
5682         if old_cinfo and 'serial' not in old_cinfo:
5683             old_cinfo['serial'] = old_volume_id
5684         # NOTE(lyarwood): serial is not always present in the returned
5685         # connection_info so set it if it is missing as we do in
5686         # DriverVolumeBlockDevice.attach().
5687         if 'serial' not in new_cinfo:
5688             new_cinfo['serial'] = new_volume_id
5689         return (old_cinfo, new_cinfo)
5690 
5691     def _swap_volume(self, context, instance, bdm, connector,
5692                      old_volume_id, new_volume, resize_to,
5693                      new_attachment_id, is_cinder_migration):
5694         new_volume_id = new_volume['id']
5695         mountpoint = bdm['device_name']
5696         failed = False
5697         new_cinfo = None
5698         try:
5699             old_cinfo, new_cinfo = self._init_volume_connection(
5700                 context, new_volume, old_volume_id, connector,
5701                 bdm, new_attachment_id, mountpoint)
5702             # NOTE(lyarwood): The Libvirt driver, the only virt driver
5703             # currently implementing swap_volume, will modify the contents of
5704             # new_cinfo when connect_volume is called. This is then saved to
5705             # the BDM in swap_volume for future use outside of this flow.
5706             msg = ("swap_volume: Calling driver volume swap with "
5707                    "connection infos: new: %(new_cinfo)s; "
5708                    "old: %(old_cinfo)s" %
5709                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
5710             # Both new and old info might contain password
5711             LOG.debug(strutils.mask_password(msg), instance=instance)
5712 
5713             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
5714                                     mountpoint, resize_to)
5715             if new_attachment_id:
5716                 self.volume_api.attachment_complete(context, new_attachment_id)
5717             msg = ("swap_volume: Driver volume swap returned, new "
5718                    "connection_info is now : %(new_cinfo)s" %
5719                    {'new_cinfo': new_cinfo})
5720             LOG.debug(strutils.mask_password(msg))
5721         except Exception as ex:
5722             failed = True
5723             with excutils.save_and_reraise_exception():
5724                 tb = traceback.format_exc()
5725                 compute_utils.notify_about_volume_swap(
5726                     context, instance, self.host,
5727                     fields.NotificationPhase.ERROR,
5728                     old_volume_id, new_volume_id, ex, tb)
5729                 if new_cinfo:
5730                     msg = ("Failed to swap volume %(old_volume_id)s "
5731                            "for %(new_volume_id)s")
5732                     LOG.exception(msg, {'old_volume_id': old_volume_id,
5733                                         'new_volume_id': new_volume_id},
5734                                   instance=instance)
5735                 else:
5736                     msg = ("Failed to connect to volume %(volume_id)s "
5737                            "with volume at %(mountpoint)s")
5738                     LOG.exception(msg, {'volume_id': new_volume_id,
5739                                         'mountpoint': bdm['device_name']},
5740                                   instance=instance)
5741 
5742                 # The API marked the volume as 'detaching' for the old volume
5743                 # so we need to roll that back so the volume goes back to
5744                 # 'in-use' state.
5745                 self.volume_api.roll_detaching(context, old_volume_id)
5746 
5747                 if new_attachment_id is None:
5748                     # The API reserved the new volume so it would be in
5749                     # 'attaching' status, so we need to unreserve it so it
5750                     # goes back to 'available' status.
5751                     self.volume_api.unreserve_volume(context, new_volume_id)
5752                 else:
5753                     # This is a new style attachment for the new volume, which
5754                     # was created in the API. We just need to delete it here
5755                     # to put the new volume back into 'available' status.
5756                     self.volume_api.attachment_delete(
5757                         context, new_attachment_id)
5758         finally:
5759             # TODO(mriedem): This finally block is terribly confusing and is
5760             # trying to do too much. We should consider removing the finally
5761             # block and move whatever needs to happen on success and failure
5762             # into the blocks above for clarity, even if it means a bit of
5763             # redundant code.
5764             conn_volume = new_volume_id if failed else old_volume_id
5765             if new_cinfo:
5766                 LOG.debug("swap_volume: removing Cinder connection "
5767                           "for volume %(volume)s", {'volume': conn_volume},
5768                           instance=instance)
5769                 if bdm.attachment_id is None:
5770                     # This is the pre-3.44 flow for new-style volume
5771                     # attachments so just terminate the connection.
5772                     self.volume_api.terminate_connection(context,
5773                                                          conn_volume,
5774                                                          connector)
5775                 else:
5776                     # This is a new style volume attachment. If we failed, then
5777                     # the new attachment was already deleted above in the
5778                     # exception block and we have nothing more to do here. If
5779                     # swap_volume was successful in the driver, then we need to
5780                     # "detach" the original attachment by deleting it.
5781                     if not failed:
5782                         self.volume_api.attachment_delete(
5783                             context, bdm.attachment_id)
5784 
5785             # Need to make some decisions based on whether this was
5786             # a Cinder initiated migration or not. The callback to
5787             # migration completion isn't needed in the case of a
5788             # nova initiated simple swap of two volume
5789             # "volume-update" call so skip that. The new attachment
5790             # scenarios will give us a new attachment record and
5791             # that's what we want.
5792             if bdm.attachment_id and not is_cinder_migration:
5793                 # we don't callback to cinder
5794                 comp_ret = {'save_volume_id': new_volume_id}
5795             else:
5796                 # NOTE(lyarwood): The following call to
5797                 # os-migrate-volume-completion returns a dict containing
5798                 # save_volume_id, this volume id has two possible values :
5799                 # 1. old_volume_id if we are migrating (retyping) volumes
5800                 # 2. new_volume_id if we are swapping between two existing
5801                 #    volumes
5802                 # This volume id is later used to update the volume_id and
5803                 # connection_info['serial'] of the BDM.
5804                 comp_ret = self.volume_api.migrate_volume_completion(
5805                                                           context,
5806                                                           old_volume_id,
5807                                                           new_volume_id,
5808                                                           error=failed)
5809                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
5810                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
5811                           instance=instance)
5812 
5813         return (comp_ret, new_cinfo)
5814 
5815     @wrap_exception()
5816     @wrap_instance_event(prefix='compute')
5817     @wrap_instance_fault
5818     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
5819                     new_attachment_id):
5820         """Swap volume for an instance."""
5821         context = context.elevated()
5822 
5823         compute_utils.notify_about_volume_swap(
5824             context, instance, self.host,
5825             fields.NotificationPhase.START,
5826             old_volume_id, new_volume_id)
5827 
5828         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5829                 context, old_volume_id, instance.uuid)
5830         connector = self.driver.get_volume_connector(instance)
5831 
5832         resize_to = 0
5833         old_volume = self.volume_api.get(context, old_volume_id)
5834         # Yes this is a tightly-coupled state check of what's going on inside
5835         # cinder, but we need this while we still support old (v1/v2) and
5836         # new style attachments (v3.44). Once we drop support for old style
5837         # attachments we could think about cleaning up the cinder-initiated
5838         # swap volume API flows.
5839         is_cinder_migration = (
5840             True if old_volume['status'] in ('retyping',
5841                                              'migrating') else False)
5842         old_vol_size = old_volume['size']
5843         new_volume = self.volume_api.get(context, new_volume_id)
5844         new_vol_size = new_volume['size']
5845         if new_vol_size > old_vol_size:
5846             resize_to = new_vol_size
5847 
5848         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
5849                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
5850                  instance=instance)
5851         comp_ret, new_cinfo = self._swap_volume(context,
5852                                                 instance,
5853                                                 bdm,
5854                                                 connector,
5855                                                 old_volume_id,
5856                                                 new_volume,
5857                                                 resize_to,
5858                                                 new_attachment_id,
5859                                                 is_cinder_migration)
5860 
5861         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
5862         # correct volume_id returned by Cinder.
5863         save_volume_id = comp_ret['save_volume_id']
5864         new_cinfo['serial'] = save_volume_id
5865         values = {
5866             'connection_info': jsonutils.dumps(new_cinfo),
5867             'source_type': 'volume',
5868             'destination_type': 'volume',
5869             'snapshot_id': None,
5870             'volume_id': save_volume_id,
5871             'no_device': None}
5872 
5873         if resize_to:
5874             values['volume_size'] = resize_to
5875 
5876         if new_attachment_id is not None:
5877             # This was a volume swap for a new-style attachment so we
5878             # need to update the BDM attachment_id for the new attachment.
5879             values['attachment_id'] = new_attachment_id
5880 
5881         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
5882                   "%(updates)s", {'volume_id': bdm.volume_id,
5883                                   'updates': values},
5884                   instance=instance)
5885         bdm.update(values)
5886         bdm.save()
5887 
5888         compute_utils.notify_about_volume_swap(
5889             context, instance, self.host,
5890             fields.NotificationPhase.END,
5891             old_volume_id, new_volume_id)
5892 
5893     @wrap_exception()
5894     def remove_volume_connection(self, context, volume_id, instance):
5895         """Remove the volume connection on this host
5896 
5897         Detach the volume from this instance on this host, and if this is
5898         the cinder v2 flow, call cinder to terminate the connection.
5899         """
5900         try:
5901             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
5902                     context, volume_id, instance.uuid)
5903             driver_bdm = driver_block_device.convert_volume(bdm)
5904             driver_bdm.driver_detach(context, instance,
5905                                      self.volume_api, self.driver)
5906             if bdm.attachment_id is None:
5907                 # cinder v2 api flow
5908                 connector = self.driver.get_volume_connector(instance)
5909                 self.volume_api.terminate_connection(context, volume_id,
5910                                                      connector)
5911         except exception.NotFound:
5912             pass
5913 
5914     @wrap_exception()
5915     @wrap_instance_event(prefix='compute')
5916     @wrap_instance_fault
5917     def attach_interface(self, context, instance, network_id, port_id,
5918                          requested_ip, tag):
5919         """Use hotplug to add an network adapter to an instance."""
5920         if not self.driver.capabilities.get('supports_attach_interface',
5921                                             False):
5922             raise exception.AttachInterfaceNotSupported(
5923                 instance_uuid=instance.uuid)
5924         if (tag and not
5925             self.driver.capabilities.get('supports_tagged_attach_interface',
5926                                          False)):
5927             raise exception.NetworkInterfaceTaggedAttachNotSupported()
5928 
5929         compute_utils.notify_about_instance_action(
5930             context, instance, self.host,
5931             action=fields.NotificationAction.INTERFACE_ATTACH,
5932             phase=fields.NotificationPhase.START)
5933 
5934         bind_host_id = self.driver.network_binding_host_id(context, instance)
5935         network_info = self.network_api.allocate_port_for_instance(
5936             context, instance, port_id, network_id, requested_ip,
5937             bind_host_id=bind_host_id, tag=tag)
5938         if len(network_info) != 1:
5939             LOG.error('allocate_port_for_instance returned %(ports)s '
5940                       'ports', {'ports': len(network_info)})
5941             # TODO(elod.illes): an instance.interface_attach.error notification
5942             # should be sent here
5943             raise exception.InterfaceAttachFailed(
5944                     instance_uuid=instance.uuid)
5945         image_meta = objects.ImageMeta.from_instance(instance)
5946 
5947         try:
5948             self.driver.attach_interface(context, instance, image_meta,
5949                                          network_info[0])
5950         except exception.NovaException as ex:
5951             port_id = network_info[0].get('id')
5952             LOG.warning("attach interface failed , try to deallocate "
5953                         "port %(port_id)s, reason: %(msg)s",
5954                         {'port_id': port_id, 'msg': ex},
5955                         instance=instance)
5956             try:
5957                 self.network_api.deallocate_port_for_instance(
5958                     context, instance, port_id)
5959             except Exception:
5960                 LOG.warning("deallocate port %(port_id)s failed",
5961                             {'port_id': port_id}, instance=instance)
5962 
5963             tb = traceback.format_exc()
5964             compute_utils.notify_about_instance_action(
5965                 context, instance, self.host,
5966                 action=fields.NotificationAction.INTERFACE_ATTACH,
5967                 phase=fields.NotificationPhase.ERROR,
5968                 exception=ex, tb=tb)
5969 
5970             raise exception.InterfaceAttachFailed(
5971                 instance_uuid=instance.uuid)
5972 
5973         compute_utils.notify_about_instance_action(
5974             context, instance, self.host,
5975             action=fields.NotificationAction.INTERFACE_ATTACH,
5976             phase=fields.NotificationPhase.END)
5977 
5978         return network_info[0]
5979 
5980     @wrap_exception()
5981     @wrap_instance_event(prefix='compute')
5982     @wrap_instance_fault
5983     def detach_interface(self, context, instance, port_id):
5984         """Detach a network adapter from an instance."""
5985         network_info = instance.info_cache.network_info
5986         condemned = None
5987         for vif in network_info:
5988             if vif['id'] == port_id:
5989                 condemned = vif
5990                 break
5991         if condemned is None:
5992             raise exception.PortNotFound(_("Port %s is not "
5993                                            "attached") % port_id)
5994 
5995         compute_utils.notify_about_instance_action(
5996             context, instance, self.host,
5997             action=fields.NotificationAction.INTERFACE_DETACH,
5998             phase=fields.NotificationPhase.START)
5999 
6000         try:
6001             self.driver.detach_interface(context, instance, condemned)
6002         except exception.NovaException as ex:
6003             # If the instance was deleted before the interface was detached,
6004             # just log it at debug.
6005             log_level = (logging.DEBUG
6006                          if isinstance(ex, exception.InstanceNotFound)
6007                          else logging.WARNING)
6008             LOG.log(log_level,
6009                     "Detach interface failed, port_id=%(port_id)s, reason: "
6010                     "%(msg)s", {'port_id': port_id, 'msg': ex},
6011                     instance=instance)
6012             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
6013         else:
6014             try:
6015                 self.network_api.deallocate_port_for_instance(
6016                     context, instance, port_id)
6017             except Exception as ex:
6018                 with excutils.save_and_reraise_exception():
6019                     # Since this is a cast operation, log the failure for
6020                     # triage.
6021                     LOG.warning('Failed to deallocate port %(port_id)s '
6022                                 'for instance. Error: %(error)s',
6023                                 {'port_id': port_id, 'error': ex},
6024                                 instance=instance)
6025 
6026         compute_utils.notify_about_instance_action(
6027             context, instance, self.host,
6028             action=fields.NotificationAction.INTERFACE_DETACH,
6029             phase=fields.NotificationPhase.END)
6030 
6031     def _get_compute_info(self, context, host):
6032         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
6033             context, host)
6034 
6035     @wrap_exception()
6036     def check_instance_shared_storage(self, ctxt, instance, data):
6037         """Check if the instance files are shared
6038 
6039         :param ctxt: security context
6040         :param instance: dict of instance data
6041         :param data: result of driver.check_instance_shared_storage_local
6042 
6043         Returns True if instance disks located on shared storage and
6044         False otherwise.
6045         """
6046         return self.driver.check_instance_shared_storage_remote(ctxt, data)
6047 
6048     @wrap_exception()
6049     @wrap_instance_event(prefix='compute')
6050     @wrap_instance_fault
6051     def check_can_live_migrate_destination(self, ctxt, instance,
6052                                            block_migration, disk_over_commit):
6053         """Check if it is possible to execute live migration.
6054 
6055         This runs checks on the destination host, and then calls
6056         back to the source host to check the results.
6057 
6058         :param context: security context
6059         :param instance: dict of instance data
6060         :param block_migration: if true, prepare for block migration
6061                                 if None, calculate it in driver
6062         :param disk_over_commit: if true, allow disk over commit
6063                                  if None, ignore disk usage checking
6064         :returns: a dict containing migration info
6065         """
6066         src_compute_info = obj_base.obj_to_primitive(
6067             self._get_compute_info(ctxt, instance.host))
6068         dst_compute_info = obj_base.obj_to_primitive(
6069             self._get_compute_info(ctxt, CONF.host))
6070         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
6071             instance, src_compute_info, dst_compute_info,
6072             block_migration, disk_over_commit)
6073         LOG.debug('destination check data is %s', dest_check_data)
6074         try:
6075             migrate_data = self.compute_rpcapi.\
6076                                 check_can_live_migrate_source(ctxt, instance,
6077                                                               dest_check_data)
6078         finally:
6079             self.driver.cleanup_live_migration_destination_check(ctxt,
6080                     dest_check_data)
6081         return migrate_data
6082 
6083     @wrap_exception()
6084     @wrap_instance_event(prefix='compute')
6085     @wrap_instance_fault
6086     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
6087         """Check if it is possible to execute live migration.
6088 
6089         This checks if the live migration can succeed, based on the
6090         results from check_can_live_migrate_destination.
6091 
6092         :param ctxt: security context
6093         :param instance: dict of instance data
6094         :param dest_check_data: result of check_can_live_migrate_destination
6095         :returns: a dict containing migration info
6096         """
6097         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6098             ctxt, instance.uuid)
6099         is_volume_backed = compute_utils.is_volume_backed_instance(
6100             ctxt, instance, bdms)
6101         dest_check_data.is_volume_backed = is_volume_backed
6102         block_device_info = self._get_instance_block_device_info(
6103                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
6104         result = self.driver.check_can_live_migrate_source(ctxt, instance,
6105                                                            dest_check_data,
6106                                                            block_device_info)
6107         LOG.debug('source check data is %s', result)
6108         return result
6109 
6110     @wrap_exception()
6111     @wrap_instance_event(prefix='compute')
6112     @wrap_instance_fault
6113     def pre_live_migration(self, context, instance, block_migration, disk,
6114                            migrate_data):
6115         """Preparations for live migration at dest host.
6116 
6117         :param context: security context
6118         :param instance: dict of instance data
6119         :param block_migration: if true, prepare for block migration
6120         :param disk: disk info of instance
6121         :param migrate_data: A dict or LiveMigrateData object holding data
6122                              required for live migration without shared
6123                              storage.
6124         :returns: migrate_data containing additional migration info
6125         """
6126         LOG.debug('pre_live_migration data is %s', migrate_data)
6127 
6128         migrate_data.old_vol_attachment_ids = {}
6129         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6130             context, instance.uuid)
6131         network_info = self.network_api.get_instance_nw_info(context, instance)
6132         self._notify_about_instance_usage(
6133             context, instance, "live_migration.pre.start",
6134             network_info=network_info)
6135         compute_utils.notify_about_instance_action(
6136             context, instance, self.host,
6137             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
6138             phase=fields.NotificationPhase.START, bdms=bdms)
6139 
6140         connector = self.driver.get_volume_connector(instance)
6141         try:
6142             for bdm in bdms:
6143                 if bdm.is_volume and bdm.attachment_id is not None:
6144                     # This bdm uses the new cinder v3.44 API.
6145                     # We will create a new attachment for this
6146                     # volume on this migration destination host. The old
6147                     # attachment will be deleted on the source host
6148                     # when the migration succeeds. The old attachment_id
6149                     # is stored in dict with the key being the bdm.volume_id
6150                     # so it can be restored on rollback.
6151                     #
6152                     # Also note that attachment_update is not needed as we
6153                     # are providing the connector in the create call.
6154                     attach_ref = self.volume_api.attachment_create(
6155                         context, bdm.volume_id, bdm.instance_uuid,
6156                         connector=connector, mountpoint=bdm.device_name)
6157 
6158                     # save current attachment so we can detach it on success,
6159                     # or restore it on a rollback.
6160                     # NOTE(mdbooth): This data is no longer used by the source
6161                     # host since change I0390c9ff. We can't remove it until we
6162                     # are sure the source host has been upgraded.
6163                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
6164                         bdm.attachment_id
6165 
6166                     # update the bdm with the new attachment_id.
6167                     bdm.attachment_id = attach_ref['id']
6168                     bdm.save()
6169 
6170             block_device_info = self._get_instance_block_device_info(
6171                                 context, instance, refresh_conn_info=True,
6172                                 bdms=bdms)
6173 
6174             # The driver pre_live_migration will plug vifs on the host. We call
6175             # plug_vifs before calling ensure_filtering_rules_for_instance, to
6176             # ensure bridge is set up.
6177             migrate_data = self.driver.pre_live_migration(context,
6178                                            instance,
6179                                            block_device_info,
6180                                            network_info,
6181                                            disk,
6182                                            migrate_data)
6183             LOG.debug('driver pre_live_migration data is %s', migrate_data)
6184             # driver.pre_live_migration is what plugs vifs on the destination
6185             # host so now we can set the wait_for_vif_plugged flag in the
6186             # migrate_data object which the source compute will use to
6187             # determine if it should wait for a 'network-vif-plugged' event
6188             # from neutron before starting the actual guest transfer in the
6189             # hypervisor
6190             migrate_data.wait_for_vif_plugged = (
6191                 CONF.compute.live_migration_wait_for_vif_plug)
6192 
6193             # NOTE(tr3buchet): setup networks on destination host
6194             self.network_api.setup_networks_on_host(context, instance,
6195                                                              self.host)
6196 
6197             # Creating filters to hypervisors and firewalls.
6198             # An example is that nova-instance-instance-xxx,
6199             # which is written to libvirt.xml(Check "virsh nwfilter-list")
6200             # This nwfilter is necessary on the destination host.
6201             # In addition, this method is creating filtering rule
6202             # onto destination host.
6203             self.driver.ensure_filtering_rules_for_instance(instance,
6204                                                 network_info)
6205         except Exception:
6206             # If we raise, migrate_data with the updated attachment ids
6207             # will not be returned to the source host for rollback.
6208             # So we need to rollback new attachments here.
6209             with excutils.save_and_reraise_exception():
6210                 old_attachments = migrate_data.old_vol_attachment_ids
6211                 for bdm in bdms:
6212                     if (bdm.is_volume and bdm.attachment_id is not None and
6213                             bdm.volume_id in old_attachments):
6214                         self.volume_api.attachment_delete(context,
6215                                                           bdm.attachment_id)
6216                         bdm.attachment_id = old_attachments[bdm.volume_id]
6217                         bdm.save()
6218 
6219         # Volume connections are complete, tell cinder that all the
6220         # attachments have completed.
6221         for bdm in bdms:
6222             if bdm.is_volume and bdm.attachment_id is not None:
6223                 self.volume_api.attachment_complete(context,
6224                                                     bdm.attachment_id)
6225 
6226         self._notify_about_instance_usage(
6227                      context, instance, "live_migration.pre.end",
6228                      network_info=network_info)
6229         compute_utils.notify_about_instance_action(
6230             context, instance, self.host,
6231             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
6232             phase=fields.NotificationPhase.END, bdms=bdms)
6233 
6234         LOG.debug('pre_live_migration result data is %s', migrate_data)
6235         return migrate_data
6236 
6237     @staticmethod
6238     def _neutron_failed_live_migration_callback(event_name, instance):
6239         msg = ('Neutron reported failure during live migration '
6240                'with %(event)s for instance %(uuid)s')
6241         msg_args = {'event': event_name, 'uuid': instance.uuid}
6242         if CONF.vif_plugging_is_fatal:
6243             raise exception.VirtualInterfacePlugException(msg % msg_args)
6244         LOG.error(msg, msg_args)
6245 
6246     @staticmethod
6247     def _get_neutron_events_for_live_migration(instance):
6248         # We don't generate events if CONF.vif_plugging_timeout=0
6249         # meaning that the operator disabled using them.
6250         if CONF.vif_plugging_timeout and utils.is_neutron():
6251             return [('network-vif-plugged', vif['id'])
6252                     for vif in instance.get_network_info()]
6253         else:
6254             return []
6255 
6256     def _cleanup_pre_live_migration(self, context, dest, instance,
6257                                     migration, migrate_data):
6258         """Helper method for when pre_live_migration fails
6259 
6260         Sets the migration status to "error" and rolls back the live migration
6261         setup on the destination host.
6262 
6263         :param context: The user request context.
6264         :type context: nova.context.RequestContext
6265         :param dest: The live migration destination hostname.
6266         :type dest: str
6267         :param instance: The instance being live migrated.
6268         :type instance: nova.objects.Instance
6269         :param migration: The migration record tracking this live migration.
6270         :type migration: nova.objects.Migration
6271         :param migrate_data: Data about the live migration, populated from
6272                              the destination host.
6273         :type migrate_data: Subclass of nova.objects.LiveMigrateData
6274         """
6275         self._set_migration_status(migration, 'error')
6276         # Make sure we set this for _rollback_live_migration()
6277         # so it can find it, as expected if it was called later
6278         migrate_data.migration = migration
6279         self._rollback_live_migration(context, instance, dest,
6280                                       migrate_data)
6281 
6282     def _do_live_migration(self, context, dest, instance, block_migration,
6283                            migration, migrate_data):
6284         # NOTE(danms): We should enhance the RT to account for migrations
6285         # and use the status field to denote when the accounting has been
6286         # done on source/destination. For now, this is just here for status
6287         # reporting
6288         self._set_migration_status(migration, 'preparing')
6289         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6290                 context, instance.uuid)
6291 
6292         class _BreakWaitForInstanceEvent(Exception):
6293             """Used as a signal to stop waiting for the network-vif-plugged
6294             event when we discover that
6295             [compute]/live_migration_wait_for_vif_plug is not set on the
6296             destination.
6297             """
6298             pass
6299 
6300         events = self._get_neutron_events_for_live_migration(instance)
6301         try:
6302             if ('block_migration' in migrate_data and
6303                     migrate_data.block_migration):
6304                 block_device_info = self._get_instance_block_device_info(
6305                     context, instance, bdms=source_bdms)
6306                 disk = self.driver.get_instance_disk_info(
6307                     instance, block_device_info=block_device_info)
6308             else:
6309                 disk = None
6310 
6311             deadline = CONF.vif_plugging_timeout
6312             error_cb = self._neutron_failed_live_migration_callback
6313             # In order to avoid a race with the vif plugging that the virt
6314             # driver does on the destination host, we register our events
6315             # to wait for before calling pre_live_migration. Then if the
6316             # dest host reports back that we shouldn't wait, we can break
6317             # out of the context manager using _BreakWaitForInstanceEvent.
6318             with self.virtapi.wait_for_instance_event(
6319                     instance, events, deadline=deadline,
6320                     error_callback=error_cb):
6321                 with timeutils.StopWatch() as timer:
6322                     migrate_data = self.compute_rpcapi.pre_live_migration(
6323                         context, instance,
6324                         block_migration, disk, dest, migrate_data)
6325                 LOG.info('Took %0.2f seconds for pre_live_migration on '
6326                          'destination host %s.',
6327                          timer.elapsed(), dest, instance=instance)
6328                 wait_for_vif_plugged = (
6329                     'wait_for_vif_plugged' in migrate_data and
6330                     migrate_data.wait_for_vif_plugged)
6331                 if events and not wait_for_vif_plugged:
6332                     raise _BreakWaitForInstanceEvent
6333         except _BreakWaitForInstanceEvent:
6334             if events:
6335                 LOG.debug('Not waiting for events after pre_live_migration: '
6336                           '%s. ', events, instance=instance)
6337             # This is a bit weird, but we need to clear sys.exc_info() so that
6338             # oslo.log formatting does not inadvertently use it later if an
6339             # error message is logged without an explicit exc_info. This is
6340             # only a problem with python 2.
6341             if six.PY2:
6342                 sys.exc_clear()
6343         except exception.VirtualInterfacePlugException:
6344             with excutils.save_and_reraise_exception():
6345                 LOG.exception('Failed waiting for network virtual interfaces '
6346                               'to be plugged on the destination host %s.',
6347                               dest, instance=instance)
6348                 self._cleanup_pre_live_migration(
6349                     context, dest, instance, migration, migrate_data)
6350         except eventlet.timeout.Timeout:
6351             msg = 'Timed out waiting for events: %s'
6352             LOG.warning(msg, events, instance=instance)
6353             if CONF.vif_plugging_is_fatal:
6354                 self._cleanup_pre_live_migration(
6355                     context, dest, instance, migration, migrate_data)
6356                 raise exception.MigrationError(reason=msg % events)
6357         except Exception:
6358             with excutils.save_and_reraise_exception():
6359                 LOG.exception('Pre live migration failed at %s',
6360                               dest, instance=instance)
6361                 self._cleanup_pre_live_migration(
6362                     context, dest, instance, migration, migrate_data)
6363 
6364         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
6365         # if it exist in the queue, then we are good to moving on, if
6366         # not, some other process must have aborted it, then we should
6367         # rollback.
6368         try:
6369             self._waiting_live_migrations.pop(instance.uuid)
6370         except KeyError:
6371             LOG.debug('Migration %s aborted by another process, rollback.',
6372                       migration.uuid, instance=instance)
6373             migrate_data.migration = migration
6374             self._rollback_live_migration(context, instance, dest,
6375                                           migrate_data, 'cancelled')
6376             self._notify_live_migrate_abort_end(context, instance)
6377             return
6378 
6379         self._set_migration_status(migration, 'running')
6380         if migrate_data:
6381             migrate_data.migration = migration
6382 
6383         # NOTE(mdbooth): pre_live_migration will update connection_info and
6384         # attachment_id on all volume BDMS to reflect the new destination
6385         # host attachment. We fetch BDMs before that to retain connection_info
6386         # and attachment_id relating to the source host for post migration
6387         # cleanup.
6388         post_live_migration = functools.partial(self._post_live_migration,
6389                                                 source_bdms=source_bdms)
6390 
6391         LOG.debug('live_migration data is %s', migrate_data)
6392         try:
6393             self.driver.live_migration(context, instance, dest,
6394                                        post_live_migration,
6395                                        self._rollback_live_migration,
6396                                        block_migration, migrate_data)
6397         except Exception:
6398             LOG.exception('Live migration failed.', instance=instance)
6399             with excutils.save_and_reraise_exception():
6400                 # Put instance and migration into error state,
6401                 # as its almost certainly too late to rollback
6402                 self._set_migration_status(migration, 'error')
6403                 # first refresh instance as it may have got updated by
6404                 # post_live_migration_at_destination
6405                 instance.refresh()
6406                 self._set_instance_obj_error_state(context, instance,
6407                                                    clean_task_state=True)
6408 
6409     @wrap_exception()
6410     @wrap_instance_event(prefix='compute')
6411     @wrap_instance_fault
6412     def live_migration(self, context, dest, instance, block_migration,
6413                        migration, migrate_data):
6414         """Executing live migration.
6415 
6416         :param context: security context
6417         :param dest: destination host
6418         :param instance: a nova.objects.instance.Instance object
6419         :param block_migration: if true, prepare for block migration
6420         :param migration: an nova.objects.Migration object
6421         :param migrate_data: implementation specific params
6422 
6423         """
6424         self._set_migration_status(migration, 'queued')
6425         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
6426         # put the returned Future object into dict mapped with migration.uuid
6427         # in order to be able to track and abort it in the future.
6428         self._waiting_live_migrations[instance.uuid] = (None, None)
6429         try:
6430             future = self._live_migration_executor.submit(
6431                 self._do_live_migration, context, dest, instance,
6432                 block_migration, migration, migrate_data)
6433             self._waiting_live_migrations[instance.uuid] = (migration, future)
6434         except RuntimeError:
6435             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
6436             # pool is shutdown, which happens in
6437             # _cleanup_live_migrations_in_pool.
6438             LOG.info('Migration %s failed to submit as the compute service '
6439                      'is shutting down.', migration.uuid, instance=instance)
6440             self._set_migration_status(migration, 'error')
6441             raise exception.LiveMigrationNotSubmitted(
6442                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
6443 
6444     @wrap_exception()
6445     @wrap_instance_event(prefix='compute')
6446     @wrap_instance_fault
6447     def live_migration_force_complete(self, context, instance):
6448         """Force live migration to complete.
6449 
6450         :param context: Security context
6451         :param instance: The instance that is being migrated
6452         """
6453 
6454         self._notify_about_instance_usage(
6455             context, instance, 'live.migration.force.complete.start')
6456         compute_utils.notify_about_instance_action(
6457             context, instance, self.host,
6458             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
6459             phase=fields.NotificationPhase.START)
6460         self.driver.live_migration_force_complete(instance)
6461         self._notify_about_instance_usage(
6462             context, instance, 'live.migration.force.complete.end')
6463         compute_utils.notify_about_instance_action(
6464             context, instance, self.host,
6465             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
6466             phase=fields.NotificationPhase.END)
6467 
6468     def _notify_live_migrate_abort_end(self, context, instance):
6469         self._notify_about_instance_usage(
6470             context, instance, 'live.migration.abort.end')
6471         compute_utils.notify_about_instance_action(
6472             context, instance, self.host,
6473             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
6474             phase=fields.NotificationPhase.END)
6475 
6476     @wrap_exception()
6477     @wrap_instance_event(prefix='compute')
6478     @wrap_instance_fault
6479     def live_migration_abort(self, context, instance, migration_id):
6480         """Abort an in-progress live migration.
6481 
6482         :param context: Security context
6483         :param instance: The instance that is being migrated
6484         :param migration_id: ID of in-progress live migration
6485 
6486         """
6487         self._notify_about_instance_usage(
6488             context, instance, 'live.migration.abort.start')
6489         compute_utils.notify_about_instance_action(
6490             context, instance, self.host,
6491             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
6492             phase=fields.NotificationPhase.START)
6493         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
6494         # lead to 3 scenarios:
6495         # 1. The selected migration is still in queue, and the future.cancel()
6496         #    succeed, then the abort action is succeed, mark the migration
6497         #    status to 'cancelled'.
6498         # 2. The selected migration is still in queue, but the future.cancel()
6499         #    failed, then the _do_live_migration() has started executing, and
6500         #    the migration status is 'preparing', then we just pop it from the
6501         #    queue, and the migration process will handle it later. And the
6502         #    migration status couldn't be 'running' in this scenario because
6503         #    if _do_live_migration has started executing and we've already
6504         #    popped it from the queue and set the migration status to
6505         #    'running' at this point, popping it here will raise KeyError at
6506         #    which point we check if it's running and if so, we abort the old
6507         #    way.
6508         # 3. The selected migration is not in the queue, then the migration
6509         #    status is 'running', let the driver handle it.
6510         try:
6511             migration, future = (
6512                 self._waiting_live_migrations.pop(instance.uuid))
6513             if future and future.cancel():
6514                 # If we got here, we've successfully aborted the queued
6515                 # migration and _do_live_migration won't run so we need
6516                 # to set the migration status to cancelled and send the
6517                 # notification. If Future.cancel() fails, it means
6518                 # _do_live_migration is running and the migration status
6519                 # is preparing, and _do_live_migration() itself will attempt
6520                 # to pop the queued migration, hit a KeyError, and rollback,
6521                 # set the migration to cancelled and send the
6522                 # live.migration.abort.end notification.
6523                 self._set_migration_status(migration, 'cancelled')
6524         except KeyError:
6525             migration = objects.Migration.get_by_id(context, migration_id)
6526             if migration.status != 'running':
6527                 raise exception.InvalidMigrationState(
6528                     migration_id=migration_id, instance_uuid=instance.uuid,
6529                     state=migration.status, method='abort live migration')
6530             self.driver.live_migration_abort(instance)
6531         self._notify_live_migrate_abort_end(context, instance)
6532 
6533     def _live_migration_cleanup_flags(self, migrate_data):
6534         """Determine whether disks or instance path need to be cleaned up after
6535         live migration (at source on success, at destination on rollback)
6536 
6537         Block migration needs empty image at destination host before migration
6538         starts, so if any failure occurs, any empty images has to be deleted.
6539 
6540         Also Volume backed live migration w/o shared storage needs to delete
6541         newly created instance-xxx dir on the destination as a part of its
6542         rollback process
6543 
6544         :param migrate_data: implementation specific data
6545         :returns: (bool, bool) -- do_cleanup, destroy_disks
6546         """
6547         # NOTE(pkoniszewski): block migration specific params are set inside
6548         # migrate_data objects for drivers that expose block live migration
6549         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
6550         # cleanup is not needed.
6551         do_cleanup = False
6552         destroy_disks = False
6553         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
6554             # No instance booting at source host, but instance dir
6555             # must be deleted for preparing next block migration
6556             # must be deleted for preparing next live migration w/o shared
6557             # storage
6558             do_cleanup = not migrate_data.is_shared_instance_path
6559             destroy_disks = not migrate_data.is_shared_block_storage
6560         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
6561             do_cleanup = migrate_data.block_migration
6562             destroy_disks = migrate_data.block_migration
6563         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
6564             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
6565             do_cleanup = True
6566             destroy_disks = not migrate_data.is_shared_instance_path
6567 
6568         return (do_cleanup, destroy_disks)
6569 
6570     @wrap_exception()
6571     @wrap_instance_fault
6572     def _post_live_migration(self, ctxt, instance, dest,
6573                              block_migration=False, migrate_data=None,
6574                              source_bdms=None):
6575         """Post operations for live migration.
6576 
6577         This method is called from live_migration
6578         and mainly updating database record.
6579 
6580         :param ctxt: security context
6581         :param instance: instance dict
6582         :param dest: destination host
6583         :param block_migration: if true, prepare for block migration
6584         :param migrate_data: if not None, it is a dict which has data
6585         :param source_bdms: BDMs prior to modification by the destination
6586                             compute host. Set by _do_live_migration and not
6587                             part of the callback interface, so this is never
6588                             None
6589         required for live migration without shared storage
6590 
6591         """
6592         LOG.info('_post_live_migration() is started..',
6593                  instance=instance)
6594 
6595         # Cleanup source host post live-migration
6596         block_device_info = self._get_instance_block_device_info(
6597                             ctxt, instance, bdms=source_bdms)
6598         self.driver.post_live_migration(ctxt, instance, block_device_info,
6599                                         migrate_data)
6600 
6601         # Detaching volumes.
6602         connector = self.driver.get_volume_connector(instance)
6603         for bdm in source_bdms:
6604             if bdm.is_volume:
6605                 # Detaching volumes is a call to an external API that can fail.
6606                 # If it does, we need to handle it gracefully so that the call
6607                 # to post_live_migration_at_destination - where we set instance
6608                 # host and task state - still happens. We need to rethink the
6609                 # current approach of setting instance host and task state
6610                 # AFTER a whole bunch of things that could fail in unhandled
6611                 # ways, but that is left as a TODO(artom).
6612                 try:
6613                     if bdm.attachment_id is None:
6614                         # Prior to cinder v3.44:
6615                         # We don't want to actually mark the volume detached,
6616                         # or delete the bdm, just remove the connection from
6617                         # this host.
6618                         #
6619                         # remove the volume connection without detaching from
6620                         # hypervisor because the instance is not running
6621                         # anymore on the current host
6622                         self.volume_api.terminate_connection(ctxt,
6623                                                              bdm.volume_id,
6624                                                              connector)
6625                     else:
6626                         # cinder v3.44 api flow - delete the old attachment
6627                         # for the source host
6628                         self.volume_api.attachment_delete(ctxt,
6629                                                           bdm.attachment_id)
6630 
6631                 except Exception as e:
6632                     if bdm.attachment_id is None:
6633                         LOG.error('Connection for volume %s not terminated on '
6634                                   'source host %s during post_live_migration: '
6635                                    '%s', bdm.volume_id, self.host,
6636                                    six.text_type(e), instance=instance)
6637                     else:
6638                         LOG.error('Volume attachment %s not deleted on source '
6639                                   'host %s during post_live_migration: %s',
6640                                   bdm.attachment_id, self.host,
6641                                   six.text_type(e), instance=instance)
6642 
6643         # Releasing vlan.
6644         # (not necessary in current implementation?)
6645 
6646         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6647 
6648         self._notify_about_instance_usage(ctxt, instance,
6649                                           "live_migration._post.start",
6650                                           network_info=network_info)
6651         compute_utils.notify_about_instance_action(
6652             ctxt, instance, self.host,
6653             action=fields.NotificationAction.LIVE_MIGRATION_POST,
6654             phase=fields.NotificationPhase.START)
6655         # Releasing security group ingress rule.
6656         LOG.debug('Calling driver.unfilter_instance from _post_live_migration',
6657                   instance=instance)
6658         self.driver.unfilter_instance(instance,
6659                                       network_info)
6660 
6661         migration = {'source_compute': self.host,
6662                      'dest_compute': dest, }
6663         # For neutron, migrate_instance_start will activate the destination
6664         # host port bindings, if there are any created by conductor before live
6665         # migration started.
6666         self.network_api.migrate_instance_start(ctxt,
6667                                                 instance,
6668                                                 migration)
6669 
6670         destroy_vifs = False
6671         try:
6672             # It's possible that the vif type changed on the destination
6673             # host and is already bound and active, so we need to use the
6674             # stashed source vifs in migrate_data.vifs (if present) to unplug
6675             # on the source host.
6676             unplug_nw_info = network_info
6677             if migrate_data and 'vifs' in migrate_data:
6678                 nw_info = []
6679                 for migrate_vif in migrate_data.vifs:
6680                     nw_info.append(migrate_vif.source_vif)
6681                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
6682                 LOG.debug('Calling driver.post_live_migration_at_source '
6683                           'with original source VIFs from migrate_data: %s',
6684                           unplug_nw_info, instance=instance)
6685             self.driver.post_live_migration_at_source(ctxt, instance,
6686                                                       unplug_nw_info)
6687         except NotImplementedError as ex:
6688             LOG.debug(ex, instance=instance)
6689             # For all hypervisors other than libvirt, there is a possibility
6690             # they are unplugging networks from source node in the cleanup
6691             # method
6692             destroy_vifs = True
6693 
6694         # NOTE(danms): Save source node before calling post method on
6695         # destination, which will update it
6696         source_node = instance.node
6697 
6698         # Define domain at destination host, without doing it,
6699         # pause/suspend/terminate do not work.
6700         post_at_dest_success = True
6701         try:
6702             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
6703                     instance, block_migration, dest)
6704         except Exception as error:
6705             post_at_dest_success = False
6706             # We don't want to break _post_live_migration() if
6707             # post_live_migration_at_destination() fails as it should never
6708             # affect cleaning up source node.
6709             LOG.exception("Post live migration at destination %s failed",
6710                           dest, instance=instance, error=error)
6711 
6712         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
6713                 migrate_data)
6714 
6715         if do_cleanup:
6716             LOG.debug('Calling driver.cleanup from _post_live_migration',
6717                       instance=instance)
6718             self.driver.cleanup(ctxt, instance, unplug_nw_info,
6719                                 destroy_disks=destroy_disks,
6720                                 migrate_data=migrate_data,
6721                                 destroy_vifs=destroy_vifs)
6722 
6723         self.instance_events.clear_events_for_instance(instance)
6724 
6725         # NOTE(timello): make sure we update available resources on source
6726         # host even before next periodic task.
6727         self.update_available_resource(ctxt)
6728 
6729         self._update_scheduler_instance_info(ctxt, instance)
6730         self._notify_about_instance_usage(ctxt, instance,
6731                                           "live_migration._post.end",
6732                                           network_info=network_info)
6733         compute_utils.notify_about_instance_action(
6734             ctxt, instance, self.host,
6735             action=fields.NotificationAction.LIVE_MIGRATION_POST,
6736             phase=fields.NotificationPhase.END)
6737         if post_at_dest_success:
6738             LOG.info('Migrating instance to %s finished successfully.',
6739                      dest, instance=instance)
6740 
6741         self._clean_instance_console_tokens(ctxt, instance)
6742         if migrate_data and migrate_data.obj_attr_is_set('migration'):
6743             migrate_data.migration.status = 'completed'
6744             migrate_data.migration.save()
6745             self._delete_allocation_after_move(ctxt,
6746                                                instance,
6747                                                migrate_data.migration)
6748         else:
6749             # We didn't have data on a migration, which means we can't
6750             # look up to see if we had new-style migration-based
6751             # allocations. This should really only happen in cases of
6752             # a buggy virt driver. Log a warning so we know it happened.
6753             LOG.warning('Live migration ended with no migrate_data '
6754                         'record. Unable to clean up migration-based '
6755                         'allocations for node %s which is almost certainly '
6756                         'not an expected situation.', source_node,
6757                         instance=instance)
6758 
6759     def _consoles_enabled(self):
6760         """Returns whether a console is enable."""
6761         return (CONF.vnc.enabled or CONF.spice.enabled or
6762                 CONF.rdp.enabled or CONF.serial_console.enabled or
6763                 CONF.mks.enabled)
6764 
6765     def _clean_instance_console_tokens(self, ctxt, instance):
6766         """Clean console tokens stored for an instance."""
6767         # If the database backend isn't in use, don't bother trying to clean
6768         # tokens. The database backend is not supported for cells v1.
6769         if not CONF.cells.enable and self._consoles_enabled():
6770             objects.ConsoleAuthToken.\
6771                 clean_console_auths_for_instance(ctxt, instance.uuid)
6772 
6773     @wrap_exception()
6774     @wrap_instance_event(prefix='compute')
6775     @wrap_instance_fault
6776     def post_live_migration_at_destination(self, context, instance,
6777                                            block_migration):
6778         """Post operations for live migration .
6779 
6780         :param context: security context
6781         :param instance: Instance dict
6782         :param block_migration: if true, prepare for block migration
6783 
6784         """
6785         LOG.info('Post operation of migration started',
6786                  instance=instance)
6787 
6788         # NOTE(tr3buchet): setup networks on destination host
6789         #                  this is called a second time because
6790         #                  multi_host does not create the bridge in
6791         #                  plug_vifs
6792         # NOTE(mriedem): This is a no-op for neutron.
6793         self.network_api.setup_networks_on_host(context, instance,
6794                                                          self.host)
6795         migration = {'source_compute': instance.host,
6796                      'dest_compute': self.host, }
6797         self.network_api.migrate_instance_finish(context,
6798                                                  instance,
6799                                                  migration)
6800 
6801         network_info = self.network_api.get_instance_nw_info(context, instance)
6802         self._notify_about_instance_usage(
6803                      context, instance, "live_migration.post.dest.start",
6804                      network_info=network_info)
6805         compute_utils.notify_about_instance_action(context, instance,
6806                 self.host,
6807                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
6808                 phase=fields.NotificationPhase.START)
6809         block_device_info = self._get_instance_block_device_info(context,
6810                                                                  instance)
6811 
6812         try:
6813             self.driver.post_live_migration_at_destination(
6814                 context, instance, network_info, block_migration,
6815                 block_device_info)
6816         except Exception:
6817             with excutils.save_and_reraise_exception():
6818                 instance.vm_state = vm_states.ERROR
6819                 LOG.error('Unexpected error during post live migration at '
6820                           'destination host.', instance=instance)
6821         finally:
6822             # Restore instance state and update host
6823             current_power_state = self._get_power_state(context, instance)
6824             node_name = None
6825             prev_host = instance.host
6826             try:
6827                 compute_node = self._get_compute_info(context, self.host)
6828                 node_name = compute_node.hypervisor_hostname
6829             except exception.ComputeHostNotFound:
6830                 LOG.exception('Failed to get compute_info for %s', self.host)
6831             finally:
6832                 instance.host = self.host
6833                 instance.power_state = current_power_state
6834                 instance.task_state = None
6835                 instance.node = node_name
6836                 instance.progress = 0
6837                 instance.save(expected_task_state=task_states.MIGRATING)
6838 
6839         # NOTE(tr3buchet): tear down networks on source host (nova-net)
6840         # NOTE(mriedem): For neutron, this will delete any inactive source
6841         # host port bindings.
6842         try:
6843             self.network_api.setup_networks_on_host(context, instance,
6844                                                     prev_host, teardown=True)
6845         except exception.PortBindingDeletionFailed as e:
6846             # Removing the inactive port bindings from the source host is not
6847             # critical so just log an error but don't fail.
6848             LOG.error('Network cleanup failed for source host %s during post '
6849                       'live migration. You may need to manually clean up '
6850                       'resources in the network service. Error: %s',
6851                       prev_host, six.text_type(e))
6852         # NOTE(vish): this is necessary to update dhcp for nova-network
6853         # NOTE(mriedem): This is a no-op for neutron.
6854         self.network_api.setup_networks_on_host(context, instance, self.host)
6855         self._notify_about_instance_usage(
6856                      context, instance, "live_migration.post.dest.end",
6857                      network_info=network_info)
6858         compute_utils.notify_about_instance_action(context, instance,
6859                 self.host,
6860                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
6861                 phase=fields.NotificationPhase.END)
6862 
6863     @wrap_exception()
6864     @wrap_instance_fault
6865     def _rollback_live_migration(self, context, instance,
6866                                  dest, migrate_data=None,
6867                                  migration_status='error'):
6868         """Recovers Instance/volume state from migrating -> running.
6869 
6870         :param context: security context
6871         :param instance: nova.objects.instance.Instance object
6872         :param dest:
6873             This method is called from live migration src host.
6874             This param specifies destination host.
6875         :param migrate_data:
6876             if not none, contains implementation specific data.
6877         :param migration_status:
6878             Contains the status we want to set for the migration object
6879 
6880         """
6881         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
6882               migrate_data.obj_attr_is_set('migration')):
6883             migration = migrate_data.migration
6884         else:
6885             migration = None
6886 
6887         if migration:
6888             # Remove allocations created in Placement for the dest node.
6889             # If migration is None, the virt driver didn't pass it which is
6890             # a bug.
6891             self._revert_allocation(context, instance, migration)
6892         else:
6893             LOG.error('Unable to revert allocations during live migration '
6894                       'rollback; compute driver did not provide migrate_data',
6895                       instance=instance)
6896 
6897         instance.task_state = None
6898         instance.progress = 0
6899         instance.save(expected_task_state=[task_states.MIGRATING])
6900 
6901         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
6902         #                  for nova-network)
6903         # NOTE(mriedem): This is a no-op for neutron.
6904         self.network_api.setup_networks_on_host(context, instance, self.host)
6905 
6906         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6907                 context, instance.uuid)
6908         for bdm in bdms:
6909             if bdm.is_volume:
6910                 # remove the connection on the destination host
6911                 self.compute_rpcapi.remove_volume_connection(
6912                         context, instance, bdm.volume_id, dest)
6913 
6914                 if bdm.attachment_id:
6915                     # 3.44 cinder api flow. Set the bdm's
6916                     # attachment_id to the old attachment of the source
6917                     # host. If old_attachments is not there, then
6918                     # there was an error before the new attachment was made.
6919                     old_attachments = migrate_data.old_vol_attachment_ids \
6920                         if 'old_vol_attachment_ids' in migrate_data else None
6921                     if old_attachments and bdm.volume_id in old_attachments:
6922                         self.volume_api.attachment_delete(context,
6923                                                           bdm.attachment_id)
6924                         bdm.attachment_id = old_attachments[bdm.volume_id]
6925                         bdm.save()
6926 
6927         self._notify_about_instance_usage(context, instance,
6928                                           "live_migration._rollback.start")
6929         compute_utils.notify_about_instance_action(context, instance,
6930                 self.host,
6931                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
6932                 phase=fields.NotificationPhase.START,
6933                 bdms=bdms)
6934 
6935         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
6936                 migrate_data)
6937 
6938         if do_cleanup:
6939             self.compute_rpcapi.rollback_live_migration_at_destination(
6940                     context, instance, dest, destroy_disks=destroy_disks,
6941                     migrate_data=migrate_data)
6942         elif utils.is_neutron():
6943             # The port binding profiles need to be cleaned up.
6944             with errors_out_migration_ctxt(migration):
6945                 try:
6946                     # This call will delete any inactive destination host
6947                     # port bindings.
6948                     self.network_api.setup_networks_on_host(
6949                         context, instance, host=dest, teardown=True)
6950                 except exception.PortBindingDeletionFailed as e:
6951                     # Removing the inactive port bindings from the destination
6952                     # host is not critical so just log an error but don't fail.
6953                     LOG.error(
6954                         'Network cleanup failed for destination host %s '
6955                         'during live migration rollback. You may need to '
6956                         'manually clean up resources in the network service. '
6957                         'Error: %s', dest, six.text_type(e))
6958                 except Exception:
6959                     with excutils.save_and_reraise_exception():
6960                         LOG.exception(
6961                             'An error occurred while cleaning up networking '
6962                             'during live migration rollback.',
6963                             instance=instance)
6964 
6965         self._notify_about_instance_usage(context, instance,
6966                                           "live_migration._rollback.end")
6967         compute_utils.notify_about_instance_action(context, instance,
6968                 self.host,
6969                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
6970                 phase=fields.NotificationPhase.END,
6971                 bdms=bdms)
6972 
6973         self._set_migration_status(migration, migration_status)
6974 
6975     @wrap_exception()
6976     @wrap_instance_event(prefix='compute')
6977     @wrap_instance_fault
6978     def rollback_live_migration_at_destination(self, context, instance,
6979                                                destroy_disks,
6980                                                migrate_data):
6981         """Cleaning up image directory that is created pre_live_migration.
6982 
6983         :param context: security context
6984         :param instance: a nova.objects.instance.Instance object sent over rpc
6985         :param destroy_disks: whether to destroy volumes or not
6986         :param migrate_data: contains migration info
6987         """
6988         network_info = self.network_api.get_instance_nw_info(context, instance)
6989         self._notify_about_instance_usage(
6990                       context, instance, "live_migration.rollback.dest.start",
6991                       network_info=network_info)
6992         compute_utils.notify_about_instance_action(
6993             context, instance, self.host,
6994             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
6995             phase=fields.NotificationPhase.START)
6996         try:
6997             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
6998             # NOTE(mriedem): For neutron, this call will delete any
6999             # destination host port bindings.
7000             # TODO(mriedem): We should eventually remove this call from
7001             # this method (rollback_live_migration_at_destination) since this
7002             # method is only called conditionally based on whether or not the
7003             # instance is running on shared storage. _rollback_live_migration
7004             # already calls this method for neutron if we are running on
7005             # shared storage.
7006             self.network_api.setup_networks_on_host(context, instance,
7007                                                     self.host, teardown=True)
7008         except exception.PortBindingDeletionFailed as e:
7009             # Removing the inactive port bindings from the destination
7010             # host is not critical so just log an error but don't fail.
7011             LOG.error(
7012                 'Network cleanup failed for destination host %s '
7013                 'during live migration rollback. You may need to '
7014                 'manually clean up resources in the network service. '
7015                 'Error: %s', self.host, six.text_type(e))
7016         except Exception:
7017             with excutils.save_and_reraise_exception():
7018                 # NOTE(tdurakov): even if teardown networks fails driver
7019                 # should try to rollback live migration on destination.
7020                 LOG.exception('An error occurred while deallocating network.',
7021                               instance=instance)
7022         finally:
7023             # always run this even if setup_networks_on_host fails
7024             # NOTE(vish): The mapping is passed in so the driver can disconnect
7025             #             from remote volumes if necessary
7026             block_device_info = self._get_instance_block_device_info(context,
7027                                                                      instance)
7028             self.driver.rollback_live_migration_at_destination(
7029                 context, instance, network_info, block_device_info,
7030                 destroy_disks=destroy_disks, migrate_data=migrate_data)
7031 
7032         self._notify_about_instance_usage(
7033                         context, instance, "live_migration.rollback.dest.end",
7034                         network_info=network_info)
7035         compute_utils.notify_about_instance_action(
7036             context, instance, self.host,
7037             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
7038             phase=fields.NotificationPhase.END)
7039 
7040     @periodic_task.periodic_task(
7041         spacing=CONF.heal_instance_info_cache_interval)
7042     def _heal_instance_info_cache(self, context):
7043         """Called periodically.  On every call, try to update the
7044         info_cache's network information for another instance by
7045         calling to the network manager.
7046 
7047         This is implemented by keeping a cache of uuids of instances
7048         that live on this host.  On each call, we pop one off of a
7049         list, pull the DB record, and try the call to the network API.
7050         If anything errors don't fail, as it's possible the instance
7051         has been deleted, etc.
7052         """
7053         heal_interval = CONF.heal_instance_info_cache_interval
7054         if not heal_interval:
7055             return
7056 
7057         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
7058         instance = None
7059 
7060         LOG.debug('Starting heal instance info cache')
7061 
7062         if not instance_uuids:
7063             # The list of instances to heal is empty so rebuild it
7064             LOG.debug('Rebuilding the list of instances to heal')
7065             db_instances = objects.InstanceList.get_by_host(
7066                 context, self.host, expected_attrs=[], use_slave=True)
7067             for inst in db_instances:
7068                 # We don't want to refresh the cache for instances
7069                 # which are building or deleting so don't put them
7070                 # in the list. If they are building they will get
7071                 # added to the list next time we build it.
7072                 if (inst.vm_state == vm_states.BUILDING):
7073                     LOG.debug('Skipping network cache update for instance '
7074                               'because it is Building.', instance=inst)
7075                     continue
7076                 if (inst.task_state == task_states.DELETING):
7077                     LOG.debug('Skipping network cache update for instance '
7078                               'because it is being deleted.', instance=inst)
7079                     continue
7080 
7081                 if not instance:
7082                     # Save the first one we find so we don't
7083                     # have to get it again
7084                     instance = inst
7085                 else:
7086                     instance_uuids.append(inst['uuid'])
7087 
7088             self._instance_uuids_to_heal = instance_uuids
7089         else:
7090             # Find the next valid instance on the list
7091             while instance_uuids:
7092                 try:
7093                     inst = objects.Instance.get_by_uuid(
7094                             context, instance_uuids.pop(0),
7095                             expected_attrs=['system_metadata', 'info_cache',
7096                                             'flavor'],
7097                             use_slave=True)
7098                 except exception.InstanceNotFound:
7099                     # Instance is gone.  Try to grab another.
7100                     continue
7101 
7102                 # Check the instance hasn't been migrated
7103                 if inst.host != self.host:
7104                     LOG.debug('Skipping network cache update for instance '
7105                               'because it has been migrated to another '
7106                               'host.', instance=inst)
7107                 # Check the instance isn't being deleting
7108                 elif inst.task_state == task_states.DELETING:
7109                     LOG.debug('Skipping network cache update for instance '
7110                               'because it is being deleted.', instance=inst)
7111                 else:
7112                     instance = inst
7113                     break
7114 
7115         if instance:
7116             # We have an instance now to refresh
7117             try:
7118                 # Call to network API to get instance info.. this will
7119                 # force an update to the instance's info_cache
7120                 self.network_api.get_instance_nw_info(
7121                     context, instance, force_refresh=True)
7122                 LOG.debug('Updated the network info_cache for instance',
7123                           instance=instance)
7124             except exception.InstanceNotFound:
7125                 # Instance is gone.
7126                 LOG.debug('Instance no longer exists. Unable to refresh',
7127                           instance=instance)
7128                 return
7129             except exception.InstanceInfoCacheNotFound:
7130                 # InstanceInfoCache is gone.
7131                 LOG.debug('InstanceInfoCache no longer exists. '
7132                           'Unable to refresh', instance=instance)
7133             except Exception:
7134                 LOG.error('An error occurred while refreshing the network '
7135                           'cache.', instance=instance, exc_info=True)
7136         else:
7137             LOG.debug("Didn't find any instances for network info cache "
7138                       "update.")
7139 
7140     @periodic_task.periodic_task
7141     def _poll_rebooting_instances(self, context):
7142         if CONF.reboot_timeout > 0:
7143             filters = {'task_state':
7144                        [task_states.REBOOTING,
7145                         task_states.REBOOT_STARTED,
7146                         task_states.REBOOT_PENDING],
7147                        'host': self.host}
7148             rebooting = objects.InstanceList.get_by_filters(
7149                 context, filters, expected_attrs=[], use_slave=True)
7150 
7151             to_poll = []
7152             for instance in rebooting:
7153                 if timeutils.is_older_than(instance.updated_at,
7154                                            CONF.reboot_timeout):
7155                     to_poll.append(instance)
7156 
7157             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
7158 
7159     @periodic_task.periodic_task
7160     def _poll_rescued_instances(self, context):
7161         if CONF.rescue_timeout > 0:
7162             filters = {'vm_state': vm_states.RESCUED,
7163                        'host': self.host}
7164             rescued_instances = objects.InstanceList.get_by_filters(
7165                 context, filters, expected_attrs=["system_metadata"],
7166                 use_slave=True)
7167 
7168             to_unrescue = []
7169             for instance in rescued_instances:
7170                 if timeutils.is_older_than(instance.launched_at,
7171                                            CONF.rescue_timeout):
7172                     to_unrescue.append(instance)
7173 
7174             for instance in to_unrescue:
7175                 self.compute_api.unrescue(context, instance)
7176 
7177     @periodic_task.periodic_task
7178     def _poll_unconfirmed_resizes(self, context):
7179         if CONF.resize_confirm_window == 0:
7180             return
7181 
7182         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
7183                 context, CONF.resize_confirm_window, self.host,
7184                 use_slave=True)
7185 
7186         migrations_info = dict(migration_count=len(migrations),
7187                 confirm_window=CONF.resize_confirm_window)
7188 
7189         if migrations_info["migration_count"] > 0:
7190             LOG.info("Found %(migration_count)d unconfirmed migrations "
7191                      "older than %(confirm_window)d seconds",
7192                      migrations_info)
7193 
7194         def _set_migration_to_error(migration, reason, **kwargs):
7195             LOG.warning("Setting migration %(migration_id)s to error: "
7196                         "%(reason)s",
7197                         {'migration_id': migration['id'], 'reason': reason},
7198                         **kwargs)
7199             migration.status = 'error'
7200             with migration.obj_as_admin():
7201                 migration.save()
7202 
7203         for migration in migrations:
7204             instance_uuid = migration.instance_uuid
7205             LOG.info("Automatically confirming migration "
7206                      "%(migration_id)s for instance %(instance_uuid)s",
7207                      {'migration_id': migration.id,
7208                       'instance_uuid': instance_uuid})
7209             expected_attrs = ['metadata', 'system_metadata']
7210             try:
7211                 instance = objects.Instance.get_by_uuid(context,
7212                             instance_uuid, expected_attrs=expected_attrs,
7213                             use_slave=True)
7214             except exception.InstanceNotFound:
7215                 reason = (_("Instance %s not found") %
7216                           instance_uuid)
7217                 _set_migration_to_error(migration, reason)
7218                 continue
7219             if instance.vm_state == vm_states.ERROR:
7220                 reason = _("In ERROR state")
7221                 _set_migration_to_error(migration, reason,
7222                                         instance=instance)
7223                 continue
7224             # race condition: The instance in DELETING state should not be
7225             # set the migration state to error, otherwise the instance in
7226             # to be deleted which is in RESIZED state
7227             # will not be able to confirm resize
7228             if instance.task_state in [task_states.DELETING,
7229                                        task_states.SOFT_DELETING]:
7230                 msg = ("Instance being deleted or soft deleted during resize "
7231                        "confirmation. Skipping.")
7232                 LOG.debug(msg, instance=instance)
7233                 continue
7234 
7235             # race condition: This condition is hit when this method is
7236             # called between the save of the migration record with a status of
7237             # finished and the save of the instance object with a state of
7238             # RESIZED. The migration record should not be set to error.
7239             if instance.task_state == task_states.RESIZE_FINISH:
7240                 msg = ("Instance still resizing during resize "
7241                        "confirmation. Skipping.")
7242                 LOG.debug(msg, instance=instance)
7243                 continue
7244 
7245             vm_state = instance.vm_state
7246             task_state = instance.task_state
7247             if vm_state != vm_states.RESIZED or task_state is not None:
7248                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
7249                            "RESIZED/None") %
7250                           {'vm_state': vm_state,
7251                            'task_state': task_state})
7252                 _set_migration_to_error(migration, reason,
7253                                         instance=instance)
7254                 continue
7255             try:
7256                 self.compute_api.confirm_resize(context, instance,
7257                                                 migration=migration)
7258             except Exception as e:
7259                 LOG.info("Error auto-confirming resize: %s. "
7260                          "Will retry later.", e, instance=instance)
7261 
7262     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
7263     def _poll_shelved_instances(self, context):
7264 
7265         if CONF.shelved_offload_time <= 0:
7266             return
7267 
7268         filters = {'vm_state': vm_states.SHELVED,
7269                    'task_state': None,
7270                    'host': self.host}
7271         shelved_instances = objects.InstanceList.get_by_filters(
7272             context, filters=filters, expected_attrs=['system_metadata'],
7273             use_slave=True)
7274 
7275         to_gc = []
7276         for instance in shelved_instances:
7277             sys_meta = instance.system_metadata
7278             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
7279             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
7280                 to_gc.append(instance)
7281 
7282         for instance in to_gc:
7283             try:
7284                 instance.task_state = task_states.SHELVING_OFFLOADING
7285                 instance.save(expected_task_state=(None,))
7286                 self.shelve_offload_instance(context, instance,
7287                                              clean_shutdown=False)
7288             except Exception:
7289                 LOG.exception('Periodic task failed to offload instance.',
7290                               instance=instance)
7291 
7292     @periodic_task.periodic_task
7293     def _instance_usage_audit(self, context):
7294         if not CONF.instance_usage_audit:
7295             return
7296 
7297         begin, end = utils.last_completed_audit_period()
7298         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
7299                                self.host):
7300             return
7301 
7302         instances = objects.InstanceList.get_active_by_window_joined(
7303             context, begin, end, host=self.host,
7304             expected_attrs=['system_metadata', 'info_cache', 'metadata',
7305                             'flavor'],
7306             use_slave=True)
7307         num_instances = len(instances)
7308         errors = 0
7309         successes = 0
7310         LOG.info("Running instance usage audit for host %(host)s "
7311                  "from %(begin_time)s to %(end_time)s. "
7312                  "%(number_instances)s instances.",
7313                  {'host': self.host,
7314                   'begin_time': begin,
7315                   'end_time': end,
7316                   'number_instances': num_instances})
7317         start_time = time.time()
7318         task_log = objects.TaskLog(context)
7319         task_log.task_name = 'instance_usage_audit'
7320         task_log.period_beginning = begin
7321         task_log.period_ending = end
7322         task_log.host = self.host
7323         task_log.task_items = num_instances
7324         task_log.message = 'Instance usage audit started...'
7325         task_log.begin_task()
7326         for instance in instances:
7327             try:
7328                 compute_utils.notify_usage_exists(
7329                     self.notifier, context, instance, self.host,
7330                     ignore_missing_network_data=False)
7331                 successes += 1
7332             except Exception:
7333                 LOG.exception('Failed to generate usage '
7334                               'audit for instance '
7335                               'on host %s', self.host,
7336                               instance=instance)
7337                 errors += 1
7338         task_log.errors = errors
7339         task_log.message = (
7340             'Instance usage audit ran for host %s, %s instances in %s seconds.'
7341             % (self.host, num_instances, time.time() - start_time))
7342         task_log.end_task()
7343 
7344     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
7345     def _poll_bandwidth_usage(self, context):
7346 
7347         if not self._bw_usage_supported:
7348             return
7349 
7350         prev_time, start_time = utils.last_completed_audit_period()
7351 
7352         curr_time = time.time()
7353         if (curr_time - self._last_bw_usage_poll >
7354                 CONF.bandwidth_poll_interval):
7355             self._last_bw_usage_poll = curr_time
7356             LOG.info("Updating bandwidth usage cache")
7357             cells_update_interval = CONF.cells.bandwidth_update_interval
7358             if (cells_update_interval > 0 and
7359                    curr_time - self._last_bw_usage_cell_update >
7360                            cells_update_interval):
7361                 self._last_bw_usage_cell_update = curr_time
7362                 update_cells = True
7363             else:
7364                 update_cells = False
7365 
7366             instances = objects.InstanceList.get_by_host(context,
7367                                                               self.host,
7368                                                               use_slave=True)
7369             try:
7370                 bw_counters = self.driver.get_all_bw_counters(instances)
7371             except NotImplementedError:
7372                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
7373                 # implemented yet.  If they don't it doesn't break anything,
7374                 # they just don't get the info in the usage events.
7375                 # NOTE(PhilDay): Record that its not supported so we can
7376                 # skip fast on future calls rather than waste effort getting
7377                 # the list of instances.
7378                 LOG.info("Bandwidth usage not supported by %(driver)s.",
7379                          {'driver': CONF.compute_driver})
7380                 self._bw_usage_supported = False
7381                 return
7382 
7383             refreshed = timeutils.utcnow()
7384             for bw_ctr in bw_counters:
7385                 # Allow switching of greenthreads between queries.
7386                 greenthread.sleep(0)
7387                 bw_in = 0
7388                 bw_out = 0
7389                 last_ctr_in = None
7390                 last_ctr_out = None
7391                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
7392                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
7393                     start_period=start_time, use_slave=True)
7394                 if usage:
7395                     bw_in = usage.bw_in
7396                     bw_out = usage.bw_out
7397                     last_ctr_in = usage.last_ctr_in
7398                     last_ctr_out = usage.last_ctr_out
7399                 else:
7400                     usage = (objects.BandwidthUsage.
7401                              get_by_instance_uuid_and_mac(
7402                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
7403                         start_period=prev_time, use_slave=True))
7404                     if usage:
7405                         last_ctr_in = usage.last_ctr_in
7406                         last_ctr_out = usage.last_ctr_out
7407 
7408                 if last_ctr_in is not None:
7409                     if bw_ctr['bw_in'] < last_ctr_in:
7410                         # counter rollover
7411                         bw_in += bw_ctr['bw_in']
7412                     else:
7413                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
7414 
7415                 if last_ctr_out is not None:
7416                     if bw_ctr['bw_out'] < last_ctr_out:
7417                         # counter rollover
7418                         bw_out += bw_ctr['bw_out']
7419                     else:
7420                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
7421 
7422                 objects.BandwidthUsage(context=context).create(
7423                                               bw_ctr['uuid'],
7424                                               bw_ctr['mac_address'],
7425                                               bw_in,
7426                                               bw_out,
7427                                               bw_ctr['bw_in'],
7428                                               bw_ctr['bw_out'],
7429                                               start_period=start_time,
7430                                               last_refreshed=refreshed,
7431                                               update_cells=update_cells)
7432 
7433     def _get_host_volume_bdms(self, context, use_slave=False):
7434         """Return all block device mappings on a compute host."""
7435         compute_host_bdms = []
7436         instances = objects.InstanceList.get_by_host(context, self.host,
7437             use_slave=use_slave)
7438         for instance in instances:
7439             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7440                     context, instance.uuid, use_slave=use_slave)
7441             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
7442             compute_host_bdms.append(dict(instance=instance,
7443                                           instance_bdms=instance_bdms))
7444 
7445         return compute_host_bdms
7446 
7447     def _update_volume_usage_cache(self, context, vol_usages):
7448         """Updates the volume usage cache table with a list of stats."""
7449         for usage in vol_usages:
7450             # Allow switching of greenthreads between queries.
7451             greenthread.sleep(0)
7452             vol_usage = objects.VolumeUsage(context)
7453             vol_usage.volume_id = usage['volume']
7454             vol_usage.instance_uuid = usage['instance'].uuid
7455             vol_usage.project_id = usage['instance'].project_id
7456             vol_usage.user_id = usage['instance'].user_id
7457             vol_usage.availability_zone = usage['instance'].availability_zone
7458             vol_usage.curr_reads = usage['rd_req']
7459             vol_usage.curr_read_bytes = usage['rd_bytes']
7460             vol_usage.curr_writes = usage['wr_req']
7461             vol_usage.curr_write_bytes = usage['wr_bytes']
7462             vol_usage.save()
7463             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
7464             compute_utils.notify_about_volume_usage(context, vol_usage,
7465                                                     self.host)
7466 
7467     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
7468     def _poll_volume_usage(self, context):
7469         if CONF.volume_usage_poll_interval == 0:
7470             return
7471 
7472         compute_host_bdms = self._get_host_volume_bdms(context,
7473                                                        use_slave=True)
7474         if not compute_host_bdms:
7475             return
7476 
7477         LOG.debug("Updating volume usage cache")
7478         try:
7479             vol_usages = self.driver.get_all_volume_usage(context,
7480                                                           compute_host_bdms)
7481         except NotImplementedError:
7482             return
7483 
7484         self._update_volume_usage_cache(context, vol_usages)
7485 
7486     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
7487                                  run_immediately=True)
7488     def _sync_power_states(self, context):
7489         """Align power states between the database and the hypervisor.
7490 
7491         To sync power state data we make a DB call to get the number of
7492         virtual machines known by the hypervisor and if the number matches the
7493         number of virtual machines known by the database, we proceed in a lazy
7494         loop, one database record at a time, checking if the hypervisor has the
7495         same power state as is in the database.
7496         """
7497         db_instances = objects.InstanceList.get_by_host(context, self.host,
7498                                                         expected_attrs=[],
7499                                                         use_slave=True)
7500 
7501         try:
7502             num_vm_instances = self.driver.get_num_instances()
7503         except exception.VirtDriverNotReady as e:
7504             # If the virt driver is not ready, like ironic-api not being up
7505             # yet in the case of ironic, just log it and exit.
7506             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
7507             return
7508 
7509         num_db_instances = len(db_instances)
7510 
7511         if num_vm_instances != num_db_instances:
7512             LOG.warning("While synchronizing instance power states, found "
7513                         "%(num_db_instances)s instances in the database "
7514                         "and %(num_vm_instances)s instances on the "
7515                         "hypervisor.",
7516                         {'num_db_instances': num_db_instances,
7517                          'num_vm_instances': num_vm_instances})
7518 
7519         def _sync(db_instance):
7520             # NOTE(melwitt): This must be synchronized as we query state from
7521             #                two separate sources, the driver and the database.
7522             #                They are set (in stop_instance) and read, in sync.
7523             @utils.synchronized(db_instance.uuid)
7524             def query_driver_power_state_and_sync():
7525                 self._query_driver_power_state_and_sync(context, db_instance)
7526 
7527             try:
7528                 query_driver_power_state_and_sync()
7529             except Exception:
7530                 LOG.exception("Periodic sync_power_state task had an "
7531                               "error while processing an instance.",
7532                               instance=db_instance)
7533 
7534             self._syncs_in_progress.pop(db_instance.uuid)
7535 
7536         for db_instance in db_instances:
7537             # process syncs asynchronously - don't want instance locking to
7538             # block entire periodic task thread
7539             uuid = db_instance.uuid
7540             if uuid in self._syncs_in_progress:
7541                 LOG.debug('Sync already in progress for %s', uuid)
7542             else:
7543                 LOG.debug('Triggering sync for uuid %s', uuid)
7544                 self._syncs_in_progress[uuid] = True
7545                 self._sync_power_pool.spawn_n(_sync, db_instance)
7546 
7547     def _query_driver_power_state_and_sync(self, context, db_instance):
7548         if db_instance.task_state is not None:
7549             LOG.info("During sync_power_state the instance has a "
7550                      "pending task (%(task)s). Skip.",
7551                      {'task': db_instance.task_state}, instance=db_instance)
7552             return
7553         # No pending tasks. Now try to figure out the real vm_power_state.
7554         try:
7555             vm_instance = self.driver.get_info(db_instance)
7556             vm_power_state = vm_instance.state
7557         except exception.InstanceNotFound:
7558             vm_power_state = power_state.NOSTATE
7559         # Note(maoy): the above get_info call might take a long time,
7560         # for example, because of a broken libvirt driver.
7561         try:
7562             self._sync_instance_power_state(context,
7563                                             db_instance,
7564                                             vm_power_state,
7565                                             use_slave=True)
7566         except exception.InstanceNotFound:
7567             # NOTE(hanlind): If the instance gets deleted during sync,
7568             # silently ignore.
7569             pass
7570 
7571     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
7572                                    use_slave=False):
7573         """Align instance power state between the database and hypervisor.
7574 
7575         If the instance is not found on the hypervisor, but is in the database,
7576         then a stop() API will be called on the instance.
7577         """
7578 
7579         # We re-query the DB to get the latest instance info to minimize
7580         # (not eliminate) race condition.
7581         db_instance.refresh(use_slave=use_slave)
7582         db_power_state = db_instance.power_state
7583         vm_state = db_instance.vm_state
7584 
7585         if self.host != db_instance.host:
7586             # on the sending end of nova-compute _sync_power_state
7587             # may have yielded to the greenthread performing a live
7588             # migration; this in turn has changed the resident-host
7589             # for the VM; However, the instance is still active, it
7590             # is just in the process of migrating to another host.
7591             # This implies that the compute source must relinquish
7592             # control to the compute destination.
7593             LOG.info("During the sync_power process the "
7594                      "instance has moved from "
7595                      "host %(src)s to host %(dst)s",
7596                      {'src': db_instance.host,
7597                       'dst': self.host},
7598                      instance=db_instance)
7599             return
7600         elif db_instance.task_state is not None:
7601             # on the receiving end of nova-compute, it could happen
7602             # that the DB instance already report the new resident
7603             # but the actual VM has not showed up on the hypervisor
7604             # yet. In this case, let's allow the loop to continue
7605             # and run the state sync in a later round
7606             LOG.info("During sync_power_state the instance has a "
7607                      "pending task (%(task)s). Skip.",
7608                      {'task': db_instance.task_state},
7609                      instance=db_instance)
7610             return
7611 
7612         orig_db_power_state = db_power_state
7613         if vm_power_state != db_power_state:
7614             LOG.info('During _sync_instance_power_state the DB '
7615                      'power_state (%(db_power_state)s) does not match '
7616                      'the vm_power_state from the hypervisor '
7617                      '(%(vm_power_state)s). Updating power_state in the '
7618                      'DB to match the hypervisor.',
7619                      {'db_power_state': db_power_state,
7620                       'vm_power_state': vm_power_state},
7621                      instance=db_instance)
7622             # power_state is always updated from hypervisor to db
7623             db_instance.power_state = vm_power_state
7624             db_instance.save()
7625             db_power_state = vm_power_state
7626 
7627         # Note(maoy): Now resolve the discrepancy between vm_state and
7628         # vm_power_state. We go through all possible vm_states.
7629         if vm_state in (vm_states.BUILDING,
7630                         vm_states.RESCUED,
7631                         vm_states.RESIZED,
7632                         vm_states.SUSPENDED,
7633                         vm_states.ERROR):
7634             # TODO(maoy): we ignore these vm_state for now.
7635             pass
7636         elif vm_state == vm_states.ACTIVE:
7637             # The only rational power state should be RUNNING
7638             if vm_power_state in (power_state.SHUTDOWN,
7639                                   power_state.CRASHED):
7640                 LOG.warning("Instance shutdown by itself. Calling the "
7641                             "stop API. Current vm_state: %(vm_state)s, "
7642                             "current task_state: %(task_state)s, "
7643                             "original DB power_state: %(db_power_state)s, "
7644                             "current VM power_state: %(vm_power_state)s",
7645                             {'vm_state': vm_state,
7646                              'task_state': db_instance.task_state,
7647                              'db_power_state': orig_db_power_state,
7648                              'vm_power_state': vm_power_state},
7649                             instance=db_instance)
7650                 try:
7651                     # Note(maoy): here we call the API instead of
7652                     # brutally updating the vm_state in the database
7653                     # to allow all the hooks and checks to be performed.
7654                     if db_instance.shutdown_terminate:
7655                         self.compute_api.delete(context, db_instance)
7656                     else:
7657                         self.compute_api.stop(context, db_instance)
7658                 except Exception:
7659                     # Note(maoy): there is no need to propagate the error
7660                     # because the same power_state will be retrieved next
7661                     # time and retried.
7662                     # For example, there might be another task scheduled.
7663                     LOG.exception("error during stop() in sync_power_state.",
7664                                   instance=db_instance)
7665             elif vm_power_state == power_state.SUSPENDED:
7666                 LOG.warning("Instance is suspended unexpectedly. Calling "
7667                             "the stop API.", instance=db_instance)
7668                 try:
7669                     self.compute_api.stop(context, db_instance)
7670                 except Exception:
7671                     LOG.exception("error during stop() in sync_power_state.",
7672                                   instance=db_instance)
7673             elif vm_power_state == power_state.PAUSED:
7674                 # Note(maoy): a VM may get into the paused state not only
7675                 # because the user request via API calls, but also
7676                 # due to (temporary) external instrumentations.
7677                 # Before the virt layer can reliably report the reason,
7678                 # we simply ignore the state discrepancy. In many cases,
7679                 # the VM state will go back to running after the external
7680                 # instrumentation is done. See bug 1097806 for details.
7681                 LOG.warning("Instance is paused unexpectedly. Ignore.",
7682                             instance=db_instance)
7683             elif vm_power_state == power_state.NOSTATE:
7684                 # Occasionally, depending on the status of the hypervisor,
7685                 # which could be restarting for example, an instance may
7686                 # not be found.  Therefore just log the condition.
7687                 LOG.warning("Instance is unexpectedly not found. Ignore.",
7688                             instance=db_instance)
7689         elif vm_state == vm_states.STOPPED:
7690             if vm_power_state not in (power_state.NOSTATE,
7691                                       power_state.SHUTDOWN,
7692                                       power_state.CRASHED):
7693                 LOG.warning("Instance is not stopped. Calling "
7694                             "the stop API. Current vm_state: %(vm_state)s,"
7695                             " current task_state: %(task_state)s, "
7696                             "original DB power_state: %(db_power_state)s, "
7697                             "current VM power_state: %(vm_power_state)s",
7698                             {'vm_state': vm_state,
7699                              'task_state': db_instance.task_state,
7700                              'db_power_state': orig_db_power_state,
7701                              'vm_power_state': vm_power_state},
7702                             instance=db_instance)
7703                 try:
7704                     # NOTE(russellb) Force the stop, because normally the
7705                     # compute API would not allow an attempt to stop a stopped
7706                     # instance.
7707                     self.compute_api.force_stop(context, db_instance)
7708                 except Exception:
7709                     LOG.exception("error during stop() in sync_power_state.",
7710                                   instance=db_instance)
7711         elif vm_state == vm_states.PAUSED:
7712             if vm_power_state in (power_state.SHUTDOWN,
7713                                   power_state.CRASHED):
7714                 LOG.warning("Paused instance shutdown by itself. Calling "
7715                             "the stop API.", instance=db_instance)
7716                 try:
7717                     self.compute_api.force_stop(context, db_instance)
7718                 except Exception:
7719                     LOG.exception("error during stop() in sync_power_state.",
7720                                   instance=db_instance)
7721         elif vm_state in (vm_states.SOFT_DELETED,
7722                           vm_states.DELETED):
7723             if vm_power_state not in (power_state.NOSTATE,
7724                                       power_state.SHUTDOWN):
7725                 # Note(maoy): this should be taken care of periodically in
7726                 # _cleanup_running_deleted_instances().
7727                 LOG.warning("Instance is not (soft-)deleted.",
7728                             instance=db_instance)
7729 
7730     @periodic_task.periodic_task
7731     def _reclaim_queued_deletes(self, context):
7732         """Reclaim instances that are queued for deletion."""
7733         interval = CONF.reclaim_instance_interval
7734         if interval <= 0:
7735             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
7736             return
7737 
7738         filters = {'vm_state': vm_states.SOFT_DELETED,
7739                    'task_state': None,
7740                    'host': self.host}
7741         instances = objects.InstanceList.get_by_filters(
7742             context, filters,
7743             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
7744             use_slave=True)
7745         for instance in instances:
7746             if self._deleted_old_enough(instance, interval):
7747                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7748                         context, instance.uuid)
7749                 LOG.info('Reclaiming deleted instance', instance=instance)
7750                 try:
7751                     self._delete_instance(context, instance, bdms)
7752                 except Exception as e:
7753                     LOG.warning("Periodic reclaim failed to delete "
7754                                 "instance: %s",
7755                                 e, instance=instance)
7756 
7757     def _get_nodename(self, instance, refresh=False):
7758         """Helper method to get the name of the first available node
7759         on this host. This method should not be used with any operations
7760         on ironic instances since it does not handle multiple nodes.
7761         """
7762         node = self.driver.get_available_nodes(refresh=refresh)[0]
7763         LOG.debug("No node specified, defaulting to %s", node,
7764                   instance=instance)
7765         return node
7766 
7767     def _update_available_resource_for_node(self, context, nodename,
7768                                             startup=False):
7769 
7770         try:
7771             self.rt.update_available_resource(context, nodename,
7772                                               startup=startup)
7773         except exception.ComputeHostNotFound:
7774             LOG.warning("Compute node '%s' not found in "
7775                         "update_available_resource.", nodename)
7776         except exception.ReshapeFailed:
7777             # We're only supposed to get here on startup, if a reshape was
7778             # needed, was attempted, and failed. We want to kill the service.
7779             with excutils.save_and_reraise_exception():
7780                 LOG.critical("Resource provider data migration failed "
7781                              "fatally during startup for node %s.", nodename)
7782         except exception.ReshapeNeeded:
7783             # This exception should only find its way here if the virt driver's
7784             # update_provider_tree raised it incorrectly: either
7785             # a) After the resource tracker already caught it once and
7786             # reinvoked update_provider_tree with allocations. At this point
7787             # the driver is just supposed to *do* the reshape, so if it raises
7788             # ReshapeNeeded, it's a bug, and we want to kill the compute
7789             # service.
7790             # b) On periodic rather than startup (we only allow reshapes to
7791             # happen on startup). In this case we'll just make the logs red and
7792             # go again at the next periodic interval, where the same thing may
7793             # or may not happen again. Depending on the previous and intended
7794             # shape of the providers/inventories, this may not actually cause
7795             # any immediately visible symptoms (in terms of scheduling, etc.)
7796             # If this becomes a problem, we may wish to make it pop immediately
7797             # (e.g. disable the service).
7798             with excutils.save_and_reraise_exception():
7799                 LOG.exception("ReshapeNeeded exception is unexpected here!")
7800         except Exception:
7801             LOG.exception("Error updating resources for node %(node)s.",
7802                           {'node': nodename})
7803 
7804     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
7805     def update_available_resource(self, context, startup=False):
7806         """See driver.get_available_resource()
7807 
7808         Periodic process that keeps that the compute host's understanding of
7809         resource availability and usage in sync with the underlying hypervisor.
7810 
7811         :param context: security context
7812         :param startup: True if this is being called when the nova-compute
7813             service is starting, False otherwise.
7814         """
7815 
7816         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
7817                                                             use_slave=True,
7818                                                             startup=startup)
7819         try:
7820             nodenames = set(self.driver.get_available_nodes())
7821         except exception.VirtDriverNotReady:
7822             LOG.warning("Virt driver is not ready.")
7823             return
7824 
7825         # Delete orphan compute node not reported by driver but still in db
7826         for cn in compute_nodes_in_db:
7827             if cn.hypervisor_hostname not in nodenames:
7828                 LOG.info("Deleting orphan compute node %(id)s "
7829                          "hypervisor host is %(hh)s, "
7830                          "nodes are %(nodes)s",
7831                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
7832                           'nodes': nodenames})
7833                 cn.destroy()
7834                 self.rt.remove_node(cn.hypervisor_hostname)
7835                 # Delete the corresponding resource provider in placement,
7836                 # along with any associated allocations and inventory.
7837                 self.reportclient.delete_resource_provider(context, cn,
7838                                                            cascade=True)
7839 
7840         for nodename in nodenames:
7841             self._update_available_resource_for_node(context, nodename,
7842                                                      startup=startup)
7843 
7844     def _get_compute_nodes_in_db(self, context, use_slave=False,
7845                                  startup=False):
7846         try:
7847             return objects.ComputeNodeList.get_all_by_host(context, self.host,
7848                                                            use_slave=use_slave)
7849         except exception.NotFound:
7850             if startup:
7851                 LOG.warning(
7852                     "No compute node record found for host %s. If this is "
7853                     "the first time this service is starting on this "
7854                     "host, then you can ignore this warning.", self.host)
7855             else:
7856                 LOG.error("No compute node record for host %s", self.host)
7857             return []
7858 
7859     @periodic_task.periodic_task(
7860         spacing=CONF.running_deleted_instance_poll_interval)
7861     def _cleanup_running_deleted_instances(self, context):
7862         """Cleanup any instances which are erroneously still running after
7863         having been deleted.
7864 
7865         Valid actions to take are:
7866 
7867             1. noop - do nothing
7868             2. log - log which instances are erroneously running
7869             3. reap - shutdown and cleanup any erroneously running instances
7870             4. shutdown - power off *and disable* any erroneously running
7871                           instances
7872 
7873         The use-case for this cleanup task is: for various reasons, it may be
7874         possible for the database to show an instance as deleted but for that
7875         instance to still be running on a host machine (see bug
7876         https://bugs.launchpad.net/nova/+bug/911366).
7877 
7878         This cleanup task is a cross-hypervisor utility for finding these
7879         zombied instances and either logging the discrepancy (likely what you
7880         should do in production), or automatically reaping the instances (more
7881         appropriate for dev environments).
7882         """
7883         action = CONF.running_deleted_instance_action
7884 
7885         if action == "noop":
7886             return
7887 
7888         # NOTE(sirp): admin contexts don't ordinarily return deleted records
7889         with utils.temporary_mutation(context, read_deleted="yes"):
7890             for instance in self._running_deleted_instances(context):
7891                 if action == "log":
7892                     LOG.warning("Detected instance with name label "
7893                                 "'%s' which is marked as "
7894                                 "DELETED but still present on host.",
7895                                 instance.name, instance=instance)
7896 
7897                 elif action == 'shutdown':
7898                     LOG.info("Powering off instance with name label "
7899                              "'%s' which is marked as "
7900                              "DELETED but still present on host.",
7901                              instance.name, instance=instance)
7902                     try:
7903                         try:
7904                             # disable starting the instance
7905                             self.driver.set_bootable(instance, False)
7906                         except NotImplementedError:
7907                             LOG.debug("set_bootable is not implemented "
7908                                       "for the current driver")
7909                         # and power it off
7910                         self.driver.power_off(instance)
7911                     except Exception:
7912                         LOG.warning("Failed to power off instance",
7913                                     instance=instance, exc_info=True)
7914 
7915                 elif action == 'reap':
7916                     LOG.info("Destroying instance with name label "
7917                              "'%s' which is marked as "
7918                              "DELETED but still present on host.",
7919                              instance.name, instance=instance)
7920                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7921                         context, instance.uuid, use_slave=True)
7922                     self.instance_events.clear_events_for_instance(instance)
7923                     try:
7924                         self._shutdown_instance(context, instance, bdms,
7925                                                 notify=False)
7926                         self._cleanup_volumes(context, instance, bdms,
7927                                               detach=False)
7928                     except Exception as e:
7929                         LOG.warning("Periodic cleanup failed to delete "
7930                                     "instance: %s",
7931                                     e, instance=instance)
7932                 else:
7933                     raise Exception(_("Unrecognized value '%s'"
7934                                       " for CONF.running_deleted_"
7935                                       "instance_action") % action)
7936 
7937     def _running_deleted_instances(self, context):
7938         """Returns a list of instances nova thinks is deleted,
7939         but the hypervisor thinks is still running.
7940         """
7941         timeout = CONF.running_deleted_instance_timeout
7942         filters = {'deleted': True,
7943                    'soft_deleted': False}
7944         instances = self._get_instances_on_driver(context, filters)
7945         return [i for i in instances if self._deleted_old_enough(i, timeout)]
7946 
7947     def _deleted_old_enough(self, instance, timeout):
7948         deleted_at = instance.deleted_at
7949         if deleted_at:
7950             deleted_at = deleted_at.replace(tzinfo=None)
7951         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
7952 
7953     @contextlib.contextmanager
7954     def _error_out_instance_on_exception(self, context, instance,
7955                                          instance_state=vm_states.ACTIVE):
7956         instance_uuid = instance.uuid
7957         try:
7958             yield
7959         except NotImplementedError as error:
7960             with excutils.save_and_reraise_exception():
7961                 LOG.info("Setting instance back to %(state)s after: "
7962                          "%(error)s",
7963                          {'state': instance_state, 'error': error},
7964                          instance_uuid=instance_uuid)
7965                 self._instance_update(context, instance,
7966                                       vm_state=instance_state,
7967                                       task_state=None)
7968         except exception.InstanceFaultRollback as error:
7969             LOG.info("Setting instance back to ACTIVE after: %s",
7970                      error, instance_uuid=instance_uuid)
7971             self._instance_update(context, instance,
7972                                   vm_state=vm_states.ACTIVE,
7973                                   task_state=None)
7974             raise error.inner_exception
7975         except Exception:
7976             LOG.exception('Setting instance vm_state to ERROR',
7977                           instance_uuid=instance_uuid)
7978             with excutils.save_and_reraise_exception():
7979                 self._set_instance_obj_error_state(context, instance)
7980 
7981     @wrap_exception()
7982     def add_aggregate_host(self, context, aggregate, host, slave_info):
7983         """Notify hypervisor of change (for hypervisor pools)."""
7984         try:
7985             self.driver.add_to_aggregate(context, aggregate, host,
7986                                          slave_info=slave_info)
7987         except NotImplementedError:
7988             LOG.debug('Hypervisor driver does not support '
7989                       'add_aggregate_host')
7990         except exception.AggregateError:
7991             with excutils.save_and_reraise_exception():
7992                 self.driver.undo_aggregate_operation(
7993                                     context,
7994                                     aggregate.delete_host,
7995                                     aggregate, host)
7996 
7997     @wrap_exception()
7998     def remove_aggregate_host(self, context, host, slave_info, aggregate):
7999         """Removes a host from a physical hypervisor pool."""
8000         try:
8001             self.driver.remove_from_aggregate(context, aggregate, host,
8002                                               slave_info=slave_info)
8003         except NotImplementedError:
8004             LOG.debug('Hypervisor driver does not support '
8005                       'remove_aggregate_host')
8006         except (exception.AggregateError,
8007                 exception.InvalidAggregateAction) as e:
8008             with excutils.save_and_reraise_exception():
8009                 self.driver.undo_aggregate_operation(
8010                                     context,
8011                                     aggregate.add_host,
8012                                     aggregate, host,
8013                                     isinstance(e, exception.AggregateError))
8014 
8015     def _process_instance_event(self, instance, event):
8016         _event = self.instance_events.pop_instance_event(instance, event)
8017         if _event:
8018             LOG.debug('Processing event %(event)s',
8019                       {'event': event.key}, instance=instance)
8020             _event.send(event)
8021         else:
8022             # If it's a network-vif-unplugged event and the instance is being
8023             # deleted then we don't need to make this a warning as it's
8024             # expected. There are other things which could trigger this like
8025             # detaching an interface, but we don't have a task state for that.
8026             if (event.name == 'network-vif-unplugged' and
8027                     instance.task_state == task_states.DELETING):
8028                 LOG.debug('Received event %s for instance which is being '
8029                           'deleted.', event.key, instance=instance)
8030             else:
8031                 LOG.warning('Received unexpected event %(event)s for '
8032                             'instance with vm_state %(vm_state)s and '
8033                             'task_state %(task_state)s.',
8034                             {'event': event.key,
8035                              'vm_state': instance.vm_state,
8036                              'task_state': instance.task_state},
8037                             instance=instance)
8038 
8039     def _process_instance_vif_deleted_event(self, context, instance,
8040                                             deleted_vif_id):
8041         # If an attached port is deleted by neutron, it needs to
8042         # be detached from the instance.
8043         # And info cache needs to be updated.
8044         network_info = instance.info_cache.network_info
8045         for index, vif in enumerate(network_info):
8046             if vif['id'] == deleted_vif_id:
8047                 LOG.info('Neutron deleted interface %(intf)s; '
8048                          'detaching it from the instance and '
8049                          'deleting it from the info cache',
8050                          {'intf': vif['id']},
8051                          instance=instance)
8052                 del network_info[index]
8053                 base_net_api.update_instance_cache_with_nw_info(
8054                                  self.network_api, context,
8055                                  instance,
8056                                  nw_info=network_info)
8057                 try:
8058                     self.driver.detach_interface(context, instance, vif)
8059                 except NotImplementedError:
8060                     # Not all virt drivers support attach/detach of interfaces
8061                     # yet (like Ironic), so just ignore this.
8062                     pass
8063                 except exception.NovaException as ex:
8064                     # If the instance was deleted before the interface was
8065                     # detached, just log it at debug.
8066                     log_level = (logging.DEBUG
8067                                  if isinstance(ex, exception.InstanceNotFound)
8068                                  else logging.WARNING)
8069                     LOG.log(log_level,
8070                             "Detach interface failed, "
8071                             "port_id=%(port_id)s, reason: %(msg)s",
8072                             {'port_id': deleted_vif_id, 'msg': ex},
8073                             instance=instance)
8074                 break
8075 
8076     @wrap_instance_event(prefix='compute')
8077     @wrap_instance_fault
8078     def extend_volume(self, context, instance, extended_volume_id):
8079 
8080         # If an attached volume is extended by cinder, it needs to
8081         # be extended by virt driver so host can detect its new size.
8082         # And bdm needs to be updated.
8083         LOG.debug('Handling volume-extended event for volume %(vol)s',
8084                   {'vol': extended_volume_id}, instance=instance)
8085 
8086         try:
8087             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
8088                    context, extended_volume_id, instance.uuid)
8089         except exception.NotFound:
8090             LOG.warning('Extend volume failed, '
8091                         'volume %(vol)s is not attached to instance.',
8092                         {'vol': extended_volume_id},
8093                         instance=instance)
8094             return
8095 
8096         LOG.info('Cinder extended volume %(vol)s; '
8097                  'extending it to detect new size',
8098                  {'vol': extended_volume_id},
8099                  instance=instance)
8100         volume = self.volume_api.get(context, bdm.volume_id)
8101 
8102         if bdm.connection_info is None:
8103             LOG.warning('Extend volume failed, '
8104                         'attached volume %(vol)s has no connection_info',
8105                         {'vol': extended_volume_id},
8106                         instance=instance)
8107             return
8108 
8109         connection_info = jsonutils.loads(bdm.connection_info)
8110         bdm.volume_size = volume['size']
8111         bdm.save()
8112 
8113         if not self.driver.capabilities.get('supports_extend_volume', False):
8114             raise exception.ExtendVolumeNotSupported()
8115 
8116         try:
8117             self.driver.extend_volume(connection_info,
8118                                       instance)
8119         except Exception as ex:
8120             LOG.warning('Extend volume failed, '
8121                         'volume_id=%(volume_id)s, reason: %(msg)s',
8122                         {'volume_id': extended_volume_id, 'msg': ex},
8123                         instance=instance)
8124             raise
8125 
8126     @wrap_exception()
8127     def external_instance_event(self, context, instances, events):
8128         # NOTE(danms): Some event types are handled by the manager, such
8129         # as when we're asked to update the instance's info_cache. If it's
8130         # not one of those, look for some thread(s) waiting for the event and
8131         # unblock them if so.
8132         for event in events:
8133             instance = [inst for inst in instances
8134                         if inst.uuid == event.instance_uuid][0]
8135             LOG.debug('Received event %(event)s',
8136                       {'event': event.key},
8137                       instance=instance)
8138             if event.name == 'network-changed':
8139                 try:
8140                     LOG.debug('Refreshing instance network info cache due to '
8141                               'event %s.', event.key, instance=instance)
8142                     self.network_api.get_instance_nw_info(
8143                         context, instance, refresh_vif_id=event.tag)
8144                 except exception.NotFound as e:
8145                     LOG.info('Failed to process external instance event '
8146                              '%(event)s due to: %(error)s',
8147                              {'event': event.key, 'error': six.text_type(e)},
8148                              instance=instance)
8149             elif event.name == 'network-vif-deleted':
8150                 try:
8151                     self._process_instance_vif_deleted_event(context,
8152                                                              instance,
8153                                                              event.tag)
8154                 except exception.NotFound as e:
8155                     LOG.info('Failed to process external instance event '
8156                              '%(event)s due to: %(error)s',
8157                              {'event': event.key, 'error': six.text_type(e)},
8158                              instance=instance)
8159             elif event.name == 'volume-extended':
8160                 self.extend_volume(context, instance, event.tag)
8161             else:
8162                 self._process_instance_event(instance, event)
8163 
8164     @periodic_task.periodic_task(spacing=CONF.image_cache_manager_interval,
8165                                  external_process_ok=True)
8166     def _run_image_cache_manager_pass(self, context):
8167         """Run a single pass of the image cache manager."""
8168 
8169         if not self.driver.capabilities.get("has_imagecache", False):
8170             return
8171 
8172         # Determine what other nodes use this storage
8173         storage_users.register_storage_use(CONF.instances_path, CONF.host)
8174         nodes = storage_users.get_storage_users(CONF.instances_path)
8175 
8176         # Filter all_instances to only include those nodes which share this
8177         # storage path.
8178         # TODO(mikal): this should be further refactored so that the cache
8179         # cleanup code doesn't know what those instances are, just a remote
8180         # count, and then this logic should be pushed up the stack.
8181         filters = {'deleted': False,
8182                    'soft_deleted': True,
8183                    'host': nodes}
8184         filtered_instances = objects.InstanceList.get_by_filters(context,
8185                                  filters, expected_attrs=[], use_slave=True)
8186 
8187         self.driver.manage_image_cache(context, filtered_instances)
8188 
8189     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
8190     def _run_pending_deletes(self, context):
8191         """Retry any pending instance file deletes."""
8192         LOG.debug('Cleaning up deleted instances')
8193         filters = {'deleted': True,
8194                    'soft_deleted': False,
8195                    'host': CONF.host,
8196                    'cleaned': False}
8197         attrs = ['system_metadata']
8198         with utils.temporary_mutation(context, read_deleted='yes'):
8199             instances = objects.InstanceList.get_by_filters(
8200                 context, filters, expected_attrs=attrs, use_slave=True)
8201         LOG.debug('There are %d instances to clean', len(instances))
8202 
8203         for instance in instances:
8204             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
8205             LOG.debug('Instance has had %(attempts)s of %(max)s '
8206                       'cleanup attempts',
8207                       {'attempts': attempts,
8208                        'max': CONF.maximum_instance_delete_attempts},
8209                       instance=instance)
8210             if attempts < CONF.maximum_instance_delete_attempts:
8211                 success = self.driver.delete_instance_files(instance)
8212 
8213                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
8214                 if success:
8215                     instance.cleaned = True
8216                 with utils.temporary_mutation(context, read_deleted='yes'):
8217                     instance.save()
8218 
8219     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
8220     def _cleanup_incomplete_migrations(self, context):
8221         """Delete instance files on failed resize/revert-resize operation
8222 
8223         During resize/revert-resize operation, if that instance gets deleted
8224         in-between then instance files might remain either on source or
8225         destination compute node because of race condition.
8226         """
8227         LOG.debug('Cleaning up deleted instances with incomplete migration ')
8228         migration_filters = {'host': CONF.host,
8229                              'status': 'error'}
8230         migrations = objects.MigrationList.get_by_filters(context,
8231                                                           migration_filters)
8232 
8233         if not migrations:
8234             return
8235 
8236         inst_uuid_from_migrations = set([migration.instance_uuid for migration
8237                                          in migrations])
8238 
8239         inst_filters = {'deleted': True, 'soft_deleted': False,
8240                         'uuid': inst_uuid_from_migrations}
8241         attrs = ['info_cache', 'security_groups', 'system_metadata']
8242         with utils.temporary_mutation(context, read_deleted='yes'):
8243             instances = objects.InstanceList.get_by_filters(
8244                 context, inst_filters, expected_attrs=attrs, use_slave=True)
8245 
8246         for instance in instances:
8247             if instance.host != CONF.host:
8248                 for migration in migrations:
8249                     if instance.uuid == migration.instance_uuid:
8250                         # Delete instance files if not cleanup properly either
8251                         # from the source or destination compute nodes when
8252                         # the instance is deleted during resizing.
8253                         self.driver.delete_instance_files(instance)
8254                         try:
8255                             migration.status = 'failed'
8256                             with migration.obj_as_admin():
8257                                 migration.save()
8258                         except exception.MigrationNotFound:
8259                             LOG.warning("Migration %s is not found.",
8260                                         migration.id,
8261                                         instance=instance)
8262                         break
8263 
8264     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
8265                                    exception.QemuGuestAgentNotEnabled,
8266                                    exception.NovaException,
8267                                    NotImplementedError)
8268     @wrap_exception()
8269     def quiesce_instance(self, context, instance):
8270         """Quiesce an instance on this host."""
8271         context = context.elevated()
8272         image_meta = objects.ImageMeta.from_instance(instance)
8273         self.driver.quiesce(context, instance, image_meta)
8274 
8275     def _wait_for_snapshots_completion(self, context, mapping):
8276         for mapping_dict in mapping:
8277             if mapping_dict.get('source_type') == 'snapshot':
8278 
8279                 def _wait_snapshot():
8280                     snapshot = self.volume_api.get_snapshot(
8281                         context, mapping_dict['snapshot_id'])
8282                     if snapshot.get('status') != 'creating':
8283                         raise loopingcall.LoopingCallDone()
8284 
8285                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
8286                 timer.start(interval=0.5).wait()
8287 
8288     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
8289                                    exception.QemuGuestAgentNotEnabled,
8290                                    exception.NovaException,
8291                                    NotImplementedError)
8292     @wrap_exception()
8293     def unquiesce_instance(self, context, instance, mapping=None):
8294         """Unquiesce an instance on this host.
8295 
8296         If snapshots' image mapping is provided, it waits until snapshots are
8297         completed before unqueiscing.
8298         """
8299         context = context.elevated()
8300         if mapping:
8301             try:
8302                 self._wait_for_snapshots_completion(context, mapping)
8303             except Exception as error:
8304                 LOG.exception("Exception while waiting completion of "
8305                               "volume snapshots: %s",
8306                               error, instance=instance)
8307         image_meta = objects.ImageMeta.from_instance(instance)
8308         self.driver.unquiesce(context, instance, image_meta)
8309 
8310     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
8311     def _cleanup_expired_console_auth_tokens(self, context):
8312         """Remove expired console auth tokens for this host.
8313 
8314         Console authorization tokens and their connection data are stored
8315         in the database when a user asks for a console connection to an
8316         instance. After a time they expire. We periodically remove any expired
8317         tokens from the database.
8318         """
8319         # If the database backend isn't in use, don't bother looking for
8320         # expired tokens. The database backend is not supported for cells v1.
8321         if not CONF.cells.enable:
8322             objects.ConsoleAuthToken.\
8323                 clean_expired_console_auths_for_host(context, self.host)
