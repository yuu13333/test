Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import collections
23 import copy
24 import functools
25 import re
26 import string
27 
28 from castellan import key_manager
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_utils import excutils
33 from oslo_utils import strutils
34 from oslo_utils import timeutils
35 from oslo_utils import units
36 from oslo_utils import uuidutils
37 import six
38 from six.moves import range
39 
40 from nova import availability_zones
41 from nova import block_device
42 from nova.cells import opts as cells_opts
43 from nova.compute import flavors
44 from nova.compute import instance_actions
45 from nova.compute import instance_list
46 from nova.compute import migration_list
47 from nova.compute import power_state
48 from nova.compute import rpcapi as compute_rpcapi
49 from nova.compute import task_states
50 from nova.compute import utils as compute_utils
51 from nova.compute.utils import wrap_instance_event
52 from nova.compute import vm_states
53 from nova import conductor
54 import nova.conf
55 from nova.consoleauth import rpcapi as consoleauth_rpcapi
56 from nova import context as nova_context
57 from nova import crypto
58 from nova.db import base
59 from nova import exception
60 from nova import exception_wrapper
61 from nova import hooks
62 from nova.i18n import _
63 from nova import image
64 from nova import network
65 from nova.network import model as network_model
66 from nova.network.security_group import openstack_driver
67 from nova.network.security_group import security_group_base
68 from nova import objects
69 from nova.objects import base as obj_base
70 from nova.objects import block_device as block_device_obj
71 from nova.objects import fields as fields_obj
72 from nova.objects import keypair as keypair_obj
73 from nova.objects import quotas as quotas_obj
74 from nova.pci import request as pci_request
75 from nova.policies import servers as servers_policies
76 import nova.policy
77 from nova import profiler
78 from nova import rpc
79 from nova.scheduler.client import query
80 from nova.scheduler.client import report
81 from nova.scheduler import utils as scheduler_utils
82 from nova import servicegroup
83 from nova import utils
84 from nova.virt import hardware
85 from nova.volume import cinder
86 
87 LOG = logging.getLogger(__name__)
88 
89 get_notifier = functools.partial(rpc.get_notifier, service='compute')
90 # NOTE(gibi): legacy notification used compute as a service but these
91 # calls still run on the client side of the compute service which is
92 # nova-api. By setting the binary to nova-api below, we can make sure
93 # that the new versioned notifications has the right publisher_id but the
94 # legacy notifications does not change.
95 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
96                                    get_notifier=get_notifier,
97                                    binary='nova-api')
98 CONF = nova.conf.CONF
99 
100 RO_SECURITY_GROUPS = ['default']
101 
102 AGGREGATE_ACTION_UPDATE = 'Update'
103 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
104 AGGREGATE_ACTION_DELETE = 'Delete'
105 AGGREGATE_ACTION_ADD = 'Add'
106 CINDER_V3_ATTACH_MIN_COMPUTE_VERSION = 24
107 MIN_COMPUTE_MULTIATTACH = 27
108 MIN_COMPUTE_TRUSTED_CERTS = 31
109 MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION = 34
110 MIN_COMPUTE_VOLUME_TYPE = 36
111 
112 # FIXME(danms): Keep a global cache of the cells we find the
113 # first time we look. This needs to be refreshed on a timer or
114 # trigger.
115 CELLS = []
116 
117 
118 def check_instance_state(vm_state=None, task_state=(None,),
119                          must_have_launched=True):
120     """Decorator to check VM and/or task state before entry to API functions.
121 
122     If the instance is in the wrong state, or has not been successfully
123     started at least once the wrapper will raise an exception.
124     """
125 
126     if vm_state is not None and not isinstance(vm_state, set):
127         vm_state = set(vm_state)
128     if task_state is not None and not isinstance(task_state, set):
129         task_state = set(task_state)
130 
131     def outer(f):
132         @six.wraps(f)
133         def inner(self, context, instance, *args, **kw):
134             if vm_state is not None and instance.vm_state not in vm_state:
135                 raise exception.InstanceInvalidState(
136                     attr='vm_state',
137                     instance_uuid=instance.uuid,
138                     state=instance.vm_state,
139                     method=f.__name__)
140             if (task_state is not None and
141                     instance.task_state not in task_state):
142                 raise exception.InstanceInvalidState(
143                     attr='task_state',
144                     instance_uuid=instance.uuid,
145                     state=instance.task_state,
146                     method=f.__name__)
147             if must_have_launched and not instance.launched_at:
148                 raise exception.InstanceInvalidState(
149                     attr='launched_at',
150                     instance_uuid=instance.uuid,
151                     state=instance.launched_at,
152                     method=f.__name__)
153 
154             return f(self, context, instance, *args, **kw)
155         return inner
156     return outer
157 
158 
159 def _set_or_none(q):
160     return q if q is None or isinstance(q, set) else set(q)
161 
162 
163 def reject_instance_state(vm_state=None, task_state=None):
164     """Decorator.  Raise InstanceInvalidState if instance is in any of the
165     given states.
166     """
167 
168     vm_state = _set_or_none(vm_state)
169     task_state = _set_or_none(task_state)
170 
171     def outer(f):
172         @six.wraps(f)
173         def inner(self, context, instance, *args, **kw):
174             _InstanceInvalidState = functools.partial(
175                 exception.InstanceInvalidState,
176                 instance_uuid=instance.uuid,
177                 method=f.__name__)
178 
179             if vm_state is not None and instance.vm_state in vm_state:
180                 raise _InstanceInvalidState(
181                     attr='vm_state', state=instance.vm_state)
182 
183             if task_state is not None and instance.task_state in task_state:
184                 raise _InstanceInvalidState(
185                     attr='task_state', state=instance.task_state)
186 
187             return f(self, context, instance, *args, **kw)
188         return inner
189     return outer
190 
191 
192 def check_instance_host(function):
193     @six.wraps(function)
194     def wrapped(self, context, instance, *args, **kwargs):
195         if not instance.host:
196             raise exception.InstanceNotReady(instance_id=instance.uuid)
197         return function(self, context, instance, *args, **kwargs)
198     return wrapped
199 
200 
201 def check_instance_lock(function):
202     @six.wraps(function)
203     def inner(self, context, instance, *args, **kwargs):
204         if instance.locked and not context.is_admin:
205             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
206         return function(self, context, instance, *args, **kwargs)
207     return inner
208 
209 
210 def check_instance_cell(fn):
211     @six.wraps(fn)
212     def _wrapped(self, context, instance, *args, **kwargs):
213         self._validate_cell(instance)
214         return fn(self, context, instance, *args, **kwargs)
215     return _wrapped
216 
217 
218 def _diff_dict(orig, new):
219     """Return a dict describing how to change orig to new.  The keys
220     correspond to values that have changed; the value will be a list
221     of one or two elements.  The first element of the list will be
222     either '+' or '-', indicating whether the key was updated or
223     deleted; if the key was updated, the list will contain a second
224     element, giving the updated value.
225     """
226     # Figure out what keys went away
227     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
228     # Compute the updates
229     for key, value in new.items():
230         if key not in orig or value != orig[key]:
231             result[key] = ['+', value]
232     return result
233 
234 
235 def load_cells():
236     global CELLS
237     if not CELLS:
238         CELLS = objects.CellMappingList.get_all(
239             nova_context.get_admin_context())
240         LOG.debug('Found %(count)i cells: %(cells)s',
241                   dict(count=len(CELLS),
242                        cells=','.join([c.identity for c in CELLS])))
243 
244     if not CELLS:
245         LOG.error('No cells are configured, unable to continue')
246 
247 
248 def _get_image_meta_obj(image_meta_dict):
249     try:
250         image_meta = objects.ImageMeta.from_dict(image_meta_dict)
251     except ValueError as e:
252         # there must be invalid values in the image meta properties so
253         # consider this an invalid request
254         msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
255         raise exception.InvalidRequest(msg)
256     return image_meta
257 
258 
259 @profiler.trace_cls("compute_api")
260 class API(base.Base):
261     """API for interacting with the compute manager."""
262 
263     def __init__(self, image_api=None, network_api=None, volume_api=None,
264                  security_group_api=None, **kwargs):
265         self.image_api = image_api or image.API()
266         self.network_api = network_api or network.API()
267         self.volume_api = volume_api or cinder.API()
268         self._placementclient = None  # Lazy-load on first access.
269         self.security_group_api = (security_group_api or
270             openstack_driver.get_openstack_security_group_driver())
271         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
272         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
273         self.compute_task_api = conductor.ComputeTaskAPI()
274         self.servicegroup_api = servicegroup.API()
275         self.notifier = rpc.get_notifier('compute', CONF.host)
276         if CONF.ephemeral_storage_encryption.enabled:
277             self.key_manager = key_manager.API()
278         # Help us to record host in EventReporter
279         self.host = CONF.host
280         super(API, self).__init__(**kwargs)
281 
282     @property
283     def cell_type(self):
284         return getattr(self, '_cell_type', cells_opts.get_cell_type())
285 
286     def _validate_cell(self, instance):
287         if self.cell_type != 'api':
288             return
289         cell_name = instance.cell_name
290         if not cell_name:
291             raise exception.InstanceUnknownCell(
292                     instance_uuid=instance.uuid)
293 
294     def _record_action_start(self, context, instance, action):
295         objects.InstanceAction.action_start(context, instance.uuid,
296                                             action, want_result=False)
297 
298     def _check_injected_file_quota(self, context, injected_files):
299         """Enforce quota limits on injected files.
300 
301         Raises a QuotaError if any limit is exceeded.
302         """
303         if injected_files is None:
304             return
305 
306         # Check number of files first
307         try:
308             objects.Quotas.limit_check(context,
309                                        injected_files=len(injected_files))
310         except exception.OverQuota:
311             raise exception.OnsetFileLimitExceeded()
312 
313         # OK, now count path and content lengths; we're looking for
314         # the max...
315         max_path = 0
316         max_content = 0
317         for path, content in injected_files:
318             max_path = max(max_path, len(path))
319             max_content = max(max_content, len(content))
320 
321         try:
322             objects.Quotas.limit_check(context,
323                                        injected_file_path_bytes=max_path,
324                                        injected_file_content_bytes=max_content)
325         except exception.OverQuota as exc:
326             # Favor path limit over content limit for reporting
327             # purposes
328             if 'injected_file_path_bytes' in exc.kwargs['overs']:
329                 raise exception.OnsetFilePathLimitExceeded(
330                       allowed=exc.kwargs['quotas']['injected_file_path_bytes'])
331             else:
332                 raise exception.OnsetFileContentLimitExceeded(
333                    allowed=exc.kwargs['quotas']['injected_file_content_bytes'])
334 
335     def _check_metadata_properties_quota(self, context, metadata=None):
336         """Enforce quota limits on metadata properties."""
337         if not metadata:
338             metadata = {}
339         if not isinstance(metadata, dict):
340             msg = (_("Metadata type should be dict."))
341             raise exception.InvalidMetadata(reason=msg)
342         num_metadata = len(metadata)
343         try:
344             objects.Quotas.limit_check(context, metadata_items=num_metadata)
345         except exception.OverQuota as exc:
346             quota_metadata = exc.kwargs['quotas']['metadata_items']
347             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
348 
349         # Because metadata is stored in the DB, we hard-code the size limits
350         # In future, we may support more variable length strings, so we act
351         #  as if this is quota-controlled for forwards compatibility.
352         # Those are only used in V2 API, from V2.1 API, those checks are
353         # validated at API layer schema validation.
354         for k, v in metadata.items():
355             try:
356                 utils.check_string_length(v)
357                 utils.check_string_length(k, min_length=1)
358             except exception.InvalidInput as e:
359                 raise exception.InvalidMetadata(reason=e.format_message())
360 
361             if len(k) > 255:
362                 msg = _("Metadata property key greater than 255 characters")
363                 raise exception.InvalidMetadataSize(reason=msg)
364             if len(v) > 255:
365                 msg = _("Metadata property value greater than 255 characters")
366                 raise exception.InvalidMetadataSize(reason=msg)
367 
368     def _check_requested_secgroups(self, context, secgroups):
369         """Check if the security group requested exists and belongs to
370         the project.
371 
372         :param context: The nova request context.
373         :type context: nova.context.RequestContext
374         :param secgroups: list of requested security group names, or uuids in
375             the case of Neutron.
376         :type secgroups: list
377         :returns: list of requested security group names unmodified if using
378             nova-network. If using Neutron, the list returned is all uuids.
379             Note that 'default' is a special case and will be unmodified if
380             it's requested.
381         """
382         security_groups = []
383         for secgroup in secgroups:
384             # NOTE(sdague): default is handled special
385             if secgroup == "default":
386                 security_groups.append(secgroup)
387                 continue
388             secgroup_dict = self.security_group_api.get(context, secgroup)
389             if not secgroup_dict:
390                 raise exception.SecurityGroupNotFoundForProject(
391                     project_id=context.project_id, security_group_id=secgroup)
392 
393             # Check to see if it's a nova-network or neutron type.
394             if isinstance(secgroup_dict['id'], int):
395                 # This is nova-network so just return the requested name.
396                 security_groups.append(secgroup)
397             else:
398                 # The id for neutron is a uuid, so we return the id (uuid).
399                 security_groups.append(secgroup_dict['id'])
400 
401         return security_groups
402 
403     def _check_requested_networks(self, context, requested_networks,
404                                   max_count):
405         """Check if the networks requested belongs to the project
406         and the fixed IP address for each network provided is within
407         same the network block
408         """
409         if requested_networks is not None:
410             if requested_networks.no_allocate:
411                 # If the network request was specifically 'none' meaning don't
412                 # allocate any networks, we just return the number of requested
413                 # instances since quotas don't change at all.
414                 return max_count
415 
416             # NOTE(danms): Temporary transition
417             requested_networks = requested_networks.as_tuples()
418 
419         return self.network_api.validate_networks(context, requested_networks,
420                                                   max_count)
421 
422     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
423                                    image):
424         """Choose kernel and ramdisk appropriate for the instance.
425 
426         The kernel and ramdisk can be chosen in one of two ways:
427 
428             1. Passed in with create-instance request.
429 
430             2. Inherited from image metadata.
431 
432         If inherited from image metadata, and if that image metadata value is
433         set to 'nokernel', both kernel and ramdisk will default to None.
434         """
435         # Inherit from image if not specified
436         image_properties = image.get('properties', {})
437 
438         if kernel_id is None:
439             kernel_id = image_properties.get('kernel_id')
440 
441         if ramdisk_id is None:
442             ramdisk_id = image_properties.get('ramdisk_id')
443 
444         # Force to None if kernel_id indicates that a kernel is not to be used
445         if kernel_id == 'nokernel':
446             kernel_id = None
447             ramdisk_id = None
448 
449         # Verify kernel and ramdisk exist (fail-fast)
450         if kernel_id is not None:
451             kernel_image = self.image_api.get(context, kernel_id)
452             # kernel_id could have been a URI, not a UUID, so to keep behaviour
453             # from before, which leaked that implementation detail out to the
454             # caller, we return the image UUID of the kernel image and ramdisk
455             # image (below) and not any image URIs that might have been
456             # supplied.
457             # TODO(jaypipes): Get rid of this silliness once we move to a real
458             # Image object and hide all of that stuff within nova.image.api.
459             kernel_id = kernel_image['id']
460 
461         if ramdisk_id is not None:
462             ramdisk_image = self.image_api.get(context, ramdisk_id)
463             ramdisk_id = ramdisk_image['id']
464 
465         return kernel_id, ramdisk_id
466 
467     @staticmethod
468     def parse_availability_zone(context, availability_zone):
469         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
470         #             via az using az:host:node. It might be nice to expose an
471         #             api to specify specific hosts to force onto, but for
472         #             now it just supports this legacy hack.
473         # NOTE(deva): It is also possible to specify az::node, in which case
474         #             the host manager will determine the correct host.
475         forced_host = None
476         forced_node = None
477         if availability_zone and ':' in availability_zone:
478             c = availability_zone.count(':')
479             if c == 1:
480                 availability_zone, forced_host = availability_zone.split(':')
481             elif c == 2:
482                 if '::' in availability_zone:
483                     availability_zone, forced_node = \
484                             availability_zone.split('::')
485                 else:
486                     availability_zone, forced_host, forced_node = \
487                             availability_zone.split(':')
488             else:
489                 raise exception.InvalidInput(
490                         reason="Unable to parse availability_zone")
491 
492         if not availability_zone:
493             availability_zone = CONF.default_schedule_zone
494 
495         return availability_zone, forced_host, forced_node
496 
497     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
498                                           auto_disk_config, image):
499         auto_disk_config_disabled = \
500                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
501         if auto_disk_config_disabled and auto_disk_config:
502             raise exception.AutoDiskConfigDisabledByImage(image=image)
503 
504     def _inherit_properties_from_image(self, image, auto_disk_config):
505         image_properties = image.get('properties', {})
506         auto_disk_config_img = \
507                 utils.get_auto_disk_config_from_image_props(image_properties)
508         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
509                                                auto_disk_config,
510                                                image.get("id"))
511         if auto_disk_config is None:
512             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
513 
514         return {
515             'os_type': image_properties.get('os_type'),
516             'architecture': image_properties.get('architecture'),
517             'vm_mode': image_properties.get('vm_mode'),
518             'auto_disk_config': auto_disk_config
519         }
520 
521     def _check_config_drive(self, config_drive):
522         if config_drive:
523             try:
524                 bool_val = strutils.bool_from_string(config_drive,
525                                                      strict=True)
526             except ValueError:
527                 raise exception.ConfigDriveInvalidValue(option=config_drive)
528         else:
529             bool_val = False
530         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
531         # but this is because the config drive column is a String.  False
532         # is represented by using an empty string.  And for whatever
533         # reason, we rely on the DB to cast True to a String.
534         return True if bool_val else ''
535 
536     def _validate_flavor_image(self, context, image_id, image,
537                                instance_type, root_bdm, validate_numa=True):
538         """Validate the flavor and image.
539 
540         This is called from the API service to ensure that the flavor
541         extra-specs and image properties are self-consistent and compatible
542         with each other.
543 
544         :param context: A context.RequestContext
545         :param image_id: UUID of the image
546         :param image: a dict representation of the image including properties,
547                       enforces the image status is active.
548         :param instance_type: Flavor object
549         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
550                the resize case.
551         :param validate_numa: Flag to indicate whether or not to validate
552                the NUMA-related metadata.
553         :raises: Many different possible exceptions.  See
554                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
555                  for the full list.
556         """
557         if image and image['status'] != 'active':
558             raise exception.ImageNotActive(image_id=image_id)
559         self._validate_flavor_image_nostatus(context, image, instance_type,
560                                              root_bdm, validate_numa)
561 
562     @staticmethod
563     def _validate_flavor_image_nostatus(context, image, instance_type,
564                                         root_bdm, validate_numa=True,
565                                         validate_pci=False):
566         """Validate the flavor and image.
567 
568         This is called from the API service to ensure that the flavor
569         extra-specs and image properties are self-consistent and compatible
570         with each other.
571 
572         :param context: A context.RequestContext
573         :param image: a dict representation of the image including properties,
574                       enforces the image status is active.
575         :param instance_type: Flavor object
576         :param root_bdm: BlockDeviceMapping for root disk.  Will be None for
577                the resize case.
578         :param validate_numa: Flag to indicate whether or not to validate
579                the NUMA-related metadata.
580         :param validate_pci: Flag to indicate whether or not to validate
581                the PCI-related metadata.
582         :raises: Many different possible exceptions.  See
583                  api.openstack.compute.servers.INVALID_FLAVOR_IMAGE_EXCEPTIONS
584                  for the full list.
585         """
586         if not image:
587             return
588 
589         image_properties = image.get('properties', {})
590         config_drive_option = image_properties.get(
591             'img_config_drive', 'optional')
592         if config_drive_option not in ['optional', 'mandatory']:
593             raise exception.InvalidImageConfigDrive(
594                 config_drive=config_drive_option)
595 
596         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
597             raise exception.FlavorMemoryTooSmall()
598 
599         # Image min_disk is in gb, size is in bytes. For sanity, have them both
600         # in bytes.
601         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
602         image_size = int(image.get('size') or 0)
603 
604         # Target disk is a volume. Don't check flavor disk size because it
605         # doesn't make sense, and check min_disk against the volume size.
606         if (root_bdm is not None and root_bdm.is_volume):
607             # There are 2 possibilities here: either the target volume already
608             # exists, or it doesn't, in which case the bdm will contain the
609             # intended volume size.
610             #
611             # Cinder does its own check against min_disk, so if the target
612             # volume already exists this has already been done and we don't
613             # need to check it again here. In this case, volume_size may not be
614             # set on the bdm.
615             #
616             # If we're going to create the volume, the bdm will contain
617             # volume_size. Therefore we should check it if it exists. This will
618             # still be checked again by cinder when the volume is created, but
619             # that will not happen until the request reaches a host. By
620             # checking it here, the user gets an immediate and useful failure
621             # indication.
622             #
623             # The third possibility is that we have failed to consider
624             # something, and there are actually more than 2 possibilities. In
625             # this case cinder will still do the check at volume creation time.
626             # The behaviour will still be correct, but the user will not get an
627             # immediate failure from the api, and will instead have to
628             # determine why the instance is in an error state with a task of
629             # block_device_mapping.
630             #
631             # We could reasonably refactor this check into _validate_bdm at
632             # some future date, as the various size logic is already split out
633             # in there.
634             dest_size = root_bdm.volume_size
635             if dest_size is not None:
636                 dest_size *= units.Gi
637 
638                 if image_min_disk > dest_size:
639                     raise exception.VolumeSmallerThanMinDisk(
640                         volume_size=dest_size, image_min_disk=image_min_disk)
641 
642         # Target disk is a local disk whose size is taken from the flavor
643         else:
644             dest_size = instance_type['root_gb'] * units.Gi
645 
646             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
647             # since libvirt interpreted the value differently than other
648             # drivers. A value of 0 means don't check size.
649             if dest_size != 0:
650                 if image_size > dest_size:
651                     raise exception.FlavorDiskSmallerThanImage(
652                         flavor_size=dest_size, image_size=image_size)
653 
654                 if image_min_disk > dest_size:
655                     raise exception.FlavorDiskSmallerThanMinDisk(
656                         flavor_size=dest_size, image_min_disk=image_min_disk)
657             else:
658                 # The user is attempting to create a server with a 0-disk
659                 # image-backed flavor, which can lead to issues with a large
660                 # image consuming an unexpectedly large amount of local disk
661                 # on the compute host. Check to see if the deployment will
662                 # allow that.
663                 if not context.can(
664                         servers_policies.ZERO_DISK_FLAVOR, fatal=False):
665                     raise exception.BootFromVolumeRequiredForZeroDiskFlavor()
666 
667         image_meta = _get_image_meta_obj(image)
668 
669         # Only validate values of flavor/image so the return results of
670         # following 'get' functions are not used.
671         hardware.get_number_of_serial_ports(instance_type, image_meta)
672         if hardware.is_realtime_enabled(instance_type):
673             hardware.vcpus_realtime_topology(instance_type, image_meta)
674         hardware.get_cpu_topology_constraints(instance_type, image_meta)
675         if validate_numa:
676             hardware.numa_get_constraints(instance_type, image_meta)
677         if validate_pci:
678             pci_request.get_pci_requests_from_flavor(instance_type)
679 
680     def _get_image_defined_bdms(self, instance_type, image_meta,
681                                 root_device_name):
682         image_properties = image_meta.get('properties', {})
683 
684         # Get the block device mappings defined by the image.
685         image_defined_bdms = image_properties.get('block_device_mapping', [])
686         legacy_image_defined = not image_properties.get('bdm_v2', False)
687 
688         image_mapping = image_properties.get('mappings', [])
689 
690         if legacy_image_defined:
691             image_defined_bdms = block_device.from_legacy_mapping(
692                 image_defined_bdms, None, root_device_name)
693         else:
694             image_defined_bdms = list(map(block_device.BlockDeviceDict,
695                                           image_defined_bdms))
696 
697         if image_mapping:
698             image_mapping = self._prepare_image_mapping(instance_type,
699                                                         image_mapping)
700             image_defined_bdms = self._merge_bdms_lists(
701                 image_mapping, image_defined_bdms)
702 
703         return image_defined_bdms
704 
705     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
706         flavor_defined_bdms = []
707 
708         have_ephemeral_bdms = any(filter(
709             block_device.new_format_is_ephemeral, block_device_mapping))
710         have_swap_bdms = any(filter(
711             block_device.new_format_is_swap, block_device_mapping))
712 
713         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
714             flavor_defined_bdms.append(
715                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
716         if instance_type.get('swap') and not have_swap_bdms:
717             flavor_defined_bdms.append(
718                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
719 
720         return flavor_defined_bdms
721 
722     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
723         """Override any block devices from the first list by device name
724 
725         :param overridable_mappings: list which items are overridden
726         :param overrider_mappings: list which items override
727 
728         :returns: A merged list of bdms
729         """
730         device_names = set(bdm['device_name'] for bdm in overrider_mappings
731                            if bdm['device_name'])
732         return (overrider_mappings +
733                 [bdm for bdm in overridable_mappings
734                  if bdm['device_name'] not in device_names])
735 
736     def _check_and_transform_bdm(self, context, base_options, instance_type,
737                                  image_meta, min_count, max_count,
738                                  block_device_mapping, legacy_bdm):
739         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
740         #                  It's needed for legacy conversion to work.
741         root_device_name = (base_options.get('root_device_name') or 'vda')
742         image_ref = base_options.get('image_ref', '')
743         # If the instance is booted by image and has a volume attached,
744         # the volume cannot have the same device name as root_device_name
745         if image_ref:
746             for bdm in block_device_mapping:
747                 if (bdm.get('destination_type') == 'volume' and
748                     block_device.strip_dev(bdm.get(
749                     'device_name')) == root_device_name):
750                     msg = _('The volume cannot be assigned the same device'
751                             ' name as the root device %s') % root_device_name
752                     raise exception.InvalidRequest(msg)
753 
754         image_defined_bdms = self._get_image_defined_bdms(
755             instance_type, image_meta, root_device_name)
756         root_in_image_bdms = (
757             block_device.get_root_bdm(image_defined_bdms) is not None)
758 
759         if legacy_bdm:
760             block_device_mapping = block_device.from_legacy_mapping(
761                 block_device_mapping, image_ref, root_device_name,
762                 no_root=root_in_image_bdms)
763         elif root_in_image_bdms:
764             # NOTE (ndipanov): client will insert an image mapping into the v2
765             # block_device_mapping, but if there is a bootable device in image
766             # mappings - we need to get rid of the inserted image
767             # NOTE (gibi): another case is when a server is booted with an
768             # image to bdm mapping where the image only contains a bdm to a
769             # snapshot. In this case the other image to bdm mapping
770             # contains an unnecessary device with boot_index == 0.
771             # Also in this case the image_ref is None as we are booting from
772             # an image to volume bdm.
773             def not_image_and_root_bdm(bdm):
774                 return not (bdm.get('boot_index') == 0 and
775                             bdm.get('source_type') == 'image')
776 
777             block_device_mapping = list(
778                 filter(not_image_and_root_bdm, block_device_mapping))
779 
780         block_device_mapping = self._merge_bdms_lists(
781             image_defined_bdms, block_device_mapping)
782 
783         if min_count > 1 or max_count > 1:
784             if any(map(lambda bdm: bdm['source_type'] == 'volume',
785                        block_device_mapping)):
786                 msg = _('Cannot attach one or more volumes to multiple'
787                         ' instances')
788                 raise exception.InvalidRequest(msg)
789 
790         block_device_mapping += self._get_flavor_defined_bdms(
791             instance_type, block_device_mapping)
792 
793         return block_device_obj.block_device_make_list_from_dicts(
794                 context, block_device_mapping)
795 
796     def _get_image(self, context, image_href):
797         if not image_href:
798             return None, {}
799 
800         image = self.image_api.get(context, image_href)
801         return image['id'], image
802 
803     def _checks_for_create_and_rebuild(self, context, image_id, image,
804                                        instance_type, metadata,
805                                        files_to_inject, root_bdm,
806                                        validate_numa=True):
807         self._check_metadata_properties_quota(context, metadata)
808         self._check_injected_file_quota(context, files_to_inject)
809         self._validate_flavor_image(context, image_id, image,
810                                     instance_type, root_bdm,
811                                     validate_numa=validate_numa)
812 
813     def _validate_and_build_base_options(self, context, instance_type,
814                                          boot_meta, image_href, image_id,
815                                          kernel_id, ramdisk_id, display_name,
816                                          display_description, key_name,
817                                          key_data, security_groups,
818                                          availability_zone, user_data,
819                                          metadata, access_ip_v4, access_ip_v6,
820                                          requested_networks, config_drive,
821                                          auto_disk_config, reservation_id,
822                                          max_count,
823                                          supports_port_resource_request):
824         """Verify all the input parameters regardless of the provisioning
825         strategy being performed.
826         """
827         if instance_type['disabled']:
828             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
829 
830         if user_data:
831             try:
832                 base64utils.decode_as_bytes(user_data)
833             except TypeError:
834                 raise exception.InstanceUserDataMalformed()
835 
836         # When using Neutron, _check_requested_secgroups will translate and
837         # return any requested security group names to uuids.
838         security_groups = (
839             self._check_requested_secgroups(context, security_groups))
840 
841         # Note:  max_count is the number of instances requested by the user,
842         # max_network_count is the maximum number of instances taking into
843         # account any network quotas
844         max_network_count = self._check_requested_networks(context,
845                                      requested_networks, max_count)
846 
847         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
848                 context, kernel_id, ramdisk_id, boot_meta)
849 
850         config_drive = self._check_config_drive(config_drive)
851 
852         if key_data is None and key_name is not None:
853             key_pair = objects.KeyPair.get_by_name(context,
854                                                    context.user_id,
855                                                    key_name)
856             key_data = key_pair.public_key
857         else:
858             key_pair = None
859 
860         root_device_name = block_device.prepend_dev(
861                 block_device.properties_root_device_name(
862                     boot_meta.get('properties', {})))
863 
864         image_meta = _get_image_meta_obj(boot_meta)
865         numa_topology = hardware.numa_get_constraints(
866                 instance_type, image_meta)
867 
868         system_metadata = {}
869 
870         # PCI requests come from two sources: instance flavor and
871         # requested_networks. The first call in below returns an
872         # InstancePCIRequests object which is a list of InstancePCIRequest
873         # objects. The second call in below creates an InstancePCIRequest
874         # object for each SR-IOV port, and append it to the list in the
875         # InstancePCIRequests object
876         pci_request_info = pci_request.get_pci_requests_from_flavor(
877             instance_type)
878         result = self.network_api.create_resource_requests(
879             context, requested_networks, pci_request_info)
880         network_metadata, port_resource_requests = result
881 
882         # Creating servers with ports that have resource requests, like QoS
883         # minimum bandwidth rules, is only supported in a requested minimum
884         # microversion.
885         if port_resource_requests and not supports_port_resource_request:
886             raise exception.CreateWithPortResourceRequestOldVersion()
887 
888         base_options = {
889             'reservation_id': reservation_id,
890             'image_ref': image_href,
891             'kernel_id': kernel_id or '',
892             'ramdisk_id': ramdisk_id or '',
893             'power_state': power_state.NOSTATE,
894             'vm_state': vm_states.BUILDING,
895             'config_drive': config_drive,
896             'user_id': context.user_id,
897             'project_id': context.project_id,
898             'instance_type_id': instance_type['id'],
899             'memory_mb': instance_type['memory_mb'],
900             'vcpus': instance_type['vcpus'],
901             'root_gb': instance_type['root_gb'],
902             'ephemeral_gb': instance_type['ephemeral_gb'],
903             'display_name': display_name,
904             'display_description': display_description,
905             'user_data': user_data,
906             'key_name': key_name,
907             'key_data': key_data,
908             'locked': False,
909             'metadata': metadata or {},
910             'access_ip_v4': access_ip_v4,
911             'access_ip_v6': access_ip_v6,
912             'availability_zone': availability_zone,
913             'root_device_name': root_device_name,
914             'progress': 0,
915             'pci_requests': pci_request_info,
916             'numa_topology': numa_topology,
917             'system_metadata': system_metadata,
918             'port_resource_requests': port_resource_requests}
919 
920         options_from_image = self._inherit_properties_from_image(
921                 boot_meta, auto_disk_config)
922 
923         base_options.update(options_from_image)
924 
925         # return the validated options and maximum number of instances allowed
926         # by the network quotas
927         return (base_options, max_network_count, key_pair, security_groups,
928                 network_metadata)
929 
930     def _provision_instances(self, context, instance_type, min_count,
931             max_count, base_options, boot_meta, security_groups,
932             block_device_mapping, shutdown_terminate,
933             instance_group, check_server_group_quota, filter_properties,
934             key_pair, tags, trusted_certs, supports_multiattach,
935             network_metadata=None):
936         # Check quotas
937         num_instances = compute_utils.check_num_instances_quota(
938                 context, instance_type, min_count, max_count)
939         security_groups = self.security_group_api.populate_security_groups(
940                 security_groups)
941         self.security_group_api.ensure_default(context)
942         port_resource_requests = base_options.pop('port_resource_requests')
943         LOG.debug("Going to run %s instances...", num_instances)
944         instances_to_build = []
945         try:
946             for i in range(num_instances):
947                 # Create a uuid for the instance so we can store the
948                 # RequestSpec before the instance is created.
949                 instance_uuid = uuidutils.generate_uuid()
950                 # Store the RequestSpec that will be used for scheduling.
951                 req_spec = objects.RequestSpec.from_components(context,
952                         instance_uuid, boot_meta, instance_type,
953                         base_options['numa_topology'],
954                         base_options['pci_requests'], filter_properties,
955                         instance_group, base_options['availability_zone'],
956                         security_groups=security_groups,
957                         port_resource_requests=port_resource_requests)
958 
959                 if block_device_mapping:
960                     # Record whether or not we are a BFV instance
961                     root = block_device_mapping.root_bdm()
962                     req_spec.is_bfv = bool(root and root.is_volume)
963                 else:
964                     # If we have no BDMs, we're clearly not BFV
965                     req_spec.is_bfv = False
966 
967                 # NOTE(danms): We need to record num_instances on the request
968                 # spec as this is how the conductor knows how many were in this
969                 # batch.
970                 req_spec.num_instances = num_instances
971                 req_spec.create()
972 
973                 # NOTE(stephenfin): The network_metadata field is not persisted
974                 # and is therefore set after 'create' is called.
975                 if network_metadata:
976                     req_spec.network_metadata = network_metadata
977 
978                 # Create an instance object, but do not store in db yet.
979                 instance = objects.Instance(context=context)
980                 instance.uuid = instance_uuid
981                 instance.update(base_options)
982                 instance.keypairs = objects.KeyPairList(objects=[])
983                 if key_pair:
984                     instance.keypairs.objects.append(key_pair)
985 
986                 instance.trusted_certs = self._retrieve_trusted_certs_object(
987                     context, trusted_certs)
988 
989                 instance = self.create_db_entry_for_new_instance(context,
990                         instance_type, boot_meta, instance, security_groups,
991                         block_device_mapping, num_instances, i,
992                         shutdown_terminate, create_instance=False)
993                 block_device_mapping = (
994                     self._bdm_validate_set_size_and_instance(context,
995                         instance, instance_type, block_device_mapping,
996                         supports_multiattach))
997                 instance_tags = self._transform_tags(tags, instance.uuid)
998 
999                 build_request = objects.BuildRequest(context,
1000                         instance=instance, instance_uuid=instance.uuid,
1001                         project_id=instance.project_id,
1002                         block_device_mappings=block_device_mapping,
1003                         tags=instance_tags)
1004                 build_request.create()
1005 
1006                 # Create an instance_mapping.  The null cell_mapping indicates
1007                 # that the instance doesn't yet exist in a cell, and lookups
1008                 # for it need to instead look for the RequestSpec.
1009                 # cell_mapping will be populated after scheduling, with a
1010                 # scheduling failure using the cell_mapping for the special
1011                 # cell0.
1012                 inst_mapping = objects.InstanceMapping(context=context)
1013                 inst_mapping.instance_uuid = instance_uuid
1014                 inst_mapping.project_id = context.project_id
1015                 inst_mapping.user_id = context.user_id
1016                 inst_mapping.cell_mapping = None
1017                 inst_mapping.create()
1018 
1019                 instances_to_build.append(
1020                     (req_spec, build_request, inst_mapping))
1021 
1022                 if instance_group:
1023                     if check_server_group_quota:
1024                         try:
1025                             objects.Quotas.check_deltas(
1026                                 context, {'server_group_members': 1},
1027                                 instance_group, context.user_id)
1028                         except exception.OverQuota:
1029                             msg = _("Quota exceeded, too many servers in "
1030                                     "group")
1031                             raise exception.QuotaError(msg)
1032 
1033                     members = objects.InstanceGroup.add_members(
1034                         context, instance_group.uuid, [instance.uuid])
1035 
1036                     # NOTE(melwitt): We recheck the quota after creating the
1037                     # object to prevent users from allocating more resources
1038                     # than their allowed quota in the event of a race. This is
1039                     # configurable because it can be expensive if strict quota
1040                     # limits are not required in a deployment.
1041                     if CONF.quota.recheck_quota and check_server_group_quota:
1042                         try:
1043                             objects.Quotas.check_deltas(
1044                                 context, {'server_group_members': 0},
1045                                 instance_group, context.user_id)
1046                         except exception.OverQuota:
1047                             objects.InstanceGroup._remove_members_in_db(
1048                                 context, instance_group.id, [instance.uuid])
1049                             msg = _("Quota exceeded, too many servers in "
1050                                     "group")
1051                             raise exception.QuotaError(msg)
1052                     # list of members added to servers group in this iteration
1053                     # is needed to check quota of server group during add next
1054                     # instance
1055                     instance_group.members.extend(members)
1056 
1057         # In the case of any exceptions, attempt DB cleanup
1058         except Exception:
1059             with excutils.save_and_reraise_exception():
1060                 self._cleanup_build_artifacts(None, instances_to_build)
1061 
1062         return instances_to_build
1063 
1064     @staticmethod
1065     def _retrieve_trusted_certs_object(context, trusted_certs, rebuild=False):
1066         """Convert user-requested trusted cert IDs to TrustedCerts object
1067 
1068         Also validates that the deployment is new enough to support trusted
1069         image certification validation.
1070 
1071         :param context: The user request auth context
1072         :param trusted_certs: list of user-specified trusted cert string IDs,
1073             may be None
1074         :param rebuild: True if rebuilding the server, False if creating a
1075             new server
1076         :returns: nova.objects.TrustedCerts object or None if no user-specified
1077             trusted cert IDs were given and nova is not configured with
1078             default trusted cert IDs
1079         :raises: nova.exception.CertificateValidationNotYetAvailable: If
1080             rebuilding a server with trusted certs on a compute host that is
1081             too old to supported trusted image cert validation, or if creating
1082             a server with trusted certs and there are no compute hosts in the
1083             deployment that are new enough to support trusted image cert
1084             validation
1085         """
1086         # Retrieve trusted_certs parameter, or use CONF value if certificate
1087         # validation is enabled
1088         if trusted_certs:
1089             certs_to_return = objects.TrustedCerts(ids=trusted_certs)
1090         elif (CONF.glance.verify_glance_signatures and
1091               CONF.glance.enable_certificate_validation and
1092               CONF.glance.default_trusted_certificate_ids):
1093             certs_to_return = objects.TrustedCerts(
1094                 ids=CONF.glance.default_trusted_certificate_ids)
1095         else:
1096             return None
1097 
1098         # Confirm trusted_certs are supported by the minimum nova
1099         # compute service version
1100         # TODO(mriedem): This minimum version compat code can be dropped in the
1101         # 19.0.0 Stein release when all computes must be at a minimum running
1102         # Rocky code.
1103         if rebuild:
1104             # we only care about the current cell since this is
1105             # a rebuild
1106             min_compute_version = objects.Service.get_minimum_version(
1107                 context, 'nova-compute')
1108         else:
1109             # we don't know which cell it's going to get scheduled
1110             # to, so check all cells
1111             # NOTE(mriedem): For multi-create server requests, we're hitting
1112             # this for each instance since it's not cached; we could likely
1113             # optimize this.
1114             min_compute_version = \
1115                 objects.service.get_minimum_version_all_cells(
1116                     context, ['nova-compute'])
1117 
1118         if min_compute_version < MIN_COMPUTE_TRUSTED_CERTS:
1119             raise exception.CertificateValidationNotYetAvailable()
1120 
1121         return certs_to_return
1122 
1123     def _get_bdm_image_metadata(self, context, block_device_mapping,
1124                                 legacy_bdm=True):
1125         """If we are booting from a volume, we need to get the
1126         volume details from Cinder and make sure we pass the
1127         metadata back accordingly.
1128         """
1129         if not block_device_mapping:
1130             return {}
1131 
1132         for bdm in block_device_mapping:
1133             if (legacy_bdm and
1134                     block_device.get_device_letter(
1135                        bdm.get('device_name', '')) != 'a'):
1136                 continue
1137             elif not legacy_bdm and bdm.get('boot_index') != 0:
1138                 continue
1139 
1140             volume_id = bdm.get('volume_id')
1141             snapshot_id = bdm.get('snapshot_id')
1142             if snapshot_id:
1143                 # NOTE(alaski): A volume snapshot inherits metadata from the
1144                 # originating volume, but the API does not expose metadata
1145                 # on the snapshot itself.  So we query the volume for it below.
1146                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1147                 volume_id = snapshot['volume_id']
1148 
1149             if bdm.get('image_id'):
1150                 try:
1151                     image_id = bdm['image_id']
1152                     image_meta = self.image_api.get(context, image_id)
1153                     return image_meta
1154                 except Exception:
1155                     raise exception.InvalidBDMImage(id=image_id)
1156             elif volume_id:
1157                 try:
1158                     volume = self.volume_api.get(context, volume_id)
1159                 except exception.CinderConnectionFailed:
1160                     raise
1161                 except Exception:
1162                     raise exception.InvalidBDMVolume(id=volume_id)
1163 
1164                 if not volume.get('bootable', True):
1165                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1166 
1167                 return utils.get_image_metadata_from_volume(volume)
1168         return {}
1169 
1170     @staticmethod
1171     def _get_requested_instance_group(context, filter_properties):
1172         if (not filter_properties or
1173                 not filter_properties.get('scheduler_hints')):
1174             return
1175 
1176         group_hint = filter_properties.get('scheduler_hints').get('group')
1177         if not group_hint:
1178             return
1179 
1180         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1181 
1182     def _create_instance(self, context, instance_type,
1183                image_href, kernel_id, ramdisk_id,
1184                min_count, max_count,
1185                display_name, display_description,
1186                key_name, key_data, security_groups,
1187                availability_zone, user_data, metadata, injected_files,
1188                admin_password, access_ip_v4, access_ip_v6,
1189                requested_networks, config_drive,
1190                block_device_mapping, auto_disk_config, filter_properties,
1191                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1192                check_server_group_quota=False, tags=None,
1193                supports_multiattach=False, trusted_certs=None,
1194                supports_port_resource_request=False):
1195         """Verify all the input parameters regardless of the provisioning
1196         strategy being performed and schedule the instance(s) for
1197         creation.
1198         """
1199 
1200         # Normalize and setup some parameters
1201         if reservation_id is None:
1202             reservation_id = utils.generate_uid('r')
1203         security_groups = security_groups or ['default']
1204         min_count = min_count or 1
1205         max_count = max_count or min_count
1206         block_device_mapping = block_device_mapping or []
1207         tags = tags or []
1208 
1209         if image_href:
1210             image_id, boot_meta = self._get_image(context, image_href)
1211         else:
1212             # This is similar to the logic in _retrieve_trusted_certs_object.
1213             if (trusted_certs or
1214                 (CONF.glance.verify_glance_signatures and
1215                  CONF.glance.enable_certificate_validation and
1216                  CONF.glance.default_trusted_certificate_ids)):
1217                 msg = _("Image certificate validation is not supported "
1218                         "when booting from volume")
1219                 raise exception.CertificateValidationFailed(message=msg)
1220             image_id = None
1221             boot_meta = self._get_bdm_image_metadata(
1222                 context, block_device_mapping, legacy_bdm)
1223 
1224         self._check_auto_disk_config(image=boot_meta,
1225                                      auto_disk_config=auto_disk_config)
1226 
1227         base_options, max_net_count, key_pair, security_groups, \
1228             network_metadata = self._validate_and_build_base_options(
1229                     context, instance_type, boot_meta, image_href, image_id,
1230                     kernel_id, ramdisk_id, display_name, display_description,
1231                     key_name, key_data, security_groups, availability_zone,
1232                     user_data, metadata, access_ip_v4, access_ip_v6,
1233                     requested_networks, config_drive, auto_disk_config,
1234                     reservation_id, max_count, supports_port_resource_request)
1235 
1236         # max_net_count is the maximum number of instances requested by the
1237         # user adjusted for any network quota constraints, including
1238         # consideration of connections to each requested network
1239         if max_net_count < min_count:
1240             raise exception.PortLimitExceeded()
1241         elif max_net_count < max_count:
1242             LOG.info("max count reduced from %(max_count)d to "
1243                      "%(max_net_count)d due to network port quota",
1244                      {'max_count': max_count,
1245                       'max_net_count': max_net_count})
1246             max_count = max_net_count
1247 
1248         block_device_mapping = self._check_and_transform_bdm(context,
1249             base_options, instance_type, boot_meta, min_count, max_count,
1250             block_device_mapping, legacy_bdm)
1251 
1252         # We can't do this check earlier because we need bdms from all sources
1253         # to have been merged in order to get the root bdm.
1254         # Set validate_numa=False since numa validation is already done by
1255         # _validate_and_build_base_options().
1256         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1257                 instance_type, metadata, injected_files,
1258                 block_device_mapping.root_bdm(), validate_numa=False)
1259 
1260         instance_group = self._get_requested_instance_group(context,
1261                                    filter_properties)
1262 
1263         tags = self._create_tag_list_obj(context, tags)
1264 
1265         instances_to_build = self._provision_instances(
1266             context, instance_type, min_count, max_count, base_options,
1267             boot_meta, security_groups, block_device_mapping,
1268             shutdown_terminate, instance_group, check_server_group_quota,
1269             filter_properties, key_pair, tags, trusted_certs,
1270             supports_multiattach, network_metadata)
1271 
1272         instances = []
1273         request_specs = []
1274         build_requests = []
1275         for rs, build_request, im in instances_to_build:
1276             build_requests.append(build_request)
1277             instance = build_request.get_new_instance(context)
1278             instances.append(instance)
1279             request_specs.append(rs)
1280 
1281         if CONF.cells.enable:
1282             # NOTE(danms): CellsV1 can't do the new thing, so we
1283             # do the old thing here. We can remove this path once
1284             # we stop supporting v1.
1285             for instance in instances:
1286                 instance.create()
1287             # NOTE(melwitt): We recheck the quota after creating the objects
1288             # to prevent users from allocating more resources than their
1289             # allowed quota in the event of a race. This is configurable
1290             # because it can be expensive if strict quota limits are not
1291             # required in a deployment.
1292             if CONF.quota.recheck_quota:
1293                 try:
1294                     compute_utils.check_num_instances_quota(
1295                         context, instance_type, 0, 0,
1296                         orig_num_req=len(instances))
1297                 except exception.TooManyInstances:
1298                     with excutils.save_and_reraise_exception():
1299                         # Need to clean up all the instances we created
1300                         # along with the build requests, request specs,
1301                         # and instance mappings.
1302                         self._cleanup_build_artifacts(instances,
1303                                                       instances_to_build)
1304 
1305             self.compute_task_api.build_instances(context,
1306                 instances=instances, image=boot_meta,
1307                 filter_properties=filter_properties,
1308                 admin_password=admin_password,
1309                 injected_files=injected_files,
1310                 requested_networks=requested_networks,
1311                 security_groups=security_groups,
1312                 block_device_mapping=block_device_mapping,
1313                 legacy_bdm=False)
1314         else:
1315             self.compute_task_api.schedule_and_build_instances(
1316                 context,
1317                 build_requests=build_requests,
1318                 request_spec=request_specs,
1319                 image=boot_meta,
1320                 admin_password=admin_password,
1321                 injected_files=injected_files,
1322                 requested_networks=requested_networks,
1323                 block_device_mapping=block_device_mapping,
1324                 tags=tags)
1325 
1326         return instances, reservation_id
1327 
1328     @staticmethod
1329     def _cleanup_build_artifacts(instances, instances_to_build):
1330         # instances_to_build is a list of tuples:
1331         # (RequestSpec, BuildRequest, InstanceMapping)
1332 
1333         # Be paranoid about artifacts being deleted underneath us.
1334         for instance in instances or []:
1335             try:
1336                 instance.destroy()
1337             except exception.InstanceNotFound:
1338                 pass
1339         for rs, build_request, im in instances_to_build or []:
1340             try:
1341                 rs.destroy()
1342             except exception.RequestSpecNotFound:
1343                 pass
1344             try:
1345                 build_request.destroy()
1346             except exception.BuildRequestNotFound:
1347                 pass
1348             try:
1349                 im.destroy()
1350             except exception.InstanceMappingNotFound:
1351                 pass
1352 
1353     @staticmethod
1354     def _volume_size(instance_type, bdm):
1355         size = bdm.get('volume_size')
1356         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1357         if (size is None and bdm.get('source_type') == 'blank' and
1358                 bdm.get('destination_type') == 'local'):
1359             if bdm.get('guest_format') == 'swap':
1360                 size = instance_type.get('swap', 0)
1361             else:
1362                 size = instance_type.get('ephemeral_gb', 0)
1363         return size
1364 
1365     def _prepare_image_mapping(self, instance_type, mappings):
1366         """Extract and format blank devices from image mappings."""
1367 
1368         prepared_mappings = []
1369 
1370         for bdm in block_device.mappings_prepend_dev(mappings):
1371             LOG.debug("Image bdm %s", bdm)
1372 
1373             virtual_name = bdm['virtual']
1374             if virtual_name == 'ami' or virtual_name == 'root':
1375                 continue
1376 
1377             if not block_device.is_swap_or_ephemeral(virtual_name):
1378                 continue
1379 
1380             guest_format = bdm.get('guest_format')
1381             if virtual_name == 'swap':
1382                 guest_format = 'swap'
1383             if not guest_format:
1384                 guest_format = CONF.default_ephemeral_format
1385 
1386             values = block_device.BlockDeviceDict({
1387                 'device_name': bdm['device'],
1388                 'source_type': 'blank',
1389                 'destination_type': 'local',
1390                 'device_type': 'disk',
1391                 'guest_format': guest_format,
1392                 'delete_on_termination': True,
1393                 'boot_index': -1})
1394 
1395             values['volume_size'] = self._volume_size(
1396                 instance_type, values)
1397             if values['volume_size'] == 0:
1398                 continue
1399 
1400             prepared_mappings.append(values)
1401 
1402         return prepared_mappings
1403 
1404     def _bdm_validate_set_size_and_instance(self, context, instance,
1405                                             instance_type,
1406                                             block_device_mapping,
1407                                             supports_multiattach=False):
1408         """Ensure the bdms are valid, then set size and associate with instance
1409 
1410         Because this method can be called multiple times when more than one
1411         instance is booted in a single request it makes a copy of the bdm list.
1412         """
1413         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1414                   instance_uuid=instance.uuid)
1415         self._validate_bdm(
1416             context, instance, instance_type, block_device_mapping,
1417             supports_multiattach)
1418         instance_block_device_mapping = block_device_mapping.obj_clone()
1419         for bdm in instance_block_device_mapping:
1420             bdm.volume_size = self._volume_size(instance_type, bdm)
1421             bdm.instance_uuid = instance.uuid
1422         return instance_block_device_mapping
1423 
1424     def _create_block_device_mapping(self, block_device_mapping):
1425         # Copy the block_device_mapping because this method can be called
1426         # multiple times when more than one instance is booted in a single
1427         # request. This avoids 'id' being set and triggering the object dupe
1428         # detection
1429         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1430         # Create the BlockDeviceMapping objects in the db.
1431         for bdm in db_block_device_mapping:
1432             # TODO(alaski): Why is this done?
1433             if bdm.volume_size == 0:
1434                 continue
1435 
1436             bdm.update_or_create()
1437 
1438     @staticmethod
1439     def _check_requested_volume_type(bdm, volume_type_id_or_name,
1440                                      volume_types):
1441         """If we are specifying a volume type, we need to get the
1442         volume type details from Cinder and make sure the ``volume_type``
1443         is available.
1444         """
1445 
1446         # NOTE(brinzhang): Verify that the specified volume type exists.
1447         # And save the volume type name internally for consistency in the
1448         # BlockDeviceMapping object.
1449         for vol_type in volume_types:
1450             if (volume_type_id_or_name == vol_type['id'] or
1451                         volume_type_id_or_name == vol_type['name']):
1452                 bdm.volume_type = vol_type['name']
1453                 break
1454         else:
1455             raise exception.VolumeTypeNotFound(
1456                 id_or_name=volume_type_id_or_name)
1457 
1458     @staticmethod
1459     def _check_compute_supports_volume_type(context):
1460         # NOTE(brinzhang): Checking the minimum nova-compute service
1461         # version across the deployment. Just make sure the volume
1462         # type can be supported when the bdm.volume_type is requested.
1463         min_compute_version = objects.service.get_minimum_version_all_cells(
1464             context, ['nova-compute'])
1465         if min_compute_version < MIN_COMPUTE_VOLUME_TYPE:
1466             raise exception.VolumeTypeSupportNotYetAvailable()
1467 
1468     def _validate_bdm(self, context, instance, instance_type,
1469                       block_device_mappings, supports_multiattach=False):
1470         # Make sure that the boot indexes make sense.
1471         # Setting a negative value or None indicates that the device should not
1472         # be used for booting.
1473         boot_indexes = sorted([bdm.boot_index
1474                                for bdm in block_device_mappings
1475                                if bdm.boot_index is not None
1476                                and bdm.boot_index >= 0])
1477 
1478         # Each device which is capable of being used as boot device should
1479         # be given a unique boot index, starting from 0 in ascending order, and
1480         # there needs to be at least one boot device.
1481         if not boot_indexes or any(i != v for i, v in enumerate(boot_indexes)):
1482             # Convert the BlockDeviceMappingList to a list for repr details.
1483             LOG.debug('Invalid block device mapping boot sequence for '
1484                       'instance: %s', list(block_device_mappings),
1485                       instance=instance)
1486             raise exception.InvalidBDMBootSequence()
1487 
1488         volume_types = None
1489         volume_type_is_supported = False
1490         for bdm in block_device_mappings:
1491             volume_type = bdm.volume_type
1492             if volume_type:
1493                 if not volume_type_is_supported:
1494                     # The following method raises
1495                     # VolumeTypeSupportNotYetAvailable if the minimum
1496                     # nova-compute service version across the deployment is
1497                     # not new enough to support creating volumes with a
1498                     # specific type.
1499                     self._check_compute_supports_volume_type(context)
1500                     # Set the flag to avoid calling
1501                     # _check_compute_supports_volume_type more than once in
1502                     # this for loop.
1503                     volume_type_is_supported = True
1504 
1505                 if not volume_types:
1506                     # In order to reduce the number of hit cinder APIs,
1507                     # initialize our cache of volume types.
1508                     volume_types = self.volume_api.get_all_volume_types(
1509                         context)
1510                 # NOTE(brinzhang): Ensure the validity of volume_type.
1511                 self._check_requested_volume_type(bdm, volume_type,
1512                                                   volume_types)
1513 
1514             # NOTE(vish): For now, just make sure the volumes are accessible.
1515             # Additionally, check that the volume can be attached to this
1516             # instance.
1517             snapshot_id = bdm.snapshot_id
1518             volume_id = bdm.volume_id
1519             image_id = bdm.image_id
1520             if image_id is not None:
1521                 if image_id != instance.get('image_ref'):
1522                     try:
1523                         self._get_image(context, image_id)
1524                     except Exception:
1525                         raise exception.InvalidBDMImage(id=image_id)
1526                 if (bdm.source_type == 'image' and
1527                         bdm.destination_type == 'volume' and
1528                         not bdm.volume_size):
1529                     raise exception.InvalidBDM(message=_("Images with "
1530                         "destination_type 'volume' need to have a non-zero "
1531                         "size specified"))
1532             elif volume_id is not None:
1533                 try:
1534                     volume = self.volume_api.get(context, volume_id)
1535                     self._check_attach_and_reserve_volume(
1536                         context, volume, instance, bdm, supports_multiattach)
1537                     bdm.volume_size = volume.get('size')
1538 
1539                     # NOTE(mnaser): If we end up reserving the volume, it will
1540                     #               not have an attachment_id which is needed
1541                     #               for cleanups.  This can be removed once
1542                     #               all calls to reserve_volume are gone.
1543                     if 'attachment_id' not in bdm:
1544                         bdm.attachment_id = None
1545                 except (exception.CinderConnectionFailed,
1546                         exception.InvalidVolume,
1547                         exception.MultiattachNotSupportedOldMicroversion,
1548                         exception.MultiattachSupportNotYetAvailable):
1549                     raise
1550                 except exception.InvalidInput as exc:
1551                     raise exception.InvalidVolume(reason=exc.format_message())
1552                 except Exception:
1553                     raise exception.InvalidBDMVolume(id=volume_id)
1554             elif snapshot_id is not None:
1555                 try:
1556                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1557                     bdm.volume_size = bdm.volume_size or snap.get('size')
1558                 except exception.CinderConnectionFailed:
1559                     raise
1560                 except Exception:
1561                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1562             elif (bdm.source_type == 'blank' and
1563                     bdm.destination_type == 'volume' and
1564                     not bdm.volume_size):
1565                 raise exception.InvalidBDM(message=_("Blank volumes "
1566                     "(source: 'blank', dest: 'volume') need to have non-zero "
1567                     "size"))
1568 
1569         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1570                 for bdm in block_device_mappings
1571                 if block_device.new_format_is_ephemeral(bdm))
1572         if ephemeral_size > instance_type['ephemeral_gb']:
1573             raise exception.InvalidBDMEphemeralSize()
1574 
1575         # There should be only one swap
1576         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1577         if len(swap_list) > 1:
1578             msg = _("More than one swap drive requested.")
1579             raise exception.InvalidBDMFormat(details=msg)
1580 
1581         if swap_list:
1582             swap_size = swap_list[0].volume_size or 0
1583             if swap_size > instance_type['swap']:
1584                 raise exception.InvalidBDMSwapSize()
1585 
1586         max_local = CONF.max_local_block_devices
1587         if max_local >= 0:
1588             num_local = len([bdm for bdm in block_device_mappings
1589                              if bdm.destination_type == 'local'])
1590             if num_local > max_local:
1591                 raise exception.InvalidBDMLocalsLimit()
1592 
1593     def _populate_instance_names(self, instance, num_instances, index):
1594         """Populate instance display_name and hostname.
1595 
1596         :param instance: The instance to set the display_name, hostname for
1597         :type instance: nova.objects.Instance
1598         :param num_instances: Total number of instances being created in this
1599             request
1600         :param index: The 0-based index of this particular instance
1601         """
1602         # NOTE(mriedem): This is only here for test simplicity since a server
1603         # name is required in the REST API.
1604         if 'display_name' not in instance or instance.display_name is None:
1605             instance.display_name = 'Server %s' % instance.uuid
1606 
1607         # if we're booting multiple instances, we need to add an indexing
1608         # suffix to both instance.hostname and instance.display_name. This is
1609         # not necessary for a single instance.
1610         if num_instances == 1:
1611             default_hostname = 'Server-%s' % instance.uuid
1612             instance.hostname = utils.sanitize_hostname(
1613                 instance.display_name, default_hostname)
1614         elif num_instances > 1 and self.cell_type != 'api':
1615             old_display_name = instance.display_name
1616             new_display_name = '%s-%d' % (old_display_name, index + 1)
1617 
1618             if utils.sanitize_hostname(old_display_name) == "":
1619                 instance.hostname = 'Server-%s' % instance.uuid
1620             else:
1621                 instance.hostname = utils.sanitize_hostname(
1622                     new_display_name)
1623 
1624             instance.display_name = new_display_name
1625 
1626     def _populate_instance_for_create(self, context, instance, image,
1627                                       index, security_groups, instance_type,
1628                                       num_instances, shutdown_terminate):
1629         """Build the beginning of a new instance."""
1630 
1631         instance.launch_index = index
1632         instance.vm_state = vm_states.BUILDING
1633         instance.task_state = task_states.SCHEDULING
1634         info_cache = objects.InstanceInfoCache()
1635         info_cache.instance_uuid = instance.uuid
1636         info_cache.network_info = network_model.NetworkInfo()
1637         instance.info_cache = info_cache
1638         instance.flavor = instance_type
1639         instance.old_flavor = None
1640         instance.new_flavor = None
1641         if CONF.ephemeral_storage_encryption.enabled:
1642             # NOTE(kfarr): dm-crypt expects the cipher in a
1643             # hyphenated format: cipher-chainmode-ivmode
1644             # (ex: aes-xts-plain64). The algorithm needs
1645             # to be parsed out to pass to the key manager (ex: aes).
1646             cipher = CONF.ephemeral_storage_encryption.cipher
1647             algorithm = cipher.split('-')[0] if cipher else None
1648             instance.ephemeral_key_uuid = self.key_manager.create_key(
1649                 context,
1650                 algorithm=algorithm,
1651                 length=CONF.ephemeral_storage_encryption.key_size)
1652         else:
1653             instance.ephemeral_key_uuid = None
1654 
1655         # Store image properties so we can use them later
1656         # (for notifications, etc).  Only store what we can.
1657         if not instance.obj_attr_is_set('system_metadata'):
1658             instance.system_metadata = {}
1659         # Make sure we have the dict form that we need for instance_update.
1660         instance.system_metadata = utils.instance_sys_meta(instance)
1661 
1662         system_meta = utils.get_system_metadata_from_image(
1663             image, instance_type)
1664 
1665         # In case we couldn't find any suitable base_image
1666         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1667 
1668         system_meta['owner_user_name'] = context.user_name
1669         system_meta['owner_project_name'] = context.project_name
1670 
1671         instance.system_metadata.update(system_meta)
1672 
1673         if CONF.use_neutron:
1674             # For Neutron we don't actually store anything in the database, we
1675             # proxy the security groups on the instance from the ports
1676             # attached to the instance.
1677             instance.security_groups = objects.SecurityGroupList()
1678         else:
1679             instance.security_groups = security_groups
1680 
1681         self._populate_instance_names(instance, num_instances, index)
1682         instance.shutdown_terminate = shutdown_terminate
1683 
1684         return instance
1685 
1686     def _create_tag_list_obj(self, context, tags):
1687         """Create TagList objects from simple string tags.
1688 
1689         :param context: security context.
1690         :param tags: simple string tags from API request.
1691         :returns: TagList object.
1692         """
1693         tag_list = [objects.Tag(context=context, tag=t) for t in tags]
1694         tag_list_obj = objects.TagList(objects=tag_list)
1695         return tag_list_obj
1696 
1697     def _transform_tags(self, tags, resource_id):
1698         """Change the resource_id of the tags according to the input param.
1699 
1700         Because this method can be called multiple times when more than one
1701         instance is booted in a single request it makes a copy of the tags
1702         list.
1703 
1704         :param tags: TagList object.
1705         :param resource_id: string.
1706         :returns: TagList object.
1707         """
1708         instance_tags = tags.obj_clone()
1709         for tag in instance_tags:
1710             tag.resource_id = resource_id
1711         return instance_tags
1712 
1713     # This method remains because cellsv1 uses it in the scheduler
1714     def create_db_entry_for_new_instance(self, context, instance_type, image,
1715             instance, security_group, block_device_mapping, num_instances,
1716             index, shutdown_terminate=False, create_instance=True):
1717         """Create an entry in the DB for this new instance,
1718         including any related table updates (such as security group,
1719         etc).
1720 
1721         This is called by the scheduler after a location for the
1722         instance has been determined.
1723 
1724         :param create_instance: Determines if the instance is created here or
1725             just populated for later creation. This is done so that this code
1726             can be shared with cellsv1 which needs the instance creation to
1727             happen here. It should be removed and this method cleaned up when
1728             cellsv1 is a distant memory.
1729         """
1730         self._populate_instance_for_create(context, instance, image, index,
1731                                            security_group, instance_type,
1732                                            num_instances, shutdown_terminate)
1733 
1734         if create_instance:
1735             instance.create()
1736 
1737         return instance
1738 
1739     def _check_multiple_instances_with_neutron_ports(self,
1740                                                      requested_networks):
1741         """Check whether multiple instances are created from port id(s)."""
1742         for requested_net in requested_networks:
1743             if requested_net.port_id:
1744                 msg = _("Unable to launch multiple instances with"
1745                         " a single configured port ID. Please launch your"
1746                         " instance one by one with different ports.")
1747                 raise exception.MultiplePortsNotApplicable(reason=msg)
1748 
1749     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1750         """Check whether multiple instances are created with specified ip."""
1751 
1752         for requested_net in requested_networks:
1753             if requested_net.network_id and requested_net.address:
1754                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1755                         "is specified.")
1756                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1757 
1758     @hooks.add_hook("create_instance")
1759     def create(self, context, instance_type,
1760                image_href, kernel_id=None, ramdisk_id=None,
1761                min_count=None, max_count=None,
1762                display_name=None, display_description=None,
1763                key_name=None, key_data=None, security_groups=None,
1764                availability_zone=None, forced_host=None, forced_node=None,
1765                user_data=None, metadata=None, injected_files=None,
1766                admin_password=None, block_device_mapping=None,
1767                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1768                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1769                legacy_bdm=True, shutdown_terminate=False,
1770                check_server_group_quota=False, tags=None,
1771                supports_multiattach=False, trusted_certs=None,
1772                supports_port_resource_request=False):
1773         """Provision instances, sending instance information to the
1774         scheduler.  The scheduler will determine where the instance(s)
1775         go and will handle creating the DB entries.
1776 
1777         Returns a tuple of (instances, reservation_id)
1778         """
1779         if requested_networks and max_count is not None and max_count > 1:
1780             self._check_multiple_instances_with_specified_ip(
1781                 requested_networks)
1782             if utils.is_neutron():
1783                 self._check_multiple_instances_with_neutron_ports(
1784                     requested_networks)
1785 
1786         if availability_zone:
1787             available_zones = availability_zones.\
1788                 get_availability_zones(context.elevated(), True)
1789             if forced_host is None and availability_zone not in \
1790                     available_zones:
1791                 msg = _('The requested availability zone is not available')
1792                 raise exception.InvalidRequest(msg)
1793 
1794         filter_properties = scheduler_utils.build_filter_properties(
1795                 scheduler_hints, forced_host, forced_node, instance_type)
1796 
1797         return self._create_instance(
1798             context, instance_type,
1799             image_href, kernel_id, ramdisk_id,
1800             min_count, max_count,
1801             display_name, display_description,
1802             key_name, key_data, security_groups,
1803             availability_zone, user_data, metadata,
1804             injected_files, admin_password,
1805             access_ip_v4, access_ip_v6,
1806             requested_networks, config_drive,
1807             block_device_mapping, auto_disk_config,
1808             filter_properties=filter_properties,
1809             legacy_bdm=legacy_bdm,
1810             shutdown_terminate=shutdown_terminate,
1811             check_server_group_quota=check_server_group_quota,
1812             tags=tags, supports_multiattach=supports_multiattach,
1813             trusted_certs=trusted_certs,
1814             supports_port_resource_request=supports_port_resource_request)
1815 
1816     def _check_auto_disk_config(self, instance=None, image=None,
1817                                 **extra_instance_updates):
1818         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1819         if auto_disk_config is None:
1820             return
1821         if not image and not instance:
1822             return
1823 
1824         if image:
1825             image_props = image.get("properties", {})
1826             auto_disk_config_img = \
1827                 utils.get_auto_disk_config_from_image_props(image_props)
1828             image_ref = image.get("id")
1829         else:
1830             sys_meta = utils.instance_sys_meta(instance)
1831             image_ref = sys_meta.get('image_base_image_ref')
1832             auto_disk_config_img = \
1833                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1834 
1835         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1836                                                auto_disk_config,
1837                                                image_ref)
1838 
1839     def _lookup_instance(self, context, uuid):
1840         '''Helper method for pulling an instance object from a database.
1841 
1842         During the transition to cellsv2 there is some complexity around
1843         retrieving an instance from the database which this method hides. If
1844         there is an instance mapping then query the cell for the instance, if
1845         no mapping exists then query the configured nova database.
1846 
1847         Once we are past the point that all deployments can be assumed to be
1848         migrated to cellsv2 this method can go away.
1849         '''
1850         inst_map = None
1851         try:
1852             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1853                 context, uuid)
1854         except exception.InstanceMappingNotFound:
1855             # TODO(alaski): This exception block can be removed once we're
1856             # guaranteed everyone is using cellsv2.
1857             pass
1858 
1859         if (inst_map is None or inst_map.cell_mapping is None or
1860                 CONF.cells.enable):
1861             # If inst_map is None then the deployment has not migrated to
1862             # cellsv2 yet.
1863             # If inst_map.cell_mapping is None then the instance is not in a
1864             # cell yet. Until instance creation moves to the conductor the
1865             # instance can be found in the configured database, so attempt
1866             # to look it up.
1867             # If we're on cellsv1, we can't yet short-circuit the cells
1868             # messaging path
1869             cell = None
1870             try:
1871                 instance = objects.Instance.get_by_uuid(context, uuid)
1872             except exception.InstanceNotFound:
1873                 # If we get here then the conductor is in charge of writing the
1874                 # instance to the database and hasn't done that yet. It's up to
1875                 # the caller of this method to determine what to do with that
1876                 # information.
1877                 return None, None
1878         else:
1879             cell = inst_map.cell_mapping
1880             with nova_context.target_cell(context, cell) as cctxt:
1881                 try:
1882                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1883                 except exception.InstanceNotFound:
1884                     # Since the cell_mapping exists we know the instance is in
1885                     # the cell, however InstanceNotFound means it's already
1886                     # deleted.
1887                     return None, None
1888         return cell, instance
1889 
1890     def _delete_while_booting(self, context, instance):
1891         """Handle deletion if the instance has not reached a cell yet
1892 
1893         Deletion before an instance reaches a cell needs to be handled
1894         differently. What we're attempting to do is delete the BuildRequest
1895         before the api level conductor does.  If we succeed here then the boot
1896         request stops before reaching a cell.  If not then the instance will
1897         need to be looked up in a cell db and the normal delete path taken.
1898         """
1899         deleted = self._attempt_delete_of_buildrequest(context, instance)
1900         if deleted:
1901             # If we've reached this block the successful deletion of the
1902             # buildrequest indicates that the build process should be halted by
1903             # the conductor.
1904 
1905             # NOTE(alaski): Though the conductor halts the build process it
1906             # does not currently delete the instance record. This is
1907             # because in the near future the instance record will not be
1908             # created if the buildrequest has been deleted here. For now we
1909             # ensure the instance has been set to deleted at this point.
1910             # Yes this directly contradicts the comment earlier in this
1911             # method, but this is a temporary measure.
1912             # Look up the instance because the current instance object was
1913             # stashed on the buildrequest and therefore not complete enough
1914             # to run .destroy().
1915             try:
1916                 instance_uuid = instance.uuid
1917                 cell, instance = self._lookup_instance(context, instance_uuid)
1918                 if instance is not None:
1919                     # If instance is None it has already been deleted.
1920                     if cell:
1921                         with nova_context.target_cell(context, cell) as cctxt:
1922                             # FIXME: When the instance context is targeted,
1923                             # we can remove this
1924                             with compute_utils.notify_about_instance_delete(
1925                                     self.notifier, cctxt, instance):
1926                                 instance.destroy()
1927                     else:
1928                         instance.destroy()
1929             except exception.InstanceNotFound:
1930                 pass
1931 
1932             return True
1933         return False
1934 
1935     def _attempt_delete_of_buildrequest(self, context, instance):
1936         # If there is a BuildRequest then the instance may not have been
1937         # written to a cell db yet. Delete the BuildRequest here, which
1938         # will indicate that the Instance build should not proceed.
1939         try:
1940             build_req = objects.BuildRequest.get_by_instance_uuid(
1941                 context, instance.uuid)
1942             build_req.destroy()
1943         except exception.BuildRequestNotFound:
1944             # This means that conductor has deleted the BuildRequest so the
1945             # instance is now in a cell and the delete needs to proceed
1946             # normally.
1947             return False
1948 
1949         # We need to detach from any volumes so they aren't orphaned.
1950         self._local_cleanup_bdm_volumes(
1951             build_req.block_device_mappings, instance, context)
1952 
1953         return True
1954 
1955     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1956         if instance.disable_terminate:
1957             LOG.info('instance termination disabled', instance=instance)
1958             return
1959 
1960         cell = None
1961         # If there is an instance.host (or the instance is shelved-offloaded or
1962         # in error state), the instance has been scheduled and sent to a
1963         # cell/compute which means it was pulled from the cell db.
1964         # Normal delete should be attempted.
1965         may_have_ports_or_volumes = compute_utils.may_have_ports_or_volumes(
1966             instance)
1967         if not instance.host and not may_have_ports_or_volumes:
1968             try:
1969                 if self._delete_while_booting(context, instance):
1970                     return
1971                 # If instance.host was not set it's possible that the Instance
1972                 # object here was pulled from a BuildRequest object and is not
1973                 # fully populated. Notably it will be missing an 'id' field
1974                 # which will prevent instance.destroy from functioning
1975                 # properly. A lookup is attempted which will either return a
1976                 # full Instance or None if not found. If not found then it's
1977                 # acceptable to skip the rest of the delete processing.
1978                 cell, instance = self._lookup_instance(context, instance.uuid)
1979                 if cell and instance:
1980                     try:
1981                         # Now destroy the instance from the cell it lives in.
1982                         with compute_utils.notify_about_instance_delete(
1983                                 self.notifier, context, instance):
1984                             instance.destroy()
1985                     except exception.InstanceNotFound:
1986                         pass
1987                     # The instance was deleted or is already gone.
1988                     return
1989                 if not instance:
1990                     # Instance is already deleted.
1991                     return
1992             except exception.ObjectActionError:
1993                 # NOTE(melwitt): This means the instance.host changed
1994                 # under us indicating the instance became scheduled
1995                 # during the destroy(). Refresh the instance from the DB and
1996                 # continue on with the delete logic for a scheduled instance.
1997                 # NOTE(danms): If instance.host is set, we should be able to
1998                 # do the following lookup. If not, there's not much we can
1999                 # do to recover.
2000                 cell, instance = self._lookup_instance(context, instance.uuid)
2001                 if not instance:
2002                     # Instance is already deleted
2003                     return
2004 
2005         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2006                 context, instance.uuid)
2007 
2008         # At these states an instance has a snapshot associate.
2009         if instance.vm_state in (vm_states.SHELVED,
2010                                  vm_states.SHELVED_OFFLOADED):
2011             snapshot_id = instance.system_metadata.get('shelved_image_id')
2012             LOG.info("Working on deleting snapshot %s "
2013                      "from shelved instance...",
2014                      snapshot_id, instance=instance)
2015             try:
2016                 self.image_api.delete(context, snapshot_id)
2017             except (exception.ImageNotFound,
2018                     exception.ImageNotAuthorized) as exc:
2019                 LOG.warning("Failed to delete snapshot "
2020                             "from shelved instance (%s).",
2021                             exc.format_message(), instance=instance)
2022             except Exception:
2023                 LOG.exception("Something wrong happened when trying to "
2024                               "delete snapshot from shelved instance.",
2025                               instance=instance)
2026 
2027         original_task_state = instance.task_state
2028         try:
2029             # NOTE(maoy): no expected_task_state needs to be set
2030             instance.update(instance_attrs)
2031             instance.progress = 0
2032             instance.save()
2033 
2034             # NOTE(dtp): cells.enable = False means "use cells v2".
2035             # Run everywhere except v1 compute cells.
2036             if (not CONF.cells.enable and CONF.workarounds.enable_consoleauth
2037                ) or self.cell_type == 'api':
2038                 # TODO(melwitt): Remove the conditions for running this line
2039                 # with cells v2, when consoleauth is no longer being used by
2040                 # cells v2, in Stein.
2041                 self.consoleauth_rpcapi.delete_tokens_for_instance(
2042                     context, instance.uuid)
2043 
2044             if self.cell_type == 'api':
2045                 # NOTE(comstud): If we're in the API cell, we need to
2046                 # skip all remaining logic and just call the callback,
2047                 # which will cause a cast to the child cell.
2048                 cb(context, instance, bdms)
2049                 return
2050             if not instance.host and not may_have_ports_or_volumes:
2051                 try:
2052                     with compute_utils.notify_about_instance_delete(
2053                             self.notifier, context, instance,
2054                             delete_type
2055                             if delete_type != 'soft_delete'
2056                             else 'delete'):
2057                         instance.destroy()
2058                     LOG.info('Instance deleted and does not have host '
2059                              'field, its vm_state is %(state)s.',
2060                              {'state': instance.vm_state},
2061                               instance=instance)
2062                     return
2063                 except exception.ObjectActionError as ex:
2064                     # The instance's host likely changed under us as
2065                     # this instance could be building and has since been
2066                     # scheduled. Continue with attempts to delete it.
2067                     LOG.debug('Refreshing instance because: %s', ex,
2068                               instance=instance)
2069                     instance.refresh()
2070 
2071             if instance.vm_state == vm_states.RESIZED:
2072                 self._confirm_resize_on_deleting(context, instance)
2073                 # NOTE(neha_alhat): After confirm resize vm_state will become
2074                 # 'active' and task_state will be set to 'None'. But for soft
2075                 # deleting a vm, the _do_soft_delete callback requires
2076                 # task_state in 'SOFT_DELETING' status. So, we need to set
2077                 # task_state as 'SOFT_DELETING' again for soft_delete case.
2078                 # After confirm resize and before saving the task_state to
2079                 # "SOFT_DELETING", during the short window, user can submit
2080                 # soft delete vm request again and system will accept and
2081                 # process it without any errors.
2082                 if delete_type == 'soft_delete':
2083                     instance.task_state = instance_attrs['task_state']
2084                     instance.save()
2085 
2086             is_local_delete = True
2087             try:
2088                 # instance.host must be set in order to look up the service.
2089                 if instance.host is not None:
2090                     service = objects.Service.get_by_compute_host(
2091                         context.elevated(), instance.host)
2092                     is_local_delete = not self.servicegroup_api.service_is_up(
2093                         service)
2094                 if not is_local_delete:
2095                     if original_task_state in (task_states.DELETING,
2096                                                   task_states.SOFT_DELETING):
2097                         LOG.info('Instance is already in deleting state, '
2098                                  'ignoring this request',
2099                                  instance=instance)
2100                         return
2101                     self._record_action_start(context, instance,
2102                                               instance_actions.DELETE)
2103 
2104                     cb(context, instance, bdms)
2105             except exception.ComputeHostNotFound:
2106                 LOG.debug('Compute host %s not found during service up check, '
2107                           'going to local delete instance', instance.host,
2108                           instance=instance)
2109 
2110             if is_local_delete:
2111                 # If instance is in shelved_offloaded state or compute node
2112                 # isn't up, delete instance from db and clean bdms info and
2113                 # network info
2114                 if cell is None:
2115                     # NOTE(danms): If we didn't get our cell from one of the
2116                     # paths above, look it up now.
2117                     try:
2118                         im = objects.InstanceMapping.get_by_instance_uuid(
2119                             context, instance.uuid)
2120                         cell = im.cell_mapping
2121                     except exception.InstanceMappingNotFound:
2122                         LOG.warning('During local delete, failed to find '
2123                                     'instance mapping', instance=instance)
2124                         return
2125 
2126                 LOG.debug('Doing local delete in cell %s', cell.identity,
2127                           instance=instance)
2128                 with nova_context.target_cell(context, cell) as cctxt:
2129                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
2130 
2131         except exception.InstanceNotFound:
2132             # NOTE(comstud): Race condition. Instance already gone.
2133             pass
2134 
2135     def _confirm_resize_on_deleting(self, context, instance):
2136         # If in the middle of a resize, use confirm_resize to
2137         # ensure the original instance is cleaned up too along
2138         # with its allocations (and migration-based allocations)
2139         # in placement.
2140         migration = None
2141         for status in ('finished', 'confirming'):
2142             try:
2143                 migration = objects.Migration.get_by_instance_and_status(
2144                         context.elevated(), instance.uuid, status)
2145                 LOG.info('Found an unconfirmed migration during delete, '
2146                          'id: %(id)s, status: %(status)s',
2147                          {'id': migration.id,
2148                           'status': migration.status},
2149                          instance=instance)
2150                 break
2151             except exception.MigrationNotFoundByStatus:
2152                 pass
2153 
2154         if not migration:
2155             LOG.info('Instance may have been confirmed during delete',
2156                      instance=instance)
2157             return
2158 
2159         src_host = migration.source_compute
2160 
2161         self._record_action_start(context, instance,
2162                                   instance_actions.CONFIRM_RESIZE)
2163 
2164         self.compute_rpcapi.confirm_resize(context,
2165                 instance, migration, src_host, cast=False)
2166 
2167     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
2168         """The method deletes the bdm records and, if a bdm is a volume, call
2169         the terminate connection and the detach volume via the Volume API.
2170         """
2171         elevated = context.elevated()
2172         for bdm in bdms:
2173             if bdm.is_volume:
2174                 try:
2175                     if bdm.attachment_id:
2176                         self.volume_api.attachment_delete(context,
2177                                                           bdm.attachment_id)
2178                     else:
2179                         connector = compute_utils.get_stashed_volume_connector(
2180                             bdm, instance)
2181                         if connector:
2182                             self.volume_api.terminate_connection(context,
2183                                                                  bdm.volume_id,
2184                                                                  connector)
2185                         else:
2186                             LOG.debug('Unable to find connector for volume %s,'
2187                                       ' not attempting terminate_connection.',
2188                                       bdm.volume_id, instance=instance)
2189                         # Attempt to detach the volume. If there was no
2190                         # connection made in the first place this is just
2191                         # cleaning up the volume state in the Cinder DB.
2192                         self.volume_api.detach(elevated, bdm.volume_id,
2193                                                instance.uuid)
2194 
2195                     if bdm.delete_on_termination:
2196                         self.volume_api.delete(context, bdm.volume_id)
2197                 except Exception as exc:
2198                     LOG.warning("Ignoring volume cleanup failure due to %s",
2199                                 exc, instance=instance)
2200             # If we're cleaning up volumes from an instance that wasn't yet
2201             # created in a cell, i.e. the user deleted the server while
2202             # the BuildRequest still existed, then the BDM doesn't actually
2203             # exist in the DB to destroy it.
2204             if 'id' in bdm:
2205                 bdm.destroy()
2206 
2207     @property
2208     def placementclient(self):
2209         if self._placementclient is None:
2210             self._placementclient = report.SchedulerReportClient()
2211         return self._placementclient
2212 
2213     def _local_delete(self, context, instance, bdms, delete_type, cb):
2214         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2215             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
2216                      " the instance's info from database.",
2217                      instance=instance)
2218         else:
2219             LOG.warning("instance's host %s is down, deleting from "
2220                         "database", instance.host, instance=instance)
2221         with compute_utils.notify_about_instance_delete(
2222                 self.notifier, context, instance,
2223                 delete_type if delete_type != 'soft_delete' else 'delete'):
2224 
2225             elevated = context.elevated()
2226             if self.cell_type != 'api':
2227                 # NOTE(liusheng): In nova-network multi_host scenario,deleting
2228                 # network info of the instance may need instance['host'] as
2229                 # destination host of RPC call. If instance in
2230                 # SHELVED_OFFLOADED state, instance['host'] is None, here, use
2231                 # shelved_host as host to deallocate network info and reset
2232                 # instance['host'] after that. Here we shouldn't use
2233                 # instance.save(), because this will mislead user who may think
2234                 # the instance's host has been changed, and actually, the
2235                 # instance.host is always None.
2236                 orig_host = instance.host
2237                 try:
2238                     if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2239                         sysmeta = getattr(instance,
2240                                           obj_base.get_attrname(
2241                                               'system_metadata'))
2242                         instance.host = sysmeta.get('shelved_host')
2243                     self.network_api.deallocate_for_instance(elevated,
2244                                                              instance)
2245                 finally:
2246                     instance.host = orig_host
2247 
2248             # cleanup volumes
2249             self._local_cleanup_bdm_volumes(bdms, instance, context)
2250             # Cleanup allocations in Placement since we can't do it from the
2251             # compute service.
2252             self.placementclient.delete_allocation_for_instance(
2253                 context, instance.uuid)
2254             cb(context, instance, bdms, local=True)
2255             instance.destroy()
2256 
2257     @staticmethod
2258     def _update_queued_for_deletion(context, instance, qfd):
2259         # NOTE(tssurya): We query the instance_mapping record of this instance
2260         # and update the queued_for_delete flag to True (or False according to
2261         # the state of the instance). This just means that the instance is
2262         # queued for deletion (or is no longer queued for deletion). It does
2263         # not guarantee its successful deletion (or restoration). Hence the
2264         # value could be stale which is fine, considering its use is only
2265         # during down cell (desperate) situation.
2266         im = objects.InstanceMapping.get_by_instance_uuid(context,
2267                                                           instance.uuid)
2268         im.queued_for_delete = qfd
2269         im.save()
2270 
2271     def _do_delete(self, context, instance, bdms, local=False):
2272         if local:
2273             instance.vm_state = vm_states.DELETED
2274             instance.task_state = None
2275             instance.terminated_at = timeutils.utcnow()
2276             instance.save()
2277         else:
2278             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2279                                                    delete_type='delete')
2280         self._update_queued_for_deletion(context, instance, True)
2281 
2282     def _do_force_delete(self, context, instance, bdms, local=False):
2283         if local:
2284             instance.vm_state = vm_states.DELETED
2285             instance.task_state = None
2286             instance.terminated_at = timeutils.utcnow()
2287             instance.save()
2288         else:
2289             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2290                                                    delete_type='force_delete')
2291         self._update_queued_for_deletion(context, instance, True)
2292 
2293     def _do_soft_delete(self, context, instance, bdms, local=False):
2294         if local:
2295             instance.vm_state = vm_states.SOFT_DELETED
2296             instance.task_state = None
2297             instance.terminated_at = timeutils.utcnow()
2298             instance.save()
2299         else:
2300             self.compute_rpcapi.soft_delete_instance(context, instance)
2301         self._update_queued_for_deletion(context, instance, True)
2302 
2303     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2304     @check_instance_lock
2305     @check_instance_cell
2306     @check_instance_state(vm_state=None, task_state=None,
2307                           must_have_launched=True)
2308     def soft_delete(self, context, instance):
2309         """Terminate an instance."""
2310         LOG.debug('Going to try to soft delete instance',
2311                   instance=instance)
2312 
2313         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2314                      task_state=task_states.SOFT_DELETING,
2315                      deleted_at=timeutils.utcnow())
2316 
2317     def _delete_instance(self, context, instance):
2318         self._delete(context, instance, 'delete', self._do_delete,
2319                      task_state=task_states.DELETING)
2320 
2321     @check_instance_lock
2322     @check_instance_cell
2323     @check_instance_state(vm_state=None, task_state=None,
2324                           must_have_launched=False)
2325     def delete(self, context, instance):
2326         """Terminate an instance."""
2327         LOG.debug("Going to try to terminate instance", instance=instance)
2328         self._delete_instance(context, instance)
2329 
2330     @check_instance_lock
2331     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2332     def restore(self, context, instance):
2333         """Restore a previously deleted (but not reclaimed) instance."""
2334         # Check quotas
2335         flavor = instance.get_flavor()
2336         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2337         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2338                 project_id=project_id, user_id=user_id)
2339 
2340         self._record_action_start(context, instance, instance_actions.RESTORE)
2341 
2342         if instance.host:
2343             instance.task_state = task_states.RESTORING
2344             instance.deleted_at = None
2345             instance.save(expected_task_state=[None])
2346             # TODO(melwitt): We're not rechecking for strict quota here to
2347             # guard against going over quota during a race at this time because
2348             # the resource consumption for this operation is written to the
2349             # database by compute.
2350             self.compute_rpcapi.restore_instance(context, instance)
2351         else:
2352             instance.vm_state = vm_states.ACTIVE
2353             instance.task_state = None
2354             instance.deleted_at = None
2355             instance.save(expected_task_state=[None])
2356         self._update_queued_for_deletion(context, instance, False)
2357 
2358     @check_instance_lock
2359     @check_instance_state(task_state=None,
2360                           must_have_launched=False)
2361     def force_delete(self, context, instance):
2362         """Force delete an instance in any vm_state/task_state."""
2363         self._delete(context, instance, 'force_delete', self._do_force_delete,
2364                      task_state=task_states.DELETING)
2365 
2366     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2367         LOG.debug("Going to try to stop instance", instance=instance)
2368 
2369         instance.task_state = task_states.POWERING_OFF
2370         instance.progress = 0
2371         instance.save(expected_task_state=[None])
2372 
2373         self._record_action_start(context, instance, instance_actions.STOP)
2374 
2375         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2376                                           clean_shutdown=clean_shutdown)
2377 
2378     @check_instance_lock
2379     @check_instance_host
2380     @check_instance_cell
2381     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2382     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2383         """Stop an instance."""
2384         self.force_stop(context, instance, do_cast, clean_shutdown)
2385 
2386     @check_instance_lock
2387     @check_instance_host
2388     @check_instance_cell
2389     @check_instance_state(vm_state=[vm_states.STOPPED])
2390     def start(self, context, instance):
2391         """Start an instance."""
2392         LOG.debug("Going to try to start instance", instance=instance)
2393 
2394         instance.task_state = task_states.POWERING_ON
2395         instance.save(expected_task_state=[None])
2396 
2397         self._record_action_start(context, instance, instance_actions.START)
2398         # TODO(yamahata): injected_files isn't supported right now.
2399         #                 It is used only for osapi. not for ec2 api.
2400         #                 availability_zone isn't used by run_instance.
2401         self.compute_rpcapi.start_instance(context, instance)
2402 
2403     @check_instance_lock
2404     @check_instance_host
2405     @check_instance_cell
2406     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2407     def trigger_crash_dump(self, context, instance):
2408         """Trigger crash dump in an instance."""
2409         LOG.debug("Try to trigger crash dump", instance=instance)
2410 
2411         self._record_action_start(context, instance,
2412                                   instance_actions.TRIGGER_CRASH_DUMP)
2413 
2414         self.compute_rpcapi.trigger_crash_dump(context, instance)
2415 
2416     def _generate_minimal_construct_for_down_cells(self, context,
2417                                                    down_cell_uuids,
2418                                                    project, limit):
2419         """Generate a list of minimal instance constructs for a given list of
2420         cells that did not respond to a list operation. This will list
2421         every instance mapping in the affected cells and return a minimal
2422         objects.Instance for each (non-queued-for-delete) mapping.
2423 
2424         :param context: RequestContext
2425         :param down_cell_uuids: A list of cell UUIDs that did not respond
2426         :param project: A project ID to filter mappings, or None
2427         :param limit: A numeric limit on the number of results, or None
2428         :returns: An InstanceList() of partial Instance() objects
2429         """
2430         unavailable_servers = objects.InstanceList()
2431         for cell_uuid in down_cell_uuids:
2432             LOG.warning("Cell %s is not responding and hence only "
2433                         "partial results are available from this "
2434                         "cell if any.", cell_uuid)
2435             instance_mappings = (objects.InstanceMappingList.
2436                 get_not_deleted_by_cell_and_project(context, cell_uuid,
2437                                                     project, limit=limit))
2438             for im in instance_mappings:
2439                 unavailable_servers.objects.append(
2440                     objects.Instance(
2441                         context=context,
2442                         uuid=im.instance_uuid,
2443                         project_id=im.project_id,
2444                         created_at=im.created_at
2445                     )
2446                 )
2447             if limit is not None:
2448                 limit -= len(instance_mappings)
2449                 if limit <= 0:
2450                     break
2451         return unavailable_servers
2452 
2453     def _get_instance_map_or_none(self, context, instance_uuid):
2454         try:
2455             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2456                     context, instance_uuid)
2457         except exception.InstanceMappingNotFound:
2458             # InstanceMapping should always be found generally. This exception
2459             # may be raised if a deployment has partially migrated the nova-api
2460             # services.
2461             inst_map = None
2462         return inst_map
2463 
2464     @staticmethod
2465     def _save_user_id_in_instance_mapping(mapping, instance):
2466         # TODO(melwitt): We take the opportunity to migrate user_id on the
2467         # instance mapping if it's not yet been migrated. This can be removed
2468         # in a future release, when all migrations are complete.
2469         # If the instance came from a RequestSpec because of a down cell, its
2470         # user_id could be None and the InstanceMapping.user_id field is
2471         # non-nullable. Avoid trying to set/save the user_id in that case.
2472         if 'user_id' not in mapping and instance.user_id is not None:
2473             mapping.user_id = instance.user_id
2474             mapping.save()
2475 
2476     def _get_instance_from_cell(self, context, im, expected_attrs,
2477                                 cell_down_support):
2478         # NOTE(danms): Even though we're going to scatter/gather to the
2479         # right cell, other code depends on this being force targeted when
2480         # the get call returns.
2481         nova_context.set_target_cell(context, im.cell_mapping)
2482 
2483         uuid = im.instance_uuid
2484         result = nova_context.scatter_gather_single_cell(context,
2485             im.cell_mapping, objects.Instance.get_by_uuid, uuid,
2486             expected_attrs=expected_attrs)
2487         cell_uuid = im.cell_mapping.uuid
2488         if not nova_context.is_cell_failure_sentinel(result[cell_uuid]):
2489             inst = result[cell_uuid]
2490             self._save_user_id_in_instance_mapping(im, inst)
2491             return inst
2492         elif isinstance(result[cell_uuid], exception.InstanceNotFound):
2493             raise exception.InstanceNotFound(instance_id=uuid)
2494         elif cell_down_support:
2495             if im.queued_for_delete:
2496                 # should be treated like deleted instance.
2497                 raise exception.InstanceNotFound(instance_id=uuid)
2498 
2499             # instance in down cell, return a minimal construct
2500             LOG.warning("Cell %s is not responding and hence only "
2501                         "partial results are available from this "
2502                         "cell.", cell_uuid)
2503             try:
2504                 rs = objects.RequestSpec.get_by_instance_uuid(context,
2505                                                               uuid)
2506                 # For BFV case, we could have rs.image but rs.image.id might
2507                 # still not be set. So we check the existence of both image
2508                 # and its id.
2509                 image_ref = (rs.image.id if rs.image and
2510                              'id' in rs.image else None)
2511                 inst = objects.Instance(context=context, power_state=0,
2512                                         uuid=uuid,
2513                                         project_id=im.project_id,
2514                                         created_at=im.created_at,
2515                                         user_id=rs.user_id,
2516                                         flavor=rs.flavor,
2517                                         image_ref=image_ref,
2518                                         availability_zone=rs.availability_zone)
2519                 self._save_user_id_in_instance_mapping(im, inst)
2520                 return inst
2521             except exception.RequestSpecNotFound:
2522                 # could be that a deleted instance whose request
2523                 # spec has been archived is being queried.
2524                 raise exception.InstanceNotFound(instance_id=uuid)
2525         else:
2526             raise exception.NovaException(
2527                 _("Cell %s is not responding and hence instance "
2528                   "info is not available.") % cell_uuid)
2529 
2530     def _get_instance(self, context, instance_uuid, expected_attrs,
2531                       cell_down_support=False):
2532         # If we're on cellsv1, we need to consult the top-level
2533         # merged replica instead of the cell directly.
2534         if CONF.cells.enable:
2535             return objects.Instance.get_by_uuid(context, instance_uuid,
2536                                                 expected_attrs=expected_attrs)
2537         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2538         if inst_map and (inst_map.cell_mapping is not None):
2539             instance = self._get_instance_from_cell(context, inst_map,
2540                 expected_attrs, cell_down_support)
2541         elif inst_map and (inst_map.cell_mapping is None):
2542             # This means the instance has not been scheduled and put in
2543             # a cell yet. For now it also may mean that the deployer
2544             # has not created their cell(s) yet.
2545             try:
2546                 build_req = objects.BuildRequest.get_by_instance_uuid(
2547                         context, instance_uuid)
2548                 instance = build_req.instance
2549             except exception.BuildRequestNotFound:
2550                 # Instance was mapped and the BuildRequest was deleted
2551                 # while fetching. Try again.
2552                 inst_map = self._get_instance_map_or_none(context,
2553                                                           instance_uuid)
2554                 if inst_map and (inst_map.cell_mapping is not None):
2555                     instance = self._get_instance_from_cell(context, inst_map,
2556                         expected_attrs, cell_down_support)
2557                 else:
2558                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2559         else:
2560             # If we got here, we don't have an instance mapping, but we aren't
2561             # sure why. The instance mapping might be missing because the
2562             # upgrade is incomplete (map_instances wasn't run). Or because the
2563             # instance was deleted and the DB was archived at which point the
2564             # mapping is deleted. The former case is bad, but because of the
2565             # latter case we can't really log any kind of warning/error here
2566             # since it might be normal.
2567             raise exception.InstanceNotFound(instance_id=instance_uuid)
2568 
2569         return instance
2570 
2571     def get(self, context, instance_id, expected_attrs=None,
2572             cell_down_support=False):
2573         """Get a single instance with the given instance_id.
2574 
2575         :param cell_down_support: True if the API (and caller) support
2576                                   returning a minimal instance
2577                                   construct if the relevant cell is
2578                                   down. If False, an error is raised
2579                                   since the instance cannot be retrieved
2580                                   due to the cell being down.
2581         """
2582         if not expected_attrs:
2583             expected_attrs = []
2584         expected_attrs.extend(['metadata', 'system_metadata',
2585                                'security_groups', 'info_cache'])
2586         # NOTE(ameade): we still need to support integer ids for ec2
2587         try:
2588             if uuidutils.is_uuid_like(instance_id):
2589                 LOG.debug("Fetching instance by UUID",
2590                            instance_uuid=instance_id)
2591 
2592                 instance = self._get_instance(context, instance_id,
2593                     expected_attrs, cell_down_support=cell_down_support)
2594             else:
2595                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2596                 raise exception.InstanceNotFound(instance_id=instance_id)
2597         except exception.InvalidID:
2598             LOG.debug("Invalid instance id %s", instance_id)
2599             raise exception.InstanceNotFound(instance_id=instance_id)
2600 
2601         return instance
2602 
2603     def get_all(self, context, search_opts=None, limit=None, marker=None,
2604                 expected_attrs=None, sort_keys=None, sort_dirs=None,
2605                 cell_down_support=False, all_tenants=False):
2606         """Get all instances filtered by one of the given parameters.
2607 
2608         If there is no filter and the context is an admin, it will retrieve
2609         all instances in the system.
2610 
2611         Deleted instances will be returned by default, unless there is a
2612         search option that says otherwise.
2613 
2614         The results will be sorted based on the list of sort keys in the
2615         'sort_keys' parameter (first value is primary sort key, second value is
2616         secondary sort ket, etc.). For each sort key, the associated sort
2617         direction is based on the list of sort directions in the 'sort_dirs'
2618         parameter.
2619 
2620         :param cell_down_support: True if the API (and caller) support
2621                                   returning a minimal instance
2622                                   construct if the relevant cell is
2623                                   down. If False, instances from
2624                                   unreachable cells will be omitted.
2625         :param all_tenants: True if the "all_tenants" filter was passed.
2626 
2627         """
2628         if search_opts is None:
2629             search_opts = {}
2630 
2631         LOG.debug("Searching by: %s", str(search_opts))
2632 
2633         # Fixups for the DB call
2634         filters = {}
2635 
2636         def _remap_flavor_filter(flavor_id):
2637             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2638             filters['instance_type_id'] = flavor.id
2639 
2640         def _remap_fixed_ip_filter(fixed_ip):
2641             # Turn fixed_ip into a regexp match. Since '.' matches
2642             # any character, we need to use regexp escaping for it.
2643             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2644 
2645         # search_option to filter_name mapping.
2646         filter_mapping = {
2647                 'image': 'image_ref',
2648                 'name': 'display_name',
2649                 'tenant_id': 'project_id',
2650                 'flavor': _remap_flavor_filter,
2651                 'fixed_ip': _remap_fixed_ip_filter}
2652 
2653         # copy from search_opts, doing various remappings as necessary
2654         for opt, value in search_opts.items():
2655             # Do remappings.
2656             # Values not in the filter_mapping table are copied as-is.
2657             # If remapping is None, option is not copied
2658             # If the remapping is a string, it is the filter_name to use
2659             try:
2660                 remap_object = filter_mapping[opt]
2661             except KeyError:
2662                 filters[opt] = value
2663             else:
2664                 # Remaps are strings to translate to, or functions to call
2665                 # to do the translating as defined by the table above.
2666                 if isinstance(remap_object, six.string_types):
2667                     filters[remap_object] = value
2668                 else:
2669                     try:
2670                         remap_object(value)
2671 
2672                     # We already know we can't match the filter, so
2673                     # return an empty list
2674                     except ValueError:
2675                         return objects.InstanceList()
2676 
2677         # IP address filtering cannot be applied at the DB layer, remove any DB
2678         # limit so that it can be applied after the IP filter.
2679         filter_ip = 'ip6' in filters or 'ip' in filters
2680         skip_build_request = False
2681         orig_limit = limit
2682         if filter_ip:
2683             # We cannot skip build requests if there is a marker since the
2684             # the marker could be a build request.
2685             skip_build_request = marker is None
2686             if self.network_api.has_substr_port_filtering_extension(context):
2687                 # We're going to filter by IP using Neutron so set filter_ip
2688                 # to False so we don't attempt post-DB query filtering in
2689                 # memory below.
2690                 filter_ip = False
2691                 instance_uuids = self._ip_filter_using_neutron(context,
2692                                                                filters)
2693                 if instance_uuids:
2694                     # Note that 'uuid' is not in the 2.1 GET /servers query
2695                     # parameter schema, however, we allow additionalProperties
2696                     # so someone could filter instances by uuid, which doesn't
2697                     # make a lot of sense but we have to account for it.
2698                     if 'uuid' in filters and filters['uuid']:
2699                         filter_uuids = filters['uuid']
2700                         if isinstance(filter_uuids, list):
2701                             instance_uuids.extend(filter_uuids)
2702                         else:
2703                             # Assume a string. If it's a dict or tuple or
2704                             # something, well...that's too bad. This is why
2705                             # we have query parameter schema definitions.
2706                             if filter_uuids not in instance_uuids:
2707                                 instance_uuids.append(filter_uuids)
2708                     filters['uuid'] = instance_uuids
2709                 else:
2710                     # No matches on the ip filter(s), return an empty list.
2711                     return objects.InstanceList()
2712             elif limit:
2713                 LOG.debug('Removing limit for DB query due to IP filter')
2714                 limit = None
2715 
2716         # Skip get BuildRequest if filtering by IP address, as building
2717         # instances will not have IP addresses.
2718         if skip_build_request:
2719             build_requests = objects.BuildRequestList()
2720         else:
2721             # The ordering of instances will be
2722             # [sorted instances with no host] + [sorted instances with host].
2723             # This means BuildRequest and cell0 instances first, then cell
2724             # instances
2725             try:
2726                 build_requests = objects.BuildRequestList.get_by_filters(
2727                     context, filters, limit=limit, marker=marker,
2728                     sort_keys=sort_keys, sort_dirs=sort_dirs)
2729                 # If we found the marker in we need to set it to None
2730                 # so we don't expect to find it in the cells below.
2731                 marker = None
2732             except exception.MarkerNotFound:
2733                 # If we didn't find the marker in the build requests then keep
2734                 # looking for it in the cells.
2735                 build_requests = objects.BuildRequestList()
2736 
2737         build_req_instances = objects.InstanceList(
2738             objects=[build_req.instance for build_req in build_requests])
2739         # Only subtract from limit if it is not None
2740         limit = (limit - len(build_req_instances)) if limit else limit
2741 
2742         # We could arguably avoid joining on security_groups if we're using
2743         # neutron (which is the default) but if you're using neutron then the
2744         # security_group_instance_association table should be empty anyway
2745         # and the DB should optimize out that join, making it insignificant.
2746         fields = ['metadata', 'info_cache', 'security_groups']
2747         if expected_attrs:
2748             fields.extend(expected_attrs)
2749 
2750         if CONF.cells.enable:
2751             insts = self._do_old_style_instance_list_for_poor_cellsv1_users(
2752                 context, filters, limit, marker, fields, sort_keys,
2753                 sort_dirs)
2754         else:
2755             insts, down_cell_uuids = instance_list.get_instance_objects_sorted(
2756                 context, filters, limit, marker, fields, sort_keys, sort_dirs,
2757                 cell_down_support=cell_down_support)
2758 
2759         def _get_unique_filter_method():
2760             seen_uuids = set()
2761 
2762             def _filter(instance):
2763                 # During a cross-cell move operation we could have the instance
2764                 # in more than one cell database so we not only have to filter
2765                 # duplicates but we want to make sure we only return the
2766                 # "current" one which should also be the one that the instance
2767                 # mapping points to, but we don't want to do that expensive
2768                 # lookup here.
2769                 # NOTE(mriedem): We could make this better in the case that we
2770                 # have duplicate instances that are both hidden=False by
2771                 # showing the one with the newer updated_at value, but that
2772                 # could be tricky if the user is filtering on
2773                 # changes-since/before or updated_at, or sorting on updated_at,
2774                 # but technically that was already potentially broken with this
2775                 # _filter method if we return an older BuildRequest.instance,
2776                 # and given the window should be very small where we have
2777                 # duplicates, it's probably not worth the complexity.
2778                 if instance.uuid in seen_uuids or instance.hidden:
2779                     return False
2780                 seen_uuids.add(instance.uuid)
2781                 return True
2782 
2783             return _filter
2784 
2785         filter_method = _get_unique_filter_method()
2786         # Only subtract from limit if it is not None
2787         limit = (limit - len(insts)) if limit else limit
2788         # TODO(alaski): Clean up the objects concatenation when List objects
2789         # support it natively.
2790         instances = objects.InstanceList(
2791             objects=list(filter(filter_method,
2792                            build_req_instances.objects +
2793                            insts.objects)))
2794 
2795         if filter_ip:
2796             instances = self._ip_filter(instances, filters, orig_limit)
2797 
2798         if cell_down_support:
2799             # API and client want minimal construct instances for any cells
2800             # that didn't return, so generate and prefix those to the actual
2801             # results.
2802             project = search_opts.get('project_id', context.project_id)
2803             if all_tenants:
2804                 # NOTE(tssurya): The only scenario where project has to be None
2805                 # is when using "all_tenants" in which case we do not want
2806                 # the query to be restricted based on the project_id.
2807                 project = None
2808             limit = (orig_limit - len(instances)) if limit else limit
2809             return (self._generate_minimal_construct_for_down_cells(context,
2810                 down_cell_uuids, project, limit) + instances)
2811 
2812         return instances
2813 
2814     def _do_old_style_instance_list_for_poor_cellsv1_users(self,
2815                                                            context, filters,
2816                                                            limit, marker,
2817                                                            fields,
2818                                                            sort_keys,
2819                                                            sort_dirs):
2820         try:
2821             cell0_mapping = objects.CellMapping.get_by_uuid(context,
2822                 objects.CellMapping.CELL0_UUID)
2823         except exception.CellMappingNotFound:
2824             cell0_instances = objects.InstanceList(objects=[])
2825         else:
2826             with nova_context.target_cell(context, cell0_mapping) as cctxt:
2827                 try:
2828                     cell0_instances = self._get_instances_by_filters(
2829                         cctxt, filters, limit=limit, marker=marker,
2830                         fields=fields, sort_keys=sort_keys,
2831                         sort_dirs=sort_dirs)
2832                     # If we found the marker in cell0 we need to set it to None
2833                     # so we don't expect to find it in the cells below.
2834                     marker = None
2835                 except exception.MarkerNotFound:
2836                     # We can ignore this since we need to look in the cell DB
2837                     cell0_instances = objects.InstanceList(objects=[])
2838         # Only subtract from limit if it is not None
2839         limit = (limit - len(cell0_instances)) if limit else limit
2840 
2841         # There is only planned support for a single cell here. Multiple cell
2842         # instance lists should be proxied to project Searchlight, or a similar
2843         # alternative.
2844         if limit is None or limit > 0:
2845             # NOTE(melwitt): If we're on cells v1, we need to read
2846             # instances from the top-level database because reading from
2847             # cells results in changed behavior, because of the syncing.
2848             # We can remove this path once we stop supporting cells v1.
2849             cell_instances = self._get_instances_by_filters(
2850                 context, filters, limit=limit, marker=marker,
2851                 fields=fields, sort_keys=sort_keys,
2852                 sort_dirs=sort_dirs)
2853         else:
2854             LOG.debug('Limit excludes any results from real cells')
2855             cell_instances = objects.InstanceList(objects=[])
2856 
2857         return cell0_instances + cell_instances
2858 
2859     @staticmethod
2860     def _ip_filter(inst_models, filters, limit):
2861         ipv4_f = re.compile(str(filters.get('ip')))
2862         ipv6_f = re.compile(str(filters.get('ip6')))
2863 
2864         def _match_instance(instance):
2865             nw_info = instance.get_network_info()
2866             for vif in nw_info:
2867                 for fixed_ip in vif.fixed_ips():
2868                     address = fixed_ip.get('address')
2869                     if not address:
2870                         continue
2871                     version = fixed_ip.get('version')
2872                     if ((version == 4 and ipv4_f.match(address)) or
2873                         (version == 6 and ipv6_f.match(address))):
2874                         return True
2875             return False
2876 
2877         result_objs = []
2878         for instance in inst_models:
2879             if _match_instance(instance):
2880                 result_objs.append(instance)
2881                 if limit and len(result_objs) == limit:
2882                     break
2883         return objects.InstanceList(objects=result_objs)
2884 
2885     def _ip_filter_using_neutron(self, context, filters):
2886         ip4_address = filters.get('ip')
2887         ip6_address = filters.get('ip6')
2888         addresses = [ip4_address, ip6_address]
2889         uuids = []
2890         for address in addresses:
2891             if address:
2892                 try:
2893                     ports = self.network_api.list_ports(
2894                         context, fixed_ips='ip_address_substr=' + address,
2895                         fields=['device_id'])['ports']
2896                     for port in ports:
2897                         uuids.append(port['device_id'])
2898                 except Exception as e:
2899                     LOG.error('An error occurred while listing ports '
2900                               'with an ip_address filter value of "%s". '
2901                               'Error: %s',
2902                               address, six.text_type(e))
2903         return uuids
2904 
2905     def _get_instances_by_filters(self, context, filters,
2906                                   limit=None, marker=None, fields=None,
2907                                   sort_keys=None, sort_dirs=None):
2908         return objects.InstanceList.get_by_filters(
2909             context, filters=filters, limit=limit, marker=marker,
2910             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2911 
2912     def update_instance(self, context, instance, updates):
2913         """Updates a single Instance object with some updates dict.
2914 
2915         Returns the updated instance.
2916         """
2917 
2918         # NOTE(sbauza): Given we only persist the Instance object after we
2919         # create the BuildRequest, we are sure that if the Instance object
2920         # has an ID field set, then it was persisted in the right Cell DB.
2921         if instance.obj_attr_is_set('id'):
2922             instance.update(updates)
2923             # Instance has been scheduled and the BuildRequest has been deleted
2924             # we can directly write the update down to the right cell.
2925             inst_map = self._get_instance_map_or_none(context, instance.uuid)
2926             # If we have a cell_mapping and we're not on cells v1, then
2927             # look up the instance in the cell database
2928             if inst_map and (inst_map.cell_mapping is not None) and (
2929                     not CONF.cells.enable):
2930                 with nova_context.target_cell(context,
2931                                               inst_map.cell_mapping) as cctxt:
2932                     with instance.obj_alternate_context(cctxt):
2933                         instance.save()
2934             else:
2935                 # If inst_map.cell_mapping does not point at a cell then cell
2936                 # migration has not happened yet.
2937                 # TODO(alaski): Make this a failure case after we put in
2938                 # a block that requires migrating to cellsv2.
2939                 instance.save()
2940         else:
2941             # Instance is not yet mapped to a cell, so we need to update
2942             # BuildRequest instead
2943             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2944             # could be deleted because of either a concurrent instance delete
2945             # or because the scheduler just returned a destination right
2946             # after we called the instance in the API.
2947             try:
2948                 build_req = objects.BuildRequest.get_by_instance_uuid(
2949                     context, instance.uuid)
2950                 instance = build_req.instance
2951                 instance.update(updates)
2952                 # FIXME(sbauza): Here we are updating the current
2953                 # thread-related BuildRequest object. Given that another worker
2954                 # could have looking up at that BuildRequest in the API, it
2955                 # means that it could pass it down to the conductor without
2956                 # making sure that it's not updated, we could have some race
2957                 # condition where it would missing the updated fields, but
2958                 # that's something we could discuss once the instance record
2959                 # is persisted by the conductor.
2960                 build_req.save()
2961             except exception.BuildRequestNotFound:
2962                 # Instance was mapped and the BuildRequest was deleted
2963                 # while fetching (and possibly the instance could have been
2964                 # deleted as well). We need to lookup again the Instance object
2965                 # in order to correctly update it.
2966                 # TODO(sbauza): Figure out a good way to know the expected
2967                 # attributes by checking which fields are set or not.
2968                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2969                                   'tags', 'metadata', 'system_metadata',
2970                                   'security_groups', 'info_cache']
2971                 inst_map = self._get_instance_map_or_none(context,
2972                                                           instance.uuid)
2973                 if inst_map and (inst_map.cell_mapping is not None):
2974                     with nova_context.target_cell(
2975                             context,
2976                             inst_map.cell_mapping) as cctxt:
2977                         instance = objects.Instance.get_by_uuid(
2978                             cctxt, instance.uuid,
2979                             expected_attrs=expected_attrs)
2980                         instance.update(updates)
2981                         instance.save()
2982                 else:
2983                     # If inst_map.cell_mapping does not point at a cell then
2984                     # cell migration has not happened yet.
2985                     # TODO(alaski): Make this a failure case after we put in
2986                     # a block that requires migrating to cellsv2.
2987                     instance = objects.Instance.get_by_uuid(
2988                         context, instance.uuid, expected_attrs=expected_attrs)
2989                     instance.update(updates)
2990                     instance.save()
2991         return instance
2992 
2993     # NOTE(melwitt): We don't check instance lock for backup because lock is
2994     #                intended to prevent accidental change/delete of instances
2995     @check_instance_cell
2996     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2997                                     vm_states.PAUSED, vm_states.SUSPENDED])
2998     def backup(self, context, instance, name, backup_type, rotation,
2999                extra_properties=None):
3000         """Backup the given instance
3001 
3002         :param instance: nova.objects.instance.Instance object
3003         :param name: name of the backup
3004         :param backup_type: 'daily' or 'weekly'
3005         :param rotation: int representing how many backups to keep around;
3006             None if rotation shouldn't be used (as in the case of snapshots)
3007         :param extra_properties: dict of extra image properties to include
3008                                  when creating the image.
3009         :returns: A dict containing image metadata
3010         """
3011         props_copy = dict(extra_properties, backup_type=backup_type)
3012 
3013         if compute_utils.is_volume_backed_instance(context, instance):
3014             LOG.info("It's not supported to backup volume backed "
3015                      "instance.", instance=instance)
3016             raise exception.InvalidRequest(
3017                 _('Backup is not supported for volume-backed instances.'))
3018         else:
3019             image_meta = compute_utils.create_image(
3020                 context, instance, name, 'backup', self.image_api,
3021                 extra_properties=props_copy)
3022 
3023         # NOTE(comstud): Any changes to this method should also be made
3024         # to the backup_instance() method in nova/cells/messaging.py
3025 
3026         instance.task_state = task_states.IMAGE_BACKUP
3027         instance.save(expected_task_state=[None])
3028 
3029         self._record_action_start(context, instance,
3030                                   instance_actions.BACKUP)
3031 
3032         self.compute_rpcapi.backup_instance(context, instance,
3033                                             image_meta['id'],
3034                                             backup_type,
3035                                             rotation)
3036         return image_meta
3037 
3038     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3039     #                intended to prevent accidental change/delete of instances
3040     @check_instance_cell
3041     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3042                                     vm_states.PAUSED, vm_states.SUSPENDED])
3043     def snapshot(self, context, instance, name, extra_properties=None):
3044         """Snapshot the given instance.
3045 
3046         :param instance: nova.objects.instance.Instance object
3047         :param name: name of the snapshot
3048         :param extra_properties: dict of extra image properties to include
3049                                  when creating the image.
3050         :returns: A dict containing image metadata
3051         """
3052         image_meta = compute_utils.create_image(
3053             context, instance, name, 'snapshot', self.image_api,
3054             extra_properties=extra_properties)
3055 
3056         # NOTE(comstud): Any changes to this method should also be made
3057         # to the snapshot_instance() method in nova/cells/messaging.py
3058         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
3059         try:
3060             instance.save(expected_task_state=[None])
3061         except (exception.InstanceNotFound,
3062                 exception.UnexpectedDeletingTaskStateError) as ex:
3063             # Changing the instance task state to use in raising the
3064             # InstanceInvalidException below
3065             LOG.debug('Instance disappeared during snapshot.',
3066                       instance=instance)
3067             try:
3068                 image_id = image_meta['id']
3069                 self.image_api.delete(context, image_id)
3070                 LOG.info('Image %s deleted because instance '
3071                          'deleted before snapshot started.',
3072                          image_id, instance=instance)
3073             except exception.ImageNotFound:
3074                 pass
3075             except Exception as exc:
3076                 LOG.warning("Error while trying to clean up image %(img_id)s: "
3077                             "%(error_msg)s",
3078                             {"img_id": image_meta['id'],
3079                              "error_msg": six.text_type(exc)})
3080             attr = 'task_state'
3081             state = task_states.DELETING
3082             if type(ex) == exception.InstanceNotFound:
3083                 attr = 'vm_state'
3084                 state = vm_states.DELETED
3085             raise exception.InstanceInvalidState(attr=attr,
3086                                            instance_uuid=instance.uuid,
3087                                            state=state,
3088                                            method='snapshot')
3089 
3090         self._record_action_start(context, instance,
3091                                   instance_actions.CREATE_IMAGE)
3092 
3093         self.compute_rpcapi.snapshot_instance(context, instance,
3094                                               image_meta['id'])
3095 
3096         return image_meta
3097 
3098     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
3099     #                intended to prevent accidental change/delete of instances
3100     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3101                                     vm_states.SUSPENDED])
3102     def snapshot_volume_backed(self, context, instance, name,
3103                                extra_properties=None):
3104         """Snapshot the given volume-backed instance.
3105 
3106         :param instance: nova.objects.instance.Instance object
3107         :param name: name of the backup or snapshot
3108         :param extra_properties: dict of extra image properties to include
3109 
3110         :returns: the new image metadata
3111         """
3112         image_meta = compute_utils.initialize_instance_snapshot_metadata(
3113             context, instance, name, extra_properties)
3114         # the new image is simply a bucket of properties (particularly the
3115         # block device mapping, kernel and ramdisk IDs) with no image data,
3116         # hence the zero size
3117         image_meta['size'] = 0
3118         for attr in ('container_format', 'disk_format'):
3119             image_meta.pop(attr, None)
3120         properties = image_meta['properties']
3121         # clean properties before filling
3122         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
3123             properties.pop(key, None)
3124         if instance.root_device_name:
3125             properties['root_device_name'] = instance.root_device_name
3126 
3127         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3128                 context, instance.uuid)
3129 
3130         mapping = []  # list of BDM dicts that can go into the image properties
3131         # Do some up-front filtering of the list of BDMs from
3132         # which we are going to create snapshots.
3133         volume_bdms = []
3134         for bdm in bdms:
3135             if bdm.no_device:
3136                 continue
3137             if bdm.is_volume:
3138                 # These will be handled below.
3139                 volume_bdms.append(bdm)
3140             else:
3141                 mapping.append(bdm.get_image_mapping())
3142 
3143         # Check limits in Cinder before creating snapshots to avoid going over
3144         # quota in the middle of a list of volumes. This is a best-effort check
3145         # but concurrently running snapshot requests from the same project
3146         # could still fail to create volume snapshots if they go over limit.
3147         if volume_bdms:
3148             limits = self.volume_api.get_absolute_limits(context)
3149             total_snapshots_used = limits['totalSnapshotsUsed']
3150             max_snapshots = limits['maxTotalSnapshots']
3151             # -1 means there is unlimited quota for snapshots
3152             if (max_snapshots > -1 and
3153                     len(volume_bdms) + total_snapshots_used > max_snapshots):
3154                 LOG.debug('Unable to create volume snapshots for instance. '
3155                           'Currently has %s snapshots, requesting %s new '
3156                           'snapshots, with a limit of %s.',
3157                           total_snapshots_used, len(volume_bdms),
3158                           max_snapshots, instance=instance)
3159                 raise exception.OverQuota(overs='snapshots')
3160 
3161         quiesced = False
3162         if instance.vm_state == vm_states.ACTIVE:
3163             try:
3164                 LOG.info("Attempting to quiesce instance before volume "
3165                          "snapshot.", instance=instance)
3166                 self.compute_rpcapi.quiesce_instance(context, instance)
3167                 quiesced = True
3168             except (exception.InstanceQuiesceNotSupported,
3169                     exception.QemuGuestAgentNotEnabled,
3170                     exception.NovaException, NotImplementedError) as err:
3171                 if strutils.bool_from_string(instance.system_metadata.get(
3172                         'image_os_require_quiesce')):
3173                     raise
3174                 else:
3175                     LOG.info('Skipping quiescing instance: %(reason)s.',
3176                              {'reason': err},
3177                              instance=instance)
3178             # NOTE(tasker): discovered that an uncaught exception could occur
3179             #               after the instance has been frozen. catch and thaw.
3180             except Exception as ex:
3181                 with excutils.save_and_reraise_exception():
3182                     LOG.error("An error occurred during quiesce of instance. "
3183                               "Unquiescing to ensure instance is thawed. "
3184                               "Error: %s", six.text_type(ex),
3185                               instance=instance)
3186                     self.compute_rpcapi.unquiesce_instance(context, instance,
3187                                                            mapping=None)
3188 
3189         @wrap_instance_event(prefix='api')
3190         def snapshot_instance(self, context, instance, bdms):
3191             try:
3192                 for bdm in volume_bdms:
3193                     # create snapshot based on volume_id
3194                     volume = self.volume_api.get(context, bdm.volume_id)
3195                     # NOTE(yamahata): Should we wait for snapshot creation?
3196                     #                 Linux LVM snapshot creation completes in
3197                     #                 short time, it doesn't matter for now.
3198                     name = _('snapshot for %s') % image_meta['name']
3199                     LOG.debug('Creating snapshot from volume %s.',
3200                               volume['id'], instance=instance)
3201                     snapshot = self.volume_api.create_snapshot_force(
3202                         context, volume['id'],
3203                         name, volume['display_description'])
3204                     mapping_dict = block_device.snapshot_from_bdm(
3205                         snapshot['id'], bdm)
3206                     mapping_dict = mapping_dict.get_image_mapping()
3207                     mapping.append(mapping_dict)
3208                 return mapping
3209             # NOTE(tasker): No error handling is done in the above for loop.
3210             # This means that if the snapshot fails and throws an exception
3211             # the traceback will skip right over the unquiesce needed below.
3212             # Here, catch any exception, unquiesce the instance, and raise the
3213             # error so that the calling function can do what it needs to in
3214             # order to properly treat a failed snap.
3215             except Exception:
3216                 with excutils.save_and_reraise_exception():
3217                     if quiesced:
3218                         LOG.info("Unquiescing instance after volume snapshot "
3219                                  "failure.", instance=instance)
3220                         self.compute_rpcapi.unquiesce_instance(
3221                             context, instance, mapping)
3222 
3223         self._record_action_start(context, instance,
3224                                   instance_actions.CREATE_IMAGE)
3225         mapping = snapshot_instance(self, context, instance, bdms)
3226 
3227         if quiesced:
3228             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
3229 
3230         if mapping:
3231             properties['block_device_mapping'] = mapping
3232             properties['bdm_v2'] = True
3233 
3234         return self.image_api.create(context, image_meta)
3235 
3236     @check_instance_lock
3237     def reboot(self, context, instance, reboot_type):
3238         """Reboot the given instance."""
3239         if reboot_type == 'SOFT':
3240             self._soft_reboot(context, instance)
3241         else:
3242             self._hard_reboot(context, instance)
3243 
3244     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
3245                           task_state=[None])
3246     def _soft_reboot(self, context, instance):
3247         expected_task_state = [None]
3248         instance.task_state = task_states.REBOOTING
3249         instance.save(expected_task_state=expected_task_state)
3250 
3251         self._record_action_start(context, instance, instance_actions.REBOOT)
3252 
3253         self.compute_rpcapi.reboot_instance(context, instance=instance,
3254                                             block_device_info=None,
3255                                             reboot_type='SOFT')
3256 
3257     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
3258                           task_state=task_states.ALLOW_REBOOT)
3259     def _hard_reboot(self, context, instance):
3260         instance.task_state = task_states.REBOOTING_HARD
3261         instance.save(expected_task_state=task_states.ALLOW_REBOOT)
3262 
3263         self._record_action_start(context, instance, instance_actions.REBOOT)
3264 
3265         self.compute_rpcapi.reboot_instance(context, instance=instance,
3266                                             block_device_info=None,
3267                                             reboot_type='HARD')
3268 
3269     @check_instance_lock
3270     @check_instance_cell
3271     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3272                                     vm_states.ERROR])
3273     def rebuild(self, context, instance, image_href, admin_password,
3274                 files_to_inject=None, **kwargs):
3275         """Rebuild the given instance with the provided attributes."""
3276         files_to_inject = files_to_inject or []
3277         metadata = kwargs.get('metadata', {})
3278         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
3279         auto_disk_config = kwargs.get('auto_disk_config')
3280 
3281         if 'key_name' in kwargs:
3282             key_name = kwargs.pop('key_name')
3283             if key_name:
3284                 # NOTE(liuyulong): we are intentionally using the user_id from
3285                 # the request context rather than the instance.user_id because
3286                 # users own keys but instances are owned by projects, and
3287                 # another user in the same project can rebuild an instance
3288                 # even if they didn't create it.
3289                 key_pair = objects.KeyPair.get_by_name(context,
3290                                                        context.user_id,
3291                                                        key_name)
3292                 instance.key_name = key_pair.name
3293                 instance.key_data = key_pair.public_key
3294                 instance.keypairs = objects.KeyPairList(objects=[key_pair])
3295             else:
3296                 instance.key_name = None
3297                 instance.key_data = None
3298                 instance.keypairs = objects.KeyPairList(objects=[])
3299 
3300         # Use trusted_certs value from kwargs to create TrustedCerts object
3301         trusted_certs = None
3302         if 'trusted_certs' in kwargs:
3303             # Note that the user can set, change, or unset / reset trusted
3304             # certs. If they are explicitly specifying
3305             # trusted_image_certificates=None, that means we'll either unset
3306             # them on the instance *or* reset to use the defaults (if defaults
3307             # are configured).
3308             trusted_certs = kwargs.pop('trusted_certs')
3309             instance.trusted_certs = self._retrieve_trusted_certs_object(
3310                 context, trusted_certs, rebuild=True)
3311 
3312         image_id, image = self._get_image(context, image_href)
3313         self._check_auto_disk_config(image=image, **kwargs)
3314 
3315         flavor = instance.get_flavor()
3316         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3317             context, instance.uuid)
3318         root_bdm = compute_utils.get_root_bdm(context, instance, bdms)
3319 
3320         # Check to see if the image is changing and we have a volume-backed
3321         # server. The compute doesn't support changing the image in the
3322         # root disk of a volume-backed server, so we need to just fail fast.
3323         is_volume_backed = compute_utils.is_volume_backed_instance(
3324             context, instance, bdms)
3325         if is_volume_backed:
3326             if trusted_certs:
3327                 # The only way we can get here is if the user tried to set
3328                 # trusted certs or specified trusted_image_certificates=None
3329                 # and default_trusted_certificate_ids is configured.
3330                 msg = _("Image certificate validation is not supported "
3331                         "for volume-backed servers.")
3332                 raise exception.CertificateValidationFailed(message=msg)
3333 
3334             # For boot from volume, instance.image_ref is empty, so we need to
3335             # query the image from the volume.
3336             if root_bdm is None:
3337                 # This shouldn't happen and is an error, we need to fail. This
3338                 # is not the users fault, it's an internal error. Without a
3339                 # root BDM we have no way of knowing the backing volume (or
3340                 # image in that volume) for this instance.
3341                 raise exception.NovaException(
3342                     _('Unable to find root block device mapping for '
3343                       'volume-backed instance.'))
3344 
3345             volume = self.volume_api.get(context, root_bdm.volume_id)
3346             volume_image_metadata = volume.get('volume_image_metadata', {})
3347             orig_image_ref = volume_image_metadata.get('image_id')
3348 
3349             if orig_image_ref != image_href:
3350                 # Leave a breadcrumb.
3351                 LOG.debug('Requested to rebuild instance with a new image %s '
3352                           'for a volume-backed server with image %s in its '
3353                           'root volume which is not supported.', image_href,
3354                           orig_image_ref, instance=instance)
3355                 msg = _('Unable to rebuild with a different image for a '
3356                         'volume-backed server.')
3357                 raise exception.ImageUnacceptable(
3358                     image_id=image_href, reason=msg)
3359         else:
3360             orig_image_ref = instance.image_ref
3361 
3362         request_spec = objects.RequestSpec.get_by_instance_uuid(
3363             context, instance.uuid)
3364 
3365         self._checks_for_create_and_rebuild(context, image_id, image,
3366                 flavor, metadata, files_to_inject, root_bdm)
3367 
3368         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
3369                 context, None, None, image)
3370 
3371         def _reset_image_metadata():
3372             """Remove old image properties that we're storing as instance
3373             system metadata.  These properties start with 'image_'.
3374             Then add the properties for the new image.
3375             """
3376             # FIXME(comstud): There's a race condition here in that if
3377             # the system_metadata for this instance is updated after
3378             # we do the previous save() and before we update.. those
3379             # other updates will be lost. Since this problem exists in
3380             # a lot of other places, I think it should be addressed in
3381             # a DB layer overhaul.
3382 
3383             orig_sys_metadata = dict(instance.system_metadata)
3384             # Remove the old keys
3385             for key in list(instance.system_metadata.keys()):
3386                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
3387                     del instance.system_metadata[key]
3388 
3389             # Add the new ones
3390             new_sys_metadata = utils.get_system_metadata_from_image(
3391                 image, flavor)
3392 
3393             instance.system_metadata.update(new_sys_metadata)
3394             instance.save()
3395             return orig_sys_metadata
3396 
3397         # Since image might have changed, we may have new values for
3398         # os_type, vm_mode, etc
3399         options_from_image = self._inherit_properties_from_image(
3400                 image, auto_disk_config)
3401         instance.update(options_from_image)
3402 
3403         instance.task_state = task_states.REBUILDING
3404         # An empty instance.image_ref is currently used as an indication
3405         # of BFV.  Preserve that over a rebuild to not break users.
3406         if not is_volume_backed:
3407             instance.image_ref = image_href
3408         instance.kernel_id = kernel_id or ""
3409         instance.ramdisk_id = ramdisk_id or ""
3410         instance.progress = 0
3411         instance.update(kwargs)
3412         instance.save(expected_task_state=[None])
3413 
3414         # On a rebuild, since we're potentially changing images, we need to
3415         # wipe out the old image properties that we're storing as instance
3416         # system metadata... and copy in the properties for the new image.
3417         orig_sys_metadata = _reset_image_metadata()
3418 
3419         self._record_action_start(context, instance, instance_actions.REBUILD)
3420 
3421         # NOTE(sbauza): The migration script we provided in Newton should make
3422         # sure that all our instances are currently migrated to have an
3423         # attached RequestSpec object but let's consider that the operator only
3424         # half migrated all their instances in the meantime.
3425         host = instance.host
3426         # If a new image is provided on rebuild, we will need to run
3427         # through the scheduler again, but we want the instance to be
3428         # rebuilt on the same host it's already on.
3429         if orig_image_ref != image_href:
3430             # We have to modify the request spec that goes to the scheduler
3431             # to contain the new image. We persist this since we've already
3432             # changed the instance.image_ref above so we're being
3433             # consistent.
3434             request_spec.image = objects.ImageMeta.from_dict(image)
3435             request_spec.save()
3436             if 'scheduler_hints' not in request_spec:
3437                 request_spec.scheduler_hints = {}
3438             # Nuke the id on this so we can't accidentally save
3439             # this hint hack later
3440             del request_spec.id
3441 
3442             # NOTE(danms): Passing host=None tells conductor to
3443             # call the scheduler. The _nova_check_type hint
3444             # requires that the scheduler returns only the same
3445             # host that we are currently on and only checks
3446             # rebuild-related filters.
3447             request_spec.scheduler_hints['_nova_check_type'] = ['rebuild']
3448             request_spec.force_hosts = [instance.host]
3449             request_spec.force_nodes = [instance.node]
3450             host = None
3451 
3452         self.compute_task_api.rebuild_instance(context, instance=instance,
3453                 new_pass=admin_password, injected_files=files_to_inject,
3454                 image_ref=image_href, orig_image_ref=orig_image_ref,
3455                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
3456                 preserve_ephemeral=preserve_ephemeral, host=host,
3457                 request_spec=request_spec,
3458                 kwargs=kwargs)
3459 
3460     @staticmethod
3461     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
3462         project_id, user_id = quotas_obj.ids_from_instance(context,
3463                                                            instance)
3464         # Deltas will be empty if the resize is not an upsize.
3465         deltas = compute_utils.upsize_quota_delta(new_flavor,
3466                                                   current_flavor)
3467         if deltas:
3468             try:
3469                 res_deltas = {'cores': deltas.get('cores', 0),
3470                               'ram': deltas.get('ram', 0)}
3471                 objects.Quotas.check_deltas(context, res_deltas,
3472                                             project_id, user_id=user_id,
3473                                             check_project_id=project_id,
3474                                             check_user_id=user_id)
3475             except exception.OverQuota as exc:
3476                 quotas = exc.kwargs['quotas']
3477                 overs = exc.kwargs['overs']
3478                 usages = exc.kwargs['usages']
3479                 headroom = compute_utils.get_headroom(quotas, usages,
3480                                                       deltas)
3481                 (overs, reqs, total_alloweds,
3482                  useds) = compute_utils.get_over_quota_detail(headroom,
3483                                                               overs,
3484                                                               quotas,
3485                                                               deltas)
3486                 LOG.info("%(overs)s quota exceeded for %(pid)s,"
3487                          " tried to resize instance.",
3488                          {'overs': overs, 'pid': context.project_id})
3489                 raise exception.TooManyInstances(overs=overs,
3490                                                  req=reqs,
3491                                                  used=useds,
3492                                                  allowed=total_alloweds)
3493 
3494     @check_instance_lock
3495     @check_instance_cell
3496     @check_instance_state(vm_state=[vm_states.RESIZED])
3497     def revert_resize(self, context, instance):
3498         """Reverts a resize, deleting the 'new' instance in the process."""
3499         elevated = context.elevated()
3500         migration = objects.Migration.get_by_instance_and_status(
3501             elevated, instance.uuid, 'finished')
3502 
3503         # If this is a resize down, a revert might go over quota.
3504         self._check_quota_for_upsize(context, instance, instance.flavor,
3505                                      instance.old_flavor)
3506 
3507         # The AZ for the server may have changed when it was migrated so while
3508         # we are in the API and have access to the API DB, update the
3509         # instance.availability_zone before casting off to the compute service.
3510         # Note that we do this in the API to avoid an "up-call" from the
3511         # compute service to the API DB. This is not great in case something
3512         # fails during revert before the instance.host is updated to the
3513         # original source host, but it is good enough for now. Long-term we
3514         # could consider passing the AZ down to compute so it can set it when
3515         # the instance.host value is set in finish_revert_resize.
3516         instance.availability_zone = (
3517             availability_zones.get_host_availability_zone(
3518                 context, migration.source_compute))
3519 
3520         # Conductor updated the RequestSpec.flavor during the initial resize
3521         # operation to point at the new flavor, so we need to update the
3522         # RequestSpec to point back at the original flavor, otherwise
3523         # subsequent move operations through the scheduler will be using the
3524         # wrong flavor.
3525         reqspec = objects.RequestSpec.get_by_instance_uuid(
3526             context, instance.uuid)
3527         reqspec.flavor = instance.old_flavor
3528         reqspec.save()
3529 
3530         instance.task_state = task_states.RESIZE_REVERTING
3531         instance.save(expected_task_state=[None])
3532 
3533         migration.status = 'reverting'
3534         migration.save()
3535 
3536         self._record_action_start(context, instance,
3537                                   instance_actions.REVERT_RESIZE)
3538 
3539         # TODO(melwitt): We're not rechecking for strict quota here to guard
3540         # against going over quota during a race at this time because the
3541         # resource consumption for this operation is written to the database
3542         # by compute.
3543         self.compute_rpcapi.revert_resize(context, instance,
3544                                           migration,
3545                                           migration.dest_compute)
3546 
3547     @check_instance_lock
3548     @check_instance_cell
3549     @check_instance_state(vm_state=[vm_states.RESIZED])
3550     def confirm_resize(self, context, instance, migration=None):
3551         """Confirms a migration/resize and deletes the 'old' instance."""
3552         elevated = context.elevated()
3553         # NOTE(melwitt): We're not checking quota here because there isn't a
3554         # change in resource usage when confirming a resize. Resource
3555         # consumption for resizes are written to the database by compute, so
3556         # a confirm resize is just a clean up of the migration objects and a
3557         # state change in compute.
3558         if migration is None:
3559             migration = objects.Migration.get_by_instance_and_status(
3560                 elevated, instance.uuid, 'finished')
3561 
3562         migration.status = 'confirming'
3563         migration.save()
3564 
3565         self._record_action_start(context, instance,
3566                                   instance_actions.CONFIRM_RESIZE)
3567 
3568         self.compute_rpcapi.confirm_resize(context,
3569                                            instance,
3570                                            migration,
3571                                            migration.source_compute)
3572 
3573     @staticmethod
3574     def _resize_cells_support(context, instance,
3575                               current_instance_type, new_instance_type):
3576         """Special API cell logic for resize."""
3577         # NOTE(johannes/comstud): The API cell needs a local migration
3578         # record for later resize_confirm and resize_reverts.
3579         # We don't need source and/or destination
3580         # information, just the old and new flavors. Status is set to
3581         # 'finished' since nothing else will update the status along
3582         # the way.
3583         mig = objects.Migration(context=context.elevated())
3584         mig.instance_uuid = instance.uuid
3585         mig.old_instance_type_id = current_instance_type['id']
3586         mig.new_instance_type_id = new_instance_type['id']
3587         mig.status = 'finished'
3588         mig.migration_type = (
3589             mig.old_instance_type_id != mig.new_instance_type_id and
3590             'resize' or 'migration')
3591         mig.create()
3592 
3593     @check_instance_lock
3594     @check_instance_cell
3595     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3596     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3597                host_name=None, **extra_instance_updates):
3598         """Resize (ie, migrate) a running instance.
3599 
3600         If flavor_id is None, the process is considered a migration, keeping
3601         the original flavor_id. If flavor_id is not None, the instance should
3602         be migrated to a new host and resized to the new flavor_id.
3603         host_name is always None in the resize case.
3604         host_name can be set in the cold migration case only.
3605         """
3606         if host_name is not None:
3607             # Cannot migrate to the host where the instance exists
3608             # because it is useless.
3609             if host_name == instance.host:
3610                 raise exception.CannotMigrateToSameHost()
3611 
3612             # Check whether host exists or not.
3613             node = objects.ComputeNode.get_first_node_by_host_for_old_compat(
3614                 context, host_name, use_slave=True)
3615 
3616         self._check_auto_disk_config(instance, **extra_instance_updates)
3617 
3618         current_instance_type = instance.get_flavor()
3619 
3620         # If flavor_id is not provided, only migrate the instance.
3621         if not flavor_id:
3622             LOG.debug("flavor_id is None. Assuming migration.",
3623                       instance=instance)
3624             new_instance_type = current_instance_type
3625         else:
3626             new_instance_type = flavors.get_flavor_by_flavor_id(
3627                     flavor_id, read_deleted="no")
3628             if (new_instance_type.get('root_gb') == 0 and
3629                 current_instance_type.get('root_gb') != 0 and
3630                 not compute_utils.is_volume_backed_instance(context,
3631                     instance)):
3632                 reason = _('Resize to zero disk flavor is not allowed.')
3633                 raise exception.CannotResizeDisk(reason=reason)
3634 
3635         if not new_instance_type:
3636             raise exception.FlavorNotFound(flavor_id=flavor_id)
3637 
3638         current_instance_type_name = current_instance_type['name']
3639         new_instance_type_name = new_instance_type['name']
3640         LOG.debug("Old instance type %(current_instance_type_name)s, "
3641                   "new instance type %(new_instance_type_name)s",
3642                   {'current_instance_type_name': current_instance_type_name,
3643                    'new_instance_type_name': new_instance_type_name},
3644                   instance=instance)
3645 
3646         same_instance_type = (current_instance_type['id'] ==
3647                               new_instance_type['id'])
3648 
3649         # NOTE(sirp): We don't want to force a customer to change their flavor
3650         # when Ops is migrating off of a failed host.
3651         if not same_instance_type and new_instance_type.get('disabled'):
3652             raise exception.FlavorNotFound(flavor_id=flavor_id)
3653 
3654         if same_instance_type and flavor_id and self.cell_type != 'compute':
3655             raise exception.CannotResizeToSameFlavor()
3656 
3657         # ensure there is sufficient headroom for upsizes
3658         if flavor_id:
3659             self._check_quota_for_upsize(context, instance,
3660                                          current_instance_type,
3661                                          new_instance_type)
3662 
3663         if not same_instance_type:
3664             image = utils.get_image_from_system_metadata(
3665                 instance.system_metadata)
3666             # Can skip root_bdm check since it will not change during resize.
3667             self._validate_flavor_image_nostatus(
3668                 context, image, new_instance_type, root_bdm=None,
3669                 validate_pci=True)
3670 
3671         filter_properties = {'ignore_hosts': []}
3672 
3673         if not CONF.allow_resize_to_same_host:
3674             filter_properties['ignore_hosts'].append(instance.host)
3675 
3676         request_spec = objects.RequestSpec.get_by_instance_uuid(
3677             context, instance.uuid)
3678         request_spec.ignore_hosts = filter_properties['ignore_hosts']
3679 
3680         instance.task_state = task_states.RESIZE_PREP
3681         instance.progress = 0
3682         instance.update(extra_instance_updates)
3683         instance.save(expected_task_state=[None])
3684 
3685         if self.cell_type == 'api':
3686             # Create migration record.
3687             self._resize_cells_support(context, instance,
3688                                        current_instance_type,
3689                                        new_instance_type)
3690 
3691         if not flavor_id:
3692             self._record_action_start(context, instance,
3693                                       instance_actions.MIGRATE)
3694         else:
3695             self._record_action_start(context, instance,
3696                                       instance_actions.RESIZE)
3697 
3698         # TODO(melwitt): We're not rechecking for strict quota here to guard
3699         # against going over quota during a race at this time because the
3700         # resource consumption for this operation is written to the database
3701         # by compute.
3702         scheduler_hint = {'filter_properties': filter_properties}
3703 
3704         if host_name is None:
3705             # If 'host_name' is not specified,
3706             # clear the 'requested_destination' field of the RequestSpec.
3707             request_spec.requested_destination = None
3708         else:
3709             # Set the host and the node so that the scheduler will
3710             # validate them.
3711             request_spec.requested_destination = objects.Destination(
3712                 host=node.host, node=node.hypervisor_hostname)
3713 
3714         self.compute_task_api.resize_instance(context, instance,
3715                 extra_instance_updates, scheduler_hint=scheduler_hint,
3716                 flavor=new_instance_type,
3717                 clean_shutdown=clean_shutdown,
3718                 request_spec=request_spec)
3719 
3720     @check_instance_lock
3721     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3722                                     vm_states.PAUSED, vm_states.SUSPENDED])
3723     def shelve(self, context, instance, clean_shutdown=True):
3724         """Shelve an instance.
3725 
3726         Shuts down an instance and frees it up to be removed from the
3727         hypervisor.
3728         """
3729         instance.task_state = task_states.SHELVING
3730         instance.save(expected_task_state=[None])
3731 
3732         self._record_action_start(context, instance, instance_actions.SHELVE)
3733 
3734         if not compute_utils.is_volume_backed_instance(context, instance):
3735             name = '%s-shelved' % instance.display_name
3736             image_meta = compute_utils.create_image(
3737                 context, instance, name, 'snapshot', self.image_api)
3738             image_id = image_meta['id']
3739             self.compute_rpcapi.shelve_instance(context, instance=instance,
3740                     image_id=image_id, clean_shutdown=clean_shutdown)
3741         else:
3742             self.compute_rpcapi.shelve_offload_instance(context,
3743                     instance=instance, clean_shutdown=clean_shutdown)
3744 
3745     @check_instance_lock
3746     @check_instance_state(vm_state=[vm_states.SHELVED])
3747     def shelve_offload(self, context, instance, clean_shutdown=True):
3748         """Remove a shelved instance from the hypervisor."""
3749         instance.task_state = task_states.SHELVING_OFFLOADING
3750         instance.save(expected_task_state=[None])
3751 
3752         self._record_action_start(context, instance,
3753                                   instance_actions.SHELVE_OFFLOAD)
3754 
3755         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3756             clean_shutdown=clean_shutdown)
3757 
3758     @check_instance_lock
3759     @check_instance_state(vm_state=[vm_states.SHELVED,
3760         vm_states.SHELVED_OFFLOADED])
3761     def unshelve(self, context, instance):
3762         """Restore a shelved instance."""
3763         request_spec = objects.RequestSpec.get_by_instance_uuid(
3764             context, instance.uuid)
3765 
3766         instance.task_state = task_states.UNSHELVING
3767         instance.save(expected_task_state=[None])
3768 
3769         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3770 
3771         self.compute_task_api.unshelve_instance(context, instance,
3772                                                 request_spec)
3773 
3774     @check_instance_lock
3775     def add_fixed_ip(self, context, instance, network_id):
3776         """Add fixed_ip from specified network to given instance."""
3777         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3778                 instance=instance, network_id=network_id)
3779 
3780     @check_instance_lock
3781     def remove_fixed_ip(self, context, instance, address):
3782         """Remove fixed_ip from specified network to given instance."""
3783         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3784                 instance=instance, address=address)
3785 
3786     @check_instance_lock
3787     @check_instance_cell
3788     @check_instance_state(vm_state=[vm_states.ACTIVE])
3789     def pause(self, context, instance):
3790         """Pause the given instance."""
3791         instance.task_state = task_states.PAUSING
3792         instance.save(expected_task_state=[None])
3793         self._record_action_start(context, instance, instance_actions.PAUSE)
3794         self.compute_rpcapi.pause_instance(context, instance)
3795 
3796     @check_instance_lock
3797     @check_instance_cell
3798     @check_instance_state(vm_state=[vm_states.PAUSED])
3799     def unpause(self, context, instance):
3800         """Unpause the given instance."""
3801         instance.task_state = task_states.UNPAUSING
3802         instance.save(expected_task_state=[None])
3803         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3804         self.compute_rpcapi.unpause_instance(context, instance)
3805 
3806     @check_instance_host
3807     def get_diagnostics(self, context, instance):
3808         """Retrieve diagnostics for the given instance."""
3809         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3810 
3811     @check_instance_host
3812     def get_instance_diagnostics(self, context, instance):
3813         """Retrieve diagnostics for the given instance."""
3814         return self.compute_rpcapi.get_instance_diagnostics(context,
3815                                                             instance=instance)
3816 
3817     @check_instance_lock
3818     @check_instance_cell
3819     @check_instance_state(vm_state=[vm_states.ACTIVE])
3820     def suspend(self, context, instance):
3821         """Suspend the given instance."""
3822         instance.task_state = task_states.SUSPENDING
3823         instance.save(expected_task_state=[None])
3824         self._record_action_start(context, instance, instance_actions.SUSPEND)
3825         self.compute_rpcapi.suspend_instance(context, instance)
3826 
3827     @check_instance_lock
3828     @check_instance_cell
3829     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3830     def resume(self, context, instance):
3831         """Resume the given instance."""
3832         instance.task_state = task_states.RESUMING
3833         instance.save(expected_task_state=[None])
3834         self._record_action_start(context, instance, instance_actions.RESUME)
3835         self.compute_rpcapi.resume_instance(context, instance)
3836 
3837     @check_instance_lock
3838     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3839                                     vm_states.ERROR])
3840     def rescue(self, context, instance, rescue_password=None,
3841                rescue_image_ref=None, clean_shutdown=True):
3842         """Rescue the given instance."""
3843 
3844         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3845                     context, instance.uuid)
3846         for bdm in bdms:
3847             if bdm.volume_id:
3848                 vol = self.volume_api.get(context, bdm.volume_id)
3849                 self.volume_api.check_attached(context, vol)
3850         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3851             reason = _("Cannot rescue a volume-backed instance")
3852             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3853                                                  reason=reason)
3854 
3855         instance.task_state = task_states.RESCUING
3856         instance.save(expected_task_state=[None])
3857 
3858         self._record_action_start(context, instance, instance_actions.RESCUE)
3859 
3860         self.compute_rpcapi.rescue_instance(context, instance=instance,
3861             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3862             clean_shutdown=clean_shutdown)
3863 
3864     @check_instance_lock
3865     @check_instance_state(vm_state=[vm_states.RESCUED])
3866     def unrescue(self, context, instance):
3867         """Unrescue the given instance."""
3868         instance.task_state = task_states.UNRESCUING
3869         instance.save(expected_task_state=[None])
3870 
3871         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3872 
3873         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3874 
3875     @check_instance_lock
3876     @check_instance_cell
3877     @check_instance_state(vm_state=[vm_states.ACTIVE])
3878     def set_admin_password(self, context, instance, password=None):
3879         """Set the root/admin password for the given instance.
3880 
3881         @param context: Nova auth context.
3882         @param instance: Nova instance object.
3883         @param password: The admin password for the instance.
3884         """
3885         instance.task_state = task_states.UPDATING_PASSWORD
3886         instance.save(expected_task_state=[None])
3887 
3888         self._record_action_start(context, instance,
3889                                   instance_actions.CHANGE_PASSWORD)
3890 
3891         self.compute_rpcapi.set_admin_password(context,
3892                                                instance=instance,
3893                                                new_pass=password)
3894 
3895     @check_instance_host
3896     @reject_instance_state(
3897         task_state=[task_states.DELETING, task_states.MIGRATING])
3898     def get_vnc_console(self, context, instance, console_type):
3899         """Get a url to an instance Console."""
3900         connect_info = self.compute_rpcapi.get_vnc_console(context,
3901                 instance=instance, console_type=console_type)
3902 
3903         # TODO(melwitt): In Rocky, the compute manager puts the
3904         # console authorization in the database in the above method.
3905         # The following will be removed when everything has been
3906         # converted to use the database, in Stein.
3907         if CONF.workarounds.enable_consoleauth:
3908             self.consoleauth_rpcapi.authorize_console(context,
3909                     connect_info['token'], console_type,
3910                     connect_info['host'], connect_info['port'],
3911                     connect_info['internal_access_path'], instance.uuid,
3912                     access_url=connect_info['access_url'])
3913 
3914         return {'url': connect_info['access_url']}
3915 
3916     @check_instance_host
3917     def get_vnc_connect_info(self, context, instance, console_type):
3918         """Used in a child cell to get console info."""
3919         connect_info = self.compute_rpcapi.get_vnc_console(context,
3920                 instance=instance, console_type=console_type)
3921         return connect_info
3922 
3923     @check_instance_host
3924     @reject_instance_state(
3925         task_state=[task_states.DELETING, task_states.MIGRATING])
3926     def get_spice_console(self, context, instance, console_type):
3927         """Get a url to an instance Console."""
3928         connect_info = self.compute_rpcapi.get_spice_console(context,
3929                 instance=instance, console_type=console_type)
3930         # TODO(melwitt): In Rocky, the compute manager puts the
3931         # console authorization in the database in the above method.
3932         # The following will be removed when everything has been
3933         # converted to use the database, in Stein.
3934         if CONF.workarounds.enable_consoleauth:
3935             self.consoleauth_rpcapi.authorize_console(context,
3936                     connect_info['token'], console_type,
3937                     connect_info['host'], connect_info['port'],
3938                     connect_info['internal_access_path'], instance.uuid,
3939                     access_url=connect_info['access_url'])
3940 
3941         return {'url': connect_info['access_url']}
3942 
3943     @check_instance_host
3944     def get_spice_connect_info(self, context, instance, console_type):
3945         """Used in a child cell to get console info."""
3946         connect_info = self.compute_rpcapi.get_spice_console(context,
3947                 instance=instance, console_type=console_type)
3948         return connect_info
3949 
3950     @check_instance_host
3951     @reject_instance_state(
3952         task_state=[task_states.DELETING, task_states.MIGRATING])
3953     def get_rdp_console(self, context, instance, console_type):
3954         """Get a url to an instance Console."""
3955         connect_info = self.compute_rpcapi.get_rdp_console(context,
3956                 instance=instance, console_type=console_type)
3957         # TODO(melwitt): In Rocky, the compute manager puts the
3958         # console authorization in the database in the above method.
3959         # The following will be removed when everything has been
3960         # converted to use the database, in Stein.
3961         if CONF.workarounds.enable_consoleauth:
3962             self.consoleauth_rpcapi.authorize_console(context,
3963                     connect_info['token'], console_type,
3964                     connect_info['host'], connect_info['port'],
3965                     connect_info['internal_access_path'], instance.uuid,
3966                     access_url=connect_info['access_url'])
3967 
3968         return {'url': connect_info['access_url']}
3969 
3970     @check_instance_host
3971     def get_rdp_connect_info(self, context, instance, console_type):
3972         """Used in a child cell to get console info."""
3973         connect_info = self.compute_rpcapi.get_rdp_console(context,
3974                 instance=instance, console_type=console_type)
3975         return connect_info
3976 
3977     @check_instance_host
3978     @reject_instance_state(
3979         task_state=[task_states.DELETING, task_states.MIGRATING])
3980     def get_serial_console(self, context, instance, console_type):
3981         """Get a url to a serial console."""
3982         connect_info = self.compute_rpcapi.get_serial_console(context,
3983                 instance=instance, console_type=console_type)
3984 
3985         # TODO(melwitt): In Rocky, the compute manager puts the
3986         # console authorization in the database in the above method.
3987         # The following will be removed when everything has been
3988         # converted to use the database, in Stein.
3989         if CONF.workarounds.enable_consoleauth:
3990             self.consoleauth_rpcapi.authorize_console(context,
3991                     connect_info['token'], console_type,
3992                     connect_info['host'], connect_info['port'],
3993                     connect_info['internal_access_path'], instance.uuid,
3994                     access_url=connect_info['access_url'])
3995         return {'url': connect_info['access_url']}
3996 
3997     @check_instance_host
3998     def get_serial_console_connect_info(self, context, instance, console_type):
3999         """Used in a child cell to get serial console."""
4000         connect_info = self.compute_rpcapi.get_serial_console(context,
4001                 instance=instance, console_type=console_type)
4002         return connect_info
4003 
4004     @check_instance_host
4005     @reject_instance_state(
4006         task_state=[task_states.DELETING, task_states.MIGRATING])
4007     def get_mks_console(self, context, instance, console_type):
4008         """Get a url to a MKS console."""
4009         connect_info = self.compute_rpcapi.get_mks_console(context,
4010                 instance=instance, console_type=console_type)
4011         # TODO(melwitt): In Rocky, the compute manager puts the
4012         # console authorization in the database in the above method.
4013         # The following will be removed when everything has been
4014         # converted to use the database, in Stein.
4015         if CONF.workarounds.enable_consoleauth:
4016             self.consoleauth_rpcapi.authorize_console(context,
4017                     connect_info['token'], console_type,
4018                     connect_info['host'], connect_info['port'],
4019                     connect_info['internal_access_path'], instance.uuid,
4020                     access_url=connect_info['access_url'])
4021         return {'url': connect_info['access_url']}
4022 
4023     @check_instance_host
4024     def get_console_output(self, context, instance, tail_length=None):
4025         """Get console output for an instance."""
4026         return self.compute_rpcapi.get_console_output(context,
4027                 instance=instance, tail_length=tail_length)
4028 
4029     def lock(self, context, instance):
4030         """Lock the given instance."""
4031         # Only update the lock if we are an admin (non-owner)
4032         is_owner = instance.project_id == context.project_id
4033         if instance.locked and is_owner:
4034             return
4035 
4036         context = context.elevated()
4037         self._record_action_start(context, instance,
4038                                   instance_actions.LOCK)
4039 
4040         @wrap_instance_event(prefix='api')
4041         def lock(self, context, instance):
4042             LOG.debug('Locking', instance=instance)
4043             instance.locked = True
4044             instance.locked_by = 'owner' if is_owner else 'admin'
4045             instance.save()
4046 
4047         lock(self, context, instance)
4048         compute_utils.notify_about_instance_action(
4049             context, instance, CONF.host,
4050             action=fields_obj.NotificationAction.LOCK,
4051             source=fields_obj.NotificationSource.API)
4052 
4053     def is_expected_locked_by(self, context, instance):
4054         is_owner = instance.project_id == context.project_id
4055         expect_locked_by = 'owner' if is_owner else 'admin'
4056         locked_by = instance.locked_by
4057         if locked_by and locked_by != expect_locked_by:
4058             return False
4059         return True
4060 
4061     def unlock(self, context, instance):
4062         """Unlock the given instance."""
4063         context = context.elevated()
4064         self._record_action_start(context, instance,
4065                                   instance_actions.UNLOCK)
4066 
4067         @wrap_instance_event(prefix='api')
4068         def unlock(self, context, instance):
4069             LOG.debug('Unlocking', instance=instance)
4070             instance.locked = False
4071             instance.locked_by = None
4072             instance.save()
4073 
4074         unlock(self, context, instance)
4075         compute_utils.notify_about_instance_action(
4076             context, instance, CONF.host,
4077             action=fields_obj.NotificationAction.UNLOCK,
4078             source=fields_obj.NotificationSource.API)
4079 
4080     @check_instance_lock
4081     @check_instance_cell
4082     def reset_network(self, context, instance):
4083         """Reset networking on the instance."""
4084         self.compute_rpcapi.reset_network(context, instance=instance)
4085 
4086     @check_instance_lock
4087     @check_instance_cell
4088     def inject_network_info(self, context, instance):
4089         """Inject network info for the instance."""
4090         self.compute_rpcapi.inject_network_info(context, instance=instance)
4091 
4092     def _create_volume_bdm(self, context, instance, device, volume,
4093                            disk_bus, device_type, is_local_creation=False,
4094                            tag=None):
4095         volume_id = volume['id']
4096         if is_local_creation:
4097             # when the creation is done locally we can't specify the device
4098             # name as we do not have a way to check that the name specified is
4099             # a valid one.
4100             # We leave the setting of that value when the actual attach
4101             # happens on the compute manager
4102             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
4103             # support device tagging because we have no way to call the compute
4104             # manager to check that it supports device tagging. In fact, we
4105             # don't even know which computer manager the instance will
4106             # eventually end up on when it's unshelved.
4107             volume_bdm = objects.BlockDeviceMapping(
4108                 context=context,
4109                 source_type='volume', destination_type='volume',
4110                 instance_uuid=instance.uuid, boot_index=None,
4111                 volume_id=volume_id,
4112                 device_name=None, guest_format=None,
4113                 disk_bus=disk_bus, device_type=device_type)
4114             volume_bdm.create()
4115         else:
4116             # NOTE(vish): This is done on the compute host because we want
4117             #             to avoid a race where two devices are requested at
4118             #             the same time. When db access is removed from
4119             #             compute, the bdm will be created here and we will
4120             #             have to make sure that they are assigned atomically.
4121             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
4122                 context, instance, device, volume_id, disk_bus=disk_bus,
4123                 device_type=device_type, tag=tag,
4124                 multiattach=volume['multiattach'])
4125         return volume_bdm
4126 
4127     def _check_volume_already_attached_to_instance(self, context, instance,
4128                                                    volume_id):
4129         """Avoid attaching the same volume to the same instance twice.
4130 
4131            As the new Cinder flow (microversion 3.44) is handling the checks
4132            differently and allows to attach the same volume to the same
4133            instance twice to enable live_migrate we are checking whether the
4134            BDM already exists for this combination for the new flow and fail
4135            if it does.
4136         """
4137 
4138         try:
4139             objects.BlockDeviceMapping.get_by_volume_and_instance(
4140                 context, volume_id, instance.uuid)
4141 
4142             msg = _("volume %s already attached") % volume_id
4143             raise exception.InvalidVolume(reason=msg)
4144         except exception.VolumeBDMNotFound:
4145             pass
4146 
4147     def _check_attach_and_reserve_volume(self, context, volume, instance,
4148                                          bdm, supports_multiattach=False):
4149         volume_id = volume['id']
4150         self.volume_api.check_availability_zone(context, volume,
4151                                                 instance=instance)
4152         # If volume.multiattach=True and the microversion to
4153         # support multiattach is not used, fail the request.
4154         if volume['multiattach'] and not supports_multiattach:
4155             raise exception.MultiattachNotSupportedOldMicroversion()
4156 
4157         if 'id' in instance:
4158             # This is a volume attach to an existing instance, so
4159             # we only care about the cell the instance is in.
4160             min_compute_version = objects.Service.get_minimum_version(
4161                 context, 'nova-compute')
4162         else:
4163             # The instance is being created and we don't know which
4164             # cell it's going to land in, so check all cells.
4165             # NOTE(danms): We don't require all cells to report here since
4166             # we're really concerned about the new-ness of cells that the
4167             # instance may be scheduled into. If a cell doesn't respond here,
4168             # then it won't be a candidate for the instance and thus doesn't
4169             # matter.
4170             min_compute_version = \
4171                 objects.service.get_minimum_version_all_cells(
4172                     context, ['nova-compute'])
4173             # Check to see if the computes have been upgraded to support
4174             # booting from a multiattach volume.
4175             if (volume['multiattach'] and
4176                     min_compute_version < MIN_COMPUTE_MULTIATTACH):
4177                 raise exception.MultiattachSupportNotYetAvailable()
4178 
4179         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
4180             # Attempt a new style volume attachment, but fallback to old-style
4181             # in case Cinder API 3.44 isn't available.
4182             try:
4183                 attachment_id = self.volume_api.attachment_create(
4184                     context, volume_id, instance.uuid)['id']
4185                 bdm.attachment_id = attachment_id
4186                 # NOTE(ildikov): In case of boot from volume the BDM at this
4187                 # point is not yet created in a cell database, so we can't
4188                 # call save().  When attaching a volume to an existing
4189                 # instance, the instance is already in a cell and the BDM has
4190                 # been created in that same cell so updating here in that case
4191                 # is "ok".
4192                 if bdm.obj_attr_is_set('id'):
4193                     bdm.save()
4194             except exception.CinderAPIVersionNotAvailable:
4195                 LOG.debug('The available Cinder microversion is not high '
4196                           'enough to create new style volume attachment.')
4197                 self.volume_api.reserve_volume(context, volume_id)
4198         else:
4199             LOG.debug('The compute service version is not high enough to '
4200                       'create a new style volume attachment.')
4201             self.volume_api.reserve_volume(context, volume_id)
4202 
4203     def _attach_volume(self, context, instance, volume, device,
4204                        disk_bus, device_type, tag=None,
4205                        supports_multiattach=False):
4206         """Attach an existing volume to an existing instance.
4207 
4208         This method is separated to make it possible for cells version
4209         to override it.
4210         """
4211         volume_bdm = self._create_volume_bdm(
4212             context, instance, device, volume, disk_bus=disk_bus,
4213             device_type=device_type, tag=tag)
4214         try:
4215             self._check_attach_and_reserve_volume(context, volume, instance,
4216                                                   volume_bdm,
4217                                                   supports_multiattach)
4218             self._record_action_start(
4219                 context, instance, instance_actions.ATTACH_VOLUME)
4220             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
4221         except Exception:
4222             with excutils.save_and_reraise_exception():
4223                 volume_bdm.destroy()
4224 
4225         return volume_bdm.device_name
4226 
4227     def _attach_volume_shelved_offloaded(self, context, instance, volume,
4228                                          device, disk_bus, device_type):
4229         """Attach an existing volume to an instance in shelved offloaded state.
4230 
4231         Attaching a volume for an instance in shelved offloaded state requires
4232         to perform the regular check to see if we can attach and reserve the
4233         volume then we need to call the attach method on the volume API
4234         to mark the volume as 'in-use'.
4235         The instance at this stage is not managed by a compute manager
4236         therefore the actual attachment will be performed once the
4237         instance will be unshelved.
4238         """
4239         volume_id = volume['id']
4240 
4241         @wrap_instance_event(prefix='api')
4242         def attach_volume(self, context, v_id, instance, dev, attachment_id):
4243             if attachment_id:
4244                 # Normally we wouldn't complete an attachment without a host
4245                 # connector, but we do this to make the volume status change
4246                 # to "in-use" to maintain the API semantics with the old flow.
4247                 # When unshelving the instance, the compute service will deal
4248                 # with this disconnected attachment.
4249                 self.volume_api.attachment_complete(context, attachment_id)
4250             else:
4251                 self.volume_api.attach(context,
4252                                        v_id,
4253                                        instance.uuid,
4254                                        dev)
4255 
4256         volume_bdm = self._create_volume_bdm(
4257             context, instance, device, volume, disk_bus=disk_bus,
4258             device_type=device_type, is_local_creation=True)
4259         try:
4260             self._check_attach_and_reserve_volume(context, volume, instance,
4261                                                   volume_bdm)
4262             self._record_action_start(
4263                 context, instance,
4264                 instance_actions.ATTACH_VOLUME)
4265             attach_volume(self, context, volume_id, instance, device,
4266                           volume_bdm.attachment_id)
4267         except Exception:
4268             with excutils.save_and_reraise_exception():
4269                 volume_bdm.destroy()
4270 
4271         return volume_bdm.device_name
4272 
4273     @check_instance_lock
4274     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4275                                     vm_states.STOPPED, vm_states.RESIZED,
4276                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4277                                     vm_states.SHELVED_OFFLOADED])
4278     def attach_volume(self, context, instance, volume_id, device=None,
4279                       disk_bus=None, device_type=None, tag=None,
4280                       supports_multiattach=False):
4281         """Attach an existing volume to an existing instance."""
4282         # NOTE(vish): Fail fast if the device is not going to pass. This
4283         #             will need to be removed along with the test if we
4284         #             change the logic in the manager for what constitutes
4285         #             a valid device.
4286         if device and not block_device.match_device(device):
4287             raise exception.InvalidDevicePath(path=device)
4288 
4289         # Check to see if the computes in this cell can support new-style
4290         # volume attachments.
4291         min_compute_version = objects.Service.get_minimum_version(
4292             context, 'nova-compute')
4293         if min_compute_version >= CINDER_V3_ATTACH_MIN_COMPUTE_VERSION:
4294             try:
4295                 # Check to see if Cinder is new enough to create new-style
4296                 # attachments.
4297                 cinder.is_microversion_supported(context, '3.44')
4298             except exception.CinderAPIVersionNotAvailable:
4299                 pass
4300             else:
4301                 # Make sure the volume isn't already attached to this instance
4302                 # because based on the above checks, we'll use the new style
4303                 # attachment flow in _check_attach_and_reserve_volume and
4304                 # Cinder will allow multiple attachments between the same
4305                 # volume and instance but the old flow API semantics don't
4306                 # allow that so we enforce it here.
4307                 self._check_volume_already_attached_to_instance(context,
4308                                                                 instance,
4309                                                                 volume_id)
4310 
4311         volume = self.volume_api.get(context, volume_id)
4312         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
4313         if is_shelved_offloaded:
4314             if tag:
4315                 # NOTE(artom) Local attach (to a shelved-offload instance)
4316                 # cannot support device tagging because we have no way to call
4317                 # the compute manager to check that it supports device tagging.
4318                 # In fact, we don't even know which computer manager the
4319                 # instance will eventually end up on when it's unshelved.
4320                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
4321             if volume['multiattach']:
4322                 # NOTE(mriedem): Similar to tagged attach, we don't support
4323                 # attaching a multiattach volume to shelved offloaded instances
4324                 # because we can't tell if the compute host (since there isn't
4325                 # one) supports it. This could possibly be supported in the
4326                 # future if the scheduler was made aware of which computes
4327                 # support multiattach volumes.
4328                 raise exception.MultiattachToShelvedNotSupported()
4329             return self._attach_volume_shelved_offloaded(context,
4330                                                          instance,
4331                                                          volume,
4332                                                          device,
4333                                                          disk_bus,
4334                                                          device_type)
4335 
4336         return self._attach_volume(context, instance, volume, device,
4337                                    disk_bus, device_type, tag=tag,
4338                                    supports_multiattach=supports_multiattach)
4339 
4340     def _detach_volume(self, context, instance, volume):
4341         """Detach volume from instance.
4342 
4343         This method is separated to make it easier for cells version
4344         to override.
4345         """
4346         try:
4347             self.volume_api.begin_detaching(context, volume['id'])
4348         except exception.InvalidInput as exc:
4349             raise exception.InvalidVolume(reason=exc.format_message())
4350         attachments = volume.get('attachments', {})
4351         attachment_id = None
4352         if attachments and instance.uuid in attachments:
4353             attachment_id = attachments[instance.uuid]['attachment_id']
4354         self._record_action_start(
4355             context, instance, instance_actions.DETACH_VOLUME)
4356         self.compute_rpcapi.detach_volume(context, instance=instance,
4357                 volume_id=volume['id'], attachment_id=attachment_id)
4358 
4359     def _detach_volume_shelved_offloaded(self, context, instance, volume):
4360         """Detach a volume from an instance in shelved offloaded state.
4361 
4362         If the instance is shelved offloaded we just need to cleanup volume
4363         calling the volume api detach, the volume api terminate_connection
4364         and delete the bdm record.
4365         If the volume has delete_on_termination option set then we call the
4366         volume api delete as well.
4367         """
4368         @wrap_instance_event(prefix='api')
4369         def detach_volume(self, context, instance, bdms):
4370             self._local_cleanup_bdm_volumes(bdms, instance, context)
4371 
4372         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
4373                 context, volume['id'], instance.uuid)]
4374         # The begin_detaching() call only works with in-use volumes,
4375         # which will not be the case for volumes attached to a shelved
4376         # offloaded server via the attachments API since those volumes
4377         # will have `reserved` status.
4378         if not bdms[0].attachment_id:
4379             try:
4380                 self.volume_api.begin_detaching(context, volume['id'])
4381             except exception.InvalidInput as exc:
4382                 raise exception.InvalidVolume(reason=exc.format_message())
4383         self._record_action_start(
4384             context, instance,
4385             instance_actions.DETACH_VOLUME)
4386         detach_volume(self, context, instance, bdms)
4387 
4388     @check_instance_lock
4389     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4390                                     vm_states.STOPPED, vm_states.RESIZED,
4391                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
4392                                     vm_states.SHELVED_OFFLOADED])
4393     def detach_volume(self, context, instance, volume):
4394         """Detach a volume from an instance."""
4395         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4396             self._detach_volume_shelved_offloaded(context, instance, volume)
4397         else:
4398             self._detach_volume(context, instance, volume)
4399 
4400     @check_instance_lock
4401     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4402                                     vm_states.RESIZED])
4403     def swap_volume(self, context, instance, old_volume, new_volume):
4404         """Swap volume attached to an instance."""
4405         # The caller likely got the instance from volume['attachments']
4406         # in the first place, but let's sanity check.
4407         if not old_volume.get('attachments', {}).get(instance.uuid):
4408             msg = _("Old volume is attached to a different instance.")
4409             raise exception.InvalidVolume(reason=msg)
4410         if new_volume['attach_status'] == 'attached':
4411             msg = _("New volume must be detached in order to swap.")
4412             raise exception.InvalidVolume(reason=msg)
4413         if int(new_volume['size']) < int(old_volume['size']):
4414             msg = _("New volume must be the same size or larger.")
4415             raise exception.InvalidVolume(reason=msg)
4416         self.volume_api.check_availability_zone(context, new_volume,
4417                                                 instance=instance)
4418         try:
4419             self.volume_api.begin_detaching(context, old_volume['id'])
4420         except exception.InvalidInput as exc:
4421             raise exception.InvalidVolume(reason=exc.format_message())
4422 
4423         # Get the BDM for the attached (old) volume so we can tell if it was
4424         # attached with the new-style Cinder 3.44 API.
4425         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
4426             context, old_volume['id'], instance.uuid)
4427         new_attachment_id = None
4428         if bdm.attachment_id is None:
4429             # This is an old-style attachment so reserve the new volume before
4430             # we cast to the compute host.
4431             self.volume_api.reserve_volume(context, new_volume['id'])
4432         else:
4433             try:
4434                 self._check_volume_already_attached_to_instance(
4435                     context, instance, new_volume['id'])
4436             except exception.InvalidVolume:
4437                 with excutils.save_and_reraise_exception():
4438                     self.volume_api.roll_detaching(context, old_volume['id'])
4439 
4440             # This is a new-style attachment so for the volume that we are
4441             # going to swap to, create a new volume attachment.
4442             new_attachment_id = self.volume_api.attachment_create(
4443                 context, new_volume['id'], instance.uuid)['id']
4444 
4445         self._record_action_start(
4446             context, instance, instance_actions.SWAP_VOLUME)
4447 
4448         try:
4449             self.compute_rpcapi.swap_volume(
4450                     context, instance=instance,
4451                     old_volume_id=old_volume['id'],
4452                     new_volume_id=new_volume['id'],
4453                     new_attachment_id=new_attachment_id)
4454         except Exception:
4455             with excutils.save_and_reraise_exception():
4456                 self.volume_api.roll_detaching(context, old_volume['id'])
4457                 if new_attachment_id is None:
4458                     self.volume_api.unreserve_volume(context, new_volume['id'])
4459                 else:
4460                     self.volume_api.attachment_delete(
4461                         context, new_attachment_id)
4462 
4463     @check_instance_lock
4464     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4465                                     vm_states.STOPPED],
4466                           task_state=[None])
4467     def attach_interface(self, context, instance, network_id, port_id,
4468                          requested_ip, tag=None):
4469         """Use hotplug to add an network adapter to an instance."""
4470         self._record_action_start(
4471             context, instance, instance_actions.ATTACH_INTERFACE)
4472 
4473         # NOTE(gibi): Checking if the requested port has resource request as
4474         # such ports are currently not supported as they would at least
4475         # need resource allocation manipulation in placement but might also
4476         # need a new scheduling if resource on this host is not available.
4477         if port_id:
4478             port = self.network_api.show_port(context, port_id)
4479             if port['port'].get('resource_request'):
4480                 raise exception.AttachInterfaceWithQoSPolicyNotSupported(
4481                     instance_uuid=instance.uuid)
4482 
4483         return self.compute_rpcapi.attach_interface(context,
4484             instance=instance, network_id=network_id, port_id=port_id,
4485             requested_ip=requested_ip, tag=tag)
4486 
4487     @check_instance_lock
4488     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4489                                     vm_states.STOPPED],
4490                           task_state=[None])
4491     def detach_interface(self, context, instance, port_id):
4492         """Detach an network adapter from an instance."""
4493         self._record_action_start(
4494             context, instance, instance_actions.DETACH_INTERFACE)
4495         self.compute_rpcapi.detach_interface(context, instance=instance,
4496             port_id=port_id)
4497 
4498     def get_instance_metadata(self, context, instance):
4499         """Get all metadata associated with an instance."""
4500         return self.db.instance_metadata_get(context, instance.uuid)
4501 
4502     @check_instance_lock
4503     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4504                                     vm_states.SUSPENDED, vm_states.STOPPED],
4505                           task_state=None)
4506     def delete_instance_metadata(self, context, instance, key):
4507         """Delete the given metadata item from an instance."""
4508         instance.delete_metadata_key(key)
4509         self.compute_rpcapi.change_instance_metadata(context,
4510                                                      instance=instance,
4511                                                      diff={key: ['-']})
4512 
4513     @check_instance_lock
4514     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
4515                                     vm_states.SUSPENDED, vm_states.STOPPED],
4516                           task_state=None)
4517     def update_instance_metadata(self, context, instance,
4518                                  metadata, delete=False):
4519         """Updates or creates instance metadata.
4520 
4521         If delete is True, metadata items that are not specified in the
4522         `metadata` argument will be deleted.
4523 
4524         """
4525         orig = dict(instance.metadata)
4526         if delete:
4527             _metadata = metadata
4528         else:
4529             _metadata = dict(instance.metadata)
4530             _metadata.update(metadata)
4531 
4532         self._check_metadata_properties_quota(context, _metadata)
4533         instance.metadata = _metadata
4534         instance.save()
4535         diff = _diff_dict(orig, instance.metadata)
4536         self.compute_rpcapi.change_instance_metadata(context,
4537                                                      instance=instance,
4538                                                      diff=diff)
4539         return _metadata
4540 
4541     @check_instance_lock
4542     @check_instance_cell
4543     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
4544     def live_migrate(self, context, instance, block_migration,
4545                      disk_over_commit, host_name, force=None, async_=False):
4546         """Migrate a server lively to a new host."""
4547         LOG.debug("Going to try to live migrate instance to %s",
4548                   host_name or "another host", instance=instance)
4549 
4550         if host_name:
4551             # Validate the specified host before changing the instance task
4552             # state.
4553             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
4554 
4555         request_spec = objects.RequestSpec.get_by_instance_uuid(
4556             context, instance.uuid)
4557 
4558         instance.task_state = task_states.MIGRATING
4559         instance.save(expected_task_state=[None])
4560 
4561         self._record_action_start(context, instance,
4562                                   instance_actions.LIVE_MIGRATION)
4563 
4564         # TODO(melwitt): In Rocky, we optionally store console authorizations
4565         # in both the consoleauth service and the database while
4566         # we convert to using the database. Remove the condition for running
4567         # this line with cells v2, when consoleauth is no longer being used by
4568         # cells v2, in Stein.
4569         if CONF.cells.enable or CONF.workarounds.enable_consoleauth:
4570             self.consoleauth_rpcapi.delete_tokens_for_instance(
4571                 context, instance.uuid)
4572 
4573         # NOTE(sbauza): Force is a boolean by the new related API version
4574         if force is False and host_name:
4575             # Unset the host to make sure we call the scheduler
4576             # from the conductor LiveMigrationTask. Yes this is tightly-coupled
4577             # to behavior in conductor and not great.
4578             host_name = None
4579             # FIXME(sbauza): Since only Ironic driver uses more than one
4580             # compute per service but doesn't support live migrations,
4581             # let's provide the first one.
4582             target = nodes[0]
4583             destination = objects.Destination(
4584                 host=target.host,
4585                 node=target.hypervisor_hostname
4586             )
4587             # This is essentially a hint to the scheduler to only consider
4588             # the specified host but still run it through the filters.
4589             request_spec.requested_destination = destination
4590 
4591         try:
4592             self.compute_task_api.live_migrate_instance(context, instance,
4593                 host_name, block_migration=block_migration,
4594                 disk_over_commit=disk_over_commit,
4595                 request_spec=request_spec, async_=async_)
4596         except oslo_exceptions.MessagingTimeout as messaging_timeout:
4597             with excutils.save_and_reraise_exception():
4598                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
4599                 # occurs, but LM will still be in progress, so write
4600                 # instance fault to database
4601                 compute_utils.add_instance_fault_from_exc(context,
4602                                                           instance,
4603                                                           messaging_timeout)
4604 
4605     @check_instance_lock
4606     @check_instance_cell
4607     @check_instance_state(vm_state=[vm_states.ACTIVE],
4608                           task_state=[task_states.MIGRATING])
4609     def live_migrate_force_complete(self, context, instance, migration_id):
4610         """Force live migration to complete.
4611 
4612         :param context: Security context
4613         :param instance: The instance that is being migrated
4614         :param migration_id: ID of ongoing migration
4615 
4616         """
4617         LOG.debug("Going to try to force live migration to complete",
4618                   instance=instance)
4619 
4620         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
4621         # live migration for particular instance. Also pass migration id to
4622         # compute to double check and avoid possible race condition.
4623         migration = objects.Migration.get_by_id_and_instance(
4624             context, migration_id, instance.uuid)
4625         if migration.status != 'running':
4626             raise exception.InvalidMigrationState(migration_id=migration_id,
4627                                                   instance_uuid=instance.uuid,
4628                                                   state=migration.status,
4629                                                   method='force complete')
4630 
4631         self._record_action_start(
4632             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
4633 
4634         self.compute_rpcapi.live_migration_force_complete(
4635             context, instance, migration)
4636 
4637     @check_instance_lock
4638     @check_instance_cell
4639     @check_instance_state(task_state=[task_states.MIGRATING])
4640     def live_migrate_abort(self, context, instance, migration_id,
4641                            support_abort_in_queue=False):
4642         """Abort an in-progress live migration.
4643 
4644         :param context: Security context
4645         :param instance: The instance that is being migrated
4646         :param migration_id: ID of in-progress live migration
4647         :param support_abort_in_queue: Flag indicating whether we can support
4648             abort migrations in "queued" or "preparing" status.
4649 
4650         """
4651         migration = objects.Migration.get_by_id_and_instance(context,
4652                     migration_id, instance.uuid)
4653         LOG.debug("Going to cancel live migration %s",
4654                   migration.id, instance=instance)
4655 
4656         # If the microversion does not support abort migration in queue,
4657         # we are only be able to abort migrations with `running` status;
4658         # if it is supported, we are able to also abort migrations in
4659         # `queued` and `preparing` status.
4660         allowed_states = ['running']
4661         queued_states = ['queued', 'preparing']
4662         if support_abort_in_queue:
4663             # The user requested a microversion that supports aborting a queued
4664             # or preparing live migration. But we need to check that the
4665             # compute service hosting the instance is new enough to support
4666             # aborting a queued/preparing live migration, so we check the
4667             # service version here.
4668             # TODO(Kevin_Zheng): This service version check can be removed in
4669             # Stein (at the earliest) when the API only supports Rocky or
4670             # newer computes.
4671             if migration.status in queued_states:
4672                 service = objects.Service.get_by_compute_host(
4673                     context, instance.host)
4674                 if service.version < MIN_COMPUTE_ABORT_QUEUED_LIVE_MIGRATION:
4675                     raise exception.AbortQueuedLiveMigrationNotYetSupported(
4676                         migration_id=migration_id, status=migration.status)
4677             allowed_states.extend(queued_states)
4678 
4679         if migration.status not in allowed_states:
4680             raise exception.InvalidMigrationState(migration_id=migration_id,
4681                     instance_uuid=instance.uuid,
4682                     state=migration.status,
4683                     method='abort live migration')
4684         self._record_action_start(context, instance,
4685                                   instance_actions.LIVE_MIGRATION_CANCEL)
4686 
4687         self.compute_rpcapi.live_migration_abort(context,
4688                 instance, migration.id)
4689 
4690     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
4691                                     vm_states.ERROR])
4692     def evacuate(self, context, instance, host, on_shared_storage,
4693                  admin_password=None, force=None):
4694         """Running evacuate to target host.
4695 
4696         Checking vm compute host state, if the host not in expected_state,
4697         raising an exception.
4698 
4699         :param instance: The instance to evacuate
4700         :param host: Target host. if not set, the scheduler will pick up one
4701         :param on_shared_storage: True if instance files on shared storage
4702         :param admin_password: password to set on rebuilt instance
4703         :param force: Force the evacuation to the specific host target
4704 
4705         """
4706         LOG.debug('vm evacuation scheduled', instance=instance)
4707         inst_host = instance.host
4708         service = objects.Service.get_by_compute_host(context, inst_host)
4709         if self.servicegroup_api.service_is_up(service):
4710             LOG.error('Instance compute service state on %s '
4711                       'expected to be down, but it was up.', inst_host)
4712             raise exception.ComputeServiceInUse(host=inst_host)
4713 
4714         request_spec = objects.RequestSpec.get_by_instance_uuid(
4715             context, instance.uuid)
4716 
4717         instance.task_state = task_states.REBUILDING
4718         instance.save(expected_task_state=[None])
4719         self._record_action_start(context, instance, instance_actions.EVACUATE)
4720 
4721         # NOTE(danms): Create this as a tombstone for the source compute
4722         # to find and cleanup. No need to pass it anywhere else.
4723         migration = objects.Migration(context,
4724                                       source_compute=instance.host,
4725                                       source_node=instance.node,
4726                                       instance_uuid=instance.uuid,
4727                                       status='accepted',
4728                                       migration_type='evacuation')
4729         if host:
4730             migration.dest_compute = host
4731         migration.create()
4732 
4733         compute_utils.notify_about_instance_usage(
4734             self.notifier, context, instance, "evacuate")
4735         compute_utils.notify_about_instance_action(
4736             context, instance, CONF.host,
4737             action=fields_obj.NotificationAction.EVACUATE,
4738             source=fields_obj.NotificationSource.API)
4739 
4740         # NOTE(sbauza): Force is a boolean by the new related API version
4741         # TODO(stephenfin): Any reason we can't use 'not force' here to handle
4742         # the pre-v2.29 API microversion, which wouldn't set force
4743         if force is False and host:
4744             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
4745             # NOTE(sbauza): Unset the host to make sure we call the scheduler
4746             host = None
4747             # FIXME(sbauza): Since only Ironic driver uses more than one
4748             # compute per service but doesn't support evacuations,
4749             # let's provide the first one.
4750             target = nodes[0]
4751             destination = objects.Destination(
4752                 host=target.host,
4753                 node=target.hypervisor_hostname
4754             )
4755             request_spec.requested_destination = destination
4756 
4757         return self.compute_task_api.rebuild_instance(context,
4758                        instance=instance,
4759                        new_pass=admin_password,
4760                        injected_files=None,
4761                        image_ref=None,
4762                        orig_image_ref=None,
4763                        orig_sys_metadata=None,
4764                        bdms=None,
4765                        recreate=True,
4766                        on_shared_storage=on_shared_storage,
4767                        host=host,
4768                        request_spec=request_spec,
4769                        )
4770 
4771     def get_migrations(self, context, filters):
4772         """Get all migrations for the given filters."""
4773         load_cells()
4774 
4775         migrations = []
4776         for cell in CELLS:
4777             if cell.uuid == objects.CellMapping.CELL0_UUID:
4778                 continue
4779             with nova_context.target_cell(context, cell) as cctxt:
4780                 migrations.extend(objects.MigrationList.get_by_filters(
4781                     cctxt, filters).objects)
4782         return objects.MigrationList(objects=migrations)
4783 
4784     def get_migrations_sorted(self, context, filters, sort_dirs=None,
4785                               sort_keys=None, limit=None, marker=None):
4786         """Get all migrations for the given parameters."""
4787         mig_objs = migration_list.get_migration_objects_sorted(
4788             context, filters, limit, marker, sort_keys, sort_dirs)
4789         return mig_objs
4790 
4791     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
4792                                                migration_type=None):
4793         """Get all migrations of an instance in progress."""
4794         return objects.MigrationList.get_in_progress_by_instance(
4795                 context, instance_uuid, migration_type)
4796 
4797     def get_migration_by_id_and_instance(self, context,
4798                                          migration_id, instance_uuid):
4799         """Get the migration of an instance by id."""
4800         return objects.Migration.get_by_id_and_instance(
4801                 context, migration_id, instance_uuid)
4802 
4803     def _get_bdm_by_volume_id(self, context, volume_id, expected_attrs=None):
4804         """Retrieve a BDM without knowing its cell.
4805 
4806         .. note:: The context will be targeted to the cell in which the
4807             BDM is found, if any.
4808 
4809         :param context: The API request context.
4810         :param volume_id: The ID of the volume.
4811         :param expected_attrs: list of any additional attributes that should
4812             be joined when the BDM is loaded from the database.
4813         :raises: nova.exception.VolumeBDMNotFound if not found in any cell
4814         """
4815         load_cells()
4816         for cell in CELLS:
4817             nova_context.set_target_cell(context, cell)
4818             try:
4819                 return objects.BlockDeviceMapping.get_by_volume(
4820                     context, volume_id, expected_attrs=expected_attrs)
4821             except exception.NotFound:
4822                 continue
4823         raise exception.VolumeBDMNotFound(volume_id=volume_id)
4824 
4825     def volume_snapshot_create(self, context, volume_id, create_info):
4826         bdm = self._get_bdm_by_volume_id(
4827             context, volume_id, expected_attrs=['instance'])
4828 
4829         # We allow creating the snapshot in any vm_state as long as there is
4830         # no task being performed on the instance and it has a host.
4831         @check_instance_host
4832         @check_instance_state(vm_state=None)
4833         def do_volume_snapshot_create(self, context, instance):
4834             self.compute_rpcapi.volume_snapshot_create(context, instance,
4835                     volume_id, create_info)
4836             snapshot = {
4837                 'snapshot': {
4838                     'id': create_info.get('id'),
4839                     'volumeId': volume_id
4840                 }
4841             }
4842             return snapshot
4843 
4844         return do_volume_snapshot_create(self, context, bdm.instance)
4845 
4846     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4847                                delete_info):
4848         bdm = self._get_bdm_by_volume_id(
4849             context, volume_id, expected_attrs=['instance'])
4850 
4851         # We allow deleting the snapshot in any vm_state as long as there is
4852         # no task being performed on the instance and it has a host.
4853         @check_instance_host
4854         @check_instance_state(vm_state=None)
4855         def do_volume_snapshot_delete(self, context, instance):
4856             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4857                     volume_id, snapshot_id, delete_info)
4858 
4859         do_volume_snapshot_delete(self, context, bdm.instance)
4860 
4861     def external_instance_event(self, api_context, instances, events):
4862         # NOTE(danms): The external API consumer just provides events,
4863         # but doesn't know where they go. We need to collate lists
4864         # by the host the affected instance is on and dispatch them
4865         # according to host
4866         instances_by_host = collections.defaultdict(list)
4867         events_by_host = collections.defaultdict(list)
4868         hosts_by_instance = collections.defaultdict(list)
4869         cell_contexts_by_host = {}
4870         for instance in instances:
4871             # instance._context is used here since it's already targeted to
4872             # the cell that the instance lives in, and we need to use that
4873             # cell context to lookup any migrations associated to the instance.
4874             for host in self._get_relevant_hosts(instance._context, instance):
4875                 # NOTE(danms): All instances on a host must have the same
4876                 # mapping, so just use that
4877                 # NOTE(mdbooth): We don't currently support migrations between
4878                 # cells, and given that the Migration record is hosted in the
4879                 # cell _get_relevant_hosts will likely have to change before we
4880                 # do. Consequently we can currently assume that the context for
4881                 # both the source and destination hosts of a migration is the
4882                 # same.
4883                 if host not in cell_contexts_by_host:
4884                     cell_contexts_by_host[host] = instance._context
4885 
4886                 instances_by_host[host].append(instance)
4887                 hosts_by_instance[instance.uuid].append(host)
4888 
4889         for event in events:
4890             if event.name == 'volume-extended':
4891                 # Volume extend is a user-initiated operation starting in the
4892                 # Block Storage service API. We record an instance action so
4893                 # the user can monitor the operation to completion.
4894                 host = hosts_by_instance[event.instance_uuid][0]
4895                 cell_context = cell_contexts_by_host[host]
4896                 objects.InstanceAction.action_start(
4897                     cell_context, event.instance_uuid,
4898                     instance_actions.EXTEND_VOLUME, want_result=False)
4899             for host in hosts_by_instance[event.instance_uuid]:
4900                 events_by_host[host].append(event)
4901 
4902         for host in instances_by_host:
4903             cell_context = cell_contexts_by_host[host]
4904 
4905             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4906             # in order to ensure that a failure in processing events on a host
4907             # will not prevent processing events on other hosts
4908             self.compute_rpcapi.external_instance_event(
4909                 cell_context, instances_by_host[host], events_by_host[host],
4910                 host=host)
4911 
4912     def _get_relevant_hosts(self, context, instance):
4913         hosts = set()
4914         hosts.add(instance.host)
4915         if instance.migration_context is not None:
4916             migration_id = instance.migration_context.migration_id
4917             migration = objects.Migration.get_by_id(context, migration_id)
4918             hosts.add(migration.dest_compute)
4919             hosts.add(migration.source_compute)
4920             LOG.debug('Instance %(instance)s is migrating, '
4921                       'copying events to all relevant hosts: '
4922                       '%(hosts)s', {'instance': instance.uuid,
4923                                     'hosts': hosts})
4924         return hosts
4925 
4926     def get_instance_host_status(self, instance):
4927         if instance.host:
4928             try:
4929                 service = [service for service in instance.services if
4930                            service.binary == 'nova-compute'][0]
4931                 if service.forced_down:
4932                     host_status = fields_obj.HostStatus.DOWN
4933                 elif service.disabled:
4934                     host_status = fields_obj.HostStatus.MAINTENANCE
4935                 else:
4936                     alive = self.servicegroup_api.service_is_up(service)
4937                     host_status = ((alive and fields_obj.HostStatus.UP) or
4938                                    fields_obj.HostStatus.UNKNOWN)
4939             except IndexError:
4940                 host_status = fields_obj.HostStatus.NONE
4941         else:
4942             host_status = fields_obj.HostStatus.NONE
4943         return host_status
4944 
4945     def get_instances_host_statuses(self, instance_list):
4946         host_status_dict = dict()
4947         host_statuses = dict()
4948         for instance in instance_list:
4949             if instance.host:
4950                 if instance.host not in host_status_dict:
4951                     host_status = self.get_instance_host_status(instance)
4952                     host_status_dict[instance.host] = host_status
4953                 else:
4954                     host_status = host_status_dict[instance.host]
4955             else:
4956                 host_status = fields_obj.HostStatus.NONE
4957             host_statuses[instance.uuid] = host_status
4958         return host_statuses
4959 
4960 
4961 def target_host_cell(fn):
4962     """Target a host-based function to a cell.
4963 
4964     Expects to wrap a function of signature:
4965 
4966        func(self, context, host, ...)
4967     """
4968 
4969     @functools.wraps(fn)
4970     def targeted(self, context, host, *args, **kwargs):
4971         mapping = objects.HostMapping.get_by_host(context, host)
4972         nova_context.set_target_cell(context, mapping.cell_mapping)
4973         return fn(self, context, host, *args, **kwargs)
4974     return targeted
4975 
4976 
4977 def _find_service_in_cell(context, service_id=None, service_host=None):
4978     """Find a service by id or hostname by searching all cells.
4979 
4980     If one matching service is found, return it. If none or multiple
4981     are found, raise an exception.
4982 
4983     :param context: A context.RequestContext
4984     :param service_id: If not none, the DB ID of the service to find
4985     :param service_host: If not None, the hostname of the service to find
4986     :returns: An objects.Service
4987     :raises: ServiceNotUnique if multiple matching IDs are found
4988     :raises: NotFound if no matches are found
4989     :raises: NovaException if called with neither search option
4990     """
4991 
4992     load_cells()
4993     service = None
4994     found_in_cell = None
4995 
4996     is_uuid = False
4997     if service_id is not None:
4998         is_uuid = uuidutils.is_uuid_like(service_id)
4999         if is_uuid:
5000             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
5001         else:
5002             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
5003     elif service_host is not None:
5004         lookup_fn = lambda c: (
5005             objects.Service.get_by_compute_host(c, service_host))
5006     else:
5007         LOG.exception('_find_service_in_cell called with no search parameters')
5008         # This is intentionally cryptic so we don't leak implementation details
5009         # out of the API.
5010         raise exception.NovaException()
5011 
5012     for cell in CELLS:
5013         # NOTE(danms): Services can be in cell0, so don't skip it here
5014         try:
5015             with nova_context.target_cell(context, cell) as cctxt:
5016                 cell_service = lookup_fn(cctxt)
5017         except exception.NotFound:
5018             # NOTE(danms): Keep looking in other cells
5019             continue
5020         if service and cell_service:
5021             raise exception.ServiceNotUnique()
5022         service = cell_service
5023         found_in_cell = cell
5024         if service and is_uuid:
5025             break
5026 
5027     if service:
5028         # NOTE(danms): Set the cell on the context so it remains
5029         # when we return to our caller
5030         nova_context.set_target_cell(context, found_in_cell)
5031         return service
5032     else:
5033         raise exception.NotFound()
5034 
5035 
5036 class HostAPI(base.Base):
5037     """Sub-set of the Compute Manager API for managing host operations."""
5038 
5039     def __init__(self, rpcapi=None):
5040         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
5041         self.servicegroup_api = servicegroup.API()
5042         super(HostAPI, self).__init__()
5043 
5044     def _assert_host_exists(self, context, host_name, must_be_up=False):
5045         """Raise HostNotFound if compute host doesn't exist."""
5046         service = objects.Service.get_by_compute_host(context, host_name)
5047         if not service:
5048             raise exception.HostNotFound(host=host_name)
5049         if must_be_up and not self.servicegroup_api.service_is_up(service):
5050             raise exception.ComputeServiceUnavailable(host=host_name)
5051         return service['host']
5052 
5053     @wrap_exception()
5054     @target_host_cell
5055     def set_host_enabled(self, context, host_name, enabled):
5056         """Sets the specified host's ability to accept new instances."""
5057         host_name = self._assert_host_exists(context, host_name)
5058         payload = {'host_name': host_name, 'enabled': enabled}
5059         compute_utils.notify_about_host_update(context,
5060                                                'set_enabled.start',
5061                                                payload)
5062         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
5063                 host=host_name)
5064         compute_utils.notify_about_host_update(context,
5065                                                'set_enabled.end',
5066                                                payload)
5067         return result
5068 
5069     @target_host_cell
5070     def get_host_uptime(self, context, host_name):
5071         """Returns the result of calling "uptime" on the target host."""
5072         host_name = self._assert_host_exists(context, host_name,
5073                          must_be_up=True)
5074         return self.rpcapi.get_host_uptime(context, host=host_name)
5075 
5076     @wrap_exception()
5077     @target_host_cell
5078     def host_power_action(self, context, host_name, action):
5079         """Reboots, shuts down or powers up the host."""
5080         host_name = self._assert_host_exists(context, host_name)
5081         payload = {'host_name': host_name, 'action': action}
5082         compute_utils.notify_about_host_update(context,
5083                                                'power_action.start',
5084                                                payload)
5085         result = self.rpcapi.host_power_action(context, action=action,
5086                 host=host_name)
5087         compute_utils.notify_about_host_update(context,
5088                                                'power_action.end',
5089                                                payload)
5090         return result
5091 
5092     @wrap_exception()
5093     @target_host_cell
5094     def set_host_maintenance(self, context, host_name, mode):
5095         """Start/Stop host maintenance window. On start, it triggers
5096         guest VMs evacuation.
5097         """
5098         host_name = self._assert_host_exists(context, host_name)
5099         payload = {'host_name': host_name, 'mode': mode}
5100         compute_utils.notify_about_host_update(context,
5101                                                'set_maintenance.start',
5102                                                payload)
5103         result = self.rpcapi.host_maintenance_mode(context,
5104                 host_param=host_name, mode=mode, host=host_name)
5105         compute_utils.notify_about_host_update(context,
5106                                                'set_maintenance.end',
5107                                                payload)
5108         return result
5109 
5110     def service_get_all(self, context, filters=None, set_zones=False,
5111                         all_cells=False, cell_down_support=False):
5112         """Returns a list of services, optionally filtering the results.
5113 
5114         If specified, 'filters' should be a dictionary containing services
5115         attributes and matching values.  Ie, to get a list of services for
5116         the 'compute' topic, use filters={'topic': 'compute'}.
5117 
5118         If all_cells=True, then scan all cells and merge the results.
5119 
5120         If cell_down_support=True then return minimal service records
5121         for cells that do not respond based on what we have in the
5122         host mappings. These will have only 'binary' and 'host' set.
5123         """
5124         if filters is None:
5125             filters = {}
5126         disabled = filters.pop('disabled', None)
5127         if 'availability_zone' in filters:
5128             set_zones = True
5129 
5130         # NOTE(danms): Eventually this all_cells nonsense should go away
5131         # and we should always iterate over the cells. However, certain
5132         # callers need the legacy behavior for now.
5133         if all_cells:
5134             services = []
5135             service_dict = nova_context.scatter_gather_all_cells(context,
5136                 objects.ServiceList.get_all, disabled, set_zones=set_zones)
5137             for cell_uuid, service in service_dict.items():
5138                 if not nova_context.is_cell_failure_sentinel(service):
5139                     services.extend(service)
5140                 elif cell_down_support:
5141                     unavailable_services = objects.ServiceList()
5142                     cid = [cm.id for cm in nova_context.CELLS
5143                            if cm.uuid == cell_uuid]
5144                     # We know cid[0] is in the list because we are using the
5145                     # same list that scatter_gather_all_cells used
5146                     hms = objects.HostMappingList.get_by_cell_id(context,
5147                                                                  cid[0])
5148                     for hm in hms:
5149                         unavailable_services.objects.append(objects.Service(
5150                             binary='nova-compute', host=hm.host))
5151                     LOG.warning("Cell %s is not responding and hence only "
5152                                 "partial results are available from this "
5153                                 "cell.", cell_uuid)
5154                     services.extend(unavailable_services)
5155                 else:
5156                     LOG.warning("Cell %s is not responding and hence skipped "
5157                                 "from the results.", cell_uuid)
5158         else:
5159             services = objects.ServiceList.get_all(context, disabled,
5160                                                    set_zones=set_zones)
5161         ret_services = []
5162         for service in services:
5163             for key, val in filters.items():
5164                 if service[key] != val:
5165                     break
5166             else:
5167                 # All filters matched.
5168                 ret_services.append(service)
5169         return ret_services
5170 
5171     def service_get_by_id(self, context, service_id):
5172         """Get service entry for the given service id or uuid."""
5173         try:
5174             return _find_service_in_cell(context, service_id=service_id)
5175         except exception.NotFound:
5176             raise exception.ServiceNotFound(service_id=service_id)
5177 
5178     @target_host_cell
5179     def service_get_by_compute_host(self, context, host_name):
5180         """Get service entry for the given compute hostname."""
5181         return objects.Service.get_by_compute_host(context, host_name)
5182 
5183     def _service_update(self, context, host_name, binary, params_to_update):
5184         """Performs the actual service update operation."""
5185         service = objects.Service.get_by_args(context, host_name, binary)
5186         service.update(params_to_update)
5187         service.save()
5188         return service
5189 
5190     @target_host_cell
5191     def service_update(self, context, host_name, binary, params_to_update):
5192         """Enable / Disable a service.
5193 
5194         For compute services, this stops new builds and migrations going to
5195         the host.
5196         """
5197         return self._service_update(context, host_name, binary,
5198                                     params_to_update)
5199 
5200     def _service_delete(self, context, service_id):
5201         """Performs the actual Service deletion operation."""
5202         try:
5203             service = _find_service_in_cell(context, service_id=service_id)
5204         except exception.NotFound:
5205             raise exception.ServiceNotFound(service_id=service_id)
5206         service.destroy()
5207 
5208     # TODO(mriedem): Nothing outside of tests is using this now so we should
5209     # be able to remove it.
5210     def service_delete(self, context, service_id):
5211         """Deletes the specified service found via id or uuid."""
5212         self._service_delete(context, service_id)
5213 
5214     @target_host_cell
5215     def instance_get_all_by_host(self, context, host_name):
5216         """Return all instances on the given host."""
5217         return objects.InstanceList.get_by_host(context, host_name)
5218 
5219     def task_log_get_all(self, context, task_name, period_beginning,
5220                          period_ending, host=None, state=None):
5221         """Return the task logs within a given range, optionally
5222         filtering by host and/or state.
5223         """
5224         return self.db.task_log_get_all(context, task_name,
5225                                         period_beginning,
5226                                         period_ending,
5227                                         host=host,
5228                                         state=state)
5229 
5230     def compute_node_get(self, context, compute_id):
5231         """Return compute node entry for particular integer ID or UUID."""
5232         load_cells()
5233 
5234         # NOTE(danms): Unfortunately this API exposes database identifiers
5235         # which means we really can't do something efficient here
5236         is_uuid = uuidutils.is_uuid_like(compute_id)
5237         for cell in CELLS:
5238             if cell.uuid == objects.CellMapping.CELL0_UUID:
5239                 continue
5240             with nova_context.target_cell(context, cell) as cctxt:
5241                 try:
5242                     if is_uuid:
5243                         return objects.ComputeNode.get_by_uuid(cctxt,
5244                                                                compute_id)
5245                     return objects.ComputeNode.get_by_id(cctxt,
5246                                                          int(compute_id))
5247                 except exception.ComputeHostNotFound:
5248                     # NOTE(danms): Keep looking in other cells
5249                     continue
5250 
5251         raise exception.ComputeHostNotFound(host=compute_id)
5252 
5253     def compute_node_get_all(self, context, limit=None, marker=None):
5254         load_cells()
5255 
5256         computes = []
5257         uuid_marker = marker and uuidutils.is_uuid_like(marker)
5258         for cell in CELLS:
5259             if cell.uuid == objects.CellMapping.CELL0_UUID:
5260                 continue
5261             with nova_context.target_cell(context, cell) as cctxt:
5262 
5263                 # If we have a marker and it's a uuid, see if the compute node
5264                 # is in this cell.
5265                 if marker and uuid_marker:
5266                     try:
5267                         compute_marker = objects.ComputeNode.get_by_uuid(
5268                             cctxt, marker)
5269                         # we found the marker compute node, so use it's id
5270                         # for the actual marker for paging in this cell's db
5271                         marker = compute_marker.id
5272                     except exception.ComputeHostNotFound:
5273                         # The marker node isn't in this cell so keep looking.
5274                         continue
5275 
5276                 try:
5277                     cell_computes = objects.ComputeNodeList.get_by_pagination(
5278                         cctxt, limit=limit, marker=marker)
5279                 except exception.MarkerNotFound:
5280                     # NOTE(danms): Keep looking through cells
5281                     continue
5282                 computes.extend(cell_computes)
5283                 # NOTE(danms): We must have found the marker, so continue on
5284                 # without one
5285                 marker = None
5286                 if limit:
5287                     limit -= len(cell_computes)
5288                     if limit <= 0:
5289                         break
5290 
5291         if marker is not None and len(computes) == 0:
5292             # NOTE(danms): If we did not find the marker in any cell,
5293             # mimic the db_api behavior here.
5294             raise exception.MarkerNotFound(marker=marker)
5295 
5296         return objects.ComputeNodeList(objects=computes)
5297 
5298     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
5299         load_cells()
5300 
5301         computes = []
5302         for cell in CELLS:
5303             if cell.uuid == objects.CellMapping.CELL0_UUID:
5304                 continue
5305             with nova_context.target_cell(context, cell) as cctxt:
5306                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
5307                     cctxt, hypervisor_match)
5308             computes.extend(cell_computes)
5309         return objects.ComputeNodeList(objects=computes)
5310 
5311     def compute_node_statistics(self, context):
5312         load_cells()
5313 
5314         cell_stats = []
5315         for cell in CELLS:
5316             if cell.uuid == objects.CellMapping.CELL0_UUID:
5317                 continue
5318             with nova_context.target_cell(context, cell) as cctxt:
5319                 cell_stats.append(self.db.compute_node_statistics(cctxt))
5320 
5321         if cell_stats:
5322             keys = cell_stats[0].keys()
5323             return {k: sum(stats[k] for stats in cell_stats)
5324                     for k in keys}
5325         else:
5326             return {}
5327 
5328 
5329 class InstanceActionAPI(base.Base):
5330     """Sub-set of the Compute Manager API for managing instance actions."""
5331 
5332     def actions_get(self, context, instance, limit=None, marker=None,
5333                     filters=None):
5334         return objects.InstanceActionList.get_by_instance_uuid(
5335             context, instance.uuid, limit, marker, filters)
5336 
5337     def action_get_by_request_id(self, context, instance, request_id):
5338         return objects.InstanceAction.get_by_request_id(
5339             context, instance.uuid, request_id)
5340 
5341     def action_events_get(self, context, instance, action_id):
5342         return objects.InstanceActionEventList.get_by_action(
5343             context, action_id)
5344 
5345 
5346 class AggregateAPI(base.Base):
5347     """Sub-set of the Compute Manager API for managing host aggregates."""
5348     def __init__(self, **kwargs):
5349         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5350         self.query_client = query.SchedulerQueryClient()
5351         self._placement_client = None  # Lazy-load on first access.
5352         super(AggregateAPI, self).__init__(**kwargs)
5353 
5354     @property
5355     def placement_client(self):
5356         if self._placement_client is None:
5357             self._placement_client = report.SchedulerReportClient()
5358         return self._placement_client
5359 
5360     @wrap_exception()
5361     def create_aggregate(self, context, aggregate_name, availability_zone):
5362         """Creates the model for the aggregate."""
5363 
5364         aggregate = objects.Aggregate(context=context)
5365         aggregate.name = aggregate_name
5366         if availability_zone:
5367             aggregate.metadata = {'availability_zone': availability_zone}
5368         aggregate.create()
5369         self.query_client.update_aggregates(context, [aggregate])
5370         return aggregate
5371 
5372     def get_aggregate(self, context, aggregate_id):
5373         """Get an aggregate by id."""
5374         return objects.Aggregate.get_by_id(context, aggregate_id)
5375 
5376     def get_aggregate_list(self, context):
5377         """Get all the aggregates."""
5378         return objects.AggregateList.get_all(context)
5379 
5380     def get_aggregates_by_host(self, context, compute_host):
5381         """Get all the aggregates where the given host is presented."""
5382         return objects.AggregateList.get_by_host(context, compute_host)
5383 
5384     @wrap_exception()
5385     def update_aggregate(self, context, aggregate_id, values):
5386         """Update the properties of an aggregate."""
5387         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5388         if 'name' in values:
5389             aggregate.name = values.pop('name')
5390             aggregate.save()
5391         self.is_safe_to_update_az(context, values, aggregate=aggregate,
5392                                   action_name=AGGREGATE_ACTION_UPDATE,
5393                                   check_no_instances_in_az=True)
5394         if values:
5395             aggregate.update_metadata(values)
5396             aggregate.updated_at = timeutils.utcnow()
5397         self.query_client.update_aggregates(context, [aggregate])
5398         # If updated values include availability_zones, then the cache
5399         # which stored availability_zones and host need to be reset
5400         if values.get('availability_zone'):
5401             availability_zones.reset_cache()
5402         return aggregate
5403 
5404     @wrap_exception()
5405     def update_aggregate_metadata(self, context, aggregate_id, metadata):
5406         """Updates the aggregate metadata."""
5407         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5408         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
5409                                   action_name=AGGREGATE_ACTION_UPDATE_META,
5410                                   check_no_instances_in_az=True)
5411         aggregate.update_metadata(metadata)
5412         self.query_client.update_aggregates(context, [aggregate])
5413         # If updated metadata include availability_zones, then the cache
5414         # which stored availability_zones and host need to be reset
5415         if metadata and metadata.get('availability_zone'):
5416             availability_zones.reset_cache()
5417         aggregate.updated_at = timeutils.utcnow()
5418         return aggregate
5419 
5420     @wrap_exception()
5421     def delete_aggregate(self, context, aggregate_id):
5422         """Deletes the aggregate."""
5423         aggregate_payload = {'aggregate_id': aggregate_id}
5424         compute_utils.notify_about_aggregate_update(context,
5425                                                     "delete.start",
5426                                                     aggregate_payload)
5427         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5428 
5429         compute_utils.notify_about_aggregate_action(
5430             context=context,
5431             aggregate=aggregate,
5432             action=fields_obj.NotificationAction.DELETE,
5433             phase=fields_obj.NotificationPhase.START)
5434 
5435         if len(aggregate.hosts) > 0:
5436             msg = _("Host aggregate is not empty")
5437             raise exception.InvalidAggregateActionDelete(
5438                 aggregate_id=aggregate_id, reason=msg)
5439         aggregate.destroy()
5440         self.query_client.delete_aggregate(context, aggregate)
5441         compute_utils.notify_about_aggregate_update(context,
5442                                                     "delete.end",
5443                                                     aggregate_payload)
5444         compute_utils.notify_about_aggregate_action(
5445             context=context,
5446             aggregate=aggregate,
5447             action=fields_obj.NotificationAction.DELETE,
5448             phase=fields_obj.NotificationPhase.END)
5449 
5450     def is_safe_to_update_az(self, context, metadata, aggregate,
5451                              hosts=None,
5452                              action_name=AGGREGATE_ACTION_ADD,
5453                              check_no_instances_in_az=False):
5454         """Determine if updates alter an aggregate's availability zone.
5455 
5456             :param context: local context
5457             :param metadata: Target metadata for updating aggregate
5458             :param aggregate: Aggregate to update
5459             :param hosts: Hosts to check. If None, aggregate.hosts is used
5460             :type hosts: list
5461             :param action_name: Calling method for logging purposes
5462             :param check_no_instances_in_az: if True, it checks
5463                 there is no instances on any hosts of the aggregate
5464 
5465         """
5466         if 'availability_zone' in metadata:
5467             if not metadata['availability_zone']:
5468                 msg = _("Aggregate %s does not support empty named "
5469                         "availability zone") % aggregate.name
5470                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5471                                                   msg)
5472             _hosts = hosts or aggregate.hosts
5473             host_aggregates = objects.AggregateList.get_by_metadata_key(
5474                 context, 'availability_zone', hosts=_hosts)
5475             conflicting_azs = [
5476                 agg.availability_zone for agg in host_aggregates
5477                 if agg.availability_zone != metadata['availability_zone']
5478                 and agg.id != aggregate.id]
5479             if conflicting_azs:
5480                 msg = _("One or more hosts already in availability zone(s) "
5481                         "%s") % conflicting_azs
5482                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
5483                                                   msg)
5484             same_az_name = (aggregate.availability_zone ==
5485                             metadata['availability_zone'])
5486             if check_no_instances_in_az and not same_az_name:
5487                 instance_count_by_cell = (
5488                     nova_context.scatter_gather_skip_cell0(
5489                         context,
5490                         objects.InstanceList.get_count_by_hosts,
5491                         _hosts))
5492                 if any(cnt for cnt in instance_count_by_cell.values()):
5493                     msg = _("One or more hosts contain instances in this zone")
5494                     self._raise_invalid_aggregate_exc(
5495                         action_name, aggregate.id, msg)
5496 
5497     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
5498         if action_name == AGGREGATE_ACTION_ADD:
5499             raise exception.InvalidAggregateActionAdd(
5500                 aggregate_id=aggregate_id, reason=reason)
5501         elif action_name == AGGREGATE_ACTION_UPDATE:
5502             raise exception.InvalidAggregateActionUpdate(
5503                 aggregate_id=aggregate_id, reason=reason)
5504         elif action_name == AGGREGATE_ACTION_UPDATE_META:
5505             raise exception.InvalidAggregateActionUpdateMeta(
5506                 aggregate_id=aggregate_id, reason=reason)
5507         elif action_name == AGGREGATE_ACTION_DELETE:
5508             raise exception.InvalidAggregateActionDelete(
5509                 aggregate_id=aggregate_id, reason=reason)
5510 
5511         raise exception.NovaException(
5512             _("Unexpected aggregate action %s") % action_name)
5513 
5514     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
5515         # Update the availability_zone cache to avoid getting wrong
5516         # availability_zone in cache retention time when add/remove
5517         # host to/from aggregate.
5518         if aggregate_meta and aggregate_meta.get('availability_zone'):
5519             availability_zones.update_host_availability_zone_cache(context,
5520                                                                    host_name)
5521 
5522     @wrap_exception()
5523     def add_host_to_aggregate(self, context, aggregate_id, host_name):
5524         """Adds the host to an aggregate."""
5525         aggregate_payload = {'aggregate_id': aggregate_id,
5526                              'host_name': host_name}
5527         compute_utils.notify_about_aggregate_update(context,
5528                                                     "addhost.start",
5529                                                     aggregate_payload)
5530         # validates the host; HostMappingNotFound or ComputeHostNotFound
5531         # is raised if invalid
5532         try:
5533             mapping = objects.HostMapping.get_by_host(context, host_name)
5534             nova_context.set_target_cell(context, mapping.cell_mapping)
5535             service = objects.Service.get_by_compute_host(context, host_name)
5536         except exception.HostMappingNotFound:
5537             try:
5538                 # NOTE(danms): This targets our cell
5539                 service = _find_service_in_cell(context,
5540                                                 service_host=host_name)
5541             except exception.NotFound:
5542                 raise exception.ComputeHostNotFound(host=host_name)
5543 
5544         if service.host != host_name:
5545             # NOTE(danms): If we found a service but it is not an
5546             # exact match, we may have a case-insensitive backend
5547             # database (like mysql) which will end up with us
5548             # adding the host-aggregate mapping with a
5549             # non-matching hostname.
5550             raise exception.ComputeHostNotFound(host=host_name)
5551 
5552         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5553 
5554         compute_utils.notify_about_aggregate_action(
5555             context=context,
5556             aggregate=aggregate,
5557             action=fields_obj.NotificationAction.ADD_HOST,
5558             phase=fields_obj.NotificationPhase.START)
5559 
5560         self.is_safe_to_update_az(context, aggregate.metadata,
5561                                   hosts=[host_name], aggregate=aggregate)
5562 
5563         aggregate.add_host(host_name)
5564         self.query_client.update_aggregates(context, [aggregate])
5565         try:
5566             self.placement_client.aggregate_add_host(
5567                 context, aggregate.uuid, host_name)
5568         except exception.PlacementAPIConnectFailure:
5569             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5570             # service not communicating with the Placement API, so just log a
5571             # warning here.
5572             # TODO(jaypipes): Remove this in Stein, when placement must be able
5573             # to be contacted from the nova-api service.
5574             LOG.warning("Failed to associate %s with a placement "
5575                         "aggregate: %s. There was a failure to communicate "
5576                         "with the placement service.",
5577                         host_name, aggregate.uuid)
5578         except (exception.ResourceProviderNotFound,
5579                 exception.ResourceProviderAggregateRetrievalFailed,
5580                 exception.ResourceProviderUpdateFailed,
5581                 exception.ResourceProviderUpdateConflict) as err:
5582             # NOTE(jaypipes): We don't want a failure perform the mirroring
5583             # action in the placement service to be returned to the user (they
5584             # probably don't know anything about the placement service and
5585             # would just be confused). So, we just log a warning here, noting
5586             # that on the next run of nova-manage placement sync_aggregates
5587             # things will go back to normal
5588             LOG.warning("Failed to associate %s with a placement "
5589                         "aggregate: %s. This may be corrected after running "
5590                         "nova-manage placement sync_aggregates.",
5591                         host_name, err)
5592         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5593         # NOTE(jogo): Send message to host to support resource pools
5594         self.compute_rpcapi.add_aggregate_host(context,
5595                 aggregate=aggregate, host_param=host_name, host=host_name)
5596         aggregate_payload.update({'name': aggregate.name})
5597         compute_utils.notify_about_aggregate_update(context,
5598                                                     "addhost.end",
5599                                                     aggregate_payload)
5600         compute_utils.notify_about_aggregate_action(
5601             context=context,
5602             aggregate=aggregate,
5603             action=fields_obj.NotificationAction.ADD_HOST,
5604             phase=fields_obj.NotificationPhase.END)
5605 
5606         return aggregate
5607 
5608     @wrap_exception()
5609     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
5610         """Removes host from the aggregate."""
5611         aggregate_payload = {'aggregate_id': aggregate_id,
5612                              'host_name': host_name}
5613         compute_utils.notify_about_aggregate_update(context,
5614                                                     "removehost.start",
5615                                                     aggregate_payload)
5616         # validates the host; HostMappingNotFound or ComputeHostNotFound
5617         # is raised if invalid
5618         mapping = objects.HostMapping.get_by_host(context, host_name)
5619         nova_context.set_target_cell(context, mapping.cell_mapping)
5620         objects.Service.get_by_compute_host(context, host_name)
5621         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
5622 
5623         compute_utils.notify_about_aggregate_action(
5624             context=context,
5625             aggregate=aggregate,
5626             action=fields_obj.NotificationAction.REMOVE_HOST,
5627             phase=fields_obj.NotificationPhase.START)
5628 
5629         aggregate.delete_host(host_name)
5630         self.query_client.update_aggregates(context, [aggregate])
5631         try:
5632             self.placement_client.aggregate_remove_host(
5633                 context, aggregate.uuid, host_name)
5634         except exception.PlacementAPIConnectFailure:
5635             # NOTE(jaypipes): Rocky should be able to tolerate the nova-api
5636             # service not communicating with the Placement API, so just log a
5637             # warning here.
5638             # TODO(jaypipes): Remove this in Stein, when placement must be able
5639             # to be contacted from the nova-api service.
5640             LOG.warning("Failed to remove association of %s with a placement "
5641                         "aggregate: %s. There was a failure to communicate "
5642                         "with the placement service.",
5643                         host_name, aggregate.uuid)
5644         except (exception.ResourceProviderNotFound,
5645                 exception.ResourceProviderAggregateRetrievalFailed,
5646                 exception.ResourceProviderUpdateFailed,
5647                 exception.ResourceProviderUpdateConflict) as err:
5648             # NOTE(jaypipes): We don't want a failure perform the mirroring
5649             # action in the placement service to be returned to the user (they
5650             # probably don't know anything about the placement service and
5651             # would just be confused). So, we just log a warning here, noting
5652             # that on the next run of nova-manage placement sync_aggregates
5653             # things will go back to normal
5654             LOG.warning("Failed to remove association of %s with a placement "
5655                         "aggregate: %s. This may be corrected after running "
5656                         "nova-manage placement sync_aggregates.",
5657                         host_name, err)
5658         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
5659         self.compute_rpcapi.remove_aggregate_host(context,
5660                 aggregate=aggregate, host_param=host_name, host=host_name)
5661         compute_utils.notify_about_aggregate_update(context,
5662                                                     "removehost.end",
5663                                                     aggregate_payload)
5664         compute_utils.notify_about_aggregate_action(
5665             context=context,
5666             aggregate=aggregate,
5667             action=fields_obj.NotificationAction.REMOVE_HOST,
5668             phase=fields_obj.NotificationPhase.END)
5669         return aggregate
5670 
5671 
5672 class KeypairAPI(base.Base):
5673     """Subset of the Compute Manager API for managing key pairs."""
5674 
5675     get_notifier = functools.partial(rpc.get_notifier, service='api')
5676     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
5677                                        get_notifier=get_notifier,
5678                                        binary='nova-api')
5679 
5680     def _notify(self, context, event_suffix, keypair_name):
5681         payload = {
5682             'tenant_id': context.project_id,
5683             'user_id': context.user_id,
5684             'key_name': keypair_name,
5685         }
5686         notify = self.get_notifier()
5687         notify.info(context, 'keypair.%s' % event_suffix, payload)
5688 
5689     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
5690         safe_chars = "_- " + string.digits + string.ascii_letters
5691         clean_value = "".join(x for x in key_name if x in safe_chars)
5692         if clean_value != key_name:
5693             raise exception.InvalidKeypair(
5694                 reason=_("Keypair name contains unsafe characters"))
5695 
5696         try:
5697             utils.check_string_length(key_name, min_length=1, max_length=255)
5698         except exception.InvalidInput:
5699             raise exception.InvalidKeypair(
5700                 reason=_('Keypair name must be string and between '
5701                          '1 and 255 characters long'))
5702         try:
5703             objects.Quotas.check_deltas(context, {'key_pairs': 1}, user_id)
5704         except exception.OverQuota:
5705             raise exception.KeypairLimitExceeded()
5706 
5707     @wrap_exception()
5708     def import_key_pair(self, context, user_id, key_name, public_key,
5709                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5710         """Import a key pair using an existing public key."""
5711         self._validate_new_key_pair(context, user_id, key_name, key_type)
5712 
5713         self._notify(context, 'import.start', key_name)
5714 
5715         keypair = objects.KeyPair(context)
5716         keypair.user_id = user_id
5717         keypair.name = key_name
5718         keypair.type = key_type
5719         keypair.fingerprint = None
5720         keypair.public_key = public_key
5721 
5722         compute_utils.notify_about_keypair_action(
5723             context=context,
5724             keypair=keypair,
5725             action=fields_obj.NotificationAction.IMPORT,
5726             phase=fields_obj.NotificationPhase.START)
5727 
5728         fingerprint = self._generate_fingerprint(public_key, key_type)
5729 
5730         keypair.fingerprint = fingerprint
5731         keypair.create()
5732 
5733         compute_utils.notify_about_keypair_action(
5734             context=context,
5735             keypair=keypair,
5736             action=fields_obj.NotificationAction.IMPORT,
5737             phase=fields_obj.NotificationPhase.END)
5738         self._notify(context, 'import.end', key_name)
5739 
5740         return keypair
5741 
5742     @wrap_exception()
5743     def create_key_pair(self, context, user_id, key_name,
5744                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
5745         """Create a new key pair."""
5746         self._validate_new_key_pair(context, user_id, key_name, key_type)
5747 
5748         keypair = objects.KeyPair(context)
5749         keypair.user_id = user_id
5750         keypair.name = key_name
5751         keypair.type = key_type
5752         keypair.fingerprint = None
5753         keypair.public_key = None
5754 
5755         self._notify(context, 'create.start', key_name)
5756         compute_utils.notify_about_keypair_action(
5757             context=context,
5758             keypair=keypair,
5759             action=fields_obj.NotificationAction.CREATE,
5760             phase=fields_obj.NotificationPhase.START)
5761 
5762         private_key, public_key, fingerprint = self._generate_key_pair(
5763             user_id, key_type)
5764 
5765         keypair.fingerprint = fingerprint
5766         keypair.public_key = public_key
5767         keypair.create()
5768 
5769         # NOTE(melwitt): We recheck the quota after creating the object to
5770         # prevent users from allocating more resources than their allowed quota
5771         # in the event of a race. This is configurable because it can be
5772         # expensive if strict quota limits are not required in a deployment.
5773         if CONF.quota.recheck_quota:
5774             try:
5775                 objects.Quotas.check_deltas(context, {'key_pairs': 0}, user_id)
5776             except exception.OverQuota:
5777                 keypair.destroy()
5778                 raise exception.KeypairLimitExceeded()
5779 
5780         compute_utils.notify_about_keypair_action(
5781             context=context,
5782             keypair=keypair,
5783             action=fields_obj.NotificationAction.CREATE,
5784             phase=fields_obj.NotificationPhase.END)
5785 
5786         self._notify(context, 'create.end', key_name)
5787 
5788         return keypair, private_key
5789 
5790     def _generate_fingerprint(self, public_key, key_type):
5791         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5792             return crypto.generate_fingerprint(public_key)
5793         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5794             return crypto.generate_x509_fingerprint(public_key)
5795 
5796     def _generate_key_pair(self, user_id, key_type):
5797         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
5798             return crypto.generate_key_pair()
5799         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
5800             return crypto.generate_winrm_x509_cert(user_id)
5801 
5802     @wrap_exception()
5803     def delete_key_pair(self, context, user_id, key_name):
5804         """Delete a keypair by name."""
5805         self._notify(context, 'delete.start', key_name)
5806         keypair = self.get_key_pair(context, user_id, key_name)
5807         compute_utils.notify_about_keypair_action(
5808             context=context,
5809             keypair=keypair,
5810             action=fields_obj.NotificationAction.DELETE,
5811             phase=fields_obj.NotificationPhase.START)
5812         objects.KeyPair.destroy_by_name(context, user_id, key_name)
5813         compute_utils.notify_about_keypair_action(
5814             context=context,
5815             keypair=keypair,
5816             action=fields_obj.NotificationAction.DELETE,
5817             phase=fields_obj.NotificationPhase.END)
5818         self._notify(context, 'delete.end', key_name)
5819 
5820     def get_key_pairs(self, context, user_id, limit=None, marker=None):
5821         """List key pairs."""
5822         return objects.KeyPairList.get_by_user(
5823             context, user_id, limit=limit, marker=marker)
5824 
5825     def get_key_pair(self, context, user_id, key_name):
5826         """Get a keypair by name."""
5827         return objects.KeyPair.get_by_name(context, user_id, key_name)
5828 
5829 
5830 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
5831     """Sub-set of the Compute API related to managing security groups
5832     and security group rules
5833     """
5834 
5835     # The nova security group api does not use a uuid for the id.
5836     id_is_uuid = False
5837 
5838     def __init__(self, **kwargs):
5839         super(SecurityGroupAPI, self).__init__(**kwargs)
5840         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
5841 
5842     def validate_property(self, value, property, allowed):
5843         """Validate given security group property.
5844 
5845         :param value:          the value to validate, as a string or unicode
5846         :param property:       the property, either 'name' or 'description'
5847         :param allowed:        the range of characters allowed
5848         """
5849 
5850         try:
5851             val = value.strip()
5852         except AttributeError:
5853             msg = _("Security group %s is not a string or unicode") % property
5854             self.raise_invalid_property(msg)
5855         utils.check_string_length(val, name=property, min_length=1,
5856                                   max_length=255)
5857 
5858         if allowed and not re.match(allowed, val):
5859             # Some validation to ensure that values match API spec.
5860             # - Alphanumeric characters, spaces, dashes, and underscores.
5861             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
5862             #  probably create a param validator that can be used elsewhere.
5863             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
5864                      "invalid. Content limited to '%(allowed)s'.") %
5865                    {'value': value, 'allowed': allowed,
5866                     'property': property.capitalize()})
5867             self.raise_invalid_property(msg)
5868 
5869     def ensure_default(self, context):
5870         """Ensure that a context has a security group.
5871 
5872         Creates a security group for the security context if it does not
5873         already exist.
5874 
5875         :param context: the security context
5876         """
5877         self.db.security_group_ensure_default(context)
5878 
5879     def create_security_group(self, context, name, description):
5880         try:
5881             objects.Quotas.check_deltas(context, {'security_groups': 1},
5882                                         context.project_id,
5883                                         user_id=context.user_id)
5884         except exception.OverQuota:
5885             msg = _("Quota exceeded, too many security groups.")
5886             self.raise_over_quota(msg)
5887 
5888         LOG.info("Create Security Group %s", name)
5889 
5890         self.ensure_default(context)
5891 
5892         group = {'user_id': context.user_id,
5893                  'project_id': context.project_id,
5894                  'name': name,
5895                  'description': description}
5896         try:
5897             group_ref = self.db.security_group_create(context, group)
5898         except exception.SecurityGroupExists:
5899             msg = _('Security group %s already exists') % name
5900             self.raise_group_already_exists(msg)
5901 
5902         # NOTE(melwitt): We recheck the quota after creating the object to
5903         # prevent users from allocating more resources than their allowed quota
5904         # in the event of a race. This is configurable because it can be
5905         # expensive if strict quota limits are not required in a deployment.
5906         if CONF.quota.recheck_quota:
5907             try:
5908                 objects.Quotas.check_deltas(context, {'security_groups': 0},
5909                                             context.project_id,
5910                                             user_id=context.user_id)
5911             except exception.OverQuota:
5912                 self.db.security_group_destroy(context, group_ref['id'])
5913                 msg = _("Quota exceeded, too many security groups.")
5914                 self.raise_over_quota(msg)
5915 
5916         return group_ref
5917 
5918     def update_security_group(self, context, security_group,
5919                                 name, description):
5920         if security_group['name'] in RO_SECURITY_GROUPS:
5921             msg = (_("Unable to update system group '%s'") %
5922                     security_group['name'])
5923             self.raise_invalid_group(msg)
5924 
5925         group = {'name': name,
5926                  'description': description}
5927 
5928         columns_to_join = ['rules.grantee_group']
5929         group_ref = self.db.security_group_update(context,
5930                 security_group['id'],
5931                 group,
5932                 columns_to_join=columns_to_join)
5933         return group_ref
5934 
5935     def get(self, context, name=None, id=None, map_exception=False):
5936         self.ensure_default(context)
5937         cols = ['rules']
5938         try:
5939             if name:
5940                 return self.db.security_group_get_by_name(context,
5941                                                           context.project_id,
5942                                                           name,
5943                                                           columns_to_join=cols)
5944             elif id:
5945                 return self.db.security_group_get(context, id,
5946                                                   columns_to_join=cols)
5947         except exception.NotFound as exp:
5948             if map_exception:
5949                 msg = exp.format_message()
5950                 self.raise_not_found(msg)
5951             else:
5952                 raise
5953 
5954     def list(self, context, names=None, ids=None, project=None,
5955              search_opts=None):
5956         self.ensure_default(context)
5957 
5958         groups = []
5959         if names or ids:
5960             if names:
5961                 for name in names:
5962                     groups.append(self.db.security_group_get_by_name(context,
5963                                                                      project,
5964                                                                      name))
5965             if ids:
5966                 for id in ids:
5967                     groups.append(self.db.security_group_get(context, id))
5968 
5969         elif context.is_admin:
5970             # TODO(eglynn): support a wider set of search options than just
5971             # all_tenants, at least include the standard filters defined for
5972             # the EC2 DescribeSecurityGroups API for the non-admin case also
5973             if (search_opts and 'all_tenants' in search_opts):
5974                 groups = self.db.security_group_get_all(context)
5975             else:
5976                 groups = self.db.security_group_get_by_project(context,
5977                                                                project)
5978 
5979         elif project:
5980             groups = self.db.security_group_get_by_project(context, project)
5981 
5982         return groups
5983 
5984     def destroy(self, context, security_group):
5985         if security_group['name'] in RO_SECURITY_GROUPS:
5986             msg = _("Unable to delete system group '%s'") % \
5987                     security_group['name']
5988             self.raise_invalid_group(msg)
5989 
5990         if self.db.security_group_in_use(context, security_group['id']):
5991             msg = _("Security group is still in use")
5992             self.raise_invalid_group(msg)
5993 
5994         LOG.info("Delete security group %s", security_group['name'])
5995         self.db.security_group_destroy(context, security_group['id'])
5996 
5997     def is_associated_with_server(self, security_group, instance_uuid):
5998         """Check if the security group is already associated
5999            with the instance. If Yes, return True.
6000         """
6001 
6002         if not security_group:
6003             return False
6004 
6005         instances = security_group.get('instances')
6006         if not instances:
6007             return False
6008 
6009         for inst in instances:
6010             if (instance_uuid == inst['uuid']):
6011                 return True
6012 
6013         return False
6014 
6015     def add_to_instance(self, context, instance, security_group_name):
6016         """Add security group to the instance."""
6017         security_group = self.db.security_group_get_by_name(context,
6018                 context.project_id,
6019                 security_group_name)
6020 
6021         instance_uuid = instance.uuid
6022 
6023         # check if the security group is associated with the server
6024         if self.is_associated_with_server(security_group, instance_uuid):
6025             raise exception.SecurityGroupExistsForInstance(
6026                                         security_group_id=security_group['id'],
6027                                         instance_id=instance_uuid)
6028 
6029         self.db.instance_add_security_group(context.elevated(),
6030                                             instance_uuid,
6031                                             security_group['id'])
6032         if instance.host:
6033             self.compute_rpcapi.refresh_instance_security_rules(
6034                     context, instance, instance.host)
6035 
6036     def remove_from_instance(self, context, instance, security_group_name):
6037         """Remove the security group associated with the instance."""
6038         security_group = self.db.security_group_get_by_name(context,
6039                 context.project_id,
6040                 security_group_name)
6041 
6042         instance_uuid = instance.uuid
6043 
6044         # check if the security group is associated with the server
6045         if not self.is_associated_with_server(security_group, instance_uuid):
6046             raise exception.SecurityGroupNotExistsForInstance(
6047                                     security_group_id=security_group['id'],
6048                                     instance_id=instance_uuid)
6049 
6050         self.db.instance_remove_security_group(context.elevated(),
6051                                                instance_uuid,
6052                                                security_group['id'])
6053         if instance.host:
6054             self.compute_rpcapi.refresh_instance_security_rules(
6055                     context, instance, instance.host)
6056 
6057     def get_rule(self, context, id):
6058         self.ensure_default(context)
6059         try:
6060             return self.db.security_group_rule_get(context, id)
6061         except exception.NotFound:
6062             msg = _("Rule (%s) not found") % id
6063             self.raise_not_found(msg)
6064 
6065     def add_rules(self, context, id, name, vals):
6066         """Add security group rule(s) to security group.
6067 
6068         Note: the Nova security group API doesn't support adding multiple
6069         security group rules at once but the EC2 one does. Therefore,
6070         this function is written to support both.
6071         """
6072 
6073         try:
6074             objects.Quotas.check_deltas(context,
6075                                         {'security_group_rules': len(vals)},
6076                                         id)
6077         except exception.OverQuota:
6078             msg = _("Quota exceeded, too many security group rules.")
6079             self.raise_over_quota(msg)
6080 
6081         msg = ("Security group %(name)s added %(protocol)s ingress "
6082                "(%(from_port)s:%(to_port)s)")
6083         rules = []
6084         for v in vals:
6085             rule = self.db.security_group_rule_create(context, v)
6086 
6087             # NOTE(melwitt): We recheck the quota after creating the object to
6088             # prevent users from allocating more resources than their allowed
6089             # quota in the event of a race. This is configurable because it can
6090             # be expensive if strict quota limits are not required in a
6091             # deployment.
6092             if CONF.quota.recheck_quota:
6093                 try:
6094                     objects.Quotas.check_deltas(context,
6095                                                 {'security_group_rules': 0},
6096                                                 id)
6097                 except exception.OverQuota:
6098                     self.db.security_group_rule_destroy(context, rule['id'])
6099                     msg = _("Quota exceeded, too many security group rules.")
6100                     self.raise_over_quota(msg)
6101 
6102             rules.append(rule)
6103             LOG.info(msg, {'name': name,
6104                            'protocol': rule.protocol,
6105                            'from_port': rule.from_port,
6106                            'to_port': rule.to_port})
6107 
6108         self.trigger_rules_refresh(context, id=id)
6109         return rules
6110 
6111     def remove_rules(self, context, security_group, rule_ids):
6112         msg = ("Security group %(name)s removed %(protocol)s ingress "
6113                "(%(from_port)s:%(to_port)s)")
6114         for rule_id in rule_ids:
6115             rule = self.get_rule(context, rule_id)
6116             LOG.info(msg, {'name': security_group['name'],
6117                            'protocol': rule.protocol,
6118                            'from_port': rule.from_port,
6119                            'to_port': rule.to_port})
6120 
6121             self.db.security_group_rule_destroy(context, rule_id)
6122 
6123         # NOTE(vish): we removed some rules, so refresh
6124         self.trigger_rules_refresh(context, id=security_group['id'])
6125 
6126     def remove_default_rules(self, context, rule_ids):
6127         for rule_id in rule_ids:
6128             self.db.security_group_default_rule_destroy(context, rule_id)
6129 
6130     def add_default_rules(self, context, vals):
6131         rules = [self.db.security_group_default_rule_create(context, v)
6132                  for v in vals]
6133         return rules
6134 
6135     def default_rule_exists(self, context, values):
6136         """Indicates whether the specified rule values are already
6137            defined in the default security group rules.
6138         """
6139         for rule in self.db.security_group_default_rule_list(context):
6140             keys = ('cidr', 'from_port', 'to_port', 'protocol')
6141             for key in keys:
6142                 if rule.get(key) != values.get(key):
6143                     break
6144             else:
6145                 return rule.get('id') or True
6146         return False
6147 
6148     def get_all_default_rules(self, context):
6149         try:
6150             rules = self.db.security_group_default_rule_list(context)
6151         except Exception:
6152             msg = 'cannot get default security group rules'
6153             raise exception.SecurityGroupDefaultRuleNotFound(msg)
6154 
6155         return rules
6156 
6157     def get_default_rule(self, context, id):
6158         return self.db.security_group_default_rule_get(context, id)
6159 
6160     def validate_id(self, id):
6161         try:
6162             return int(id)
6163         except ValueError:
6164             msg = _("Security group id should be integer")
6165             self.raise_invalid_property(msg)
6166 
6167     def _refresh_instance_security_rules(self, context, instances):
6168         for instance in instances:
6169             if instance.host is not None:
6170                 self.compute_rpcapi.refresh_instance_security_rules(
6171                         context, instance, instance.host)
6172 
6173     def trigger_rules_refresh(self, context, id):
6174         """Called when a rule is added to or removed from a security_group."""
6175         instances = objects.InstanceList.get_by_security_group_id(context, id)
6176         self._refresh_instance_security_rules(context, instances)
6177 
6178     def trigger_members_refresh(self, context, group_ids):
6179         """Called when a security group gains a new or loses a member.
6180 
6181         Sends an update request to each compute node for each instance for
6182         which this is relevant.
6183         """
6184         instances = objects.InstanceList.get_by_grantee_security_group_ids(
6185             context, group_ids)
6186         self._refresh_instance_security_rules(context, instances)
6187 
6188     def get_instance_security_groups(self, context, instance, detailed=False):
6189         if detailed:
6190             return self.db.security_group_get_by_instance(context,
6191                                                           instance.uuid)
6192         return [{'name': group.name} for group in instance.security_groups]
