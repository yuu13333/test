Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import copy
32 import functools
33 import inspect
34 import sys
35 import time
36 import traceback
37 import typing as ty
38 
39 from cinderclient import exceptions as cinder_exception
40 from cursive import exception as cursive_exception
41 import eventlet.event
42 from eventlet import greenthread
43 import eventlet.semaphore
44 import eventlet.timeout
45 import futurist
46 from keystoneauth1 import exceptions as keystone_exception
47 import os_traits
48 from oslo_log import log as logging
49 import oslo_messaging as messaging
50 from oslo_serialization import jsonutils
51 from oslo_service import loopingcall
52 from oslo_service import periodic_task
53 from oslo_utils import excutils
54 from oslo_utils import strutils
55 from oslo_utils import timeutils
56 from oslo_utils import units
57 
58 from nova.accelerator import cyborg
59 from nova import block_device
60 from nova.compute import api as compute
61 from nova.compute import build_results
62 from nova.compute import claims
63 from nova.compute import power_state
64 from nova.compute import resource_tracker
65 from nova.compute import rpcapi as compute_rpcapi
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 from nova.compute.utils import wrap_instance_event
69 from nova.compute import vm_states
70 from nova import conductor
71 import nova.conf
72 import nova.context
73 from nova import exception
74 from nova import exception_wrapper
75 from nova.i18n import _
76 from nova.image import glance
77 from nova import manager
78 from nova.network import model as network_model
79 from nova.network import neutron
80 from nova import objects
81 from nova.objects import base as obj_base
82 from nova.objects import external_event as external_event_obj
83 from nova.objects import fields
84 from nova.objects import instance as obj_instance
85 from nova.objects import migrate_data as migrate_data_obj
86 from nova.pci import request as pci_req_module
87 from nova.pci import whitelist
88 from nova import safe_utils
89 from nova.scheduler.client import query
90 from nova.scheduler.client import report
91 from nova.scheduler import utils as scheduler_utils
92 from nova import utils
93 from nova.virt import block_device as driver_block_device
94 from nova.virt import configdrive
95 from nova.virt import driver
96 from nova.virt import event as virtevent
97 from nova.virt import hardware
98 from nova.virt import storage_users
99 from nova.virt import virtapi
100 from nova.volume import cinder
101 
102 CONF = nova.conf.CONF
103 
104 LOG = logging.getLogger(__name__)
105 
106 wrap_exception = functools.partial(
107     exception_wrapper.wrap_exception, service='compute', binary='nova-compute')
108 
109 
110 @contextlib.contextmanager
111 def errors_out_migration_ctxt(migration):
112     """Context manager to error out migration on failure."""
113 
114     try:
115         yield
116     except Exception:
117         with excutils.save_and_reraise_exception():
118             if migration:
119                 # We may have been passed None for our migration if we're
120                 # receiving from an older client. The migration will be
121                 # errored via the legacy path.
122                 migration.status = 'error'
123                 try:
124                     migration.save()
125                 except Exception:
126                     LOG.debug(
127                         'Error setting migration status for instance %s.',
128                         migration.instance_uuid, exc_info=True)
129 
130 
131 @utils.expects_func_args('migration')
132 def errors_out_migration(function):
133     """Decorator to error out migration on failure."""
134 
135     @functools.wraps(function)
136     def decorated_function(self, context, *args, **kwargs):
137         wrapped_func = safe_utils.get_wrapped_function(function)
138         keyed_args = inspect.getcallargs(wrapped_func, self, context,
139                                          *args, **kwargs)
140         migration = keyed_args['migration']
141         with errors_out_migration_ctxt(migration):
142             return function(self, context, *args, **kwargs)
143 
144     return decorated_function
145 
146 
147 @utils.expects_func_args('instance')
148 def reverts_task_state(function):
149     """Decorator to revert task_state on failure."""
150 
151     @functools.wraps(function)
152     def decorated_function(self, context, *args, **kwargs):
153         try:
154             return function(self, context, *args, **kwargs)
155         except exception.UnexpectedTaskStateError as e:
156             # Note(maoy): unexpected task state means the current
157             # task is preempted. Do not clear task state in this
158             # case.
159             with excutils.save_and_reraise_exception():
160                 LOG.info("Task possibly preempted: %s",
161                          e.format_message())
162         except Exception:
163             with excutils.save_and_reraise_exception():
164                 wrapped_func = safe_utils.get_wrapped_function(function)
165                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
166                                                  *args, **kwargs)
167                 # NOTE(mriedem): 'instance' must be in keyed_args because we
168                 # have utils.expects_func_args('instance') decorating this
169                 # method.
170                 instance = keyed_args['instance']
171                 original_task_state = instance.task_state
172                 try:
173                     self._instance_update(context, instance, task_state=None)
174                     LOG.info("Successfully reverted task state from %s on "
175                              "failure for instance.",
176                              original_task_state, instance=instance)
177                 except exception.InstanceNotFound:
178                     # We might delete an instance that failed to build shortly
179                     # after it errored out this is an expected case and we
180                     # should not trace on it.
181                     pass
182                 except Exception as e:
183                     LOG.warning("Failed to revert task state for instance. "
184                                 "Error: %s", e, instance=instance)
185 
186     return decorated_function
187 
188 
189 @utils.expects_func_args('instance')
190 def wrap_instance_fault(function):
191     """Wraps a method to catch exceptions related to instances.
192 
193     This decorator wraps a method to catch any exceptions having to do with
194     an instance that may get thrown. It then logs an instance fault in the db.
195     """
196 
197     @functools.wraps(function)
198     def decorated_function(self, context, *args, **kwargs):
199         try:
200             return function(self, context, *args, **kwargs)
201         except exception.InstanceNotFound:
202             raise
203         except Exception as e:
204             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
205             # we will get a KeyError exception which will cover up the real
206             # exception. So, we update kwargs with the values from args first.
207             # then, we can get 'instance' from kwargs easily.
208             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
209 
210             with excutils.save_and_reraise_exception():
211                 compute_utils.add_instance_fault_from_exc(context,
212                         kwargs['instance'], e, sys.exc_info())
213 
214     return decorated_function
215 
216 
217 @utils.expects_func_args('image_id', 'instance')
218 def delete_image_on_error(function):
219     """Used for snapshot related method to ensure the image created in
220     compute.api is deleted when an error occurs.
221     """
222 
223     @functools.wraps(function)
224     def decorated_function(self, context, image_id, instance,
225                            *args, **kwargs):
226         try:
227             return function(self, context, image_id, instance,
228                             *args, **kwargs)
229         except Exception:
230             with excutils.save_and_reraise_exception():
231                 compute_utils.delete_image(
232                     context, instance, self.image_api, image_id,
233                     log_exc_info=True)
234 
235     return decorated_function
236 
237 
238 # Each collection of events is a dict of eventlet Events keyed by a tuple of
239 # event name and associated tag
240 _InstanceEvents = ty.Dict[ty.Tuple[str, str], eventlet.event.Event]
241 
242 
243 class InstanceEvents(object):
244     def __init__(self):
245         self._events: ty.Optional[ty.Dict[str, _InstanceEvents]] = {}
246 
247     @staticmethod
248     def _lock_name(instance) -> str:
249         return '%s-%s' % (instance.uuid, 'events')
250 
251     def prepare_for_instance_event(
252         self,
253         instance: 'objects.Instance',
254         name: str,
255         tag: str,
256     ) -> eventlet.event.Event:
257         """Prepare to receive an event for an instance.
258 
259         This will register an event for the given instance that we will
260         wait on later. This should be called before initiating whatever
261         action will trigger the event. The resulting eventlet.event.Event
262         object should be wait()'d on to ensure completion.
263 
264         :param instance: the instance for which the event will be generated
265         :param name: the name of the event we're expecting
266         :param tag: the tag associated with the event we're expecting
267         :returns: an event object that should be wait()'d on
268         """
269         @utils.synchronized(self._lock_name(instance))
270         def _create_or_get_event():
271             if self._events is None:
272                 # NOTE(danms): We really should have a more specific error
273                 # here, but this is what we use for our default error case
274                 raise exception.NovaException(
275                     'In shutdown, no new events can be scheduled')
276 
277             instance_events = self._events.setdefault(instance.uuid, {})
278             return instance_events.setdefault((name, tag),
279                                               eventlet.event.Event())
280         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
281                   {'name': name, 'tag': tag}, instance=instance)
282         return _create_or_get_event()
283 
284     def pop_instance_event(self, instance, event):
285         """Remove a pending event from the wait list.
286 
287         This will remove a pending event from the wait list so that it
288         can be used to signal the waiters to wake up.
289 
290         :param instance: the instance for which the event was generated
291         :param event: the nova.objects.external_event.InstanceExternalEvent
292                       that describes the event
293         :returns: the eventlet.event.Event object on which the waiters
294                   are blocked
295         """
296         no_events_sentinel = object()
297         no_matching_event_sentinel = object()
298 
299         @utils.synchronized(self._lock_name(instance))
300         def _pop_event():
301             if self._events is None:
302                 LOG.debug('Unexpected attempt to pop events during shutdown',
303                           instance=instance)
304                 return no_events_sentinel
305             events = self._events.get(instance.uuid)
306             if not events:
307                 return no_events_sentinel
308             _event = events.pop((event.name, event.tag), None)
309             if not events:
310                 del self._events[instance.uuid]
311             if _event is None:
312                 return no_matching_event_sentinel
313             return _event
314 
315         result = _pop_event()
316         if result is no_events_sentinel:
317             LOG.debug('No waiting events found dispatching %(event)s',
318                       {'event': event.key},
319                       instance=instance)
320             return None
321         elif result is no_matching_event_sentinel:
322             LOG.debug(
323                 'No event matching %(event)s in %(events)s',
324                 {
325                     'event': event.key,
326                     # mypy can't identify the none check in _pop_event
327                     'events': self._events.get(  # type: ignore
328                         instance.uuid, {}).keys(),
329                 },
330                 instance=instance,
331             )
332             return None
333         else:
334             return result
335 
336     def clear_events_for_instance(self, instance):
337         """Remove all pending events for an instance.
338 
339         This will remove all events currently pending for an instance
340         and return them (indexed by event name).
341 
342         :param instance: the instance for which events should be purged
343         :returns: a dictionary of {event_name: eventlet.event.Event}
344         """
345         @utils.synchronized(self._lock_name(instance))
346         def _clear_events():
347             if self._events is None:
348                 LOG.debug('Unexpected attempt to clear events during shutdown',
349                           instance=instance)
350                 return dict()
351             # NOTE(danms): We have historically returned the raw internal
352             # format here, which is {event.key: [events, ...])} so just
353             # trivially convert it here.
354             return {'%s-%s' % k: e
355                     for k, e in self._events.pop(instance.uuid, {}).items()}
356         return _clear_events()
357 
358     def cancel_all_events(self):
359         if self._events is None:
360             LOG.debug('Unexpected attempt to cancel events during shutdown.')
361             return
362         our_events = self._events
363         # NOTE(danms): Block new events
364         self._events = None
365 
366         for instance_uuid, events in our_events.items():
367             for (name, tag), eventlet_event in events.items():
368                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
369                           'instance %(instance_uuid)s',
370                           {'name': name,
371                            'tag': tag,
372                            'instance_uuid': instance_uuid})
373                 event = objects.InstanceExternalEvent(
374                     instance_uuid=instance_uuid,
375                     name=name, status='failed',
376                     tag=tag, data={})
377                 eventlet_event.send(event)
378 
379 
380 class ComputeVirtAPI(virtapi.VirtAPI):
381     def __init__(self, compute):
382         super(ComputeVirtAPI, self).__init__()
383         self._compute = compute
384         self.reportclient = compute.reportclient
385 
386         class ExitEarly(Exception):
387             def __init__(self, events):
388                 super(Exception, self).__init__()
389                 self.events = events
390 
391         self._exit_early_exc = ExitEarly
392 
393     def exit_wait_early(self, events):
394         """Exit a wait_for_instance_event() immediately and avoid
395         waiting for some events.
396 
397         :param: events: A list of (name, tag) tuples for events that we should
398                         skip waiting for during a wait_for_instance_event().
399         """
400         raise self._exit_early_exc(events=events)
401 
402     def _default_error_callback(self, event_name, instance):
403         raise exception.NovaException(_('Instance event failed'))
404 
405     @contextlib.contextmanager
406     def wait_for_instance_event(self, instance, event_names, deadline=300,
407                                 error_callback=None):
408         """Plan to wait for some events, run some code, then wait.
409 
410         This context manager will first create plans to wait for the
411         provided event_names, yield, and then wait for all the scheduled
412         events to complete.
413 
414         Note that this uses an eventlet.timeout.Timeout to bound the
415         operation, so callers should be prepared to catch that
416         failure and handle that situation appropriately.
417 
418         If the event is not received by the specified timeout deadline,
419         eventlet.timeout.Timeout is raised.
420 
421         If the event is received but did not have a 'completed'
422         status, a NovaException is raised.  If an error_callback is
423         provided, instead of raising an exception as detailed above
424         for the failure case, the callback will be called with the
425         event_name and instance, and can return True to continue
426         waiting for the rest of the events, False to stop processing,
427         or raise an exception which will bubble up to the waiter.
428 
429         If the inner code wishes to abort waiting for one or more
430         events because it knows some state to be finished or condition
431         to be satisfied, it can use VirtAPI.exit_wait_early() with a
432         list of event (name,tag) items to avoid waiting for those
433         events upon context exit. Note that exit_wait_early() exits
434         the context immediately and should be used to signal that all
435         work has been completed and provide the unified list of events
436         that need not be waited for. Waiting for the remaining events
437         will begin immediately upon early exit as if the context was
438         exited normally.
439 
440         :param instance: The instance for which an event is expected
441         :param event_names: A list of event names. Each element is a
442                             tuple of strings to indicate (name, tag),
443                             where name is required, but tag may be None.
444         :param deadline: Maximum number of seconds we should wait for all
445                          of the specified events to arrive.
446         :param error_callback: A function to be called if an event arrives
447 
448         """
449 
450         if error_callback is None:
451             error_callback = self._default_error_callback
452         events = {}
453         for event_name in event_names:
454             name, tag = event_name
455             event_name = objects.InstanceExternalEvent.make_key(name, tag)
456             try:
457                 events[event_name] = (
458                     self._compute.instance_events.prepare_for_instance_event(
459                         instance, name, tag))
460             except exception.NovaException:
461                 error_callback(event_name, instance)
462                 # NOTE(danms): Don't wait for any of the events. They
463                 # should all be canceled and fired immediately below,
464                 # but don't stick around if not.
465                 deadline = 0
466         try:
467             yield
468         except self._exit_early_exc as e:
469             early_events = set([objects.InstanceExternalEvent.make_key(n, t)
470                                 for n, t in e.events])
471         else:
472             early_events = set([])
473 
474         with eventlet.timeout.Timeout(deadline):
475             for event_name, event in events.items():
476                 if event_name in early_events:
477                     continue
478                 else:
479                     actual_event = event.wait()
480                     if actual_event.status == 'completed':
481                         continue
482                 # If we get here, we have an event that was not completed,
483                 # nor skipped via exit_wait_early(). Decide whether to
484                 # keep waiting by calling the error_callback() hook.
485                 decision = error_callback(event_name, instance)
486                 if decision is False:
487                     break
488 
489     def update_compute_provider_status(self, context, rp_uuid, enabled):
490         """Used to add/remove the COMPUTE_STATUS_DISABLED trait on the provider
491 
492         :param context: nova auth RequestContext
493         :param rp_uuid: UUID of a compute node resource provider in Placement
494         :param enabled: True if the node is enabled in which case the trait
495             would be removed, False if the node is disabled in which case
496             the trait would be added.
497         :raises: ResourceProviderTraitRetrievalFailed
498         :raises: ResourceProviderUpdateConflict
499         :raises: ResourceProviderUpdateFailed
500         :raises: TraitRetrievalFailed
501         :raises: keystoneauth1.exceptions.ClientException
502         """
503         trait_name = os_traits.COMPUTE_STATUS_DISABLED
504         # Get the current traits (and generation) for the provider.
505         # TODO(mriedem): Leverage the ProviderTree cache in get_provider_traits
506         trait_info = self.reportclient.get_provider_traits(context, rp_uuid)
507         # If the host is enabled, remove the trait (if set), else add
508         # the trait if it doesn't already exist.
509         original_traits = trait_info.traits
510         new_traits = None
511         if enabled and trait_name in original_traits:
512             new_traits = original_traits - {trait_name}
513             LOG.debug('Removing trait %s from compute node resource '
514                       'provider %s in placement.', trait_name, rp_uuid)
515         elif not enabled and trait_name not in original_traits:
516             new_traits = original_traits | {trait_name}
517             LOG.debug('Adding trait %s to compute node resource '
518                       'provider %s in placement.', trait_name, rp_uuid)
519 
520         if new_traits is not None:
521             self.reportclient.set_traits_for_provider(
522                 context, rp_uuid, new_traits, generation=trait_info.generation)
523 
524 
525 class ComputeManager(manager.Manager):
526     """Manages the running instances from creation to destruction."""
527 
528     target = messaging.Target(version='5.13')
529 
530     def __init__(self, compute_driver=None, *args, **kwargs):
531         """Load configuration options and connect to the hypervisor."""
532         # We want the ComputeManager, ResourceTracker and ComputeVirtAPI all
533         # using the same instance of SchedulerReportClient which has the
534         # ProviderTree cache for this compute service.
535         self.reportclient = report.SchedulerReportClient()
536         self.virtapi = ComputeVirtAPI(self)
537         self.network_api = neutron.API()
538         self.volume_api = cinder.API()
539         self.image_api = glance.API()
540         self._last_bw_usage_poll = 0.0
541         self.compute_api = compute.API()
542         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
543         self.compute_task_api = conductor.ComputeTaskAPI()
544         self.query_client = query.SchedulerQueryClient()
545         self.instance_events = InstanceEvents()
546         self._sync_power_pool = eventlet.GreenPool(
547             size=CONF.sync_power_state_pool_size)
548         self._syncs_in_progress = {}
549         self.send_instance_updates = (
550             CONF.filter_scheduler.track_instance_changes)
551         if CONF.max_concurrent_builds != 0:
552             self._build_semaphore = eventlet.semaphore.Semaphore(
553                 CONF.max_concurrent_builds)
554         else:
555             self._build_semaphore = compute_utils.UnlimitedSemaphore()
556         if CONF.max_concurrent_snapshots > 0:
557             self._snapshot_semaphore = eventlet.semaphore.Semaphore(
558                 CONF.max_concurrent_snapshots)
559         else:
560             self._snapshot_semaphore = compute_utils.UnlimitedSemaphore()
561         if CONF.max_concurrent_live_migrations > 0:
562             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
563                 max_workers=CONF.max_concurrent_live_migrations)
564         else:
565             # CONF.max_concurrent_live_migrations is 0 (unlimited)
566             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
567         # This is a dict, keyed by instance uuid, to a two-item tuple of
568         # migration object and Future for the queued live migration.
569         self._waiting_live_migrations = {}
570 
571         super(ComputeManager, self).__init__(service_name="compute",
572                                              *args, **kwargs)
573 
574         # NOTE(russellb) Load the driver last.  It may call back into the
575         # compute manager via the virtapi, so we want it to be fully
576         # initialized before that happens.
577         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
578         self.use_legacy_block_device_info = \
579                             self.driver.need_legacy_block_device_info
580         self.rt = resource_tracker.ResourceTracker(
581             self.host, self.driver, reportclient=self.reportclient)
582 
583     def reset(self):
584         LOG.info('Reloading compute RPC API')
585         compute_rpcapi.reset_globals()
586         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
587         self.reportclient.clear_provider_cache()
588 
589     def _update_resource_tracker(self, context, instance):
590         """Let the resource tracker know that an instance has changed state."""
591 
592         if instance.host == self.host:
593             self.rt.update_usage(context, instance, instance.node)
594 
595     def _instance_update(self, context, instance, **kwargs):
596         """Update an instance in the database using kwargs as value."""
597 
598         for k, v in kwargs.items():
599             setattr(instance, k, v)
600         instance.save()
601         self._update_resource_tracker(context, instance)
602 
603     def _nil_out_instance_obj_host_and_node(self, instance):
604         # NOTE(jwcroppe): We don't do instance.save() here for performance
605         # reasons; a call to this is expected to be immediately followed by
606         # another call that does instance.save(), thus avoiding two writes
607         # to the database layer.
608         instance.host = None
609         instance.node = None
610         # ResourceTracker._set_instance_host_and_node also sets launched_on
611         # to the same value as host and is really only ever used by legacy
612         # nova-network code, but we should also null it out to avoid confusion
613         # if there is an instance in the database with no host set but
614         # launched_on is set. Note that we do not care about using launched_on
615         # as some kind of debug helper if diagnosing a build failure, that is
616         # what instance action events are for.
617         instance.launched_on = None
618         # If the instance is not on a host, it's not in an aggregate and
619         # therefore is not in an availability zone.
620         instance.availability_zone = None
621 
622     def _set_instance_obj_error_state(self, instance, clean_task_state=False):
623         try:
624             instance.vm_state = vm_states.ERROR
625             if clean_task_state:
626                 instance.task_state = None
627             instance.save()
628         except exception.InstanceNotFound:
629             LOG.debug('Instance has been destroyed from under us while '
630                       'trying to set it to ERROR', instance=instance)
631 
632     def _get_instances_on_driver(self, context, filters=None):
633         """Return a list of instance records for the instances found
634         on the hypervisor which satisfy the specified filters. If filters=None
635         return a list of instance records for all the instances found on the
636         hypervisor.
637         """
638         if not filters:
639             filters = {}
640         try:
641             driver_uuids = self.driver.list_instance_uuids()
642             if len(driver_uuids) == 0:
643                 # Short circuit, don't waste a DB call
644                 return objects.InstanceList()
645             filters['uuid'] = driver_uuids
646             local_instances = objects.InstanceList.get_by_filters(
647                 context, filters, use_slave=True)
648             return local_instances
649         except NotImplementedError:
650             pass
651 
652         # The driver doesn't support uuids listing, so we'll have
653         # to brute force.
654         driver_instances = self.driver.list_instances()
655         # NOTE(mjozefcz): In this case we need to apply host filter.
656         # Without this all instance data would be fetched from db.
657         filters['host'] = self.host
658         instances = objects.InstanceList.get_by_filters(context, filters,
659                                                         use_slave=True)
660         name_map = {instance.name: instance for instance in instances}
661         local_instances = []
662         for driver_instance in driver_instances:
663             instance = name_map.get(driver_instance)
664             if not instance:
665                 continue
666             local_instances.append(instance)
667         return local_instances
668 
669     def _destroy_evacuated_instances(self, context, node_cache):
670         """Destroys evacuated instances.
671 
672         While nova-compute was down, the instances running on it could be
673         evacuated to another host. This method looks for evacuation migration
674         records where this is the source host and which were either started
675         (accepted), in-progress (pre-migrating) or migrated (done). From those
676         migration records, local instances reported by the hypervisor are
677         compared to the instances for the migration records and those local
678         guests are destroyed, along with instance allocation records in
679         Placement for this node.
680         Then allocations are removed from Placement for every instance that is
681         evacuated from this host regardless if the instance is reported by the
682         hypervisor or not.
683 
684         :param context: The request context
685         :param node_cache: A dict of ComputeNode objects keyed by the UUID of
686             the compute node
687         :return: A dict keyed by instance uuid mapped to Migration objects
688             for instances that were migrated away from this host
689         """
690         filters = {
691             'source_compute': self.host,
692             # NOTE(mriedem): Migration records that have been accepted are
693             # included in case the source node comes back up while instances
694             # are being evacuated to another host. We don't want the same
695             # instance being reported from multiple hosts.
696             # NOTE(lyarwood): pre-migrating is also included here as the
697             # source compute can come back online shortly after the RT
698             # claims on the destination that in-turn moves the migration to
699             # pre-migrating. If the evacuate fails on the destination host,
700             # the user can rebuild the instance (in ERROR state) on the source
701             # host.
702             'status': ['accepted', 'pre-migrating', 'done'],
703             'migration_type': fields.MigrationType.EVACUATION,
704         }
705         with utils.temporary_mutation(context, read_deleted='yes'):
706             evacuations = objects.MigrationList.get_by_filters(context,
707                                                                filters)
708         if not evacuations:
709             return {}
710         evacuations = {mig.instance_uuid: mig for mig in evacuations}
711 
712         # TODO(mriedem): We could optimize by pre-loading the joined fields
713         # we know we'll use, like info_cache and flavor.
714         local_instances = self._get_instances_on_driver(context)
715         evacuated_local_instances = {inst.uuid: inst
716                                      for inst in local_instances
717                                      if inst.uuid in evacuations}
718 
719         for instance in evacuated_local_instances.values():
720             LOG.info('Destroying instance as it has been evacuated from '
721                      'this host but still exists in the hypervisor',
722                      instance=instance)
723             try:
724                 network_info = self.network_api.get_instance_nw_info(
725                     context, instance)
726                 bdi = self._get_instance_block_device_info(context,
727                                                            instance)
728                 destroy_disks = not (self._is_instance_storage_shared(
729                     context, instance))
730             except exception.InstanceNotFound:
731                 network_info = network_model.NetworkInfo()
732                 bdi = {}
733                 LOG.info('Instance has been marked deleted already, '
734                          'removing it from the hypervisor.',
735                          instance=instance)
736                 # always destroy disks if the instance was deleted
737                 destroy_disks = True
738             self.driver.destroy(context, instance,
739                                 network_info,
740                                 bdi, destroy_disks)
741 
742         hostname_to_cn_uuid = {
743             cn.hypervisor_hostname: cn.uuid
744             for cn in node_cache.values()}
745 
746         for instance_uuid, migration in evacuations.items():
747             try:
748                 if instance_uuid in evacuated_local_instances:
749                     # Avoid the db call if we already have the instance loaded
750                     # above
751                     instance = evacuated_local_instances[instance_uuid]
752                 else:
753                     instance = objects.Instance.get_by_uuid(
754                         context, instance_uuid)
755             except exception.InstanceNotFound:
756                 # The instance already deleted so we expect that every
757                 # allocation of that instance has already been cleaned up
758                 continue
759 
760             LOG.info('Cleaning up allocations of the instance as it has been '
761                      'evacuated from this host',
762                      instance=instance)
763             if migration.source_node not in hostname_to_cn_uuid:
764                 LOG.error("Failed to clean allocation of evacuated "
765                           "instance as the source node %s is not found",
766                           migration.source_node, instance=instance)
767                 continue
768             cn_uuid = hostname_to_cn_uuid[migration.source_node]
769 
770             # If the instance was deleted in the interim, assume its
771             # allocations were properly cleaned up (either by its hosting
772             # compute service or the API).
773             if (not instance.deleted and
774                     not self.reportclient.
775                         remove_provider_tree_from_instance_allocation(
776                             context, instance.uuid, cn_uuid)):
777                 LOG.error("Failed to clean allocation of evacuated instance "
778                           "on the source node %s",
779                           cn_uuid, instance=instance)
780 
781             migration.status = 'completed'
782             migration.save()
783         return evacuations
784 
785     def _is_instance_storage_shared(self, context, instance, host=None):
786         shared_storage = True
787         data = None
788         try:
789             data = self.driver.check_instance_shared_storage_local(context,
790                                                        instance)
791             if data:
792                 shared_storage = (self.compute_rpcapi.
793                                   check_instance_shared_storage(context,
794                                   instance, data, host=host))
795         except NotImplementedError:
796             LOG.debug('Hypervisor driver does not support '
797                       'instance shared storage check, '
798                       'assuming it\'s not on shared storage',
799                       instance=instance)
800             shared_storage = False
801         except Exception:
802             LOG.exception('Failed to check if instance shared',
803                           instance=instance)
804         finally:
805             if data:
806                 self.driver.check_instance_shared_storage_cleanup(context,
807                                                                   data)
808         return shared_storage
809 
810     def _complete_partial_deletion(self, context, instance):
811         """Complete deletion for instances in DELETED status but not marked as
812         deleted in the DB
813         """
814         instance.destroy()
815         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
816                 context, instance.uuid)
817         self._complete_deletion(context,
818                                 instance)
819         self._notify_about_instance_usage(context, instance, "delete.end")
820         compute_utils.notify_about_instance_action(context, instance,
821                 self.host, action=fields.NotificationAction.DELETE,
822                 phase=fields.NotificationPhase.END, bdms=bdms)
823 
824     def _complete_deletion(self, context, instance):
825         self._update_resource_tracker(context, instance)
826 
827         self.reportclient.delete_allocation_for_instance(context,
828                                                          instance.uuid)
829 
830         self._clean_instance_console_tokens(context, instance)
831         self._delete_scheduler_instance_info(context, instance.uuid)
832 
833     def _validate_pinning_configuration(self, instances):
834         if not self.driver.capabilities.get('supports_pcpus', False):
835             return
836 
837         for instance in instances:
838             # ignore deleted instances
839             if instance.deleted:
840                 continue
841 
842             # if this is an unpinned instance and the host only has
843             # 'cpu_dedicated_set' configured, we need to tell the operator to
844             # correct their configuration
845             if not instance.numa_topology or (
846                 instance.numa_topology.cpu_policy in (
847                     None, fields.CPUAllocationPolicy.SHARED
848                 )
849             ):
850                 # we don't need to check 'vcpu_pin_set' since it can't coexist
851                 # alongside 'cpu_dedicated_set'
852                 if (CONF.compute.cpu_dedicated_set and
853                         not CONF.compute.cpu_shared_set):
854                     msg = _("This host has unpinned instances but has no CPUs "
855                             "set aside for this purpose; configure '[compute] "
856                             "cpu_shared_set' instead of, or in addition to, "
857                             "'[compute] cpu_dedicated_set'")
858                     raise exception.InvalidConfiguration(msg)
859 
860                 continue
861 
862             # ditto for pinned instances if only 'cpu_shared_set' is configured
863             if (CONF.compute.cpu_shared_set and
864                     not CONF.compute.cpu_dedicated_set and
865                     not CONF.vcpu_pin_set):
866                 msg = _("This host has pinned instances but has no CPUs "
867                         "set aside for this purpose; configure '[compute] "
868                         "cpu_dedicated_set' instead of, or in addition to, "
869                         "'[compute] cpu_shared_set'.")
870                 raise exception.InvalidConfiguration(msg)
871 
872             # if this is a mixed instance with both pinned and unpinned CPUs,
873             # the host must have both 'cpu_dedicated_set' and 'cpu_shared_set'
874             # configured. check if 'cpu_shared_set' is set.
875             if (instance.numa_topology.cpu_policy ==
876                     fields.CPUAllocationPolicy.MIXED and
877                     not CONF.compute.cpu_shared_set):
878                 msg = _("This host has mixed instance requesting both pinned "
879                         "and unpinned CPUs but hasn't set aside unpinned CPUs "
880                         "for this purpose; Configure "
881                         "'[compute] cpu_shared_set'.")
882                 raise exception.InvalidConfiguration(msg)
883 
884             # for mixed instance check if 'cpu_dedicated_set' is set.
885             if (instance.numa_topology.cpu_policy ==
886                     fields.CPUAllocationPolicy.MIXED and
887                     not CONF.compute.cpu_dedicated_set):
888                 msg = _("This host has mixed instance requesting both pinned "
889                         "and unpinned CPUs but hasn't set aside pinned CPUs "
890                         "for this purpose; Configure "
891                         "'[compute] cpu_dedicated_set'")
892                 raise exception.InvalidConfiguration(msg)
893 
894             # also check to make sure the operator hasn't accidentally
895             # dropped some cores that instances are currently using
896             available_dedicated_cpus = (hardware.get_vcpu_pin_set() or
897                                         hardware.get_cpu_dedicated_set())
898             pinned_cpus = instance.numa_topology.cpu_pinning
899             if available_dedicated_cpus and (
900                     pinned_cpus - available_dedicated_cpus):
901                 # we can't raise an exception because of bug #1289064,
902                 # which meant we didn't recalculate CPU pinning information
903                 # when we live migrated a pinned instance
904                 LOG.warning(
905                     "Instance is pinned to host CPUs %(cpus)s "
906                     "but one or more of these CPUs are not included in "
907                     "either '[compute] cpu_dedicated_set' or "
908                     "'vcpu_pin_set'; you should update these "
909                     "configuration options to include the missing CPUs "
910                     "or rebuild or cold migrate this instance.",
911                     {'cpus': list(pinned_cpus)},
912                     instance=instance)
913 
914     def _validate_vtpm_configuration(self, instances):
915         if self.driver.capabilities.get('supports_vtpm', False):
916             return
917 
918         for instance in instances:
919             if instance.deleted:
920                 continue
921 
922             # NOTE(stephenfin): We don't have an attribute on the instance to
923             # check for this, so we need to inspect the flavor/image metadata
924             if hardware.get_vtpm_constraint(
925                 instance.flavor, instance.image_meta,
926             ):
927                 msg = _(
928                     'This host has instances with the vTPM feature enabled, '
929                     'but the host is not correctly configured; enable '
930                     'vTPM support.'
931                 )
932                 raise exception.InvalidConfiguration(msg)
933 
934     def _reset_live_migration(self, context, instance):
935         migration = None
936         try:
937             migration = objects.Migration.get_by_instance_and_status(
938                                       context, instance.uuid, 'running')
939             if migration:
940                 self.live_migration_abort(context, instance, migration.id)
941         except Exception:
942             LOG.exception('Failed to abort live-migration',
943                           instance=instance)
944         finally:
945             if migration:
946                 self._set_migration_status(migration, 'error')
947             LOG.info('Instance found in migrating state during '
948                      'startup. Resetting task_state',
949                      instance=instance)
950             instance.task_state = None
951             instance.save(expected_task_state=[task_states.MIGRATING])
952 
953     def _init_instance(self, context, instance):
954         """Initialize this instance during service init."""
955 
956         # NOTE(danms): If the instance appears to not be owned by this
957         # host, it may have been evacuated away, but skipped by the
958         # evacuation cleanup code due to configuration. Thus, if that
959         # is a possibility, don't touch the instance in any way, but
960         # log the concern. This will help avoid potential issues on
961         # startup due to misconfiguration.
962         if instance.host != self.host:
963             LOG.warning('Instance %(uuid)s appears to not be owned '
964                         'by this host, but by %(host)s. Startup '
965                         'processing is being skipped.',
966                         {'uuid': instance.uuid,
967                          'host': instance.host})
968             return
969 
970         # Instances that are shut down, or in an error state can not be
971         # initialized and are not attempted to be recovered. The exception
972         # to this are instances that are in RESIZE_MIGRATING or DELETING,
973         # which are dealt with further down.
974         if (instance.vm_state == vm_states.SOFT_DELETED or
975             (instance.vm_state == vm_states.ERROR and
976             instance.task_state not in
977             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
978             LOG.debug("Instance is in %s state.",
979                       instance.vm_state, instance=instance)
980             return
981 
982         if instance.vm_state == vm_states.DELETED:
983             try:
984                 self._complete_partial_deletion(context, instance)
985             except Exception:
986                 # we don't want that an exception blocks the init_host
987                 LOG.exception('Failed to complete a deletion',
988                               instance=instance)
989             return
990 
991         if (instance.vm_state == vm_states.BUILDING or
992             instance.task_state in [task_states.SCHEDULING,
993                                     task_states.BLOCK_DEVICE_MAPPING,
994                                     task_states.NETWORKING,
995                                     task_states.SPAWNING]):
996             # NOTE(dave-mcnally) compute stopped before instance was fully
997             # spawned so set to ERROR state. This is safe to do as the state
998             # may be set by the api but the host is not so if we get here the
999             # instance has already been scheduled to this particular host.
1000             LOG.debug("Instance failed to spawn correctly, "
1001                       "setting to ERROR state", instance=instance)
1002             self._set_instance_obj_error_state(instance, clean_task_state=True)
1003             return
1004 
1005         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
1006             instance.task_state in [task_states.REBUILDING,
1007                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
1008                                     task_states.REBUILD_SPAWNING]):
1009             # NOTE(jichenjc) compute stopped before instance was fully
1010             # spawned so set to ERROR state. This is consistent to BUILD
1011             LOG.debug("Instance failed to rebuild correctly, "
1012                       "setting to ERROR state", instance=instance)
1013             self._set_instance_obj_error_state(instance, clean_task_state=True)
1014             return
1015 
1016         if (instance.vm_state != vm_states.ERROR and
1017             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
1018                                     task_states.IMAGE_PENDING_UPLOAD,
1019                                     task_states.IMAGE_UPLOADING,
1020                                     task_states.IMAGE_SNAPSHOT]):
1021             LOG.debug("Instance in transitional state %s at start-up "
1022                       "clearing task state",
1023                       instance.task_state, instance=instance)
1024             instance.task_state = None
1025             instance.save()
1026 
1027         if (instance.vm_state != vm_states.ERROR and
1028             instance.task_state in [task_states.RESIZE_PREP]):
1029             LOG.debug("Instance in transitional state %s at start-up "
1030                       "clearing task state",
1031                       instance['task_state'], instance=instance)
1032             instance.task_state = None
1033             instance.save()
1034 
1035         if instance.task_state == task_states.DELETING:
1036             try:
1037                 LOG.info('Service started deleting the instance during '
1038                          'the previous run, but did not finish. Restarting'
1039                          ' the deletion now.', instance=instance)
1040                 instance.obj_load_attr('metadata')
1041                 instance.obj_load_attr('system_metadata')
1042                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1043                         context, instance.uuid)
1044                 self._delete_instance(context, instance, bdms)
1045             except Exception:
1046                 # we don't want that an exception blocks the init_host
1047                 LOG.exception('Failed to complete a deletion',
1048                               instance=instance)
1049                 self._set_instance_obj_error_state(instance)
1050             return
1051 
1052         current_power_state = self._get_power_state(instance)
1053         try_reboot, reboot_type = self._retry_reboot(
1054             instance, current_power_state)
1055 
1056         if try_reboot:
1057             LOG.debug("Instance in transitional state (%(task_state)s) at "
1058                       "start-up and power state is (%(power_state)s), "
1059                       "triggering reboot",
1060                       {'task_state': instance.task_state,
1061                        'power_state': current_power_state},
1062                       instance=instance)
1063 
1064             # NOTE(mikal): if the instance was doing a soft reboot that got as
1065             # far as shutting down the instance but not as far as starting it
1066             # again, then we've just become a hard reboot. That means the
1067             # task state for the instance needs to change so that we're in one
1068             # of the expected task states for a hard reboot.
1069             if (instance.task_state in task_states.soft_reboot_states and
1070                 reboot_type == 'HARD'):
1071                 instance.task_state = task_states.REBOOT_PENDING_HARD
1072                 instance.save()
1073 
1074             self.reboot_instance(context, instance, block_device_info=None,
1075                                  reboot_type=reboot_type)
1076             return
1077 
1078         elif (current_power_state == power_state.RUNNING and
1079               instance.task_state in [task_states.REBOOT_STARTED,
1080                                       task_states.REBOOT_STARTED_HARD,
1081                                       task_states.PAUSING,
1082                                       task_states.UNPAUSING]):
1083             LOG.warning("Instance in transitional state "
1084                         "(%(task_state)s) at start-up and power state "
1085                         "is (%(power_state)s), clearing task state",
1086                         {'task_state': instance.task_state,
1087                          'power_state': current_power_state},
1088                         instance=instance)
1089             instance.task_state = None
1090             instance.vm_state = vm_states.ACTIVE
1091             instance.save()
1092         elif (current_power_state == power_state.PAUSED and
1093               instance.task_state == task_states.UNPAUSING):
1094             LOG.warning("Instance in transitional state "
1095                         "(%(task_state)s) at start-up and power state "
1096                         "is (%(power_state)s), clearing task state "
1097                         "and unpausing the instance",
1098                         {'task_state': instance.task_state,
1099                          'power_state': current_power_state},
1100                         instance=instance)
1101             try:
1102                 self.unpause_instance(context, instance)
1103             except NotImplementedError:
1104                 # Some virt driver didn't support pause and unpause
1105                 pass
1106             except Exception:
1107                 LOG.exception('Failed to unpause instance', instance=instance)
1108             return
1109 
1110         if instance.task_state == task_states.POWERING_OFF:
1111             try:
1112                 LOG.debug("Instance in transitional state %s at start-up "
1113                           "retrying stop request",
1114                           instance.task_state, instance=instance)
1115                 self.stop_instance(context, instance, True)
1116             except Exception:
1117                 # we don't want that an exception blocks the init_host
1118                 LOG.exception('Failed to stop instance', instance=instance)
1119             return
1120 
1121         if instance.task_state == task_states.POWERING_ON:
1122             try:
1123                 LOG.debug("Instance in transitional state %s at start-up "
1124                           "retrying start request",
1125                           instance.task_state, instance=instance)
1126                 self.start_instance(context, instance)
1127             except Exception:
1128                 # we don't want that an exception blocks the init_host
1129                 LOG.exception('Failed to start instance', instance=instance)
1130             return
1131 
1132         net_info = instance.get_network_info()
1133         try:
1134             self.driver.plug_vifs(instance, net_info)
1135         except NotImplementedError as e:
1136             LOG.debug(e, instance=instance)
1137         except exception.VirtualInterfacePlugException:
1138             # NOTE(mriedem): If we get here, it could be because the vif_type
1139             # in the cache is "binding_failed" or "unbound".
1140             # The periodic task _heal_instance_info_cache checks for this
1141             # condition. It should fix this by binding the ports again when
1142             # it gets to this instance.
1143             LOG.exception('Virtual interface plugging failed for instance. '
1144                           'The port binding:host_id may need to be manually '
1145                           'updated.', instance=instance)
1146             self._set_instance_obj_error_state(instance)
1147             return
1148 
1149         if instance.task_state == task_states.RESIZE_MIGRATING:
1150             # We crashed during resize/migration, so roll back for safety
1151             try:
1152                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
1153                 # not in system_metadata we default to True for backwards
1154                 # compatibility
1155                 power_on = (instance.system_metadata.get('old_vm_state') !=
1156                             vm_states.STOPPED)
1157 
1158                 block_dev_info = self._get_instance_block_device_info(context,
1159                                                                       instance)
1160 
1161                 migration = objects.Migration.get_by_id_and_instance(
1162                     context, instance.migration_context.migration_id,
1163                     instance.uuid)
1164                 self.driver.finish_revert_migration(context, instance,
1165                     net_info, migration, block_dev_info, power_on)
1166 
1167             except Exception:
1168                 LOG.exception('Failed to revert crashed migration',
1169                               instance=instance)
1170             finally:
1171                 LOG.info('Instance found in migrating state during '
1172                          'startup. Resetting task_state',
1173                          instance=instance)
1174                 instance.task_state = None
1175                 instance.save()
1176         if instance.task_state == task_states.MIGRATING:
1177             # Live migration did not complete, but instance is on this
1178             # host. Abort ongoing migration if still running and reset state.
1179             self._reset_live_migration(context, instance)
1180 
1181         db_state = instance.power_state
1182         drv_state = self._get_power_state(instance)
1183         expect_running = (db_state == power_state.RUNNING and
1184                           drv_state != db_state)
1185 
1186         LOG.debug('Current state is %(drv_state)s, state in DB is '
1187                   '%(db_state)s.',
1188                   {'drv_state': drv_state, 'db_state': db_state},
1189                   instance=instance)
1190 
1191         if expect_running and CONF.resume_guests_state_on_host_boot:
1192             self._resume_guests_state(context, instance, net_info)
1193 
1194     def _resume_guests_state(self, context, instance, net_info):
1195         LOG.info('Rebooting instance after nova-compute restart.',
1196                  instance=instance)
1197         block_device_info = \
1198             self._get_instance_block_device_info(context, instance)
1199 
1200         try:
1201             self.driver.resume_state_on_host_boot(
1202                 context, instance, net_info, block_device_info)
1203         except NotImplementedError:
1204             LOG.warning('Hypervisor driver does not support '
1205                         'resume guests', instance=instance)
1206         except Exception:
1207             # NOTE(vish): The instance failed to resume, so we set the
1208             #             instance to error and attempt to continue.
1209             LOG.warning('Failed to resume instance',
1210                         instance=instance)
1211             self._set_instance_obj_error_state(instance)
1212 
1213     def _retry_reboot(self, instance, current_power_state):
1214         current_task_state = instance.task_state
1215         retry_reboot = False
1216         reboot_type = compute_utils.get_reboot_type(current_task_state,
1217                                                     current_power_state)
1218 
1219         pending_soft = (
1220             current_task_state == task_states.REBOOT_PENDING and
1221             instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1222         pending_hard = (
1223             current_task_state == task_states.REBOOT_PENDING_HARD and
1224             instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1225         started_not_running = (current_task_state in
1226                                [task_states.REBOOT_STARTED,
1227                                 task_states.REBOOT_STARTED_HARD] and
1228                                current_power_state != power_state.RUNNING)
1229 
1230         if pending_soft or pending_hard or started_not_running:
1231             retry_reboot = True
1232 
1233         return retry_reboot, reboot_type
1234 
1235     def handle_lifecycle_event(self, event):
1236         LOG.info("VM %(state)s (Lifecycle Event)",
1237                  {'state': event.get_name()},
1238                  instance_uuid=event.get_instance_uuid())
1239         context = nova.context.get_admin_context(read_deleted='yes')
1240         vm_power_state = None
1241         event_transition = event.get_transition()
1242         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1243             vm_power_state = power_state.SHUTDOWN
1244         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1245             vm_power_state = power_state.RUNNING
1246         elif event_transition in (
1247                 virtevent.EVENT_LIFECYCLE_PAUSED,
1248                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1249                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1250             vm_power_state = power_state.PAUSED
1251         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1252             vm_power_state = power_state.RUNNING
1253         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1254             vm_power_state = power_state.SUSPENDED
1255         else:
1256             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1257 
1258         migrate_finish_statuses = {
1259             # This happens on the source node and indicates live migration
1260             # entered post-copy mode.
1261             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1262             # Suspended for offline migration.
1263             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1264         }
1265 
1266         expected_attrs = []
1267         if event_transition in migrate_finish_statuses:
1268             # Join on info_cache since that's needed in migrate_instance_start.
1269             expected_attrs.append('info_cache')
1270         instance = objects.Instance.get_by_uuid(context,
1271                                                 event.get_instance_uuid(),
1272                                                 expected_attrs=expected_attrs)
1273 
1274         # Note(lpetrut): The event may be delayed, thus not reflecting
1275         # the current instance power state. In that case, ignore the event.
1276         current_power_state = self._get_power_state(instance)
1277         if current_power_state == vm_power_state:
1278             LOG.debug('Synchronizing instance power state after lifecycle '
1279                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1280                       'current task_state: %(task_state)s, current DB '
1281                       'power_state: %(db_power_state)s, VM power_state: '
1282                       '%(vm_power_state)s',
1283                       {'event': event.get_name(),
1284                        'vm_state': instance.vm_state,
1285                        'task_state': instance.task_state,
1286                        'db_power_state': instance.power_state,
1287                        'vm_power_state': vm_power_state},
1288                       instance_uuid=instance.uuid)
1289             self._sync_instance_power_state(context,
1290                                             instance,
1291                                             vm_power_state)
1292 
1293         # The following checks are for live migration. We want to activate
1294         # the port binding for the destination host before the live migration
1295         # is resumed on the destination host in order to reduce network
1296         # downtime. Otherwise the ports are bound to the destination host
1297         # in post_live_migration_at_destination.
1298         # TODO(danms): Explore options for using a different live migration
1299         # specific callback for this instead of piggy-backing on the
1300         # handle_lifecycle_event callback.
1301         if (instance.task_state == task_states.MIGRATING and
1302                 event_transition in migrate_finish_statuses):
1303             status = migrate_finish_statuses[event_transition]
1304             try:
1305                 migration = objects.Migration.get_by_instance_and_status(
1306                             context, instance.uuid, status)
1307                 LOG.debug('Binding ports to destination host: %s',
1308                           migration.dest_compute, instance=instance)
1309                 # For neutron, migrate_instance_start will activate the
1310                 # destination host port bindings, if there are any created by
1311                 # conductor before live migration started.
1312                 self.network_api.migrate_instance_start(
1313                     context, instance, migration)
1314             except exception.MigrationNotFoundByStatus:
1315                 LOG.warning("Unable to find migration record with status "
1316                             "'%s' for instance. Port binding will happen in "
1317                             "post live migration.", status, instance=instance)
1318 
1319     def handle_events(self, event):
1320         if isinstance(event, virtevent.LifecycleEvent):
1321             try:
1322                 self.handle_lifecycle_event(event)
1323             except exception.InstanceNotFound:
1324                 LOG.debug("Event %s arrived for non-existent instance. The "
1325                           "instance was probably deleted.", event)
1326         else:
1327             LOG.debug("Ignoring event %s", event)
1328 
1329     def init_virt_events(self):
1330         if CONF.workarounds.handle_virt_lifecycle_events:
1331             self.driver.register_event_listener(self.handle_events)
1332         else:
1333             # NOTE(mriedem): If the _sync_power_states periodic task is
1334             # disabled we should emit a warning in the logs.
1335             if CONF.sync_power_state_interval < 0:
1336                 LOG.warning('Instance lifecycle events from the compute '
1337                             'driver have been disabled. Note that lifecycle '
1338                             'changes to an instance outside of the compute '
1339                             'service will not be synchronized '
1340                             'automatically since the _sync_power_states '
1341                             'periodic task is also disabled.')
1342             else:
1343                 LOG.info('Instance lifecycle events from the compute '
1344                          'driver have been disabled. Note that lifecycle '
1345                          'changes to an instance outside of the compute '
1346                          'service will only be synchronized by the '
1347                          '_sync_power_states periodic task.')
1348 
1349     def _get_nodes(self, context):
1350         """Queried the ComputeNode objects from the DB that are reported by the
1351         hypervisor.
1352 
1353         :param context: the request context
1354         :return: a dict of ComputeNode objects keyed by the UUID of the given
1355             node.
1356         """
1357         nodes_by_uuid = {}
1358         try:
1359             node_names = self.driver.get_available_nodes()
1360         except exception.VirtDriverNotReady:
1361             LOG.warning(
1362                 "Virt driver is not ready. If this is the first time this "
1363                 "service is starting on this host, then you can ignore this "
1364                 "warning.")
1365             return {}
1366 
1367         for node_name in node_names:
1368             try:
1369                 node = objects.ComputeNode.get_by_host_and_nodename(
1370                     context, self.host, node_name)
1371                 nodes_by_uuid[node.uuid] = node
1372             except exception.ComputeHostNotFound:
1373                 LOG.warning(
1374                     "Compute node %s not found in the database. If this is "
1375                     "the first time this service is starting on this host, "
1376                     "then you can ignore this warning.", node_name)
1377         return nodes_by_uuid
1378 
1379     def init_host(self):
1380         """Initialization for a standalone compute service."""
1381 
1382         if CONF.pci.passthrough_whitelist:
1383             # Simply loading the PCI passthrough whitelist will do a bunch of
1384             # validation that would otherwise wait until the PciDevTracker is
1385             # constructed when updating available resources for the compute
1386             # node(s) in the resource tracker, effectively killing that task.
1387             # So load up the whitelist when starting the compute service to
1388             # flush any invalid configuration early so we can kill the service
1389             # if the configuration is wrong.
1390             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1391 
1392         nova.conf.neutron.register_dynamic_opts(CONF)
1393         # Even if only libvirt uses them, make it available for all drivers
1394         nova.conf.devices.register_dynamic_opts(CONF)
1395 
1396         # Override the number of concurrent disk operations allowed if the
1397         # user has specified a limit.
1398         if CONF.compute.max_concurrent_disk_ops != 0:
1399             compute_utils.disk_ops_semaphore = \
1400                 eventlet.semaphore.BoundedSemaphore(
1401                     CONF.compute.max_concurrent_disk_ops)
1402 
1403         if CONF.compute.max_disk_devices_to_attach == 0:
1404             msg = _('[compute]max_disk_devices_to_attach has been set to 0, '
1405                     'which will prevent instances from being able to boot. '
1406                     'Set -1 for unlimited or set >= 1 to limit the maximum '
1407                     'number of disk devices.')
1408             raise exception.InvalidConfiguration(msg)
1409 
1410         self.driver.init_host(host=self.host)
1411         context = nova.context.get_admin_context()
1412         instances = objects.InstanceList.get_by_host(
1413             context, self.host,
1414             expected_attrs=['info_cache', 'metadata', 'numa_topology'])
1415 
1416         self.init_virt_events()
1417 
1418         self._validate_pinning_configuration(instances)
1419         self._validate_vtpm_configuration(instances)
1420 
1421         # NOTE(gibi): At this point the compute_nodes of the resource tracker
1422         # has not been populated yet so we cannot rely on the resource tracker
1423         # here.
1424         # NOTE(gibi): If ironic and vcenter virt driver slow start time
1425         # becomes problematic here then we should consider adding a config
1426         # option or a driver flag to tell us if we should thread
1427         # _destroy_evacuated_instances and
1428         # _error_out_instances_whose_build_was_interrupted out in the
1429         # background on startup
1430         nodes_by_uuid = self._get_nodes(context)
1431 
1432         try:
1433             # checking that instance was not already evacuated to other host
1434             evacuated_instances = self._destroy_evacuated_instances(
1435                 context, nodes_by_uuid)
1436 
1437             # Initialise instances on the host that are not evacuating
1438             for instance in instances:
1439                 if instance.uuid not in evacuated_instances:
1440                     self._init_instance(context, instance)
1441 
1442             # NOTE(gibi): collect all the instance uuids that is in some way
1443             # was already handled above. Either by init_instance or by
1444             # _destroy_evacuated_instances. This way we can limit the scope of
1445             # the _error_out_instances_whose_build_was_interrupted call to look
1446             # only for instances that have allocations on this node and not
1447             # handled by the above calls.
1448             already_handled = {instance.uuid for instance in instances}.union(
1449                 evacuated_instances)
1450             self._error_out_instances_whose_build_was_interrupted(
1451                 context, already_handled, nodes_by_uuid.keys())
1452 
1453         finally:
1454             if instances:
1455                 # We only send the instance info to the scheduler on startup
1456                 # if there is anything to send, otherwise this host might
1457                 # not be mapped yet in a cell and the scheduler may have
1458                 # issues dealing with the information. Later changes to
1459                 # instances on this host will update the scheduler, or the
1460                 # _sync_scheduler_instance_info periodic task will.
1461                 self._update_scheduler_instance_info(context, instances)
1462 
1463     def _error_out_instances_whose_build_was_interrupted(
1464             self, context, already_handled_instances, node_uuids):
1465         """If there are instances in BUILDING state that are not
1466         assigned to this host but have allocations in placement towards
1467         this compute that means the nova-compute service was
1468         restarted while those instances waited for the resource claim
1469         to finish and the _set_instance_host_and_node() to update the
1470         instance.host field. We need to push them to ERROR state here to
1471         prevent keeping them in BUILDING state forever.
1472 
1473         :param context: The request context
1474         :param already_handled_instances: The set of instance UUIDs that the
1475             host initialization process already handled in some way.
1476         :param node_uuids: The list of compute node uuids handled by this
1477             service
1478         """
1479 
1480         # Strategy:
1481         # 1) Get the allocations from placement for our compute node(s)
1482         # 2) Remove the already handled instances from the consumer list;
1483         #    they are either already initialized or need to be skipped.
1484         # 3) Check which remaining consumer is an instance in BUILDING state
1485         #    and push it to ERROR state.
1486 
1487         LOG.info(
1488             "Looking for unclaimed instances stuck in BUILDING status for "
1489             "nodes managed by this host")
1490         for cn_uuid in node_uuids:
1491             try:
1492                 f = self.reportclient.get_allocations_for_resource_provider
1493                 allocations = f(context, cn_uuid).allocations
1494             except (exception.ResourceProviderAllocationRetrievalFailed,
1495                     keystone_exception.ClientException) as e:
1496                 LOG.error(
1497                     "Could not retrieve compute node resource provider %s and "
1498                     "therefore unable to error out any instances stuck in "
1499                     "BUILDING state. Error: %s", cn_uuid, str(e))
1500                 continue
1501 
1502             not_handled_consumers = (set(allocations) -
1503                                      already_handled_instances)
1504 
1505             if not not_handled_consumers:
1506                 continue
1507 
1508             filters = {
1509                 'vm_state': vm_states.BUILDING,
1510                 'uuid': not_handled_consumers
1511             }
1512 
1513             instances = objects.InstanceList.get_by_filters(
1514                 context, filters, expected_attrs=[])
1515 
1516             for instance in instances:
1517                 LOG.debug(
1518                     "Instance spawn was interrupted before instance_claim, "
1519                     "setting instance to ERROR state", instance=instance)
1520                 self._set_instance_obj_error_state(
1521                     instance, clean_task_state=True)
1522 
1523     def cleanup_host(self):
1524         self.driver.register_event_listener(None)
1525         self.instance_events.cancel_all_events()
1526         self.driver.cleanup_host(host=self.host)
1527         self._cleanup_live_migrations_in_pool()
1528 
1529     def _cleanup_live_migrations_in_pool(self):
1530         # Shutdown the pool so we don't get new requests.
1531         self._live_migration_executor.shutdown(wait=False)
1532         # For any queued migrations, cancel the migration and update
1533         # its status.
1534         for migration, future in self._waiting_live_migrations.values():
1535             # If we got here before the Future was submitted then we need
1536             # to move on since there isn't anything we can do.
1537             if future is None:
1538                 continue
1539             if future.cancel():
1540                 self._set_migration_status(migration, 'cancelled')
1541                 LOG.info('Successfully cancelled queued live migration.',
1542                          instance_uuid=migration.instance_uuid)
1543             else:
1544                 LOG.warning('Unable to cancel live migration.',
1545                             instance_uuid=migration.instance_uuid)
1546         self._waiting_live_migrations.clear()
1547 
1548     def pre_start_hook(self):
1549         """After the service is initialized, but before we fully bring
1550         the service up by listening on RPC queues, make sure to update
1551         our available resources (and indirectly our available nodes).
1552         """
1553         self.update_available_resource(nova.context.get_admin_context(),
1554                                        startup=True)
1555 
1556     def _get_power_state(self, instance):
1557         """Retrieve the power state for the given instance."""
1558         LOG.debug('Checking state', instance=instance)
1559         try:
1560             return self.driver.get_info(instance, use_cache=False).state
1561         except exception.InstanceNotFound:
1562             return power_state.NOSTATE
1563 
1564     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1565     def get_console_topic(self, context):
1566         """Retrieves the console host for a project on this host.
1567 
1568         Currently this is just set in the flags for each compute host.
1569 
1570         """
1571         # TODO(mdragon): perhaps make this variable by console_type?
1572         return 'console.%s' % CONF.console_host
1573 
1574     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1575     @wrap_exception()
1576     def get_console_pool_info(self, context, console_type):
1577         raise NotImplementedError()
1578 
1579     # TODO(stephenfin): Remove this as it's nova-network only
1580     @wrap_exception()
1581     def refresh_instance_security_rules(self, context, instance):
1582         """Tell the virtualization driver to refresh security rules for
1583         an instance.
1584 
1585         Passes straight through to the virtualization driver.
1586 
1587         Synchronize the call because we may still be in the middle of
1588         creating the instance.
1589         """
1590         pass
1591 
1592     def _await_block_device_map_created(self, context, vol_id):
1593         # TODO(yamahata): creating volume simultaneously
1594         #                 reduces creation time?
1595         # TODO(yamahata): eliminate dumb polling
1596         start = time.time()
1597         retries = CONF.block_device_allocate_retries
1598         # (1) if the configured value is 0, one attempt should be made
1599         # (2) if the configured value is > 0, then the total number attempts
1600         #      is (retries + 1)
1601         attempts = 1
1602         if retries >= 1:
1603             attempts = retries + 1
1604         for attempt in range(1, attempts + 1):
1605             volume = self.volume_api.get(context, vol_id)
1606             volume_status = volume['status']
1607             if volume_status not in ['creating', 'downloading']:
1608                 if volume_status == 'available':
1609                     return attempt
1610                 LOG.warning("Volume id: %(vol_id)s finished being "
1611                             "created but its status is %(vol_status)s.",
1612                             {'vol_id': vol_id,
1613                              'vol_status': volume_status})
1614                 break
1615             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1616         raise exception.VolumeNotCreated(volume_id=vol_id,
1617                                          seconds=int(time.time() - start),
1618                                          attempts=attempt,
1619                                          volume_status=volume_status)
1620 
1621     def _decode_files(self, injected_files):
1622         """Base64 decode the list of files to inject."""
1623         if not injected_files:
1624             return []
1625 
1626         def _decode(f):
1627             path, contents = f
1628             # Py3 raises binascii.Error instead of TypeError as in Py27
1629             try:
1630                 decoded = base64.b64decode(contents)
1631                 return path, decoded
1632             except (TypeError, binascii.Error):
1633                 raise exception.Base64Exception(path=path)
1634 
1635         return [_decode(f) for f in injected_files]
1636 
1637     def _validate_instance_group_policy(self, context, instance,
1638                                         scheduler_hints):
1639         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1640         # However, there is a race condition with the enforcement of
1641         # the policy.  Since more than one instance may be scheduled at the
1642         # same time, it's possible that more than one instance with an
1643         # anti-affinity policy may end up here.  It's also possible that
1644         # multiple instances with an affinity policy could end up on different
1645         # hosts.  This is a validation step to make sure that starting the
1646         # instance here doesn't violate the policy.
1647         group_hint = scheduler_hints.get('group')
1648         if not group_hint:
1649             return
1650 
1651         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1652         # to check the type on the value and pull the single entry out. The
1653         # API request schema validates that the 'group' hint is a single value.
1654         if isinstance(group_hint, list):
1655             group_hint = group_hint[0]
1656 
1657         @utils.synchronized(group_hint)
1658         def _do_validation(context, instance, group_hint):
1659             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1660             if group.policy and 'anti-affinity' == group.policy:
1661                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1662                     context, self.host)
1663                 ins_on_host = set(instances_uuids)
1664                 members = set(group.members)
1665                 # Determine the set of instance group members on this host
1666                 # which are not the instance in question. This is used to
1667                 # determine how many other members from the same anti-affinity
1668                 # group can be on this host.
1669                 members_on_host = ins_on_host & members - set([instance.uuid])
1670                 rules = group.rules
1671                 if rules and 'max_server_per_host' in rules:
1672                     max_server = rules['max_server_per_host']
1673                 else:
1674                     max_server = 1
1675                 if len(members_on_host) >= max_server:
1676                     msg = _("Anti-affinity instance group policy "
1677                             "was violated.")
1678                     raise exception.RescheduledException(
1679                             instance_uuid=instance.uuid,
1680                             reason=msg)
1681             elif group.policy and 'affinity' == group.policy:
1682                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1683                 if group_hosts and self.host not in group_hosts:
1684                     msg = _("Affinity instance group policy was violated.")
1685                     raise exception.RescheduledException(
1686                             instance_uuid=instance.uuid,
1687                             reason=msg)
1688 
1689         if not CONF.workarounds.disable_group_policy_check_upcall:
1690             _do_validation(context, instance, group_hint)
1691 
1692     def _log_original_error(self, exc_info, instance_uuid):
1693         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1694                   exc_info=exc_info)
1695 
1696     @periodic_task.periodic_task
1697     def _check_instance_build_time(self, context):
1698         """Ensure that instances are not stuck in build."""
1699         timeout = CONF.instance_build_timeout
1700         if timeout == 0:
1701             return
1702 
1703         filters = {'vm_state': vm_states.BUILDING,
1704                    'host': self.host}
1705 
1706         building_insts = objects.InstanceList.get_by_filters(context,
1707                            filters, expected_attrs=[], use_slave=True)
1708 
1709         for instance in building_insts:
1710             if timeutils.is_older_than(instance.created_at, timeout):
1711                 self._set_instance_obj_error_state(instance)
1712                 LOG.warning("Instance build timed out. Set to error "
1713                             "state.", instance=instance)
1714 
1715     def _check_instance_exists(self, instance):
1716         """Ensure an instance with the same name is not already present."""
1717         if self.driver.instance_exists(instance):
1718             raise exception.InstanceExists(name=instance.name)
1719 
1720     def _allocate_network_async(self, context, instance, requested_networks,
1721                                 security_groups, resource_provider_mapping):
1722         """Method used to allocate networks in the background.
1723 
1724         Broken out for testing.
1725         """
1726         # First check to see if we're specifically not supposed to allocate
1727         # networks because if so, we can exit early.
1728         if requested_networks and requested_networks.no_allocate:
1729             LOG.debug("Not allocating networking since 'none' was specified.",
1730                       instance=instance)
1731             return network_model.NetworkInfo([])
1732 
1733         LOG.debug("Allocating IP information in the background.",
1734                   instance=instance)
1735         retries = CONF.network_allocate_retries
1736         attempts = retries + 1
1737         retry_time = 1
1738         bind_host_id = self.driver.network_binding_host_id(context, instance)
1739         for attempt in range(1, attempts + 1):
1740             try:
1741                 nwinfo = self.network_api.allocate_for_instance(
1742                         context, instance,
1743                         requested_networks=requested_networks,
1744                         security_groups=security_groups,
1745                         bind_host_id=bind_host_id,
1746                         resource_provider_mapping=resource_provider_mapping)
1747                 LOG.debug('Instance network_info: |%s|', nwinfo,
1748                           instance=instance)
1749                 instance.system_metadata['network_allocated'] = 'True'
1750                 # NOTE(JoshNang) do not save the instance here, as it can cause
1751                 # races. The caller shares a reference to instance and waits
1752                 # for this async greenthread to finish before calling
1753                 # instance.save().
1754                 return nwinfo
1755             except Exception as e:
1756                 log_info = {'attempt': attempt,
1757                             'attempts': attempts}
1758                 if attempt == attempts:
1759                     LOG.exception('Instance failed network setup '
1760                                   'after %(attempts)d attempt(s)',
1761                                   log_info)
1762                     raise e
1763                 LOG.warning('Instance failed network setup '
1764                             '(attempt %(attempt)d of %(attempts)d)',
1765                             log_info, instance=instance)
1766                 time.sleep(retry_time)
1767                 retry_time *= 2
1768                 if retry_time > 30:
1769                     retry_time = 30
1770         # Not reached.
1771 
1772     def _build_networks_for_instance(self, context, instance,
1773             requested_networks, security_groups, resource_provider_mapping):
1774 
1775         # If we're here from a reschedule the network may already be allocated.
1776         if strutils.bool_from_string(
1777                 instance.system_metadata.get('network_allocated', 'False')):
1778             # NOTE(alex_xu): The network_allocated is True means the network
1779             # resource already allocated at previous scheduling, and the
1780             # network setup is cleanup at previous. After rescheduling, the
1781             # network resource need setup on the new host.
1782             self.network_api.setup_instance_network_on_host(
1783                 context, instance, instance.host)
1784             return self.network_api.get_instance_nw_info(context, instance)
1785 
1786         network_info = self._allocate_network(context, instance,
1787                 requested_networks, security_groups,
1788                 resource_provider_mapping)
1789 
1790         return network_info
1791 
1792     def _allocate_network(self, context, instance, requested_networks,
1793                           security_groups, resource_provider_mapping):
1794         """Start network allocation asynchronously.  Return an instance
1795         of NetworkInfoAsyncWrapper that can be used to retrieve the
1796         allocated networks when the operation has finished.
1797         """
1798         # NOTE(comstud): Since we're allocating networks asynchronously,
1799         # this task state has little meaning, as we won't be in this
1800         # state for very long.
1801         instance.vm_state = vm_states.BUILDING
1802         instance.task_state = task_states.NETWORKING
1803         instance.save(expected_task_state=[None])
1804 
1805         return network_model.NetworkInfoAsyncWrapper(
1806                 self._allocate_network_async, context, instance,
1807                 requested_networks, security_groups, resource_provider_mapping)
1808 
1809     def _default_root_device_name(self, instance, image_meta, root_bdm):
1810         """Gets a default root device name from the driver.
1811 
1812         :param nova.objects.Instance instance:
1813             The instance for which to get the root device name.
1814         :param nova.objects.ImageMeta image_meta:
1815             The metadata of the image of the instance.
1816         :param nova.objects.BlockDeviceMapping root_bdm:
1817             The description of the root device.
1818         :returns: str -- The default root device name.
1819         :raises: InternalError, TooManyDiskDevices
1820         """
1821         try:
1822             return self.driver.default_root_device_name(instance,
1823                                                         image_meta,
1824                                                         root_bdm)
1825         except NotImplementedError:
1826             return compute_utils.get_next_device_name(instance, [])
1827 
1828     def _default_device_names_for_instance(self, instance,
1829                                            root_device_name,
1830                                            *block_device_lists):
1831         """Default the missing device names in the BDM from the driver.
1832 
1833         :param nova.objects.Instance instance:
1834             The instance for which to get default device names.
1835         :param str root_device_name: The root device name.
1836         :param list block_device_lists: List of block device mappings.
1837         :returns: None
1838         :raises: InternalError, TooManyDiskDevices
1839         """
1840         try:
1841             self.driver.default_device_names_for_instance(instance,
1842                                                           root_device_name,
1843                                                           *block_device_lists)
1844         except NotImplementedError:
1845             compute_utils.default_device_names_for_instance(
1846                 instance, root_device_name, *block_device_lists)
1847 
1848     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1849         """Get the next device name from the driver, based on the BDM.
1850 
1851         :param nova.objects.Instance instance:
1852             The instance whose volume is requesting a device name.
1853         :param nova.objects.BlockDeviceMappingList bdms:
1854             The block device mappings for the instance.
1855         :param nova.objects.BlockDeviceMapping block_device_obj:
1856             A block device mapping containing info about the requested block
1857             device.
1858         :returns: The next device name.
1859         :raises: InternalError, TooManyDiskDevices
1860         """
1861         # NOTE(ndipanov): Copy obj to avoid changing the original
1862         block_device_obj = block_device_obj.obj_clone()
1863         try:
1864             return self.driver.get_device_name_for_instance(
1865                 instance, bdms, block_device_obj)
1866         except NotImplementedError:
1867             return compute_utils.get_device_name_for_instance(
1868                 instance, bdms, block_device_obj.get("device_name"))
1869 
1870     def _default_block_device_names(self, instance, image_meta, block_devices):
1871         """Verify that all the devices have the device_name set. If not,
1872         provide a default name.
1873 
1874         It also ensures that there is a root_device_name and is set to the
1875         first block device in the boot sequence (boot_index=0).
1876         """
1877         root_bdm = block_device.get_root_bdm(block_devices)
1878         if not root_bdm:
1879             return
1880 
1881         # Get the root_device_name from the root BDM or the instance
1882         root_device_name = None
1883         update_root_bdm = False
1884 
1885         if root_bdm.device_name:
1886             root_device_name = root_bdm.device_name
1887             instance.root_device_name = root_device_name
1888         elif instance.root_device_name:
1889             root_device_name = instance.root_device_name
1890             root_bdm.device_name = root_device_name
1891             update_root_bdm = True
1892         else:
1893             root_device_name = self._default_root_device_name(instance,
1894                                                               image_meta,
1895                                                               root_bdm)
1896 
1897             instance.root_device_name = root_device_name
1898             root_bdm.device_name = root_device_name
1899             update_root_bdm = True
1900 
1901         if update_root_bdm:
1902             root_bdm.save()
1903 
1904         ephemerals = []
1905         swap = []
1906         block_device_mapping = []
1907 
1908         for device in block_devices:
1909             if block_device.new_format_is_ephemeral(device):
1910                 ephemerals.append(device)
1911 
1912             if block_device.new_format_is_swap(device):
1913                 swap.append(device)
1914 
1915             if driver_block_device.is_block_device_mapping(device):
1916                 block_device_mapping.append(device)
1917 
1918         self._default_device_names_for_instance(instance,
1919                                                 root_device_name,
1920                                                 ephemerals,
1921                                                 swap,
1922                                                 block_device_mapping)
1923 
1924     def _block_device_info_to_legacy(self, block_device_info):
1925         """Convert BDI to the old format for drivers that need it."""
1926 
1927         if self.use_legacy_block_device_info:
1928             ephemerals = driver_block_device.legacy_block_devices(
1929                 driver.block_device_info_get_ephemerals(block_device_info))
1930             mapping = driver_block_device.legacy_block_devices(
1931                 driver.block_device_info_get_mapping(block_device_info))
1932             swap = block_device_info['swap']
1933             if swap:
1934                 swap = swap.legacy()
1935 
1936             block_device_info.update({
1937                 'ephemerals': ephemerals,
1938                 'swap': swap,
1939                 'block_device_mapping': mapping})
1940 
1941     def _add_missing_dev_names(self, bdms, instance):
1942         for bdm in bdms:
1943             if bdm.device_name is not None:
1944                 continue
1945 
1946             device_name = self._get_device_name_for_instance(instance,
1947                                                              bdms, bdm)
1948             values = {'device_name': device_name}
1949             bdm.update(values)
1950             bdm.save()
1951 
1952     def _prep_block_device(self, context, instance, bdms):
1953         """Set up the block device for an instance with error logging."""
1954         try:
1955             self._add_missing_dev_names(bdms, instance)
1956             block_device_info = driver.get_block_device_info(instance, bdms)
1957             mapping = driver.block_device_info_get_mapping(block_device_info)
1958             driver_block_device.attach_block_devices(
1959                 mapping, context, instance, self.volume_api, self.driver,
1960                 wait_func=self._await_block_device_map_created)
1961 
1962             self._block_device_info_to_legacy(block_device_info)
1963             return block_device_info
1964 
1965         except exception.OverQuota as e:
1966             LOG.warning('Failed to create block device for instance due'
1967                         ' to exceeding volume related resource quota.'
1968                         ' Error: %s', e.message, instance=instance)
1969             raise
1970 
1971         except Exception as ex:
1972             LOG.exception('Instance failed block device setup',
1973                           instance=instance)
1974             # InvalidBDM will eventually result in a BuildAbortException when
1975             # booting from volume, and will be recorded as an instance fault.
1976             # Maintain the original exception message which most likely has
1977             # useful details which the standard InvalidBDM error message lacks.
1978             raise exception.InvalidBDM(str(ex))
1979 
1980     def _update_instance_after_spawn(self, instance,
1981                                      vm_state=vm_states.ACTIVE):
1982         instance.power_state = self._get_power_state(instance)
1983         instance.vm_state = vm_state
1984         instance.task_state = None
1985         # NOTE(sean-k-mooney): configdrive.update_instance checks
1986         # instance.launched_at to determine if it is the first or
1987         # subsequent spawn of an instance. We need to call update_instance
1988         # first before setting instance.launched_at or instance.config_drive
1989         # will never be set to true based on the value of force_config_drive.
1990         # As a result the config drive will be lost on a hard reboot of the
1991         # instance even when force_config_drive=true. see bug #1835822.
1992         configdrive.update_instance(instance)
1993         instance.launched_at = timeutils.utcnow()
1994 
1995     def _update_scheduler_instance_info(self, context, instance):
1996         """Sends an InstanceList with created or updated Instance objects to
1997         the Scheduler client.
1998 
1999         In the case of init_host, the value passed will already be an
2000         InstanceList. Other calls will send individual Instance objects that
2001         have been created or resized. In this case, we create an InstanceList
2002         object containing that Instance.
2003         """
2004         if not self.send_instance_updates:
2005             return
2006         if isinstance(instance, obj_instance.Instance):
2007             instance = objects.InstanceList(objects=[instance])
2008         context = context.elevated()
2009         self.query_client.update_instance_info(context, self.host,
2010                                                instance)
2011 
2012     def _delete_scheduler_instance_info(self, context, instance_uuid):
2013         """Sends the uuid of the deleted Instance to the Scheduler client."""
2014         if not self.send_instance_updates:
2015             return
2016         context = context.elevated()
2017         self.query_client.delete_instance_info(context, self.host,
2018                                                instance_uuid)
2019 
2020     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
2021     def _sync_scheduler_instance_info(self, context):
2022         if not self.send_instance_updates:
2023             return
2024         context = context.elevated()
2025         instances = objects.InstanceList.get_by_host(context, self.host,
2026                                                      expected_attrs=[],
2027                                                      use_slave=True)
2028         uuids = [instance.uuid for instance in instances]
2029         self.query_client.sync_instance_info(context, self.host, uuids)
2030 
2031     def _notify_about_instance_usage(self, context, instance, event_suffix,
2032                                      network_info=None, extra_usage_info=None,
2033                                      fault=None):
2034         compute_utils.notify_about_instance_usage(
2035             self.notifier, context, instance, event_suffix,
2036             network_info=network_info,
2037             extra_usage_info=extra_usage_info, fault=fault)
2038 
2039     def _deallocate_network(self, context, instance,
2040                             requested_networks=None):
2041         # If we were told not to allocate networks let's save ourselves
2042         # the trouble of calling the network API.
2043         if requested_networks and requested_networks.no_allocate:
2044             LOG.debug("Skipping network deallocation for instance since "
2045                       "networking was not requested.", instance=instance)
2046             return
2047 
2048         LOG.debug('Deallocating network for instance', instance=instance)
2049         with timeutils.StopWatch() as timer:
2050             self.network_api.deallocate_for_instance(
2051                 context, instance, requested_networks=requested_networks)
2052         # nova-network does an rpc call so we're OK tracking time spent here
2053         LOG.info('Took %0.2f seconds to deallocate network for instance.',
2054                  timer.elapsed(), instance=instance)
2055 
2056     def _get_instance_block_device_info(self, context, instance,
2057                                         refresh_conn_info=False,
2058                                         bdms=None):
2059         """Transform block devices to the driver block_device format."""
2060 
2061         if bdms is None:
2062             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2063                     context, instance.uuid)
2064         block_device_info = driver.get_block_device_info(instance, bdms)
2065 
2066         if not refresh_conn_info:
2067             # if the block_device_mapping has no value in connection_info
2068             # (returned as None), don't include in the mapping
2069             block_device_info['block_device_mapping'] = [
2070                 bdm for bdm in driver.block_device_info_get_mapping(
2071                                     block_device_info)
2072                 if bdm.get('connection_info')]
2073         else:
2074             driver_block_device.refresh_conn_infos(
2075                 driver.block_device_info_get_mapping(block_device_info),
2076                 context, instance, self.volume_api, self.driver)
2077 
2078         self._block_device_info_to_legacy(block_device_info)
2079 
2080         return block_device_info
2081 
2082     def _build_failed(self, node):
2083         if CONF.compute.consecutive_build_service_disable_threshold:
2084             # NOTE(danms): Update our counter, but wait for the next
2085             # update_available_resource() periodic to flush it to the DB
2086             self.rt.build_failed(node)
2087 
2088     def _build_succeeded(self, node):
2089         self.rt.build_succeeded(node)
2090 
2091     @wrap_exception()
2092     @reverts_task_state
2093     @wrap_instance_fault
2094     def build_and_run_instance(self, context, instance, image, request_spec,
2095                      filter_properties, admin_password=None,
2096                      injected_files=None, requested_networks=None,
2097                      security_groups=None, block_device_mapping=None,
2098                      node=None, limits=None, host_list=None, accel_uuids=None):
2099 
2100         @utils.synchronized(instance.uuid)
2101         def _locked_do_build_and_run_instance(*args, **kwargs):
2102             # NOTE(danms): We grab the semaphore with the instance uuid
2103             # locked because we could wait in line to build this instance
2104             # for a while and we want to make sure that nothing else tries
2105             # to do anything with this instance while we wait.
2106             with self._build_semaphore:
2107                 try:
2108                     result = self._do_build_and_run_instance(*args, **kwargs)
2109                 except Exception:
2110                     # NOTE(mriedem): This should really only happen if
2111                     # _decode_files in _do_build_and_run_instance fails, and
2112                     # that's before a guest is spawned so it's OK to remove
2113                     # allocations for the instance for this node from Placement
2114                     # below as there is no guest consuming resources anyway.
2115                     # The _decode_files case could be handled more specifically
2116                     # but that's left for another day.
2117                     result = build_results.FAILED
2118                     raise
2119                 finally:
2120                     if result == build_results.FAILED:
2121                         # Remove the allocation records from Placement for the
2122                         # instance if the build failed. The instance.host is
2123                         # likely set to None in _do_build_and_run_instance
2124                         # which means if the user deletes the instance, it
2125                         # will be deleted in the API, not the compute service.
2126                         # Setting the instance.host to None in
2127                         # _do_build_and_run_instance means that the
2128                         # ResourceTracker will no longer consider this instance
2129                         # to be claiming resources against it, so we want to
2130                         # reflect that same thing in Placement.  No need to
2131                         # call this for a reschedule, as the allocations will
2132                         # have already been removed in
2133                         # self._do_build_and_run_instance().
2134                         self.reportclient.delete_allocation_for_instance(
2135                             context, instance.uuid)
2136 
2137                     if result in (build_results.FAILED,
2138                                   build_results.RESCHEDULED):
2139                         self._build_failed(node)
2140                     else:
2141                         self._build_succeeded(node)
2142 
2143         # NOTE(danms): We spawn here to return the RPC worker thread back to
2144         # the pool. Since what follows could take a really long time, we don't
2145         # want to tie up RPC workers.
2146         utils.spawn_n(_locked_do_build_and_run_instance,
2147                       context, instance, image, request_spec,
2148                       filter_properties, admin_password, injected_files,
2149                       requested_networks, security_groups,
2150                       block_device_mapping, node, limits, host_list,
2151                       accel_uuids)
2152 
2153     def _check_device_tagging(self, requested_networks, block_device_mapping):
2154         tagging_requested = False
2155         if requested_networks:
2156             for net in requested_networks:
2157                 if 'tag' in net and net.tag is not None:
2158                     tagging_requested = True
2159                     break
2160         if block_device_mapping and not tagging_requested:
2161             for bdm in block_device_mapping:
2162                 if 'tag' in bdm and bdm.tag is not None:
2163                     tagging_requested = True
2164                     break
2165         if (tagging_requested and
2166                 not self.driver.capabilities.get('supports_device_tagging',
2167                                                  False)):
2168             raise exception.BuildAbortException('Attempt to boot guest with '
2169                                                 'tagged devices on host that '
2170                                                 'does not support tagging.')
2171 
2172     def _check_trusted_certs(self, instance):
2173         if (instance.trusted_certs and
2174                 not self.driver.capabilities.get('supports_trusted_certs',
2175                                                  False)):
2176             raise exception.BuildAbortException(
2177                 'Trusted image certificates provided on host that does not '
2178                 'support certificate validation.')
2179 
2180     @wrap_exception()
2181     @reverts_task_state
2182     @wrap_instance_event(prefix='compute')
2183     @wrap_instance_fault
2184     def _do_build_and_run_instance(self, context, instance, image,
2185             request_spec, filter_properties, admin_password, injected_files,
2186             requested_networks, security_groups, block_device_mapping,
2187             node=None, limits=None, host_list=None, accel_uuids=None):
2188 
2189         try:
2190             LOG.debug('Starting instance...', instance=instance)
2191             instance.vm_state = vm_states.BUILDING
2192             instance.task_state = None
2193             instance.save(expected_task_state=
2194                     (task_states.SCHEDULING, None))
2195         except exception.InstanceNotFound:
2196             msg = 'Instance disappeared before build.'
2197             LOG.debug(msg, instance=instance)
2198             return build_results.FAILED
2199         except exception.UnexpectedTaskStateError as e:
2200             LOG.debug(e.format_message(), instance=instance)
2201             return build_results.FAILED
2202 
2203         # b64 decode the files to inject:
2204         decoded_files = self._decode_files(injected_files)
2205 
2206         if limits is None:
2207             limits = {}
2208 
2209         if node is None:
2210             node = self._get_nodename(instance, refresh=True)
2211 
2212         try:
2213             with timeutils.StopWatch() as timer:
2214                 self._build_and_run_instance(context, instance, image,
2215                         decoded_files, admin_password, requested_networks,
2216                         security_groups, block_device_mapping, node, limits,
2217                         filter_properties, request_spec, accel_uuids)
2218             LOG.info('Took %0.2f seconds to build instance.',
2219                      timer.elapsed(), instance=instance)
2220             return build_results.ACTIVE
2221         except exception.RescheduledException as e:
2222             retry = filter_properties.get('retry')
2223             if not retry:
2224                 # no retry information, do not reschedule.
2225                 LOG.debug("Retry info not present, will not reschedule",
2226                     instance=instance)
2227                 self._cleanup_allocated_networks(context, instance,
2228                     requested_networks)
2229                 compute_utils.add_instance_fault_from_exc(context,
2230                         instance, e, sys.exc_info(),
2231                         fault_message=e.kwargs['reason'])
2232                 self._nil_out_instance_obj_host_and_node(instance)
2233                 self._set_instance_obj_error_state(instance,
2234                                                    clean_task_state=True)
2235                 return build_results.FAILED
2236             LOG.debug(e.format_message(), instance=instance)
2237             # This will be used for logging the exception
2238             retry['exc'] = traceback.format_exception(*sys.exc_info())
2239             # This will be used for setting the instance fault message
2240             retry['exc_reason'] = e.kwargs['reason']
2241 
2242             self._cleanup_allocated_networks(context, instance,
2243                                              requested_networks)
2244 
2245             self._nil_out_instance_obj_host_and_node(instance)
2246             instance.task_state = task_states.SCHEDULING
2247             instance.save()
2248             # The instance will have already claimed resources from this host
2249             # before this build was attempted. Now that it has failed, we need
2250             # to unclaim those resources before casting to the conductor, so
2251             # that if there are alternate hosts available for a retry, it can
2252             # claim resources on that new host for the instance.
2253             self.reportclient.delete_allocation_for_instance(context,
2254                                                              instance.uuid)
2255 
2256             self.compute_task_api.build_instances(context, [instance],
2257                     image, filter_properties, admin_password,
2258                     injected_files, requested_networks, security_groups,
2259                     block_device_mapping, request_spec=request_spec,
2260                     host_lists=[host_list])
2261             return build_results.RESCHEDULED
2262         except (exception.InstanceNotFound,
2263                 exception.UnexpectedDeletingTaskStateError):
2264             msg = 'Instance disappeared during build.'
2265             LOG.debug(msg, instance=instance)
2266             self._cleanup_allocated_networks(context, instance,
2267                     requested_networks)
2268             return build_results.FAILED
2269         except Exception as e:
2270             if isinstance(e, exception.BuildAbortException):
2271                 LOG.error(e.format_message(), instance=instance)
2272             else:
2273                 # Should not reach here.
2274                 LOG.exception('Unexpected build failure, not rescheduling '
2275                               'build.', instance=instance)
2276             self._cleanup_allocated_networks(context, instance,
2277                     requested_networks)
2278             self._cleanup_volumes(context, instance,
2279                     block_device_mapping, raise_exc=False)
2280             compute_utils.add_instance_fault_from_exc(context, instance,
2281                     e, sys.exc_info())
2282             self._nil_out_instance_obj_host_and_node(instance)
2283             self._set_instance_obj_error_state(instance, clean_task_state=True)
2284             return build_results.FAILED
2285 
2286     @staticmethod
2287     def _get_scheduler_hints(filter_properties, request_spec=None):
2288         """Helper method to get scheduler hints.
2289 
2290         This method prefers to get the hints out of the request spec, but that
2291         might not be provided. Conductor will pass request_spec down to the
2292         first compute chosen for a build but older computes will not pass
2293         the request_spec to conductor's build_instances method for a
2294         a reschedule, so if we're on a host via a retry, request_spec may not
2295         be provided so we need to fallback to use the filter_properties
2296         to get scheduler hints.
2297         """
2298         hints = {}
2299         if request_spec is not None and 'scheduler_hints' in request_spec:
2300             hints = request_spec.scheduler_hints
2301         if not hints:
2302             hints = filter_properties.get('scheduler_hints') or {}
2303         return hints
2304 
2305     @staticmethod
2306     def _get_request_group_mapping(request_spec):
2307         """Return request group resource - provider mapping. This is currently
2308         used for Neutron ports that have resource request due to the port
2309         having QoS minimum bandwidth policy rule attached.
2310 
2311         :param request_spec: A RequestSpec object or None
2312         :returns: A dict keyed by RequestGroup requester_id, currently Neutron
2313         port_id, to resource provider UUID that provides resource for that
2314         RequestGroup. Or None if the request_spec was None.
2315         """
2316         if request_spec:
2317             return request_spec.get_request_group_mapping()
2318         else:
2319             return None
2320 
2321     def _build_and_run_instance(self, context, instance, image, injected_files,
2322             admin_password, requested_networks, security_groups,
2323             block_device_mapping, node, limits, filter_properties,
2324             request_spec=None, accel_uuids=None):
2325 
2326         image_name = image.get('name')
2327         self._notify_about_instance_usage(context, instance, 'create.start',
2328                 extra_usage_info={'image_name': image_name})
2329         compute_utils.notify_about_instance_create(
2330             context, instance, self.host,
2331             phase=fields.NotificationPhase.START,
2332             bdms=block_device_mapping)
2333 
2334         # NOTE(mikal): cache the keystone roles associated with the instance
2335         # at boot time for later reference
2336         instance.system_metadata.update(
2337             {'boot_roles': ','.join(context.roles)})
2338 
2339         self._check_device_tagging(requested_networks, block_device_mapping)
2340         self._check_trusted_certs(instance)
2341 
2342         provider_mapping = self._get_request_group_mapping(request_spec)
2343 
2344         if provider_mapping:
2345             try:
2346                 compute_utils\
2347                     .update_pci_request_spec_with_allocated_interface_name(
2348                         context, self.reportclient,
2349                         instance.pci_requests.requests, provider_mapping)
2350             except (exception.AmbiguousResourceProviderForPCIRequest,
2351                     exception.UnexpectedResourceProviderNameForPCIRequest
2352                     ) as e:
2353                 raise exception.BuildAbortException(
2354                     reason=str(e), instance_uuid=instance.uuid)
2355 
2356         # TODO(Luyao) cut over to get_allocs_for_consumer
2357         allocs = self.reportclient.get_allocations_for_consumer(
2358                 context, instance.uuid)
2359 
2360         try:
2361             scheduler_hints = self._get_scheduler_hints(filter_properties,
2362                                                         request_spec)
2363             with self.rt.instance_claim(context, instance, node, allocs,
2364                                         limits):
2365                 # NOTE(russellb) It's important that this validation be done
2366                 # *after* the resource tracker instance claim, as that is where
2367                 # the host is set on the instance.
2368                 self._validate_instance_group_policy(context, instance,
2369                                                      scheduler_hints)
2370                 image_meta = objects.ImageMeta.from_dict(image)
2371 
2372                 with self._build_resources(context, instance,
2373                         requested_networks, security_groups, image_meta,
2374                         block_device_mapping, provider_mapping,
2375                         accel_uuids) as resources:
2376                     instance.vm_state = vm_states.BUILDING
2377                     instance.task_state = task_states.SPAWNING
2378                     # NOTE(JoshNang) This also saves the changes to the
2379                     # instance from _allocate_network_async, as they aren't
2380                     # saved in that function to prevent races.
2381                     instance.save(expected_task_state=
2382                             task_states.BLOCK_DEVICE_MAPPING)
2383                     block_device_info = resources['block_device_info']
2384                     network_info = resources['network_info']
2385                     accel_info = resources['accel_info']
2386                     LOG.debug('Start spawning the instance on the hypervisor.',
2387                               instance=instance)
2388                     with timeutils.StopWatch() as timer:
2389                         self.driver.spawn(context, instance, image_meta,
2390                                           injected_files, admin_password,
2391                                           allocs, network_info=network_info,
2392                                           block_device_info=block_device_info,
2393                                           accel_info=accel_info)
2394                     LOG.info('Took %0.2f seconds to spawn the instance on '
2395                              'the hypervisor.', timer.elapsed(),
2396                              instance=instance)
2397         except (exception.InstanceNotFound,
2398                 exception.UnexpectedDeletingTaskStateError) as e:
2399             with excutils.save_and_reraise_exception():
2400                 self._notify_about_instance_usage(context, instance,
2401                     'create.error', fault=e)
2402                 compute_utils.notify_about_instance_create(
2403                     context, instance, self.host,
2404                     phase=fields.NotificationPhase.ERROR, exception=e,
2405                     bdms=block_device_mapping)
2406         except exception.ComputeResourcesUnavailable as e:
2407             LOG.debug(e.format_message(), instance=instance)
2408             self._notify_about_instance_usage(context, instance,
2409                     'create.error', fault=e)
2410             compute_utils.notify_about_instance_create(
2411                     context, instance, self.host,
2412                     phase=fields.NotificationPhase.ERROR, exception=e,
2413                     bdms=block_device_mapping)
2414             raise exception.RescheduledException(
2415                     instance_uuid=instance.uuid, reason=e.format_message())
2416         except exception.BuildAbortException as e:
2417             with excutils.save_and_reraise_exception():
2418                 LOG.debug(e.format_message(), instance=instance)
2419                 self._notify_about_instance_usage(context, instance,
2420                     'create.error', fault=e)
2421                 compute_utils.notify_about_instance_create(
2422                     context, instance, self.host,
2423                     phase=fields.NotificationPhase.ERROR, exception=e,
2424                     bdms=block_device_mapping)
2425         except exception.NoMoreFixedIps as e:
2426             LOG.warning('No more fixed IP to be allocated',
2427                         instance=instance)
2428             self._notify_about_instance_usage(context, instance,
2429                     'create.error', fault=e)
2430             compute_utils.notify_about_instance_create(
2431                     context, instance, self.host,
2432                     phase=fields.NotificationPhase.ERROR, exception=e,
2433                     bdms=block_device_mapping)
2434             msg = _('Failed to allocate the network(s) with error %s, '
2435                     'not rescheduling.') % e.format_message()
2436             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2437                     reason=msg)
2438         except (exception.ExternalNetworkAttachForbidden,
2439                 exception.VirtualInterfaceCreateException,
2440                 exception.VirtualInterfaceMacAddressException,
2441                 exception.FixedIpInvalidOnHost,
2442                 exception.UnableToAutoAllocateNetwork,
2443                 exception.NetworksWithQoSPolicyNotSupported) as e:
2444             LOG.exception('Failed to allocate network(s)',
2445                           instance=instance)
2446             self._notify_about_instance_usage(context, instance,
2447                     'create.error', fault=e)
2448             compute_utils.notify_about_instance_create(
2449                     context, instance, self.host,
2450                     phase=fields.NotificationPhase.ERROR, exception=e,
2451                     bdms=block_device_mapping)
2452             msg = _('Failed to allocate the network(s), not rescheduling.')
2453             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2454                     reason=msg)
2455         except (exception.FlavorDiskTooSmall,
2456                 exception.FlavorMemoryTooSmall,
2457                 exception.ImageNotActive,
2458                 exception.ImageUnacceptable,
2459                 exception.InvalidDiskInfo,
2460                 exception.InvalidDiskFormat,
2461                 cursive_exception.SignatureVerificationError,
2462                 exception.CertificateValidationFailed,
2463                 exception.VolumeEncryptionNotSupported,
2464                 exception.InvalidInput,
2465                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2466                 # in the API during server create and rebuild.
2467                 exception.RequestedVRamTooHigh) as e:
2468             self._notify_about_instance_usage(context, instance,
2469                     'create.error', fault=e)
2470             compute_utils.notify_about_instance_create(
2471                     context, instance, self.host,
2472                     phase=fields.NotificationPhase.ERROR, exception=e,
2473                     bdms=block_device_mapping)
2474             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2475                     reason=e.format_message())
2476         except Exception as e:
2477             LOG.exception('Failed to build and run instance',
2478                           instance=instance)
2479             self._notify_about_instance_usage(context, instance,
2480                     'create.error', fault=e)
2481             compute_utils.notify_about_instance_create(
2482                     context, instance, self.host,
2483                     phase=fields.NotificationPhase.ERROR, exception=e,
2484                     bdms=block_device_mapping)
2485             raise exception.RescheduledException(
2486                     instance_uuid=instance.uuid, reason=str(e))
2487 
2488         # NOTE(alaski): This is only useful during reschedules, remove it now.
2489         instance.system_metadata.pop('network_allocated', None)
2490 
2491         # If CONF.default_access_ip_network_name is set, grab the
2492         # corresponding network and set the access ip values accordingly.
2493         network_name = CONF.default_access_ip_network_name
2494         if (network_name and not instance.access_ip_v4 and
2495                 not instance.access_ip_v6):
2496             # Note that when there are multiple ips to choose from, an
2497             # arbitrary one will be chosen.
2498             for vif in network_info:
2499                 if vif['network']['label'] == network_name:
2500                     for ip in vif.fixed_ips():
2501                         if not instance.access_ip_v4 and ip['version'] == 4:
2502                             instance.access_ip_v4 = ip['address']
2503                         if not instance.access_ip_v6 and ip['version'] == 6:
2504                             instance.access_ip_v6 = ip['address']
2505                     break
2506 
2507         self._update_instance_after_spawn(instance)
2508 
2509         try:
2510             instance.save(expected_task_state=task_states.SPAWNING)
2511         except (exception.InstanceNotFound,
2512                 exception.UnexpectedDeletingTaskStateError) as e:
2513             with excutils.save_and_reraise_exception():
2514                 self._notify_about_instance_usage(context, instance,
2515                     'create.error', fault=e)
2516                 compute_utils.notify_about_instance_create(
2517                     context, instance, self.host,
2518                     phase=fields.NotificationPhase.ERROR, exception=e,
2519                     bdms=block_device_mapping)
2520 
2521         self._update_scheduler_instance_info(context, instance)
2522         self._notify_about_instance_usage(context, instance, 'create.end',
2523                 extra_usage_info={'message': _('Success')},
2524                 network_info=network_info)
2525         compute_utils.notify_about_instance_create(context, instance,
2526                 self.host, phase=fields.NotificationPhase.END,
2527                 bdms=block_device_mapping)
2528 
2529     def _build_resources_cleanup(self, instance, network_info):
2530         # Make sure the async call finishes
2531         if network_info is not None:
2532             network_info.wait(do_raise=False)
2533             self.driver.clean_networks_preparation(instance,
2534                                                    network_info)
2535         self.driver.failed_spawn_cleanup(instance)
2536 
2537     @contextlib.contextmanager
2538     def _build_resources(self, context, instance, requested_networks,
2539                          security_groups, image_meta, block_device_mapping,
2540                          resource_provider_mapping, accel_uuids):
2541         resources = {}
2542         network_info = None
2543         try:
2544             LOG.debug('Start building networks asynchronously for instance.',
2545                       instance=instance)
2546             network_info = self._build_networks_for_instance(context, instance,
2547                     requested_networks, security_groups,
2548                     resource_provider_mapping)
2549             resources['network_info'] = network_info
2550         except (exception.InstanceNotFound,
2551                 exception.UnexpectedDeletingTaskStateError):
2552             raise
2553         except exception.UnexpectedTaskStateError as e:
2554             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2555                     reason=e.format_message())
2556         except Exception:
2557             # Because this allocation is async any failures are likely to occur
2558             # when the driver accesses network_info during spawn().
2559             LOG.exception('Failed to allocate network(s)',
2560                           instance=instance)
2561             msg = _('Failed to allocate the network(s), not rescheduling.')
2562             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2563                     reason=msg)
2564 
2565         try:
2566             # Perform any driver preparation work for the driver.
2567             self.driver.prepare_for_spawn(instance)
2568 
2569             # Depending on a virt driver, some network configuration is
2570             # necessary before preparing block devices.
2571             self.driver.prepare_networks_before_block_device_mapping(
2572                 instance, network_info)
2573 
2574             # Verify that all the BDMs have a device_name set and assign a
2575             # default to the ones missing it with the help of the driver.
2576             self._default_block_device_names(instance, image_meta,
2577                                              block_device_mapping)
2578 
2579             LOG.debug('Start building block device mappings for instance.',
2580                       instance=instance)
2581             instance.vm_state = vm_states.BUILDING
2582             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2583             instance.save()
2584 
2585             block_device_info = self._prep_block_device(context, instance,
2586                     block_device_mapping)
2587             resources['block_device_info'] = block_device_info
2588         except (exception.InstanceNotFound,
2589                 exception.UnexpectedDeletingTaskStateError):
2590             with excutils.save_and_reraise_exception():
2591                 self._build_resources_cleanup(instance, network_info)
2592         except (exception.UnexpectedTaskStateError,
2593                 exception.OverQuota, exception.InvalidBDM) as e:
2594             self._build_resources_cleanup(instance, network_info)
2595             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2596                     reason=e.format_message())
2597         except Exception:
2598             LOG.exception('Failure prepping block device',
2599                           instance=instance)
2600             self._build_resources_cleanup(instance, network_info)
2601             msg = _('Failure prepping block device.')
2602             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2603                     reason=msg)
2604 
2605         arqs = []
2606         if instance.flavor.extra_specs.get('accel:device_profile'):
2607             try:
2608                 arqs = self._get_bound_arq_resources(
2609                     context, instance, accel_uuids)
2610             except (Exception, eventlet.timeout.Timeout) as exc:
2611                 LOG.exception(exc)
2612                 self._build_resources_cleanup(instance, network_info)
2613                 compute_utils.delete_arqs_if_needed(context, instance)
2614                 msg = _('Failure getting accelerator requests.')
2615                 raise exception.BuildAbortException(
2616                     reason=msg, instance_uuid=instance.uuid)
2617 
2618         resources['accel_info'] = arqs
2619         try:
2620             yield resources
2621         except Exception as exc:
2622             with excutils.save_and_reraise_exception() as ctxt:
2623                 if not isinstance(exc, (
2624                         exception.InstanceNotFound,
2625                         exception.UnexpectedDeletingTaskStateError)):
2626                     LOG.exception('Instance failed to spawn',
2627                                   instance=instance)
2628                 # Make sure the async call finishes
2629                 if network_info is not None:
2630                     network_info.wait(do_raise=False)
2631                 # if network_info is empty we're likely here because of
2632                 # network allocation failure. Since nothing can be reused on
2633                 # rescheduling it's better to deallocate network to eliminate
2634                 # the chance of orphaned ports in neutron
2635                 deallocate_networks = False if network_info else True
2636                 try:
2637                     self._shutdown_instance(context, instance,
2638                             block_device_mapping, requested_networks,
2639                             try_deallocate_networks=deallocate_networks)
2640                 except Exception as exc2:
2641                     ctxt.reraise = False
2642                     LOG.warning('Could not clean up failed build,'
2643                                 ' not rescheduling. Error: %s',
2644                                 str(exc2))
2645                     raise exception.BuildAbortException(
2646                             instance_uuid=instance.uuid,
2647                             reason=str(exc))
2648                 finally:
2649                     # Call Cyborg to delete accelerator requests
2650                     compute_utils.delete_arqs_if_needed(context, instance)
2651 
2652     def _get_bound_arq_resources(self, context, instance, arq_uuids):
2653         """Get bound accelerator requests.
2654 
2655         The ARQ binding was kicked off in the conductor as an async
2656         operation. Here we wait for the notification from Cyborg.
2657 
2658         If the notification arrived before this point, which can happen
2659         in many/most cases (see [1]), it will be lost. To handle that,
2660         we use exit_wait_early.
2661         [1] https://review.opendev.org/#/c/631244/46/nova/compute/
2662             manager.py@2627
2663 
2664         :param instance: instance object
2665         :param arq_uuids: List of accelerator request (ARQ) UUIDs.
2666         :returns: List of ARQs for which bindings have completed,
2667                   successfully or otherwise
2668         """
2669 
2670         cyclient = cyborg.get_client(context)
2671         if arq_uuids is None:
2672             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2673             arq_uuids = [arq['uuid'] for arq in arqs]
2674         events = [('accelerator-request-bound', arq_uuid)
2675                   for arq_uuid in arq_uuids]
2676 
2677         timeout = CONF.arq_binding_timeout
2678         with self.virtapi.wait_for_instance_event(
2679                 instance, events, deadline=timeout):
2680             resolved_arqs = cyclient.get_arqs_for_instance(
2681                     instance.uuid, only_resolved=True)
2682             # Events for these resolved ARQs may have already arrived.
2683             # Such 'early' events need to be ignored.
2684             early_events = [('accelerator-request-bound', arq['uuid'])
2685                              for arq in resolved_arqs]
2686             if early_events:
2687                 self.virtapi.exit_wait_early(early_events)
2688 
2689         # Since a timeout in wait_for_instance_event will raise, we get
2690         # here only if all binding events have been received.
2691         resolved_uuids = [arq['uuid'] for arq in resolved_arqs]
2692         if sorted(resolved_uuids) != sorted(arq_uuids):
2693             # Query Cyborg to get all.
2694             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2695         else:
2696             arqs = resolved_arqs
2697         return arqs
2698 
2699     def _cleanup_allocated_networks(self, context, instance,
2700             requested_networks):
2701         """Cleanup networks allocated for instance.
2702 
2703         :param context: nova request context
2704         :param instance: nova.objects.instance.Instance object
2705         :param requested_networks: nova.objects.NetworkRequestList
2706         """
2707         LOG.debug('Unplugging VIFs for instance', instance=instance)
2708 
2709         network_info = instance.get_network_info()
2710 
2711         # NOTE(stephenfin) to avoid nova destroying the instance without
2712         # unplugging the interface, refresh network_info if it is empty.
2713         if not network_info:
2714             try:
2715                 network_info = self.network_api.get_instance_nw_info(
2716                     context, instance,
2717                 )
2718             except Exception as exc:
2719                 LOG.warning(
2720                     'Failed to update network info cache when cleaning up '
2721                     'allocated networks. Stale VIFs may be left on this host.'
2722                     'Error: %s', str(exc)
2723                 )
2724                 return
2725 
2726         try:
2727             self.driver.unplug_vifs(instance, network_info)
2728         except NotImplementedError:
2729             # This is an optional method so ignore things if it doesn't exist
2730             LOG.debug(
2731                 'Virt driver does not provide unplug_vifs method, so it '
2732                 'is not possible determine if VIFs should be unplugged.'
2733             )
2734         except exception.NovaException as exc:
2735             # It's possible that the instance never got as far as plugging
2736             # VIFs, in which case we would see an exception which can be
2737             # mostly ignored
2738             LOG.warning(
2739                 'Cleaning up VIFs failed for instance. Error: %s',
2740                 str(exc), instance=instance,
2741             )
2742         else:
2743             LOG.debug('Unplugged VIFs for instance', instance=instance)
2744 
2745         try:
2746             self._deallocate_network(context, instance, requested_networks)
2747         except Exception:
2748             LOG.exception('Failed to deallocate networks', instance=instance)
2749             return
2750 
2751         instance.system_metadata['network_allocated'] = 'False'
2752         try:
2753             instance.save()
2754         except exception.InstanceNotFound:
2755             # NOTE(alaski): It's possible that we're cleaning up the networks
2756             # because the instance was deleted.  If that's the case then this
2757             # exception will be raised by instance.save()
2758             pass
2759 
2760     def _try_deallocate_network(self, context, instance,
2761                                 requested_networks=None):
2762 
2763         # During auto-scale cleanup, we could be deleting a large number
2764         # of servers at the same time and overloading parts of the system,
2765         # so we retry a few times in case of connection failures to the
2766         # networking service.
2767         @loopingcall.RetryDecorator(
2768             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2769             exceptions=(keystone_exception.connection.ConnectFailure,))
2770         def _deallocate_network_with_retries():
2771             try:
2772                 self._deallocate_network(
2773                     context, instance, requested_networks)
2774             except keystone_exception.connection.ConnectFailure as e:
2775                 # Provide a warning that something is amiss.
2776                 with excutils.save_and_reraise_exception():
2777                     LOG.warning('Failed to deallocate network for instance; '
2778                                 'retrying. Error: %s', str(e),
2779                                 instance=instance)
2780 
2781         try:
2782             # tear down allocated network structure
2783             _deallocate_network_with_retries()
2784         except Exception as ex:
2785             with excutils.save_and_reraise_exception():
2786                 LOG.error('Failed to deallocate network for instance. '
2787                           'Error: %s', ex, instance=instance)
2788                 self._set_instance_obj_error_state(instance)
2789 
2790     def _get_power_off_values(self, instance, clean_shutdown):
2791         """Get the timing configuration for powering down this instance."""
2792         if clean_shutdown:
2793             timeout = compute_utils.get_value_from_system_metadata(instance,
2794                           key='image_os_shutdown_timeout', type=int,
2795                           default=CONF.shutdown_timeout)
2796             retry_interval = CONF.compute.shutdown_retry_interval
2797         else:
2798             timeout = 0
2799             retry_interval = 0
2800 
2801         return timeout, retry_interval
2802 
2803     def _power_off_instance(self, instance, clean_shutdown=True):
2804         """Power off an instance on this host."""
2805         timeout, retry_interval = self._get_power_off_values(
2806             instance, clean_shutdown)
2807         self.driver.power_off(instance, timeout, retry_interval)
2808 
2809     def _shutdown_instance(self, context, instance,
2810                            bdms, requested_networks=None, notify=True,
2811                            try_deallocate_networks=True):
2812         """Shutdown an instance on this host.
2813 
2814         :param:context: security context
2815         :param:instance: a nova.objects.Instance object
2816         :param:bdms: the block devices for the instance to be torn
2817                      down
2818         :param:requested_networks: the networks on which the instance
2819                                    has ports
2820         :param:notify: true if a final usage notification should be
2821                        emitted
2822         :param:try_deallocate_networks: false if we should avoid
2823                                         trying to teardown networking
2824         """
2825         context = context.elevated()
2826         LOG.info('Terminating instance', instance=instance)
2827 
2828         if notify:
2829             self._notify_about_instance_usage(context, instance,
2830                                               "shutdown.start")
2831             compute_utils.notify_about_instance_action(context, instance,
2832                     self.host, action=fields.NotificationAction.SHUTDOWN,
2833                     phase=fields.NotificationPhase.START, bdms=bdms)
2834 
2835         network_info = instance.get_network_info()
2836 
2837         # NOTE(arnaudmorin) to avoid nova destroying the instance without
2838         # unplugging the interface, refresh network_info if it is empty.
2839         if not network_info:
2840             network_info = self.network_api.get_instance_nw_info(
2841                 context, instance)
2842 
2843         # NOTE(vish) get bdms before destroying the instance
2844         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2845         block_device_info = self._get_instance_block_device_info(
2846             context, instance, bdms=bdms)
2847 
2848         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2849         #                want to keep ip allocated for certain failures
2850         try:
2851             LOG.debug('Start destroying the instance on the hypervisor.',
2852                       instance=instance)
2853             with timeutils.StopWatch() as timer:
2854                 self.driver.destroy(context, instance, network_info,
2855                                     block_device_info)
2856             LOG.info('Took %0.2f seconds to destroy the instance on the '
2857                      'hypervisor.', timer.elapsed(), instance=instance)
2858         except exception.InstancePowerOffFailure:
2859             # if the instance can't power off, don't release the ip
2860             with excutils.save_and_reraise_exception():
2861                 pass
2862         except Exception:
2863             with excutils.save_and_reraise_exception():
2864                 # deallocate ip and fail without proceeding to
2865                 # volume api calls, preserving current behavior
2866                 if try_deallocate_networks:
2867                     self._try_deallocate_network(context, instance,
2868                                                  requested_networks)
2869 
2870         if try_deallocate_networks:
2871             self._try_deallocate_network(context, instance, requested_networks)
2872 
2873         timer.restart()
2874         for bdm in vol_bdms:
2875             try:
2876                 if bdm.attachment_id:
2877                     self.volume_api.attachment_delete(context,
2878                                                       bdm.attachment_id)
2879                 else:
2880                     # NOTE(vish): actual driver detach done in driver.destroy,
2881                     #             so just tell cinder that we are done with it.
2882                     connector = self.driver.get_volume_connector(instance)
2883                     self.volume_api.terminate_connection(context,
2884                                                          bdm.volume_id,
2885                                                          connector)
2886                     self.volume_api.detach(context, bdm.volume_id,
2887                                            instance.uuid)
2888 
2889             except exception.VolumeAttachmentNotFound as exc:
2890                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2891                           instance=instance)
2892             except exception.DiskNotFound as exc:
2893                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2894                           instance=instance)
2895             except exception.VolumeNotFound as exc:
2896                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2897                           instance=instance)
2898             except (cinder_exception.EndpointNotFound,
2899                     keystone_exception.EndpointNotFound) as exc:
2900                 LOG.warning('Ignoring EndpointNotFound for '
2901                             'volume %(volume_id)s: %(exc)s',
2902                             {'exc': exc, 'volume_id': bdm.volume_id},
2903                             instance=instance)
2904             except cinder_exception.ClientException as exc:
2905                 LOG.warning('Ignoring unknown cinder exception for '
2906                             'volume %(volume_id)s: %(exc)s',
2907                             {'exc': exc, 'volume_id': bdm.volume_id},
2908                             instance=instance)
2909             except Exception as exc:
2910                 LOG.warning('Ignoring unknown exception for '
2911                             'volume %(volume_id)s: %(exc)s',
2912                             {'exc': exc, 'volume_id': bdm.volume_id},
2913                             instance=instance)
2914         if vol_bdms:
2915             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2916                      'for instance.',
2917                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2918                      instance=instance)
2919 
2920         if notify:
2921             self._notify_about_instance_usage(context, instance,
2922                                               "shutdown.end")
2923             compute_utils.notify_about_instance_action(context, instance,
2924                     self.host, action=fields.NotificationAction.SHUTDOWN,
2925                     phase=fields.NotificationPhase.END, bdms=bdms)
2926 
2927     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2928                          detach=True):
2929         original_exception = None
2930         for bdm in bdms:
2931             if detach and bdm.volume_id:
2932                 try:
2933                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2934                               instance_uuid=instance.uuid)
2935                     destroy = bdm.delete_on_termination
2936                     self._detach_volume(context, bdm, instance,
2937                                         destroy_bdm=destroy)
2938                 except Exception as exc:
2939                     original_exception = exc
2940                     LOG.warning('Failed to detach volume: %(volume_id)s '
2941                                 'due to %(exc)s',
2942                                 {'volume_id': bdm.volume_id, 'exc': exc})
2943 
2944             if bdm.volume_id and bdm.delete_on_termination:
2945                 try:
2946                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2947                               instance_uuid=instance.uuid)
2948                     self.volume_api.delete(context, bdm.volume_id)
2949                 except Exception as exc:
2950                     original_exception = exc
2951                     LOG.warning('Failed to delete volume: %(volume_id)s '
2952                                 'due to %(exc)s',
2953                                 {'volume_id': bdm.volume_id, 'exc': exc})
2954         if original_exception is not None and raise_exc:
2955             raise original_exception
2956 
2957     def _delete_instance(self, context, instance, bdms):
2958         """Delete an instance on this host.
2959 
2960         :param context: nova request context
2961         :param instance: nova.objects.instance.Instance object
2962         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2963         """
2964         events = self.instance_events.clear_events_for_instance(instance)
2965         if events:
2966             LOG.debug('Events pending at deletion: %(events)s',
2967                       {'events': ','.join(events.keys())},
2968                       instance=instance)
2969         self._notify_about_instance_usage(context, instance,
2970                                           "delete.start")
2971         compute_utils.notify_about_instance_action(context, instance,
2972                 self.host, action=fields.NotificationAction.DELETE,
2973                 phase=fields.NotificationPhase.START, bdms=bdms)
2974 
2975         self._shutdown_instance(context, instance, bdms)
2976 
2977         # NOTE(vish): We have already deleted the instance, so we have
2978         #             to ignore problems cleaning up the volumes. It
2979         #             would be nice to let the user know somehow that
2980         #             the volume deletion failed, but it is not
2981         #             acceptable to have an instance that can not be
2982         #             deleted. Perhaps this could be reworked in the
2983         #             future to set an instance fault the first time
2984         #             and to only ignore the failure if the instance
2985         #             is already in ERROR.
2986 
2987         # NOTE(ameeda): The volumes have already been detached during
2988         #               the above _shutdown_instance() call and this is
2989         #               why detach is not requested from
2990         #               _cleanup_volumes() in this case
2991 
2992         self._cleanup_volumes(context, instance, bdms,
2993                 raise_exc=False, detach=False)
2994         # Delete Cyborg ARQs if the instance has a device profile.
2995         compute_utils.delete_arqs_if_needed(context, instance)
2996         # if a delete task succeeded, always update vm state and task
2997         # state without expecting task state to be DELETING
2998         instance.vm_state = vm_states.DELETED
2999         instance.task_state = None
3000         instance.power_state = power_state.NOSTATE
3001         instance.terminated_at = timeutils.utcnow()
3002         instance.save()
3003 
3004         self._complete_deletion(context, instance)
3005         # only destroy the instance in the db if the _complete_deletion
3006         # doesn't raise and therefore allocation is successfully
3007         # deleted in placement
3008         instance.destroy()
3009 
3010         self._notify_about_instance_usage(context, instance, "delete.end")
3011         compute_utils.notify_about_instance_action(context, instance,
3012                 self.host, action=fields.NotificationAction.DELETE,
3013                 phase=fields.NotificationPhase.END, bdms=bdms)
3014 
3015     @wrap_exception()
3016     @reverts_task_state
3017     @wrap_instance_event(prefix='compute')
3018     @wrap_instance_fault
3019     def terminate_instance(self, context, instance, bdms):
3020         """Terminate an instance on this host."""
3021         @utils.synchronized(instance.uuid)
3022         def do_terminate_instance(instance, bdms):
3023             # NOTE(mriedem): If we are deleting the instance while it was
3024             # booting from volume, we could be racing with a database update of
3025             # the BDM volume_id. Since the compute API passes the BDMs over RPC
3026             # to compute here, the BDMs may be stale at this point. So check
3027             # for any volume BDMs that don't have volume_id set and if we
3028             # detect that, we need to refresh the BDM list before proceeding.
3029             # TODO(mriedem): Move this into _delete_instance and make the bdms
3030             # parameter optional.
3031             for bdm in list(bdms):
3032                 if bdm.is_volume and not bdm.volume_id:
3033                     LOG.debug('There are potentially stale BDMs during '
3034                               'delete, refreshing the BlockDeviceMappingList.',
3035                               instance=instance)
3036                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3037                         context, instance.uuid)
3038                     break
3039             try:
3040                 self._delete_instance(context, instance, bdms)
3041             except exception.InstanceNotFound:
3042                 LOG.info("Instance disappeared during terminate",
3043                          instance=instance)
3044             except Exception:
3045                 # As we're trying to delete always go to Error if something
3046                 # goes wrong that _delete_instance can't handle.
3047                 with excutils.save_and_reraise_exception():
3048                     LOG.exception('Setting instance vm_state to ERROR',
3049                                   instance=instance)
3050                     self._set_instance_obj_error_state(instance)
3051 
3052         do_terminate_instance(instance, bdms)
3053 
3054     # NOTE(johannes): This is probably better named power_off_instance
3055     # so it matches the driver method, but because of other issues, we
3056     # can't use that name in grizzly.
3057     @wrap_exception()
3058     @reverts_task_state
3059     @wrap_instance_event(prefix='compute')
3060     @wrap_instance_fault
3061     def stop_instance(self, context, instance, clean_shutdown):
3062         """Stopping an instance on this host."""
3063 
3064         @utils.synchronized(instance.uuid)
3065         def do_stop_instance():
3066             current_power_state = self._get_power_state(instance)
3067             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
3068                       'current task_state: %(task_state)s, current DB '
3069                       'power_state: %(db_power_state)s, current VM '
3070                       'power_state: %(current_power_state)s',
3071                       {'vm_state': instance.vm_state,
3072                        'task_state': instance.task_state,
3073                        'db_power_state': instance.power_state,
3074                        'current_power_state': current_power_state},
3075                       instance_uuid=instance.uuid)
3076 
3077             # NOTE(mriedem): If the instance is already powered off, we are
3078             # possibly tearing down and racing with other operations, so we can
3079             # expect the task_state to be None if something else updates the
3080             # instance and we're not locking it.
3081             expected_task_state = [task_states.POWERING_OFF]
3082             # The list of power states is from _sync_instance_power_state.
3083             if current_power_state in (power_state.NOSTATE,
3084                                        power_state.SHUTDOWN,
3085                                        power_state.CRASHED):
3086                 LOG.info('Instance is already powered off in the '
3087                          'hypervisor when stop is called.',
3088                          instance=instance)
3089                 expected_task_state.append(None)
3090 
3091             self._notify_about_instance_usage(context, instance,
3092                                               "power_off.start")
3093 
3094             compute_utils.notify_about_instance_action(context, instance,
3095                         self.host, action=fields.NotificationAction.POWER_OFF,
3096                         phase=fields.NotificationPhase.START)
3097 
3098             self._power_off_instance(instance, clean_shutdown)
3099             instance.power_state = self._get_power_state(instance)
3100             instance.vm_state = vm_states.STOPPED
3101             instance.task_state = None
3102             instance.save(expected_task_state=expected_task_state)
3103             self._notify_about_instance_usage(context, instance,
3104                                               "power_off.end")
3105 
3106             compute_utils.notify_about_instance_action(context, instance,
3107                         self.host, action=fields.NotificationAction.POWER_OFF,
3108                         phase=fields.NotificationPhase.END)
3109 
3110         do_stop_instance()
3111 
3112     def _power_on(self, context, instance):
3113         network_info = self.network_api.get_instance_nw_info(context, instance)
3114         block_device_info = self._get_instance_block_device_info(context,
3115                                                                  instance)
3116         accel_info = self._get_accel_info(context, instance)
3117         self.driver.power_on(context, instance,
3118                              network_info,
3119                              block_device_info, accel_info)
3120 
3121     def _delete_snapshot_of_shelved_instance(self, context, instance,
3122                                              snapshot_id):
3123         """Delete snapshot of shelved instance."""
3124         try:
3125             self.image_api.delete(context, snapshot_id)
3126         except (exception.ImageNotFound,
3127                 exception.ImageNotAuthorized) as exc:
3128             LOG.warning("Failed to delete snapshot "
3129                         "from shelved instance (%s).",
3130                         exc.format_message(), instance=instance)
3131         except Exception:
3132             LOG.exception("Something wrong happened when trying to "
3133                           "delete snapshot from shelved instance.",
3134                           instance=instance)
3135 
3136     # NOTE(johannes): This is probably better named power_on_instance
3137     # so it matches the driver method, but because of other issues, we
3138     # can't use that name in grizzly.
3139     @wrap_exception()
3140     @reverts_task_state
3141     @wrap_instance_event(prefix='compute')
3142     @wrap_instance_fault
3143     def start_instance(self, context, instance):
3144         """Starting an instance on this host."""
3145         self._notify_about_instance_usage(context, instance, "power_on.start")
3146         compute_utils.notify_about_instance_action(context, instance,
3147             self.host, action=fields.NotificationAction.POWER_ON,
3148             phase=fields.NotificationPhase.START)
3149         self._power_on(context, instance)
3150         instance.power_state = self._get_power_state(instance)
3151         instance.vm_state = vm_states.ACTIVE
3152         instance.task_state = None
3153 
3154         # Delete an image(VM snapshot) for a shelved instance
3155         snapshot_id = instance.system_metadata.get('shelved_image_id')
3156         if snapshot_id:
3157             self._delete_snapshot_of_shelved_instance(context, instance,
3158                                                       snapshot_id)
3159 
3160         # Delete system_metadata for a shelved instance
3161         compute_utils.remove_shelved_keys_from_system_metadata(instance)
3162 
3163         instance.save(expected_task_state=task_states.POWERING_ON)
3164         self._notify_about_instance_usage(context, instance, "power_on.end")
3165         compute_utils.notify_about_instance_action(context, instance,
3166             self.host, action=fields.NotificationAction.POWER_ON,
3167             phase=fields.NotificationPhase.END)
3168 
3169     @messaging.expected_exceptions(NotImplementedError,
3170                                    exception.TriggerCrashDumpNotSupported,
3171                                    exception.InstanceNotRunning)
3172     @wrap_exception()
3173     @wrap_instance_event(prefix='compute')
3174     @wrap_instance_fault
3175     def trigger_crash_dump(self, context, instance):
3176         """Trigger crash dump in an instance."""
3177 
3178         self._notify_about_instance_usage(context, instance,
3179                                           "trigger_crash_dump.start")
3180         compute_utils.notify_about_instance_action(context, instance,
3181                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3182                 phase=fields.NotificationPhase.START)
3183 
3184         # This method does not change task_state and power_state because the
3185         # effect of a trigger depends on user's configuration.
3186         self.driver.trigger_crash_dump(instance)
3187 
3188         self._notify_about_instance_usage(context, instance,
3189                                           "trigger_crash_dump.end")
3190         compute_utils.notify_about_instance_action(context, instance,
3191                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3192                 phase=fields.NotificationPhase.END)
3193 
3194     @wrap_exception()
3195     @reverts_task_state
3196     @wrap_instance_event(prefix='compute')
3197     @wrap_instance_fault
3198     def soft_delete_instance(self, context, instance):
3199         """Soft delete an instance on this host."""
3200         with compute_utils.notify_about_instance_delete(
3201                 self.notifier, context, instance, 'soft_delete',
3202                 source=fields.NotificationSource.COMPUTE):
3203             try:
3204                 self.driver.soft_delete(instance)
3205             except NotImplementedError:
3206                 # Fallback to just powering off the instance if the
3207                 # hypervisor doesn't implement the soft_delete method
3208                 self.driver.power_off(instance)
3209             instance.power_state = self._get_power_state(instance)
3210             instance.vm_state = vm_states.SOFT_DELETED
3211             instance.task_state = None
3212             instance.save(expected_task_state=[task_states.SOFT_DELETING])
3213 
3214     @wrap_exception()
3215     @reverts_task_state
3216     @wrap_instance_event(prefix='compute')
3217     @wrap_instance_fault
3218     def restore_instance(self, context, instance):
3219         """Restore a soft-deleted instance on this host."""
3220         self._notify_about_instance_usage(context, instance, "restore.start")
3221         compute_utils.notify_about_instance_action(context, instance,
3222             self.host, action=fields.NotificationAction.RESTORE,
3223             phase=fields.NotificationPhase.START)
3224         try:
3225             self.driver.restore(instance)
3226         except NotImplementedError:
3227             # Fallback to just powering on the instance if the hypervisor
3228             # doesn't implement the restore method
3229             self._power_on(context, instance)
3230         instance.power_state = self._get_power_state(instance)
3231         instance.vm_state = vm_states.ACTIVE
3232         instance.task_state = None
3233         instance.save(expected_task_state=task_states.RESTORING)
3234         self._notify_about_instance_usage(context, instance, "restore.end")
3235         compute_utils.notify_about_instance_action(context, instance,
3236             self.host, action=fields.NotificationAction.RESTORE,
3237             phase=fields.NotificationPhase.END)
3238 
3239     @staticmethod
3240     def _set_migration_status(migration, status):
3241         """Set the status, and guard against a None being passed in.
3242 
3243         This is useful as some of the compute RPC calls will not pass
3244         a migration object in older versions. The check can be removed when
3245         we move past 4.x major version of the RPC API.
3246         """
3247         if migration:
3248             migration.status = status
3249             migration.save()
3250 
3251     def _rebuild_default_impl(
3252             self, context, instance, image_meta, injected_files,
3253             admin_password, allocations, bdms, detach_block_devices,
3254             attach_block_devices, network_info=None, evacuate=False,
3255             block_device_info=None, preserve_ephemeral=False,
3256             accel_uuids=None):
3257         if preserve_ephemeral:
3258             # The default code path does not support preserving ephemeral
3259             # partitions.
3260             raise exception.PreserveEphemeralNotSupported()
3261 
3262         accel_info = []
3263         if evacuate:
3264             if instance.flavor.extra_specs.get('accel:device_profile'):
3265                 try:
3266                     accel_info = self._get_bound_arq_resources(
3267                         context, instance, accel_uuids or [])
3268                 except (Exception, eventlet.timeout.Timeout) as exc:
3269                     LOG.exception(exc)
3270                     self._build_resources_cleanup(instance, network_info)
3271                     msg = _('Failure getting accelerator resources.')
3272                     raise exception.BuildAbortException(
3273                         instance_uuid=instance.uuid, reason=msg)
3274             detach_block_devices(context, bdms)
3275         else:
3276             self._power_off_instance(instance, clean_shutdown=True)
3277             detach_block_devices(context, bdms)
3278             self.driver.destroy(context, instance,
3279                                 network_info=network_info,
3280                                 block_device_info=block_device_info)
3281             try:
3282                 accel_info = self._get_accel_info(context, instance)
3283             except Exception as exc:
3284                 LOG.exception(exc)
3285                 self._build_resources_cleanup(instance, network_info)
3286                 msg = _('Failure getting accelerator resources.')
3287                 raise exception.BuildAbortException(
3288                     instance_uuid=instance.uuid, reason=msg)
3289 
3290         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
3291         instance.save(expected_task_state=[task_states.REBUILDING])
3292 
3293         new_block_device_info = attach_block_devices(context, instance, bdms)
3294 
3295         instance.task_state = task_states.REBUILD_SPAWNING
3296         instance.save(
3297             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
3298 
3299         with instance.mutated_migration_context():
3300             self.driver.spawn(context, instance, image_meta, injected_files,
3301                               admin_password, allocations,
3302                               network_info=network_info,
3303                               block_device_info=new_block_device_info,
3304                               accel_info=accel_info)
3305 
3306     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
3307         self._notify_about_instance_usage(context, instance,
3308                                           'rebuild.error', fault=error)
3309         compute_utils.notify_about_instance_rebuild(
3310             context, instance, self.host,
3311             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms)
3312 
3313     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported,
3314                                    exception.BuildAbortException)
3315     @wrap_exception()
3316     @reverts_task_state
3317     @wrap_instance_event(prefix='compute')
3318     @wrap_instance_fault
3319     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
3320                          injected_files, new_pass, orig_sys_metadata,
3321                          bdms, recreate, on_shared_storage,
3322                          preserve_ephemeral, migration,
3323                          scheduled_node, limits, request_spec,
3324                          accel_uuids=None):
3325         """Destroy and re-make this instance.
3326 
3327         A 'rebuild' effectively purges all existing data from the system and
3328         remakes the VM with given 'metadata' and 'personalities'.
3329 
3330         :param context: `nova.RequestContext` object
3331         :param instance: Instance object
3332         :param orig_image_ref: Original image_ref before rebuild
3333         :param image_ref: New image_ref for rebuild
3334         :param injected_files: Files to inject
3335         :param new_pass: password to set on rebuilt instance
3336         :param orig_sys_metadata: instance system metadata from pre-rebuild
3337         :param bdms: block-device-mappings to use for rebuild
3338         :param recreate: True if the instance is being evacuated (e.g. the
3339             hypervisor it was on failed) - cleanup of old state will be
3340             skipped.
3341         :param on_shared_storage: True if instance files on shared storage.
3342                                   If not provided then information from the
3343                                   driver will be used to decide if the instance
3344                                   files are available or not on the target host
3345         :param preserve_ephemeral: True if the default ephemeral storage
3346                                    partition must be preserved on rebuild
3347         :param migration: a Migration object if one was created for this
3348                           rebuild operation (if it's a part of evacuate)
3349         :param scheduled_node: A node of the host chosen by the scheduler. If a
3350                                host was specified by the user, this will be
3351                                None
3352         :param limits: Overcommit limits set by the scheduler. If a host was
3353                        specified by the user, this will be None
3354         :param request_spec: a RequestSpec object used to schedule the instance
3355         :param accel_uuids: a list of cyborg ARQ uuids or None if the RPC API
3356                             is <=5.11
3357 
3358         """
3359         # recreate=True means the instance is being evacuated from a failed
3360         # host to a new destination host (this host). The 'recreate' variable
3361         # name is confusing, so rename it to evacuate here at the top, which
3362         # is simpler than renaming a parameter in an RPC versioned method.
3363         evacuate = recreate
3364         context = context.elevated()
3365 
3366         if evacuate:
3367             LOG.info("Evacuating instance", instance=instance)
3368         else:
3369             LOG.info("Rebuilding instance", instance=instance)
3370 
3371         if evacuate:
3372             # This is an evacuation to a new host, so we need to perform a
3373             # resource claim.
3374             rebuild_claim = self.rt.rebuild_claim
3375         else:
3376             # This is a rebuild to the same host, so we don't need to make
3377             # a claim since the instance is already on this host.
3378             rebuild_claim = claims.NopClaim
3379 
3380         if image_ref:
3381             image_meta = objects.ImageMeta.from_image_ref(
3382                 context, self.image_api, image_ref)
3383         elif evacuate:
3384             # For evacuate the API does not send down the image_ref since the
3385             # image does not change so just get it from what was stashed in
3386             # the instance system_metadata when the instance was created (or
3387             # last rebuilt). This also works for volume-backed instances.
3388             image_meta = instance.image_meta
3389         else:
3390             image_meta = objects.ImageMeta()
3391 
3392         # NOTE(mriedem): On an evacuate, we need to update
3393         # the instance's host and node properties to reflect it's
3394         # destination node for the evacuate.
3395         if not scheduled_node:
3396             if evacuate:
3397                 try:
3398                     compute_node = self._get_compute_info(context, self.host)
3399                     scheduled_node = compute_node.hypervisor_hostname
3400                 except exception.ComputeHostNotFound:
3401                     LOG.exception('Failed to get compute_info for %s',
3402                                   self.host)
3403             else:
3404                 scheduled_node = instance.node
3405 
3406         allocs = self.reportclient.get_allocations_for_consumer(
3407                     context, instance.uuid)
3408 
3409         # If the resource claim or group policy validation fails before we
3410         # do anything to the guest or its networking/volumes we want to keep
3411         # the current status rather than put the instance into ERROR status.
3412         instance_state = instance.vm_state
3413         with self._error_out_instance_on_exception(
3414                 context, instance, instance_state=instance_state):
3415             try:
3416                 self._do_rebuild_instance_with_claim(
3417                     context, instance, orig_image_ref,
3418                     image_meta, injected_files, new_pass, orig_sys_metadata,
3419                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3420                     migration, request_spec, allocs, rebuild_claim,
3421                     scheduled_node, limits, accel_uuids)
3422             except (exception.ComputeResourcesUnavailable,
3423                     exception.RescheduledException) as e:
3424                 if isinstance(e, exception.ComputeResourcesUnavailable):
3425                     LOG.debug("Could not rebuild instance on this host, not "
3426                               "enough resources available.", instance=instance)
3427                 else:
3428                     # RescheduledException is raised by the late server group
3429                     # policy check during evacuation if a parallel scheduling
3430                     # violated the policy.
3431                     # We catch the RescheduledException here but we don't have
3432                     # the plumbing to do an actual reschedule so we abort the
3433                     # operation.
3434                     LOG.debug("Could not rebuild instance on this host, "
3435                               "late server group check failed.",
3436                               instance=instance)
3437                 # NOTE(ndipanov): We just abort the build for now and leave a
3438                 # migration record for potential cleanup later
3439                 self._set_migration_status(migration, 'failed')
3440                 # Since the claim failed, we need to remove the allocation
3441                 # created against the destination node. Note that we can only
3442                 # get here when evacuating to a destination node. Rebuilding
3443                 # on the same host (not evacuate) uses the NopClaim which will
3444                 # not raise ComputeResourcesUnavailable.
3445                 self.rt.delete_allocation_for_evacuated_instance(
3446                     context, instance, scheduled_node, node_type='destination')
3447                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3448                 # Wrap this in InstanceFaultRollback so that the
3449                 # _error_out_instance_on_exception context manager keeps the
3450                 # vm_state unchanged.
3451                 raise exception.InstanceFaultRollback(
3452                     inner_exception=exception.BuildAbortException(
3453                         instance_uuid=instance.uuid,
3454                         reason=e.format_message()))
3455             except (exception.InstanceNotFound,
3456                     exception.UnexpectedDeletingTaskStateError) as e:
3457                 LOG.debug('Instance was deleted while rebuilding',
3458                           instance=instance)
3459                 self._set_migration_status(migration, 'failed')
3460                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3461             except Exception as e:
3462                 self._set_migration_status(migration, 'failed')
3463                 if evacuate or scheduled_node is not None:
3464                     self.rt.delete_allocation_for_evacuated_instance(
3465                         context, instance, scheduled_node,
3466                         node_type='destination')
3467                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3468                 raise
3469             else:
3470                 # NOTE(gibi): Let the resource tracker set the instance
3471                 # host and drop the migration context as we need to hold the
3472                 # COMPUTE_RESOURCE_SEMAPHORE to avoid the race with
3473                 # _update_available_resources. See bug 1896463.
3474                 self.rt.finish_evacuation(instance, scheduled_node, migration)
3475 
3476     def _do_rebuild_instance_with_claim(
3477             self, context, instance, orig_image_ref, image_meta,
3478             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3479             on_shared_storage, preserve_ephemeral, migration, request_spec,
3480             allocations, rebuild_claim, scheduled_node, limits, accel_uuids):
3481         """Helper to avoid deep nesting in the top-level method."""
3482 
3483         provider_mapping = None
3484         if evacuate:
3485             provider_mapping = self._get_request_group_mapping(request_spec)
3486 
3487             if provider_mapping:
3488                 compute_utils.\
3489                     update_pci_request_spec_with_allocated_interface_name(
3490                         context, self.reportclient,
3491                         instance.pci_requests.requests, provider_mapping)
3492 
3493         claim_context = rebuild_claim(
3494             context, instance, scheduled_node, allocations,
3495             limits=limits, image_meta=image_meta, migration=migration)
3496 
3497         with claim_context:
3498             self._do_rebuild_instance(
3499                 context, instance, orig_image_ref, image_meta, injected_files,
3500                 new_pass, orig_sys_metadata, bdms, evacuate, on_shared_storage,
3501                 preserve_ephemeral, migration, request_spec, allocations,
3502                 provider_mapping, accel_uuids)
3503 
3504     @staticmethod
3505     def _get_image_name(image_meta):
3506         if image_meta.obj_attr_is_set("name"):
3507             return image_meta.name
3508         else:
3509             return ''
3510 
3511     def _do_rebuild_instance(
3512             self, context, instance, orig_image_ref, image_meta,
3513             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3514             on_shared_storage, preserve_ephemeral, migration, request_spec,
3515             allocations, request_group_resource_providers_mapping,
3516             accel_uuids):
3517         orig_vm_state = instance.vm_state
3518 
3519         if evacuate:
3520             if request_spec:
3521                 # NOTE(gibi): Do a late check of server group policy as
3522                 # parallel scheduling could violate such policy. This will
3523                 # cause the evacuate to fail as rebuild does not implement
3524                 # reschedule.
3525                 hints = self._get_scheduler_hints({}, request_spec)
3526                 self._validate_instance_group_policy(context, instance, hints)
3527 
3528             if not self.driver.capabilities.get("supports_evacuate", False):
3529                 raise exception.InstanceEvacuateNotSupported
3530 
3531             self._check_instance_exists(instance)
3532 
3533             if on_shared_storage is None:
3534                 LOG.debug('on_shared_storage is not provided, using driver '
3535                           'information to decide if the instance needs to '
3536                           'be evacuated')
3537                 on_shared_storage = self.driver.instance_on_disk(instance)
3538 
3539             elif (on_shared_storage !=
3540                     self.driver.instance_on_disk(instance)):
3541                 # To cover case when admin expects that instance files are
3542                 # on shared storage, but not accessible and vice versa
3543                 raise exception.InvalidSharedStorage(
3544                         _("Invalid state of instance files on shared"
3545                             " storage"))
3546 
3547             if on_shared_storage:
3548                 LOG.info('disk on shared storage, evacuating using'
3549                          ' existing disk')
3550             elif instance.image_ref:
3551                 orig_image_ref = instance.image_ref
3552                 LOG.info("disk not on shared storage, evacuating from "
3553                          "image: '%s'", str(orig_image_ref))
3554             else:
3555                 LOG.info('disk on volume, evacuating using existing '
3556                          'volume')
3557 
3558         # We check trusted certs capabilities for both evacuate (rebuild on
3559         # another host) and rebuild (rebuild on the same host) because for
3560         # evacuate we need to make sure an instance with trusted certs can
3561         # have the image verified with those certs during rebuild, and for
3562         # rebuild we could be rebuilding a server that started out with no
3563         # trusted certs on this host, and then was rebuilt with trusted certs
3564         # for a new image, in which case we need to validate that new image
3565         # with the trusted certs during the rebuild.
3566         self._check_trusted_certs(instance)
3567 
3568         # This instance.exists message should contain the original
3569         # image_ref, not the new one.  Since the DB has been updated
3570         # to point to the new one... we have to override it.
3571         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3572                                                                context)
3573         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3574         compute_utils.notify_usage_exists(
3575                 self.notifier, context, instance, self.host,
3576                 current_period=True, system_metadata=orig_sys_metadata,
3577                 extra_usage_info=extra_usage_info)
3578 
3579         # This message should contain the new image_ref
3580         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3581         self._notify_about_instance_usage(context, instance,
3582                 "rebuild.start", extra_usage_info=extra_usage_info)
3583         # NOTE: image_name is not included in the versioned notification
3584         # because we already provide the image_uuid in the notification
3585         # payload and the image details can be looked up via the uuid.
3586         compute_utils.notify_about_instance_rebuild(
3587             context, instance, self.host,
3588             phase=fields.NotificationPhase.START,
3589             bdms=bdms)
3590 
3591         instance.power_state = self._get_power_state(instance)
3592         instance.task_state = task_states.REBUILDING
3593         instance.save(expected_task_state=[task_states.REBUILDING])
3594 
3595         if evacuate:
3596             self.network_api.setup_networks_on_host(
3597                     context, instance, self.host)
3598             # For nova-network this is needed to move floating IPs
3599             # For neutron this updates the host in the port binding
3600             # TODO(cfriesen): this network_api call and the one above
3601             # are so similar, we should really try to unify them.
3602             self.network_api.setup_instance_network_on_host(
3603                 context, instance, self.host, migration,
3604                 provider_mappings=request_group_resource_providers_mapping)
3605             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3606             # with @api.refresh_cache and then we wouldn't need this explicit
3607             # call to get_instance_nw_info.
3608             network_info = self.network_api.get_instance_nw_info(context,
3609                                                                  instance)
3610         else:
3611             network_info = instance.get_network_info()
3612 
3613         if bdms is None:
3614             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3615                     context, instance.uuid)
3616 
3617         block_device_info = \
3618             self._get_instance_block_device_info(
3619                     context, instance, bdms=bdms)
3620 
3621         def detach_block_devices(context, bdms):
3622             for bdm in bdms:
3623                 if bdm.is_volume:
3624                     # NOTE (ildikov): Having the attachment_id set in the BDM
3625                     # means that it's the new Cinder attach/detach flow
3626                     # (available from v3.44). In that case we explicitly
3627                     # attach and detach the volumes through attachment level
3628                     # operations. In this scenario _detach_volume will delete
3629                     # the existing attachment which would make the volume
3630                     # status change to 'available' if we don't pre-create
3631                     # another empty attachment before deleting the old one.
3632                     attachment_id = None
3633                     if bdm.attachment_id:
3634                         attachment_id = self.volume_api.attachment_create(
3635                             context, bdm['volume_id'], instance.uuid)['id']
3636                     self._detach_volume(context, bdm, instance,
3637                                         destroy_bdm=False)
3638                     if attachment_id:
3639                         bdm.attachment_id = attachment_id
3640                         bdm.save()
3641 
3642         files = self._decode_files(injected_files)
3643 
3644         kwargs = dict(
3645             context=context,
3646             instance=instance,
3647             image_meta=image_meta,
3648             injected_files=files,
3649             admin_password=new_pass,
3650             allocations=allocations,
3651             bdms=bdms,
3652             detach_block_devices=detach_block_devices,
3653             attach_block_devices=self._prep_block_device,
3654             block_device_info=block_device_info,
3655             network_info=network_info,
3656             preserve_ephemeral=preserve_ephemeral,
3657             evacuate=evacuate,
3658             accel_uuids=accel_uuids)
3659         try:
3660             with instance.mutated_migration_context():
3661                 self.driver.rebuild(**kwargs)
3662         except NotImplementedError:
3663             # NOTE(rpodolyaka): driver doesn't provide specialized version
3664             # of rebuild, fall back to the default implementation
3665             self._rebuild_default_impl(**kwargs)
3666         self._update_instance_after_spawn(instance)
3667         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3668 
3669         if orig_vm_state == vm_states.STOPPED:
3670             LOG.info("bringing vm to original state: '%s'",
3671                      orig_vm_state, instance=instance)
3672             instance.vm_state = vm_states.ACTIVE
3673             instance.task_state = task_states.POWERING_OFF
3674             instance.progress = 0
3675             instance.save()
3676             self.stop_instance(context, instance, False)
3677         # TODO(melwitt): We should clean up instance console tokens here in the
3678         # case of evacuate. The instance is on a new host and will need to
3679         # establish a new console connection.
3680         self._update_scheduler_instance_info(context, instance)
3681         self._notify_about_instance_usage(
3682                 context, instance, "rebuild.end",
3683                 network_info=network_info,
3684                 extra_usage_info=extra_usage_info)
3685         compute_utils.notify_about_instance_rebuild(
3686             context, instance, self.host,
3687             phase=fields.NotificationPhase.END,
3688             bdms=bdms)
3689 
3690     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3691                                      block_device_info):
3692         """Handle cases where the virt-layer had to detach non-working volumes
3693         in order to complete an operation.
3694         """
3695         for bdm in block_device_info['block_device_mapping']:
3696             if bdm.get('mount_device') in bad_devices:
3697                 try:
3698                     volume_id = bdm['connection_info']['data']['volume_id']
3699                 except KeyError:
3700                     continue
3701 
3702                 # NOTE(sirp): ideally we'd just call
3703                 # `compute_api.detach_volume` here but since that hits the
3704                 # DB directly, that's off limits from within the
3705                 # compute-manager.
3706                 #
3707                 # API-detach
3708                 LOG.info("Detaching from volume api: %s", volume_id)
3709                 self.volume_api.begin_detaching(context, volume_id)
3710 
3711                 # Manager-detach
3712                 self.detach_volume(context, volume_id, instance)
3713 
3714     def _get_accel_info(self, context, instance):
3715         dp_name = instance.flavor.extra_specs.get('accel:device_profile')
3716         if dp_name:
3717             cyclient = cyborg.get_client(context)
3718             accel_info = cyclient.get_arqs_for_instance(instance.uuid)
3719         else:
3720             accel_info = []
3721         return accel_info
3722 
3723     @wrap_exception()
3724     @reverts_task_state
3725     @wrap_instance_event(prefix='compute')
3726     @wrap_instance_fault
3727     def reboot_instance(self, context, instance, block_device_info,
3728                         reboot_type):
3729         @utils.synchronized(instance.uuid)
3730         def do_reboot_instance(context, instance, block_device_info,
3731                                reboot_type):
3732             self._reboot_instance(context, instance, block_device_info,
3733                                   reboot_type)
3734         do_reboot_instance(context, instance, block_device_info, reboot_type)
3735 
3736     def _reboot_instance(self, context, instance, block_device_info,
3737                          reboot_type):
3738         """Reboot an instance on this host."""
3739         # acknowledge the request made it to the manager
3740         if reboot_type == "SOFT":
3741             instance.task_state = task_states.REBOOT_PENDING
3742             expected_states = task_states.soft_reboot_states
3743         else:
3744             instance.task_state = task_states.REBOOT_PENDING_HARD
3745             expected_states = task_states.hard_reboot_states
3746 
3747         context = context.elevated()
3748         LOG.info("Rebooting instance", instance=instance)
3749 
3750         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3751             context, instance.uuid)
3752         block_device_info = self._get_instance_block_device_info(
3753             context, instance, bdms=bdms)
3754 
3755         network_info = self.network_api.get_instance_nw_info(context, instance)
3756 
3757         accel_info = self._get_accel_info(context, instance)
3758 
3759         self._notify_about_instance_usage(context, instance, "reboot.start")
3760         compute_utils.notify_about_instance_action(
3761             context, instance, self.host,
3762             action=fields.NotificationAction.REBOOT,
3763             phase=fields.NotificationPhase.START,
3764             bdms=bdms
3765         )
3766 
3767         instance.power_state = self._get_power_state(instance)
3768         instance.save(expected_task_state=expected_states)
3769 
3770         if instance.power_state != power_state.RUNNING:
3771             state = instance.power_state
3772             running = power_state.RUNNING
3773             LOG.warning('trying to reboot a non-running instance:'
3774                         ' (state: %(state)s expected: %(running)s)',
3775                         {'state': state, 'running': running},
3776                         instance=instance)
3777 
3778         def bad_volumes_callback(bad_devices):
3779             self._handle_bad_volumes_detached(
3780                     context, instance, bad_devices, block_device_info)
3781 
3782         try:
3783             # Don't change it out of rescue mode
3784             if instance.vm_state == vm_states.RESCUED:
3785                 new_vm_state = vm_states.RESCUED
3786             else:
3787                 new_vm_state = vm_states.ACTIVE
3788             new_power_state = None
3789             if reboot_type == "SOFT":
3790                 instance.task_state = task_states.REBOOT_STARTED
3791                 expected_state = task_states.REBOOT_PENDING
3792             else:
3793                 instance.task_state = task_states.REBOOT_STARTED_HARD
3794                 expected_state = task_states.REBOOT_PENDING_HARD
3795             instance.save(expected_task_state=expected_state)
3796             self.driver.reboot(context, instance,
3797                                network_info,
3798                                reboot_type,
3799                                block_device_info=block_device_info,
3800                                accel_info=accel_info,
3801                                bad_volumes_callback=bad_volumes_callback)
3802 
3803         except Exception as error:
3804             with excutils.save_and_reraise_exception() as ctxt:
3805                 exc_info = sys.exc_info()
3806                 # if the reboot failed but the VM is running don't
3807                 # put it into an error state
3808                 new_power_state = self._get_power_state(instance)
3809                 if new_power_state == power_state.RUNNING:
3810                     LOG.warning('Reboot failed but instance is running',
3811                                 instance=instance)
3812                     compute_utils.add_instance_fault_from_exc(context,
3813                             instance, error, exc_info)
3814                     self._notify_about_instance_usage(context, instance,
3815                             'reboot.error', fault=error)
3816                     compute_utils.notify_about_instance_action(
3817                         context, instance, self.host,
3818                         action=fields.NotificationAction.REBOOT,
3819                         phase=fields.NotificationPhase.ERROR,
3820                         exception=error, bdms=bdms
3821                     )
3822                     ctxt.reraise = False
3823                 else:
3824                     LOG.error('Cannot reboot instance: %s', error,
3825                               instance=instance)
3826                     self._set_instance_obj_error_state(instance)
3827 
3828         if not new_power_state:
3829             new_power_state = self._get_power_state(instance)
3830         try:
3831             instance.power_state = new_power_state
3832             instance.vm_state = new_vm_state
3833             instance.task_state = None
3834             instance.save()
3835         except exception.InstanceNotFound:
3836             LOG.warning("Instance disappeared during reboot",
3837                         instance=instance)
3838 
3839         self._notify_about_instance_usage(context, instance, "reboot.end")
3840         compute_utils.notify_about_instance_action(
3841             context, instance, self.host,
3842             action=fields.NotificationAction.REBOOT,
3843             phase=fields.NotificationPhase.END,
3844             bdms=bdms
3845         )
3846 
3847     @delete_image_on_error
3848     def _do_snapshot_instance(self, context, image_id, instance):
3849         self._snapshot_instance(context, image_id, instance,
3850                                 task_states.IMAGE_BACKUP)
3851 
3852     @wrap_exception()
3853     @reverts_task_state
3854     @wrap_instance_event(prefix='compute')
3855     @wrap_instance_fault
3856     def backup_instance(self, context, image_id, instance, backup_type,
3857                         rotation):
3858         """Backup an instance on this host.
3859 
3860         :param backup_type: daily | weekly
3861         :param rotation: int representing how many backups to keep around
3862         """
3863         self._do_snapshot_instance(context, image_id, instance)
3864         self._rotate_backups(context, instance, backup_type, rotation)
3865 
3866     @wrap_exception()
3867     @reverts_task_state
3868     @wrap_instance_event(prefix='compute')
3869     @wrap_instance_fault
3870     @delete_image_on_error
3871     def snapshot_instance(self, context, image_id, instance):
3872         """Snapshot an instance on this host.
3873 
3874         :param context: security context
3875         :param image_id: glance.db.sqlalchemy.models.Image.Id
3876         :param instance: a nova.objects.instance.Instance object
3877         """
3878         # NOTE(dave-mcnally) the task state will already be set by the api
3879         # but if the compute manager has crashed/been restarted prior to the
3880         # request getting here the task state may have been cleared so we set
3881         # it again and things continue normally
3882         try:
3883             instance.task_state = task_states.IMAGE_SNAPSHOT
3884             instance.save(
3885                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3886         except exception.InstanceNotFound:
3887             # possibility instance no longer exists, no point in continuing
3888             LOG.debug("Instance not found, could not set state %s "
3889                       "for instance.",
3890                       task_states.IMAGE_SNAPSHOT, instance=instance)
3891             return
3892 
3893         except exception.UnexpectedDeletingTaskStateError:
3894             LOG.debug("Instance being deleted, snapshot cannot continue",
3895                       instance=instance)
3896             return
3897 
3898         with self._snapshot_semaphore:
3899             self._snapshot_instance(context, image_id, instance,
3900                                     task_states.IMAGE_SNAPSHOT)
3901 
3902     def _snapshot_instance(self, context, image_id, instance,
3903                            expected_task_state):
3904         context = context.elevated()
3905 
3906         instance.power_state = self._get_power_state(instance)
3907         try:
3908             instance.save()
3909 
3910             LOG.info('instance snapshotting', instance=instance)
3911 
3912             if instance.power_state != power_state.RUNNING:
3913                 state = instance.power_state
3914                 running = power_state.RUNNING
3915                 LOG.warning('trying to snapshot a non-running instance: '
3916                             '(state: %(state)s expected: %(running)s)',
3917                             {'state': state, 'running': running},
3918                             instance=instance)
3919 
3920             self._notify_about_instance_usage(
3921                 context, instance, "snapshot.start")
3922             compute_utils.notify_about_instance_snapshot(context, instance,
3923                 self.host, phase=fields.NotificationPhase.START,
3924                 snapshot_image_id=image_id)
3925 
3926             def update_task_state(task_state,
3927                                   expected_state=expected_task_state):
3928                 instance.task_state = task_state
3929                 instance.save(expected_task_state=expected_state)
3930 
3931             with timeutils.StopWatch() as timer:
3932                 self.driver.snapshot(context, instance, image_id,
3933                                      update_task_state)
3934             LOG.info('Took %0.2f seconds to snapshot the instance on '
3935                      'the hypervisor.', timer.elapsed(), instance=instance)
3936 
3937             instance.task_state = None
3938             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3939 
3940             self._notify_about_instance_usage(context, instance,
3941                                               "snapshot.end")
3942             compute_utils.notify_about_instance_snapshot(context, instance,
3943                 self.host, phase=fields.NotificationPhase.END,
3944                 snapshot_image_id=image_id)
3945         except (exception.InstanceNotFound,
3946                 exception.InstanceNotRunning,
3947                 exception.UnexpectedDeletingTaskStateError):
3948             # the instance got deleted during the snapshot
3949             # Quickly bail out of here
3950             msg = 'Instance disappeared during snapshot'
3951             LOG.debug(msg, instance=instance)
3952             try:
3953                 image = self.image_api.get(context, image_id)
3954                 if image['status'] != 'active':
3955                     self.image_api.delete(context, image_id)
3956             except exception.ImageNotFound:
3957                 LOG.debug('Image not found during clean up %s', image_id)
3958             except Exception:
3959                 LOG.warning("Error while trying to clean up image %s",
3960                             image_id, instance=instance)
3961         except exception.ImageNotFound:
3962             instance.task_state = None
3963             instance.save()
3964             LOG.warning("Image not found during snapshot", instance=instance)
3965 
3966     @messaging.expected_exceptions(NotImplementedError)
3967     @wrap_exception()
3968     def volume_snapshot_create(self, context, instance, volume_id,
3969                                create_info):
3970         try:
3971             self.driver.volume_snapshot_create(context, instance, volume_id,
3972                                                create_info)
3973         except exception.InstanceNotRunning:
3974             # Libvirt driver can raise this exception
3975             LOG.debug('Instance disappeared during volume snapshot create',
3976                       instance=instance)
3977 
3978     @messaging.expected_exceptions(NotImplementedError)
3979     @wrap_exception()
3980     def volume_snapshot_delete(self, context, instance, volume_id,
3981                                snapshot_id, delete_info):
3982         try:
3983             self.driver.volume_snapshot_delete(context, instance, volume_id,
3984                                                snapshot_id, delete_info)
3985         except exception.InstanceNotRunning:
3986             # Libvirt driver can raise this exception
3987             LOG.debug('Instance disappeared during volume snapshot delete',
3988                       instance=instance)
3989 
3990     @wrap_instance_fault
3991     def _rotate_backups(self, context, instance, backup_type, rotation):
3992         """Delete excess backups associated to an instance.
3993 
3994         Instances are allowed a fixed number of backups (the rotation number);
3995         this method deletes the oldest backups that exceed the rotation
3996         threshold.
3997 
3998         :param context: security context
3999         :param instance: Instance dict
4000         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
4001         :param rotation: int representing how many backups to keep around;
4002             None if rotation shouldn't be used (as in the case of snapshots)
4003         """
4004         filters = {'property-image_type': 'backup',
4005                    'property-backup_type': backup_type,
4006                    'property-instance_uuid': instance.uuid}
4007 
4008         images = self.image_api.get_all(context, filters=filters,
4009                                         sort_key='created_at', sort_dir='desc')
4010         num_images = len(images)
4011         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
4012                   {'num_images': num_images, 'rotation': rotation},
4013                   instance=instance)
4014 
4015         if num_images > rotation:
4016             # NOTE(sirp): this deletes all backups that exceed the rotation
4017             # limit
4018             excess = len(images) - rotation
4019             LOG.debug("Rotating out %d backups", excess,
4020                       instance=instance)
4021             for i in range(excess):
4022                 image = images.pop()
4023                 image_id = image['id']
4024                 LOG.debug("Deleting image %s", image_id,
4025                           instance=instance)
4026                 try:
4027                     self.image_api.delete(context, image_id)
4028                 except exception.ImageNotFound:
4029                     LOG.info("Failed to find image %(image_id)s to "
4030                              "delete", {'image_id': image_id},
4031                              instance=instance)
4032                 except (exception.ImageDeleteConflict, Exception) as exc:
4033                     LOG.info("Failed to delete image %(image_id)s during "
4034                              "deleting excess backups. "
4035                              "Continuing for next image.. %(exc)s",
4036                              {'image_id': image_id, 'exc': exc},
4037                              instance=instance)
4038 
4039     @wrap_exception()
4040     @reverts_task_state
4041     @wrap_instance_event(prefix='compute')
4042     @wrap_instance_fault
4043     def set_admin_password(self, context, instance, new_pass):
4044         """Set the root/admin password for an instance on this host.
4045 
4046         This is generally only called by API password resets after an
4047         image has been built.
4048 
4049         @param context: Nova auth context.
4050         @param instance: Nova instance object.
4051         @param new_pass: The admin password for the instance.
4052         """
4053 
4054         context = context.elevated()
4055         current_power_state = self._get_power_state(instance)
4056         expected_state = power_state.RUNNING
4057 
4058         if current_power_state != expected_state:
4059             instance.task_state = None
4060             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
4061             _msg = _('instance %s is not running') % instance.uuid
4062             raise exception.InstancePasswordSetFailed(
4063                 instance=instance.uuid, reason=_msg)
4064 
4065         try:
4066             self.driver.set_admin_password(instance, new_pass)
4067             LOG.info("Admin password set", instance=instance)
4068             instance.task_state = None
4069             instance.save(
4070                 expected_task_state=task_states.UPDATING_PASSWORD)
4071         except exception.InstanceAgentNotEnabled:
4072             with excutils.save_and_reraise_exception():
4073                 LOG.debug('Guest agent is not enabled for the instance.',
4074                           instance=instance)
4075                 instance.task_state = None
4076                 instance.save(
4077                     expected_task_state=task_states.UPDATING_PASSWORD)
4078         except exception.SetAdminPasswdNotSupported:
4079             with excutils.save_and_reraise_exception():
4080                 LOG.info('set_admin_password is not supported '
4081                          'by this driver or guest instance.',
4082                          instance=instance)
4083                 instance.task_state = None
4084                 instance.save(
4085                     expected_task_state=task_states.UPDATING_PASSWORD)
4086         except NotImplementedError:
4087             LOG.warning('set_admin_password is not implemented '
4088                         'by this driver or guest instance.',
4089                         instance=instance)
4090             instance.task_state = None
4091             instance.save(
4092                 expected_task_state=task_states.UPDATING_PASSWORD)
4093             raise NotImplementedError(_('set_admin_password is not '
4094                                         'implemented by this driver or guest '
4095                                         'instance.'))
4096         except exception.UnexpectedTaskStateError:
4097             # interrupted by another (most likely delete) task
4098             # do not retry
4099             raise
4100         except Exception:
4101             # Catch all here because this could be anything.
4102             LOG.exception('set_admin_password failed', instance=instance)
4103             # We create a new exception here so that we won't
4104             # potentially reveal password information to the
4105             # API caller.  The real exception is logged above
4106             _msg = _('error setting admin password')
4107             raise exception.InstancePasswordSetFailed(
4108                 instance=instance.uuid, reason=_msg)
4109 
4110     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
4111         """Determine what image should be used to boot the rescue VM."""
4112         # 1. If rescue_image_ref is passed in, use that for rescue.
4113         # 2. Else, use the base image associated with instance's current image.
4114         #       The idea here is to provide the customer with a rescue
4115         #       environment which they are familiar with.
4116         #       So, if they built their instance off of a Debian image,
4117         #       their rescue VM will also be Debian.
4118         # 3. As a last resort, use instance's current image.
4119         if not rescue_image_ref:
4120             system_meta = utils.instance_sys_meta(instance)
4121             rescue_image_ref = system_meta.get('image_base_image_ref')
4122 
4123         if not rescue_image_ref:
4124             LOG.warning('Unable to find a different image to use for '
4125                         'rescue VM, using instance\'s current image',
4126                         instance=instance)
4127             rescue_image_ref = instance.image_ref
4128 
4129         return objects.ImageMeta.from_image_ref(
4130             context, self.image_api, rescue_image_ref)
4131 
4132     @wrap_exception()
4133     @reverts_task_state
4134     @wrap_instance_event(prefix='compute')
4135     @wrap_instance_fault
4136     def rescue_instance(self, context, instance, rescue_password,
4137                         rescue_image_ref, clean_shutdown):
4138         context = context.elevated()
4139         LOG.info('Rescuing', instance=instance)
4140 
4141         admin_password = (rescue_password if rescue_password else
4142                       utils.generate_password())
4143 
4144         network_info = self.network_api.get_instance_nw_info(context, instance)
4145 
4146         rescue_image_meta = self._get_rescue_image(context, instance,
4147                                                    rescue_image_ref)
4148 
4149         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4150                                               context, instance.uuid)
4151         block_device_info = self._get_instance_block_device_info(
4152                                 context, instance, bdms=bdms)
4153 
4154         extra_usage_info = {'rescue_image_name':
4155                             self._get_image_name(rescue_image_meta)}
4156         self._notify_about_instance_usage(context, instance,
4157                 "rescue.start", extra_usage_info=extra_usage_info,
4158                 network_info=network_info)
4159         compute_utils.notify_about_instance_rescue_action(
4160             context, instance, self.host, rescue_image_ref,
4161             phase=fields.NotificationPhase.START)
4162 
4163         try:
4164             self._power_off_instance(instance, clean_shutdown)
4165 
4166             self.driver.rescue(context, instance, network_info,
4167                                rescue_image_meta, admin_password,
4168                                block_device_info)
4169         except Exception as e:
4170             LOG.exception("Error trying to Rescue Instance",
4171                           instance=instance)
4172             self._set_instance_obj_error_state(instance)
4173             raise exception.InstanceNotRescuable(
4174                 instance_id=instance.uuid,
4175                 reason=_("Driver Error: %s") % e)
4176 
4177         compute_utils.notify_usage_exists(self.notifier, context, instance,
4178                                           self.host, current_period=True)
4179 
4180         instance.vm_state = vm_states.RESCUED
4181         instance.task_state = None
4182         instance.power_state = self._get_power_state(instance)
4183         instance.launched_at = timeutils.utcnow()
4184         instance.save(expected_task_state=task_states.RESCUING)
4185 
4186         self._notify_about_instance_usage(context, instance,
4187                 "rescue.end", extra_usage_info=extra_usage_info,
4188                 network_info=network_info)
4189         compute_utils.notify_about_instance_rescue_action(
4190             context, instance, self.host, rescue_image_ref,
4191             phase=fields.NotificationPhase.END)
4192 
4193     @wrap_exception()
4194     @reverts_task_state
4195     @wrap_instance_event(prefix='compute')
4196     @wrap_instance_fault
4197     def unrescue_instance(self, context, instance):
4198         orig_context = context
4199         context = context.elevated()
4200         LOG.info('Unrescuing', instance=instance)
4201 
4202         network_info = self.network_api.get_instance_nw_info(context, instance)
4203         self._notify_about_instance_usage(context, instance,
4204                 "unrescue.start", network_info=network_info)
4205         compute_utils.notify_about_instance_action(context, instance,
4206             self.host, action=fields.NotificationAction.UNRESCUE,
4207             phase=fields.NotificationPhase.START)
4208 
4209         with self._error_out_instance_on_exception(context, instance):
4210             self.driver.unrescue(orig_context, instance)
4211 
4212         instance.vm_state = vm_states.ACTIVE
4213         instance.task_state = None
4214         instance.power_state = self._get_power_state(instance)
4215         instance.save(expected_task_state=task_states.UNRESCUING)
4216 
4217         self._notify_about_instance_usage(context,
4218                                           instance,
4219                                           "unrescue.end",
4220                                           network_info=network_info)
4221         compute_utils.notify_about_instance_action(context, instance,
4222             self.host, action=fields.NotificationAction.UNRESCUE,
4223             phase=fields.NotificationPhase.END)
4224 
4225     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
4226     @wrap_exception()
4227     @wrap_instance_fault
4228     def change_instance_metadata(self, context, diff, instance):
4229         raise NotImplementedError()
4230 
4231     @wrap_exception()
4232     @wrap_instance_event(prefix='compute')
4233     @errors_out_migration
4234     @wrap_instance_fault
4235     def confirm_resize(self, context, instance, migration):
4236         """Confirms a migration/resize and deletes the 'old' instance.
4237 
4238         This is called from the API and runs on the source host.
4239 
4240         Nothing needs to happen on the destination host at this point since
4241         the instance is already running there. This routine just cleans up the
4242         source host.
4243         """
4244         @utils.synchronized(instance.uuid)
4245         def do_confirm_resize(context, instance, migration):
4246             LOG.debug("Going to confirm migration %s", migration.id,
4247                       instance=instance)
4248 
4249             if migration.status == 'confirmed':
4250                 LOG.info("Migration %s is already confirmed",
4251                          migration.id, instance=instance)
4252                 return
4253 
4254             if migration.status not in ('finished', 'confirming'):
4255                 LOG.warning("Unexpected confirmation status '%(status)s' "
4256                             "of migration %(id)s, exit confirmation process",
4257                             {"status": migration.status, "id": migration.id},
4258                             instance=instance)
4259                 return
4260 
4261             # NOTE(wangpan): Get the instance from db, if it has been
4262             #                deleted, we do nothing and return here
4263             expected_attrs = ['metadata', 'system_metadata', 'flavor']
4264             try:
4265                 instance = objects.Instance.get_by_uuid(
4266                         context, instance.uuid,
4267                         expected_attrs=expected_attrs)
4268             except exception.InstanceNotFound:
4269                 LOG.info("Instance is not found during confirmation",
4270                          instance=instance)
4271                 return
4272 
4273             with self._error_out_instance_on_exception(context, instance):
4274                 try:
4275                     self._confirm_resize(
4276                         context, instance, migration=migration)
4277                 except Exception:
4278                     # Something failed when cleaning up the source host so
4279                     # log a traceback and leave a hint about hard rebooting
4280                     # the server to correct its state in the DB.
4281                     with excutils.save_and_reraise_exception(logger=LOG):
4282                         LOG.exception(
4283                             'Confirm resize failed on source host %s. '
4284                             'Resource allocations in the placement service '
4285                             'will be removed regardless because the instance '
4286                             'is now on the destination host %s. You can try '
4287                             'hard rebooting the instance to correct its '
4288                             'state.', self.host, migration.dest_compute,
4289                             instance=instance)
4290                 finally:
4291                     # Whether an error occurred or not, at this point the
4292                     # instance is on the dest host. Avoid leaking allocations
4293                     # in placement by deleting them here...
4294                     self._delete_allocation_after_move(
4295                         context, instance, migration)
4296                     # ...inform the scheduler about the move...
4297                     self._delete_scheduler_instance_info(
4298                         context, instance.uuid)
4299                     # ...and unset the cached flavor information (this is done
4300                     # last since the resource tracker relies on it for its
4301                     # periodic tasks)
4302                     self._delete_stashed_flavor_info(instance)
4303 
4304         do_confirm_resize(context, instance, migration)
4305 
4306     def _get_updated_nw_info_with_pci_mapping(self, nw_info, pci_mapping):
4307         # NOTE(adrianc): This method returns a copy of nw_info if modifications
4308         # are made else it returns the original nw_info.
4309         updated_nw_info = nw_info
4310         if nw_info and pci_mapping:
4311             updated_nw_info = copy.deepcopy(nw_info)
4312             for vif in updated_nw_info:
4313                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
4314                     try:
4315                         vif_pci_addr = vif['profile']['pci_slot']
4316                         new_addr = pci_mapping[vif_pci_addr].address
4317                         vif['profile']['pci_slot'] = new_addr
4318                         LOG.debug("Updating VIF's PCI address for VIF %(id)s. "
4319                                   "Original value %(orig_val)s, "
4320                                   "new value %(new_val)s",
4321                                   {'id': vif['id'],
4322                                    'orig_val': vif_pci_addr,
4323                                    'new_val': new_addr})
4324                     except (KeyError, AttributeError):
4325                         with excutils.save_and_reraise_exception():
4326                             # NOTE(adrianc): This should never happen. If we
4327                             # get here it means there is some inconsistency
4328                             # with either 'nw_info' or 'pci_mapping'.
4329                             LOG.error("Unexpected error when updating network "
4330                                       "information with PCI mapping.")
4331         return updated_nw_info
4332 
4333     def _confirm_resize(self, context, instance, migration=None):
4334         """Destroys the source instance."""
4335         self._notify_about_instance_usage(context, instance,
4336                                           "resize.confirm.start")
4337         compute_utils.notify_about_instance_action(context, instance,
4338             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4339             phase=fields.NotificationPhase.START)
4340 
4341         # NOTE(tr3buchet): tear down networks on source host
4342         self.network_api.setup_networks_on_host(context, instance,
4343                            migration.source_compute, teardown=True)
4344 
4345         # TODO(stephenfin): These next three calls should be bundled
4346         network_info = self.network_api.get_instance_nw_info(context,
4347                                                              instance)
4348 
4349         # NOTE(adrianc): Populate old PCI device in VIF profile
4350         # to allow virt driver to properly unplug it from Hypervisor.
4351         pci_mapping = (instance.migration_context.
4352                        get_pci_mapping_for_migration(True))
4353         network_info = self._get_updated_nw_info_with_pci_mapping(
4354             network_info, pci_mapping)
4355 
4356         self.driver.confirm_migration(context, migration, instance,
4357                                       network_info)
4358 
4359         # Free up the old_flavor usage from the resource tracker for this host.
4360         self.rt.drop_move_claim_at_source(context, instance, migration)
4361 
4362         # NOTE(mriedem): The old_vm_state could be STOPPED but the user
4363         # might have manually powered up the instance to confirm the
4364         # resize/migrate, so we need to check the current power state
4365         # on the instance and set the vm_state appropriately. We default
4366         # to ACTIVE because if the power state is not SHUTDOWN, we
4367         # assume _sync_instance_power_state will clean it up.
4368         p_state = instance.power_state
4369         vm_state = None
4370         if p_state == power_state.SHUTDOWN:
4371             vm_state = vm_states.STOPPED
4372             LOG.debug("Resized/migrated instance is powered off. "
4373                       "Setting vm_state to '%s'.", vm_state,
4374                       instance=instance)
4375         else:
4376             vm_state = vm_states.ACTIVE
4377 
4378         instance.vm_state = vm_state
4379         instance.task_state = None
4380         instance.save(expected_task_state=[None, task_states.DELETING,
4381                                            task_states.SOFT_DELETING])
4382 
4383         self._notify_about_instance_usage(
4384             context, instance, "resize.confirm.end",
4385             network_info=network_info)
4386         compute_utils.notify_about_instance_action(context, instance,
4387                self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4388                phase=fields.NotificationPhase.END)
4389 
4390     def _delete_allocation_after_move(self, context, instance, migration):
4391         """Deletes resource allocations held by the migration record against
4392         the source compute node resource provider after a confirmed cold /
4393         successful live migration.
4394         """
4395         try:
4396             # NOTE(danms): We're finishing on the source node, so try
4397             # to delete the allocation based on the migration uuid
4398             self.reportclient.delete_allocation_for_instance(
4399                 context, migration.uuid, consumer_type='migration')
4400         except exception.AllocationDeleteFailed:
4401             LOG.error('Deleting allocation in placement for migration '
4402                       '%(migration_uuid)s failed. The instance '
4403                       '%(instance_uuid)s will be put to ERROR state '
4404                       'but the allocation held by the migration is '
4405                       'leaked.',
4406                       {'instance_uuid': instance.uuid,
4407                        'migration_uuid': migration.uuid})
4408             raise
4409 
4410     def _delete_stashed_flavor_info(self, instance):
4411         """Remove information about the flavor change after a resize."""
4412         instance.old_flavor = None
4413         instance.new_flavor = None
4414         instance.system_metadata.pop('old_vm_state', None)
4415         instance.save()
4416 
4417     @wrap_exception()
4418     @wrap_instance_event(prefix='compute')
4419     @errors_out_migration
4420     @wrap_instance_fault
4421     def confirm_snapshot_based_resize_at_source(
4422             self, ctxt, instance, migration):
4423         """Confirms a snapshot-based resize on the source host.
4424 
4425         Cleans the guest from the source hypervisor including disks and drops
4426         the MoveClaim which will free up "old_flavor" usage from the
4427         ResourceTracker.
4428 
4429         Deletes the allocations held by the migration consumer against the
4430         source compute node resource provider.
4431 
4432         :param ctxt: nova auth request context targeted at the source cell
4433         :param instance: Instance object being resized which should have the
4434             "old_flavor" attribute set
4435         :param migration: Migration object for the resize operation
4436         """
4437 
4438         @utils.synchronized(instance.uuid)
4439         def do_confirm():
4440             LOG.info('Confirming resize on source host.', instance=instance)
4441             with self._error_out_instance_on_exception(ctxt, instance):
4442                 # TODO(mriedem): Could probably make this try/except/finally
4443                 # a context manager to share with confirm_resize().
4444                 try:
4445                     self._confirm_snapshot_based_resize_at_source(
4446                         ctxt, instance, migration)
4447                 except Exception:
4448                     # Something failed when cleaning up the source host so
4449                     # log a traceback and leave a hint about hard rebooting
4450                     # the server to correct its state in the DB.
4451                     with excutils.save_and_reraise_exception(logger=LOG):
4452                         LOG.exception(
4453                             'Confirm resize failed on source host %s. '
4454                             'Resource allocations in the placement service '
4455                             'will be removed regardless because the instance '
4456                             'is now on the destination host %s. You can try '
4457                             'hard rebooting the instance to correct its '
4458                             'state.', self.host, migration.dest_compute,
4459                             instance=instance)
4460                 finally:
4461                     # Whether an error occurred or not, at this point the
4462                     # instance is on the dest host so to avoid leaking
4463                     # allocations in placement, delete them here.
4464                     # TODO(mriedem): Should we catch and just log
4465                     # AllocationDeleteFailed? What is the user's recourse if
4466                     # we got this far but this fails? At this point the
4467                     # instance is on the target host and the allocations
4468                     # could just be manually cleaned up by the operator.
4469                     self._delete_allocation_after_move(ctxt, instance,
4470                                                        migration)
4471         do_confirm()
4472 
4473     def _confirm_snapshot_based_resize_at_source(
4474             self, ctxt, instance, migration):
4475         """Private version of confirm_snapshot_based_resize_at_source
4476 
4477         This allows the main method to be decorated with error handlers.
4478 
4479         :param ctxt: nova auth request context targeted at the source cell
4480         :param instance: Instance object being resized which should have the
4481             "old_flavor" attribute set
4482         :param migration: Migration object for the resize operation
4483         """
4484         # Cleanup the guest from the hypervisor including local disks.
4485         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4486         LOG.debug('Cleaning up guest from source hypervisor including disks.',
4487                   instance=instance)
4488 
4489         # FIXME(mriedem): Per bug 1809095, _confirm_resize calls
4490         # _get_updated_nw_info_with_pci_mapping here prior to unplugging
4491         # VIFs on the source, but in our case we have already unplugged
4492         # VIFs during prep_snapshot_based_resize_at_source, so what do we
4493         # need to do about those kinds of ports? Do we need to wait to unplug
4494         # VIFs until confirm like normal resize?
4495 
4496         # Note that prep_snapshot_based_resize_at_source already destroyed the
4497         # guest which disconnected volumes and unplugged VIFs but did not
4498         # destroy disks in case something failed during the resize and the
4499         # instance needed to be rebooted or rebuilt on the source host. Now
4500         # that we are confirming the resize we want to cleanup the disks left
4501         # on the source host. We call cleanup() instead of destroy() to avoid
4502         # any InstanceNotFound confusion from the driver since the guest was
4503         # already destroyed on this host. block_device_info=None and
4504         # destroy_vifs=False means cleanup() will not try to disconnect volumes
4505         # or unplug VIFs.
4506         self.driver.cleanup(
4507             ctxt, instance, network_info, block_device_info=None,
4508             destroy_disks=True, destroy_vifs=False)
4509 
4510         # Delete port bindings for the source host.
4511         self._confirm_snapshot_based_resize_delete_port_bindings(
4512             ctxt, instance)
4513 
4514         # Delete volume attachments for the source host.
4515         self._delete_volume_attachments(ctxt, instance.get_bdms())
4516 
4517         # Free up the old_flavor usage from the resource tracker for this host.
4518         self.rt.drop_move_claim_at_source(ctxt, instance, migration)
4519 
4520     def _confirm_snapshot_based_resize_delete_port_bindings(
4521             self, ctxt, instance):
4522         """Delete port bindings for the source host when confirming
4523         snapshot-based resize on the source host."
4524 
4525         :param ctxt: nova auth RequestContext
4526         :param instance: Instance object that was resized/cold migrated
4527         """
4528         LOG.debug('Deleting port bindings for source host.',
4529                   instance=instance)
4530         try:
4531             self.network_api.cleanup_instance_network_on_host(
4532                 ctxt, instance, self.host)
4533         except exception.PortBindingDeletionFailed as e:
4534             # Do not let this stop us from cleaning up since the guest
4535             # is already gone.
4536             LOG.error('Failed to delete port bindings from source host. '
4537                       'Error: %s', str(e), instance=instance)
4538 
4539     def _delete_volume_attachments(self, ctxt, bdms):
4540         """Deletes volume attachment records for the given bdms.
4541 
4542         This method will log but not re-raise any exceptions if the volume
4543         attachment delete fails.
4544 
4545         :param ctxt: nova auth request context used to make
4546             DELETE /attachments/{attachment_id} requests to cinder.
4547         :param bdms: objects.BlockDeviceMappingList representing volume
4548             attachments to delete based on BlockDeviceMapping.attachment_id.
4549         """
4550         for bdm in bdms:
4551             if bdm.attachment_id:
4552                 try:
4553                     self.volume_api.attachment_delete(ctxt, bdm.attachment_id)
4554                 except Exception as e:
4555                     LOG.error('Failed to delete volume attachment with ID %s. '
4556                               'Error: %s', bdm.attachment_id, str(e),
4557                               instance_uuid=bdm.instance_uuid)
4558 
4559     @wrap_exception()
4560     @reverts_task_state
4561     @wrap_instance_event(prefix='compute')
4562     @errors_out_migration
4563     @wrap_instance_fault
4564     def revert_snapshot_based_resize_at_dest(self, ctxt, instance, migration):
4565         """Reverts a snapshot-based resize at the destination host.
4566 
4567         Cleans the guest from the destination compute service host hypervisor
4568         and related resources (ports, volumes) and frees resource usage from
4569         the compute service on that host.
4570 
4571         :param ctxt: nova auth request context targeted at the target cell
4572         :param instance: Instance object whose vm_state is "resized" and
4573             task_state is "resize_reverting".
4574         :param migration: Migration object whose status is "reverting".
4575         """
4576         # A resize revert is essentially a resize back to the old size, so we
4577         # need to send a usage event here.
4578         compute_utils.notify_usage_exists(
4579             self.notifier, ctxt, instance, self.host, current_period=True)
4580 
4581         @utils.synchronized(instance.uuid)
4582         def do_revert():
4583             LOG.info('Reverting resize on destination host.',
4584                      instance=instance)
4585             with self._error_out_instance_on_exception(ctxt, instance):
4586                 self._revert_snapshot_based_resize_at_dest(
4587                     ctxt, instance, migration)
4588         do_revert()
4589 
4590         # Broadcast to all schedulers that the instance is no longer on
4591         # this host and clear any waiting callback events. This is best effort
4592         # so if anything fails just log it.
4593         try:
4594             self._delete_scheduler_instance_info(ctxt, instance.uuid)
4595             self.instance_events.clear_events_for_instance(instance)
4596         except Exception as e:
4597             LOG.warning('revert_snapshot_based_resize_at_dest failed during '
4598                         'post-processing. Error: %s', e, instance=instance)
4599 
4600     def _revert_snapshot_based_resize_at_dest(
4601             self, ctxt, instance, migration):
4602         """Private version of revert_snapshot_based_resize_at_dest.
4603 
4604         This allows the main method to be decorated with error handlers.
4605 
4606         :param ctxt: nova auth request context targeted at the target cell
4607         :param instance: Instance object whose vm_state is "resized" and
4608             task_state is "resize_reverting".
4609         :param migration: Migration object whose status is "reverting".
4610         """
4611         # Cleanup the guest from the hypervisor including local disks.
4612         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4613         bdms = instance.get_bdms()
4614         block_device_info = self._get_instance_block_device_info(
4615             ctxt, instance, bdms=bdms)
4616         LOG.debug('Destroying guest from destination hypervisor including '
4617                   'disks.', instance=instance)
4618         self.driver.destroy(
4619             ctxt, instance, network_info, block_device_info=block_device_info)
4620 
4621         # Activate source host port bindings. We need to do this before
4622         # deleting the (active) dest host port bindings in
4623         # setup_networks_on_host otherwise the ports will be unbound and
4624         # finish on the source will fail.
4625         # migrate_instance_start uses migration.dest_compute for the port
4626         # binding host and since we want to activate the source host port
4627         # bindings, we need to temporarily mutate the migration object.
4628         with utils.temporary_mutation(
4629                 migration, dest_compute=migration.source_compute):
4630             LOG.debug('Activating port bindings for source host %s.',
4631                       migration.source_compute, instance=instance)
4632             # TODO(mriedem): https://review.opendev.org/#/c/594139/ would allow
4633             # us to remove this and make setup_networks_on_host do it.
4634             # TODO(mriedem): Should we try/except/log any errors but continue?
4635             self.network_api.migrate_instance_start(
4636                 ctxt, instance, migration)
4637 
4638         # Delete port bindings for the target host.
4639         LOG.debug('Deleting port bindings for target host %s.',
4640                   self.host, instance=instance)
4641         try:
4642             # Note that deleting the destination host port bindings does
4643             # not automatically activate the source host port bindings.
4644             self.network_api.cleanup_instance_network_on_host(
4645                 ctxt, instance, self.host)
4646         except exception.PortBindingDeletionFailed as e:
4647             # Do not let this stop us from cleaning up since the guest
4648             # is already gone.
4649             LOG.error('Failed to delete port bindings from target host. '
4650                       'Error: %s', str(e), instance=instance)
4651 
4652         # Delete any volume attachments remaining for this target host.
4653         LOG.debug('Deleting volume attachments for target host.',
4654                   instance=instance)
4655         self._delete_volume_attachments(ctxt, bdms)
4656 
4657         # Free up the new_flavor usage from the resource tracker for this host.
4658         self.rt.drop_move_claim_at_dest(ctxt, instance, migration)
4659 
4660     def _revert_instance_flavor_host_node(self, instance, migration):
4661         """Revert host, node and flavor fields after a resize-revert."""
4662         self._set_instance_info(instance, instance.old_flavor)
4663         instance.host = migration.source_compute
4664         instance.node = migration.source_node
4665         instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4666 
4667     @wrap_exception()
4668     @reverts_task_state
4669     @wrap_instance_event(prefix='compute')
4670     @errors_out_migration
4671     @wrap_instance_fault
4672     def finish_revert_snapshot_based_resize_at_source(
4673             self, ctxt, instance, migration):
4674         """Reverts a snapshot-based resize at the source host.
4675 
4676         Spawn the guest and re-connect volumes/VIFs on the source host and
4677         revert the instance to use the old_flavor for resource usage reporting.
4678 
4679         Updates allocations in the placement service to move the source node
4680         allocations, held by the migration record, to the instance and drop
4681         the allocations held by the instance on the destination node.
4682 
4683         :param ctxt: nova auth request context targeted at the target cell
4684         :param instance: Instance object whose vm_state is "resized" and
4685             task_state is "resize_reverting".
4686         :param migration: Migration object whose status is "reverting".
4687         """
4688 
4689         @utils.synchronized(instance.uuid)
4690         def do_revert():
4691             LOG.info('Reverting resize on source host.', instance=instance)
4692             with self._error_out_instance_on_exception(ctxt, instance):
4693                 self._finish_revert_snapshot_based_resize_at_source(
4694                     ctxt, instance, migration)
4695 
4696         try:
4697             do_revert()
4698         finally:
4699             self._delete_stashed_flavor_info(instance)
4700 
4701         # Broadcast to all schedulers that the instance is on this host.
4702         # This is best effort so if anything fails just log it.
4703         try:
4704             self._update_scheduler_instance_info(ctxt, instance)
4705         except Exception as e:
4706             LOG.warning('finish_revert_snapshot_based_resize_at_source failed '
4707                         'during post-processing. Error: %s', e,
4708                         instance=instance)
4709 
4710     def _finish_revert_snapshot_based_resize_at_source(
4711             self, ctxt, instance, migration):
4712         """Private version of finish_revert_snapshot_based_resize_at_source.
4713 
4714         This allows the main method to be decorated with error handlers.
4715 
4716         :param ctxt: nova auth request context targeted at the source cell
4717         :param instance: Instance object whose vm_state is "resized" and
4718             task_state is "resize_reverting".
4719         :param migration: Migration object whose status is "reverting".
4720         """
4721         # Get stashed old_vm_state information to determine if guest should
4722         # be powered on after spawn; we default to ACTIVE for backwards
4723         # compatibility if old_vm_state is not set
4724         old_vm_state = instance.system_metadata.get(
4725             'old_vm_state', vm_states.ACTIVE)
4726 
4727         # Revert the flavor and host/node fields to their previous values
4728         self._revert_instance_flavor_host_node(instance, migration)
4729 
4730         # Move the allocations against the source compute node resource
4731         # provider, held by the migration, to the instance which will drop
4732         # the destination compute node resource provider allocations held by
4733         # the instance. This puts the allocations against the source node
4734         # back to the old_flavor and owned by the instance.
4735         try:
4736             self._revert_allocation(ctxt, instance, migration)
4737         except exception.AllocationMoveFailed:
4738             # Log the error but do not re-raise because we want to continue to
4739             # process ports and volumes below.
4740             LOG.error('Reverting allocation in placement for migration '
4741                       '%(migration_uuid)s failed. You may need to manually '
4742                       'remove the allocations for the migration consumer '
4743                       'against the source node resource provider '
4744                       '%(source_provider)s and the allocations for the '
4745                       'instance consumer against the destination node '
4746                       'resource provider %(dest_provider)s and then run the '
4747                       '"nova-manage placement heal_allocations" command.',
4748                       {'instance_uuid': instance.uuid,
4749                        'migration_uuid': migration.uuid,
4750                        'source_provider': migration.source_node,
4751                        'dest_provider': migration.dest_node},
4752                       instance=instance)
4753 
4754         bdms = instance.get_bdms()
4755         # prep_snapshot_based_resize_at_source created empty volume attachments
4756         # that we need to update here to get the connection_info before calling
4757         # driver.finish_revert_migration which will connect the volumes to this
4758         # host.
4759         LOG.debug('Updating volume attachments for target host %s.',
4760                   self.host, instance=instance)
4761         # TODO(mriedem): We should probably make _update_volume_attachments
4762         # (optionally) graceful to errors so we (1) try to process all
4763         # attachments and (2) continue to process networking below.
4764         self._update_volume_attachments(ctxt, instance, bdms)
4765 
4766         LOG.debug('Updating port bindings for source host %s.',
4767                   self.host, instance=instance)
4768         # TODO(mriedem): Calculate provider mappings when we support
4769         # cross-cell resize/migrate with ports having resource requests.
4770         self._finish_revert_resize_network_migrate_finish(
4771             ctxt, instance, migration, provider_mappings=None)
4772         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4773 
4774         # Remember that prep_snapshot_based_resize_at_source destroyed the
4775         # guest but left the disks intact so we cannot call spawn() here but
4776         # finish_revert_migration should do the job.
4777         block_device_info = self._get_instance_block_device_info(
4778             ctxt, instance, bdms=bdms)
4779         power_on = old_vm_state == vm_states.ACTIVE
4780         driver_error = None
4781         try:
4782             self.driver.finish_revert_migration(
4783                 ctxt, instance, network_info, migration,
4784                 block_device_info=block_device_info, power_on=power_on)
4785         except Exception as e:
4786             driver_error = e
4787             # Leave a hint about hard rebooting the guest and reraise so the
4788             # instance is put into ERROR state.
4789             with excutils.save_and_reraise_exception(logger=LOG):
4790                 LOG.error('An error occurred during finish_revert_migration. '
4791                           'The instance may need to be hard rebooted. Error: '
4792                           '%s', driver_error, instance=instance)
4793         else:
4794             # Perform final cleanup of the instance in the database.
4795             instance.drop_migration_context()
4796             # If the original vm_state was STOPPED, set it back to STOPPED.
4797             vm_state = vm_states.ACTIVE if power_on else vm_states.STOPPED
4798             self._update_instance_after_spawn(instance, vm_state=vm_state)
4799             instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4800         finally:
4801             # Complete any volume attachments so the volumes are in-use. We
4802             # do this regardless of finish_revert_migration failing because
4803             # the instance is back on this host now and we do not want to leave
4804             # the volumes in a pending state in case the instance is hard
4805             # rebooted.
4806             LOG.debug('Completing volume attachments for instance on source '
4807                       'host.', instance=instance)
4808             with excutils.save_and_reraise_exception(
4809                     reraise=driver_error is not None, logger=LOG):
4810                 self._complete_volume_attachments(ctxt, bdms)
4811 
4812         migration.status = 'reverted'
4813         migration.save()
4814 
4815     @wrap_exception()
4816     @reverts_task_state
4817     @wrap_instance_event(prefix='compute')
4818     @errors_out_migration
4819     @wrap_instance_fault
4820     def revert_resize(self, context, instance, migration, request_spec=None):
4821         """Destroys the new instance on the destination machine.
4822 
4823         Reverts the model changes, and powers on the old instance on the
4824         source machine.
4825 
4826         """
4827         # NOTE(comstud): A revert_resize is essentially a resize back to
4828         # the old size, so we need to send a usage event here.
4829         compute_utils.notify_usage_exists(self.notifier, context, instance,
4830                                           self.host, current_period=True)
4831 
4832         with self._error_out_instance_on_exception(context, instance):
4833             # NOTE(tr3buchet): tear down networks on destination host
4834             self.network_api.setup_networks_on_host(context, instance,
4835                                                     teardown=True)
4836 
4837             self.network_api.migrate_instance_start(context,
4838                                                     instance,
4839                                                     migration)
4840 
4841             network_info = self.network_api.get_instance_nw_info(context,
4842                                                                  instance)
4843             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4844                     context, instance.uuid)
4845             block_device_info = self._get_instance_block_device_info(
4846                                 context, instance, bdms=bdms)
4847 
4848             destroy_disks = not self._is_instance_storage_shared(
4849                 context, instance, host=migration.source_compute)
4850             self.driver.destroy(context, instance, network_info,
4851                                 block_device_info, destroy_disks)
4852 
4853             self._terminate_volume_connections(context, instance, bdms)
4854 
4855             # Free up the new_flavor usage from the resource tracker for this
4856             # host.
4857             self.rt.drop_move_claim_at_dest(context, instance, migration)
4858 
4859             # RPC cast back to the source host to finish the revert there.
4860             self.compute_rpcapi.finish_revert_resize(context, instance,
4861                     migration, migration.source_compute, request_spec)
4862 
4863     def _finish_revert_resize_network_migrate_finish(
4864             self, context, instance, migration, provider_mappings):
4865         """Causes port binding to be updated. In some Neutron or port
4866         configurations - see NetworkModel.get_bind_time_events() - we
4867         expect the vif-plugged event from Neutron immediately and wait for it.
4868         The rest of the time, the event is expected further along in the
4869         virt driver, so we don't wait here.
4870 
4871         :param context: The request context.
4872         :param instance: The instance undergoing the revert resize.
4873         :param migration: The Migration object of the resize being reverted.
4874         :param provider_mappings: a dict of list of resource provider uuids
4875             keyed by port uuid
4876         :raises: eventlet.timeout.Timeout or
4877                  exception.VirtualInterfacePlugException.
4878         """
4879         network_info = instance.get_network_info()
4880         events = []
4881         deadline = CONF.vif_plugging_timeout
4882         if deadline and network_info:
4883             events = network_info.get_bind_time_events(migration)
4884             if events:
4885                 LOG.debug('Will wait for bind-time events: %s', events)
4886         error_cb = self._neutron_failed_migration_callback
4887         try:
4888             with self.virtapi.wait_for_instance_event(instance, events,
4889                                                       deadline=deadline,
4890                                                       error_callback=error_cb):
4891                 # NOTE(hanrong): we need to change migration.dest_compute to
4892                 # source host temporarily.
4893                 # "network_api.migrate_instance_finish" will setup the network
4894                 # for the instance on the destination host. For revert resize,
4895                 # the instance will back to the source host, the setup of the
4896                 # network for instance should be on the source host. So set
4897                 # the migration.dest_compute to source host at here.
4898                 with utils.temporary_mutation(
4899                         migration, dest_compute=migration.source_compute):
4900                     self.network_api.migrate_instance_finish(
4901                         context, instance, migration, provider_mappings)
4902         except eventlet.timeout.Timeout:
4903             with excutils.save_and_reraise_exception():
4904                 LOG.error('Timeout waiting for Neutron events: %s', events,
4905                           instance=instance)
4906 
4907     @wrap_exception()
4908     @reverts_task_state
4909     @wrap_instance_event(prefix='compute')
4910     @errors_out_migration
4911     @wrap_instance_fault
4912     def finish_revert_resize(
4913             self, context, instance, migration, request_spec=None):
4914         """Finishes the second half of reverting a resize on the source host.
4915 
4916         Bring the original source instance state back (active/shutoff) and
4917         revert the resized attributes in the database.
4918 
4919         """
4920         try:
4921             self._finish_revert_resize(
4922                 context, instance, migration, request_spec)
4923         finally:
4924             self._delete_stashed_flavor_info(instance)
4925 
4926     def _finish_revert_resize(
4927         self, context, instance, migration, request_spec=None,
4928     ):
4929         """Inner version of finish_revert_resize."""
4930         with self._error_out_instance_on_exception(context, instance):
4931             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4932                 context, instance.uuid)
4933             self._notify_about_instance_usage(
4934                     context, instance, "resize.revert.start")
4935             compute_utils.notify_about_instance_action(context, instance,
4936                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4937                     phase=fields.NotificationPhase.START, bdms=bdms)
4938 
4939             # Get stashed old_vm_state information to determine if guest should
4940             # be powered on after spawn; we default to ACTIVE for backwards
4941             # compatibility if old_vm_state is not set
4942             old_vm_state = instance.system_metadata.get(
4943                 'old_vm_state', vm_states.ACTIVE)
4944 
4945             # Revert the flavor and host/node fields to their previous values
4946             self._revert_instance_flavor_host_node(instance, migration)
4947 
4948             try:
4949                 source_allocations = self._revert_allocation(
4950                     context, instance, migration)
4951             except exception.AllocationMoveFailed:
4952                 LOG.error('Reverting allocation in placement for migration '
4953                           '%(migration_uuid)s failed. The instance '
4954                           '%(instance_uuid)s will be put into ERROR state but '
4955                           'the allocation held by the migration is leaked.',
4956                           {'instance_uuid': instance.uuid,
4957                            'migration_uuid': migration.uuid})
4958                 raise
4959 
4960             provider_mappings = self._fill_provider_mapping_based_on_allocs(
4961                 context, source_allocations, request_spec)
4962 
4963             self.network_api.setup_networks_on_host(context, instance,
4964                                                     migration.source_compute)
4965             self._finish_revert_resize_network_migrate_finish(
4966                 context, instance, migration, provider_mappings)
4967             network_info = self.network_api.get_instance_nw_info(context,
4968                                                                  instance)
4969 
4970             # revert_resize deleted any volume attachments for the instance
4971             # and created new ones to be used on this host, but we
4972             # have to update those attachments with the host connector so the
4973             # BDM.connection_info will get set in the call to
4974             # _get_instance_block_device_info below with refresh_conn_info=True
4975             # and then the volumes can be re-connected via the driver on this
4976             # host.
4977             self._update_volume_attachments(context, instance, bdms)
4978 
4979             block_device_info = self._get_instance_block_device_info(
4980                     context, instance, refresh_conn_info=True, bdms=bdms)
4981 
4982             power_on = old_vm_state != vm_states.STOPPED
4983             self.driver.finish_revert_migration(
4984                 context, instance, network_info, migration, block_device_info,
4985                 power_on)
4986 
4987             instance.drop_migration_context()
4988             instance.launched_at = timeutils.utcnow()
4989             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4990 
4991             # Complete any volume attachments so the volumes are in-use.
4992             self._complete_volume_attachments(context, bdms)
4993 
4994             # if the original vm state was STOPPED, set it back to STOPPED
4995             LOG.info("Updating instance to original state: '%s'",
4996                      old_vm_state, instance=instance)
4997             if power_on:
4998                 instance.vm_state = vm_states.ACTIVE
4999                 instance.task_state = None
5000                 instance.save()
5001             else:
5002                 instance.task_state = task_states.POWERING_OFF
5003                 instance.save()
5004                 self.stop_instance(context, instance=instance,
5005                                    clean_shutdown=True)
5006 
5007             self._notify_about_instance_usage(
5008                     context, instance, "resize.revert.end")
5009             compute_utils.notify_about_instance_action(context, instance,
5010                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
5011                     phase=fields.NotificationPhase.END, bdms=bdms)
5012 
5013     def _fill_provider_mapping_based_on_allocs(
5014             self, context, allocations, request_spec):
5015         """Fills and returns the request group - resource provider mapping
5016         based on the allocation passed in.
5017 
5018         :param context: The security context
5019         :param allocation: allocation dict keyed by RP UUID.
5020         :param request_spec: The RequestSpec object associated with the
5021             operation
5022         :returns: None if the request_spec is None. Otherwise a mapping
5023             between RequestGroup requester_id, currently Neutron port_id,
5024             and a list of resource provider UUIDs providing resource for
5025             that RequestGroup.
5026         """
5027         if request_spec:
5028             # NOTE(gibi): We need to re-calculate the resource provider -
5029             # port mapping as we have to have the neutron ports allocate
5030             # from the source compute after revert.
5031             scheduler_utils.fill_provider_mapping_based_on_allocation(
5032                 context, self.reportclient, request_spec, allocations)
5033             provider_mappings = self._get_request_group_mapping(
5034                 request_spec)
5035         else:
5036             # NOTE(gibi): The compute RPC is pinned to be older than 5.2
5037             # and therefore request_spec is not sent. We cannot calculate
5038             # the provider mappings. If the instance has ports with
5039             # resource request then the port update will fail in
5040             # _update_port_binding_for_instance() called via
5041             # _finish_revert_resize_network_migrate_finish() in
5042             # finish_revert_resize.
5043             provider_mappings = None
5044         return provider_mappings
5045 
5046     def _revert_allocation(self, context, instance, migration):
5047         """Revert an allocation that is held by migration to our instance."""
5048 
5049         # Fetch the original allocation that the instance had on the source
5050         # node, which are now held by the migration
5051         orig_alloc = self.reportclient.get_allocations_for_consumer(
5052             context, migration.uuid)
5053         if not orig_alloc:
5054             LOG.error('Did not find resource allocations for migration '
5055                       '%s on source node %s. Unable to revert source node '
5056                       'allocations back to the instance.',
5057                       migration.uuid, migration.source_node, instance=instance)
5058             return False
5059 
5060         LOG.info('Swapping old allocation on %(rp_uuids)s held by migration '
5061                  '%(mig)s for instance',
5062                  {'rp_uuids': orig_alloc.keys(), 'mig': migration.uuid},
5063                  instance=instance)
5064         # FIXME(gibi): This method is flawed in that it does not handle
5065         # allocations against sharing providers in any special way. This leads
5066         # to duplicate allocations against the sharing provider during
5067         # migration.
5068         # TODO(cdent): Should we be doing anything with return values here?
5069         self.reportclient.move_allocations(context, migration.uuid,
5070                                            instance.uuid)
5071         return orig_alloc
5072 
5073     def _prep_resize(self, context, image, instance, instance_type,
5074                      filter_properties, node, migration, request_spec,
5075                      clean_shutdown=True):
5076 
5077         if not filter_properties:
5078             filter_properties = {}
5079 
5080         if not instance.host:
5081             self._set_instance_obj_error_state(instance)
5082             msg = _('Instance has no source host')
5083             raise exception.MigrationError(reason=msg)
5084 
5085         same_host = instance.host == self.host
5086         # if the flavor IDs match, it's migrate; otherwise resize
5087         if same_host and instance_type.id == instance['instance_type_id']:
5088             # check driver whether support migrate to same host
5089             if not self.driver.capabilities.get(
5090                     'supports_migrate_to_same_host', False):
5091                 # Raise InstanceFaultRollback so that the
5092                 # _error_out_instance_on_exception context manager in
5093                 # prep_resize will set the instance.vm_state properly.
5094                 raise exception.InstanceFaultRollback(
5095                     inner_exception=exception.UnableToMigrateToSelf(
5096                         instance_id=instance.uuid, host=self.host))
5097 
5098         # NOTE(danms): Stash the new instance_type to avoid having to
5099         # look it up in the database later
5100         instance.new_flavor = instance_type
5101         # NOTE(mriedem): Stash the old vm_state so we can set the
5102         # resized/reverted instance back to the same state later.
5103         vm_state = instance.vm_state
5104         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
5105         instance.system_metadata['old_vm_state'] = vm_state
5106         instance.save()
5107 
5108         if not isinstance(request_spec, objects.RequestSpec):
5109             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
5110             # version of the request spec to compute and since Stein compute
5111             # could be sending that back to conductor on reschedule, so if we
5112             # got a dict convert it to an object.
5113             # TODO(mriedem): We can drop this compat code when we only support
5114             # compute RPC API >=6.0.
5115             request_spec = objects.RequestSpec.from_primitives(
5116                 context, request_spec, filter_properties)
5117             # We don't have to set the new flavor on the request spec because
5118             # if we got here it was due to a reschedule from the compute and
5119             # the request spec would already have the new flavor in it from the
5120             # else block below.
5121 
5122         provider_mapping = self._get_request_group_mapping(request_spec)
5123 
5124         if provider_mapping:
5125             try:
5126                 compute_utils.\
5127                     update_pci_request_spec_with_allocated_interface_name(
5128                         context, self.reportclient,
5129                         instance.pci_requests.requests, provider_mapping)
5130             except (exception.AmbiguousResourceProviderForPCIRequest,
5131                     exception.UnexpectedResourceProviderNameForPCIRequest
5132                     ) as e:
5133                 raise exception.BuildAbortException(
5134                     reason=str(e), instance_uuid=instance.uuid)
5135 
5136         limits = filter_properties.get('limits', {})
5137         allocs = self.reportclient.get_allocations_for_consumer(
5138             context, instance.uuid)
5139         with self.rt.resize_claim(context, instance, instance_type, node,
5140                                   migration, allocs, image_meta=image,
5141                                   limits=limits) as claim:
5142             LOG.info('Migrating', instance=instance)
5143             # RPC cast to the source host to start the actual resize/migration.
5144             self.compute_rpcapi.resize_instance(
5145                     context, instance, claim.migration, image,
5146                     instance_type, request_spec, clean_shutdown)
5147 
5148     def _send_prep_resize_notifications(
5149             self, context, instance, phase, flavor):
5150         """Send "resize.prep.*" notifications.
5151 
5152         :param context: nova auth request context
5153         :param instance: The instance being resized
5154         :param phase: The phase of the action (NotificationPhase enum)
5155         :param flavor: The (new) flavor for the resize (same as existing
5156             instance.flavor for a cold migration)
5157         """
5158         # Only send notify_usage_exists if it's the "start" phase.
5159         if phase == fields.NotificationPhase.START:
5160             compute_utils.notify_usage_exists(
5161                 self.notifier, context, instance, self.host,
5162                 current_period=True)
5163 
5164         # Send extra usage info about the flavor if it's the "end" phase for
5165         # the legacy unversioned notification.
5166         extra_usage_info = None
5167         if phase == fields.NotificationPhase.END:
5168             extra_usage_info = dict(
5169                 new_instance_type=flavor.name,
5170                 new_instance_type_id=flavor.id)
5171         self._notify_about_instance_usage(
5172             context, instance, "resize.prep.%s" % phase,
5173             extra_usage_info=extra_usage_info)
5174 
5175         # Send the versioned notification.
5176         compute_utils.notify_about_resize_prep_instance(
5177             context, instance, self.host, phase, flavor)
5178 
5179     @wrap_exception()
5180     @reverts_task_state
5181     @wrap_instance_event(prefix='compute')
5182     @wrap_instance_fault
5183     def prep_resize(self, context, image, instance, instance_type,
5184                     request_spec, filter_properties, node,
5185                     clean_shutdown, migration, host_list):
5186         """Initiates the process of moving a running instance to another host.
5187 
5188         Possibly changes the VCPU, RAM and disk size in the process.
5189 
5190         This is initiated from conductor and runs on the destination host.
5191 
5192         The main purpose of this method is performing some checks on the
5193         destination host and making a claim for resources. If the claim fails
5194         then a reschedule to another host may be attempted which involves
5195         calling back to conductor to start the process over again.
5196         """
5197         if node is None:
5198             node = self._get_nodename(instance, refresh=True)
5199 
5200         # Pass instance_state=instance.vm_state because we can resize
5201         # a STOPPED server and we don't want to set it back to ACTIVE
5202         # in case _prep_resize fails.
5203         instance_state = instance.vm_state
5204         with self._error_out_instance_on_exception(
5205                 context, instance, instance_state=instance_state),\
5206                 errors_out_migration_ctxt(migration):
5207             self._send_prep_resize_notifications(
5208                 context, instance, fields.NotificationPhase.START,
5209                 instance_type)
5210             try:
5211                 self._prep_resize(context, image, instance,
5212                                   instance_type, filter_properties,
5213                                   node, migration, request_spec,
5214                                   clean_shutdown)
5215             except exception.BuildAbortException:
5216                 # NOTE(gibi): We failed
5217                 # update_pci_request_spec_with_allocated_interface_name so
5218                 # there is no reason to re-schedule. Just revert the allocation
5219                 # and fail the migration.
5220                 with excutils.save_and_reraise_exception():
5221                     self._revert_allocation(context, instance, migration)
5222             except Exception:
5223                 # Since we hit a failure, we're either rescheduling or dead
5224                 # and either way we need to cleanup any allocations created
5225                 # by the scheduler for the destination node.
5226                 self._revert_allocation(context, instance, migration)
5227                 # try to re-schedule the resize elsewhere:
5228                 exc_info = sys.exc_info()
5229                 self._reschedule_resize_or_reraise(context, instance,
5230                         exc_info, instance_type, request_spec,
5231                         filter_properties, host_list)
5232             finally:
5233                 self._send_prep_resize_notifications(
5234                     context, instance, fields.NotificationPhase.END,
5235                     instance_type)
5236 
5237     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
5238             instance_type, request_spec, filter_properties, host_list):
5239         """Try to re-schedule the resize or re-raise the original error to
5240         error out the instance.
5241         """
5242         if not filter_properties:
5243             filter_properties = {}
5244 
5245         rescheduled = False
5246         instance_uuid = instance.uuid
5247 
5248         try:
5249             retry = filter_properties.get('retry')
5250             if retry:
5251                 LOG.debug('Rescheduling, attempt %d', retry['num_attempts'],
5252                           instance_uuid=instance_uuid)
5253 
5254                 # reset the task state
5255                 task_state = task_states.RESIZE_PREP
5256                 self._instance_update(context, instance, task_state=task_state)
5257 
5258                 if exc_info:
5259                     # stringify to avoid circular ref problem in json
5260                     # serialization
5261                     retry['exc'] = traceback.format_exception_only(
5262                         exc_info[0], exc_info[1])
5263 
5264                 scheduler_hint = {'filter_properties': filter_properties}
5265 
5266                 self.compute_task_api.resize_instance(
5267                     context, instance, scheduler_hint, instance_type,
5268                     request_spec=request_spec, host_list=host_list)
5269 
5270                 rescheduled = True
5271             else:
5272                 # no retry information, do not reschedule.
5273                 LOG.debug('Retry info not present, will not reschedule',
5274                           instance_uuid=instance_uuid)
5275                 rescheduled = False
5276         except Exception as error:
5277             rescheduled = False
5278             LOG.exception("Error trying to reschedule",
5279                           instance_uuid=instance_uuid)
5280             compute_utils.add_instance_fault_from_exc(context,
5281                     instance, error,
5282                     exc_info=sys.exc_info())
5283             self._notify_about_instance_usage(context, instance,
5284                     'resize.error', fault=error)
5285             compute_utils.notify_about_instance_action(
5286                 context, instance, self.host,
5287                 action=fields.NotificationAction.RESIZE,
5288                 phase=fields.NotificationPhase.ERROR,
5289                 exception=error,
5290             )
5291 
5292         if rescheduled:
5293             self._log_original_error(exc_info, instance_uuid)
5294             compute_utils.add_instance_fault_from_exc(context,
5295                     instance, exc_info[1], exc_info=exc_info)
5296             self._notify_about_instance_usage(context, instance,
5297                     'resize.error', fault=exc_info[1])
5298             compute_utils.notify_about_instance_action(
5299                 context, instance, self.host,
5300                 action=fields.NotificationAction.RESIZE,
5301                 phase=fields.NotificationPhase.ERROR,
5302                 exception=exc_info[1],
5303             )
5304         else:
5305             # not re-scheduling
5306             exc = exc_info[1] or exc_info[0]()
5307             if exc.__traceback__ is not exc_info[2]:
5308                 raise exc.with_traceback(exc_info[2])
5309             raise exc
5310 
5311     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5312     @messaging.expected_exceptions(exception.MigrationPreCheckError)
5313     @wrap_exception()
5314     @wrap_instance_event(prefix='compute')
5315     @wrap_instance_fault
5316     def prep_snapshot_based_resize_at_dest(
5317             self, ctxt, instance, flavor, nodename, migration, limits,
5318             request_spec):
5319         """Performs pre-cross-cell resize resource claim on the dest host.
5320 
5321         This runs on the destination host in a cross-cell resize operation
5322         before the resize is actually started.
5323 
5324         Performs a resize_claim for resources that are not claimed in placement
5325         like PCI devices and NUMA topology.
5326 
5327         Note that this is different from same-cell prep_resize in that this:
5328 
5329         * Does not RPC cast to the source compute, that is orchestrated from
5330           conductor.
5331         * This does not reschedule on failure, conductor handles that since
5332           conductor is synchronously RPC calling this method. As such, the
5333           reverts_task_state decorator is not used on this method.
5334 
5335         :param ctxt: user auth request context
5336         :param instance: the instance being resized
5337         :param flavor: the flavor being resized to (unchanged for cold migrate)
5338         :param nodename: Name of the target compute node
5339         :param migration: nova.objects.Migration object for the operation
5340         :param limits: nova.objects.SchedulerLimits object of resource limits
5341         :param request_spec: nova.objects.RequestSpec object for the operation
5342         :returns: nova.objects.MigrationContext; the migration context created
5343             on the destination host during the resize_claim.
5344         :raises: nova.exception.MigrationPreCheckError if the pre-check
5345             validation fails for the given host selection
5346         """
5347         LOG.debug('Checking if we can cross-cell migrate instance to this '
5348                   'host (%s).', self.host, instance=instance)
5349         self._send_prep_resize_notifications(
5350             ctxt, instance, fields.NotificationPhase.START, flavor)
5351         # TODO(mriedem): update_pci_request_spec_with_allocated_interface_name
5352         # should be called here if the request spec has request group mappings,
5353         # e.g. for things like QoS ports with resource requests. Do it outside
5354         # the try/except so if it raises BuildAbortException we do not attempt
5355         # to reschedule.
5356         try:
5357             # Get the allocations within the try/except block in case we get
5358             # an error so MigrationPreCheckError is raised up.
5359             allocations = self.reportclient.get_allocs_for_consumer(
5360                 ctxt, instance.uuid)['allocations']
5361             # Claim resources on this target host using the new flavor which
5362             # will create the MigrationContext object. Note that in the future
5363             # if we want to do other validation here we should do it within
5364             # the MoveClaim context so we can drop the claim if anything fails.
5365             self.rt.resize_claim(
5366                 ctxt, instance, flavor, nodename, migration, allocations,
5367                 image_meta=instance.image_meta, limits=limits)
5368         except Exception as ex:
5369             err = str(ex)
5370             LOG.warning(
5371                 'Cross-cell resize pre-checks failed for this host (%s). '
5372                 'Cleaning up. Failure: %s', self.host, err,
5373                 instance=instance, exc_info=True)
5374             raise exception.MigrationPreCheckError(
5375                 reason=(_("Pre-checks failed on host '%(host)s'. "
5376                           "Error: %(error)s") %
5377                         {'host': self.host, 'error': err}))
5378         finally:
5379             self._send_prep_resize_notifications(
5380                 ctxt, instance, fields.NotificationPhase.END, flavor)
5381 
5382         # ResourceTracker.resize_claim() sets instance.migration_context.
5383         return instance.migration_context
5384 
5385     @messaging.expected_exceptions(exception.InstancePowerOffFailure)
5386     @wrap_exception()
5387     @reverts_task_state
5388     @wrap_instance_event(prefix='compute')
5389     @errors_out_migration
5390     @wrap_instance_fault
5391     def prep_snapshot_based_resize_at_source(
5392             self, ctxt, instance, migration, snapshot_id=None):
5393         """Prepares the instance at the source host for cross-cell resize
5394 
5395         Performs actions like powering off the guest, upload snapshot data if
5396         the instance is not volume-backed, disconnecting volumes, unplugging
5397         VIFs and activating the destination host port bindings.
5398 
5399         :param ctxt: user auth request context targeted at source cell
5400         :param instance: nova.objects.Instance; the instance being resized.
5401             The expected instance.task_state is "resize_migrating" when calling
5402             this method, and the expected task_state upon successful completion
5403             is "resize_migrated".
5404         :param migration: nova.objects.Migration object for the operation.
5405             The expected migration.status is "pre-migrating" when calling this
5406             method and the expected status upon successful completion is
5407             "post-migrating".
5408         :param snapshot_id: ID of the image snapshot to upload if not a
5409             volume-backed instance
5410         :raises: nova.exception.InstancePowerOffFailure if stopping the
5411             instance fails
5412         """
5413         LOG.info('Preparing for snapshot based resize on source host %s.',
5414                  self.host, instance=instance)
5415         # Note that if anything fails here, the migration-based allocations
5416         # created in conductor should be reverted by conductor as well,
5417         # see MigrationTask.rollback.
5418         self._prep_snapshot_based_resize_at_source(
5419             ctxt, instance, migration, snapshot_id=snapshot_id)
5420 
5421     @delete_image_on_error
5422     def _snapshot_for_resize(self, ctxt, image_id, instance):
5423         """Uploads snapshot for the instance during a snapshot-based resize
5424 
5425         If the snapshot operation fails the image will be deleted.
5426 
5427         :param ctxt: the nova auth request context for the resize operation
5428         :param image_id: the snapshot image ID
5429         :param instance: the instance to snapshot/resize
5430         """
5431         LOG.debug('Uploading snapshot data for image %s', image_id,
5432                   instance=instance)
5433         # Note that we do not track the snapshot phase task states
5434         # during resize since we do not want to reflect those into the
5435         # actual instance.task_state.
5436         update_task_state = lambda *args, **kwargs: None
5437         with timeutils.StopWatch() as timer:
5438             self.driver.snapshot(ctxt, instance, image_id, update_task_state)
5439             LOG.debug('Took %0.2f seconds to snapshot the instance on '
5440                       'the hypervisor.', timer.elapsed(), instance=instance)
5441 
5442     def _prep_snapshot_based_resize_at_source(
5443             self, ctxt, instance, migration, snapshot_id=None):
5444         """Private method for prep_snapshot_based_resize_at_source so calling
5445         code can handle errors and perform rollbacks as necessary.
5446         """
5447         # Fetch and update the instance.info_cache.
5448         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5449         # Get the BDMs attached to this instance on this source host.
5450         bdms = instance.get_bdms()
5451         # Send the resize.start notification.
5452         self._send_resize_instance_notifications(
5453             ctxt, instance, bdms, network_info, fields.NotificationPhase.START)
5454         # Update the migration status from "pre-migrating" to "migrating".
5455         migration.status = 'migrating'
5456         migration.save()
5457 
5458         # Since the instance is going to be left on the source host during the
5459         # resize, we need to power it off so we do not have the instance
5460         # potentially running in two places.
5461         LOG.debug('Stopping instance', instance=instance)
5462         try:
5463             self._power_off_instance(instance)
5464         except Exception as e:
5465             LOG.exception('Failed to power off instance.', instance=instance)
5466             raise exception.InstancePowerOffFailure(reason=str(e))
5467         instance.power_state = self._get_power_state(instance)
5468 
5469         # If a snapshot image ID was provided, we need to snapshot the guest
5470         # disk image and upload it to the image service.
5471         if snapshot_id:
5472             self._snapshot_for_resize(ctxt, snapshot_id, instance)
5473 
5474         block_device_info = self._get_instance_block_device_info(
5475             ctxt, instance, bdms=bdms)
5476 
5477         # If something fails at this point the instance must go to ERROR
5478         # status for operator intervention or to reboot/rebuild the instance.
5479         with self._error_out_instance_on_exception(
5480                 ctxt, instance, instance_state=vm_states.ERROR):
5481 
5482             # Destroy the guest on the source host which will disconnect
5483             # volumes and unplug VIFs. Note that we DO NOT destroy disks since
5484             # we want to leave those on the source host in case of a later
5485             # failure and disks are needed to recover the guest or in case the
5486             # resize is reverted.
5487             LOG.debug('Destroying guest on source host but retaining disks.',
5488                       instance=instance)
5489             self.driver.destroy(
5490                 ctxt, instance, network_info,
5491                 block_device_info=block_device_info, destroy_disks=False)
5492 
5493             # At this point the volumes are disconnected from this source host.
5494             # Delete the old volume attachment records and create new empty
5495             # ones which will be used later if the resize is reverted.
5496             LOG.debug('Deleting volume attachments for the source host.',
5497                       instance=instance)
5498             self._terminate_volume_connections(ctxt, instance, bdms)
5499 
5500             # At this point the VIFs are unplugged from this source host.
5501             # Activate the dest host port bindings created by conductor.
5502             self.network_api.migrate_instance_start(ctxt, instance, migration)
5503 
5504             # Update the migration status from "migrating" to "post-migrating".
5505             migration.status = 'post-migrating'
5506             migration.save()
5507 
5508             # At this point, the traditional resize_instance would update the
5509             # instance host/node values to point at the dest host/node because
5510             # that is where the disk is transferred during resize_instance, but
5511             # with cross-cell resize the instance is not yet at the dest host
5512             # so we do not make that update here.
5513             instance.task_state = task_states.RESIZE_MIGRATED
5514             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5515 
5516         self._send_resize_instance_notifications(
5517             ctxt, instance, bdms, network_info,
5518             fields.NotificationPhase.END)
5519         self.instance_events.clear_events_for_instance(instance)
5520 
5521     @wrap_exception()
5522     @reverts_task_state
5523     @wrap_instance_event(prefix='compute')
5524     @wrap_instance_fault
5525     def resize_instance(self, context, instance, image,
5526                         migration, instance_type, clean_shutdown,
5527                         request_spec=None):
5528         """Starts the migration of a running instance to another host.
5529 
5530         This is initiated from the destination host's ``prep_resize`` routine
5531         and runs on the source host.
5532         """
5533         try:
5534             self._resize_instance(context, instance, image, migration,
5535                                   instance_type, clean_shutdown, request_spec)
5536         except Exception:
5537             with excutils.save_and_reraise_exception():
5538                 self._revert_allocation(context, instance, migration)
5539 
5540     def _resize_instance(self, context, instance, image,
5541                          migration, instance_type, clean_shutdown,
5542                          request_spec):
5543         # Pass instance_state=instance.vm_state because we can resize
5544         # a STOPPED server and we don't want to set it back to ACTIVE
5545         # in case migrate_disk_and_power_off raises InstanceFaultRollback.
5546         instance_state = instance.vm_state
5547         with self._error_out_instance_on_exception(
5548                 context, instance, instance_state=instance_state), \
5549              errors_out_migration_ctxt(migration):
5550             network_info = self.network_api.get_instance_nw_info(context,
5551                                                                  instance)
5552 
5553             migration.status = 'migrating'
5554             migration.save()
5555 
5556             instance.task_state = task_states.RESIZE_MIGRATING
5557             instance.save(expected_task_state=task_states.RESIZE_PREP)
5558 
5559             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5560                     context, instance.uuid)
5561             self._send_resize_instance_notifications(
5562                 context, instance, bdms, network_info,
5563                 fields.NotificationPhase.START)
5564 
5565             block_device_info = self._get_instance_block_device_info(
5566                                 context, instance, bdms=bdms)
5567 
5568             timeout, retry_interval = self._get_power_off_values(
5569                 instance, clean_shutdown)
5570             disk_info = self.driver.migrate_disk_and_power_off(
5571                     context, instance, migration.dest_host,
5572                     instance_type, network_info,
5573                     block_device_info,
5574                     timeout, retry_interval)
5575 
5576             self._terminate_volume_connections(context, instance, bdms)
5577 
5578             self.network_api.migrate_instance_start(context,
5579                                                     instance,
5580                                                     migration)
5581 
5582             migration.status = 'post-migrating'
5583             migration.save()
5584 
5585             instance.host = migration.dest_compute
5586             instance.node = migration.dest_node
5587             instance.task_state = task_states.RESIZE_MIGRATED
5588             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5589 
5590             # RPC cast to the destination host to finish the resize/migration.
5591             self.compute_rpcapi.finish_resize(context, instance,
5592                 migration, image, disk_info, migration.dest_compute,
5593                 request_spec)
5594 
5595         self._send_resize_instance_notifications(
5596             context, instance, bdms, network_info,
5597             fields.NotificationPhase.END)
5598         self.instance_events.clear_events_for_instance(instance)
5599 
5600     def _send_resize_instance_notifications(
5601             self, context, instance, bdms, network_info, phase):
5602         """Send "resize.(start|end)" notifications.
5603 
5604         :param context: nova auth request context
5605         :param instance: The instance being resized
5606         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5607             instance
5608         :param network_info: NetworkInfo for the instance info cache of ports
5609         :param phase: The phase of the action (NotificationPhase enum, either
5610             ``start`` or ``end``)
5611         """
5612         action = fields.NotificationAction.RESIZE
5613         # Send the legacy unversioned notification.
5614         self._notify_about_instance_usage(
5615             context, instance, "%s.%s" % (action, phase),
5616             network_info=network_info)
5617         # Send the versioned notification.
5618         compute_utils.notify_about_instance_action(
5619             context, instance, self.host, action=action, phase=phase,
5620             bdms=bdms)
5621 
5622     def _terminate_volume_connections(self, context, instance, bdms):
5623         connector = None
5624         for bdm in bdms:
5625             if bdm.is_volume:
5626                 if bdm.attachment_id:
5627                     # NOTE(jdg): So here's the thing, the idea behind the new
5628                     # attach API's was to have a new code fork/path that we
5629                     # followed, we're not going to do that so we have to do
5630                     # some extra work in here to make it *behave* just like the
5631                     # old code. Cinder doesn't allow disconnect/reconnect (you
5632                     # just delete the attachment and get a new one)
5633                     # attachments in the new attach code so we have to do
5634                     # a delete and create without a connector (reserve),
5635                     # in other words, beware
5636                     attachment_id = self.volume_api.attachment_create(
5637                         context, bdm.volume_id, instance.uuid)['id']
5638                     self.volume_api.attachment_delete(context,
5639                                                       bdm.attachment_id)
5640                     bdm.attachment_id = attachment_id
5641                     bdm.save()
5642 
5643                 else:
5644                     if connector is None:
5645                         connector = self.driver.get_volume_connector(instance)
5646                     self.volume_api.terminate_connection(context,
5647                                                          bdm.volume_id,
5648                                                          connector)
5649 
5650     @staticmethod
5651     def _set_instance_info(instance, instance_type):
5652         instance.instance_type_id = instance_type.id
5653         instance.memory_mb = instance_type.memory_mb
5654         instance.vcpus = instance_type.vcpus
5655         instance.root_gb = instance_type.root_gb
5656         instance.ephemeral_gb = instance_type.ephemeral_gb
5657         instance.flavor = instance_type
5658 
5659     def _update_volume_attachments(self, context, instance, bdms):
5660         """Updates volume attachments using the virt driver host connector.
5661 
5662         :param context: nova.context.RequestContext - user request context
5663         :param instance: nova.objects.Instance
5664         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5665                      device mappings for the given instance
5666         """
5667         if bdms:
5668             connector = None
5669             for bdm in bdms:
5670                 if bdm.is_volume and bdm.attachment_id:
5671                     if connector is None:
5672                         connector = self.driver.get_volume_connector(instance)
5673                     self.volume_api.attachment_update(
5674                         context, bdm.attachment_id, connector, bdm.device_name)
5675 
5676     def _complete_volume_attachments(self, context, bdms):
5677         """Completes volume attachments for the instance
5678 
5679         :param context: nova.context.RequestContext - user request context
5680         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5681                      device mappings for the given instance
5682         """
5683         if bdms:
5684             for bdm in bdms:
5685                 if bdm.is_volume and bdm.attachment_id:
5686                     self.volume_api.attachment_complete(
5687                         context, bdm.attachment_id)
5688 
5689     def _finish_resize(self, context, instance, migration, disk_info,
5690                        image_meta, bdms, request_spec):
5691         resize_instance = False  # indicates disks have been resized
5692         old_instance_type_id = migration['old_instance_type_id']
5693         new_instance_type_id = migration['new_instance_type_id']
5694         old_flavor = instance.flavor  # the current flavor is now old
5695         # NOTE(mriedem): Get the old_vm_state so we know if we should
5696         # power on the instance. If old_vm_state is not set we need to default
5697         # to ACTIVE for backwards compatibility
5698         old_vm_state = instance.system_metadata.get('old_vm_state',
5699                                                     vm_states.ACTIVE)
5700         instance.old_flavor = old_flavor
5701 
5702         if old_instance_type_id != new_instance_type_id:
5703             new_flavor = instance.new_flavor  # this is set in _prep_resize
5704             # Set the flavor-related fields on the instance object including
5705             # making instance.flavor = new_flavor.
5706             self._set_instance_info(instance, new_flavor)
5707             for key in ('root_gb', 'swap', 'ephemeral_gb'):
5708                 if old_flavor[key] != new_flavor[key]:
5709                     resize_instance = True
5710                     break
5711         instance.apply_migration_context()
5712 
5713         # NOTE(tr3buchet): setup networks on destination host
5714         self.network_api.setup_networks_on_host(context, instance,
5715                                                 migration.dest_compute)
5716         provider_mappings = self._get_request_group_mapping(request_spec)
5717 
5718         # For neutron, migrate_instance_finish updates port bindings for this
5719         # host including any PCI devices claimed for SR-IOV ports.
5720         self.network_api.migrate_instance_finish(
5721             context, instance, migration, provider_mappings)
5722 
5723         network_info = self.network_api.get_instance_nw_info(context, instance)
5724 
5725         instance.task_state = task_states.RESIZE_FINISH
5726         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5727 
5728         self._send_finish_resize_notifications(
5729             context, instance, bdms, network_info,
5730             fields.NotificationPhase.START)
5731 
5732         # We need to update any volume attachments using the destination
5733         # host connector so that we can update the BDM.connection_info
5734         # before calling driver.finish_migration otherwise the driver
5735         # won't know how to connect the volumes to this host.
5736         # Note that _get_instance_block_device_info with
5737         # refresh_conn_info=True will update the BDM.connection_info value
5738         # in the database so we must do this before calling that method.
5739         self._update_volume_attachments(context, instance, bdms)
5740 
5741         block_device_info = self._get_instance_block_device_info(
5742             context, instance, refresh_conn_info=True, bdms=bdms)
5743 
5744         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
5745         # automatically power on the instance after it's migrated
5746         power_on = old_vm_state != vm_states.STOPPED
5747 
5748         # NOTE(sbauza): During a migration, the original allocation is against
5749         # the migration UUID while the target allocation (for the destination
5750         # node) is related to the instance UUID, so here we need to pass the
5751         # new ones.
5752         allocations = self.reportclient.get_allocs_for_consumer(
5753             context, instance.uuid)['allocations']
5754 
5755         try:
5756             self.driver.finish_migration(context, migration, instance,
5757                                          disk_info,
5758                                          network_info,
5759                                          image_meta, resize_instance,
5760                                          allocations,
5761                                          block_device_info, power_on)
5762         except Exception:
5763             # Note that we do not rollback port bindings to the source host
5764             # because resize_instance (on the source host) updated the
5765             # instance.host to point to *this* host (the destination host)
5766             # so the port bindings pointing at this host are correct even
5767             # though we failed to create the guest.
5768             with excutils.save_and_reraise_exception():
5769                 # If we failed to create the guest on this host, reset the
5770                 # instance flavor-related fields to the old flavor. An
5771                 # error handler like reverts_task_state will save the changes.
5772                 if old_instance_type_id != new_instance_type_id:
5773                     self._set_instance_info(instance, old_flavor)
5774 
5775         # Now complete any volume attachments that were previously updated.
5776         self._complete_volume_attachments(context, bdms)
5777 
5778         migration.status = 'finished'
5779         migration.save()
5780 
5781         instance.vm_state = vm_states.RESIZED
5782         instance.task_state = None
5783         instance.launched_at = timeutils.utcnow()
5784         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5785 
5786         return network_info
5787 
5788     @wrap_exception()
5789     @reverts_task_state
5790     @wrap_instance_event(prefix='compute')
5791     @errors_out_migration
5792     @wrap_instance_fault
5793     def finish_resize(self, context, disk_info, image, instance,
5794                       migration, request_spec=None):
5795         """Completes the migration process.
5796 
5797         Sets up the newly transferred disk and turns on the instance at its
5798         new host machine.
5799 
5800         """
5801         try:
5802             self._finish_resize_helper(context, disk_info, image, instance,
5803                                        migration, request_spec)
5804         except Exception:
5805             with excutils.save_and_reraise_exception():
5806                 # At this point, resize_instance (which runs on the source) has
5807                 # already updated the instance host/node values to point to
5808                 # this (the dest) compute, so we need to leave the allocations
5809                 # against the dest node resource provider intact and drop the
5810                 # allocations against the source node resource provider. If the
5811                 # user tries to recover the server by hard rebooting it, it
5812                 # will happen on this host so that's where the allocations
5813                 # should go. Note that this is the same method called from
5814                 # confirm_resize to cleanup the source node allocations held
5815                 # by the migration record.
5816                 LOG.info('Deleting allocations for old flavor on source node '
5817                          '%s after finish_resize failure. You may be able to '
5818                          'recover the instance by hard rebooting it.',
5819                          migration.source_compute, instance=instance)
5820                 self._delete_allocation_after_move(
5821                     context, instance, migration)
5822 
5823     def _finish_resize_helper(self, context, disk_info, image, instance,
5824                               migration, request_spec):
5825         """Completes the migration process.
5826 
5827         The caller must revert the instance's allocations if the migration
5828         process failed.
5829         """
5830         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5831             context, instance.uuid)
5832 
5833         with self._error_out_instance_on_exception(context, instance):
5834             image_meta = objects.ImageMeta.from_dict(image)
5835             network_info = self._finish_resize(context, instance, migration,
5836                                                disk_info, image_meta, bdms,
5837                                                request_spec)
5838 
5839         # TODO(melwitt): We should clean up instance console tokens here. The
5840         # instance is on a new host and will need to establish a new console
5841         # connection.
5842         self._update_scheduler_instance_info(context, instance)
5843         self._send_finish_resize_notifications(
5844             context, instance, bdms, network_info,
5845             fields.NotificationPhase.END)
5846 
5847     def _send_finish_resize_notifications(
5848             self, context, instance, bdms, network_info, phase):
5849         """Send notifications for the finish_resize flow.
5850 
5851         :param context: nova auth request context
5852         :param instance: The instance being resized
5853         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5854             instance
5855         :param network_info: NetworkInfo for the instance info cache of ports
5856         :param phase: The phase of the action (NotificationPhase enum, either
5857             ``start`` or ``end``)
5858         """
5859         # Send the legacy unversioned notification.
5860         self._notify_about_instance_usage(
5861             context, instance, "finish_resize.%s" % phase,
5862             network_info=network_info)
5863         # Send the versioned notification.
5864         compute_utils.notify_about_instance_action(
5865             context, instance, self.host,
5866             action=fields.NotificationAction.RESIZE_FINISH, phase=phase,
5867             bdms=bdms)
5868 
5869     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5870     @wrap_exception()
5871     @reverts_task_state
5872     @wrap_instance_event(prefix='compute')
5873     @errors_out_migration
5874     @wrap_instance_fault
5875     def finish_snapshot_based_resize_at_dest(
5876             self, ctxt, instance, migration, snapshot_id, request_spec):
5877         """Finishes the snapshot-based resize at the destination compute.
5878 
5879         Sets up block devices and networking on the destination compute and
5880         spawns the guest.
5881 
5882         :param ctxt: nova auth request context targeted at the target cell DB
5883         :param instance: The Instance object being resized with the
5884             ``migration_context`` field set. Upon successful completion of this
5885             method the vm_state should be "resized", the task_state should be
5886             None, and migration context, host/node and flavor-related fields
5887             should be set on the instance.
5888         :param migration: The Migration object for this resize operation. Upon
5889             successful completion of this method the migration status should
5890             be "finished".
5891         :param snapshot_id: ID of the image snapshot created for a
5892             non-volume-backed instance, else None.
5893         :param request_spec: nova.objects.RequestSpec object for the operation
5894         """
5895         LOG.info('Finishing snapshot based resize on destination host %s.',
5896                  self.host, instance=instance)
5897         with self._error_out_instance_on_exception(ctxt, instance):
5898             # Note that if anything fails here, the migration-based allocations
5899             # created in conductor should be reverted by conductor as well,
5900             # see MigrationTask.rollback.
5901             self._finish_snapshot_based_resize_at_dest(
5902                 ctxt, instance, migration, snapshot_id)
5903 
5904     def _finish_snapshot_based_resize_at_dest(
5905             self, ctxt, instance, migration, snapshot_id):
5906         """Private variant of finish_snapshot_based_resize_at_dest so the
5907         caller can handle reverting resource allocations on failure and perform
5908         other generic error handling.
5909         """
5910         # Figure out the image metadata to use when spawning the guest.
5911         origin_image_ref = instance.image_ref
5912         if snapshot_id:
5913             instance.image_ref = snapshot_id
5914             image_meta = objects.ImageMeta.from_image_ref(
5915                 ctxt, self.image_api, snapshot_id)
5916         else:
5917             # Just use what is already on the volume-backed instance.
5918             image_meta = instance.image_meta
5919 
5920         resize = migration.migration_type == 'resize'
5921         instance.old_flavor = instance.flavor
5922         if resize:
5923             flavor = instance.new_flavor
5924             # If we are resizing to a new flavor we need to set the
5925             # flavor-related fields on the instance.
5926             # NOTE(mriedem): This is likely where storing old/new_flavor on
5927             # the MigrationContext would make this cleaner.
5928             self._set_instance_info(instance, flavor)
5929 
5930         instance.apply_migration_context()
5931         instance.task_state = task_states.RESIZE_FINISH
5932         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5933 
5934         # This seems a bit late to be sending the start notification but
5935         # it is what traditional resize has always done as well and it does
5936         # contain the changes to the instance with the new_flavor and
5937         # task_state.
5938         bdms = instance.get_bdms()
5939         network_info = instance.get_network_info()
5940         self._send_finish_resize_notifications(
5941             ctxt, instance, bdms, network_info,
5942             fields.NotificationPhase.START)
5943 
5944         # Setup volumes and networking and spawn the guest in the hypervisor.
5945         self._finish_snapshot_based_resize_at_dest_spawn(
5946             ctxt, instance, migration, image_meta, bdms)
5947 
5948         # If we spawned from a temporary snapshot image we can delete that now,
5949         # similar to how unshelve works.
5950         if snapshot_id:
5951             instance.image_ref = origin_image_ref
5952             compute_utils.delete_image(
5953                 ctxt, instance, self.image_api, snapshot_id)
5954 
5955         migration.status = 'finished'
5956         migration.save()
5957 
5958         self._update_instance_after_spawn(instance, vm_state=vm_states.RESIZED)
5959         # Setting the host/node values will make the ResourceTracker continue
5960         # to track usage for this instance on this host.
5961         instance.host = migration.dest_compute
5962         instance.node = migration.dest_node
5963         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5964 
5965         # Broadcast to all schedulers that the instance is on this host.
5966         self._update_scheduler_instance_info(ctxt, instance)
5967         self._send_finish_resize_notifications(
5968             ctxt, instance, bdms, network_info,
5969             fields.NotificationPhase.END)
5970 
5971     def _finish_snapshot_based_resize_at_dest_spawn(
5972             self, ctxt, instance, migration, image_meta, bdms):
5973         """Sets up volumes and networking and spawns the guest on the dest host
5974 
5975         If the instance was stopped when the resize was initiated the guest
5976         will be created but remain in a shutdown power state.
5977 
5978         If the spawn fails, port bindings are rolled back to the source host
5979         and volume connections are terminated for this dest host.
5980 
5981         :param ctxt: nova auth request context
5982         :param instance: Instance object being migrated
5983         :param migration: Migration object for the operation
5984         :param image_meta: ImageMeta object used during driver.spawn
5985         :param bdms: BlockDeviceMappingList of BDMs for the instance
5986         """
5987         # Update the volume attachments using this host's connector.
5988         # That will update the BlockDeviceMapping.connection_info which
5989         # will be used to connect the volumes on this host during spawn().
5990         block_device_info = self._prep_block_device(ctxt, instance, bdms)
5991 
5992         allocations = self.reportclient.get_allocations_for_consumer(
5993             ctxt, instance.uuid)
5994 
5995         # We do not call self.network_api.setup_networks_on_host here because
5996         # for neutron that sets up the port migration profile which is only
5997         # used during live migration with DVR. Yes it is gross knowing what
5998         # that method does internally. We could change this when bug 1814837
5999         # is fixed if setup_networks_on_host is made smarter by passing the
6000         # migration record and the method checks the migration_type.
6001 
6002         # Activate the port bindings for this host.
6003         # FIXME(mriedem): We're going to have the same issue as bug 1813789
6004         # here because this will update the port bindings and send the
6005         # network-vif-plugged event and that means when driver.spawn waits for
6006         # it we might have already gotten the event and neutron won't send
6007         # another one so we could timeout.
6008         # TODO(mriedem): Calculate provider mappings when we support cross-cell
6009         # resize/migrate with ports having resource requests.
6010         self.network_api.migrate_instance_finish(
6011             ctxt, instance, migration, provider_mappings=None)
6012         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6013 
6014         # If the original vm_state was STOPPED, we do not automatically
6015         # power on the instance after it is migrated.
6016         power_on = instance.system_metadata['old_vm_state'] == vm_states.ACTIVE
6017         try:
6018             # NOTE(mriedem): If this instance uses a config drive, it will get
6019             # rebuilt here which means any personality files will be lost,
6020             # similar to unshelve. If the instance is not using a config drive
6021             # and getting metadata from the metadata API service, personality
6022             # files would be lost regardless of the move operation.
6023             self.driver.spawn(
6024                 ctxt, instance, image_meta, injected_files=[],
6025                 admin_password=None, allocations=allocations,
6026                 network_info=network_info, block_device_info=block_device_info,
6027                 power_on=power_on)
6028         except Exception:
6029             with excutils.save_and_reraise_exception(logger=LOG):
6030                 # Rollback port bindings to the source host.
6031                 try:
6032                     # This is gross but migrate_instance_start looks at the
6033                     # migration.dest_compute to determine where to activate the
6034                     # port bindings and we want the source compute port
6035                     # bindings to be re-activated. Remember at this point the
6036                     # instance.host is still pointing at the source compute.
6037                     # TODO(mriedem): Maybe we should be calling
6038                     # setup_instance_network_on_host here to deal with pci
6039                     # devices?
6040                     with utils.temporary_mutation(
6041                             migration, dest_compute=migration.source_compute):
6042                         self.network_api.migrate_instance_start(
6043                             ctxt, instance, migration)
6044                 except Exception:
6045                     LOG.exception(
6046                         'Failed to activate port bindings on the source '
6047                         'host: %s', migration.source_compute,
6048                         instance=instance)
6049 
6050                 # Rollback volume connections on this host.
6051                 for bdm in bdms:
6052                     if bdm.is_volume:
6053                         try:
6054                             self._remove_volume_connection(
6055                                 ctxt, bdm, instance, delete_attachment=True)
6056                         except Exception:
6057                             LOG.exception('Failed to remove volume connection '
6058                                           'on this host %s for volume %s.',
6059                                           self.host, bdm.volume_id,
6060                                           instance=instance)
6061 
6062     @wrap_exception()
6063     @wrap_instance_fault
6064     def add_fixed_ip_to_instance(self, context, network_id, instance):
6065         """Calls network_api to add new fixed_ip to instance
6066         then injects the new network info and resets instance networking.
6067 
6068         """
6069         self._notify_about_instance_usage(
6070                 context, instance, "create_ip.start")
6071 
6072         network_info = self.network_api.add_fixed_ip_to_instance(context,
6073                                                                  instance,
6074                                                                  network_id)
6075         self._inject_network_info(instance, network_info)
6076 
6077         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6078         instance.updated_at = timeutils.utcnow()
6079         instance.save()
6080 
6081         self._notify_about_instance_usage(
6082             context, instance, "create_ip.end", network_info=network_info)
6083 
6084     @wrap_exception()
6085     @wrap_instance_fault
6086     def remove_fixed_ip_from_instance(self, context, address, instance):
6087         """Calls network_api to remove existing fixed_ip from instance
6088         by injecting the altered network info and resetting
6089         instance networking.
6090         """
6091         self._notify_about_instance_usage(
6092                 context, instance, "delete_ip.start")
6093 
6094         network_info = self.network_api.remove_fixed_ip_from_instance(context,
6095                                                                       instance,
6096                                                                       address)
6097         self._inject_network_info(instance, network_info)
6098 
6099         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6100         instance.updated_at = timeutils.utcnow()
6101         instance.save()
6102 
6103         self._notify_about_instance_usage(
6104             context, instance, "delete_ip.end", network_info=network_info)
6105 
6106     @wrap_exception()
6107     @reverts_task_state
6108     @wrap_instance_event(prefix='compute')
6109     @wrap_instance_fault
6110     def pause_instance(self, context, instance):
6111         """Pause an instance on this host."""
6112         context = context.elevated()
6113         LOG.info('Pausing', instance=instance)
6114         self._notify_about_instance_usage(context, instance, 'pause.start')
6115         compute_utils.notify_about_instance_action(context, instance,
6116                self.host, action=fields.NotificationAction.PAUSE,
6117                phase=fields.NotificationPhase.START)
6118         self.driver.pause(instance)
6119         instance.power_state = self._get_power_state(instance)
6120         instance.vm_state = vm_states.PAUSED
6121         instance.task_state = None
6122         instance.save(expected_task_state=task_states.PAUSING)
6123         self._notify_about_instance_usage(context, instance, 'pause.end')
6124         compute_utils.notify_about_instance_action(context, instance,
6125                self.host, action=fields.NotificationAction.PAUSE,
6126                phase=fields.NotificationPhase.END)
6127 
6128     @wrap_exception()
6129     @reverts_task_state
6130     @wrap_instance_event(prefix='compute')
6131     @wrap_instance_fault
6132     def unpause_instance(self, context, instance):
6133         """Unpause a paused instance on this host."""
6134         context = context.elevated()
6135         LOG.info('Unpausing', instance=instance)
6136         self._notify_about_instance_usage(context, instance, 'unpause.start')
6137         compute_utils.notify_about_instance_action(context, instance,
6138             self.host, action=fields.NotificationAction.UNPAUSE,
6139             phase=fields.NotificationPhase.START)
6140         self.driver.unpause(instance)
6141         instance.power_state = self._get_power_state(instance)
6142         instance.vm_state = vm_states.ACTIVE
6143         instance.task_state = None
6144         instance.save(expected_task_state=task_states.UNPAUSING)
6145         self._notify_about_instance_usage(context, instance, 'unpause.end')
6146         compute_utils.notify_about_instance_action(context, instance,
6147             self.host, action=fields.NotificationAction.UNPAUSE,
6148             phase=fields.NotificationPhase.END)
6149 
6150     @wrap_exception()
6151     def host_power_action(self, context, action):
6152         """Reboots, shuts down or powers up the host."""
6153         return self.driver.host_power_action(action)
6154 
6155     @wrap_exception()
6156     def host_maintenance_mode(self, context, host, mode):
6157         """Start/Stop host maintenance window. On start, it triggers
6158         guest VMs evacuation.
6159         """
6160         return self.driver.host_maintenance_mode(host, mode)
6161 
6162     def _update_compute_provider_status(self, context, enabled):
6163         """Adds or removes the COMPUTE_STATUS_DISABLED trait for this host.
6164 
6165         For each ComputeNode managed by this service, adds or removes the
6166         COMPUTE_STATUS_DISABLED traits to/from the associated resource provider
6167         in Placement.
6168 
6169         :param context: nova auth RequestContext
6170         :param enabled: True if the node is enabled in which case the trait
6171             would be removed, False if the node is disabled in which case
6172             the trait would be added.
6173         :raises: ComputeHostNotFound if there are no compute nodes found in
6174             the ResourceTracker for this service.
6175         """
6176         # Get the compute node(s) on this host. Remember that ironic can be
6177         # managing more than one compute node.
6178         nodes = self.rt.compute_nodes.values()
6179         if not nodes:
6180             raise exception.ComputeHostNotFound(host=self.host)
6181         # For each node, we want to add (or remove) the COMPUTE_STATUS_DISABLED
6182         # trait on the related resource provider in placement so the scheduler
6183         # (pre-)filters the provider based on its status.
6184         for node in nodes:
6185             try:
6186                 self.virtapi.update_compute_provider_status(
6187                     context, node.uuid, enabled)
6188             except (exception.ResourceProviderTraitRetrievalFailed,
6189                     exception.ResourceProviderUpdateConflict,
6190                     exception.ResourceProviderUpdateFailed,
6191                     exception.TraitRetrievalFailed) as e:
6192                 # This is best effort so just log a warning and continue.
6193                 LOG.warning('An error occurred while updating '
6194                             'COMPUTE_STATUS_DISABLED trait on compute node '
6195                             'resource provider %s. The trait will be '
6196                             'synchronized when the update_available_resource '
6197                             'periodic task runs. Error: %s',
6198                             node.uuid, e.format_message())
6199             except Exception:
6200                 LOG.exception('An error occurred while updating '
6201                               'COMPUTE_STATUS_DISABLED trait on compute node '
6202                               'resource provider %s. The trait will be '
6203                               'synchronized when the '
6204                               'update_available_resource periodic task runs.',
6205                               node.uuid)
6206 
6207     @wrap_exception()
6208     def set_host_enabled(self, context, enabled):
6209         """Sets the specified host's ability to accept new instances.
6210 
6211         This method will add or remove the COMPUTE_STATUS_DISABLED trait
6212         to/from the associated compute node resource provider(s) for this
6213         compute service.
6214         """
6215         try:
6216             self._update_compute_provider_status(context, enabled)
6217         except exception.ComputeHostNotFound:
6218             LOG.warning('Unable to add/remove trait COMPUTE_STATUS_DISABLED. '
6219                         'No ComputeNode(s) found for host: %s', self.host)
6220 
6221         try:
6222             return self.driver.set_host_enabled(enabled)
6223         except NotImplementedError:
6224             # Only the xenapi driver implements set_host_enabled but we don't
6225             # want NotImplementedError to get raised back to the API. We still
6226             # need to honor the compute RPC API contract and return 'enabled'
6227             # or 'disabled' though.
6228             return 'enabled' if enabled else 'disabled'
6229 
6230     @wrap_exception()
6231     def get_host_uptime(self, context):
6232         """Returns the result of calling "uptime" on the target host."""
6233         return self.driver.get_host_uptime()
6234 
6235     @wrap_exception()
6236     @wrap_instance_fault
6237     def get_diagnostics(self, context, instance):
6238         """Retrieve diagnostics for an instance on this host."""
6239         current_power_state = self._get_power_state(instance)
6240         if current_power_state == power_state.RUNNING:
6241             LOG.info("Retrieving diagnostics", instance=instance)
6242             return self.driver.get_diagnostics(instance)
6243         else:
6244             raise exception.InstanceInvalidState(
6245                 attr='power state',
6246                 instance_uuid=instance.uuid,
6247                 state=power_state.STATE_MAP[instance.power_state],
6248                 method='get_diagnostics')
6249 
6250     @wrap_exception()
6251     @wrap_instance_fault
6252     def get_instance_diagnostics(self, context, instance):
6253         """Retrieve diagnostics for an instance on this host."""
6254         current_power_state = self._get_power_state(instance)
6255         if current_power_state == power_state.RUNNING:
6256             LOG.info("Retrieving diagnostics", instance=instance)
6257             return self.driver.get_instance_diagnostics(instance)
6258         else:
6259             raise exception.InstanceInvalidState(
6260                 attr='power state',
6261                 instance_uuid=instance.uuid,
6262                 state=power_state.STATE_MAP[instance.power_state],
6263                 method='get_diagnostics')
6264 
6265     @wrap_exception()
6266     @reverts_task_state
6267     @wrap_instance_event(prefix='compute')
6268     @wrap_instance_fault
6269     def suspend_instance(self, context, instance):
6270         """Suspend the given instance."""
6271         context = context.elevated()
6272 
6273         # Store the old state
6274         instance.system_metadata['old_vm_state'] = instance.vm_state
6275         self._notify_about_instance_usage(context, instance, 'suspend.start')
6276         compute_utils.notify_about_instance_action(context, instance,
6277                 self.host, action=fields.NotificationAction.SUSPEND,
6278                 phase=fields.NotificationPhase.START)
6279         with self._error_out_instance_on_exception(context, instance,
6280              instance_state=instance.vm_state):
6281             self.driver.suspend(context, instance)
6282         instance.power_state = self._get_power_state(instance)
6283         instance.vm_state = vm_states.SUSPENDED
6284         instance.task_state = None
6285         instance.save(expected_task_state=task_states.SUSPENDING)
6286         self._notify_about_instance_usage(context, instance, 'suspend.end')
6287         compute_utils.notify_about_instance_action(context, instance,
6288                 self.host, action=fields.NotificationAction.SUSPEND,
6289                 phase=fields.NotificationPhase.END)
6290 
6291     @wrap_exception()
6292     @reverts_task_state
6293     @wrap_instance_event(prefix='compute')
6294     @wrap_instance_fault
6295     def resume_instance(self, context, instance):
6296         """Resume the given suspended instance."""
6297         context = context.elevated()
6298         LOG.info('Resuming', instance=instance)
6299 
6300         self._notify_about_instance_usage(context, instance, 'resume.start')
6301 
6302         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6303             context, instance.uuid)
6304         block_device_info = self._get_instance_block_device_info(
6305             context, instance, bdms=bdms)
6306 
6307         compute_utils.notify_about_instance_action(context, instance,
6308             self.host, action=fields.NotificationAction.RESUME,
6309             phase=fields.NotificationPhase.START, bdms=bdms)
6310 
6311         network_info = self.network_api.get_instance_nw_info(context, instance)
6312 
6313         with self._error_out_instance_on_exception(context, instance,
6314              instance_state=instance.vm_state):
6315             self.driver.resume(context, instance, network_info,
6316                                block_device_info)
6317 
6318         instance.power_state = self._get_power_state(instance)
6319 
6320         # We default to the ACTIVE state for backwards compatibility
6321         instance.vm_state = instance.system_metadata.pop('old_vm_state',
6322                                                          vm_states.ACTIVE)
6323 
6324         instance.task_state = None
6325         instance.save(expected_task_state=task_states.RESUMING)
6326         self._notify_about_instance_usage(context, instance, 'resume.end')
6327         compute_utils.notify_about_instance_action(context, instance,
6328             self.host, action=fields.NotificationAction.RESUME,
6329             phase=fields.NotificationPhase.END, bdms=bdms)
6330 
6331     @wrap_exception()
6332     @reverts_task_state
6333     @wrap_instance_event(prefix='compute')
6334     @wrap_instance_fault
6335     def shelve_instance(self, context, instance, image_id,
6336                         clean_shutdown, accel_uuids=None):
6337         """Shelve an instance.
6338 
6339         This should be used when you want to take a snapshot of the instance.
6340         It also adds system_metadata that can be used by a periodic task to
6341         offload the shelved instance after a period of time.
6342 
6343         :param context: request context
6344         :param instance: an Instance object
6345         :param image_id: an image id to snapshot to.
6346         :param clean_shutdown: give the GuestOS a chance to stop
6347         :param accel_uuids: the accelerators uuids for the instance
6348         """
6349 
6350         @utils.synchronized(instance.uuid)
6351         def do_shelve_instance():
6352             self._shelve_instance(context, instance, image_id, clean_shutdown,
6353                                   accel_uuids)
6354         do_shelve_instance()
6355 
6356     def _shelve_instance(self, context, instance, image_id,
6357                          clean_shutdown, accel_uuids=None):
6358         LOG.info('Shelving', instance=instance)
6359         offload = CONF.shelved_offload_time == 0
6360         if offload:
6361             # Get the BDMs early so we can pass them into versioned
6362             # notifications since _shelve_offload_instance needs the
6363             # BDMs anyway.
6364             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6365                 context, instance.uuid)
6366         else:
6367             bdms = None
6368         compute_utils.notify_usage_exists(self.notifier, context, instance,
6369                                           self.host, current_period=True)
6370         self._notify_about_instance_usage(context, instance, 'shelve.start')
6371         compute_utils.notify_about_instance_action(context, instance,
6372                 self.host, action=fields.NotificationAction.SHELVE,
6373                 phase=fields.NotificationPhase.START, bdms=bdms)
6374 
6375         def update_task_state(task_state, expected_state=task_states.SHELVING):
6376             shelving_state_map = {
6377                     task_states.IMAGE_PENDING_UPLOAD:
6378                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
6379                     task_states.IMAGE_UPLOADING:
6380                         task_states.SHELVING_IMAGE_UPLOADING,
6381                     task_states.SHELVING: task_states.SHELVING}
6382             task_state = shelving_state_map[task_state]
6383             expected_state = shelving_state_map[expected_state]
6384             instance.task_state = task_state
6385             instance.save(expected_task_state=expected_state)
6386         # Do not attempt a clean shutdown of a paused guest since some
6387         # hypervisors will fail the clean shutdown if the guest is not
6388         # running.
6389         if instance.power_state == power_state.PAUSED:
6390             clean_shutdown = False
6391         self._power_off_instance(instance, clean_shutdown)
6392         self.driver.snapshot(context, instance, image_id, update_task_state)
6393 
6394         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
6395         instance.system_metadata['shelved_image_id'] = image_id
6396         instance.system_metadata['shelved_host'] = self.host
6397         instance.vm_state = vm_states.SHELVED
6398         instance.task_state = None
6399         if offload:
6400             instance.task_state = task_states.SHELVING_OFFLOADING
6401         instance.power_state = self._get_power_state(instance)
6402         instance.save(expected_task_state=[
6403                 task_states.SHELVING,
6404                 task_states.SHELVING_IMAGE_UPLOADING])
6405 
6406         self._notify_about_instance_usage(context, instance, 'shelve.end')
6407         compute_utils.notify_about_instance_action(context, instance,
6408                 self.host, action=fields.NotificationAction.SHELVE,
6409                 phase=fields.NotificationPhase.END, bdms=bdms)
6410 
6411         if offload:
6412             self._shelve_offload_instance(
6413                 context, instance, clean_shutdown=False, bdms=bdms,
6414                 accel_uuids=accel_uuids)
6415 
6416     @wrap_exception()
6417     @reverts_task_state
6418     @wrap_instance_event(prefix='compute')
6419     @wrap_instance_fault
6420     def shelve_offload_instance(self, context, instance, clean_shutdown,
6421             accel_uuids=None):
6422         """Remove a shelved instance from the hypervisor.
6423 
6424         This frees up those resources for use by other instances, but may lead
6425         to slower unshelve times for this instance.  This method is used by
6426         volume backed instances since restoring them doesn't involve the
6427         potentially large download of an image.
6428 
6429         :param context: request context
6430         :param instance: nova.objects.instance.Instance
6431         :param clean_shutdown: give the GuestOS a chance to stop
6432         :param accel_uuids: the accelerators uuids for the instance
6433         """
6434 
6435         @utils.synchronized(instance.uuid)
6436         def do_shelve_offload_instance():
6437             self._shelve_offload_instance(context, instance, clean_shutdown,
6438                                           accel_uuids=accel_uuids)
6439         do_shelve_offload_instance()
6440 
6441     def _shelve_offload_instance(self, context, instance, clean_shutdown,
6442                                  bdms=None, accel_uuids=None):
6443         LOG.info('Shelve offloading', instance=instance)
6444         if bdms is None:
6445             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6446                 context, instance.uuid)
6447         self._notify_about_instance_usage(context, instance,
6448                 'shelve_offload.start')
6449         compute_utils.notify_about_instance_action(context, instance,
6450                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6451                 phase=fields.NotificationPhase.START, bdms=bdms)
6452 
6453         self._power_off_instance(instance, clean_shutdown)
6454         current_power_state = self._get_power_state(instance)
6455         network_info = self.network_api.get_instance_nw_info(context, instance)
6456 
6457         block_device_info = self._get_instance_block_device_info(context,
6458                                                                  instance,
6459                                                                  bdms=bdms)
6460         self.driver.destroy(context, instance, network_info,
6461                 block_device_info)
6462 
6463         # the instance is going to be removed from the host so we want to
6464         # terminate all the connections with the volume server and the host
6465         self._terminate_volume_connections(context, instance, bdms)
6466 
6467         # NOTE(brinzhang): Free up the accelerator resource occupied
6468         # in the cyborg service.
6469         if accel_uuids:
6470             cyclient = cyborg.get_client(context)
6471             cyclient.delete_arqs_for_instance(instance.uuid)
6472 
6473         # Free up the resource allocations in the placement service.
6474         # This should happen *before* the vm_state is changed to
6475         # SHELVED_OFFLOADED in case client-side code is polling the API to
6476         # schedule more instances (or unshelve) once this server is offloaded.
6477         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
6478                                                                 instance)
6479 
6480         instance.power_state = current_power_state
6481         # NOTE(mriedem): The vm_state has to be set before updating the
6482         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
6483         # values cannot be nulled out until after updating the resource tracker
6484         # though.
6485         instance.vm_state = vm_states.SHELVED_OFFLOADED
6486         instance.task_state = None
6487         instance.save(expected_task_state=[task_states.SHELVING,
6488                                            task_states.SHELVING_OFFLOADING])
6489 
6490         # NOTE(ndipanov): Free resources from the resource tracker
6491         self._update_resource_tracker(context, instance)
6492 
6493         # NOTE(sfinucan): RPC calls should no longer be attempted against this
6494         # instance, so ensure any calls result in errors
6495         self._nil_out_instance_obj_host_and_node(instance)
6496         instance.save(expected_task_state=None)
6497 
6498         # TODO(melwitt): We should clean up instance console tokens here. The
6499         # instance has no host at this point and will need to establish a new
6500         # console connection in the future after it is unshelved.
6501         self._delete_scheduler_instance_info(context, instance.uuid)
6502         self._notify_about_instance_usage(context, instance,
6503                 'shelve_offload.end')
6504         compute_utils.notify_about_instance_action(context, instance,
6505                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6506                 phase=fields.NotificationPhase.END, bdms=bdms)
6507 
6508     @wrap_exception()
6509     @reverts_task_state
6510     @wrap_instance_event(prefix='compute')
6511     @wrap_instance_fault
6512     def unshelve_instance(
6513             self, context, instance, image, filter_properties, node,
6514             request_spec=None, accel_uuids=None):
6515         """Unshelve the instance.
6516 
6517         :param context: request context
6518         :param instance: a nova.objects.instance.Instance object
6519         :param image: an image to build from.  If None we assume a
6520             volume backed instance.
6521         :param filter_properties: dict containing limits, retry info etc.
6522         :param node: target compute node
6523         :param request_spec: the RequestSpec object used to schedule the
6524             instance
6525         :param accel_uuids: the accelerators uuids for the instance
6526         """
6527         if filter_properties is None:
6528             filter_properties = {}
6529 
6530         @utils.synchronized(instance.uuid)
6531         def do_unshelve_instance():
6532             self._unshelve_instance(
6533                 context, instance, image, filter_properties, node,
6534                 request_spec, accel_uuids)
6535         do_unshelve_instance()
6536 
6537     def _unshelve_instance_key_scrub(self, instance):
6538         """Remove data from the instance that may cause side effects."""
6539         cleaned_keys = dict(
6540                 key_data=instance.key_data,
6541                 auto_disk_config=instance.auto_disk_config)
6542         instance.key_data = None
6543         instance.auto_disk_config = False
6544         return cleaned_keys
6545 
6546     def _unshelve_instance_key_restore(self, instance, keys):
6547         """Restore previously scrubbed keys before saving the instance."""
6548         instance.update(keys)
6549 
6550     def _unshelve_instance(self, context, instance, image, filter_properties,
6551                            node, request_spec, accel_uuids):
6552         LOG.info('Unshelving', instance=instance)
6553         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6554                 context, instance.uuid)
6555 
6556         self._notify_about_instance_usage(context, instance, 'unshelve.start')
6557         compute_utils.notify_about_instance_action(context, instance,
6558                 self.host, action=fields.NotificationAction.UNSHELVE,
6559                 phase=fields.NotificationPhase.START, bdms=bdms)
6560 
6561         instance.task_state = task_states.SPAWNING
6562         instance.save()
6563 
6564         block_device_info = self._prep_block_device(context, instance, bdms)
6565         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
6566 
6567         if node is None:
6568             node = self._get_nodename(instance)
6569 
6570         limits = filter_properties.get('limits', {})
6571 
6572         allocations = self.reportclient.get_allocations_for_consumer(
6573             context, instance.uuid)
6574 
6575         shelved_image_ref = instance.image_ref
6576         if image:
6577             instance.image_ref = image['id']
6578             image_meta = objects.ImageMeta.from_dict(image)
6579         else:
6580             image_meta = objects.ImageMeta.from_dict(
6581                 utils.get_image_from_system_metadata(
6582                     instance.system_metadata))
6583 
6584         provider_mappings = self._get_request_group_mapping(request_spec)
6585 
6586         try:
6587             if provider_mappings:
6588                 update = (
6589                     compute_utils.
6590                         update_pci_request_spec_with_allocated_interface_name)
6591                 update(
6592                     context, self.reportclient, instance.pci_requests.requests,
6593                     provider_mappings)
6594 
6595             self.network_api.setup_instance_network_on_host(
6596                 context, instance, self.host,
6597                 provider_mappings=provider_mappings)
6598             network_info = self.network_api.get_instance_nw_info(
6599                 context, instance)
6600 
6601             accel_info = []
6602             if accel_uuids:
6603                 try:
6604                     accel_info = self._get_bound_arq_resources(
6605                         context, instance, accel_uuids)
6606                 except (Exception, eventlet.timeout.Timeout) as exc:
6607                     LOG.exception('Failure getting accelerator requests '
6608                                   'with the exception: %s', exc,
6609                                   instance=instance)
6610                     self._build_resources_cleanup(instance, network_info)
6611                     raise
6612 
6613             with self.rt.instance_claim(context, instance, node, allocations,
6614                                         limits):
6615                 self.driver.spawn(context, instance, image_meta,
6616                                   injected_files=[],
6617                                   admin_password=None,
6618                                   allocations=allocations,
6619                                   network_info=network_info,
6620                                   block_device_info=block_device_info,
6621                                   accel_info=accel_info)
6622         except Exception:
6623             with excutils.save_and_reraise_exception(logger=LOG):
6624                 LOG.exception('Instance failed to spawn',
6625                               instance=instance)
6626                 # Cleanup allocations created by the scheduler on this host
6627                 # since we failed to spawn the instance. We do this both if
6628                 # the instance claim failed with ComputeResourcesUnavailable
6629                 # or if we did claim but the spawn failed, because aborting the
6630                 # instance claim will not remove the allocations.
6631                 self.reportclient.delete_allocation_for_instance(context,
6632                                                                  instance.uuid)
6633                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
6634                 self._terminate_volume_connections(context, instance, bdms)
6635                 # The reverts_task_state decorator on unshelve_instance will
6636                 # eventually save these updates.
6637                 self._nil_out_instance_obj_host_and_node(instance)
6638 
6639         if image:
6640             instance.image_ref = shelved_image_ref
6641             self._delete_snapshot_of_shelved_instance(context, instance,
6642                                                       image['id'])
6643 
6644         self._unshelve_instance_key_restore(instance, scrubbed_keys)
6645         self._update_instance_after_spawn(instance)
6646         # Delete system_metadata for a shelved instance
6647         compute_utils.remove_shelved_keys_from_system_metadata(instance)
6648 
6649         instance.save(expected_task_state=task_states.SPAWNING)
6650         self._update_scheduler_instance_info(context, instance)
6651         self._notify_about_instance_usage(context, instance, 'unshelve.end')
6652         compute_utils.notify_about_instance_action(context, instance,
6653                 self.host, action=fields.NotificationAction.UNSHELVE,
6654                 phase=fields.NotificationPhase.END, bdms=bdms)
6655 
6656     # TODO(stephenfin): Remove this in RPC 6.0 since it's nova-network only
6657     @messaging.expected_exceptions(NotImplementedError)
6658     def reset_network(self, context, instance):
6659         """Reset networking on the given instance."""
6660         raise NotImplementedError()
6661 
6662     def _inject_network_info(self, instance, network_info):
6663         """Inject network info for the given instance."""
6664         LOG.debug('Inject network info', instance=instance)
6665         LOG.debug('network_info to inject: |%s|', network_info,
6666                   instance=instance)
6667 
6668         self.driver.inject_network_info(instance, network_info)
6669 
6670     @wrap_instance_fault
6671     def inject_network_info(self, context, instance):
6672         """Inject network info, but don't return the info."""
6673         network_info = self.network_api.get_instance_nw_info(context, instance)
6674         self._inject_network_info(instance, network_info)
6675 
6676     @messaging.expected_exceptions(NotImplementedError,
6677                                    exception.ConsoleNotAvailable,
6678                                    exception.InstanceNotFound)
6679     @wrap_exception()
6680     @wrap_instance_fault
6681     def get_console_output(self, context, instance, tail_length):
6682         """Send the console output for the given instance."""
6683         context = context.elevated()
6684         LOG.info("Get console output", instance=instance)
6685         output = self.driver.get_console_output(context, instance)
6686 
6687         if type(output) is str:
6688             output = output.encode("latin-1")
6689 
6690         if tail_length is not None:
6691             output = self._tail_log(output, tail_length)
6692 
6693         return output.decode('ascii', 'replace')
6694 
6695     def _tail_log(self, log, length):
6696         try:
6697             length = int(length)
6698         except ValueError:
6699             length = 0
6700 
6701         if length == 0:
6702             return b''
6703         else:
6704             return b'\n'.join(log.split(b'\n')[-int(length):])
6705 
6706     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6707                                    exception.InstanceNotReady,
6708                                    exception.InstanceNotFound,
6709                                    exception.ConsoleTypeUnavailable,
6710                                    NotImplementedError)
6711     @wrap_exception()
6712     @wrap_instance_fault
6713     def get_vnc_console(self, context, console_type, instance):
6714         """Return connection information for a vnc console."""
6715         context = context.elevated()
6716         LOG.debug("Getting vnc console", instance=instance)
6717 
6718         if not CONF.vnc.enabled:
6719             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6720 
6721         if console_type == 'novnc':
6722             # For essex, novncproxy_base_url must include the full path
6723             # including the html file (like http://myhost/vnc_auto.html)
6724             access_url_base = CONF.vnc.novncproxy_base_url
6725         else:
6726             raise exception.ConsoleTypeInvalid(console_type=console_type)
6727 
6728         try:
6729             # Retrieve connect info from driver, and then decorate with our
6730             # access info token
6731             console = self.driver.get_vnc_console(context, instance)
6732             console_auth = objects.ConsoleAuthToken(
6733                 context=context,
6734                 console_type=console_type,
6735                 host=console.host,
6736                 port=console.port,
6737                 internal_access_path=console.internal_access_path,
6738                 instance_uuid=instance.uuid,
6739                 access_url_base=access_url_base,
6740             )
6741             console_auth.authorize(CONF.consoleauth.token_ttl)
6742             connect_info = console.get_connection_info(
6743                 console_auth.token, console_auth.access_url)
6744 
6745         except exception.InstanceNotFound:
6746             if instance.vm_state != vm_states.BUILDING:
6747                 raise
6748             raise exception.InstanceNotReady(instance_id=instance.uuid)
6749 
6750         return connect_info
6751 
6752     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6753                                    exception.InstanceNotReady,
6754                                    exception.InstanceNotFound,
6755                                    exception.ConsoleTypeUnavailable,
6756                                    NotImplementedError)
6757     @wrap_exception()
6758     @wrap_instance_fault
6759     def get_spice_console(self, context, console_type, instance):
6760         """Return connection information for a spice console."""
6761         context = context.elevated()
6762         LOG.debug("Getting spice console", instance=instance)
6763 
6764         if not CONF.spice.enabled:
6765             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6766 
6767         if console_type != 'spice-html5':
6768             raise exception.ConsoleTypeInvalid(console_type=console_type)
6769 
6770         try:
6771             # Retrieve connect info from driver, and then decorate with our
6772             # access info token
6773             console = self.driver.get_spice_console(context, instance)
6774             console_auth = objects.ConsoleAuthToken(
6775                 context=context,
6776                 console_type=console_type,
6777                 host=console.host,
6778                 port=console.port,
6779                 internal_access_path=console.internal_access_path,
6780                 instance_uuid=instance.uuid,
6781                 access_url_base=CONF.spice.html5proxy_base_url,
6782             )
6783             console_auth.authorize(CONF.consoleauth.token_ttl)
6784             connect_info = console.get_connection_info(
6785                 console_auth.token, console_auth.access_url)
6786 
6787         except exception.InstanceNotFound:
6788             if instance.vm_state != vm_states.BUILDING:
6789                 raise
6790             raise exception.InstanceNotReady(instance_id=instance.uuid)
6791 
6792         return connect_info
6793 
6794     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6795                                    exception.InstanceNotReady,
6796                                    exception.InstanceNotFound,
6797                                    exception.ConsoleTypeUnavailable,
6798                                    NotImplementedError)
6799     @wrap_exception()
6800     @wrap_instance_fault
6801     def get_rdp_console(self, context, console_type, instance):
6802         """Return connection information for a RDP console."""
6803         context = context.elevated()
6804         LOG.debug("Getting RDP console", instance=instance)
6805 
6806         if not CONF.rdp.enabled:
6807             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6808 
6809         if console_type != 'rdp-html5':
6810             raise exception.ConsoleTypeInvalid(console_type=console_type)
6811 
6812         try:
6813             # Retrieve connect info from driver, and then decorate with our
6814             # access info token
6815             console = self.driver.get_rdp_console(context, instance)
6816             console_auth = objects.ConsoleAuthToken(
6817                 context=context,
6818                 console_type=console_type,
6819                 host=console.host,
6820                 port=console.port,
6821                 internal_access_path=console.internal_access_path,
6822                 instance_uuid=instance.uuid,
6823                 access_url_base=CONF.rdp.html5_proxy_base_url,
6824             )
6825             console_auth.authorize(CONF.consoleauth.token_ttl)
6826             connect_info = console.get_connection_info(
6827                 console_auth.token, console_auth.access_url)
6828 
6829         except exception.InstanceNotFound:
6830             if instance.vm_state != vm_states.BUILDING:
6831                 raise
6832             raise exception.InstanceNotReady(instance_id=instance.uuid)
6833 
6834         return connect_info
6835 
6836     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6837                                    exception.InstanceNotReady,
6838                                    exception.InstanceNotFound,
6839                                    exception.ConsoleTypeUnavailable,
6840                                    NotImplementedError)
6841     @wrap_exception()
6842     @wrap_instance_fault
6843     def get_mks_console(self, context, console_type, instance):
6844         """Return connection information for a MKS console."""
6845         context = context.elevated()
6846         LOG.debug("Getting MKS console", instance=instance)
6847 
6848         if not CONF.mks.enabled:
6849             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6850 
6851         if console_type != 'webmks':
6852             raise exception.ConsoleTypeInvalid(console_type=console_type)
6853 
6854         try:
6855             # Retrieve connect info from driver, and then decorate with our
6856             # access info token
6857             console = self.driver.get_mks_console(context, instance)
6858             console_auth = objects.ConsoleAuthToken(
6859                 context=context,
6860                 console_type=console_type,
6861                 host=console.host,
6862                 port=console.port,
6863                 internal_access_path=console.internal_access_path,
6864                 instance_uuid=instance.uuid,
6865                 access_url_base=CONF.mks.mksproxy_base_url,
6866             )
6867             console_auth.authorize(CONF.consoleauth.token_ttl)
6868             connect_info = console.get_connection_info(
6869                 console_auth.token, console_auth.access_url)
6870 
6871         except exception.InstanceNotFound:
6872             if instance.vm_state != vm_states.BUILDING:
6873                 raise
6874             raise exception.InstanceNotReady(instance_id=instance.uuid)
6875 
6876         return connect_info
6877 
6878     @messaging.expected_exceptions(
6879         exception.ConsoleTypeInvalid,
6880         exception.InstanceNotReady,
6881         exception.InstanceNotFound,
6882         exception.ConsoleTypeUnavailable,
6883         exception.SocketPortRangeExhaustedException,
6884         exception.ImageSerialPortNumberInvalid,
6885         exception.ImageSerialPortNumberExceedFlavorValue,
6886         NotImplementedError)
6887     @wrap_exception()
6888     @wrap_instance_fault
6889     def get_serial_console(self, context, console_type, instance):
6890         """Returns connection information for a serial console."""
6891 
6892         LOG.debug("Getting serial console", instance=instance)
6893 
6894         if not CONF.serial_console.enabled:
6895             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6896 
6897         context = context.elevated()
6898 
6899         try:
6900             # Retrieve connect info from driver, and then decorate with our
6901             # access info token
6902             console = self.driver.get_serial_console(context, instance)
6903             console_auth = objects.ConsoleAuthToken(
6904                 context=context,
6905                 console_type=console_type,
6906                 host=console.host,
6907                 port=console.port,
6908                 internal_access_path=console.internal_access_path,
6909                 instance_uuid=instance.uuid,
6910                 access_url_base=CONF.serial_console.base_url,
6911             )
6912             console_auth.authorize(CONF.consoleauth.token_ttl)
6913             connect_info = console.get_connection_info(
6914                 console_auth.token, console_auth.access_url)
6915 
6916         except exception.InstanceNotFound:
6917             if instance.vm_state != vm_states.BUILDING:
6918                 raise
6919             raise exception.InstanceNotReady(instance_id=instance.uuid)
6920 
6921         return connect_info
6922 
6923     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6924                                    exception.InstanceNotReady,
6925                                    exception.InstanceNotFound)
6926     @wrap_exception()
6927     @wrap_instance_fault
6928     def validate_console_port(self, ctxt, instance, port, console_type):
6929         if console_type == "spice-html5":
6930             console_info = self.driver.get_spice_console(ctxt, instance)
6931         elif console_type == "rdp-html5":
6932             console_info = self.driver.get_rdp_console(ctxt, instance)
6933         elif console_type == "serial":
6934             console_info = self.driver.get_serial_console(ctxt, instance)
6935         elif console_type == "webmks":
6936             console_info = self.driver.get_mks_console(ctxt, instance)
6937         else:
6938             console_info = self.driver.get_vnc_console(ctxt, instance)
6939 
6940         # Some drivers may return an int on console_info.port but the port
6941         # variable in this method is a string, so cast to be sure we are
6942         # comparing the correct types.
6943         return str(console_info.port) == port
6944 
6945     @wrap_exception()
6946     @reverts_task_state
6947     @wrap_instance_fault
6948     def reserve_block_device_name(self, context, instance, device,
6949                                   volume_id, disk_bus, device_type, tag,
6950                                   multiattach):
6951         if (tag and not
6952                 self.driver.capabilities.get('supports_tagged_attach_volume',
6953                                              False)):
6954             raise exception.VolumeTaggedAttachNotSupported()
6955 
6956         if (multiattach and not
6957                 self.driver.capabilities.get('supports_multiattach', False)):
6958             raise exception.MultiattachNotSupportedByVirtDriver(
6959                 volume_id=volume_id)
6960 
6961         @utils.synchronized(instance.uuid)
6962         def do_reserve():
6963             bdms = (
6964                 objects.BlockDeviceMappingList.get_by_instance_uuid(
6965                     context, instance.uuid))
6966 
6967             # NOTE(ndipanov): We need to explicitly set all the fields on the
6968             #                 object so that obj_load_attr does not fail
6969             new_bdm = objects.BlockDeviceMapping(
6970                     context=context,
6971                     source_type='volume', destination_type='volume',
6972                     instance_uuid=instance.uuid, boot_index=None,
6973                     volume_id=volume_id,
6974                     device_name=device, guest_format=None,
6975                     disk_bus=disk_bus, device_type=device_type, tag=tag)
6976 
6977             new_bdm.device_name = self._get_device_name_for_instance(
6978                     instance, bdms, new_bdm)
6979 
6980             # NOTE(vish): create bdm here to avoid race condition
6981             new_bdm.create()
6982             return new_bdm
6983 
6984         return do_reserve()
6985 
6986     @wrap_exception()
6987     @wrap_instance_event(prefix='compute')
6988     @wrap_instance_fault
6989     def attach_volume(self, context, instance, bdm):
6990         """Attach a volume to an instance."""
6991         driver_bdm = driver_block_device.convert_volume(bdm)
6992 
6993         @utils.synchronized(instance.uuid)
6994         def do_attach_volume(context, instance, driver_bdm):
6995             try:
6996                 return self._attach_volume(context, instance, driver_bdm)
6997             except Exception:
6998                 with excutils.save_and_reraise_exception():
6999                     bdm.destroy()
7000 
7001         do_attach_volume(context, instance, driver_bdm)
7002 
7003     def _attach_volume(self, context, instance, bdm):
7004         context = context.elevated()
7005         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
7006                  {'volume_id': bdm.volume_id,
7007                   'mountpoint': bdm['mount_device']},
7008                  instance=instance)
7009         compute_utils.notify_about_volume_attach_detach(
7010             context, instance, self.host,
7011             action=fields.NotificationAction.VOLUME_ATTACH,
7012             phase=fields.NotificationPhase.START,
7013             volume_id=bdm.volume_id)
7014         try:
7015             bdm.attach(context, instance, self.volume_api, self.driver,
7016                        do_driver_attach=True)
7017         except Exception as e:
7018             with excutils.save_and_reraise_exception():
7019                 LOG.exception("Failed to attach %(volume_id)s "
7020                               "at %(mountpoint)s",
7021                               {'volume_id': bdm.volume_id,
7022                                'mountpoint': bdm['mount_device']},
7023                               instance=instance)
7024                 if bdm['attachment_id']:
7025                     # Try to delete the attachment to make the volume
7026                     # available again. Note that DriverVolumeBlockDevice
7027                     # may have already deleted the attachment so ignore
7028                     # VolumeAttachmentNotFound.
7029                     try:
7030                         self.volume_api.attachment_delete(
7031                             context, bdm['attachment_id'])
7032                     except exception.VolumeAttachmentNotFound as exc:
7033                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
7034                                   exc, instance=instance)
7035                 else:
7036                     self.volume_api.unreserve_volume(context, bdm.volume_id)
7037                 compute_utils.notify_about_volume_attach_detach(
7038                     context, instance, self.host,
7039                     action=fields.NotificationAction.VOLUME_ATTACH,
7040                     phase=fields.NotificationPhase.ERROR,
7041                     exception=e,
7042                     volume_id=bdm.volume_id)
7043 
7044         info = {'volume_id': bdm.volume_id}
7045         self._notify_about_instance_usage(
7046             context, instance, "volume.attach", extra_usage_info=info)
7047         compute_utils.notify_about_volume_attach_detach(
7048             context, instance, self.host,
7049             action=fields.NotificationAction.VOLUME_ATTACH,
7050             phase=fields.NotificationPhase.END,
7051             volume_id=bdm.volume_id)
7052 
7053     def _notify_volume_usage_detach(self, context, instance, bdm):
7054         if CONF.volume_usage_poll_interval <= 0:
7055             return
7056 
7057         mp = bdm.device_name
7058         # Handle bootable volumes which will not contain /dev/
7059         if '/dev/' in mp:
7060             mp = mp[5:]
7061         try:
7062             vol_stats = self.driver.block_stats(instance, mp)
7063             if vol_stats is None:
7064                 return
7065         except NotImplementedError:
7066             return
7067 
7068         LOG.debug("Updating volume usage cache with totals", instance=instance)
7069         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
7070         vol_usage = objects.VolumeUsage(context)
7071         vol_usage.volume_id = bdm.volume_id
7072         vol_usage.instance_uuid = instance.uuid
7073         vol_usage.project_id = instance.project_id
7074         vol_usage.user_id = instance.user_id
7075         vol_usage.availability_zone = instance.availability_zone
7076         vol_usage.curr_reads = rd_req
7077         vol_usage.curr_read_bytes = rd_bytes
7078         vol_usage.curr_writes = wr_req
7079         vol_usage.curr_write_bytes = wr_bytes
7080         vol_usage.save(update_totals=True)
7081         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
7082         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
7083 
7084     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
7085                        attachment_id=None):
7086         """Detach a volume from an instance.
7087 
7088         :param context: security context
7089         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
7090         :param instance: the Instance object to detach the volume from
7091         :param destroy_bdm: if True, the corresponding BDM entry will be marked
7092                             as deleted. Disabling this is useful for operations
7093                             like rebuild, when we don't want to destroy BDM
7094         :param attachment_id: The volume attachment_id for the given instance
7095                               and volume.
7096         """
7097         volume_id = bdm.volume_id
7098         compute_utils.notify_about_volume_attach_detach(
7099             context, instance, self.host,
7100             action=fields.NotificationAction.VOLUME_DETACH,
7101             phase=fields.NotificationPhase.START,
7102             volume_id=volume_id)
7103 
7104         self._notify_volume_usage_detach(context, instance, bdm)
7105 
7106         LOG.info('Detaching volume %(volume_id)s',
7107                  {'volume_id': volume_id}, instance=instance)
7108 
7109         driver_bdm = driver_block_device.convert_volume(bdm)
7110         driver_bdm.detach(context, instance, self.volume_api, self.driver,
7111                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
7112 
7113         info = dict(volume_id=volume_id)
7114         self._notify_about_instance_usage(
7115             context, instance, "volume.detach", extra_usage_info=info)
7116         compute_utils.notify_about_volume_attach_detach(
7117             context, instance, self.host,
7118             action=fields.NotificationAction.VOLUME_DETACH,
7119             phase=fields.NotificationPhase.END,
7120             volume_id=volume_id)
7121 
7122         if 'tag' in bdm and bdm.tag:
7123             self._delete_disk_metadata(instance, bdm)
7124         if destroy_bdm:
7125             bdm.destroy()
7126 
7127     def _delete_disk_metadata(self, instance, bdm):
7128         for device in instance.device_metadata.devices:
7129             if isinstance(device, objects.DiskMetadata):
7130                 if 'serial' in device:
7131                     if device.serial == bdm.volume_id:
7132                         instance.device_metadata.devices.remove(device)
7133                         instance.save()
7134                         break
7135                 else:
7136                     # NOTE(artom) We log the entire device object because all
7137                     # fields are nullable and may not be set
7138                     LOG.warning('Unable to determine whether to clean up '
7139                                 'device metadata for disk %s', device,
7140                                 instance=instance)
7141 
7142     @wrap_exception()
7143     @wrap_instance_event(prefix='compute')
7144     @wrap_instance_fault
7145     def detach_volume(self, context, volume_id, instance, attachment_id):
7146         """Detach a volume from an instance.
7147 
7148         :param context: security context
7149         :param volume_id: the volume id
7150         :param instance: the Instance object to detach the volume from
7151         :param attachment_id: The volume attachment_id for the given instance
7152                               and volume.
7153 
7154         """
7155         @utils.synchronized(instance.uuid)
7156         def do_detach_volume(context, volume_id, instance, attachment_id):
7157             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7158                     context, volume_id, instance.uuid)
7159             self._detach_volume(context, bdm, instance,
7160                                 attachment_id=attachment_id)
7161 
7162         do_detach_volume(context, volume_id, instance, attachment_id)
7163 
7164     def _init_volume_connection(self, context, new_volume,
7165                                 old_volume_id, connector, bdm,
7166                                 new_attachment_id, mountpoint):
7167         new_volume_id = new_volume['id']
7168         if new_attachment_id is None:
7169             # We're dealing with an old-style attachment so initialize the
7170             # connection so we can get the connection_info.
7171             new_cinfo = self.volume_api.initialize_connection(context,
7172                                                               new_volume_id,
7173                                                               connector)
7174         else:
7175             # Check for multiattach on the new volume and if True, check to
7176             # see if the virt driver supports multiattach.
7177             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
7178             # and should be consolidated into some common code at some point.
7179             vol_multiattach = new_volume.get('multiattach', False)
7180             virt_multiattach = self.driver.capabilities.get(
7181                 'supports_multiattach', False)
7182             if vol_multiattach and not virt_multiattach:
7183                 raise exception.MultiattachNotSupportedByVirtDriver(
7184                     volume_id=new_volume_id)
7185 
7186             # This is a new style attachment and the API created the new
7187             # volume attachment and passed the id to the compute over RPC.
7188             # At this point we need to update the new volume attachment with
7189             # the host connector, which will give us back the new attachment
7190             # connection_info.
7191             new_cinfo = self.volume_api.attachment_update(
7192                 context, new_attachment_id, connector,
7193                 mountpoint)['connection_info']
7194 
7195             if vol_multiattach:
7196                 # This will be used by the volume driver to determine the
7197                 # proper disk configuration.
7198                 new_cinfo['multiattach'] = True
7199 
7200         old_cinfo = jsonutils.loads(bdm['connection_info'])
7201         if old_cinfo and 'serial' not in old_cinfo:
7202             old_cinfo['serial'] = old_volume_id
7203         # NOTE(lyarwood): serial is not always present in the returned
7204         # connection_info so set it if it is missing as we do in
7205         # DriverVolumeBlockDevice.attach().
7206         if 'serial' not in new_cinfo:
7207             new_cinfo['serial'] = new_volume_id
7208         return (old_cinfo, new_cinfo)
7209 
7210     def _swap_volume(self, context, instance, bdm, connector,
7211                      old_volume_id, new_volume, resize_to,
7212                      new_attachment_id, is_cinder_migration):
7213         new_volume_id = new_volume['id']
7214         mountpoint = bdm['device_name']
7215         failed = False
7216         new_cinfo = None
7217         try:
7218             old_cinfo, new_cinfo = self._init_volume_connection(
7219                 context, new_volume, old_volume_id, connector,
7220                 bdm, new_attachment_id, mountpoint)
7221             # NOTE(lyarwood): The Libvirt driver, the only virt driver
7222             # currently implementing swap_volume, will modify the contents of
7223             # new_cinfo when connect_volume is called. This is then saved to
7224             # the BDM in swap_volume for future use outside of this flow.
7225             msg = ("swap_volume: Calling driver volume swap with "
7226                    "connection infos: new: %(new_cinfo)s; "
7227                    "old: %(old_cinfo)s" %
7228                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
7229             # Both new and old info might contain password
7230             LOG.debug(strutils.mask_password(msg), instance=instance)
7231 
7232             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
7233                                     mountpoint, resize_to)
7234             if new_attachment_id:
7235                 self.volume_api.attachment_complete(context, new_attachment_id)
7236             msg = ("swap_volume: Driver volume swap returned, new "
7237                    "connection_info is now : %(new_cinfo)s" %
7238                    {'new_cinfo': new_cinfo})
7239             LOG.debug(strutils.mask_password(msg))
7240         except Exception as ex:
7241             failed = True
7242             with excutils.save_and_reraise_exception():
7243                 compute_utils.notify_about_volume_swap(
7244                     context, instance, self.host,
7245                     fields.NotificationPhase.ERROR,
7246                     old_volume_id, new_volume_id, ex)
7247                 if new_cinfo:
7248                     msg = ("Failed to swap volume %(old_volume_id)s "
7249                            "for %(new_volume_id)s")
7250                     LOG.exception(msg, {'old_volume_id': old_volume_id,
7251                                         'new_volume_id': new_volume_id},
7252                                   instance=instance)
7253                 else:
7254                     msg = ("Failed to connect to volume %(volume_id)s "
7255                            "with volume at %(mountpoint)s")
7256                     LOG.exception(msg, {'volume_id': new_volume_id,
7257                                         'mountpoint': bdm['device_name']},
7258                                   instance=instance)
7259 
7260                 # The API marked the volume as 'detaching' for the old volume
7261                 # so we need to roll that back so the volume goes back to
7262                 # 'in-use' state.
7263                 self.volume_api.roll_detaching(context, old_volume_id)
7264 
7265                 if new_attachment_id is None:
7266                     # The API reserved the new volume so it would be in
7267                     # 'attaching' status, so we need to unreserve it so it
7268                     # goes back to 'available' status.
7269                     self.volume_api.unreserve_volume(context, new_volume_id)
7270                 else:
7271                     # This is a new style attachment for the new volume, which
7272                     # was created in the API. We just need to delete it here
7273                     # to put the new volume back into 'available' status.
7274                     self.volume_api.attachment_delete(
7275                         context, new_attachment_id)
7276         finally:
7277             # TODO(mriedem): This finally block is terribly confusing and is
7278             # trying to do too much. We should consider removing the finally
7279             # block and move whatever needs to happen on success and failure
7280             # into the blocks above for clarity, even if it means a bit of
7281             # redundant code.
7282             conn_volume = new_volume_id if failed else old_volume_id
7283             if new_cinfo:
7284                 LOG.debug("swap_volume: removing Cinder connection "
7285                           "for volume %(volume)s", {'volume': conn_volume},
7286                           instance=instance)
7287                 if bdm.attachment_id is None:
7288                     # This is the pre-3.44 flow for new-style volume
7289                     # attachments so just terminate the connection.
7290                     self.volume_api.terminate_connection(context,
7291                                                          conn_volume,
7292                                                          connector)
7293                 else:
7294                     # This is a new style volume attachment. If we failed, then
7295                     # the new attachment was already deleted above in the
7296                     # exception block and we have nothing more to do here. If
7297                     # swap_volume was successful in the driver, then we need to
7298                     # "detach" the original attachment by deleting it.
7299                     if not failed:
7300                         self.volume_api.attachment_delete(
7301                             context, bdm.attachment_id)
7302 
7303             # Need to make some decisions based on whether this was
7304             # a Cinder initiated migration or not. The callback to
7305             # migration completion isn't needed in the case of a
7306             # nova initiated simple swap of two volume
7307             # "volume-update" call so skip that. The new attachment
7308             # scenarios will give us a new attachment record and
7309             # that's what we want.
7310             if bdm.attachment_id and not is_cinder_migration:
7311                 # we don't callback to cinder
7312                 comp_ret = {'save_volume_id': new_volume_id}
7313             else:
7314                 # NOTE(lyarwood): The following call to
7315                 # os-migrate-volume-completion returns a dict containing
7316                 # save_volume_id, this volume id has two possible values :
7317                 # 1. old_volume_id if we are migrating (retyping) volumes
7318                 # 2. new_volume_id if we are swapping between two existing
7319                 #    volumes
7320                 # This volume id is later used to update the volume_id and
7321                 # connection_info['serial'] of the BDM.
7322                 comp_ret = self.volume_api.migrate_volume_completion(
7323                                                           context,
7324                                                           old_volume_id,
7325                                                           new_volume_id,
7326                                                           error=failed)
7327                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
7328                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
7329                           instance=instance)
7330 
7331         return (comp_ret, new_cinfo)
7332 
7333     @wrap_exception()
7334     @wrap_instance_event(prefix='compute')
7335     @wrap_instance_fault
7336     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
7337                     new_attachment_id):
7338         """Replace the old volume with the new volume within the active server
7339 
7340         :param context: User request context
7341         :param old_volume_id: Original volume id
7342         :param new_volume_id: New volume id being swapped to
7343         :param instance: Instance with original_volume_id attached
7344         :param new_attachment_id: ID of the new attachment for new_volume_id
7345         """
7346         @utils.synchronized(instance.uuid)
7347         def _do_locked_swap_volume(context, old_volume_id, new_volume_id,
7348                                    instance, new_attachment_id):
7349             self._do_swap_volume(context, old_volume_id, new_volume_id,
7350                                  instance, new_attachment_id)
7351         _do_locked_swap_volume(context, old_volume_id, new_volume_id, instance,
7352                                new_attachment_id)
7353 
7354     def _do_swap_volume(self, context, old_volume_id, new_volume_id,
7355                         instance, new_attachment_id):
7356         """Replace the old volume with the new volume within the active server
7357 
7358         :param context: User request context
7359         :param old_volume_id: Original volume id
7360         :param new_volume_id: New volume id being swapped to
7361         :param instance: Instance with original_volume_id attached
7362         :param new_attachment_id: ID of the new attachment for new_volume_id
7363         """
7364         context = context.elevated()
7365         compute_utils.notify_about_volume_swap(
7366             context, instance, self.host,
7367             fields.NotificationPhase.START,
7368             old_volume_id, new_volume_id)
7369 
7370         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7371                 context, old_volume_id, instance.uuid)
7372         connector = self.driver.get_volume_connector(instance)
7373 
7374         resize_to = 0
7375         old_volume = self.volume_api.get(context, old_volume_id)
7376         # Yes this is a tightly-coupled state check of what's going on inside
7377         # cinder, but we need this while we still support old (v1/v2) and
7378         # new style attachments (v3.44). Once we drop support for old style
7379         # attachments we could think about cleaning up the cinder-initiated
7380         # swap volume API flows.
7381         is_cinder_migration = False
7382         if 'migration_status' in old_volume:
7383             is_cinder_migration = old_volume['migration_status'] == 'migrating'
7384         old_vol_size = old_volume['size']
7385         new_volume = self.volume_api.get(context, new_volume_id)
7386         new_vol_size = new_volume['size']
7387         if new_vol_size > old_vol_size:
7388             resize_to = new_vol_size
7389 
7390         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
7391                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
7392                  instance=instance)
7393         comp_ret, new_cinfo = self._swap_volume(context,
7394                                                 instance,
7395                                                 bdm,
7396                                                 connector,
7397                                                 old_volume_id,
7398                                                 new_volume,
7399                                                 resize_to,
7400                                                 new_attachment_id,
7401                                                 is_cinder_migration)
7402 
7403         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
7404         # correct volume_id returned by Cinder.
7405         save_volume_id = comp_ret['save_volume_id']
7406         new_cinfo['serial'] = save_volume_id
7407         values = {
7408             'connection_info': jsonutils.dumps(new_cinfo),
7409             'source_type': 'volume',
7410             'destination_type': 'volume',
7411             'snapshot_id': None,
7412             'volume_id': save_volume_id,
7413             'no_device': None}
7414 
7415         if resize_to:
7416             values['volume_size'] = resize_to
7417 
7418         if new_attachment_id is not None:
7419             # This was a volume swap for a new-style attachment so we
7420             # need to update the BDM attachment_id for the new attachment.
7421             values['attachment_id'] = new_attachment_id
7422 
7423         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
7424                   "%(updates)s", {'volume_id': bdm.volume_id,
7425                                   'updates': values},
7426                   instance=instance)
7427         bdm.update(values)
7428         bdm.save()
7429 
7430         compute_utils.notify_about_volume_swap(
7431             context, instance, self.host,
7432             fields.NotificationPhase.END,
7433             old_volume_id, new_volume_id)
7434 
7435     @wrap_exception()
7436     def remove_volume_connection(self, context, volume_id, instance):
7437         """Remove the volume connection on this host
7438 
7439         Detach the volume from this instance on this host, and if this is
7440         the cinder v2 flow, call cinder to terminate the connection.
7441         """
7442         try:
7443             # NOTE(mriedem): If the BDM was just passed directly we would not
7444             # need to do this DB query, but this is an RPC interface so
7445             # changing that requires some care.
7446             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7447                     context, volume_id, instance.uuid)
7448             # NOTE(mriedem): Normally we would pass delete_attachment=True to
7449             # _remove_volume_connection to delete a v3 style volume attachment,
7450             # but this method is RPC called from _rollback_live_migration which
7451             # already deletes the attachment, so because of that tight coupling
7452             # we cannot simply delete a v3 style attachment here without
7453             # needing to do some behavior modification of that
7454             # _rollback_live_migration flow which gets messy.
7455             self._remove_volume_connection(context, bdm, instance)
7456         except exception.NotFound:
7457             pass
7458 
7459     def _remove_volume_connection(self, context, bdm, instance,
7460                                   delete_attachment=False):
7461         """Remove the volume connection on this host
7462 
7463         Detach the volume from this instance on this host.
7464 
7465         :param context: nova auth request context
7466         :param bdm: BlockDeviceMapping object for a volume attached to the
7467             instance
7468         :param instance: Instance object with a volume attached represented
7469             by ``bdm``
7470         :param delete_attachment: If ``bdm.attachment_id`` is not None the
7471             attachment was made as a cinder v3 style attachment and if True,
7472             then deletes the volume attachment, otherwise just terminates
7473             the connection for a cinder legacy style connection.
7474         """
7475         driver_bdm = driver_block_device.convert_volume(bdm)
7476         driver_bdm.driver_detach(context, instance,
7477                                  self.volume_api, self.driver)
7478         if bdm.attachment_id is None:
7479             # cinder v2 api flow
7480             connector = self.driver.get_volume_connector(instance)
7481             self.volume_api.terminate_connection(context, bdm.volume_id,
7482                                                  connector)
7483         elif delete_attachment:
7484             # cinder v3 api flow
7485             self.volume_api.attachment_delete(context, bdm.attachment_id)
7486 
7487     def _deallocate_port_resource_for_instance(
7488         self,
7489         context: nova.context.RequestContext,
7490         instance: 'objects.Instance',
7491         port_id: str,
7492         port_allocation: ty.Dict[str, ty.Dict[str, ty.Dict[str, int]]],
7493     ) -> None:
7494 
7495         if not port_allocation:
7496             return
7497 
7498         try:
7499             client = self.reportclient
7500             client.remove_resources_from_instance_allocation(
7501                 context, instance.uuid, port_allocation)
7502         except Exception as ex:
7503             # We always raise here as it is not a race condition where
7504             # somebody has already deleted the port we want to cleanup.
7505             # Here we see that the port exists, the allocation exists,
7506             # but we cannot clean it up so we will actually leak
7507             # allocations.
7508             with excutils.save_and_reraise_exception():
7509                 LOG.warning(
7510                     'Failed to remove resource allocation of port %(port_id)s '
7511                     'for instance. Error: %(error)s',
7512                     {'port_id': port_id, 'error': ex},
7513                     instance=instance)
7514 
7515     def _deallocate_port_for_instance(
7516             self, context, instance, port_id, raise_on_failure=False,
7517             pci_device=None):
7518         try:
7519             result = self.network_api.deallocate_port_for_instance(
7520                 context, instance, port_id)
7521             __, port_allocation = result
7522         except Exception as ex:
7523             with excutils.save_and_reraise_exception(
7524                     reraise=raise_on_failure):
7525                 LOG.warning('Failed to deallocate port %(port_id)s '
7526                             'for instance. Error: %(error)s',
7527                             {'port_id': port_id, 'error': ex},
7528                             instance=instance)
7529         else:
7530             if pci_device:
7531                 self.rt.unclaim_pci_devices(context, pci_device, instance)
7532                 instance.remove_pci_device_and_request(pci_device)
7533 
7534             # Deallocate the resources in placement that were used by the
7535             # detached port.
7536             self._deallocate_port_resource_for_instance(
7537                 context, instance, port_id, port_allocation)
7538 
7539     def _claim_pci_device_for_interface_attach(
7540         self,
7541         context: nova.context.RequestContext,
7542         instance: 'objects.Instance',
7543         pci_reqs: 'objects.InstancePCIRequests',
7544     ) -> ty.Optional['objects.PciDevice']:
7545         """Claim PCI devices if there are PCI requests
7546 
7547         :param context: nova.context.RequestContext
7548         :param instance: the objects.Instance to where the interface is being
7549             attached
7550         :param pci_reqs: A InstancePCIRequests object describing the
7551             needed PCI devices
7552         :raises InterfaceAttachPciClaimFailed: if the PCI device claim fails
7553         :returns: An objects.PciDevice describing the claimed PCI device for
7554             the interface or None if no device is requested
7555         """
7556 
7557         if not pci_reqs.requests:
7558             return None
7559 
7560         devices = self.rt.claim_pci_devices(
7561             context, pci_reqs, instance.numa_topology)
7562 
7563         if not devices:
7564             LOG.info('Failed to claim PCI devices during interface attach '
7565                      'for PCI request %s', pci_reqs, instance=instance)
7566             raise exception.InterfaceAttachPciClaimFailed(
7567                 instance_uuid=instance.uuid)
7568 
7569         # NOTE(gibi): We assume that maximum one PCI devices is attached per
7570         # interface attach request.
7571         device = devices[0]
7572         instance.pci_devices.objects.append(device)
7573 
7574         return device
7575 
7576     def _allocate_port_resource_for_instance(
7577         self,
7578         context: nova.context.RequestContext,
7579         instance: 'objects.Instance',
7580         pci_reqs: 'objects.InstancePCIRequests',
7581         request_groups: ty.List['objects.RequestGroup'],
7582     ) -> ty.Tuple[ty.Optional[ty.Dict[str, ty.List[str]]],
7583                   ty.Optional[ty.Dict[str, ty.Dict[str, ty.Dict[str, int]]]]]:
7584         """Allocate resources for the request in placement
7585 
7586         :param context: nova.context.RequestContext
7587         :param instance: the objects.Instance to where the interface is being
7588             attached
7589         :param pci_reqs: A list of InstancePCIRequest objects describing the
7590             needed PCI devices
7591         :param request_groups: A list of RequestGroup objects describing the
7592             resources the port requests from placement
7593         :raises InterfaceAttachResourceAllocationFailed: if we failed to
7594             allocate resource in placement for the request
7595         :returns: A tuple of provider mappings and allocated resources or
7596             (None, None) if no resource allocation was needed for the request
7597         """
7598 
7599         if not request_groups:
7600             return None, None
7601 
7602         request_group = request_groups[0]
7603 
7604         # restrict the resource request to the current compute node. The
7605         # compute node uuid is the uuid of the root provider of the node in
7606         # placement
7607         compute_node_uuid = objects.ComputeNode.get_by_nodename(
7608             context, instance.node).uuid
7609         request_group.in_tree = compute_node_uuid
7610 
7611         # NOTE(gibi): when support is added for attaching a cyborg based
7612         # smart NIC the ResourceRequest could be extended to handle multiple
7613         # request groups.
7614         rr = scheduler_utils.ResourceRequest.from_request_group(request_group)
7615         res = self.reportclient.get_allocation_candidates(context, rr)
7616         alloc_reqs, provider_sums, version = res
7617 
7618         if not alloc_reqs:
7619             # no allocation candidates available, we run out of free resources
7620             raise exception.InterfaceAttachResourceAllocationFailed(
7621                 instance_uuid=instance.uuid)
7622 
7623         # select one of the candidates and update the instance
7624         # allocation
7625         # TODO(gibi): We could loop over all possible candidates
7626         # if the first one selected here does not work due to race or due
7627         # to not having free PCI devices. However the latter is only
7628         # detected later in the interface attach code path.
7629         alloc_req = alloc_reqs[0]
7630         resources = alloc_req['allocations']
7631         provider_mappings = alloc_req['mappings']
7632         try:
7633             self.reportclient.add_resources_to_instance_allocation(
7634                 context, instance.uuid, resources)
7635         except exception.AllocationUpdateFailed as e:
7636             # We lost a race. We could retry another candidate
7637             raise exception.InterfaceAttachResourceAllocationFailed(
7638                 instance_uuid=instance.uuid) from e
7639         except (
7640             exception.ConsumerAllocationRetrievalFailed,
7641             keystone_exception.ClientException,
7642         ) as e:
7643             # These are non-recoverable errors so we should not retry
7644             raise exception.InterfaceAttachResourceAllocationFailed(
7645                 instance_uuid=instance.uuid) from e
7646 
7647         try:
7648             update = (
7649                 compute_utils.
7650                     update_pci_request_spec_with_allocated_interface_name)
7651             update(
7652                 context, self.reportclient, pci_reqs.requests,
7653                 provider_mappings)
7654         except (
7655             exception.AmbiguousResourceProviderForPCIRequest,
7656             exception.UnexpectedResourceProviderNameForPCIRequest
7657         ):
7658             # These are programing errors. So we clean up an re-raise to let
7659             # the request fail
7660             with excutils.save_and_reraise_exception():
7661                 self.reportclient.remove_resources_from_instance_allocation(
7662                     context, instance.uuid, resources)
7663 
7664         return provider_mappings, resources
7665 
7666     # TODO(mriedem): There are likely race failures which can result in
7667     # NotFound and QuotaError exceptions getting traced as well.
7668     @messaging.expected_exceptions(
7669         # Do not log a traceback for user errors. We use Invalid generically
7670         # since this method can raise lots of different exceptions:
7671         # AttachInterfaceNotSupported
7672         # NetworkInterfaceTaggedAttachNotSupported
7673         # NetworkAmbiguous
7674         # PortNotUsable
7675         # PortInUse
7676         # PortNotUsableDNS
7677         # AttachSRIOVPortNotSupported
7678         # NetworksWithQoSPolicyNotSupported
7679         # InterfaceAttachResourceAllocationFailed
7680         exception.Invalid)
7681     @wrap_exception()
7682     @wrap_instance_event(prefix='compute')
7683     @wrap_instance_fault
7684     def attach_interface(self, context, instance, network_id, port_id,
7685                          requested_ip, tag):
7686         """Use hotplug to add an network adapter to an instance."""
7687         lockname = 'interface-%s-%s' % (instance.uuid, port_id)
7688 
7689         @utils.synchronized(lockname)
7690         def do_attach_interface(context, instance, network_id, port_id,
7691                                 requested_ip, tag):
7692             return self._attach_interface(context, instance, network_id,
7693                                 port_id, requested_ip, tag)
7694 
7695         return do_attach_interface(context, instance, network_id, port_id,
7696                                    requested_ip, tag)
7697 
7698     def _attach_interface(self, context, instance, network_id, port_id,
7699                           requested_ip, tag):
7700         if not self.driver.capabilities.get('supports_attach_interface',
7701                                             False):
7702             raise exception.AttachInterfaceNotSupported(
7703                 instance_uuid=instance.uuid)
7704         if (tag and not
7705             self.driver.capabilities.get('supports_tagged_attach_interface',
7706                                          False)):
7707             raise exception.NetworkInterfaceTaggedAttachNotSupported()
7708 
7709         compute_utils.notify_about_instance_action(
7710             context, instance, self.host,
7711             action=fields.NotificationAction.INTERFACE_ATTACH,
7712             phase=fields.NotificationPhase.START)
7713 
7714         bind_host_id = self.driver.network_binding_host_id(context, instance)
7715 
7716         requested_networks = objects.NetworkRequestList(
7717             objects=[
7718                 objects.NetworkRequest(
7719                     network_id=network_id,
7720                     port_id=port_id,
7721                     address=requested_ip,
7722                     tag=tag,
7723                 )
7724             ]
7725         )
7726 
7727         if len(requested_networks) != 1:
7728             LOG.warning(
7729                 "Interface attach only supports one interface per attach "
7730                 "request", instance=instance)
7731             raise exception.InterfaceAttachFailed(instance_uuid=instance.uuid)
7732 
7733         pci_numa_affinity_policy = hardware.get_pci_numa_policy_constraint(
7734             instance.flavor, instance.image_meta)
7735         pci_reqs = objects.InstancePCIRequests(
7736             requests=[], instance_uuid=instance.uuid)
7737         _, request_groups = self.network_api.create_resource_requests(
7738             context, requested_networks, pci_reqs,
7739             affinity_policy=pci_numa_affinity_policy)
7740 
7741         # We only support one port per attach request so we at most have one
7742         # pci request
7743         if pci_reqs.requests:
7744             pci_req = pci_reqs.requests[0]
7745             requested_networks[0].pci_request_id = pci_req.request_id
7746 
7747         result = self._allocate_port_resource_for_instance(
7748             context, instance, pci_reqs, request_groups)
7749         provider_mappings, resources = result
7750 
7751         try:
7752             pci_device = self._claim_pci_device_for_interface_attach(
7753                 context, instance, pci_reqs)
7754         except exception.InterfaceAttachPciClaimFailed:
7755             with excutils.save_and_reraise_exception():
7756                 if resources:
7757                     # TODO(gibi): Instead of giving up we could try another
7758                     # allocation candidate from _allocate_resources() if any
7759                     self._deallocate_port_resource_for_instance(
7760                         context, instance, port_id, resources)
7761 
7762         instance.pci_requests.requests.extend(pci_reqs.requests)
7763 
7764         network_info = self.network_api.allocate_for_instance(
7765             context,
7766             instance,
7767             requested_networks,
7768             bind_host_id=bind_host_id,
7769             resource_provider_mapping=provider_mappings,
7770         )
7771 
7772         if len(network_info) != 1:
7773             LOG.error('allocate_for_instance returned %(ports)s '
7774                       'ports', {'ports': len(network_info)})
7775             # TODO(elod.illes): an instance.interface_attach.error notification
7776             # should be sent here
7777             raise exception.InterfaceAttachFailed(
7778                     instance_uuid=instance.uuid)
7779         image_meta = objects.ImageMeta.from_instance(instance)
7780 
7781         try:
7782             self.driver.attach_interface(context, instance, image_meta,
7783                                          network_info[0])
7784         except exception.NovaException as ex:
7785             port_id = network_info[0].get('id')
7786             LOG.warning("attach interface failed , try to deallocate "
7787                         "port %(port_id)s, reason: %(msg)s",
7788                         {'port_id': port_id, 'msg': ex},
7789                         instance=instance)
7790             self._deallocate_port_for_instance(
7791                 context, instance, port_id, pci_device=pci_device)
7792 
7793             compute_utils.notify_about_instance_action(
7794                 context, instance, self.host,
7795                 action=fields.NotificationAction.INTERFACE_ATTACH,
7796                 phase=fields.NotificationPhase.ERROR,
7797                 exception=ex)
7798 
7799             raise exception.InterfaceAttachFailed(
7800                 instance_uuid=instance.uuid)
7801 
7802         if pci_device:
7803             # NOTE(gibi): The _claim_pci_device_for_interface_attach() call
7804             # found a pci device but it only marked the device as claimed. The
7805             # periodic update_available_resource would move the device to
7806             # allocated state. But as driver.attach_interface() has been
7807             # succeeded we now know that the interface is also allocated
7808             # (used by) to the instance. So make sure the pci tracker also
7809             # tracks this device as allocated. This way we can avoid a possible
7810             # race condition when a detach arrives for a device that is only
7811             # in claimed state.
7812             self.rt.allocate_pci_devices_for_instance(context, instance)
7813 
7814         instance.save()
7815 
7816         compute_utils.notify_about_instance_action(
7817             context, instance, self.host,
7818             action=fields.NotificationAction.INTERFACE_ATTACH,
7819             phase=fields.NotificationPhase.END)
7820 
7821         return network_info[0]
7822 
7823     @wrap_exception()
7824     @wrap_instance_event(prefix='compute')
7825     @wrap_instance_fault
7826     def detach_interface(self, context, instance, port_id):
7827         """Detach a network adapter from an instance."""
7828         lockname = 'interface-%s-%s' % (instance.uuid, port_id)
7829 
7830         @utils.synchronized(lockname)
7831         def do_detach_interface(context, instance, port_id):
7832             self._detach_interface(context, instance, port_id)
7833 
7834         do_detach_interface(context, instance, port_id)
7835 
7836     def _detach_interface(self, context, instance, port_id):
7837         # NOTE(aarents): we need to refresh info cache from DB here,
7838         # as previous detach/attach lock holder just updated it.
7839         compute_utils.refresh_info_cache_for_instance(context, instance)
7840         network_info = instance.info_cache.network_info
7841         condemned = None
7842         for vif in network_info:
7843             if vif['id'] == port_id:
7844                 condemned = vif
7845                 break
7846         if condemned is None:
7847             raise exception.PortNotFound(_("Port %s is not "
7848                                            "attached") % port_id)
7849 
7850         pci_req = pci_req_module.get_instance_pci_request_from_vif(
7851             context, instance, condemned)
7852 
7853         pci_device = None
7854         if pci_req:
7855             pci_devices = [pci_device
7856                            for pci_device in instance.pci_devices.objects
7857                            if pci_device.request_id == pci_req.request_id]
7858 
7859             if not pci_devices:
7860                 LOG.warning(
7861                     "Detach interface failed, port_id=%(port_id)s, "
7862                     "reason: PCI device not found for PCI request %(pci_req)s",
7863                     {'port_id': port_id, 'pci_req': pci_req})
7864                 raise exception.InterfaceDetachFailed(
7865                     instance_uuid=instance.uuid)
7866 
7867             pci_device = pci_devices[0]
7868 
7869         compute_utils.notify_about_instance_action(
7870             context, instance, self.host,
7871             action=fields.NotificationAction.INTERFACE_DETACH,
7872             phase=fields.NotificationPhase.START)
7873 
7874         try:
7875             self.driver.detach_interface(context, instance, condemned)
7876         except exception.NovaException as ex:
7877             # If the instance was deleted before the interface was detached,
7878             # just log it at debug.
7879             log_level = (logging.DEBUG
7880                          if isinstance(ex, exception.InstanceNotFound)
7881                          else logging.WARNING)
7882             LOG.log(log_level,
7883                     "Detach interface failed, port_id=%(port_id)s, reason: "
7884                     "%(msg)s", {'port_id': port_id, 'msg': ex},
7885                     instance=instance)
7886             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
7887         else:
7888             self._deallocate_port_for_instance(
7889                 context, instance, port_id, raise_on_failure=True,
7890                 pci_device=pci_device)
7891 
7892         instance.save()
7893 
7894         compute_utils.notify_about_instance_action(
7895             context, instance, self.host,
7896             action=fields.NotificationAction.INTERFACE_DETACH,
7897             phase=fields.NotificationPhase.END)
7898 
7899     def _get_compute_info(self, context, host):
7900         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
7901             context, host)
7902 
7903     # TODO(stephenfin): Remove the unused instance argument in RPC version 6.0
7904     @wrap_exception()
7905     def check_instance_shared_storage(self, ctxt, instance, data):
7906         """Check if the instance files are shared
7907 
7908         :param ctxt: security context
7909         :param instance: dict of instance data
7910         :param data: result of driver.check_instance_shared_storage_local
7911 
7912         Returns True if instance disks located on shared storage and
7913         False otherwise.
7914         """
7915         return self.driver.check_instance_shared_storage_remote(ctxt, data)
7916 
7917     def _dest_can_numa_live_migrate(self, dest_check_data, migration):
7918         # TODO(artom) If we have a libvirt driver we expect it to set
7919         # dst_supports_numa_live_migration, but we have to remove it if we
7920         # did not get a migration from the conductor, indicating that it
7921         # cannot send RPC 5.3. This check can be removed in RPC 6.0.
7922         if ('dst_supports_numa_live_migration' in dest_check_data and
7923                 dest_check_data.dst_supports_numa_live_migration and
7924                 not migration):
7925             delattr(dest_check_data, 'dst_supports_numa_live_migration')
7926         return dest_check_data
7927 
7928     @wrap_exception()
7929     @wrap_instance_event(prefix='compute')
7930     @wrap_instance_fault
7931     def check_can_live_migrate_destination(self, ctxt, instance,
7932                                            block_migration, disk_over_commit,
7933                                            migration=None, limits=None):
7934         """Check if it is possible to execute live migration.
7935 
7936         This runs checks on the destination host, and then calls
7937         back to the source host to check the results.
7938 
7939         :param context: security context
7940         :param instance: dict of instance data
7941         :param block_migration: if true, prepare for block migration
7942                                 if None, calculate it in driver
7943         :param disk_over_commit: if true, allow disk over commit
7944                                  if None, ignore disk usage checking
7945         :param migration: objects.Migration object for this live migration.
7946         :param limits: objects.SchedulerLimits object for this live migration.
7947         :returns: a LiveMigrateData object (hypervisor-dependent)
7948         """
7949         src_compute_info = obj_base.obj_to_primitive(
7950             self._get_compute_info(ctxt, instance.host))
7951         dst_compute_info = obj_base.obj_to_primitive(
7952             self._get_compute_info(ctxt, self.host))
7953         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
7954             instance, src_compute_info, dst_compute_info,
7955             block_migration, disk_over_commit)
7956         dest_check_data = self._dest_can_numa_live_migrate(dest_check_data,
7957                                                            migration)
7958         LOG.debug('destination check data is %s', dest_check_data)
7959         try:
7960             allocs = self.reportclient.get_allocations_for_consumer(
7961                 ctxt, instance.uuid)
7962             migrate_data = self.compute_rpcapi.check_can_live_migrate_source(
7963                 ctxt, instance, dest_check_data)
7964             if ('src_supports_numa_live_migration' in migrate_data and
7965                     migrate_data.src_supports_numa_live_migration):
7966                 migrate_data = self._live_migration_claim(
7967                     ctxt, instance, migrate_data, migration, limits, allocs)
7968             elif 'dst_supports_numa_live_migration' in dest_check_data:
7969                 LOG.info('Destination was ready for NUMA live migration, '
7970                          'but source is either too old, or is set to an '
7971                          'older upgrade level.', instance=instance)
7972             if self.network_api.supports_port_binding_extension(ctxt):
7973                 # Create migrate_data vifs
7974                 migrate_data.vifs = \
7975                     migrate_data_obj.\
7976                     VIFMigrateData.create_skeleton_migrate_vifs(
7977                         instance.get_network_info())
7978                 # Claim PCI devices for VIFs on destination (if needed)
7979                 port_id_to_pci = self._claim_pci_for_instance_vifs(
7980                     ctxt, instance)
7981                 # Update migrate VIFs with the newly claimed PCI devices
7982                 self._update_migrate_vifs_profile_with_pci(
7983                     migrate_data.vifs, port_id_to_pci)
7984         finally:
7985             self.driver.cleanup_live_migration_destination_check(ctxt,
7986                     dest_check_data)
7987         return migrate_data
7988 
7989     def _live_migration_claim(self, ctxt, instance, migrate_data,
7990                               migration, limits, allocs):
7991         """Runs on the destination and does a resources claim, if necessary.
7992         Currently, only NUMA live migrations require it.
7993 
7994         :param ctxt: Request context
7995         :param instance: The Instance being live migrated
7996         :param migrate_data: The MigrateData object for this live migration
7997         :param migration: The Migration object for this live migration
7998         :param limits: The SchedulerLimits object for this live migration
7999         :returns: migrate_data with dst_numa_info set if necessary
8000         """
8001         try:
8002             # NOTE(artom) We might have gotten here from _find_destination() in
8003             # the conductor live migrate task. At that point,
8004             # migration.dest_node is not set yet (nor should it be, we're still
8005             # looking for a destination, after all). Therefore, we cannot use
8006             # migration.dest_node here and must use self._get_nodename().
8007             claim = self.rt.live_migration_claim(
8008                 ctxt, instance, self._get_nodename(instance), migration,
8009                 limits, allocs)
8010             LOG.debug('Created live migration claim.', instance=instance)
8011         except exception.ComputeResourcesUnavailable as e:
8012             raise exception.MigrationPreCheckError(
8013                 reason=e.format_message())
8014         return self.driver.post_claim_migrate_data(ctxt, instance,
8015                                                    migrate_data, claim)
8016 
8017     def _source_can_numa_live_migrate(self, ctxt, dest_check_data,
8018                                       source_check_data):
8019         # TODO(artom) Our virt driver may have told us that it supports NUMA
8020         # live migration. However, the following other conditions must be met
8021         # for a NUMA live migration to happen:
8022         # 1. We got a True dst_supports_numa_live_migration in
8023         #    dest_check_data, indicating that the dest virt driver supports
8024         #    NUMA live migration and that the conductor can send RPC 5.3 and
8025         #    that the destination compute manager can receive it.
8026         # 2. Ourselves, the source, can send RPC 5.3. There's no
8027         #    sentinel/parameter for this, so we just ask our rpcapi directly.
8028         # If any of these are not met, we need to remove the
8029         # src_supports_numa_live_migration flag from source_check_data to avoid
8030         # incorrectly initiating a NUMA live migration.
8031         # All of this can be removed in RPC 6.0/objects 2.0.
8032         can_numa_live_migrate = (
8033             'dst_supports_numa_live_migration' in dest_check_data and
8034             dest_check_data.dst_supports_numa_live_migration and
8035             self.compute_rpcapi.supports_numa_live_migration(ctxt))
8036         if ('src_supports_numa_live_migration' in source_check_data and
8037                 source_check_data.src_supports_numa_live_migration and
8038                 not can_numa_live_migrate):
8039             delattr(source_check_data, 'src_supports_numa_live_migration')
8040         return source_check_data
8041 
8042     @wrap_exception()
8043     @wrap_instance_event(prefix='compute')
8044     @wrap_instance_fault
8045     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
8046         """Check if it is possible to execute live migration.
8047 
8048         This checks if the live migration can succeed, based on the
8049         results from check_can_live_migrate_destination.
8050 
8051         :param ctxt: security context
8052         :param instance: dict of instance data
8053         :param dest_check_data: result of check_can_live_migrate_destination
8054         :returns: a LiveMigrateData object
8055         """
8056         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8057             ctxt, instance.uuid)
8058         is_volume_backed = compute_utils.is_volume_backed_instance(
8059             ctxt, instance, bdms)
8060         dest_check_data.is_volume_backed = is_volume_backed
8061         block_device_info = self._get_instance_block_device_info(
8062                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
8063         result = self.driver.check_can_live_migrate_source(ctxt, instance,
8064                                                            dest_check_data,
8065                                                            block_device_info)
8066         result = self._source_can_numa_live_migrate(ctxt, dest_check_data,
8067                                                     result)
8068         LOG.debug('source check data is %s', result)
8069         return result
8070 
8071     # TODO(mriedem): Remove the block_migration argument in v6.0 of the compute
8072     # RPC API.
8073     @wrap_exception()
8074     @wrap_instance_event(prefix='compute')
8075     @wrap_instance_fault
8076     def pre_live_migration(self, context, instance, block_migration, disk,
8077                            migrate_data):
8078         """Preparations for live migration at dest host.
8079 
8080         :param context: security context
8081         :param instance: dict of instance data
8082         :param block_migration: if true, prepare for block migration
8083         :param disk: disk info of instance
8084         :param migrate_data: A dict or LiveMigrateData object holding data
8085                              required for live migration without shared
8086                              storage.
8087         :returns: migrate_data containing additional migration info
8088         """
8089         LOG.debug('pre_live_migration data is %s', migrate_data)
8090 
8091         migrate_data.old_vol_attachment_ids = {}
8092         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8093             context, instance.uuid)
8094         network_info = self.network_api.get_instance_nw_info(context, instance)
8095         self._notify_about_instance_usage(
8096             context, instance, "live_migration.pre.start",
8097             network_info=network_info)
8098         compute_utils.notify_about_instance_action(
8099             context, instance, self.host,
8100             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
8101             phase=fields.NotificationPhase.START, bdms=bdms)
8102 
8103         connector = self.driver.get_volume_connector(instance)
8104         try:
8105             for bdm in bdms:
8106                 if bdm.is_volume and bdm.attachment_id is not None:
8107                     # This bdm uses the new cinder v3.44 API.
8108                     # We will create a new attachment for this
8109                     # volume on this migration destination host. The old
8110                     # attachment will be deleted on the source host
8111                     # when the migration succeeds. The old attachment_id
8112                     # is stored in dict with the key being the bdm.volume_id
8113                     # so it can be restored on rollback.
8114                     #
8115                     # Also note that attachment_update is not needed as we
8116                     # are providing the connector in the create call.
8117                     attach_ref = self.volume_api.attachment_create(
8118                         context, bdm.volume_id, bdm.instance_uuid,
8119                         connector=connector, mountpoint=bdm.device_name)
8120 
8121                     # save current attachment so we can detach it on success,
8122                     # or restore it on a rollback.
8123                     # NOTE(mdbooth): This data is no longer used by the source
8124                     # host since change Ibe9215c0. We can't remove it until we
8125                     # are sure the source host has been upgraded.
8126                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
8127                         bdm.attachment_id
8128 
8129                     # update the bdm with the new attachment_id.
8130                     bdm.attachment_id = attach_ref['id']
8131                     bdm.save()
8132 
8133             block_device_info = self._get_instance_block_device_info(
8134                                 context, instance, refresh_conn_info=True,
8135                                 bdms=bdms)
8136 
8137             # The driver pre_live_migration will plug vifs on the host
8138             migrate_data = self.driver.pre_live_migration(context,
8139                                            instance,
8140                                            block_device_info,
8141                                            network_info,
8142                                            disk,
8143                                            migrate_data)
8144             LOG.debug('driver pre_live_migration data is %s', migrate_data)
8145             # driver.pre_live_migration is what plugs vifs on the destination
8146             # host so now we can set the wait_for_vif_plugged flag in the
8147             # migrate_data object which the source compute will use to
8148             # determine if it should wait for a 'network-vif-plugged' event
8149             # from neutron before starting the actual guest transfer in the
8150             # hypervisor
8151             using_multiple_port_bindings = (
8152                 'vifs' in migrate_data and migrate_data.vifs)
8153             migrate_data.wait_for_vif_plugged = (
8154                 CONF.compute.live_migration_wait_for_vif_plug and
8155                 using_multiple_port_bindings
8156             )
8157 
8158             # NOTE(tr3buchet): setup networks on destination host
8159             self.network_api.setup_networks_on_host(context, instance,
8160                                                              self.host)
8161 
8162         except Exception:
8163             # If we raise, migrate_data with the updated attachment ids
8164             # will not be returned to the source host for rollback.
8165             # So we need to rollback new attachments here.
8166             with excutils.save_and_reraise_exception():
8167                 old_attachments = migrate_data.old_vol_attachment_ids
8168                 for bdm in bdms:
8169                     if (bdm.is_volume and bdm.attachment_id is not None and
8170                             bdm.volume_id in old_attachments):
8171                         self.volume_api.attachment_delete(context,
8172                                                           bdm.attachment_id)
8173                         bdm.attachment_id = old_attachments[bdm.volume_id]
8174                         bdm.save()
8175 
8176         # Volume connections are complete, tell cinder that all the
8177         # attachments have completed.
8178         for bdm in bdms:
8179             if bdm.is_volume and bdm.attachment_id is not None:
8180                 self.volume_api.attachment_complete(context,
8181                                                     bdm.attachment_id)
8182 
8183         self._notify_about_instance_usage(
8184                      context, instance, "live_migration.pre.end",
8185                      network_info=network_info)
8186         compute_utils.notify_about_instance_action(
8187             context, instance, self.host,
8188             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
8189             phase=fields.NotificationPhase.END, bdms=bdms)
8190 
8191         LOG.debug('pre_live_migration result data is %s', migrate_data)
8192         return migrate_data
8193 
8194     @staticmethod
8195     def _neutron_failed_migration_callback(event_name, instance):
8196         msg = ('Neutron reported failure during migration '
8197                'with %(event)s for instance %(uuid)s')
8198         msg_args = {'event': event_name, 'uuid': instance.uuid}
8199         if CONF.vif_plugging_is_fatal:
8200             raise exception.VirtualInterfacePlugException(msg % msg_args)
8201         LOG.error(msg, msg_args)
8202 
8203     @staticmethod
8204     def _get_neutron_events_for_live_migration(instance):
8205         # We don't generate events if CONF.vif_plugging_timeout=0
8206         # meaning that the operator disabled using them.
8207         if CONF.vif_plugging_timeout:
8208             return (instance.get_network_info()
8209                     .get_live_migration_plug_time_events())
8210         else:
8211             return []
8212 
8213     def _cleanup_pre_live_migration(self, context, dest, instance,
8214                                     migration, migrate_data, source_bdms):
8215         """Helper method for when pre_live_migration fails
8216 
8217         Sets the migration status to "error" and rolls back the live migration
8218         setup on the destination host.
8219 
8220         :param context: The user request context.
8221         :type context: nova.context.RequestContext
8222         :param dest: The live migration destination hostname.
8223         :type dest: str
8224         :param instance: The instance being live migrated.
8225         :type instance: nova.objects.Instance
8226         :param migration: The migration record tracking this live migration.
8227         :type migration: nova.objects.Migration
8228         :param migrate_data: Data about the live migration, populated from
8229                              the destination host.
8230         :type migrate_data: Subclass of nova.objects.LiveMigrateData
8231         :param source_bdms: BDMs prior to modification by the destination
8232                             compute host. Set by _do_live_migration and not
8233                             part of the callback interface, so this is never
8234                             None
8235         """
8236         self._set_migration_status(migration, 'error')
8237         # Make sure we set this for _rollback_live_migration()
8238         # so it can find it, as expected if it was called later
8239         migrate_data.migration = migration
8240         self._rollback_live_migration(context, instance, dest,
8241                                       migrate_data=migrate_data,
8242                                       source_bdms=source_bdms)
8243 
8244     def _do_pre_live_migration_from_source(self, context, dest, instance,
8245                                            block_migration, migration,
8246                                            migrate_data, source_bdms):
8247         """Prepares for pre-live-migration on the source host and calls dest
8248 
8249         Will setup a callback networking event handler (if configured) and
8250         then call the dest host's pre_live_migration method to prepare the
8251         dest host for live migration (plugs vifs, connect volumes, etc).
8252 
8253         _rollback_live_migration (on the source) will be called if
8254         pre_live_migration (on the dest) fails.
8255 
8256         :param context: nova auth request context for this operation
8257         :param dest: name of the destination compute service host
8258         :param instance: Instance object being live migrated
8259         :param block_migration: If true, prepare for block migration.
8260         :param migration: Migration object tracking this operation
8261         :param migrate_data: MigrateData object for this operation populated
8262             by the destination host compute driver as part of the
8263             check_can_live_migrate_destination call.
8264         :param source_bdms: BlockDeviceMappingList of BDMs currently attached
8265             to the instance from the source host.
8266         :returns: MigrateData object which is a modified version of the
8267             ``migrate_data`` argument from the compute driver on the dest
8268             host during the ``pre_live_migration`` call.
8269         :raises: MigrationError if waiting for the network-vif-plugged event
8270             timed out and is fatal.
8271         """
8272         class _BreakWaitForInstanceEvent(Exception):
8273             """Used as a signal to stop waiting for the network-vif-plugged
8274             event when we discover that
8275             [compute]/live_migration_wait_for_vif_plug is not set on the
8276             destination.
8277             """
8278             pass
8279 
8280         events = self._get_neutron_events_for_live_migration(instance)
8281         try:
8282             if ('block_migration' in migrate_data and
8283                     migrate_data.block_migration):
8284                 block_device_info = self._get_instance_block_device_info(
8285                     context, instance, bdms=source_bdms)
8286                 disk = self.driver.get_instance_disk_info(
8287                     instance, block_device_info=block_device_info)
8288             else:
8289                 disk = None
8290 
8291             deadline = CONF.vif_plugging_timeout
8292             error_cb = self._neutron_failed_migration_callback
8293             # In order to avoid a race with the vif plugging that the virt
8294             # driver does on the destination host, we register our events
8295             # to wait for before calling pre_live_migration. Then if the
8296             # dest host reports back that we shouldn't wait, we can break
8297             # out of the context manager using _BreakWaitForInstanceEvent.
8298             with self.virtapi.wait_for_instance_event(
8299                     instance, events, deadline=deadline,
8300                     error_callback=error_cb):
8301                 with timeutils.StopWatch() as timer:
8302                     # TODO(mriedem): The "block_migration" parameter passed
8303                     # here is not actually used in pre_live_migration but it
8304                     # is not optional in the RPC interface either.
8305                     migrate_data = self.compute_rpcapi.pre_live_migration(
8306                         context, instance,
8307                         block_migration, disk, dest, migrate_data)
8308                 LOG.info('Took %0.2f seconds for pre_live_migration on '
8309                          'destination host %s.',
8310                          timer.elapsed(), dest, instance=instance)
8311                 wait_for_vif_plugged = (
8312                     'wait_for_vif_plugged' in migrate_data and
8313                     migrate_data.wait_for_vif_plugged)
8314                 if events and not wait_for_vif_plugged:
8315                     raise _BreakWaitForInstanceEvent
8316         except _BreakWaitForInstanceEvent:
8317             if events:
8318                 LOG.debug('Not waiting for events after pre_live_migration: '
8319                           '%s. ', events, instance=instance)
8320         except exception.VirtualInterfacePlugException:
8321             with excutils.save_and_reraise_exception():
8322                 LOG.exception('Failed waiting for network virtual interfaces '
8323                               'to be plugged on the destination host %s.',
8324                               dest, instance=instance)
8325                 self._cleanup_pre_live_migration(
8326                     context, dest, instance, migration, migrate_data,
8327                     source_bdms)
8328         except eventlet.timeout.Timeout:
8329             # We only get here if wait_for_vif_plugged is True which means
8330             # live_migration_wait_for_vif_plug=True on the destination host.
8331             msg = (
8332                 'Timed out waiting for events: %(events)s. If these timeouts '
8333                 'are a persistent issue it could mean the networking backend '
8334                 'on host %(dest)s does not support sending these events '
8335                 'unless there are port binding host changes which does not '
8336                 'happen at this point in the live migration process. You may '
8337                 'need to disable the live_migration_wait_for_vif_plug option '
8338                 'on host %(dest)s.')
8339             subs = {'events': events, 'dest': dest}
8340             LOG.warning(msg, subs, instance=instance)
8341             if CONF.vif_plugging_is_fatal:
8342                 self._cleanup_pre_live_migration(
8343                     context, dest, instance, migration, migrate_data,
8344                     source_bdms)
8345                 raise exception.MigrationError(reason=msg % subs)
8346         except Exception:
8347             with excutils.save_and_reraise_exception():
8348                 LOG.exception('Pre live migration failed at %s',
8349                               dest, instance=instance)
8350                 self._cleanup_pre_live_migration(
8351                     context, dest, instance, migration, migrate_data,
8352                     source_bdms)
8353         return migrate_data
8354 
8355     def _do_live_migration(self, context, dest, instance, block_migration,
8356                            migration, migrate_data):
8357         # NOTE(danms): We should enhance the RT to account for migrations
8358         # and use the status field to denote when the accounting has been
8359         # done on source/destination. For now, this is just here for status
8360         # reporting
8361         self._set_migration_status(migration, 'preparing')
8362         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8363                 context, instance.uuid)
8364 
8365         migrate_data = self._do_pre_live_migration_from_source(
8366             context, dest, instance, block_migration, migration, migrate_data,
8367             source_bdms)
8368 
8369         # Set migrate_data.migration because that is how _post_live_migration
8370         # and _rollback_live_migration get the migration object for cleanup.
8371         # Yes this is gross but changing the _post_live_migration and
8372         # _rollback_live_migration interfaces would also mean changing how the
8373         # virt drivers call them from the driver.live_migration method, i.e.
8374         # we would have to pass the migration object through the driver (or
8375         # consider using a partial but some do not like that pattern).
8376         migrate_data.migration = migration
8377 
8378         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
8379         # if it exist in the queue, then we are good to moving on, if
8380         # not, some other process must have aborted it, then we should
8381         # rollback.
8382         try:
8383             self._waiting_live_migrations.pop(instance.uuid)
8384         except KeyError:
8385             LOG.debug('Migration %s aborted by another process, rollback.',
8386                       migration.uuid, instance=instance)
8387             self._rollback_live_migration(context, instance, dest,
8388                                           migrate_data, 'cancelled',
8389                                           source_bdms=source_bdms)
8390             self._notify_live_migrate_abort_end(context, instance)
8391             return
8392 
8393         self._set_migration_status(migration, 'running')
8394 
8395         # NOTE(mdbooth): pre_live_migration will update connection_info and
8396         # attachment_id on all volume BDMS to reflect the new destination
8397         # host attachment. We fetch BDMs before that to retain connection_info
8398         # and attachment_id relating to the source host for post migration
8399         # cleanup.
8400         post_live_migration = functools.partial(self._post_live_migration,
8401                                                 source_bdms=source_bdms)
8402         rollback_live_migration = functools.partial(
8403             self._rollback_live_migration, source_bdms=source_bdms)
8404 
8405         LOG.debug('live_migration data is %s', migrate_data)
8406         try:
8407             self.driver.live_migration(context, instance, dest,
8408                                        post_live_migration,
8409                                        rollback_live_migration,
8410                                        block_migration, migrate_data)
8411         except Exception:
8412             LOG.exception('Live migration failed.', instance=instance)
8413             with excutils.save_and_reraise_exception():
8414                 # Put instance and migration into error state,
8415                 # as its almost certainly too late to rollback
8416                 self._set_migration_status(migration, 'error')
8417                 # first refresh instance as it may have got updated by
8418                 # post_live_migration_at_destination
8419                 instance.refresh()
8420                 self._set_instance_obj_error_state(instance,
8421                                                    clean_task_state=True)
8422 
8423     @wrap_exception()
8424     @wrap_instance_event(prefix='compute')
8425     @errors_out_migration
8426     @wrap_instance_fault
8427     def live_migration(self, context, dest, instance, block_migration,
8428                        migration, migrate_data):
8429         """Executing live migration.
8430 
8431         :param context: security context
8432         :param dest: destination host
8433         :param instance: a nova.objects.instance.Instance object
8434         :param block_migration: if true, prepare for block migration
8435         :param migration: an nova.objects.Migration object
8436         :param migrate_data: implementation specific params
8437 
8438         """
8439         self._set_migration_status(migration, 'queued')
8440         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
8441         # put the returned Future object into dict mapped with migration.uuid
8442         # in order to be able to track and abort it in the future.
8443         self._waiting_live_migrations[instance.uuid] = (None, None)
8444         try:
8445             future = self._live_migration_executor.submit(
8446                 self._do_live_migration, context, dest, instance,
8447                 block_migration, migration, migrate_data)
8448             self._waiting_live_migrations[instance.uuid] = (migration, future)
8449         except RuntimeError:
8450             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
8451             # pool is shutdown, which happens in
8452             # _cleanup_live_migrations_in_pool.
8453             LOG.info('Migration %s failed to submit as the compute service '
8454                      'is shutting down.', migration.uuid, instance=instance)
8455             raise exception.LiveMigrationNotSubmitted(
8456                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
8457 
8458     @wrap_exception()
8459     @wrap_instance_event(prefix='compute')
8460     @wrap_instance_fault
8461     def live_migration_force_complete(self, context, instance):
8462         """Force live migration to complete.
8463 
8464         :param context: Security context
8465         :param instance: The instance that is being migrated
8466         """
8467 
8468         self._notify_about_instance_usage(
8469             context, instance, 'live.migration.force.complete.start')
8470         compute_utils.notify_about_instance_action(
8471             context, instance, self.host,
8472             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8473             phase=fields.NotificationPhase.START)
8474         self.driver.live_migration_force_complete(instance)
8475         self._notify_about_instance_usage(
8476             context, instance, 'live.migration.force.complete.end')
8477         compute_utils.notify_about_instance_action(
8478             context, instance, self.host,
8479             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8480             phase=fields.NotificationPhase.END)
8481 
8482     def _notify_live_migrate_abort_end(self, context, instance):
8483         self._notify_about_instance_usage(
8484             context, instance, 'live.migration.abort.end')
8485         compute_utils.notify_about_instance_action(
8486             context, instance, self.host,
8487             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8488             phase=fields.NotificationPhase.END)
8489 
8490     @wrap_exception()
8491     @wrap_instance_event(prefix='compute')
8492     @wrap_instance_fault
8493     def live_migration_abort(self, context, instance, migration_id):
8494         """Abort an in-progress live migration.
8495 
8496         :param context: Security context
8497         :param instance: The instance that is being migrated
8498         :param migration_id: ID of in-progress live migration
8499 
8500         """
8501         self._notify_about_instance_usage(
8502             context, instance, 'live.migration.abort.start')
8503         compute_utils.notify_about_instance_action(
8504             context, instance, self.host,
8505             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8506             phase=fields.NotificationPhase.START)
8507         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
8508         # lead to 3 scenarios:
8509         # 1. The selected migration is still in queue, and the future.cancel()
8510         #    succeed, then the abort action is succeed, mark the migration
8511         #    status to 'cancelled'.
8512         # 2. The selected migration is still in queue, but the future.cancel()
8513         #    failed, then the _do_live_migration() has started executing, and
8514         #    the migration status is 'preparing', then we just pop it from the
8515         #    queue, and the migration process will handle it later. And the
8516         #    migration status couldn't be 'running' in this scenario because
8517         #    if _do_live_migration has started executing and we've already
8518         #    popped it from the queue and set the migration status to
8519         #    'running' at this point, popping it here will raise KeyError at
8520         #    which point we check if it's running and if so, we abort the old
8521         #    way.
8522         # 3. The selected migration is not in the queue, then the migration
8523         #    status is 'running', let the driver handle it.
8524         try:
8525             migration, future = (
8526                 self._waiting_live_migrations.pop(instance.uuid))
8527             if future and future.cancel():
8528                 # If we got here, we've successfully aborted the queued
8529                 # migration and _do_live_migration won't run so we need
8530                 # to set the migration status to cancelled and send the
8531                 # notification. If Future.cancel() fails, it means
8532                 # _do_live_migration is running and the migration status
8533                 # is preparing, and _do_live_migration() itself will attempt
8534                 # to pop the queued migration, hit a KeyError, and rollback,
8535                 # set the migration to cancelled and send the
8536                 # live.migration.abort.end notification.
8537                 self._set_migration_status(migration, 'cancelled')
8538         except KeyError:
8539             migration = objects.Migration.get_by_id(context, migration_id)
8540             if migration.status != 'running':
8541                 raise exception.InvalidMigrationState(
8542                     migration_id=migration_id, instance_uuid=instance.uuid,
8543                     state=migration.status, method='abort live migration')
8544             self.driver.live_migration_abort(instance)
8545         self._notify_live_migrate_abort_end(context, instance)
8546 
8547     def _live_migration_cleanup_flags(self, migrate_data, migr_ctxt=None):
8548         """Determine whether disks, instance path or other resources
8549         need to be cleaned up after live migration (at source on success,
8550         at destination on rollback)
8551 
8552         Block migration needs empty image at destination host before migration
8553         starts, so if any failure occurs, any empty images has to be deleted.
8554 
8555         Also Volume backed live migration w/o shared storage needs to delete
8556         newly created instance-xxx dir on the destination as a part of its
8557         rollback process
8558 
8559         There may be other resources which need cleanup; currently this is
8560         limited to vPMEM devices with the libvirt driver.
8561 
8562         :param migrate_data: implementation specific data
8563         :param migr_ctxt: specific resources stored in migration_context
8564         :returns: (bool, bool) -- do_cleanup, destroy_disks
8565         """
8566         # NOTE(pkoniszewski): block migration specific params are set inside
8567         # migrate_data objects for drivers that expose block live migration
8568         # information (i.e. Libvirt, HyperV). For other drivers cleanup is not
8569         # needed.
8570         do_cleanup = False
8571         destroy_disks = False
8572         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
8573             has_vpmem = False
8574             if migr_ctxt and migr_ctxt.old_resources:
8575                 for resource in migr_ctxt.old_resources:
8576                     if ('metadata' in resource and
8577                         isinstance(resource.metadata,
8578                                    objects.LibvirtVPMEMDevice)):
8579                         has_vpmem = True
8580                         break
8581             # No instance booting at source host, but instance dir
8582             # must be deleted for preparing next block migration
8583             # must be deleted for preparing next live migration w/o shared
8584             # storage
8585             # vpmem must be cleanped
8586             do_cleanup = not migrate_data.is_shared_instance_path or has_vpmem
8587             destroy_disks = not migrate_data.is_shared_block_storage
8588         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
8589             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
8590             do_cleanup = True
8591             destroy_disks = not migrate_data.is_shared_instance_path
8592 
8593         return (do_cleanup, destroy_disks)
8594 
8595     def _post_live_migration_remove_source_vol_connections(
8596             self, context, instance, source_bdms):
8597         """Disconnect volume connections from the source host during
8598         _post_live_migration.
8599 
8600         :param context: nova auth RequestContext
8601         :param instance: Instance object being live migrated
8602         :param source_bdms: BlockDeviceMappingList representing the attached
8603             volumes with connection_info set for the source host
8604         """
8605         # Detaching volumes.
8606         connector = self.driver.get_volume_connector(instance)
8607         for bdm in source_bdms:
8608             if bdm.is_volume:
8609                 # Detaching volumes is a call to an external API that can fail.
8610                 # If it does, we need to handle it gracefully so that the call
8611                 # to post_live_migration_at_destination - where we set instance
8612                 # host and task state - still happens. We need to rethink the
8613                 # current approach of setting instance host and task state
8614                 # AFTER a whole bunch of things that could fail in unhandled
8615                 # ways, but that is left as a TODO(artom).
8616                 try:
8617                     if bdm.attachment_id is None:
8618                         # Prior to cinder v3.44:
8619                         # We don't want to actually mark the volume detached,
8620                         # or delete the bdm, just remove the connection from
8621                         # this host.
8622                         #
8623                         # remove the volume connection without detaching from
8624                         # hypervisor because the instance is not running
8625                         # anymore on the current host
8626                         self.volume_api.terminate_connection(context,
8627                                                              bdm.volume_id,
8628                                                              connector)
8629                     else:
8630                         # cinder v3.44 api flow - delete the old attachment
8631                         # for the source host
8632                         self.volume_api.attachment_delete(context,
8633                                                           bdm.attachment_id)
8634 
8635                 except Exception as e:
8636                     if bdm.attachment_id is None:
8637                         LOG.error('Connection for volume %s not terminated on '
8638                                   'source host %s during post_live_migration: '
8639                                   '%s', bdm.volume_id, self.host,
8640                                   str(e), instance=instance)
8641                     else:
8642                         LOG.error('Volume attachment %s not deleted on source '
8643                                   'host %s during post_live_migration: %s',
8644                                   bdm.attachment_id, self.host,
8645                                   str(e), instance=instance)
8646 
8647     @wrap_exception()
8648     @wrap_instance_fault
8649     def _post_live_migration(self, ctxt, instance, dest,
8650                              block_migration=False, migrate_data=None,
8651                              source_bdms=None):
8652         """Post operations for live migration.
8653 
8654         This method is called from live_migration
8655         and mainly updating database record.
8656 
8657         :param ctxt: security context
8658         :param instance: instance dict
8659         :param dest: destination host
8660         :param block_migration: if true, prepare for block migration
8661         :param migrate_data: if not None, it is a dict which has data
8662         :param source_bdms: BDMs prior to modification by the destination
8663                             compute host. Set by _do_live_migration and not
8664                             part of the callback interface, so this is never
8665                             None
8666         required for live migration without shared storage
8667 
8668         """
8669         LOG.info('_post_live_migration() is started..',
8670                  instance=instance)
8671 
8672         # Cleanup source host post live-migration
8673         block_device_info = self._get_instance_block_device_info(
8674                             ctxt, instance, bdms=source_bdms)
8675         self.driver.post_live_migration(ctxt, instance, block_device_info,
8676                                         migrate_data)
8677 
8678         # Disconnect volumes from this (the source) host.
8679         self._post_live_migration_remove_source_vol_connections(
8680             ctxt, instance, source_bdms)
8681 
8682         # NOTE(artom) At this point in time we have not bound the ports to the
8683         # destination host yet (this happens in migrate_instance_start()
8684         # below). Therefore, the "old" source network info that's still in the
8685         # instance info cache is safe to use here, since it'll be used below
8686         # during driver.post_live_migration_at_source() to unplug the VIFs on
8687         # the source.
8688         network_info = instance.get_network_info()
8689 
8690         self._notify_about_instance_usage(ctxt, instance,
8691                                           "live_migration._post.start",
8692                                           network_info=network_info)
8693         compute_utils.notify_about_instance_action(
8694             ctxt, instance, self.host,
8695             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8696             phase=fields.NotificationPhase.START)
8697 
8698         migration = {'source_compute': self.host,
8699                      'dest_compute': dest, }
8700         # For neutron, migrate_instance_start will activate the destination
8701         # host port bindings, if there are any created by conductor before live
8702         # migration started.
8703         self.network_api.migrate_instance_start(ctxt,
8704                                                 instance,
8705                                                 migration)
8706 
8707         destroy_vifs = False
8708         try:
8709             # It's possible that the vif type changed on the destination
8710             # host and is already bound and active, so we need to use the
8711             # stashed source vifs in migrate_data.vifs (if present) to unplug
8712             # on the source host.
8713             unplug_nw_info = network_info
8714             if migrate_data and 'vifs' in migrate_data:
8715                 nw_info = []
8716                 for migrate_vif in migrate_data.vifs:
8717                     nw_info.append(migrate_vif.source_vif)
8718                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
8719                 LOG.debug('Calling driver.post_live_migration_at_source '
8720                           'with original source VIFs from migrate_data: %s',
8721                           unplug_nw_info, instance=instance)
8722             self.driver.post_live_migration_at_source(ctxt, instance,
8723                                                       unplug_nw_info)
8724         except NotImplementedError as ex:
8725             LOG.debug(ex, instance=instance)
8726             # For all hypervisors other than libvirt, there is a possibility
8727             # they are unplugging networks from source node in the cleanup
8728             # method
8729             destroy_vifs = True
8730 
8731         # Free instance allocations on source before claims are allocated on
8732         # destination node
8733         self.rt.free_pci_device_allocations_for_instance(ctxt, instance)
8734         # NOTE(danms): Save source node before calling post method on
8735         # destination, which will update it
8736         source_node = instance.node
8737 
8738         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8739             migrate_data, migr_ctxt=instance.migration_context)
8740 
8741         if do_cleanup:
8742             LOG.debug('Calling driver.cleanup from _post_live_migration',
8743                       instance=instance)
8744             self.driver.cleanup(ctxt, instance, unplug_nw_info,
8745                                 destroy_disks=destroy_disks,
8746                                 migrate_data=migrate_data,
8747                                 destroy_vifs=destroy_vifs)
8748 
8749         # Define domain at destination host, without doing it,
8750         # pause/suspend/terminate do not work.
8751         post_at_dest_success = True
8752         try:
8753             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
8754                     instance, block_migration, dest)
8755         except Exception as error:
8756             post_at_dest_success = False
8757             # We don't want to break _post_live_migration() if
8758             # post_live_migration_at_destination() fails as it should never
8759             # affect cleaning up source node.
8760             LOG.exception("Post live migration at destination %s failed",
8761                           dest, instance=instance, error=error)
8762 
8763         self.instance_events.clear_events_for_instance(instance)
8764 
8765         # NOTE(timello): make sure we update available resources on source
8766         # host even before next periodic task.
8767         self.update_available_resource(ctxt)
8768 
8769         self._update_scheduler_instance_info(ctxt, instance)
8770         self._notify_about_instance_usage(ctxt, instance,
8771                                           "live_migration._post.end",
8772                                           network_info=network_info)
8773         compute_utils.notify_about_instance_action(
8774             ctxt, instance, self.host,
8775             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8776             phase=fields.NotificationPhase.END)
8777         if post_at_dest_success:
8778             LOG.info('Migrating instance to %s finished successfully.',
8779                      dest, instance=instance)
8780 
8781         self._clean_instance_console_tokens(ctxt, instance)
8782         if migrate_data and migrate_data.obj_attr_is_set('migration'):
8783             migrate_data.migration.status = 'completed'
8784             migrate_data.migration.save()
8785             self._delete_allocation_after_move(ctxt,
8786                                                instance,
8787                                                migrate_data.migration)
8788         else:
8789             # We didn't have data on a migration, which means we can't
8790             # look up to see if we had new-style migration-based
8791             # allocations. This should really only happen in cases of
8792             # a buggy virt driver. Log a warning so we know it happened.
8793             LOG.warning('Live migration ended with no migrate_data '
8794                         'record. Unable to clean up migration-based '
8795                         'allocations for node %s which is almost certainly '
8796                         'not an expected situation.', source_node,
8797                         instance=instance)
8798 
8799     def _consoles_enabled(self):
8800         """Returns whether a console is enable."""
8801         return (CONF.vnc.enabled or CONF.spice.enabled or
8802                 CONF.rdp.enabled or CONF.serial_console.enabled or
8803                 CONF.mks.enabled)
8804 
8805     def _clean_instance_console_tokens(self, ctxt, instance):
8806         """Clean console tokens stored for an instance."""
8807         # If the database backend isn't in use, don't bother trying to clean
8808         # tokens.
8809         if self._consoles_enabled():
8810             objects.ConsoleAuthToken.\
8811                 clean_console_auths_for_instance(ctxt, instance.uuid)
8812 
8813     @wrap_exception()
8814     @wrap_instance_event(prefix='compute')
8815     @wrap_instance_fault
8816     def post_live_migration_at_destination(self, context, instance,
8817                                            block_migration):
8818         """Post operations for live migration .
8819 
8820         :param context: security context
8821         :param instance: Instance dict
8822         :param block_migration: if true, prepare for block migration
8823 
8824         """
8825         LOG.info('Post operation of migration started',
8826                  instance=instance)
8827 
8828         # NOTE(tr3buchet): setup networks on destination host
8829         #                  this is called a second time because
8830         #                  multi_host does not create the bridge in
8831         #                  plug_vifs
8832         # NOTE(mriedem): This is a no-op for neutron.
8833         self.network_api.setup_networks_on_host(context, instance,
8834                                                          self.host)
8835         migration = objects.Migration(
8836             source_compute=instance.host,
8837             dest_compute=self.host,
8838             migration_type=fields.MigrationType.LIVE_MIGRATION)
8839         self.network_api.migrate_instance_finish(
8840             context, instance, migration, provider_mappings=None)
8841 
8842         network_info = self.network_api.get_instance_nw_info(context, instance)
8843         self._notify_about_instance_usage(
8844                      context, instance, "live_migration.post.dest.start",
8845                      network_info=network_info)
8846         compute_utils.notify_about_instance_action(context, instance,
8847                 self.host,
8848                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8849                 phase=fields.NotificationPhase.START)
8850         block_device_info = self._get_instance_block_device_info(context,
8851                                                                  instance)
8852         # Allocate the claimed PCI resources at destination.
8853         self.rt.allocate_pci_devices_for_instance(context, instance)
8854 
8855         try:
8856             self.driver.post_live_migration_at_destination(
8857                 context, instance, network_info, block_migration,
8858                 block_device_info)
8859         except Exception:
8860             with excutils.save_and_reraise_exception():
8861                 instance.vm_state = vm_states.ERROR
8862                 LOG.error('Unexpected error during post live migration at '
8863                           'destination host.', instance=instance)
8864         finally:
8865             # Restore instance state and update host
8866             current_power_state = self._get_power_state(instance)
8867             node_name = None
8868             prev_host = instance.host
8869             try:
8870                 compute_node = self._get_compute_info(context, self.host)
8871                 node_name = compute_node.hypervisor_hostname
8872             except exception.ComputeHostNotFound:
8873                 LOG.exception('Failed to get compute_info for %s', self.host)
8874             finally:
8875                 # NOTE(artom) We need to apply the migration context here
8876                 # regardless of whether the driver's
8877                 # post_live_migration_at_destination succeeded or not: the
8878                 # instance is on the destination, potentially with a new NUMA
8879                 # topology and resource usage. We need to persist that.
8880                 # NOTE(artom) Apply followed by drop looks weird, but apply
8881                 # just saves the new fields while drop actually removes the
8882                 # migration context from the instance.
8883                 instance.apply_migration_context()
8884                 instance.drop_migration_context()
8885                 instance.host = self.host
8886                 instance.power_state = current_power_state
8887                 instance.task_state = None
8888                 instance.node = node_name
8889                 instance.progress = 0
8890                 instance.save(expected_task_state=task_states.MIGRATING)
8891 
8892         # NOTE(tr3buchet): tear down networks on source host (nova-net)
8893         # NOTE(mriedem): For neutron, this will delete any inactive source
8894         # host port bindings.
8895         try:
8896             self.network_api.setup_networks_on_host(context, instance,
8897                                                     prev_host, teardown=True)
8898         except exception.PortBindingDeletionFailed as e:
8899             # Removing the inactive port bindings from the source host is not
8900             # critical so just log an error but don't fail.
8901             LOG.error('Network cleanup failed for source host %s during post '
8902                       'live migration. You may need to manually clean up '
8903                       'resources in the network service. Error: %s',
8904                       prev_host, str(e))
8905         # NOTE(vish): this is necessary to update dhcp for nova-network
8906         # NOTE(mriedem): This is a no-op for neutron.
8907         self.network_api.setup_networks_on_host(context, instance, self.host)
8908         self._notify_about_instance_usage(
8909                      context, instance, "live_migration.post.dest.end",
8910                      network_info=network_info)
8911         compute_utils.notify_about_instance_action(context, instance,
8912                 self.host,
8913                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8914                 phase=fields.NotificationPhase.END)
8915 
8916     def _remove_remote_volume_connections(self, context, dest, bdms, instance):
8917         """Rollback remote volume connections on the dest"""
8918         for bdm in bdms:
8919             try:
8920                 # remove the connection on the destination host
8921                 # NOTE(lyarwood): This actually calls the cinderv2
8922                 # os-terminate_connection API if required.
8923                 self.compute_rpcapi.remove_volume_connection(
8924                         context, instance, bdm.volume_id, dest)
8925             except Exception:
8926                 LOG.warning("Ignoring exception while attempting "
8927                             "to rollback volume connections for "
8928                             "volume %s on host %s.", bdm.volume_id,
8929                             dest, instance=instance)
8930 
8931     def _rollback_volume_bdms(self, context, bdms, original_bdms, instance):
8932         """Rollback the connection_info and attachment_id for each bdm"""
8933         original_bdms_by_volid = {bdm.volume_id: bdm for bdm in original_bdms
8934                                   if bdm.is_volume}
8935         for bdm in bdms:
8936             try:
8937                 original_bdm = original_bdms_by_volid[bdm.volume_id]
8938                 # NOTE(lyarwood): Only delete the referenced attachment if it
8939                 # is different to the original in order to avoid accidentally
8940                 # removing the source host volume attachment after it has
8941                 # already been rolled back by a failure in pre_live_migration.
8942                 if (bdm.attachment_id and original_bdm.attachment_id and
8943                     bdm.attachment_id != original_bdm.attachment_id):
8944                     # NOTE(lyarwood): 3.44 cinder api flow. Delete the
8945                     # attachment used by the bdm and reset it to that of
8946                     # the original bdm.
8947                     self.volume_api.attachment_delete(context,
8948                                                       bdm.attachment_id)
8949                     bdm.attachment_id = original_bdm.attachment_id
8950                 # NOTE(lyarwood): Reset the connection_info to the original
8951                 bdm.connection_info = original_bdm.connection_info
8952                 bdm.save()
8953             except cinder_exception.ClientException:
8954                 LOG.warning("Ignoring cinderclient exception when "
8955                             "attempting to delete attachment %s for volume "
8956                             "%s while rolling back volume bdms.",
8957                             bdm.attachment_id, bdm.volume_id,
8958                             instance=instance)
8959             except Exception:
8960                 with excutils.save_and_reraise_exception():
8961                     LOG.exception("Exception while attempting to rollback "
8962                                   "BDM for volume %s.", bdm.volume_id,
8963                                   instance=instance)
8964 
8965     @wrap_exception()
8966     @wrap_instance_fault
8967     def _rollback_live_migration(self, context, instance,
8968                                  dest, migrate_data=None,
8969                                  migration_status='failed',
8970                                  source_bdms=None):
8971         """Recovers Instance/volume state from migrating -> running.
8972 
8973         :param context: security context
8974         :param instance: nova.objects.instance.Instance object
8975         :param dest:
8976             This method is called from live migration src host.
8977             This param specifies destination host.
8978         :param migrate_data:
8979             if not none, contains implementation specific data.
8980         :param migration_status:
8981             Contains the status we want to set for the migration object
8982         :param source_bdms: BDMs prior to modification by the destination
8983                             compute host. Set by _do_live_migration and not
8984                             part of the callback interface, so this is never
8985                             None
8986 
8987         """
8988         # NOTE(gibi): We need to refresh pci_requests of the instance as it
8989         # might be changed by the conductor during scheduling based on the
8990         # selected destination host. If the instance has SRIOV ports with
8991         # resource request then the LiveMigrationTask._find_destination call
8992         # updated the instance.pci_requests.requests[].spec with the SRIOV PF
8993         # device name to be used on the destination host. As the migration is
8994         # rolling back to the source host now we don't want to persist the
8995         # destination host related changes in the DB.
8996         instance.pci_requests = \
8997             objects.InstancePCIRequests.get_by_instance_uuid(
8998                 context, instance.uuid)
8999 
9000         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
9001               migrate_data.obj_attr_is_set('migration')):
9002             migration = migrate_data.migration
9003         else:
9004             migration = None
9005 
9006         if migration:
9007             # Remove allocations created in Placement for the dest node.
9008             # If migration is None, the virt driver didn't pass it which is
9009             # a bug.
9010             self._revert_allocation(context, instance, migration)
9011         else:
9012             LOG.error('Unable to revert allocations during live migration '
9013                       'rollback; compute driver did not provide migrate_data',
9014                       instance=instance)
9015 
9016         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
9017         #                  for nova-network)
9018         # NOTE(mriedem): This is a no-op for neutron.
9019         self.network_api.setup_networks_on_host(context, instance, self.host)
9020         self.driver.rollback_live_migration_at_source(context, instance,
9021                                                       migrate_data)
9022 
9023         # NOTE(lyarwood): Fetch the current list of BDMs, disconnect any
9024         # connected volumes from the dest and delete any volume attachments
9025         # used by the destination host before rolling back to the original
9026         # still valid source host volume attachments.
9027         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9028                 context, instance.uuid)
9029         # TODO(lyarwood): Turn the following into a lookup method within
9030         # BlockDeviceMappingList.
9031         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
9032         self._remove_remote_volume_connections(context, dest, vol_bdms,
9033                                                instance)
9034         self._rollback_volume_bdms(context, vol_bdms, source_bdms, instance)
9035 
9036         self._notify_about_instance_usage(context, instance,
9037                                           "live_migration._rollback.start")
9038         compute_utils.notify_about_instance_action(context, instance,
9039                 self.host,
9040                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
9041                 phase=fields.NotificationPhase.START,
9042                 bdms=bdms)
9043 
9044         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
9045                 migrate_data, migr_ctxt=instance.migration_context)
9046 
9047         if do_cleanup:
9048             self.compute_rpcapi.rollback_live_migration_at_destination(
9049                     context, instance, dest, destroy_disks=destroy_disks,
9050                     migrate_data=migrate_data)
9051         else:
9052             # The port binding profiles need to be cleaned up.
9053             with errors_out_migration_ctxt(migration):
9054                 try:
9055                     # This call will delete any inactive destination host
9056                     # port bindings.
9057                     self.network_api.setup_networks_on_host(
9058                         context, instance, host=dest, teardown=True)
9059                 except exception.PortBindingDeletionFailed as e:
9060                     # Removing the inactive port bindings from the destination
9061                     # host is not critical so just log an error but don't fail.
9062                     LOG.error(
9063                         'Network cleanup failed for destination host %s '
9064                         'during live migration rollback. You may need to '
9065                         'manually clean up resources in the network service. '
9066                         'Error: %s', dest, str(e))
9067                 except Exception:
9068                     with excutils.save_and_reraise_exception():
9069                         LOG.exception(
9070                             'An error occurred while cleaning up networking '
9071                             'during live migration rollback.',
9072                             instance=instance)
9073 
9074         # NOTE(luyao): We drop move_claim and migration_context after cleanup
9075         # is complete, to ensure the specific resources claimed on destination
9076         # are released safely.
9077         # TODO(artom) drop_move_claim_at_destination() is new in RPC 5.3, only
9078         # call it if we performed a NUMA-aware live migration (which implies us
9079         # being able to send RPC 5.3). To check this, we can use the
9080         # src_supports_numa_live_migration flag, as it will be set if and only
9081         # if:
9082         # - dst_supports_numa_live_migration made its way to the source
9083         #   (meaning both dest and source are new and conductor can speak
9084         #   RPC 5.3)
9085         # - src_supports_numa_live_migration was set by the source driver and
9086         #   passed the send-RPC-5.3 check.
9087         # This check can be removed in RPC 6.0.
9088         if ('src_supports_numa_live_migration' in migrate_data and
9089                 migrate_data.src_supports_numa_live_migration):
9090             LOG.debug('Calling destination to drop move claim.',
9091                       instance=instance)
9092             self.compute_rpcapi.drop_move_claim_at_destination(context,
9093                                                                instance, dest)
9094 
9095         # NOTE(luyao): We only update instance info after rollback operations
9096         # are complete
9097         instance.task_state = None
9098         instance.progress = 0
9099         instance.drop_migration_context()
9100         instance.save(expected_task_state=[task_states.MIGRATING])
9101 
9102         self._notify_about_instance_usage(context, instance,
9103                                           "live_migration._rollback.end")
9104         compute_utils.notify_about_instance_action(context, instance,
9105                 self.host,
9106                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
9107                 phase=fields.NotificationPhase.END,
9108                 bdms=bdms)
9109 
9110         # NOTE(luyao): we have cleanup everything and get instance
9111         # back to normal status, now set migration status to 'failed'
9112         self._set_migration_status(migration, migration_status)
9113 
9114     @wrap_exception()
9115     @wrap_instance_fault
9116     def drop_move_claim_at_destination(self, context, instance):
9117         """Called by the source of a live migration during rollback to ask the
9118         destination to drop the MoveClaim object that was created for the live
9119         migration on the destination.
9120         """
9121         nodename = self._get_nodename(instance)
9122         LOG.debug('Dropping live migration resource claim on destination '
9123                   'node %s', nodename, instance=instance)
9124         self.rt.drop_move_claim(
9125             context, instance, nodename, instance_type=instance.flavor)
9126 
9127     @wrap_exception()
9128     @wrap_instance_event(prefix='compute')
9129     @wrap_instance_fault
9130     def rollback_live_migration_at_destination(self, context, instance,
9131                                                destroy_disks,
9132                                                migrate_data):
9133         """Cleaning up image directory that is created pre_live_migration.
9134 
9135         :param context: security context
9136         :param instance: a nova.objects.instance.Instance object sent over rpc
9137         :param destroy_disks: whether to destroy volumes or not
9138         :param migrate_data: contains migration info
9139         """
9140         network_info = self.network_api.get_instance_nw_info(context, instance)
9141         self._notify_about_instance_usage(
9142                       context, instance, "live_migration.rollback.dest.start",
9143                       network_info=network_info)
9144         compute_utils.notify_about_instance_action(
9145             context, instance, self.host,
9146             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
9147             phase=fields.NotificationPhase.START)
9148         try:
9149             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
9150             # NOTE(mriedem): For neutron, this call will delete any
9151             # destination host port bindings.
9152             # TODO(mriedem): We should eventually remove this call from
9153             # this method (rollback_live_migration_at_destination) since this
9154             # method is only called conditionally based on whether or not the
9155             # instance is running on shared storage. _rollback_live_migration
9156             # already calls this method for neutron if we are running on
9157             # shared storage.
9158             self.network_api.setup_networks_on_host(context, instance,
9159                                                     self.host, teardown=True)
9160         except exception.PortBindingDeletionFailed as e:
9161             # Removing the inactive port bindings from the destination
9162             # host is not critical so just log an error but don't fail.
9163             LOG.error(
9164                 'Network cleanup failed for destination host %s '
9165                 'during live migration rollback. You may need to '
9166                 'manually clean up resources in the network service. '
9167                 'Error: %s', self.host, str(e))
9168         except Exception:
9169             with excutils.save_and_reraise_exception():
9170                 # NOTE(tdurakov): even if teardown networks fails driver
9171                 # should try to rollback live migration on destination.
9172                 LOG.exception('An error occurred while deallocating network.',
9173                               instance=instance)
9174         finally:
9175             # always run this even if setup_networks_on_host fails
9176             # NOTE(vish): The mapping is passed in so the driver can disconnect
9177             #             from remote volumes if necessary
9178             block_device_info = self._get_instance_block_device_info(context,
9179                                                                      instance)
9180             # free any instance PCI claims done on destination during
9181             # check_can_live_migrate_destination()
9182             self.rt.free_pci_device_claims_for_instance(context, instance)
9183 
9184             # NOTE(luyao): Apply migration_context temporarily since it's
9185             # on destination host, we rely on instance object to cleanup
9186             # specific resources like vpmem
9187             with instance.mutated_migration_context():
9188                 self.driver.rollback_live_migration_at_destination(
9189                     context, instance, network_info, block_device_info,
9190                     destroy_disks=destroy_disks, migrate_data=migrate_data)
9191 
9192         self._notify_about_instance_usage(
9193                         context, instance, "live_migration.rollback.dest.end",
9194                         network_info=network_info)
9195         compute_utils.notify_about_instance_action(
9196             context, instance, self.host,
9197             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
9198             phase=fields.NotificationPhase.END)
9199 
9200     def _require_nw_info_update(self, context, instance):
9201         """Detect whether there is a mismatch in binding:host_id, or
9202         binding_failed or unbound binding:vif_type for any of the instances
9203         ports.
9204         """
9205         # Only update port bindings if compute manager does manage port
9206         # bindings instead of the compute driver. For example IronicDriver
9207         # manages the port binding for baremetal instance ports, hence,
9208         # external intervention with the binding is not desired.
9209         if self.driver.manages_network_binding_host_id():
9210             return False
9211 
9212         search_opts = {'device_id': instance.uuid,
9213                        'fields': ['binding:host_id', 'binding:vif_type']}
9214         ports = self.network_api.list_ports(context, **search_opts)
9215         for p in ports['ports']:
9216             if p.get('binding:host_id') != self.host:
9217                 return True
9218             vif_type = p.get('binding:vif_type')
9219             if (vif_type == network_model.VIF_TYPE_UNBOUND or
9220                     vif_type == network_model.VIF_TYPE_BINDING_FAILED):
9221                 return True
9222         return False
9223 
9224     @periodic_task.periodic_task(
9225         spacing=CONF.heal_instance_info_cache_interval)
9226     def _heal_instance_info_cache(self, context):
9227         """Called periodically.  On every call, try to update the
9228         info_cache's network information for another instance by
9229         calling to the network manager.
9230 
9231         This is implemented by keeping a cache of uuids of instances
9232         that live on this host.  On each call, we pop one off of a
9233         list, pull the DB record, and try the call to the network API.
9234         If anything errors don't fail, as it's possible the instance
9235         has been deleted, etc.
9236         """
9237         heal_interval = CONF.heal_instance_info_cache_interval
9238         if not heal_interval:
9239             return
9240 
9241         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
9242         instance = None
9243 
9244         LOG.debug('Starting heal instance info cache')
9245 
9246         if not instance_uuids:
9247             # The list of instances to heal is empty so rebuild it
9248             LOG.debug('Rebuilding the list of instances to heal')
9249             db_instances = objects.InstanceList.get_by_host(
9250                 context, self.host, expected_attrs=[], use_slave=True)
9251             for inst in db_instances:
9252                 # We don't want to refresh the cache for instances
9253                 # which are building or deleting so don't put them
9254                 # in the list. If they are building they will get
9255                 # added to the list next time we build it.
9256                 if (inst.vm_state == vm_states.BUILDING):
9257                     LOG.debug('Skipping network cache update for instance '
9258                               'because it is Building.', instance=inst)
9259                     continue
9260                 if (inst.task_state == task_states.DELETING):
9261                     LOG.debug('Skipping network cache update for instance '
9262                               'because it is being deleted.', instance=inst)
9263                     continue
9264 
9265                 if not instance:
9266                     # Save the first one we find so we don't
9267                     # have to get it again
9268                     instance = inst
9269                 else:
9270                     instance_uuids.append(inst['uuid'])
9271 
9272             self._instance_uuids_to_heal = instance_uuids
9273         else:
9274             # Find the next valid instance on the list
9275             while instance_uuids:
9276                 try:
9277                     inst = objects.Instance.get_by_uuid(
9278                             context, instance_uuids.pop(0),
9279                             expected_attrs=['system_metadata', 'info_cache',
9280                                             'flavor'],
9281                             use_slave=True)
9282                 except exception.InstanceNotFound:
9283                     # Instance is gone.  Try to grab another.
9284                     continue
9285 
9286                 # Check the instance hasn't been migrated
9287                 if inst.host != self.host:
9288                     LOG.debug('Skipping network cache update for instance '
9289                               'because it has been migrated to another '
9290                               'host.', instance=inst)
9291                 # Check the instance isn't being deleting
9292                 elif inst.task_state == task_states.DELETING:
9293                     LOG.debug('Skipping network cache update for instance '
9294                               'because it is being deleted.', instance=inst)
9295                 else:
9296                     instance = inst
9297                     break
9298 
9299         if instance:
9300             # We have an instance now to refresh
9301             try:
9302                 # Fix potential mismatch in port binding if evacuation failed
9303                 # after reassigning the port binding to the dest host but
9304                 # before the instance host is changed.
9305                 # Do this only when instance has no pending task.
9306                 if instance.task_state is None and \
9307                         self._require_nw_info_update(context, instance):
9308                     LOG.info("Updating ports in neutron", instance=instance)
9309                     self.network_api.setup_instance_network_on_host(
9310                         context, instance, self.host)
9311                 # Call to network API to get instance info.. this will
9312                 # force an update to the instance's info_cache
9313                 self.network_api.get_instance_nw_info(
9314                     context, instance, force_refresh=True)
9315                 LOG.debug('Updated the network info_cache for instance',
9316                           instance=instance)
9317             except exception.InstanceNotFound:
9318                 # Instance is gone.
9319                 LOG.debug('Instance no longer exists. Unable to refresh',
9320                           instance=instance)
9321                 return
9322             except exception.InstanceInfoCacheNotFound:
9323                 # InstanceInfoCache is gone.
9324                 LOG.debug('InstanceInfoCache no longer exists. '
9325                           'Unable to refresh', instance=instance)
9326             except Exception:
9327                 LOG.error('An error occurred while refreshing the network '
9328                           'cache.', instance=instance, exc_info=True)
9329         else:
9330             LOG.debug("Didn't find any instances for network info cache "
9331                       "update.")
9332 
9333     @periodic_task.periodic_task
9334     def _poll_rebooting_instances(self, context):
9335         if CONF.reboot_timeout > 0:
9336             filters = {'task_state':
9337                        [task_states.REBOOTING,
9338                         task_states.REBOOT_STARTED,
9339                         task_states.REBOOT_PENDING],
9340                        'host': self.host}
9341             rebooting = objects.InstanceList.get_by_filters(
9342                 context, filters, expected_attrs=[], use_slave=True)
9343 
9344             to_poll = []
9345             for instance in rebooting:
9346                 if timeutils.is_older_than(instance.updated_at,
9347                                            CONF.reboot_timeout):
9348                     to_poll.append(instance)
9349 
9350             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
9351 
9352     @periodic_task.periodic_task
9353     def _poll_rescued_instances(self, context):
9354         if CONF.rescue_timeout > 0:
9355             filters = {'vm_state': vm_states.RESCUED,
9356                        'host': self.host}
9357             rescued_instances = objects.InstanceList.get_by_filters(
9358                 context, filters, expected_attrs=["system_metadata"],
9359                 use_slave=True)
9360 
9361             to_unrescue = []
9362             for instance in rescued_instances:
9363                 if timeutils.is_older_than(instance.launched_at,
9364                                            CONF.rescue_timeout):
9365                     to_unrescue.append(instance)
9366 
9367             for instance in to_unrescue:
9368                 self.compute_api.unrescue(context, instance)
9369 
9370     @periodic_task.periodic_task
9371     def _poll_unconfirmed_resizes(self, context):
9372         if CONF.resize_confirm_window == 0:
9373             return
9374 
9375         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
9376                 context, CONF.resize_confirm_window, self.host,
9377                 use_slave=True)
9378 
9379         migrations_info = dict(migration_count=len(migrations),
9380                 confirm_window=CONF.resize_confirm_window)
9381 
9382         if migrations_info["migration_count"] > 0:
9383             LOG.info("Found %(migration_count)d unconfirmed migrations "
9384                      "older than %(confirm_window)d seconds",
9385                      migrations_info)
9386 
9387         def _set_migration_to_error(migration, reason, **kwargs):
9388             LOG.warning("Setting migration %(migration_id)s to error: "
9389                         "%(reason)s",
9390                         {'migration_id': migration['id'], 'reason': reason},
9391                         **kwargs)
9392             migration.status = 'error'
9393             migration.save()
9394 
9395         for migration in migrations:
9396             instance_uuid = migration.instance_uuid
9397             LOG.info("Automatically confirming migration "
9398                      "%(migration_id)s for instance %(instance_uuid)s",
9399                      {'migration_id': migration.id,
9400                       'instance_uuid': instance_uuid})
9401             expected_attrs = ['metadata', 'system_metadata']
9402             try:
9403                 instance = objects.Instance.get_by_uuid(context,
9404                             instance_uuid, expected_attrs=expected_attrs,
9405                             use_slave=True)
9406             except exception.InstanceNotFound:
9407                 reason = (_("Instance %s not found") %
9408                           instance_uuid)
9409                 _set_migration_to_error(migration, reason)
9410                 continue
9411             if instance.vm_state == vm_states.ERROR:
9412                 reason = _("In ERROR state")
9413                 _set_migration_to_error(migration, reason,
9414                                         instance=instance)
9415                 continue
9416             # race condition: The instance in DELETING state should not be
9417             # set the migration state to error, otherwise the instance in
9418             # to be deleted which is in RESIZED state
9419             # will not be able to confirm resize
9420             if instance.task_state in [task_states.DELETING,
9421                                        task_states.SOFT_DELETING]:
9422                 msg = ("Instance being deleted or soft deleted during resize "
9423                        "confirmation. Skipping.")
9424                 LOG.debug(msg, instance=instance)
9425                 continue
9426 
9427             # race condition: This condition is hit when this method is
9428             # called between the save of the migration record with a status of
9429             # finished and the save of the instance object with a state of
9430             # RESIZED. The migration record should not be set to error.
9431             if instance.task_state == task_states.RESIZE_FINISH:
9432                 msg = ("Instance still resizing during resize "
9433                        "confirmation. Skipping.")
9434                 LOG.debug(msg, instance=instance)
9435                 continue
9436 
9437             vm_state = instance.vm_state
9438             task_state = instance.task_state
9439             if vm_state != vm_states.RESIZED or task_state is not None:
9440                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
9441                            "RESIZED/None") %
9442                           {'vm_state': vm_state,
9443                            'task_state': task_state})
9444                 _set_migration_to_error(migration, reason,
9445                                         instance=instance)
9446                 continue
9447             try:
9448                 self.compute_api.confirm_resize(context, instance,
9449                                                 migration=migration)
9450             except Exception as e:
9451                 LOG.info("Error auto-confirming resize: %s. "
9452                          "Will retry later.", e, instance=instance)
9453 
9454     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
9455     def _poll_shelved_instances(self, context):
9456 
9457         if CONF.shelved_offload_time <= 0:
9458             return
9459 
9460         filters = {'vm_state': vm_states.SHELVED,
9461                    'task_state': None,
9462                    'host': self.host}
9463         shelved_instances = objects.InstanceList.get_by_filters(
9464             context, filters=filters, expected_attrs=['system_metadata'],
9465             use_slave=True)
9466 
9467         to_gc = []
9468         for instance in shelved_instances:
9469             sys_meta = instance.system_metadata
9470             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
9471             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
9472                 to_gc.append(instance)
9473 
9474         for instance in to_gc:
9475             try:
9476                 instance.task_state = task_states.SHELVING_OFFLOADING
9477                 instance.save(expected_task_state=(None,))
9478                 accel_uuids = []
9479                 if instance.flavor.extra_specs.get('accel:device_profile'):
9480                     cyclient = cyborg.get_client(context)
9481                     accel_uuids = cyclient.get_arq_uuids_for_instance(instance)
9482                 self.shelve_offload_instance(
9483                     context, instance, clean_shutdown=False,
9484                     accel_uuids=accel_uuids)
9485             except Exception:
9486                 LOG.exception('Periodic task failed to offload instance.',
9487                               instance=instance)
9488 
9489     @periodic_task.periodic_task
9490     def _instance_usage_audit(self, context):
9491         if not CONF.instance_usage_audit:
9492             return
9493 
9494         begin, end = utils.last_completed_audit_period()
9495         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
9496                                self.host):
9497             return
9498 
9499         instances = objects.InstanceList.get_active_by_window_joined(
9500             context, begin, end, host=self.host,
9501             expected_attrs=['system_metadata', 'info_cache', 'metadata',
9502                             'flavor'],
9503             use_slave=True)
9504         num_instances = len(instances)
9505         errors = 0
9506         successes = 0
9507         LOG.info("Running instance usage audit for host %(host)s "
9508                  "from %(begin_time)s to %(end_time)s. "
9509                  "%(number_instances)s instances.",
9510                  {'host': self.host,
9511                   'begin_time': begin,
9512                   'end_time': end,
9513                   'number_instances': num_instances})
9514         start_time = time.time()
9515         task_log = objects.TaskLog(context)
9516         task_log.task_name = 'instance_usage_audit'
9517         task_log.period_beginning = begin
9518         task_log.period_ending = end
9519         task_log.host = self.host
9520         task_log.task_items = num_instances
9521         task_log.message = 'Instance usage audit started...'
9522         task_log.begin_task()
9523         for instance in instances:
9524             try:
9525                 compute_utils.notify_usage_exists(
9526                     self.notifier, context, instance, self.host,
9527                     ignore_missing_network_data=False)
9528                 successes += 1
9529             except Exception:
9530                 LOG.exception('Failed to generate usage '
9531                               'audit for instance '
9532                               'on host %s', self.host,
9533                               instance=instance)
9534                 errors += 1
9535         task_log.errors = errors
9536         task_log.message = (
9537             'Instance usage audit ran for host %s, %s instances in %s seconds.'
9538             % (self.host, num_instances, time.time() - start_time))
9539         task_log.end_task()
9540 
9541     def _get_host_volume_bdms(self, context, use_slave=False):
9542         """Return all block device mappings on a compute host."""
9543         compute_host_bdms = []
9544         instances = objects.InstanceList.get_by_host(context, self.host,
9545             use_slave=use_slave)
9546         for instance in instances:
9547             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9548                     context, instance.uuid, use_slave=use_slave)
9549             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
9550             compute_host_bdms.append(dict(instance=instance,
9551                                           instance_bdms=instance_bdms))
9552 
9553         return compute_host_bdms
9554 
9555     def _update_volume_usage_cache(self, context, vol_usages):
9556         """Updates the volume usage cache table with a list of stats."""
9557         for usage in vol_usages:
9558             # Allow switching of greenthreads between queries.
9559             greenthread.sleep(0)
9560             vol_usage = objects.VolumeUsage(context)
9561             vol_usage.volume_id = usage['volume']
9562             vol_usage.instance_uuid = usage['instance'].uuid
9563             vol_usage.project_id = usage['instance'].project_id
9564             vol_usage.user_id = usage['instance'].user_id
9565             vol_usage.availability_zone = usage['instance'].availability_zone
9566             vol_usage.curr_reads = usage['rd_req']
9567             vol_usage.curr_read_bytes = usage['rd_bytes']
9568             vol_usage.curr_writes = usage['wr_req']
9569             vol_usage.curr_write_bytes = usage['wr_bytes']
9570             vol_usage.save()
9571             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
9572             compute_utils.notify_about_volume_usage(context, vol_usage,
9573                                                     self.host)
9574 
9575     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
9576     def _poll_volume_usage(self, context):
9577         if CONF.volume_usage_poll_interval == 0:
9578             return
9579 
9580         compute_host_bdms = self._get_host_volume_bdms(context,
9581                                                        use_slave=True)
9582         if not compute_host_bdms:
9583             return
9584 
9585         LOG.debug("Updating volume usage cache")
9586         try:
9587             vol_usages = self.driver.get_all_volume_usage(context,
9588                                                           compute_host_bdms)
9589         except NotImplementedError:
9590             return
9591 
9592         self._update_volume_usage_cache(context, vol_usages)
9593 
9594     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
9595                                  run_immediately=True)
9596     def _sync_power_states(self, context):
9597         """Align power states between the database and the hypervisor.
9598 
9599         To sync power state data we make a DB call to get the number of
9600         virtual machines known by the hypervisor and if the number matches the
9601         number of virtual machines known by the database, we proceed in a lazy
9602         loop, one database record at a time, checking if the hypervisor has the
9603         same power state as is in the database.
9604         """
9605         db_instances = objects.InstanceList.get_by_host(context, self.host,
9606                                                         expected_attrs=[],
9607                                                         use_slave=True)
9608 
9609         try:
9610             num_vm_instances = self.driver.get_num_instances()
9611         except exception.VirtDriverNotReady as e:
9612             # If the virt driver is not ready, like ironic-api not being up
9613             # yet in the case of ironic, just log it and exit.
9614             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
9615             return
9616 
9617         num_db_instances = len(db_instances)
9618 
9619         if num_vm_instances != num_db_instances:
9620             LOG.warning("While synchronizing instance power states, found "
9621                         "%(num_db_instances)s instances in the database "
9622                         "and %(num_vm_instances)s instances on the "
9623                         "hypervisor.",
9624                         {'num_db_instances': num_db_instances,
9625                          'num_vm_instances': num_vm_instances})
9626 
9627         def _sync(db_instance):
9628             # NOTE(melwitt): This must be synchronized as we query state from
9629             #                two separate sources, the driver and the database.
9630             #                They are set (in stop_instance) and read, in sync.
9631             @utils.synchronized(db_instance.uuid)
9632             def query_driver_power_state_and_sync():
9633                 self._query_driver_power_state_and_sync(context, db_instance)
9634 
9635             try:
9636                 query_driver_power_state_and_sync()
9637             except Exception:
9638                 LOG.exception("Periodic sync_power_state task had an "
9639                               "error while processing an instance.",
9640                               instance=db_instance)
9641 
9642             self._syncs_in_progress.pop(db_instance.uuid)
9643 
9644         for db_instance in db_instances:
9645             # process syncs asynchronously - don't want instance locking to
9646             # block entire periodic task thread
9647             uuid = db_instance.uuid
9648             if uuid in self._syncs_in_progress:
9649                 LOG.debug('Sync already in progress for %s', uuid)
9650             else:
9651                 LOG.debug('Triggering sync for uuid %s', uuid)
9652                 self._syncs_in_progress[uuid] = True
9653                 self._sync_power_pool.spawn_n(_sync, db_instance)
9654 
9655     def _query_driver_power_state_and_sync(self, context, db_instance):
9656         if db_instance.task_state is not None:
9657             LOG.info("During sync_power_state the instance has a "
9658                      "pending task (%(task)s). Skip.",
9659                      {'task': db_instance.task_state}, instance=db_instance)
9660             return
9661         # No pending tasks. Now try to figure out the real vm_power_state.
9662         try:
9663             vm_instance = self.driver.get_info(db_instance)
9664             vm_power_state = vm_instance.state
9665         except exception.InstanceNotFound:
9666             vm_power_state = power_state.NOSTATE
9667         # Note(maoy): the above get_info call might take a long time,
9668         # for example, because of a broken libvirt driver.
9669         try:
9670             self._sync_instance_power_state(context,
9671                                             db_instance,
9672                                             vm_power_state,
9673                                             use_slave=True)
9674         except exception.InstanceNotFound:
9675             # NOTE(hanlind): If the instance gets deleted during sync,
9676             # silently ignore.
9677             pass
9678 
9679     def _stop_unexpected_shutdown_instance(self, context, vm_state,
9680                                            db_instance, orig_db_power_state):
9681         # this is an exceptional case; make sure our data is up
9682         # to date before slamming through a power off
9683         vm_instance = self.driver.get_info(db_instance,
9684                                            use_cache=False)
9685         vm_power_state = vm_instance.state
9686 
9687         # if it still looks off, go ahead and call stop()
9688         if vm_power_state in (power_state.SHUTDOWN,
9689                               power_state.CRASHED):
9690 
9691             LOG.warning("Instance shutdown by itself. Calling the "
9692                         "stop API. Current vm_state: %(vm_state)s, "
9693                         "current task_state: %(task_state)s, "
9694                         "original DB power_state: %(db_power_state)s, "
9695                         "current VM power_state: %(vm_power_state)s",
9696                         {'vm_state': vm_state,
9697                          'task_state': db_instance.task_state,
9698                          'db_power_state': orig_db_power_state,
9699                          'vm_power_state': vm_power_state},
9700                         instance=db_instance)
9701             try:
9702                 # Note(maoy): here we call the API instead of
9703                 # brutally updating the vm_state in the database
9704                 # to allow all the hooks and checks to be performed.
9705                 if db_instance.shutdown_terminate:
9706                     self.compute_api.delete(context, db_instance)
9707                 else:
9708                     self.compute_api.stop(context, db_instance)
9709             except Exception:
9710                 # Note(maoy): there is no need to propagate the error
9711                 # because the same power_state will be retrieved next
9712                 # time and retried.
9713                 # For example, there might be another task scheduled.
9714                 LOG.exception("error during stop() in sync_power_state.",
9715                               instance=db_instance)
9716 
9717     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
9718                                    use_slave=False):
9719         """Align instance power state between the database and hypervisor.
9720 
9721         If the instance is not found on the hypervisor, but is in the database,
9722         then a stop() API will be called on the instance.
9723         """
9724 
9725         # We re-query the DB to get the latest instance info to minimize
9726         # (not eliminate) race condition.
9727         db_instance.refresh(use_slave=use_slave)
9728         db_power_state = db_instance.power_state
9729         vm_state = db_instance.vm_state
9730 
9731         if self.host != db_instance.host:
9732             # on the sending end of nova-compute _sync_power_state
9733             # may have yielded to the greenthread performing a live
9734             # migration; this in turn has changed the resident-host
9735             # for the VM; However, the instance is still active, it
9736             # is just in the process of migrating to another host.
9737             # This implies that the compute source must relinquish
9738             # control to the compute destination.
9739             LOG.info("During the sync_power process the "
9740                      "instance has moved from "
9741                      "host %(src)s to host %(dst)s",
9742                      {'src': db_instance.host,
9743                       'dst': self.host},
9744                      instance=db_instance)
9745             return
9746         elif db_instance.task_state is not None:
9747             # on the receiving end of nova-compute, it could happen
9748             # that the DB instance already report the new resident
9749             # but the actual VM has not showed up on the hypervisor
9750             # yet. In this case, let's allow the loop to continue
9751             # and run the state sync in a later round
9752             LOG.info("During sync_power_state the instance has a "
9753                      "pending task (%(task)s). Skip.",
9754                      {'task': db_instance.task_state},
9755                      instance=db_instance)
9756             return
9757 
9758         orig_db_power_state = db_power_state
9759         if vm_power_state != db_power_state:
9760             LOG.info('During _sync_instance_power_state the DB '
9761                      'power_state (%(db_power_state)s) does not match '
9762                      'the vm_power_state from the hypervisor '
9763                      '(%(vm_power_state)s). Updating power_state in the '
9764                      'DB to match the hypervisor.',
9765                      {'db_power_state': db_power_state,
9766                       'vm_power_state': vm_power_state},
9767                      instance=db_instance)
9768             # power_state is always updated from hypervisor to db
9769             db_instance.power_state = vm_power_state
9770             db_instance.save()
9771             db_power_state = vm_power_state
9772 
9773         # Note(maoy): Now resolve the discrepancy between vm_state and
9774         # vm_power_state. We go through all possible vm_states.
9775         if vm_state in (vm_states.BUILDING,
9776                         vm_states.RESCUED,
9777                         vm_states.RESIZED,
9778                         vm_states.SUSPENDED,
9779                         vm_states.ERROR):
9780             # TODO(maoy): we ignore these vm_state for now.
9781             pass
9782         elif vm_state == vm_states.ACTIVE:
9783             # The only rational power state should be RUNNING
9784             if vm_power_state in (power_state.SHUTDOWN,
9785                                   power_state.CRASHED):
9786                 self._stop_unexpected_shutdown_instance(
9787                     context, vm_state, db_instance, orig_db_power_state)
9788             elif vm_power_state == power_state.SUSPENDED:
9789                 LOG.warning("Instance is suspended unexpectedly. Calling "
9790                             "the stop API.", instance=db_instance)
9791                 try:
9792                     self.compute_api.stop(context, db_instance)
9793                 except Exception:
9794                     LOG.exception("error during stop() in sync_power_state.",
9795                                   instance=db_instance)
9796             elif vm_power_state == power_state.PAUSED:
9797                 # Note(maoy): a VM may get into the paused state not only
9798                 # because the user request via API calls, but also
9799                 # due to (temporary) external instrumentations.
9800                 # Before the virt layer can reliably report the reason,
9801                 # we simply ignore the state discrepancy. In many cases,
9802                 # the VM state will go back to running after the external
9803                 # instrumentation is done. See bug 1097806 for details.
9804                 LOG.warning("Instance is paused unexpectedly. Ignore.",
9805                             instance=db_instance)
9806             elif vm_power_state == power_state.NOSTATE:
9807                 # Occasionally, depending on the status of the hypervisor,
9808                 # which could be restarting for example, an instance may
9809                 # not be found.  Therefore just log the condition.
9810                 LOG.warning("Instance is unexpectedly not found. Ignore.",
9811                             instance=db_instance)
9812         elif vm_state == vm_states.STOPPED:
9813             if vm_power_state not in (power_state.NOSTATE,
9814                                       power_state.SHUTDOWN,
9815                                       power_state.CRASHED):
9816                 LOG.warning("Instance is not stopped. Calling "
9817                             "the stop API. Current vm_state: %(vm_state)s,"
9818                             " current task_state: %(task_state)s, "
9819                             "original DB power_state: %(db_power_state)s, "
9820                             "current VM power_state: %(vm_power_state)s",
9821                             {'vm_state': vm_state,
9822                              'task_state': db_instance.task_state,
9823                              'db_power_state': orig_db_power_state,
9824                              'vm_power_state': vm_power_state},
9825                             instance=db_instance)
9826                 try:
9827                     # NOTE(russellb) Force the stop, because normally the
9828                     # compute API would not allow an attempt to stop a stopped
9829                     # instance.
9830                     self.compute_api.force_stop(context, db_instance)
9831                 except Exception:
9832                     LOG.exception("error during stop() in sync_power_state.",
9833                                   instance=db_instance)
9834         elif vm_state == vm_states.PAUSED:
9835             if vm_power_state in (power_state.SHUTDOWN,
9836                                   power_state.CRASHED):
9837                 LOG.warning("Paused instance shutdown by itself. Calling "
9838                             "the stop API.", instance=db_instance)
9839                 try:
9840                     self.compute_api.force_stop(context, db_instance)
9841                 except Exception:
9842                     LOG.exception("error during stop() in sync_power_state.",
9843                                   instance=db_instance)
9844         elif vm_state in (vm_states.SOFT_DELETED,
9845                           vm_states.DELETED):
9846             if vm_power_state not in (power_state.NOSTATE,
9847                                       power_state.SHUTDOWN):
9848                 # Note(maoy): this should be taken care of periodically in
9849                 # _cleanup_running_deleted_instances().
9850                 LOG.warning("Instance is not (soft-)deleted.",
9851                             instance=db_instance)
9852 
9853     @periodic_task.periodic_task
9854     def _reclaim_queued_deletes(self, context):
9855         """Reclaim instances that are queued for deletion."""
9856         interval = CONF.reclaim_instance_interval
9857         if interval <= 0:
9858             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
9859             return
9860 
9861         filters = {'vm_state': vm_states.SOFT_DELETED,
9862                    'task_state': None,
9863                    'host': self.host}
9864         instances = objects.InstanceList.get_by_filters(
9865             context, filters,
9866             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
9867             use_slave=True)
9868         for instance in instances:
9869             if self._deleted_old_enough(instance, interval):
9870                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9871                         context, instance.uuid)
9872                 LOG.info('Reclaiming deleted instance', instance=instance)
9873                 try:
9874                     self._delete_instance(context, instance, bdms)
9875                 except Exception as e:
9876                     LOG.warning("Periodic reclaim failed to delete "
9877                                 "instance: %s",
9878                                 e, instance=instance)
9879 
9880     def _get_nodename(self, instance, refresh=False):
9881         """Helper method to get the name of the first available node
9882         on this host. This method should not be used with any operations
9883         on ironic instances since it does not handle multiple nodes.
9884         """
9885         node = self.driver.get_available_nodes(refresh=refresh)[0]
9886         LOG.debug("No node specified, defaulting to %s", node,
9887                   instance=instance)
9888         return node
9889 
9890     def _update_available_resource_for_node(self, context, nodename,
9891                                             startup=False):
9892 
9893         try:
9894             self.rt.update_available_resource(context, nodename,
9895                                               startup=startup)
9896         except exception.ComputeHostNotFound:
9897             LOG.warning("Compute node '%s' not found in "
9898                         "update_available_resource.", nodename)
9899         except exception.ReshapeFailed:
9900             # We're only supposed to get here on startup, if a reshape was
9901             # needed, was attempted, and failed. We want to kill the service.
9902             with excutils.save_and_reraise_exception():
9903                 LOG.critical("Resource provider data migration failed "
9904                              "fatally during startup for node %s.", nodename)
9905         except exception.ReshapeNeeded:
9906             # This exception should only find its way here if the virt driver's
9907             # update_provider_tree raised it incorrectly: either
9908             # a) After the resource tracker already caught it once and
9909             # reinvoked update_provider_tree with allocations. At this point
9910             # the driver is just supposed to *do* the reshape, so if it raises
9911             # ReshapeNeeded, it's a bug, and we want to kill the compute
9912             # service.
9913             # b) On periodic rather than startup (we only allow reshapes to
9914             # happen on startup). In this case we'll just make the logs red and
9915             # go again at the next periodic interval, where the same thing may
9916             # or may not happen again. Depending on the previous and intended
9917             # shape of the providers/inventories, this may not actually cause
9918             # any immediately visible symptoms (in terms of scheduling, etc.)
9919             # If this becomes a problem, we may wish to make it pop immediately
9920             # (e.g. disable the service).
9921             with excutils.save_and_reraise_exception():
9922                 LOG.exception("ReshapeNeeded exception is unexpected here!")
9923         except Exception:
9924             LOG.exception("Error updating resources for node %(node)s.",
9925                           {'node': nodename})
9926 
9927     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
9928     def update_available_resource(self, context, startup=False):
9929         """See driver.get_available_resource()
9930 
9931         Periodic process that keeps that the compute host's understanding of
9932         resource availability and usage in sync with the underlying hypervisor.
9933 
9934         :param context: security context
9935         :param startup: True if this is being called when the nova-compute
9936             service is starting, False otherwise.
9937         """
9938         try:
9939             nodenames = set(self.driver.get_available_nodes())
9940         except exception.VirtDriverNotReady:
9941             LOG.warning("Virt driver is not ready.")
9942             return
9943 
9944         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
9945                                                             nodenames,
9946                                                             use_slave=True,
9947                                                             startup=startup)
9948 
9949         # Delete orphan compute node not reported by driver but still in db
9950         for cn in compute_nodes_in_db:
9951             if cn.hypervisor_hostname not in nodenames:
9952                 LOG.info("Deleting orphan compute node %(id)s "
9953                          "hypervisor host is %(hh)s, "
9954                          "nodes are %(nodes)s",
9955                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
9956                           'nodes': nodenames})
9957                 cn.destroy()
9958                 self.rt.remove_node(cn.hypervisor_hostname)
9959                 # Delete the corresponding resource provider in placement,
9960                 # along with any associated allocations.
9961                 try:
9962                     self.reportclient.delete_resource_provider(context, cn,
9963                                                                cascade=True)
9964                 except keystone_exception.ClientException as e:
9965                     LOG.error(
9966                         "Failed to delete compute node resource provider "
9967                         "for compute node %s: %s", cn.uuid, str(e))
9968 
9969         for nodename in nodenames:
9970             self._update_available_resource_for_node(context, nodename,
9971                                                      startup=startup)
9972 
9973     def _get_compute_nodes_in_db(self, context, nodenames, use_slave=False,
9974                                  startup=False):
9975         try:
9976             return objects.ComputeNodeList.get_all_by_host(context, self.host,
9977                                                            use_slave=use_slave)
9978         except exception.NotFound:
9979             # If the driver is not reporting any nodenames we should not
9980             # expect there to be compute nodes so we just return in that case.
9981             # For example, this could be an ironic compute and it is not
9982             # managing any nodes yet.
9983             if nodenames:
9984                 if startup:
9985                     LOG.warning(
9986                         "No compute node record found for host %s. If this is "
9987                         "the first time this service is starting on this "
9988                         "host, then you can ignore this warning.", self.host)
9989                 else:
9990                     LOG.error("No compute node record for host %s", self.host)
9991             return []
9992 
9993     @periodic_task.periodic_task(
9994         spacing=CONF.running_deleted_instance_poll_interval,
9995         run_immediately=True)
9996     def _cleanup_running_deleted_instances(self, context):
9997         """Cleanup any instances which are erroneously still running after
9998         having been deleted.
9999 
10000         Valid actions to take are:
10001 
10002             1. noop - do nothing
10003             2. log - log which instances are erroneously running
10004             3. reap - shutdown and cleanup any erroneously running instances
10005             4. shutdown - power off *and disable* any erroneously running
10006                           instances
10007 
10008         The use-case for this cleanup task is: for various reasons, it may be
10009         possible for the database to show an instance as deleted but for that
10010         instance to still be running on a host machine (see bug
10011         https://bugs.launchpad.net/nova/+bug/911366).
10012 
10013         This cleanup task is a cross-hypervisor utility for finding these
10014         zombied instances and either logging the discrepancy (likely what you
10015         should do in production), or automatically reaping the instances (more
10016         appropriate for dev environments).
10017         """
10018         action = CONF.running_deleted_instance_action
10019 
10020         if action == "noop":
10021             return
10022 
10023         # NOTE(sirp): admin contexts don't ordinarily return deleted records
10024         with utils.temporary_mutation(context, read_deleted="yes"):
10025 
10026             try:
10027                 instances = self._running_deleted_instances(context)
10028             except exception.VirtDriverNotReady:
10029                 # Since this task runs immediately on startup, if the
10030                 # hypervisor is not yet ready handle it gracefully.
10031                 LOG.debug('Unable to check for running deleted instances '
10032                           'at this time since the hypervisor is not ready.')
10033                 return
10034 
10035             for instance in instances:
10036                 if action == "log":
10037                     LOG.warning("Detected instance with name label "
10038                                 "'%s' which is marked as "
10039                                 "DELETED but still present on host.",
10040                                 instance.name, instance=instance)
10041 
10042                 elif action == 'shutdown':
10043                     LOG.info("Powering off instance with name label "
10044                              "'%s' which is marked as "
10045                              "DELETED but still present on host.",
10046                              instance.name, instance=instance)
10047                     try:
10048                         self.driver.power_off(instance)
10049                     except Exception:
10050                         LOG.warning("Failed to power off instance",
10051                                     instance=instance, exc_info=True)
10052 
10053                 elif action == 'reap':
10054                     LOG.info("Destroying instance with name label "
10055                              "'%s' which is marked as "
10056                              "DELETED but still present on host.",
10057                              instance.name, instance=instance)
10058                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
10059                         context, instance.uuid, use_slave=True)
10060                     self.instance_events.clear_events_for_instance(instance)
10061                     try:
10062                         self._shutdown_instance(context, instance, bdms,
10063                                                 notify=False)
10064                         self._cleanup_volumes(context, instance, bdms,
10065                                               detach=False)
10066                     except Exception as e:
10067                         LOG.warning("Periodic cleanup failed to delete "
10068                                     "instance: %s",
10069                                     e, instance=instance)
10070                 else:
10071                     raise Exception(_("Unrecognized value '%s'"
10072                                       " for CONF.running_deleted_"
10073                                       "instance_action") % action)
10074 
10075     def _running_deleted_instances(self, context):
10076         """Returns a list of instances nova thinks is deleted,
10077         but the hypervisor thinks is still running.
10078         """
10079         timeout = CONF.running_deleted_instance_timeout
10080         filters = {'deleted': True,
10081                    'soft_deleted': False}
10082         instances = self._get_instances_on_driver(context, filters)
10083         return [i for i in instances if self._deleted_old_enough(i, timeout)]
10084 
10085     def _deleted_old_enough(self, instance, timeout):
10086         deleted_at = instance.deleted_at
10087         if deleted_at:
10088             deleted_at = deleted_at.replace(tzinfo=None)
10089         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
10090 
10091     @contextlib.contextmanager
10092     def _error_out_instance_on_exception(self, context, instance,
10093                                          instance_state=vm_states.ACTIVE):
10094         """Context manager to set instance.vm_state after some operation raises
10095 
10096         Used to handle NotImplementedError and InstanceFaultRollback errors
10097         and reset the instance vm_state and task_state. The vm_state is set
10098         to the $instance_state parameter and task_state is set to None.
10099         For all other types of exceptions, the vm_state is set to ERROR and
10100         the task_state is left unchanged (although most callers will have the
10101         @reverts_task_state decorator which will set the task_state to None).
10102 
10103         Re-raises the original exception *except* in the case of
10104         InstanceFaultRollback in which case the wrapped `inner_exception` is
10105         re-raised.
10106 
10107         :param context: The nova auth request context for the operation.
10108         :param instance: The instance to update. The vm_state will be set by
10109             this context manager when an exception is raised.
10110         :param instance_state: For NotImplementedError and
10111             InstanceFaultRollback this is the vm_state to set the instance to
10112             when handling one of those types of exceptions. By default the
10113             instance will be set to ACTIVE, but the caller should control this
10114             in case there have been no changes to the running state of the
10115             instance. For example, resizing a stopped server where prep_resize
10116             fails early and does not change the power state of the guest should
10117             not set the instance status to ACTIVE but remain STOPPED.
10118             This parameter is ignored for all other types of exceptions and the
10119             instance vm_state is set to ERROR.
10120         """
10121         # NOTE(mriedem): Why doesn't this method just save off the
10122         # original instance.vm_state here rather than use a parameter? Or use
10123         # instance_state=None as an override but default to the current
10124         # vm_state when rolling back.
10125         instance_uuid = instance.uuid
10126         try:
10127             yield
10128         except (NotImplementedError, exception.InstanceFaultRollback) as error:
10129             # Use reraise=False to determine if we want to raise the original
10130             # exception or something else.
10131             with excutils.save_and_reraise_exception(reraise=False) as ctxt:
10132                 LOG.info("Setting instance back to %(state)s after: %(error)s",
10133                          {'state': instance_state, 'error': error},
10134                          instance_uuid=instance_uuid)
10135                 self._instance_update(context, instance,
10136                                       vm_state=instance_state,
10137                                       task_state=None)
10138                 if isinstance(error, exception.InstanceFaultRollback):
10139                     # Raise the wrapped exception.
10140                     raise error.inner_exception
10141                 # Else re-raise the NotImplementedError.
10142                 ctxt.reraise = True
10143         except Exception:
10144             LOG.exception('Setting instance vm_state to ERROR',
10145                           instance_uuid=instance_uuid)
10146             with excutils.save_and_reraise_exception():
10147                 # NOTE(mriedem): Why don't we pass clean_task_state=True here?
10148                 self._set_instance_obj_error_state(instance)
10149 
10150     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
10151     @wrap_exception()
10152     def add_aggregate_host(self, context, aggregate, host, slave_info):
10153         """(REMOVED) Notify hypervisor of change (for hypervisor pools)."""
10154         raise NotImplementedError()
10155 
10156     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
10157     @wrap_exception()
10158     def remove_aggregate_host(self, context, host, slave_info, aggregate):
10159         """(REMOVED) Removes a host from a physical hypervisor pool."""
10160         raise NotImplementedError()
10161 
10162     def _process_instance_event(self, instance, event):
10163         _event = self.instance_events.pop_instance_event(instance, event)
10164         if _event:
10165             LOG.debug('Processing event %(event)s',
10166                       {'event': event.key}, instance=instance)
10167             _event.send(event)
10168         else:
10169             # If it's a network-vif-unplugged event and the instance is being
10170             # deleted or live migrated then we don't need to make this a
10171             # warning as it's expected. There are other expected things which
10172             # could trigger this event like detaching an interface, but we
10173             # don't have a task state for that.
10174             # TODO(mriedem): We have other move operations and things like
10175             # hard reboot (probably rebuild as well) which trigger this event
10176             # but nothing listens for network-vif-unplugged. We should either
10177             # handle those other known cases or consider just not logging a
10178             # warning if we get this event and the instance is undergoing some
10179             # task state transition.
10180             if (event.name == 'network-vif-unplugged' and
10181                     instance.task_state in (
10182                         task_states.DELETING, task_states.MIGRATING)):
10183                 LOG.debug('Received event %s for instance with task_state %s.',
10184                           event.key, instance.task_state, instance=instance)
10185             else:
10186                 LOG.warning('Received unexpected event %(event)s for '
10187                             'instance with vm_state %(vm_state)s and '
10188                             'task_state %(task_state)s.',
10189                             {'event': event.key,
10190                              'vm_state': instance.vm_state,
10191                              'task_state': instance.task_state},
10192                             instance=instance)
10193 
10194     def _process_instance_vif_deleted_event(self, context, instance,
10195                                             deleted_vif_id):
10196         # If an attached port is deleted by neutron, it needs to
10197         # be detached from the instance.
10198         # And info cache needs to be updated.
10199         network_info = instance.info_cache.network_info
10200         for index, vif in enumerate(network_info):
10201             if vif['id'] == deleted_vif_id:
10202                 LOG.info('Neutron deleted interface %(intf)s; '
10203                          'detaching it from the instance and '
10204                          'deleting it from the info cache',
10205                          {'intf': vif['id']},
10206                          instance=instance)
10207                 profile = vif.get('profile', {}) or {}  # profile can be None
10208                 if profile.get('allocation'):
10209                     LOG.error(
10210                         'The bound port %(port_id)s is deleted in Neutron but '
10211                         'the resource allocation on the resource provider '
10212                         '%(rp_uuid)s is leaked until the server '
10213                         '%(server_uuid)s is deleted.',
10214                         {'port_id': vif['id'],
10215                          'rp_uuid': vif['profile']['allocation'],
10216                          'server_uuid': instance.uuid})
10217 
10218                 del network_info[index]
10219                 neutron.update_instance_cache_with_nw_info(
10220                     self.network_api, context, instance, nw_info=network_info)
10221                 try:
10222                     self.driver.detach_interface(context, instance, vif)
10223                 except NotImplementedError:
10224                     # Not all virt drivers support attach/detach of interfaces
10225                     # yet (like Ironic), so just ignore this.
10226                     pass
10227                 except exception.NovaException as ex:
10228                     # If the instance was deleted before the interface was
10229                     # detached, just log it at debug.
10230                     log_level = (logging.DEBUG
10231                                  if isinstance(ex, exception.InstanceNotFound)
10232                                  else logging.WARNING)
10233                     LOG.log(log_level,
10234                             "Detach interface failed, "
10235                             "port_id=%(port_id)s, reason: %(msg)s",
10236                             {'port_id': deleted_vif_id, 'msg': ex},
10237                             instance=instance)
10238                 break
10239 
10240     @wrap_instance_event(prefix='compute')
10241     @wrap_instance_fault
10242     def extend_volume(self, context, instance, extended_volume_id):
10243 
10244         # If an attached volume is extended by cinder, it needs to
10245         # be extended by virt driver so host can detect its new size.
10246         # And bdm needs to be updated.
10247         LOG.debug('Handling volume-extended event for volume %(vol)s',
10248                   {'vol': extended_volume_id}, instance=instance)
10249 
10250         try:
10251             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
10252                    context, extended_volume_id, instance.uuid)
10253         except exception.NotFound:
10254             LOG.warning('Extend volume failed, '
10255                         'volume %(vol)s is not attached to instance.',
10256                         {'vol': extended_volume_id},
10257                         instance=instance)
10258             return
10259 
10260         LOG.info('Cinder extended volume %(vol)s; '
10261                  'extending it to detect new size',
10262                  {'vol': extended_volume_id},
10263                  instance=instance)
10264         volume = self.volume_api.get(context, bdm.volume_id)
10265 
10266         if bdm.connection_info is None:
10267             LOG.warning('Extend volume failed, '
10268                         'attached volume %(vol)s has no connection_info',
10269                         {'vol': extended_volume_id},
10270                         instance=instance)
10271             return
10272 
10273         connection_info = jsonutils.loads(bdm.connection_info)
10274         bdm.volume_size = volume['size']
10275         bdm.save()
10276 
10277         if not self.driver.capabilities.get('supports_extend_volume', False):
10278             raise exception.ExtendVolumeNotSupported()
10279 
10280         try:
10281             self.driver.extend_volume(context, connection_info, instance,
10282                                       bdm.volume_size * units.Gi)
10283         except Exception as ex:
10284             LOG.warning('Extend volume failed, '
10285                         'volume_id=%(volume_id)s, reason: %(msg)s',
10286                         {'volume_id': extended_volume_id, 'msg': ex},
10287                         instance=instance)
10288             raise
10289 
10290     @staticmethod
10291     def _is_state_valid_for_power_update_event(instance, target_power_state):
10292         """Check if the current state of the instance allows it to be
10293         a candidate for the power-update event.
10294 
10295         :param instance: The nova instance object.
10296         :param target_power_state: The desired target power state; this should
10297                                    either be "POWER_ON" or "POWER_OFF".
10298         :returns Boolean: True if the instance can be subjected to the
10299                           power-update event.
10300         """
10301         if ((target_power_state == external_event_obj.POWER_ON and
10302                 instance.task_state is None and
10303                 instance.vm_state == vm_states.STOPPED and
10304                 instance.power_state == power_state.SHUTDOWN) or
10305             (target_power_state == external_event_obj.POWER_OFF and
10306                 instance.task_state is None and
10307                 instance.vm_state == vm_states.ACTIVE and
10308                 instance.power_state == power_state.RUNNING)):
10309             return True
10310         return False
10311 
10312     @wrap_exception()
10313     @reverts_task_state
10314     @wrap_instance_event(prefix='compute')
10315     @wrap_instance_fault
10316     def power_update(self, context, instance, target_power_state):
10317         """Power update of an instance prompted by an external event.
10318         :param context: The API request context.
10319         :param instance: The nova instance object.
10320         :param target_power_state: The desired target power state;
10321                                    this should either be "POWER_ON" or
10322                                    "POWER_OFF".
10323         """
10324 
10325         @utils.synchronized(instance.uuid)
10326         def do_power_update():
10327             LOG.debug('Handling power-update event with target_power_state %s '
10328                       'for instance', target_power_state, instance=instance)
10329             if not self._is_state_valid_for_power_update_event(
10330                     instance, target_power_state):
10331                 pow_state = fields.InstancePowerState.from_index(
10332                     instance.power_state)
10333                 LOG.info('The power-update %(tag)s event for instance '
10334                          '%(uuid)s is a no-op since the instance is in '
10335                          'vm_state %(vm_state)s, task_state '
10336                          '%(task_state)s and power_state '
10337                          '%(power_state)s.',
10338                          {'tag': target_power_state, 'uuid': instance.uuid,
10339                          'vm_state': instance.vm_state,
10340                          'task_state': instance.task_state,
10341                          'power_state': pow_state})
10342                 return
10343             LOG.debug("Trying to %s instance",
10344                       target_power_state, instance=instance)
10345             if target_power_state == external_event_obj.POWER_ON:
10346                 action = fields.NotificationAction.POWER_ON
10347                 notification_name = "power_on."
10348                 instance.task_state = task_states.POWERING_ON
10349             else:
10350                 # It's POWER_OFF
10351                 action = fields.NotificationAction.POWER_OFF
10352                 notification_name = "power_off."
10353                 instance.task_state = task_states.POWERING_OFF
10354                 instance.progress = 0
10355 
10356             try:
10357                 # Note that the task_state is set here rather than the API
10358                 # because this is a best effort operation and deferring
10359                 # updating the task_state until we get to the compute service
10360                 # avoids error handling in the API and needing to account for
10361                 # older compute services during rolling upgrades from Stein.
10362                 # If we lose a race, UnexpectedTaskStateError is handled
10363                 # below.
10364                 instance.save(expected_task_state=[None])
10365                 self._notify_about_instance_usage(context, instance,
10366                                                   notification_name + "start")
10367                 compute_utils.notify_about_instance_action(context, instance,
10368                     self.host, action=action,
10369                     phase=fields.NotificationPhase.START)
10370                 # UnexpectedTaskStateError raised from the driver will be
10371                 # handled below and not result in a fault, error notification
10372                 # or failure of the instance action. Other driver errors like
10373                 # NotImplementedError will be record a fault, send an error
10374                 # notification and mark the instance action as failed.
10375                 self.driver.power_update_event(instance, target_power_state)
10376                 self._notify_about_instance_usage(context, instance,
10377                                                   notification_name + "end")
10378                 compute_utils.notify_about_instance_action(context, instance,
10379                     self.host, action=action,
10380                     phase=fields.NotificationPhase.END)
10381             except exception.UnexpectedTaskStateError as e:
10382                 # Handling the power-update event is best effort and if we lost
10383                 # a race with some other action happening to the instance we
10384                 # just log it and return rather than fail the action.
10385                 LOG.info("The power-update event was possibly preempted: %s ",
10386                          e.format_message(), instance=instance)
10387                 return
10388         do_power_update()
10389 
10390     @wrap_exception()
10391     def external_instance_event(self, context, instances, events):
10392         # NOTE(danms): Some event types are handled by the manager, such
10393         # as when we're asked to update the instance's info_cache. If it's
10394         # not one of those, look for some thread(s) waiting for the event and
10395         # unblock them if so.
10396         for event in events:
10397             instance = [inst for inst in instances
10398                         if inst.uuid == event.instance_uuid][0]
10399             LOG.debug('Received event %(event)s',
10400                       {'event': event.key},
10401                       instance=instance)
10402             if event.name == 'network-changed':
10403                 try:
10404                     LOG.debug('Refreshing instance network info cache due to '
10405                               'event %s.', event.key, instance=instance)
10406                     self.network_api.get_instance_nw_info(
10407                         context, instance, refresh_vif_id=event.tag)
10408                 except exception.NotFound as e:
10409                     LOG.info('Failed to process external instance event '
10410                              '%(event)s due to: %(error)s',
10411                              {'event': event.key, 'error': str(e)},
10412                              instance=instance)
10413             elif event.name == 'network-vif-deleted':
10414                 try:
10415                     self._process_instance_vif_deleted_event(context,
10416                                                              instance,
10417                                                              event.tag)
10418                 except exception.NotFound as e:
10419                     LOG.info('Failed to process external instance event '
10420                              '%(event)s due to: %(error)s',
10421                              {'event': event.key, 'error': str(e)},
10422                              instance=instance)
10423             elif event.name == 'volume-extended':
10424                 self.extend_volume(context, instance, event.tag)
10425             elif event.name == 'power-update':
10426                 self.power_update(context, instance, event.tag)
10427             else:
10428                 self._process_instance_event(instance, event)
10429 
10430     @periodic_task.periodic_task(spacing=CONF.image_cache.manager_interval,
10431                                  external_process_ok=True)
10432     def _run_image_cache_manager_pass(self, context):
10433         """Run a single pass of the image cache manager."""
10434 
10435         if not self.driver.capabilities.get("has_imagecache", False):
10436             return
10437 
10438         # Determine what other nodes use this storage
10439         storage_users.register_storage_use(CONF.instances_path, CONF.host)
10440         nodes = storage_users.get_storage_users(CONF.instances_path)
10441 
10442         # Filter all_instances to only include those nodes which share this
10443         # storage path.
10444         # TODO(mikal): this should be further refactored so that the cache
10445         # cleanup code doesn't know what those instances are, just a remote
10446         # count, and then this logic should be pushed up the stack.
10447         filters = {'deleted': False,
10448                    'soft_deleted': True,
10449                    'host': nodes}
10450         filtered_instances = objects.InstanceList.get_by_filters(context,
10451                                  filters, expected_attrs=[], use_slave=True)
10452 
10453         self.driver.manage_image_cache(context, filtered_instances)
10454 
10455     def cache_images(self, context, image_ids):
10456         """Ask the virt driver to pre-cache a set of base images.
10457 
10458         :param context: The RequestContext
10459         :param image_ids: The image IDs to be cached
10460         :return: A dict, keyed by image-id where the values are one of:
10461                  'cached' if the image was downloaded,
10462                  'existing' if the image was already in the cache,
10463                  'unsupported' if the virt driver does not support caching,
10464                  'error' if the virt driver raised an exception.
10465         """
10466 
10467         results = {}
10468 
10469         LOG.info('Caching %i image(s) by request', len(image_ids))
10470         for image_id in image_ids:
10471             try:
10472                 cached = self.driver.cache_image(context, image_id)
10473                 if cached:
10474                     results[image_id] = 'cached'
10475                 else:
10476                     results[image_id] = 'existing'
10477             except NotImplementedError:
10478                 LOG.warning('Virt driver does not support image pre-caching;'
10479                             ' ignoring request')
10480                 # NOTE(danms): Yes, technically we could short-circuit here to
10481                 # avoid trying the rest of the images, but it's very cheap to
10482                 # just keep hitting the NotImplementedError to keep the logic
10483                 # clean.
10484                 results[image_id] = 'unsupported'
10485             except Exception as e:
10486                 results[image_id] = 'error'
10487                 LOG.error('Failed to cache image %(image_id)s: %(err)s',
10488                           {'image_id': image_id,
10489                            'err': e})
10490 
10491         return results
10492 
10493     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10494     def _run_pending_deletes(self, context):
10495         """Retry any pending instance file deletes."""
10496         LOG.debug('Cleaning up deleted instances')
10497         filters = {'deleted': True,
10498                    'soft_deleted': False,
10499                    'host': CONF.host,
10500                    'cleaned': False}
10501         attrs = ['system_metadata']
10502         with utils.temporary_mutation(context, read_deleted='yes'):
10503             instances = objects.InstanceList.get_by_filters(
10504                 context, filters, expected_attrs=attrs, use_slave=True)
10505         LOG.debug('There are %d instances to clean', len(instances))
10506 
10507         for instance in instances:
10508             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
10509             LOG.debug('Instance has had %(attempts)s of %(max)s '
10510                       'cleanup attempts',
10511                       {'attempts': attempts,
10512                        'max': CONF.maximum_instance_delete_attempts},
10513                       instance=instance)
10514             if attempts < CONF.maximum_instance_delete_attempts:
10515                 success = self.driver.delete_instance_files(instance)
10516 
10517                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
10518                 if success:
10519                     instance.cleaned = True
10520                 with utils.temporary_mutation(context, read_deleted='yes'):
10521                     instance.save()
10522 
10523     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10524     def _cleanup_incomplete_migrations(self, context):
10525         """Cleanup on failed resize/revert-resize operation and
10526         failed rollback live migration operation.
10527 
10528         During resize/revert-resize operation, or after a failed rollback
10529         live migration operation, if that instance gets deleted then instance
10530         files might remain either on source or destination compute node and
10531         other specific resources might not be cleaned up because of the race
10532         condition.
10533         """
10534         LOG.debug('Cleaning up deleted instances with incomplete migration ')
10535         migration_filters = {'host': CONF.host,
10536                              'status': 'error'}
10537         migrations = objects.MigrationList.get_by_filters(context,
10538                                                           migration_filters)
10539 
10540         if not migrations:
10541             return
10542 
10543         inst_uuid_from_migrations = set([migration.instance_uuid for migration
10544                                          in migrations])
10545 
10546         inst_filters = {'deleted': True, 'soft_deleted': False,
10547                         'uuid': inst_uuid_from_migrations}
10548         attrs = ['info_cache', 'security_groups', 'system_metadata']
10549         with utils.temporary_mutation(context, read_deleted='yes'):
10550             instances = objects.InstanceList.get_by_filters(
10551                 context, inst_filters, expected_attrs=attrs, use_slave=True)
10552 
10553         for instance in instances:
10554             if instance.host == CONF.host:
10555                 continue
10556             for migration in migrations:
10557                 if instance.uuid != migration.instance_uuid:
10558                     continue
10559                 self.driver.delete_instance_files(instance)
10560                 # we are not sure whether the migration_context is applied
10561                 # during incompleted migrating, we need to apply/revert
10562                 # migration_context to get instance object content matching
10563                 # current host.
10564                 revert = (True if migration.source_compute == CONF.host
10565                           else False)
10566                 with instance.mutated_migration_context(revert=revert):
10567                     self.driver.cleanup_lingering_instance_resources(instance)
10568 
10569                 try:
10570                     migration.status = 'failed'
10571                     migration.save()
10572                 except exception.MigrationNotFound:
10573                     LOG.warning("Migration %s is not found.",
10574                                 migration.id,
10575                                 instance=instance)
10576                 break
10577 
10578     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10579                                    exception.QemuGuestAgentNotEnabled,
10580                                    exception.NovaException,
10581                                    NotImplementedError)
10582     @wrap_exception()
10583     def quiesce_instance(self, context, instance):
10584         """Quiesce an instance on this host."""
10585         context = context.elevated()
10586         image_meta = objects.ImageMeta.from_instance(instance)
10587         self.driver.quiesce(context, instance, image_meta)
10588 
10589     def _wait_for_snapshots_completion(self, context, mapping):
10590         for mapping_dict in mapping:
10591             if mapping_dict.get('source_type') == 'snapshot':
10592 
10593                 def _wait_snapshot():
10594                     snapshot = self.volume_api.get_snapshot(
10595                         context, mapping_dict['snapshot_id'])
10596                     if snapshot.get('status') != 'creating':
10597                         raise loopingcall.LoopingCallDone()
10598 
10599                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
10600                 timer.start(interval=0.5).wait()
10601 
10602     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10603                                    exception.QemuGuestAgentNotEnabled,
10604                                    exception.NovaException,
10605                                    NotImplementedError)
10606     @wrap_exception()
10607     def unquiesce_instance(self, context, instance, mapping=None):
10608         """Unquiesce an instance on this host.
10609 
10610         If snapshots' image mapping is provided, it waits until snapshots are
10611         completed before unqueiscing.
10612         """
10613         context = context.elevated()
10614         if mapping:
10615             try:
10616                 self._wait_for_snapshots_completion(context, mapping)
10617             except Exception as error:
10618                 LOG.exception("Exception while waiting completion of "
10619                               "volume snapshots: %s",
10620                               error, instance=instance)
10621         image_meta = objects.ImageMeta.from_instance(instance)
10622         self.driver.unquiesce(context, instance, image_meta)
10623 
10624     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10625     def _cleanup_expired_console_auth_tokens(self, context):
10626         """Remove all expired console auth tokens.
10627 
10628         Console authorization tokens and their connection data are stored
10629         in the database when a user asks for a console connection to an
10630         instance. After a time they expire. We periodically remove any expired
10631         tokens from the database.
10632         """
10633         objects.ConsoleAuthToken.clean_expired_console_auths(context)
10634 
10635     def _claim_pci_for_instance_vifs(self, ctxt, instance):
10636         """Claim PCI devices for the instance's VIFs on the compute node
10637 
10638         :param ctxt: Context
10639         :param instance: Instance object
10640         :return: <port ID: PciDevice> mapping for the VIFs that yielded a
10641                 PCI claim on the compute node
10642         """
10643         pci_req_id_to_port_id = {}
10644         pci_reqs = []
10645         port_id_to_pci_dev = {}
10646 
10647         for vif in instance.get_network_info():
10648             pci_req = pci_req_module.get_instance_pci_request_from_vif(
10649                 ctxt,
10650                 instance,
10651                 vif)
10652             if pci_req:
10653                 pci_req_id_to_port_id[pci_req.request_id] = vif['id']
10654                 pci_reqs.append(pci_req)
10655 
10656         if pci_reqs:
10657             # Create PCI requests and claim against PCI resource tracker
10658             # NOTE(adrianc): We claim against the same requests as on the
10659             # source node.
10660             vif_pci_requests = objects.InstancePCIRequests(
10661                 requests=pci_reqs,
10662                 instance_uuid=instance.uuid)
10663 
10664             # if we are called during the live migration with NUMA topology
10665             # support the PCI claim needs to consider the destination NUMA
10666             # topology that is then stored in the migration_context
10667             dest_topo = None
10668             if instance.migration_context:
10669                 dest_topo = instance.migration_context.new_numa_topology
10670 
10671             claimed_pci_devices_objs = self.rt.claim_pci_devices(
10672                 ctxt, vif_pci_requests, dest_topo)
10673 
10674             # Update VIFMigrateData profile with the newly claimed PCI
10675             # device
10676             for pci_dev in claimed_pci_devices_objs:
10677                 LOG.debug("PCI device: %s Claimed on destination node",
10678                           pci_dev.address)
10679                 port_id = pci_req_id_to_port_id[pci_dev.request_id]
10680                 port_id_to_pci_dev[port_id] = pci_dev
10681 
10682         return port_id_to_pci_dev
10683 
10684     def _update_migrate_vifs_profile_with_pci(self,
10685                                               migrate_vifs,
10686                                               port_id_to_pci_dev):
10687         """Update migrate vifs profile with the claimed PCI devices
10688 
10689         :param migrate_vifs: list of VIFMigrateData objects
10690         :param port_id_to_pci_dev: a <port_id: PciDevice> mapping
10691         :return: None.
10692         """
10693         for mig_vif in migrate_vifs:
10694             port_id = mig_vif.port_id
10695             if port_id not in port_id_to_pci_dev:
10696                 continue
10697 
10698             pci_dev = port_id_to_pci_dev[port_id]
10699             profile = copy.deepcopy(mig_vif.source_vif['profile'])
10700             profile['pci_slot'] = pci_dev.address
10701             profile['pci_vendor_info'] = ':'.join([pci_dev.vendor_id,
10702                                                    pci_dev.product_id])
10703             mig_vif.profile = profile
10704             LOG.debug("Updating migrate VIF profile for port %(port_id)s:"
10705                       "%(profile)s", {'port_id': port_id,
10706                                       'profile': profile})
