Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import copy
32 import functools
33 import inspect
34 import sys
35 import time
36 import traceback
37 
38 from cinderclient import exceptions as cinder_exception
39 from cursive import exception as cursive_exception
40 import eventlet.event
41 from eventlet import greenthread
42 import eventlet.semaphore
43 import eventlet.timeout
44 import futurist
45 from keystoneauth1 import exceptions as keystone_exception
46 import os_traits
47 from oslo_log import log as logging
48 import oslo_messaging as messaging
49 from oslo_serialization import jsonutils
50 from oslo_service import loopingcall
51 from oslo_service import periodic_task
52 from oslo_utils import excutils
53 from oslo_utils import strutils
54 from oslo_utils import timeutils
55 from oslo_utils import units
56 import six
57 from six.moves import range
58 
59 from nova import block_device
60 from nova.compute import api as compute
61 from nova.compute import build_results
62 from nova.compute import claims
63 from nova.compute import power_state
64 from nova.compute import resource_tracker
65 from nova.compute import rpcapi as compute_rpcapi
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 from nova.compute.utils import wrap_instance_event
69 from nova.compute import vm_states
70 from nova import conductor
71 import nova.conf
72 import nova.context
73 from nova import exception
74 from nova import exception_wrapper
75 from nova import hooks
76 from nova.i18n import _
77 from nova import image
78 from nova import manager
79 from nova import network
80 from nova.network import base_api as base_net_api
81 from nova.network import model as network_model
82 from nova.network.security_group import openstack_driver
83 from nova import objects
84 from nova.objects import base as obj_base
85 from nova.objects import external_event as external_event_obj
86 from nova.objects import fields
87 from nova.objects import instance as obj_instance
88 from nova.objects import migrate_data as migrate_data_obj
89 from nova.pci import request as pci_req_module
90 from nova.pci import whitelist
91 from nova import rpc
92 from nova import safe_utils
93 from nova.scheduler.client import query
94 from nova.scheduler.client import report
95 from nova.scheduler import utils as scheduler_utils
96 from nova import utils
97 from nova.virt import block_device as driver_block_device
98 from nova.virt import configdrive
99 from nova.virt import driver
100 from nova.virt import event as virtevent
101 from nova.virt import hardware
102 from nova.virt import storage_users
103 from nova.virt import virtapi
104 from nova.volume import cinder
105 
106 CONF = nova.conf.CONF
107 
108 LOG = logging.getLogger(__name__)
109 
110 get_notifier = functools.partial(rpc.get_notifier, service='compute')
111 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
112                                    get_notifier=get_notifier,
113                                    binary='nova-compute')
114 
115 
116 @contextlib.contextmanager
117 def errors_out_migration_ctxt(migration):
118     """Context manager to error out migration on failure."""
119 
120     try:
121         yield
122     except Exception:
123         with excutils.save_and_reraise_exception():
124             if migration:
125                 # We may have been passed None for our migration if we're
126                 # receiving from an older client. The migration will be
127                 # errored via the legacy path.
128                 migration.status = 'error'
129                 try:
130                     migration.save()
131                 except Exception:
132                     LOG.debug(
133                         'Error setting migration status for instance %s.',
134                         migration.instance_uuid, exc_info=True)
135 
136 
137 @utils.expects_func_args('migration')
138 def errors_out_migration(function):
139     """Decorator to error out migration on failure."""
140 
141     @functools.wraps(function)
142     def decorated_function(self, context, *args, **kwargs):
143         wrapped_func = safe_utils.get_wrapped_function(function)
144         keyed_args = inspect.getcallargs(wrapped_func, self, context,
145                                          *args, **kwargs)
146         migration = keyed_args['migration']
147         with errors_out_migration_ctxt(migration):
148             return function(self, context, *args, **kwargs)
149 
150     return decorated_function
151 
152 
153 @utils.expects_func_args('instance')
154 def reverts_task_state(function):
155     """Decorator to revert task_state on failure."""
156 
157     @functools.wraps(function)
158     def decorated_function(self, context, *args, **kwargs):
159         try:
160             return function(self, context, *args, **kwargs)
161         except exception.UnexpectedTaskStateError as e:
162             # Note(maoy): unexpected task state means the current
163             # task is preempted. Do not clear task state in this
164             # case.
165             with excutils.save_and_reraise_exception():
166                 LOG.info("Task possibly preempted: %s",
167                          e.format_message())
168         except Exception:
169             with excutils.save_and_reraise_exception():
170                 wrapped_func = safe_utils.get_wrapped_function(function)
171                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
172                                                  *args, **kwargs)
173                 # NOTE(mriedem): 'instance' must be in keyed_args because we
174                 # have utils.expects_func_args('instance') decorating this
175                 # method.
176                 instance = keyed_args['instance']
177                 original_task_state = instance.task_state
178                 try:
179                     self._instance_update(context, instance, task_state=None)
180                     LOG.info("Successfully reverted task state from %s on "
181                              "failure for instance.",
182                              original_task_state, instance=instance)
183                 except exception.InstanceNotFound:
184                     # We might delete an instance that failed to build shortly
185                     # after it errored out this is an expected case and we
186                     # should not trace on it.
187                     pass
188                 except Exception as e:
189                     LOG.warning("Failed to revert task state for instance. "
190                                 "Error: %s", e, instance=instance)
191 
192     return decorated_function
193 
194 
195 @utils.expects_func_args('instance')
196 def wrap_instance_fault(function):
197     """Wraps a method to catch exceptions related to instances.
198 
199     This decorator wraps a method to catch any exceptions having to do with
200     an instance that may get thrown. It then logs an instance fault in the db.
201     """
202 
203     @functools.wraps(function)
204     def decorated_function(self, context, *args, **kwargs):
205         try:
206             return function(self, context, *args, **kwargs)
207         except exception.InstanceNotFound:
208             raise
209         except Exception as e:
210             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
211             # we will get a KeyError exception which will cover up the real
212             # exception. So, we update kwargs with the values from args first.
213             # then, we can get 'instance' from kwargs easily.
214             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
215 
216             with excutils.save_and_reraise_exception():
217                 compute_utils.add_instance_fault_from_exc(context,
218                         kwargs['instance'], e, sys.exc_info())
219 
220     return decorated_function
221 
222 
223 @utils.expects_func_args('image_id', 'instance')
224 def delete_image_on_error(function):
225     """Used for snapshot related method to ensure the image created in
226     compute.api is deleted when an error occurs.
227     """
228 
229     @functools.wraps(function)
230     def decorated_function(self, context, image_id, instance,
231                            *args, **kwargs):
232         try:
233             return function(self, context, image_id, instance,
234                             *args, **kwargs)
235         except Exception:
236             with excutils.save_and_reraise_exception():
237                 compute_utils.delete_image(
238                     context, instance, self.image_api, image_id,
239                     log_exc_info=True)
240 
241     return decorated_function
242 
243 
244 # TODO(danms): Remove me after Icehouse
245 # TODO(alaski): Actually remove this after Newton, assuming a major RPC bump
246 # NOTE(mikal): if the method being decorated has more than one decorator, then
247 # put this one first. Otherwise the various exception handling decorators do
248 # not function correctly.
249 def object_compat(function):
250     """Wraps a method that expects a new-world instance
251 
252     This provides compatibility for callers passing old-style dict
253     instances.
254     """
255 
256     @functools.wraps(function)
257     def decorated_function(self, context, *args, **kwargs):
258         def _load_instance(instance_or_dict):
259             if isinstance(instance_or_dict, dict):
260                 # try to get metadata and system_metadata for most cases but
261                 # only attempt to load those if the db instance already has
262                 # those fields joined
263                 metas = [meta for meta in ('metadata', 'system_metadata')
264                          if meta in instance_or_dict]
265                 instance = objects.Instance._from_db_object(
266                     context, objects.Instance(), instance_or_dict,
267                     expected_attrs=metas)
268                 instance._context = context
269                 return instance
270             return instance_or_dict
271 
272         try:
273             kwargs['instance'] = _load_instance(kwargs['instance'])
274         except KeyError:
275             args = (_load_instance(args[0]),) + args[1:]
276 
277         migration = kwargs.get('migration')
278         if isinstance(migration, dict):
279             migration = objects.Migration._from_db_object(
280                     context.elevated(), objects.Migration(),
281                     migration)
282             kwargs['migration'] = migration
283 
284         return function(self, context, *args, **kwargs)
285 
286     return decorated_function
287 
288 
289 class InstanceEvents(object):
290     def __init__(self):
291         self._events = {}
292 
293     @staticmethod
294     def _lock_name(instance):
295         return '%s-%s' % (instance.uuid, 'events')
296 
297     def prepare_for_instance_event(self, instance, name, tag):
298         """Prepare to receive an event for an instance.
299 
300         This will register an event for the given instance that we will
301         wait on later. This should be called before initiating whatever
302         action will trigger the event. The resulting eventlet.event.Event
303         object should be wait()'d on to ensure completion.
304 
305         :param instance: the instance for which the event will be generated
306         :param name: the name of the event we're expecting
307         :param tag: the tag associated with the event we're expecting
308         :returns: an event object that should be wait()'d on
309         """
310         if self._events is None:
311             # NOTE(danms): We really should have a more specific error
312             # here, but this is what we use for our default error case
313             raise exception.NovaException('In shutdown, no new events '
314                                           'can be scheduled')
315 
316         @utils.synchronized(self._lock_name(instance))
317         def _create_or_get_event():
318             instance_events = self._events.setdefault(instance.uuid, {})
319             return instance_events.setdefault((name, tag),
320                                               eventlet.event.Event())
321         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
322                   {'name': name, 'tag': tag}, instance=instance)
323         return _create_or_get_event()
324 
325     def pop_instance_event(self, instance, event):
326         """Remove a pending event from the wait list.
327 
328         This will remove a pending event from the wait list so that it
329         can be used to signal the waiters to wake up.
330 
331         :param instance: the instance for which the event was generated
332         :param event: the nova.objects.external_event.InstanceExternalEvent
333                       that describes the event
334         :returns: the eventlet.event.Event object on which the waiters
335                   are blocked
336         """
337         no_events_sentinel = object()
338         no_matching_event_sentinel = object()
339 
340         @utils.synchronized(self._lock_name(instance))
341         def _pop_event():
342             if self._events is None:
343                 LOG.debug('Unexpected attempt to pop events during shutdown',
344                           instance=instance)
345                 return no_events_sentinel
346             events = self._events.get(instance.uuid)
347             if not events:
348                 return no_events_sentinel
349             _event = events.pop((event.name, event.tag), None)
350             if not events:
351                 del self._events[instance.uuid]
352             if _event is None:
353                 return no_matching_event_sentinel
354             return _event
355 
356         result = _pop_event()
357         if result is no_events_sentinel:
358             LOG.debug('No waiting events found dispatching %(event)s',
359                       {'event': event.key},
360                       instance=instance)
361             return None
362         elif result is no_matching_event_sentinel:
363             LOG.debug('No event matching %(event)s in %(events)s',
364                       {'event': event.key,
365                        'events': self._events.get(instance.uuid, {}).keys()},
366                       instance=instance)
367             return None
368         else:
369             return result
370 
371     def clear_events_for_instance(self, instance):
372         """Remove all pending events for an instance.
373 
374         This will remove all events currently pending for an instance
375         and return them (indexed by event name).
376 
377         :param instance: the instance for which events should be purged
378         :returns: a dictionary of {event_name: eventlet.event.Event}
379         """
380         @utils.synchronized(self._lock_name(instance))
381         def _clear_events():
382             if self._events is None:
383                 LOG.debug('Unexpected attempt to clear events during shutdown',
384                           instance=instance)
385                 return dict()
386             # NOTE(danms): We have historically returned the raw internal
387             # format here, which is {event.key: [events, ...])} so just
388             # trivially convert it here.
389             return {'%s-%s' % k: e
390                     for k, e in self._events.pop(instance.uuid, {}).items()}
391         return _clear_events()
392 
393     def cancel_all_events(self):
394         if self._events is None:
395             LOG.debug('Unexpected attempt to cancel events during shutdown.')
396             return
397         our_events = self._events
398         # NOTE(danms): Block new events
399         self._events = None
400 
401         for instance_uuid, events in our_events.items():
402             for (name, tag), eventlet_event in events.items():
403                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
404                           'instance %(instance_uuid)s',
405                           {'name': name,
406                            'tag': tag,
407                            'instance_uuid': instance_uuid})
408                 event = objects.InstanceExternalEvent(
409                     instance_uuid=instance_uuid,
410                     name=name, status='failed',
411                     tag=tag, data={})
412                 eventlet_event.send(event)
413 
414 
415 class ComputeVirtAPI(virtapi.VirtAPI):
416     def __init__(self, compute):
417         super(ComputeVirtAPI, self).__init__()
418         self._compute = compute
419         self.reportclient = compute.reportclient
420 
421         class ExitEarly(Exception):
422             def __init__(self, events):
423                 super(Exception, self).__init__()
424                 self.events = events
425 
426         self._exit_early_exc = ExitEarly
427 
428     def exit_wait_early(self, events):
429         """Exit a wait_for_instance_event() immediately and avoid
430         waiting for some events.
431 
432         :param: events: A list of (name, tag) tuples for events that we should
433                         skip waiting for during a wait_for_instance_event().
434         """
435         raise self._exit_early_exc(events=events)
436 
437     def _default_error_callback(self, event_name, instance):
438         raise exception.NovaException(_('Instance event failed'))
439 
440     @contextlib.contextmanager
441     def wait_for_instance_event(self, instance, event_names, deadline=300,
442                                 error_callback=None):
443         """Plan to wait for some events, run some code, then wait.
444 
445         This context manager will first create plans to wait for the
446         provided event_names, yield, and then wait for all the scheduled
447         events to complete.
448 
449         Note that this uses an eventlet.timeout.Timeout to bound the
450         operation, so callers should be prepared to catch that
451         failure and handle that situation appropriately.
452 
453         If the event is not received by the specified timeout deadline,
454         eventlet.timeout.Timeout is raised.
455 
456         If the event is received but did not have a 'completed'
457         status, a NovaException is raised.  If an error_callback is
458         provided, instead of raising an exception as detailed above
459         for the failure case, the callback will be called with the
460         event_name and instance, and can return True to continue
461         waiting for the rest of the events, False to stop processing,
462         or raise an exception which will bubble up to the waiter.
463 
464         If the inner code wishes to abort waiting for one or more
465         events because it knows some state to be finished or condition
466         to be satisfied, it can use VirtAPI.exit_wait_early() with a
467         list of event (name,tag) items to avoid waiting for those
468         events upon context exit. Note that exit_wait_early() exits
469         the context immediately and should be used to signal that all
470         work has been completed and provide the unified list of events
471         that need not be waited for. Waiting for the remaining events
472         will begin immediately upon early exit as if the context was
473         exited normally.
474 
475         :param instance: The instance for which an event is expected
476         :param event_names: A list of event names. Each element is a
477                             tuple of strings to indicate (name, tag),
478                             where name is required, but tag may be None.
479         :param deadline: Maximum number of seconds we should wait for all
480                          of the specified events to arrive.
481         :param error_callback: A function to be called if an event arrives
482 
483         """
484 
485         if error_callback is None:
486             error_callback = self._default_error_callback
487         events = {}
488         for event_name in event_names:
489             name, tag = event_name
490             event_name = objects.InstanceExternalEvent.make_key(name, tag)
491             try:
492                 events[event_name] = (
493                     self._compute.instance_events.prepare_for_instance_event(
494                         instance, name, tag))
495             except exception.NovaException:
496                 error_callback(event_name, instance)
497                 # NOTE(danms): Don't wait for any of the events. They
498                 # should all be canceled and fired immediately below,
499                 # but don't stick around if not.
500                 deadline = 0
501         try:
502             yield
503         except self._exit_early_exc as e:
504             early_events = set([objects.InstanceExternalEvent.make_key(n, t)
505                                 for n, t in e.events])
506         else:
507             early_events = []
508 
509         with eventlet.timeout.Timeout(deadline):
510             for event_name, event in events.items():
511                 if event_name in early_events:
512                     continue
513                 else:
514                     actual_event = event.wait()
515                     if actual_event.status == 'completed':
516                         continue
517                 # If we get here, we have an event that was not completed,
518                 # nor skipped via exit_wait_early(). Decide whether to
519                 # keep waiting by calling the error_callback() hook.
520                 decision = error_callback(event_name, instance)
521                 if decision is False:
522                     break
523 
524     def update_compute_provider_status(self, context, rp_uuid, enabled):
525         """Used to add/remove the COMPUTE_STATUS_DISABLED trait on the provider
526 
527         :param context: nova auth RequestContext
528         :param rp_uuid: UUID of a compute node resource provider in Placement
529         :param enabled: True if the node is enabled in which case the trait
530             would be removed, False if the node is disabled in which case
531             the trait would be added.
532         :raises: ResourceProviderTraitRetrievalFailed
533         :raises: ResourceProviderUpdateConflict
534         :raises: ResourceProviderUpdateFailed
535         :raises: TraitRetrievalFailed
536         :raises: keystoneauth1.exceptions.ClientException
537         """
538         trait_name = os_traits.COMPUTE_STATUS_DISABLED
539         # Get the current traits (and generation) for the provider.
540         # TODO(mriedem): Leverage the ProviderTree cache in get_provider_traits
541         trait_info = self.reportclient.get_provider_traits(context, rp_uuid)
542         # If the host is enabled, remove the trait (if set), else add
543         # the trait if it doesn't already exist.
544         original_traits = trait_info.traits
545         new_traits = None
546         if enabled and trait_name in original_traits:
547             new_traits = original_traits - {trait_name}
548             LOG.debug('Removing trait %s from compute node resource '
549                       'provider %s in placement.', trait_name, rp_uuid)
550         elif not enabled and trait_name not in original_traits:
551             new_traits = original_traits | {trait_name}
552             LOG.debug('Adding trait %s to compute node resource '
553                       'provider %s in placement.', trait_name, rp_uuid)
554 
555         if new_traits is not None:
556             self.reportclient.set_traits_for_provider(
557                 context, rp_uuid, new_traits)
558 
559 
560 class ComputeManager(manager.Manager):
561     """Manages the running instances from creation to destruction."""
562 
563     target = messaging.Target(version='5.11')
564 
565     def __init__(self, compute_driver=None, *args, **kwargs):
566         """Load configuration options and connect to the hypervisor."""
567         # We want the ComputeManager, ResourceTracker and ComputeVirtAPI all
568         # using the same instance of SchedulerReportClient which has the
569         # ProviderTree cache for this compute service.
570         self.reportclient = report.SchedulerReportClient()
571         self.virtapi = ComputeVirtAPI(self)
572         self.network_api = network.API()
573         self.volume_api = cinder.API()
574         self.image_api = image.API()
575         self._last_bw_usage_poll = 0
576         self._bw_usage_supported = True
577         self.compute_api = compute.API()
578         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
579         self.compute_task_api = conductor.ComputeTaskAPI()
580         self.is_neutron_security_groups = (
581             openstack_driver.is_neutron_security_groups())
582         self.query_client = query.SchedulerQueryClient()
583         self.instance_events = InstanceEvents()
584         self._sync_power_pool = eventlet.GreenPool(
585             size=CONF.sync_power_state_pool_size)
586         self._syncs_in_progress = {}
587         self.send_instance_updates = (
588             CONF.filter_scheduler.track_instance_changes)
589         if CONF.max_concurrent_builds != 0:
590             self._build_semaphore = eventlet.semaphore.Semaphore(
591                 CONF.max_concurrent_builds)
592         else:
593             self._build_semaphore = compute_utils.UnlimitedSemaphore()
594         if CONF.max_concurrent_live_migrations > 0:
595             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
596                 max_workers=CONF.max_concurrent_live_migrations)
597         else:
598             # CONF.max_concurrent_live_migrations is 0 (unlimited)
599             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
600         # This is a dict, keyed by instance uuid, to a two-item tuple of
601         # migration object and Future for the queued live migration.
602         self._waiting_live_migrations = {}
603 
604         super(ComputeManager, self).__init__(service_name="compute",
605                                              *args, **kwargs)
606 
607         # NOTE(russellb) Load the driver last.  It may call back into the
608         # compute manager via the virtapi, so we want it to be fully
609         # initialized before that happens.
610         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
611         self.use_legacy_block_device_info = \
612                             self.driver.need_legacy_block_device_info
613         self.rt = resource_tracker.ResourceTracker(
614             self.host, self.driver, reportclient=self.reportclient)
615 
616     def reset(self):
617         LOG.info('Reloading compute RPC API')
618         compute_rpcapi.reset_globals()
619         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
620         self.reportclient.clear_provider_cache()
621 
622     def _update_resource_tracker(self, context, instance):
623         """Let the resource tracker know that an instance has changed state."""
624 
625         if instance.host == self.host:
626             self.rt.update_usage(context, instance, instance.node)
627 
628     def _instance_update(self, context, instance, **kwargs):
629         """Update an instance in the database using kwargs as value."""
630 
631         for k, v in kwargs.items():
632             setattr(instance, k, v)
633         instance.save()
634         self._update_resource_tracker(context, instance)
635 
636     def _nil_out_instance_obj_host_and_node(self, instance):
637         # NOTE(jwcroppe): We don't do instance.save() here for performance
638         # reasons; a call to this is expected to be immediately followed by
639         # another call that does instance.save(), thus avoiding two writes
640         # to the database layer.
641         instance.host = None
642         instance.node = None
643         # ResourceTracker._set_instance_host_and_node also sets launched_on
644         # to the same value as host and is really only ever used by legacy
645         # nova-network code, but we should also null it out to avoid confusion
646         # if there is an instance in the database with no host set but
647         # launched_on is set. Note that we do not care about using launched_on
648         # as some kind of debug helper if diagnosing a build failure, that is
649         # what instance action events are for.
650         instance.launched_on = None
651         # If the instance is not on a host, it's not in an aggregate and
652         # therefore is not in an availability zone.
653         instance.availability_zone = None
654 
655     def _set_instance_obj_error_state(self, context, instance,
656                                       clean_task_state=False):
657         try:
658             instance.vm_state = vm_states.ERROR
659             if clean_task_state:
660                 instance.task_state = None
661             instance.save()
662         except exception.InstanceNotFound:
663             LOG.debug('Instance has been destroyed from under us while '
664                       'trying to set it to ERROR', instance=instance)
665 
666     def _get_instances_on_driver(self, context, filters=None):
667         """Return a list of instance records for the instances found
668         on the hypervisor which satisfy the specified filters. If filters=None
669         return a list of instance records for all the instances found on the
670         hypervisor.
671         """
672         if not filters:
673             filters = {}
674         try:
675             driver_uuids = self.driver.list_instance_uuids()
676             if len(driver_uuids) == 0:
677                 # Short circuit, don't waste a DB call
678                 return objects.InstanceList()
679             filters['uuid'] = driver_uuids
680             local_instances = objects.InstanceList.get_by_filters(
681                 context, filters, use_slave=True)
682             return local_instances
683         except NotImplementedError:
684             pass
685 
686         # The driver doesn't support uuids listing, so we'll have
687         # to brute force.
688         driver_instances = self.driver.list_instances()
689         # NOTE(mjozefcz): In this case we need to apply host filter.
690         # Without this all instance data would be fetched from db.
691         filters['host'] = self.host
692         instances = objects.InstanceList.get_by_filters(context, filters,
693                                                         use_slave=True)
694         name_map = {instance.name: instance for instance in instances}
695         local_instances = []
696         for driver_instance in driver_instances:
697             instance = name_map.get(driver_instance)
698             if not instance:
699                 continue
700             local_instances.append(instance)
701         return local_instances
702 
703     def _destroy_evacuated_instances(self, context, node_cache):
704         """Destroys evacuated instances.
705 
706         While nova-compute was down, the instances running on it could be
707         evacuated to another host. This method looks for evacuation migration
708         records where this is the source host and which were either started
709         (accepted), in-progress (pre-migrating) or migrated (done). From those
710         migration records, local instances reported by the hypervisor are
711         compared to the instances for the migration records and those local
712         guests are destroyed, along with instance allocation records in
713         Placement for this node.
714         Then allocations are removed from Placement for every instance that is
715         evacuated from this host regardless if the instance is reported by the
716         hypervisor or not.
717 
718         :param context: The request context
719         :param node_cache: A dict of ComputeNode objects keyed by the UUID of
720             the compute node
721         :return: A dict keyed by instance uuid mapped to Migration objects
722             for instances that were migrated away from this host
723         """
724         filters = {
725             'source_compute': self.host,
726             # NOTE(mriedem): Migration records that have been accepted are
727             # included in case the source node comes back up while instances
728             # are being evacuated to another host. We don't want the same
729             # instance being reported from multiple hosts.
730             # NOTE(lyarwood): pre-migrating is also included here as the
731             # source compute can come back online shortly after the RT
732             # claims on the destination that in-turn moves the migration to
733             # pre-migrating. If the evacuate fails on the destination host,
734             # the user can rebuild the instance (in ERROR state) on the source
735             # host.
736             'status': ['accepted', 'pre-migrating', 'done'],
737             'migration_type': 'evacuation',
738         }
739         with utils.temporary_mutation(context, read_deleted='yes'):
740             evacuations = objects.MigrationList.get_by_filters(context,
741                                                                filters)
742         if not evacuations:
743             return {}
744         evacuations = {mig.instance_uuid: mig for mig in evacuations}
745 
746         # TODO(mriedem): We could optimize by pre-loading the joined fields
747         # we know we'll use, like info_cache and flavor.
748         local_instances = self._get_instances_on_driver(context)
749         evacuated_local_instances = {inst.uuid: inst
750                                      for inst in local_instances
751                                      if inst.uuid in evacuations}
752 
753         for instance in evacuated_local_instances.values():
754             LOG.info('Destroying instance as it has been evacuated from '
755                      'this host but still exists in the hypervisor',
756                      instance=instance)
757             try:
758                 network_info = self.network_api.get_instance_nw_info(
759                     context, instance)
760                 bdi = self._get_instance_block_device_info(context,
761                                                            instance)
762                 destroy_disks = not (self._is_instance_storage_shared(
763                     context, instance))
764             except exception.InstanceNotFound:
765                 network_info = network_model.NetworkInfo()
766                 bdi = {}
767                 LOG.info('Instance has been marked deleted already, '
768                          'removing it from the hypervisor.',
769                          instance=instance)
770                 # always destroy disks if the instance was deleted
771                 destroy_disks = True
772             self.driver.destroy(context, instance,
773                                 network_info,
774                                 bdi, destroy_disks)
775 
776         hostname_to_cn_uuid = {
777             cn.hypervisor_hostname: cn.uuid
778             for cn in node_cache.values()}
779 
780         for instance_uuid, migration in evacuations.items():
781             try:
782                 if instance_uuid in evacuated_local_instances:
783                     # Avoid the db call if we already have the instance loaded
784                     # above
785                     instance = evacuated_local_instances[instance_uuid]
786                 else:
787                     instance = objects.Instance.get_by_uuid(
788                         context, instance_uuid)
789             except exception.InstanceNotFound:
790                 # The instance already deleted so we expect that every
791                 # allocation of that instance has already been cleaned up
792                 continue
793 
794             LOG.info('Cleaning up allocations of the instance as it has been '
795                      'evacuated from this host',
796                      instance=instance)
797             if migration.source_node not in hostname_to_cn_uuid:
798                 LOG.error("Failed to clean allocation of evacuated "
799                           "instance as the source node %s is not found",
800                           migration.source_node, instance=instance)
801                 continue
802             cn_uuid = hostname_to_cn_uuid[migration.source_node]
803 
804             # If the instance was deleted in the interim, assume its
805             # allocations were properly cleaned up (either by its hosting
806             # compute service or the API).
807             if (not instance.deleted and
808                     not self.reportclient.
809                         remove_provider_tree_from_instance_allocation(
810                             context, instance.uuid, cn_uuid)):
811                 LOG.error("Failed to clean allocation of evacuated instance "
812                           "on the source node %s",
813                           cn_uuid, instance=instance)
814 
815             migration.status = 'completed'
816             migration.save()
817         return evacuations
818 
819     def _is_instance_storage_shared(self, context, instance, host=None):
820         shared_storage = True
821         data = None
822         try:
823             data = self.driver.check_instance_shared_storage_local(context,
824                                                        instance)
825             if data:
826                 shared_storage = (self.compute_rpcapi.
827                                   check_instance_shared_storage(context,
828                                   instance, data, host=host))
829         except NotImplementedError:
830             LOG.debug('Hypervisor driver does not support '
831                       'instance shared storage check, '
832                       'assuming it\'s not on shared storage',
833                       instance=instance)
834             shared_storage = False
835         except Exception:
836             LOG.exception('Failed to check if instance shared',
837                           instance=instance)
838         finally:
839             if data:
840                 self.driver.check_instance_shared_storage_cleanup(context,
841                                                                   data)
842         return shared_storage
843 
844     def _complete_partial_deletion(self, context, instance):
845         """Complete deletion for instances in DELETED status but not marked as
846         deleted in the DB
847         """
848         instance.destroy()
849         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
850                 context, instance.uuid)
851         self._complete_deletion(context,
852                                 instance)
853         self._notify_about_instance_usage(context, instance, "delete.end")
854         compute_utils.notify_about_instance_action(context, instance,
855                 self.host, action=fields.NotificationAction.DELETE,
856                 phase=fields.NotificationPhase.END, bdms=bdms)
857 
858     def _complete_deletion(self, context, instance):
859         self._update_resource_tracker(context, instance)
860 
861         self.reportclient.delete_allocation_for_instance(context,
862                                                          instance.uuid)
863 
864         self._clean_instance_console_tokens(context, instance)
865         self._delete_scheduler_instance_info(context, instance.uuid)
866 
867     def _validate_pinning_configuration(self, instances):
868         if not self.driver.capabilities.get('supports_pcpus', False):
869             return
870 
871         for instance in instances:
872             # ignore deleted instances
873             if instance.deleted:
874                 continue
875 
876             # if this is an unpinned instance and the host only has
877             # 'cpu_dedicated_set' configured, we need to tell the operator to
878             # correct their configuration
879             if not (instance.numa_topology and
880                         instance.numa_topology.cpu_pinning_requested):
881 
882                 # we don't need to check 'vcpu_pin_set' since it can't coexist
883                 # alongside 'cpu_dedicated_set'
884                 if (CONF.compute.cpu_dedicated_set and
885                         not CONF.compute.cpu_shared_set):
886                     msg = _("This host has unpinned instances but has no CPUs "
887                             "set aside for this purpose; configure '[compute] "
888                             "cpu_shared_set' instead of, or in addition to, "
889                             "'[compute] cpu_dedicated_set'")
890                     raise exception.InvalidConfiguration(msg)
891 
892                 continue
893 
894             # ditto for pinned instances if only 'cpu_shared_set' is configured
895             if (CONF.compute.cpu_shared_set and
896                     not CONF.compute.cpu_dedicated_set and
897                     not CONF.vcpu_pin_set):
898                 msg = _("This host has pinned instances but has no CPUs "
899                         "set aside for this purpose; configure '[compute] "
900                         "cpu_dedicated_set' instead of, or in addition to, "
901                         "'[compute] cpu_shared_set'")
902                 raise exception.InvalidConfiguration(msg)
903 
904             # also check to make sure the operator hasn't accidentally
905             # dropped some cores that instances are currently using
906             available_dedicated_cpus = (hardware.get_vcpu_pin_set() or
907                                         hardware.get_cpu_dedicated_set())
908             pinned_cpus = instance.numa_topology.cpu_pinning
909             if available_dedicated_cpus and (
910                     pinned_cpus - available_dedicated_cpus):
911                 # we can't raise an exception because of bug #1289064,
912                 # which meant we didn't recalculate CPU pinning information
913                 # when we live migrated a pinned instance
914                 LOG.warning(
915                     "Instance is pinned to host CPUs %(cpus)s "
916                     "but one or more of these CPUs are not included in "
917                     "either '[compute] cpu_dedicated_set' or "
918                     "'vcpu_pin_set'; you should update these "
919                     "configuration options to include the missing CPUs "
920                     "or rebuild or cold migrate this instance.",
921                     {'cpus': list(pinned_cpus)},
922                     instance=instance)
923 
924     def _reset_live_migration(self, context, instance):
925         migration = None
926         try:
927             migration = objects.Migration.get_by_instance_and_status(
928                                       context, instance.uuid, 'running')
929             if migration:
930                 self.live_migration_abort(context, instance, migration.id)
931         except Exception:
932             LOG.exception('Failed to abort live-migration',
933                           instance=instance)
934         finally:
935             if migration:
936                 self._set_migration_status(migration, 'error')
937             LOG.info('Instance found in migrating state during '
938                      'startup. Resetting task_state',
939                      instance=instance)
940             instance.task_state = None
941             instance.save(expected_task_state=[task_states.MIGRATING])
942 
943     def _init_instance(self, context, instance):
944         """Initialize this instance during service init."""
945 
946         # NOTE(danms): If the instance appears to not be owned by this
947         # host, it may have been evacuated away, but skipped by the
948         # evacuation cleanup code due to configuration. Thus, if that
949         # is a possibility, don't touch the instance in any way, but
950         # log the concern. This will help avoid potential issues on
951         # startup due to misconfiguration.
952         if instance.host != self.host:
953             LOG.warning('Instance %(uuid)s appears to not be owned '
954                         'by this host, but by %(host)s. Startup '
955                         'processing is being skipped.',
956                         {'uuid': instance.uuid,
957                          'host': instance.host})
958             return
959 
960         # Instances that are shut down, or in an error state can not be
961         # initialized and are not attempted to be recovered. The exception
962         # to this are instances that are in RESIZE_MIGRATING or DELETING,
963         # which are dealt with further down.
964         if (instance.vm_state == vm_states.SOFT_DELETED or
965             (instance.vm_state == vm_states.ERROR and
966             instance.task_state not in
967             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
968             LOG.debug("Instance is in %s state.",
969                       instance.vm_state, instance=instance)
970             return
971 
972         if instance.vm_state == vm_states.DELETED:
973             try:
974                 self._complete_partial_deletion(context, instance)
975             except Exception:
976                 # we don't want that an exception blocks the init_host
977                 LOG.exception('Failed to complete a deletion',
978                               instance=instance)
979             return
980 
981         if (instance.vm_state == vm_states.BUILDING or
982             instance.task_state in [task_states.SCHEDULING,
983                                     task_states.BLOCK_DEVICE_MAPPING,
984                                     task_states.NETWORKING,
985                                     task_states.SPAWNING]):
986             # NOTE(dave-mcnally) compute stopped before instance was fully
987             # spawned so set to ERROR state. This is safe to do as the state
988             # may be set by the api but the host is not so if we get here the
989             # instance has already been scheduled to this particular host.
990             LOG.debug("Instance failed to spawn correctly, "
991                       "setting to ERROR state", instance=instance)
992             self._set_instance_obj_error_state(
993                 context, instance, clean_task_state=True)
994             return
995 
996         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
997             instance.task_state in [task_states.REBUILDING,
998                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
999                                     task_states.REBUILD_SPAWNING]):
1000             # NOTE(jichenjc) compute stopped before instance was fully
1001             # spawned so set to ERROR state. This is consistent to BUILD
1002             LOG.debug("Instance failed to rebuild correctly, "
1003                       "setting to ERROR state", instance=instance)
1004             self._set_instance_obj_error_state(
1005                 context, instance, clean_task_state=True)
1006             return
1007 
1008         if (instance.vm_state != vm_states.ERROR and
1009             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
1010                                     task_states.IMAGE_PENDING_UPLOAD,
1011                                     task_states.IMAGE_UPLOADING,
1012                                     task_states.IMAGE_SNAPSHOT]):
1013             LOG.debug("Instance in transitional state %s at start-up "
1014                       "clearing task state",
1015                       instance.task_state, instance=instance)
1016             try:
1017                 self._post_interrupted_snapshot_cleanup(context, instance)
1018             except Exception:
1019                 # we don't want that an exception blocks the init_host
1020                 LOG.exception('Failed to cleanup snapshot.', instance=instance)
1021             instance.task_state = None
1022             instance.save()
1023 
1024         if (instance.vm_state != vm_states.ERROR and
1025             instance.task_state in [task_states.RESIZE_PREP]):
1026             LOG.debug("Instance in transitional state %s at start-up "
1027                       "clearing task state",
1028                       instance['task_state'], instance=instance)
1029             instance.task_state = None
1030             instance.save()
1031 
1032         if instance.task_state == task_states.DELETING:
1033             try:
1034                 LOG.info('Service started deleting the instance during '
1035                          'the previous run, but did not finish. Restarting'
1036                          ' the deletion now.', instance=instance)
1037                 instance.obj_load_attr('metadata')
1038                 instance.obj_load_attr('system_metadata')
1039                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1040                         context, instance.uuid)
1041                 self._delete_instance(context, instance, bdms)
1042             except Exception:
1043                 # we don't want that an exception blocks the init_host
1044                 LOG.exception('Failed to complete a deletion',
1045                               instance=instance)
1046                 self._set_instance_obj_error_state(context, instance)
1047             return
1048 
1049         current_power_state = self._get_power_state(context, instance)
1050         try_reboot, reboot_type = self._retry_reboot(context, instance,
1051                                                      current_power_state)
1052 
1053         if try_reboot:
1054             LOG.debug("Instance in transitional state (%(task_state)s) at "
1055                       "start-up and power state is (%(power_state)s), "
1056                       "triggering reboot",
1057                       {'task_state': instance.task_state,
1058                        'power_state': current_power_state},
1059                       instance=instance)
1060 
1061             # NOTE(mikal): if the instance was doing a soft reboot that got as
1062             # far as shutting down the instance but not as far as starting it
1063             # again, then we've just become a hard reboot. That means the
1064             # task state for the instance needs to change so that we're in one
1065             # of the expected task states for a hard reboot.
1066             if (instance.task_state in task_states.soft_reboot_states and
1067                 reboot_type == 'HARD'):
1068                 instance.task_state = task_states.REBOOT_PENDING_HARD
1069                 instance.save()
1070 
1071             self.reboot_instance(context, instance, block_device_info=None,
1072                                  reboot_type=reboot_type)
1073             return
1074 
1075         elif (current_power_state == power_state.RUNNING and
1076               instance.task_state in [task_states.REBOOT_STARTED,
1077                                       task_states.REBOOT_STARTED_HARD,
1078                                       task_states.PAUSING,
1079                                       task_states.UNPAUSING]):
1080             LOG.warning("Instance in transitional state "
1081                         "(%(task_state)s) at start-up and power state "
1082                         "is (%(power_state)s), clearing task state",
1083                         {'task_state': instance.task_state,
1084                          'power_state': current_power_state},
1085                         instance=instance)
1086             instance.task_state = None
1087             instance.vm_state = vm_states.ACTIVE
1088             instance.save()
1089         elif (current_power_state == power_state.PAUSED and
1090               instance.task_state == task_states.UNPAUSING):
1091             LOG.warning("Instance in transitional state "
1092                         "(%(task_state)s) at start-up and power state "
1093                         "is (%(power_state)s), clearing task state "
1094                         "and unpausing the instance",
1095                         {'task_state': instance.task_state,
1096                          'power_state': current_power_state},
1097                         instance=instance)
1098             try:
1099                 self.unpause_instance(context, instance)
1100             except NotImplementedError:
1101                 # Some virt driver didn't support pause and unpause
1102                 pass
1103             except Exception:
1104                 LOG.exception('Failed to unpause instance', instance=instance)
1105             return
1106 
1107         if instance.task_state == task_states.POWERING_OFF:
1108             try:
1109                 LOG.debug("Instance in transitional state %s at start-up "
1110                           "retrying stop request",
1111                           instance.task_state, instance=instance)
1112                 self.stop_instance(context, instance, True)
1113             except Exception:
1114                 # we don't want that an exception blocks the init_host
1115                 LOG.exception('Failed to stop instance', instance=instance)
1116             return
1117 
1118         if instance.task_state == task_states.POWERING_ON:
1119             try:
1120                 LOG.debug("Instance in transitional state %s at start-up "
1121                           "retrying start request",
1122                           instance.task_state, instance=instance)
1123                 self.start_instance(context, instance)
1124             except Exception:
1125                 # we don't want that an exception blocks the init_host
1126                 LOG.exception('Failed to start instance', instance=instance)
1127             return
1128 
1129         net_info = instance.get_network_info()
1130         try:
1131             self.driver.plug_vifs(instance, net_info)
1132         except NotImplementedError as e:
1133             LOG.debug(e, instance=instance)
1134         except exception.VirtualInterfacePlugException:
1135             # NOTE(mriedem): If we get here, it could be because the vif_type
1136             # in the cache is "binding_failed" or "unbound".
1137             # The periodic task _heal_instance_info_cache checks for this
1138             # condition. It should fix this by binding the ports again when
1139             # it gets to this instance.
1140             LOG.exception('Virtual interface plugging failed for instance. '
1141                           'The port binding:host_id may need to be manually '
1142                           'updated.', instance=instance)
1143             self._set_instance_obj_error_state(context, instance)
1144             return
1145 
1146         if instance.task_state == task_states.RESIZE_MIGRATING:
1147             # We crashed during resize/migration, so roll back for safety
1148             try:
1149                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
1150                 # not in system_metadata we default to True for backwards
1151                 # compatibility
1152                 power_on = (instance.system_metadata.get('old_vm_state') !=
1153                             vm_states.STOPPED)
1154 
1155                 block_dev_info = self._get_instance_block_device_info(context,
1156                                                                       instance)
1157 
1158                 migration = objects.Migration.get_by_id_and_instance(
1159                     context, instance.migration_context.migration_id,
1160                     instance.uuid)
1161                 self.driver.finish_revert_migration(context, instance,
1162                     net_info, migration, block_dev_info, power_on)
1163 
1164             except Exception:
1165                 LOG.exception('Failed to revert crashed migration',
1166                               instance=instance)
1167             finally:
1168                 LOG.info('Instance found in migrating state during '
1169                          'startup. Resetting task_state',
1170                          instance=instance)
1171                 instance.task_state = None
1172                 instance.save()
1173         if instance.task_state == task_states.MIGRATING:
1174             # Live migration did not complete, but instance is on this
1175             # host. Abort ongoing migration if still running and reset state.
1176             self._reset_live_migration(context, instance)
1177 
1178         db_state = instance.power_state
1179         drv_state = self._get_power_state(context, instance)
1180         expect_running = (db_state == power_state.RUNNING and
1181                           drv_state != db_state)
1182 
1183         LOG.debug('Current state is %(drv_state)s, state in DB is '
1184                   '%(db_state)s.',
1185                   {'drv_state': drv_state, 'db_state': db_state},
1186                   instance=instance)
1187 
1188         if expect_running and CONF.resume_guests_state_on_host_boot:
1189             self._resume_guests_state(context, instance, net_info)
1190         elif drv_state == power_state.RUNNING:
1191             # VMwareAPI drivers will raise an exception
1192             try:
1193                 self.driver.ensure_filtering_rules_for_instance(
1194                                        instance, net_info)
1195             except NotImplementedError:
1196                 LOG.debug('Hypervisor driver does not support '
1197                           'firewall rules', instance=instance)
1198 
1199     def _resume_guests_state(self, context, instance, net_info):
1200         LOG.info('Rebooting instance after nova-compute restart.',
1201                  instance=instance)
1202         block_device_info = \
1203             self._get_instance_block_device_info(context, instance)
1204 
1205         try:
1206             self.driver.resume_state_on_host_boot(
1207                 context, instance, net_info, block_device_info)
1208         except NotImplementedError:
1209             LOG.warning('Hypervisor driver does not support '
1210                         'resume guests', instance=instance)
1211         except Exception:
1212             # NOTE(vish): The instance failed to resume, so we set the
1213             #             instance to error and attempt to continue.
1214             LOG.warning('Failed to resume instance',
1215                         instance=instance)
1216             self._set_instance_obj_error_state(context, instance)
1217 
1218     def _retry_reboot(self, context, instance, current_power_state):
1219         current_task_state = instance.task_state
1220         retry_reboot = False
1221         reboot_type = compute_utils.get_reboot_type(current_task_state,
1222                                                     current_power_state)
1223 
1224         pending_soft = (
1225             current_task_state == task_states.REBOOT_PENDING and
1226             instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1227         pending_hard = (
1228             current_task_state == task_states.REBOOT_PENDING_HARD and
1229             instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1230         started_not_running = (current_task_state in
1231                                [task_states.REBOOT_STARTED,
1232                                 task_states.REBOOT_STARTED_HARD] and
1233                                current_power_state != power_state.RUNNING)
1234 
1235         if pending_soft or pending_hard or started_not_running:
1236             retry_reboot = True
1237 
1238         return retry_reboot, reboot_type
1239 
1240     def handle_lifecycle_event(self, event):
1241         LOG.info("VM %(state)s (Lifecycle Event)",
1242                  {'state': event.get_name()},
1243                  instance_uuid=event.get_instance_uuid())
1244         context = nova.context.get_admin_context(read_deleted='yes')
1245         vm_power_state = None
1246         event_transition = event.get_transition()
1247         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1248             vm_power_state = power_state.SHUTDOWN
1249         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1250             vm_power_state = power_state.RUNNING
1251         elif event_transition in (
1252                 virtevent.EVENT_LIFECYCLE_PAUSED,
1253                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1254                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1255             vm_power_state = power_state.PAUSED
1256         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1257             vm_power_state = power_state.RUNNING
1258         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1259             vm_power_state = power_state.SUSPENDED
1260         else:
1261             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1262 
1263         migrate_finish_statuses = {
1264             # This happens on the source node and indicates live migration
1265             # entered post-copy mode.
1266             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1267             # Suspended for offline migration.
1268             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1269         }
1270 
1271         expected_attrs = []
1272         if event_transition in migrate_finish_statuses:
1273             # Join on info_cache since that's needed in migrate_instance_start.
1274             expected_attrs.append('info_cache')
1275         instance = objects.Instance.get_by_uuid(context,
1276                                                 event.get_instance_uuid(),
1277                                                 expected_attrs=expected_attrs)
1278 
1279         # Note(lpetrut): The event may be delayed, thus not reflecting
1280         # the current instance power state. In that case, ignore the event.
1281         current_power_state = self._get_power_state(context, instance)
1282         if current_power_state == vm_power_state:
1283             LOG.debug('Synchronizing instance power state after lifecycle '
1284                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1285                       'current task_state: %(task_state)s, current DB '
1286                       'power_state: %(db_power_state)s, VM power_state: '
1287                       '%(vm_power_state)s',
1288                       {'event': event.get_name(),
1289                        'vm_state': instance.vm_state,
1290                        'task_state': instance.task_state,
1291                        'db_power_state': instance.power_state,
1292                        'vm_power_state': vm_power_state},
1293                       instance_uuid=instance.uuid)
1294             self._sync_instance_power_state(context,
1295                                             instance,
1296                                             vm_power_state)
1297 
1298         # The following checks are for live migration. We want to activate
1299         # the port binding for the destination host before the live migration
1300         # is resumed on the destination host in order to reduce network
1301         # downtime. Otherwise the ports are bound to the destination host
1302         # in post_live_migration_at_destination.
1303         # TODO(danms): Explore options for using a different live migration
1304         # specific callback for this instead of piggy-backing on the
1305         # handle_lifecycle_event callback.
1306         if (instance.task_state == task_states.MIGRATING and
1307                 event_transition in migrate_finish_statuses):
1308             status = migrate_finish_statuses[event_transition]
1309             try:
1310                 migration = objects.Migration.get_by_instance_and_status(
1311                             context, instance.uuid, status)
1312                 LOG.debug('Binding ports to destination host: %s',
1313                           migration.dest_compute, instance=instance)
1314                 # For neutron, migrate_instance_start will activate the
1315                 # destination host port bindings, if there are any created by
1316                 # conductor before live migration started.
1317                 self.network_api.migrate_instance_start(
1318                     context, instance, migration)
1319             except exception.MigrationNotFoundByStatus:
1320                 LOG.warning("Unable to find migration record with status "
1321                             "'%s' for instance. Port binding will happen in "
1322                             "post live migration.", status, instance=instance)
1323 
1324     def handle_events(self, event):
1325         if isinstance(event, virtevent.LifecycleEvent):
1326             try:
1327                 self.handle_lifecycle_event(event)
1328             except exception.InstanceNotFound:
1329                 LOG.debug("Event %s arrived for non-existent instance. The "
1330                           "instance was probably deleted.", event)
1331         else:
1332             LOG.debug("Ignoring event %s", event)
1333 
1334     def init_virt_events(self):
1335         if CONF.workarounds.handle_virt_lifecycle_events:
1336             self.driver.register_event_listener(self.handle_events)
1337         else:
1338             # NOTE(mriedem): If the _sync_power_states periodic task is
1339             # disabled we should emit a warning in the logs.
1340             if CONF.sync_power_state_interval < 0:
1341                 LOG.warning('Instance lifecycle events from the compute '
1342                             'driver have been disabled. Note that lifecycle '
1343                             'changes to an instance outside of the compute '
1344                             'service will not be synchronized '
1345                             'automatically since the _sync_power_states '
1346                             'periodic task is also disabled.')
1347             else:
1348                 LOG.info('Instance lifecycle events from the compute '
1349                          'driver have been disabled. Note that lifecycle '
1350                          'changes to an instance outside of the compute '
1351                          'service will only be synchronized by the '
1352                          '_sync_power_states periodic task.')
1353 
1354     def _get_nodes(self, context):
1355         """Queried the ComputeNode objects from the DB that are reported by the
1356         hypervisor.
1357 
1358         :param context: the request context
1359         :return: a dict of ComputeNode objects keyed by the UUID of the given
1360             node.
1361         """
1362         nodes_by_uuid = {}
1363         try:
1364             node_names = self.driver.get_available_nodes()
1365         except exception.VirtDriverNotReady:
1366             LOG.warning(
1367                 "Virt driver is not ready. If this is the first time this "
1368                 "service is starting on this host, then you can ignore this "
1369                 "warning.")
1370             return {}
1371 
1372         for node_name in node_names:
1373             try:
1374                 node = objects.ComputeNode.get_by_host_and_nodename(
1375                     context, self.host, node_name)
1376                 nodes_by_uuid[node.uuid] = node
1377             except exception.ComputeHostNotFound:
1378                 LOG.warning(
1379                     "Compute node %s not found in the database. If this is "
1380                     "the first time this service is starting on this host, "
1381                     "then you can ignore this warning.", node_name)
1382         return nodes_by_uuid
1383 
1384     def init_host(self):
1385         """Initialization for a standalone compute service."""
1386 
1387         if CONF.pci.passthrough_whitelist:
1388             # Simply loading the PCI passthrough whitelist will do a bunch of
1389             # validation that would otherwise wait until the PciDevTracker is
1390             # constructed when updating available resources for the compute
1391             # node(s) in the resource tracker, effectively killing that task.
1392             # So load up the whitelist when starting the compute service to
1393             # flush any invalid configuration early so we can kill the service
1394             # if the configuration is wrong.
1395             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1396 
1397         nova.conf.neutron.register_dynamic_opts(CONF)
1398 
1399         # Override the number of concurrent disk operations allowed if the
1400         # user has specified a limit.
1401         if CONF.compute.max_concurrent_disk_ops != 0:
1402             compute_utils.disk_ops_semaphore = \
1403                 eventlet.semaphore.BoundedSemaphore(
1404                     CONF.compute.max_concurrent_disk_ops)
1405 
1406         self.driver.init_host(host=self.host)
1407         context = nova.context.get_admin_context()
1408         instances = objects.InstanceList.get_by_host(
1409             context, self.host,
1410             expected_attrs=['info_cache', 'metadata', 'numa_topology'])
1411 
1412         if CONF.defer_iptables_apply:
1413             self.driver.filter_defer_apply_on()
1414 
1415         self.init_virt_events()
1416 
1417         self._validate_pinning_configuration(instances)
1418 
1419         # NOTE(gibi): At this point the compute_nodes of the resource tracker
1420         # has not been populated yet so we cannot rely on the resource tracker
1421         # here.
1422         # NOTE(gibi): If ironic and vcenter virt driver slow start time
1423         # becomes problematic here then we should consider adding a config
1424         # option or a driver flag to tell us if we should thread
1425         # _destroy_evacuated_instances and
1426         # _error_out_instances_whose_build_was_interrupted out in the
1427         # background on startup
1428         nodes_by_uuid = self._get_nodes(context)
1429 
1430         try:
1431             # checking that instance was not already evacuated to other host
1432             evacuated_instances = self._destroy_evacuated_instances(
1433                 context, nodes_by_uuid)
1434 
1435             # Initialise instances on the host that are not evacuating
1436             for instance in instances:
1437                 if instance.uuid not in evacuated_instances:
1438                     self._init_instance(context, instance)
1439 
1440             # NOTE(gibi): collect all the instance uuids that is in some way
1441             # was already handled above. Either by init_instance or by
1442             # _destroy_evacuated_instances. This way we can limit the scope of
1443             # the _error_out_instances_whose_build_was_interrupted call to look
1444             # only for instances that have allocations on this node and not
1445             # handled by the above calls.
1446             already_handled = {instance.uuid for instance in instances}.union(
1447                 evacuated_instances)
1448             self._error_out_instances_whose_build_was_interrupted(
1449                 context, already_handled, nodes_by_uuid.keys())
1450 
1451         finally:
1452             if CONF.defer_iptables_apply:
1453                 self.driver.filter_defer_apply_off()
1454             if instances:
1455                 # We only send the instance info to the scheduler on startup
1456                 # if there is anything to send, otherwise this host might
1457                 # not be mapped yet in a cell and the scheduler may have
1458                 # issues dealing with the information. Later changes to
1459                 # instances on this host will update the scheduler, or the
1460                 # _sync_scheduler_instance_info periodic task will.
1461                 self._update_scheduler_instance_info(context, instances)
1462 
1463     def _error_out_instances_whose_build_was_interrupted(
1464             self, context, already_handled_instances, node_uuids):
1465         """If there are instances in BUILDING state that are not
1466         assigned to this host but have allocations in placement towards
1467         this compute that means the nova-compute service was
1468         restarted while those instances waited for the resource claim
1469         to finish and the _set_instance_host_and_node() to update the
1470         instance.host field. We need to push them to ERROR state here to
1471         prevent keeping them in BUILDING state forever.
1472 
1473         :param context: The request context
1474         :param already_handled_instances: The set of instance UUIDs that the
1475             host initialization process already handled in some way.
1476         :param node_uuids: The list of compute node uuids handled by this
1477             service
1478         """
1479 
1480         # Strategy:
1481         # 1) Get the allocations from placement for our compute node(s)
1482         # 2) Remove the already handled instances from the consumer list;
1483         #    they are either already initialized or need to be skipped.
1484         # 3) Check which remaining consumer is an instance in BUILDING state
1485         #    and push it to ERROR state.
1486 
1487         LOG.info(
1488             "Looking for unclaimed instances stuck in BUILDING status for "
1489             "nodes managed by this host")
1490         for cn_uuid in node_uuids:
1491             try:
1492                 f = self.reportclient.get_allocations_for_resource_provider
1493                 allocations = f(context, cn_uuid).allocations
1494             except (exception.ResourceProviderAllocationRetrievalFailed,
1495                     keystone_exception.ClientException) as e:
1496                 LOG.error(
1497                     "Could not retrieve compute node resource provider %s and "
1498                     "therefore unable to error out any instances stuck in "
1499                     "BUILDING state. Error: %s", cn_uuid, six.text_type(e))
1500                 continue
1501 
1502             not_handled_consumers = (set(allocations) -
1503                                      already_handled_instances)
1504 
1505             if not not_handled_consumers:
1506                 continue
1507 
1508             filters = {
1509                 'vm_state': vm_states.BUILDING,
1510                 'uuid': not_handled_consumers
1511             }
1512 
1513             instances = objects.InstanceList.get_by_filters(
1514                 context, filters, expected_attrs=[])
1515 
1516             for instance in instances:
1517                 LOG.debug(
1518                     "Instance spawn was interrupted before instance_claim, "
1519                     "setting instance to ERROR state", instance=instance)
1520                 self._set_instance_obj_error_state(
1521                     context, instance, clean_task_state=True)
1522 
1523     def cleanup_host(self):
1524         self.driver.register_event_listener(None)
1525         self.instance_events.cancel_all_events()
1526         self.driver.cleanup_host(host=self.host)
1527         self._cleanup_live_migrations_in_pool()
1528 
1529     def _cleanup_live_migrations_in_pool(self):
1530         # Shutdown the pool so we don't get new requests.
1531         self._live_migration_executor.shutdown(wait=False)
1532         # For any queued migrations, cancel the migration and update
1533         # its status.
1534         for migration, future in self._waiting_live_migrations.values():
1535             # If we got here before the Future was submitted then we need
1536             # to move on since there isn't anything we can do.
1537             if future is None:
1538                 continue
1539             if future.cancel():
1540                 self._set_migration_status(migration, 'cancelled')
1541                 LOG.info('Successfully cancelled queued live migration.',
1542                          instance_uuid=migration.instance_uuid)
1543             else:
1544                 LOG.warning('Unable to cancel live migration.',
1545                             instance_uuid=migration.instance_uuid)
1546         self._waiting_live_migrations.clear()
1547 
1548     def pre_start_hook(self):
1549         """After the service is initialized, but before we fully bring
1550         the service up by listening on RPC queues, make sure to update
1551         our available resources (and indirectly our available nodes).
1552         """
1553         self.update_available_resource(nova.context.get_admin_context(),
1554                                        startup=True)
1555 
1556     def _get_power_state(self, context, instance):
1557         """Retrieve the power state for the given instance."""
1558         LOG.debug('Checking state', instance=instance)
1559         try:
1560             return self.driver.get_info(instance, use_cache=False).state
1561         except exception.InstanceNotFound:
1562             return power_state.NOSTATE
1563 
1564     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1565     def get_console_topic(self, context):
1566         """Retrieves the console host for a project on this host.
1567 
1568         Currently this is just set in the flags for each compute host.
1569 
1570         """
1571         # TODO(mdragon): perhaps make this variable by console_type?
1572         return 'console.%s' % CONF.console_host
1573 
1574     @wrap_exception()
1575     def get_console_pool_info(self, context, console_type):
1576         return self.driver.get_console_pool_info(console_type)
1577 
1578     @wrap_exception()
1579     def refresh_instance_security_rules(self, context, instance):
1580         """Tell the virtualization driver to refresh security rules for
1581         an instance.
1582 
1583         Passes straight through to the virtualization driver.
1584 
1585         Synchronize the call because we may still be in the middle of
1586         creating the instance.
1587         """
1588         @utils.synchronized(instance.uuid)
1589         def _sync_refresh():
1590             try:
1591                 return self.driver.refresh_instance_security_rules(instance)
1592             except NotImplementedError:
1593                 LOG.debug('Hypervisor driver does not support '
1594                           'security groups.', instance=instance)
1595 
1596         return _sync_refresh()
1597 
1598     def _await_block_device_map_created(self, context, vol_id):
1599         # TODO(yamahata): creating volume simultaneously
1600         #                 reduces creation time?
1601         # TODO(yamahata): eliminate dumb polling
1602         start = time.time()
1603         retries = CONF.block_device_allocate_retries
1604         # (1) if the configured value is 0, one attempt should be made
1605         # (2) if the configured value is > 0, then the total number attempts
1606         #      is (retries + 1)
1607         attempts = 1
1608         if retries >= 1:
1609             attempts = retries + 1
1610         for attempt in range(1, attempts + 1):
1611             volume = self.volume_api.get(context, vol_id)
1612             volume_status = volume['status']
1613             if volume_status not in ['creating', 'downloading']:
1614                 if volume_status == 'available':
1615                     return attempt
1616                 LOG.warning("Volume id: %(vol_id)s finished being "
1617                             "created but its status is %(vol_status)s.",
1618                             {'vol_id': vol_id,
1619                              'vol_status': volume_status})
1620                 break
1621             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1622         raise exception.VolumeNotCreated(volume_id=vol_id,
1623                                          seconds=int(time.time() - start),
1624                                          attempts=attempt,
1625                                          volume_status=volume_status)
1626 
1627     def _decode_files(self, injected_files):
1628         """Base64 decode the list of files to inject."""
1629         if not injected_files:
1630             return []
1631 
1632         def _decode(f):
1633             path, contents = f
1634             # Py3 raises binascii.Error instead of TypeError as in Py27
1635             try:
1636                 decoded = base64.b64decode(contents)
1637                 return path, decoded
1638             except (TypeError, binascii.Error):
1639                 raise exception.Base64Exception(path=path)
1640 
1641         return [_decode(f) for f in injected_files]
1642 
1643     def _validate_instance_group_policy(self, context, instance,
1644                                         scheduler_hints, dest_node=None):
1645         """Validate server_group policy, such as affinity and anti-affinity.
1646 
1647         :param context: `nova.RequestContext` object
1648         :param instance: Instance object
1649         :param scheduler_hints: scheduler_hints object of request_spec
1650         :param dest_node: Name of destination node
1651         :raises: RescheduledException if action will break server group policy
1652         """
1653         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1654         # However, there is a race condition with the enforcement of
1655         # the policy.  Since more than one instance may be scheduled at the
1656         # same time, it's possible that more than one instance with an
1657         # anti-affinity policy may end up here.  It's also possible that
1658         # multiple instances with an affinity policy could end up on different
1659         # hosts.  This is a validation step to make sure that starting the
1660         # instance here doesn't violate the policy.
1661         group_hint = scheduler_hints.get('group')
1662         if not group_hint:
1663             return
1664 
1665         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1666         # to check the type on the value and pull the single entry out. The
1667         # API request schema validates that the 'group' hint is a single value.
1668         if isinstance(group_hint, list):
1669             group_hint = group_hint[0]
1670 
1671         @utils.synchronized(group_hint)
1672         def _do_validation(context, instance, group_hint, dest_node):
1673             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1674             if group.policy and 'anti-affinity' == group.policy:
1675                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1676                     context, self.host)
1677                 ins_on_host = set(instances_uuids)
1678                 members = set(group.members)
1679                 # Determine the set of instance group members on this host
1680                 # which are not the instance in question. This is used to
1681                 # determine how many other members from the same anti-affinity
1682                 # group can be on this host.
1683                 # Also consider group members that servers are migrating to
1684                 # this host to avoid breaking the policy of group if it is a
1685                 # migration action.
1686                 ins_uuids = set()
1687                 if dest_node:
1688                     mgs_to_same_dst = (
1689                         objects.MigrationList.get_in_progress_by_host_and_node(
1690                             context, host=self.host, node=dest_node))
1691                     ins_uuids = {mgs.instance_uuid for mgs in mgs_to_same_dst}
1692                 members_on_host = (
1693                         (ins_on_host | ins_uuids) &
1694                         members - set([instance.uuid]))
1695                 rules = group.rules
1696                 if rules and 'max_server_per_host' in rules:
1697                     max_server = rules['max_server_per_host']
1698                 else:
1699                     max_server = 1
1700                 if len(members_on_host) >= max_server:
1701                     msg = _("Anti-affinity instance group policy "
1702                             "was violated.")
1703                     raise exception.RescheduledException(
1704                             instance_uuid=instance.uuid,
1705                             reason=msg)
1706             elif group.policy and 'affinity' == group.policy:
1707                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1708                 if group_hosts and self.host not in group_hosts:
1709                     msg = _("Affinity instance group policy was violated.")
1710                     raise exception.RescheduledException(
1711                             instance_uuid=instance.uuid,
1712                             reason=msg)
1713 
1714         if not CONF.workarounds.disable_group_policy_check_upcall:
1715             _do_validation(context, instance, group_hint, dest_node)
1716 
1717     def _log_original_error(self, exc_info, instance_uuid):
1718         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1719                   exc_info=exc_info)
1720 
1721     @periodic_task.periodic_task
1722     def _check_instance_build_time(self, context):
1723         """Ensure that instances are not stuck in build."""
1724         timeout = CONF.instance_build_timeout
1725         if timeout == 0:
1726             return
1727 
1728         filters = {'vm_state': vm_states.BUILDING,
1729                    'host': self.host}
1730 
1731         building_insts = objects.InstanceList.get_by_filters(context,
1732                            filters, expected_attrs=[], use_slave=True)
1733 
1734         for instance in building_insts:
1735             if timeutils.is_older_than(instance.created_at, timeout):
1736                 self._set_instance_obj_error_state(context, instance)
1737                 LOG.warning("Instance build timed out. Set to error "
1738                             "state.", instance=instance)
1739 
1740     def _check_instance_exists(self, context, instance):
1741         """Ensure an instance with the same name is not already present."""
1742         if self.driver.instance_exists(instance):
1743             raise exception.InstanceExists(name=instance.name)
1744 
1745     def _allocate_network_async(self, context, instance, requested_networks,
1746                                 security_groups, is_vpn,
1747                                 resource_provider_mapping):
1748         """Method used to allocate networks in the background.
1749 
1750         Broken out for testing.
1751         """
1752         # First check to see if we're specifically not supposed to allocate
1753         # networks because if so, we can exit early.
1754         if requested_networks and requested_networks.no_allocate:
1755             LOG.debug("Not allocating networking since 'none' was specified.",
1756                       instance=instance)
1757             return network_model.NetworkInfo([])
1758 
1759         LOG.debug("Allocating IP information in the background.",
1760                   instance=instance)
1761         retries = CONF.network_allocate_retries
1762         attempts = retries + 1
1763         retry_time = 1
1764         bind_host_id = self.driver.network_binding_host_id(context, instance)
1765         for attempt in range(1, attempts + 1):
1766             try:
1767                 nwinfo = self.network_api.allocate_for_instance(
1768                         context, instance, vpn=is_vpn,
1769                         requested_networks=requested_networks,
1770                         security_groups=security_groups,
1771                         bind_host_id=bind_host_id,
1772                         resource_provider_mapping=resource_provider_mapping)
1773                 LOG.debug('Instance network_info: |%s|', nwinfo,
1774                           instance=instance)
1775                 instance.system_metadata['network_allocated'] = 'True'
1776                 # NOTE(JoshNang) do not save the instance here, as it can cause
1777                 # races. The caller shares a reference to instance and waits
1778                 # for this async greenthread to finish before calling
1779                 # instance.save().
1780                 return nwinfo
1781             except Exception:
1782                 exc_info = sys.exc_info()
1783                 log_info = {'attempt': attempt,
1784                             'attempts': attempts}
1785                 if attempt == attempts:
1786                     LOG.exception('Instance failed network setup '
1787                                   'after %(attempts)d attempt(s)',
1788                                   log_info)
1789                     six.reraise(*exc_info)
1790                 LOG.warning('Instance failed network setup '
1791                             '(attempt %(attempt)d of %(attempts)d)',
1792                             log_info, instance=instance)
1793                 time.sleep(retry_time)
1794                 retry_time *= 2
1795                 if retry_time > 30:
1796                     retry_time = 30
1797         # Not reached.
1798 
1799     def _build_networks_for_instance(self, context, instance,
1800             requested_networks, security_groups, resource_provider_mapping):
1801 
1802         # If we're here from a reschedule the network may already be allocated.
1803         if strutils.bool_from_string(
1804                 instance.system_metadata.get('network_allocated', 'False')):
1805             # NOTE(alex_xu): The network_allocated is True means the network
1806             # resource already allocated at previous scheduling, and the
1807             # network setup is cleanup at previous. After rescheduling, the
1808             # network resource need setup on the new host.
1809             self.network_api.setup_instance_network_on_host(
1810                 context, instance, instance.host)
1811             return self.network_api.get_instance_nw_info(context, instance)
1812 
1813         if not self.is_neutron_security_groups:
1814             security_groups = []
1815 
1816         network_info = self._allocate_network(context, instance,
1817                 requested_networks, security_groups,
1818                 resource_provider_mapping)
1819 
1820         return network_info
1821 
1822     def _allocate_network(self, context, instance, requested_networks,
1823                           security_groups, resource_provider_mapping):
1824         """Start network allocation asynchronously.  Return an instance
1825         of NetworkInfoAsyncWrapper that can be used to retrieve the
1826         allocated networks when the operation has finished.
1827         """
1828         # NOTE(comstud): Since we're allocating networks asynchronously,
1829         # this task state has little meaning, as we won't be in this
1830         # state for very long.
1831         instance.vm_state = vm_states.BUILDING
1832         instance.task_state = task_states.NETWORKING
1833         instance.save(expected_task_state=[None])
1834 
1835         is_vpn = False
1836         return network_model.NetworkInfoAsyncWrapper(
1837                 self._allocate_network_async, context, instance,
1838                 requested_networks, security_groups, is_vpn,
1839                 resource_provider_mapping)
1840 
1841     def _default_root_device_name(self, instance, image_meta, root_bdm):
1842         """Gets a default root device name from the driver.
1843 
1844         :param nova.objects.Instance instance:
1845             The instance for which to get the root device name.
1846         :param nova.objects.ImageMeta image_meta:
1847             The metadata of the image of the instance.
1848         :param nova.objects.BlockDeviceMapping root_bdm:
1849             The description of the root device.
1850         :returns: str -- The default root device name.
1851         :raises: InternalError, TooManyDiskDevices
1852         """
1853         try:
1854             return self.driver.default_root_device_name(instance,
1855                                                         image_meta,
1856                                                         root_bdm)
1857         except NotImplementedError:
1858             return compute_utils.get_next_device_name(instance, [])
1859 
1860     def _default_device_names_for_instance(self, instance,
1861                                            root_device_name,
1862                                            *block_device_lists):
1863         """Default the missing device names in the BDM from the driver.
1864 
1865         :param nova.objects.Instance instance:
1866             The instance for which to get default device names.
1867         :param str root_device_name: The root device name.
1868         :param list block_device_lists: List of block device mappings.
1869         :returns: None
1870         :raises: InternalError, TooManyDiskDevices
1871         """
1872         try:
1873             self.driver.default_device_names_for_instance(instance,
1874                                                           root_device_name,
1875                                                           *block_device_lists)
1876         except NotImplementedError:
1877             compute_utils.default_device_names_for_instance(
1878                 instance, root_device_name, *block_device_lists)
1879 
1880     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1881         """Get the next device name from the driver, based on the BDM.
1882 
1883         :param nova.objects.Instance instance:
1884             The instance whose volume is requesting a device name.
1885         :param nova.objects.BlockDeviceMappingList bdms:
1886             The block device mappings for the instance.
1887         :param nova.objects.BlockDeviceMapping block_device_obj:
1888             A block device mapping containing info about the requested block
1889             device.
1890         :returns: The next device name.
1891         :raises: InternalError, TooManyDiskDevices
1892         """
1893         # NOTE(ndipanov): Copy obj to avoid changing the original
1894         block_device_obj = block_device_obj.obj_clone()
1895         try:
1896             return self.driver.get_device_name_for_instance(
1897                 instance, bdms, block_device_obj)
1898         except NotImplementedError:
1899             return compute_utils.get_device_name_for_instance(
1900                 instance, bdms, block_device_obj.get("device_name"))
1901 
1902     def _default_block_device_names(self, instance, image_meta, block_devices):
1903         """Verify that all the devices have the device_name set. If not,
1904         provide a default name.
1905 
1906         It also ensures that there is a root_device_name and is set to the
1907         first block device in the boot sequence (boot_index=0).
1908         """
1909         root_bdm = block_device.get_root_bdm(block_devices)
1910         if not root_bdm:
1911             return
1912 
1913         # Get the root_device_name from the root BDM or the instance
1914         root_device_name = None
1915         update_root_bdm = False
1916 
1917         if root_bdm.device_name:
1918             root_device_name = root_bdm.device_name
1919             instance.root_device_name = root_device_name
1920         elif instance.root_device_name:
1921             root_device_name = instance.root_device_name
1922             root_bdm.device_name = root_device_name
1923             update_root_bdm = True
1924         else:
1925             root_device_name = self._default_root_device_name(instance,
1926                                                               image_meta,
1927                                                               root_bdm)
1928 
1929             instance.root_device_name = root_device_name
1930             root_bdm.device_name = root_device_name
1931             update_root_bdm = True
1932 
1933         if update_root_bdm:
1934             root_bdm.save()
1935 
1936         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1937                             block_devices))
1938         swap = list(filter(block_device.new_format_is_swap,
1939                       block_devices))
1940         block_device_mapping = list(filter(
1941               driver_block_device.is_block_device_mapping, block_devices))
1942 
1943         self._default_device_names_for_instance(instance,
1944                                                 root_device_name,
1945                                                 ephemerals,
1946                                                 swap,
1947                                                 block_device_mapping)
1948 
1949     def _block_device_info_to_legacy(self, block_device_info):
1950         """Convert BDI to the old format for drivers that need it."""
1951 
1952         if self.use_legacy_block_device_info:
1953             ephemerals = driver_block_device.legacy_block_devices(
1954                 driver.block_device_info_get_ephemerals(block_device_info))
1955             mapping = driver_block_device.legacy_block_devices(
1956                 driver.block_device_info_get_mapping(block_device_info))
1957             swap = block_device_info['swap']
1958             if swap:
1959                 swap = swap.legacy()
1960 
1961             block_device_info.update({
1962                 'ephemerals': ephemerals,
1963                 'swap': swap,
1964                 'block_device_mapping': mapping})
1965 
1966     def _add_missing_dev_names(self, bdms, instance):
1967         for bdm in bdms:
1968             if bdm.device_name is not None:
1969                 continue
1970 
1971             device_name = self._get_device_name_for_instance(instance,
1972                                                              bdms, bdm)
1973             values = {'device_name': device_name}
1974             bdm.update(values)
1975             bdm.save()
1976 
1977     def _prep_block_device(self, context, instance, bdms):
1978         """Set up the block device for an instance with error logging."""
1979         try:
1980             self._add_missing_dev_names(bdms, instance)
1981             block_device_info = driver.get_block_device_info(instance, bdms)
1982             mapping = driver.block_device_info_get_mapping(block_device_info)
1983             driver_block_device.attach_block_devices(
1984                 mapping, context, instance, self.volume_api, self.driver,
1985                 wait_func=self._await_block_device_map_created)
1986 
1987             self._block_device_info_to_legacy(block_device_info)
1988             return block_device_info
1989 
1990         except exception.OverQuota as e:
1991             LOG.warning('Failed to create block device for instance due'
1992                         ' to exceeding volume related resource quota.'
1993                         ' Error: %s', e.message, instance=instance)
1994             raise
1995 
1996         except Exception as ex:
1997             LOG.exception('Instance failed block device setup',
1998                           instance=instance)
1999             # InvalidBDM will eventually result in a BuildAbortException when
2000             # booting from volume, and will be recorded as an instance fault.
2001             # Maintain the original exception message which most likely has
2002             # useful details which the standard InvalidBDM error message lacks.
2003             raise exception.InvalidBDM(six.text_type(ex))
2004 
2005     def _update_instance_after_spawn(self, context, instance,
2006                                      vm_state=vm_states.ACTIVE):
2007         instance.power_state = self._get_power_state(context, instance)
2008         instance.vm_state = vm_state
2009         instance.task_state = None
2010         # NOTE(sean-k-mooney): configdrive.update_instance checks
2011         # instance.launched_at to determine if it is the first or
2012         # subsequent spawn of an instance. We need to call update_instance
2013         # first before setting instance.launched_at or instance.config_drive
2014         # will never be set to true based on the value of force_config_drive.
2015         # As a result the config drive will be lost on a hard reboot of the
2016         # instance even when force_config_drive=true. see bug #1835822.
2017         configdrive.update_instance(instance)
2018         instance.launched_at = timeutils.utcnow()
2019 
2020     def _update_scheduler_instance_info(self, context, instance):
2021         """Sends an InstanceList with created or updated Instance objects to
2022         the Scheduler client.
2023 
2024         In the case of init_host, the value passed will already be an
2025         InstanceList. Other calls will send individual Instance objects that
2026         have been created or resized. In this case, we create an InstanceList
2027         object containing that Instance.
2028         """
2029         if not self.send_instance_updates:
2030             return
2031         if isinstance(instance, obj_instance.Instance):
2032             instance = objects.InstanceList(objects=[instance])
2033         context = context.elevated()
2034         self.query_client.update_instance_info(context, self.host,
2035                                                instance)
2036 
2037     def _delete_scheduler_instance_info(self, context, instance_uuid):
2038         """Sends the uuid of the deleted Instance to the Scheduler client."""
2039         if not self.send_instance_updates:
2040             return
2041         context = context.elevated()
2042         self.query_client.delete_instance_info(context, self.host,
2043                                                instance_uuid)
2044 
2045     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
2046     def _sync_scheduler_instance_info(self, context):
2047         if not self.send_instance_updates:
2048             return
2049         context = context.elevated()
2050         instances = objects.InstanceList.get_by_host(context, self.host,
2051                                                      expected_attrs=[],
2052                                                      use_slave=True)
2053         uuids = [instance.uuid for instance in instances]
2054         self.query_client.sync_instance_info(context, self.host, uuids)
2055 
2056     def _notify_about_instance_usage(self, context, instance, event_suffix,
2057                                      network_info=None, extra_usage_info=None,
2058                                      fault=None):
2059         compute_utils.notify_about_instance_usage(
2060             self.notifier, context, instance, event_suffix,
2061             network_info=network_info,
2062             extra_usage_info=extra_usage_info, fault=fault)
2063 
2064     def _deallocate_network(self, context, instance,
2065                             requested_networks=None):
2066         # If we were told not to allocate networks let's save ourselves
2067         # the trouble of calling the network API.
2068         if requested_networks and requested_networks.no_allocate:
2069             LOG.debug("Skipping network deallocation for instance since "
2070                       "networking was not requested.", instance=instance)
2071             return
2072 
2073         LOG.debug('Deallocating network for instance', instance=instance)
2074         with timeutils.StopWatch() as timer:
2075             self.network_api.deallocate_for_instance(
2076                 context, instance, requested_networks=requested_networks)
2077         # nova-network does an rpc call so we're OK tracking time spent here
2078         LOG.info('Took %0.2f seconds to deallocate network for instance.',
2079                  timer.elapsed(), instance=instance)
2080 
2081     def _get_instance_block_device_info(self, context, instance,
2082                                         refresh_conn_info=False,
2083                                         bdms=None):
2084         """Transform block devices to the driver block_device format."""
2085 
2086         if bdms is None:
2087             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2088                     context, instance.uuid)
2089         block_device_info = driver.get_block_device_info(instance, bdms)
2090 
2091         if not refresh_conn_info:
2092             # if the block_device_mapping has no value in connection_info
2093             # (returned as None), don't include in the mapping
2094             block_device_info['block_device_mapping'] = [
2095                 bdm for bdm in driver.block_device_info_get_mapping(
2096                                     block_device_info)
2097                 if bdm.get('connection_info')]
2098         else:
2099             driver_block_device.refresh_conn_infos(
2100                 driver.block_device_info_get_mapping(block_device_info),
2101                 context, instance, self.volume_api, self.driver)
2102 
2103         self._block_device_info_to_legacy(block_device_info)
2104 
2105         return block_device_info
2106 
2107     def _build_failed(self, node):
2108         if CONF.compute.consecutive_build_service_disable_threshold:
2109             # NOTE(danms): Update our counter, but wait for the next
2110             # update_available_resource() periodic to flush it to the DB
2111             self.rt.build_failed(node)
2112 
2113     def _build_succeeded(self, node):
2114         self.rt.build_succeeded(node)
2115 
2116     @wrap_exception()
2117     @reverts_task_state
2118     @wrap_instance_fault
2119     def build_and_run_instance(self, context, instance, image, request_spec,
2120                      filter_properties, admin_password=None,
2121                      injected_files=None, requested_networks=None,
2122                      security_groups=None, block_device_mapping=None,
2123                      node=None, limits=None, host_list=None):
2124 
2125         @utils.synchronized(instance.uuid)
2126         def _locked_do_build_and_run_instance(*args, **kwargs):
2127             # NOTE(danms): We grab the semaphore with the instance uuid
2128             # locked because we could wait in line to build this instance
2129             # for a while and we want to make sure that nothing else tries
2130             # to do anything with this instance while we wait.
2131             with self._build_semaphore:
2132                 try:
2133                     result = self._do_build_and_run_instance(*args, **kwargs)
2134                 except Exception:
2135                     # NOTE(mriedem): This should really only happen if
2136                     # _decode_files in _do_build_and_run_instance fails, and
2137                     # that's before a guest is spawned so it's OK to remove
2138                     # allocations for the instance for this node from Placement
2139                     # below as there is no guest consuming resources anyway.
2140                     # The _decode_files case could be handled more specifically
2141                     # but that's left for another day.
2142                     result = build_results.FAILED
2143                     raise
2144                 finally:
2145                     if result == build_results.FAILED:
2146                         # Remove the allocation records from Placement for the
2147                         # instance if the build failed. The instance.host is
2148                         # likely set to None in _do_build_and_run_instance
2149                         # which means if the user deletes the instance, it
2150                         # will be deleted in the API, not the compute service.
2151                         # Setting the instance.host to None in
2152                         # _do_build_and_run_instance means that the
2153                         # ResourceTracker will no longer consider this instance
2154                         # to be claiming resources against it, so we want to
2155                         # reflect that same thing in Placement.  No need to
2156                         # call this for a reschedule, as the allocations will
2157                         # have already been removed in
2158                         # self._do_build_and_run_instance().
2159                         self.reportclient.delete_allocation_for_instance(
2160                             context, instance.uuid)
2161 
2162                     if result in (build_results.FAILED,
2163                                   build_results.RESCHEDULED):
2164                         self._build_failed(node)
2165                     else:
2166                         self._build_succeeded(node)
2167 
2168         # NOTE(danms): We spawn here to return the RPC worker thread back to
2169         # the pool. Since what follows could take a really long time, we don't
2170         # want to tie up RPC workers.
2171         utils.spawn_n(_locked_do_build_and_run_instance,
2172                       context, instance, image, request_spec,
2173                       filter_properties, admin_password, injected_files,
2174                       requested_networks, security_groups,
2175                       block_device_mapping, node, limits, host_list)
2176 
2177     def _check_device_tagging(self, requested_networks, block_device_mapping):
2178         tagging_requested = False
2179         if requested_networks:
2180             for net in requested_networks:
2181                 if 'tag' in net and net.tag is not None:
2182                     tagging_requested = True
2183                     break
2184         if block_device_mapping and not tagging_requested:
2185             for bdm in block_device_mapping:
2186                 if 'tag' in bdm and bdm.tag is not None:
2187                     tagging_requested = True
2188                     break
2189         if (tagging_requested and
2190                 not self.driver.capabilities.get('supports_device_tagging',
2191                                                  False)):
2192             raise exception.BuildAbortException('Attempt to boot guest with '
2193                                                 'tagged devices on host that '
2194                                                 'does not support tagging.')
2195 
2196     def _check_trusted_certs(self, instance):
2197         if (instance.trusted_certs and
2198                 not self.driver.capabilities.get('supports_trusted_certs',
2199                                                  False)):
2200             raise exception.BuildAbortException(
2201                 'Trusted image certificates provided on host that does not '
2202                 'support certificate validation.')
2203 
2204     @hooks.add_hook('build_instance')
2205     @wrap_exception()
2206     @reverts_task_state
2207     @wrap_instance_event(prefix='compute')
2208     @wrap_instance_fault
2209     def _do_build_and_run_instance(self, context, instance, image,
2210             request_spec, filter_properties, admin_password, injected_files,
2211             requested_networks, security_groups, block_device_mapping,
2212             node=None, limits=None, host_list=None):
2213 
2214         try:
2215             LOG.debug('Starting instance...', instance=instance)
2216             instance.vm_state = vm_states.BUILDING
2217             instance.task_state = None
2218             instance.save(expected_task_state=
2219                     (task_states.SCHEDULING, None))
2220         except exception.InstanceNotFound:
2221             msg = 'Instance disappeared before build.'
2222             LOG.debug(msg, instance=instance)
2223             return build_results.FAILED
2224         except exception.UnexpectedTaskStateError as e:
2225             LOG.debug(e.format_message(), instance=instance)
2226             return build_results.FAILED
2227 
2228         # b64 decode the files to inject:
2229         decoded_files = self._decode_files(injected_files)
2230 
2231         if limits is None:
2232             limits = {}
2233 
2234         if node is None:
2235             node = self._get_nodename(instance, refresh=True)
2236 
2237         try:
2238             with timeutils.StopWatch() as timer:
2239                 self._build_and_run_instance(context, instance, image,
2240                         decoded_files, admin_password, requested_networks,
2241                         security_groups, block_device_mapping, node, limits,
2242                         filter_properties, request_spec)
2243             LOG.info('Took %0.2f seconds to build instance.',
2244                      timer.elapsed(), instance=instance)
2245             return build_results.ACTIVE
2246         except exception.RescheduledException as e:
2247             retry = filter_properties.get('retry')
2248             if not retry:
2249                 # no retry information, do not reschedule.
2250                 LOG.debug("Retry info not present, will not reschedule",
2251                     instance=instance)
2252                 self._cleanup_allocated_networks(context, instance,
2253                     requested_networks)
2254                 self._cleanup_volumes(context, instance,
2255                     block_device_mapping, raise_exc=False)
2256                 compute_utils.add_instance_fault_from_exc(context,
2257                         instance, e, sys.exc_info(),
2258                         fault_message=e.kwargs['reason'])
2259                 self._nil_out_instance_obj_host_and_node(instance)
2260                 self._set_instance_obj_error_state(context, instance,
2261                                                    clean_task_state=True)
2262                 return build_results.FAILED
2263             LOG.debug(e.format_message(), instance=instance)
2264             # This will be used for logging the exception
2265             retry['exc'] = traceback.format_exception(*sys.exc_info())
2266             # This will be used for setting the instance fault message
2267             retry['exc_reason'] = e.kwargs['reason']
2268             # NOTE(comstud): Deallocate networks if the driver wants
2269             # us to do so.
2270             # NOTE(mriedem): Always deallocate networking when using Neutron.
2271             # This is to unbind any ports that the user supplied in the server
2272             # create request, or delete any ports that nova created which were
2273             # meant to be bound to this host. This check intentionally bypasses
2274             # the result of deallocate_networks_on_reschedule because the
2275             # default value in the driver is False, but that method was really
2276             # only meant for Ironic and should be removed when nova-network is
2277             # removed (since is_neutron() will then always be True).
2278             # NOTE(vladikr): SR-IOV ports should be deallocated to
2279             # allow new sriov pci devices to be allocated on a new host.
2280             # Otherwise, if devices with pci addresses are already allocated
2281             # on the destination host, the instance will fail to spawn.
2282             # info_cache.network_info should be present at this stage.
2283             if (self.driver.deallocate_networks_on_reschedule(instance) or
2284                 utils.is_neutron() or
2285                 self.deallocate_sriov_ports_on_reschedule(instance)):
2286                 self._cleanup_allocated_networks(context, instance,
2287                         requested_networks)
2288             else:
2289                 # NOTE(alex_xu): Network already allocated and we don't
2290                 # want to deallocate them before rescheduling. But we need
2291                 # to cleanup those network resources setup on this host before
2292                 # rescheduling.
2293                 self.network_api.cleanup_instance_network_on_host(
2294                     context, instance, self.host)
2295 
2296             self._nil_out_instance_obj_host_and_node(instance)
2297             instance.task_state = task_states.SCHEDULING
2298             instance.save()
2299             # The instance will have already claimed resources from this host
2300             # before this build was attempted. Now that it has failed, we need
2301             # to unclaim those resources before casting to the conductor, so
2302             # that if there are alternate hosts available for a retry, it can
2303             # claim resources on that new host for the instance.
2304             self.reportclient.delete_allocation_for_instance(context,
2305                                                              instance.uuid)
2306 
2307             self.compute_task_api.build_instances(context, [instance],
2308                     image, filter_properties, admin_password,
2309                     injected_files, requested_networks, security_groups,
2310                     block_device_mapping, request_spec=request_spec,
2311                     host_lists=[host_list])
2312             return build_results.RESCHEDULED
2313         except (exception.InstanceNotFound,
2314                 exception.UnexpectedDeletingTaskStateError):
2315             msg = 'Instance disappeared during build.'
2316             LOG.debug(msg, instance=instance)
2317             self._cleanup_allocated_networks(context, instance,
2318                     requested_networks)
2319             return build_results.FAILED
2320         except Exception as e:
2321             if isinstance(e, exception.BuildAbortException):
2322                 LOG.error(e.format_message(), instance=instance)
2323             else:
2324                 # Should not reach here.
2325                 LOG.exception('Unexpected build failure, not rescheduling '
2326                               'build.', instance=instance)
2327             self._cleanup_allocated_networks(context, instance,
2328                     requested_networks)
2329             self._cleanup_volumes(context, instance,
2330                     block_device_mapping, raise_exc=False)
2331             compute_utils.add_instance_fault_from_exc(context, instance,
2332                     e, sys.exc_info())
2333             self._nil_out_instance_obj_host_and_node(instance)
2334             self._set_instance_obj_error_state(context, instance,
2335                                                clean_task_state=True)
2336             return build_results.FAILED
2337 
2338     def deallocate_sriov_ports_on_reschedule(self, instance):
2339         """Determine if networks are needed to be deallocated before reschedule
2340 
2341         Check the cached network info for any assigned SR-IOV ports.
2342         SR-IOV ports should be deallocated prior to rescheduling
2343         in order to allow new sriov pci devices to be allocated on a new host.
2344         """
2345         info_cache = instance.info_cache
2346 
2347         def _has_sriov_port(vif):
2348             return vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV
2349 
2350         if (info_cache and info_cache.network_info):
2351             for vif in info_cache.network_info:
2352                 if _has_sriov_port(vif):
2353                     return True
2354         return False
2355 
2356     @staticmethod
2357     def _get_scheduler_hints(filter_properties, request_spec=None):
2358         """Helper method to get scheduler hints.
2359 
2360         This method prefers to get the hints out of the request spec, but that
2361         might not be provided. Conductor will pass request_spec down to the
2362         first compute chosen for a build but older computes will not pass
2363         the request_spec to conductor's build_instances method for a
2364         a reschedule, so if we're on a host via a retry, request_spec may not
2365         be provided so we need to fallback to use the filter_properties
2366         to get scheduler hints.
2367         """
2368         hints = {}
2369         if request_spec is not None and 'scheduler_hints' in request_spec:
2370             hints = request_spec.scheduler_hints
2371         if not hints:
2372             hints = filter_properties.get('scheduler_hints') or {}
2373         return hints
2374 
2375     @staticmethod
2376     def _get_request_group_mapping(request_spec):
2377         """Return request group resource - provider mapping. This is currently
2378         used for Neutron ports that have resource request due to the port
2379         having QoS minimum bandwidth policy rule attached.
2380 
2381         :param request_spec: A RequestSpec object or None
2382         :returns: A dict keyed by RequestGroup requester_id, currently Neutron
2383         port_id, to resource provider UUID that provides resource for that
2384         RequestGroup. Or None if the request_spec was None.
2385         """
2386         if request_spec:
2387             return request_spec.get_request_group_mapping()
2388         else:
2389             return None
2390 
2391     def _build_and_run_instance(self, context, instance, image, injected_files,
2392             admin_password, requested_networks, security_groups,
2393             block_device_mapping, node, limits, filter_properties,
2394             request_spec=None):
2395 
2396         image_name = image.get('name')
2397         self._notify_about_instance_usage(context, instance, 'create.start',
2398                 extra_usage_info={'image_name': image_name})
2399         compute_utils.notify_about_instance_create(
2400             context, instance, self.host,
2401             phase=fields.NotificationPhase.START,
2402             bdms=block_device_mapping)
2403 
2404         # NOTE(mikal): cache the keystone roles associated with the instance
2405         # at boot time for later reference
2406         instance.system_metadata.update(
2407             {'boot_roles': ','.join(context.roles)})
2408 
2409         self._check_device_tagging(requested_networks, block_device_mapping)
2410         self._check_trusted_certs(instance)
2411 
2412         request_group_resource_providers_mapping = \
2413             self._get_request_group_mapping(request_spec)
2414 
2415         if request_group_resource_providers_mapping:
2416             try:
2417                 compute_utils\
2418                     .update_pci_request_spec_with_allocated_interface_name(
2419                         context, self.reportclient, instance,
2420                         request_group_resource_providers_mapping)
2421             except (exception.AmbiguousResourceProviderForPCIRequest,
2422                     exception.UnexpectedResourceProviderNameForPCIRequest
2423                     ) as e:
2424                 raise exception.BuildAbortException(
2425                     reason=six.text_type(e), instance_uuid=instance.uuid)
2426 
2427         # TODO(Luyao) cut over to get_allocs_for_consumer
2428         allocs = self.reportclient.get_allocations_for_consumer(
2429                 context, instance.uuid)
2430 
2431         try:
2432             scheduler_hints = self._get_scheduler_hints(filter_properties,
2433                                                         request_spec)
2434             with self.rt.instance_claim(context, instance, node, allocs,
2435                                         limits):
2436                 # NOTE(russellb) It's important that this validation be done
2437                 # *after* the resource tracker instance claim, as that is where
2438                 # the host is set on the instance.
2439                 self._validate_instance_group_policy(context, instance,
2440                                                      scheduler_hints)
2441                 image_meta = objects.ImageMeta.from_dict(image)
2442 
2443                 request_group_resource_providers_mapping = \
2444                     self._get_request_group_mapping(request_spec)
2445 
2446                 with self._build_resources(context, instance,
2447                         requested_networks, security_groups, image_meta,
2448                         block_device_mapping,
2449                         request_group_resource_providers_mapping) as resources:
2450                     instance.vm_state = vm_states.BUILDING
2451                     instance.task_state = task_states.SPAWNING
2452                     # NOTE(JoshNang) This also saves the changes to the
2453                     # instance from _allocate_network_async, as they aren't
2454                     # saved in that function to prevent races.
2455                     instance.save(expected_task_state=
2456                             task_states.BLOCK_DEVICE_MAPPING)
2457                     block_device_info = resources['block_device_info']
2458                     network_info = resources['network_info']
2459                     LOG.debug('Start spawning the instance on the hypervisor.',
2460                               instance=instance)
2461                     with timeutils.StopWatch() as timer:
2462                         self.driver.spawn(context, instance, image_meta,
2463                                           injected_files, admin_password,
2464                                           allocs, network_info=network_info,
2465                                           block_device_info=block_device_info)
2466                     LOG.info('Took %0.2f seconds to spawn the instance on '
2467                              'the hypervisor.', timer.elapsed(),
2468                              instance=instance)
2469         except (exception.InstanceNotFound,
2470                 exception.UnexpectedDeletingTaskStateError) as e:
2471             with excutils.save_and_reraise_exception():
2472                 self._notify_about_instance_usage(context, instance,
2473                     'create.error', fault=e)
2474                 tb = traceback.format_exc()
2475                 compute_utils.notify_about_instance_create(
2476                     context, instance, self.host,
2477                     phase=fields.NotificationPhase.ERROR, exception=e,
2478                     bdms=block_device_mapping, tb=tb)
2479         except exception.ComputeResourcesUnavailable as e:
2480             LOG.debug(e.format_message(), instance=instance)
2481             self._notify_about_instance_usage(context, instance,
2482                     'create.error', fault=e)
2483             tb = traceback.format_exc()
2484             compute_utils.notify_about_instance_create(
2485                     context, instance, self.host,
2486                     phase=fields.NotificationPhase.ERROR, exception=e,
2487                     bdms=block_device_mapping, tb=tb)
2488             raise exception.RescheduledException(
2489                     instance_uuid=instance.uuid, reason=e.format_message())
2490         except exception.BuildAbortException as e:
2491             with excutils.save_and_reraise_exception():
2492                 LOG.debug(e.format_message(), instance=instance)
2493                 self._notify_about_instance_usage(context, instance,
2494                     'create.error', fault=e)
2495                 tb = traceback.format_exc()
2496                 compute_utils.notify_about_instance_create(
2497                     context, instance, self.host,
2498                     phase=fields.NotificationPhase.ERROR, exception=e,
2499                     bdms=block_device_mapping, tb=tb)
2500         except (exception.FixedIpLimitExceeded,
2501                 exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:
2502             LOG.warning('No more network or fixed IP to be allocated',
2503                         instance=instance)
2504             self._notify_about_instance_usage(context, instance,
2505                     'create.error', fault=e)
2506             tb = traceback.format_exc()
2507             compute_utils.notify_about_instance_create(
2508                     context, instance, self.host,
2509                     phase=fields.NotificationPhase.ERROR, exception=e,
2510                     bdms=block_device_mapping, tb=tb)
2511             msg = _('Failed to allocate the network(s) with error %s, '
2512                     'not rescheduling.') % e.format_message()
2513             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2514                     reason=msg)
2515         except (exception.VirtualInterfaceCreateException,
2516                 exception.VirtualInterfaceMacAddressException,
2517                 exception.FixedIpInvalidOnHost,
2518                 exception.UnableToAutoAllocateNetwork,
2519                 exception.NetworksWithQoSPolicyNotSupported) as e:
2520             LOG.exception('Failed to allocate network(s)',
2521                           instance=instance)
2522             self._notify_about_instance_usage(context, instance,
2523                     'create.error', fault=e)
2524             tb = traceback.format_exc()
2525             compute_utils.notify_about_instance_create(
2526                     context, instance, self.host,
2527                     phase=fields.NotificationPhase.ERROR, exception=e,
2528                     bdms=block_device_mapping, tb=tb)
2529             msg = _('Failed to allocate the network(s), not rescheduling.')
2530             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2531                     reason=msg)
2532         except (exception.FlavorDiskTooSmall,
2533                 exception.FlavorMemoryTooSmall,
2534                 exception.ImageNotActive,
2535                 exception.ImageUnacceptable,
2536                 exception.InvalidDiskInfo,
2537                 exception.InvalidDiskFormat,
2538                 cursive_exception.SignatureVerificationError,
2539                 exception.CertificateValidationFailed,
2540                 exception.VolumeEncryptionNotSupported,
2541                 exception.InvalidInput,
2542                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2543                 # in the API during server create and rebuild.
2544                 exception.RequestedVRamTooHigh) as e:
2545             self._notify_about_instance_usage(context, instance,
2546                     'create.error', fault=e)
2547             tb = traceback.format_exc()
2548             compute_utils.notify_about_instance_create(
2549                     context, instance, self.host,
2550                     phase=fields.NotificationPhase.ERROR, exception=e,
2551                     bdms=block_device_mapping, tb=tb)
2552             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2553                     reason=e.format_message())
2554         except Exception as e:
2555             LOG.exception('Failed to build and run instance',
2556                           instance=instance)
2557             self._notify_about_instance_usage(context, instance,
2558                     'create.error', fault=e)
2559             tb = traceback.format_exc()
2560             compute_utils.notify_about_instance_create(
2561                     context, instance, self.host,
2562                     phase=fields.NotificationPhase.ERROR, exception=e,
2563                     bdms=block_device_mapping, tb=tb)
2564             raise exception.RescheduledException(
2565                     instance_uuid=instance.uuid, reason=six.text_type(e))
2566 
2567         # NOTE(alaski): This is only useful during reschedules, remove it now.
2568         instance.system_metadata.pop('network_allocated', None)
2569 
2570         # If CONF.default_access_ip_network_name is set, grab the
2571         # corresponding network and set the access ip values accordingly.
2572         network_name = CONF.default_access_ip_network_name
2573         if (network_name and not instance.access_ip_v4 and
2574                 not instance.access_ip_v6):
2575             # Note that when there are multiple ips to choose from, an
2576             # arbitrary one will be chosen.
2577             for vif in network_info:
2578                 if vif['network']['label'] == network_name:
2579                     for ip in vif.fixed_ips():
2580                         if not instance.access_ip_v4 and ip['version'] == 4:
2581                             instance.access_ip_v4 = ip['address']
2582                         if not instance.access_ip_v6 and ip['version'] == 6:
2583                             instance.access_ip_v6 = ip['address']
2584                     break
2585 
2586         self._update_instance_after_spawn(context, instance)
2587 
2588         try:
2589             instance.save(expected_task_state=task_states.SPAWNING)
2590         except (exception.InstanceNotFound,
2591                 exception.UnexpectedDeletingTaskStateError) as e:
2592             with excutils.save_and_reraise_exception():
2593                 self._notify_about_instance_usage(context, instance,
2594                     'create.error', fault=e)
2595                 tb = traceback.format_exc()
2596                 compute_utils.notify_about_instance_create(
2597                     context, instance, self.host,
2598                     phase=fields.NotificationPhase.ERROR, exception=e,
2599                     bdms=block_device_mapping, tb=tb)
2600 
2601         self._update_scheduler_instance_info(context, instance)
2602         self._notify_about_instance_usage(context, instance, 'create.end',
2603                 extra_usage_info={'message': _('Success')},
2604                 network_info=network_info)
2605         compute_utils.notify_about_instance_create(context, instance,
2606                 self.host, phase=fields.NotificationPhase.END,
2607                 bdms=block_device_mapping)
2608 
2609     @contextlib.contextmanager
2610     def _build_resources(self, context, instance, requested_networks,
2611                          security_groups, image_meta, block_device_mapping,
2612                          resource_provider_mapping):
2613         resources = {}
2614         network_info = None
2615         try:
2616             LOG.debug('Start building networks asynchronously for instance.',
2617                       instance=instance)
2618             network_info = self._build_networks_for_instance(context, instance,
2619                     requested_networks, security_groups,
2620                     resource_provider_mapping)
2621             resources['network_info'] = network_info
2622         except (exception.InstanceNotFound,
2623                 exception.UnexpectedDeletingTaskStateError):
2624             raise
2625         except exception.UnexpectedTaskStateError as e:
2626             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2627                     reason=e.format_message())
2628         except Exception:
2629             # Because this allocation is async any failures are likely to occur
2630             # when the driver accesses network_info during spawn().
2631             LOG.exception('Failed to allocate network(s)',
2632                           instance=instance)
2633             msg = _('Failed to allocate the network(s), not rescheduling.')
2634             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2635                     reason=msg)
2636 
2637         try:
2638             # Perform any driver preparation work for the driver.
2639             self.driver.prepare_for_spawn(instance)
2640 
2641             # Depending on a virt driver, some network configuration is
2642             # necessary before preparing block devices.
2643             self.driver.prepare_networks_before_block_device_mapping(
2644                 instance, network_info)
2645 
2646             # Verify that all the BDMs have a device_name set and assign a
2647             # default to the ones missing it with the help of the driver.
2648             self._default_block_device_names(instance, image_meta,
2649                                              block_device_mapping)
2650 
2651             LOG.debug('Start building block device mappings for instance.',
2652                       instance=instance)
2653             instance.vm_state = vm_states.BUILDING
2654             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2655             instance.save()
2656 
2657             block_device_info = self._prep_block_device(context, instance,
2658                     block_device_mapping)
2659             resources['block_device_info'] = block_device_info
2660         except (exception.InstanceNotFound,
2661                 exception.UnexpectedDeletingTaskStateError):
2662             with excutils.save_and_reraise_exception():
2663                 # Make sure the async call finishes
2664                 if network_info is not None:
2665                     network_info.wait(do_raise=False)
2666                     self.driver.clean_networks_preparation(instance,
2667                                                            network_info)
2668                 self.driver.failed_spawn_cleanup(instance)
2669         except (exception.UnexpectedTaskStateError,
2670                 exception.OverQuota, exception.InvalidBDM) as e:
2671             # Make sure the async call finishes
2672             if network_info is not None:
2673                 network_info.wait(do_raise=False)
2674                 self.driver.clean_networks_preparation(instance, network_info)
2675             self.driver.failed_spawn_cleanup(instance)
2676             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2677                     reason=e.format_message())
2678         except Exception:
2679             LOG.exception('Failure prepping block device',
2680                           instance=instance)
2681             # Make sure the async call finishes
2682             if network_info is not None:
2683                 network_info.wait(do_raise=False)
2684                 self.driver.clean_networks_preparation(instance, network_info)
2685             self.driver.failed_spawn_cleanup(instance)
2686             msg = _('Failure prepping block device.')
2687             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2688                     reason=msg)
2689 
2690         try:
2691             yield resources
2692         except Exception as exc:
2693             with excutils.save_and_reraise_exception() as ctxt:
2694                 if not isinstance(exc, (
2695                         exception.InstanceNotFound,
2696                         exception.UnexpectedDeletingTaskStateError)):
2697                     LOG.exception('Instance failed to spawn',
2698                                   instance=instance)
2699                 # Make sure the async call finishes
2700                 if network_info is not None:
2701                     network_info.wait(do_raise=False)
2702                 # if network_info is empty we're likely here because of
2703                 # network allocation failure. Since nothing can be reused on
2704                 # rescheduling it's better to deallocate network to eliminate
2705                 # the chance of orphaned ports in neutron
2706                 deallocate_networks = False if network_info else True
2707                 try:
2708                     self._shutdown_instance(context, instance,
2709                             block_device_mapping, requested_networks,
2710                             try_deallocate_networks=deallocate_networks)
2711                 except Exception as exc2:
2712                     ctxt.reraise = False
2713                     LOG.warning('Could not clean up failed build,'
2714                                 ' not rescheduling. Error: %s',
2715                                 six.text_type(exc2))
2716                     raise exception.BuildAbortException(
2717                             instance_uuid=instance.uuid,
2718                             reason=six.text_type(exc))
2719 
2720     def _cleanup_allocated_networks(self, context, instance,
2721             requested_networks):
2722         try:
2723             self._deallocate_network(context, instance, requested_networks)
2724         except Exception:
2725             LOG.exception('Failed to deallocate networks', instance=instance)
2726             return
2727 
2728         instance.system_metadata['network_allocated'] = 'False'
2729         try:
2730             instance.save()
2731         except exception.InstanceNotFound:
2732             # NOTE(alaski): It's possible that we're cleaning up the networks
2733             # because the instance was deleted.  If that's the case then this
2734             # exception will be raised by instance.save()
2735             pass
2736 
2737     def _try_deallocate_network(self, context, instance,
2738                                 requested_networks=None):
2739 
2740         # During auto-scale cleanup, we could be deleting a large number
2741         # of servers at the same time and overloading parts of the system,
2742         # so we retry a few times in case of connection failures to the
2743         # networking service.
2744         @loopingcall.RetryDecorator(
2745             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2746             exceptions=(keystone_exception.connection.ConnectFailure,))
2747         def _deallocate_network_with_retries():
2748             try:
2749                 self._deallocate_network(
2750                     context, instance, requested_networks)
2751             except keystone_exception.connection.ConnectFailure as e:
2752                 # Provide a warning that something is amiss.
2753                 with excutils.save_and_reraise_exception():
2754                     LOG.warning('Failed to deallocate network for instance; '
2755                                 'retrying. Error: %s', six.text_type(e),
2756                                 instance=instance)
2757 
2758         try:
2759             # tear down allocated network structure
2760             _deallocate_network_with_retries()
2761         except Exception as ex:
2762             with excutils.save_and_reraise_exception():
2763                 LOG.error('Failed to deallocate network for instance. '
2764                           'Error: %s', ex, instance=instance)
2765                 self._set_instance_obj_error_state(context, instance)
2766 
2767     def _get_power_off_values(self, context, instance, clean_shutdown):
2768         """Get the timing configuration for powering down this instance."""
2769         if clean_shutdown:
2770             timeout = compute_utils.get_value_from_system_metadata(instance,
2771                           key='image_os_shutdown_timeout', type=int,
2772                           default=CONF.shutdown_timeout)
2773             retry_interval = CONF.compute.shutdown_retry_interval
2774         else:
2775             timeout = 0
2776             retry_interval = 0
2777 
2778         return timeout, retry_interval
2779 
2780     def _power_off_instance(self, context, instance, clean_shutdown=True):
2781         """Power off an instance on this host."""
2782         timeout, retry_interval = self._get_power_off_values(context,
2783                                         instance, clean_shutdown)
2784         self.driver.power_off(instance, timeout, retry_interval)
2785 
2786     def _shutdown_instance(self, context, instance,
2787                            bdms, requested_networks=None, notify=True,
2788                            try_deallocate_networks=True):
2789         """Shutdown an instance on this host.
2790 
2791         :param:context: security context
2792         :param:instance: a nova.objects.Instance object
2793         :param:bdms: the block devices for the instance to be torn
2794                      down
2795         :param:requested_networks: the networks on which the instance
2796                                    has ports
2797         :param:notify: true if a final usage notification should be
2798                        emitted
2799         :param:try_deallocate_networks: false if we should avoid
2800                                         trying to teardown networking
2801         """
2802         context = context.elevated()
2803         LOG.info('Terminating instance', instance=instance)
2804 
2805         if notify:
2806             self._notify_about_instance_usage(context, instance,
2807                                               "shutdown.start")
2808             compute_utils.notify_about_instance_action(context, instance,
2809                     self.host, action=fields.NotificationAction.SHUTDOWN,
2810                     phase=fields.NotificationPhase.START, bdms=bdms)
2811 
2812         network_info = instance.get_network_info()
2813 
2814         # NOTE(arnaudmorin) to avoid nova destroying the instance without
2815         # unplugging the interface, refresh network_info if it is empty.
2816         if not network_info:
2817             network_info = self.network_api.get_instance_nw_info(
2818                 context, instance)
2819 
2820         # NOTE(vish) get bdms before destroying the instance
2821         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2822         block_device_info = self._get_instance_block_device_info(
2823             context, instance, bdms=bdms)
2824 
2825         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2826         #                want to keep ip allocated for certain failures
2827         try:
2828             LOG.debug('Start destroying the instance on the hypervisor.',
2829                       instance=instance)
2830             with timeutils.StopWatch() as timer:
2831                 self.driver.destroy(context, instance, network_info,
2832                                     block_device_info)
2833             LOG.info('Took %0.2f seconds to destroy the instance on the '
2834                      'hypervisor.', timer.elapsed(), instance=instance)
2835         except exception.InstancePowerOffFailure:
2836             # if the instance can't power off, don't release the ip
2837             with excutils.save_and_reraise_exception():
2838                 pass
2839         except Exception:
2840             with excutils.save_and_reraise_exception():
2841                 # deallocate ip and fail without proceeding to
2842                 # volume api calls, preserving current behavior
2843                 if try_deallocate_networks:
2844                     self._try_deallocate_network(context, instance,
2845                                                  requested_networks)
2846 
2847         if try_deallocate_networks:
2848             self._try_deallocate_network(context, instance, requested_networks)
2849 
2850         timer.restart()
2851         for bdm in vol_bdms:
2852             try:
2853                 if bdm.attachment_id:
2854                     self.volume_api.attachment_delete(context,
2855                                                       bdm.attachment_id)
2856                 else:
2857                     # NOTE(vish): actual driver detach done in driver.destroy,
2858                     #             so just tell cinder that we are done with it.
2859                     connector = self.driver.get_volume_connector(instance)
2860                     self.volume_api.terminate_connection(context,
2861                                                          bdm.volume_id,
2862                                                          connector)
2863                     self.volume_api.detach(context, bdm.volume_id,
2864                                            instance.uuid)
2865 
2866             except exception.VolumeAttachmentNotFound as exc:
2867                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2868                           instance=instance)
2869             except exception.DiskNotFound as exc:
2870                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2871                           instance=instance)
2872             except exception.VolumeNotFound as exc:
2873                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2874                           instance=instance)
2875             except (cinder_exception.EndpointNotFound,
2876                     keystone_exception.EndpointNotFound) as exc:
2877                 LOG.warning('Ignoring EndpointNotFound for '
2878                             'volume %(volume_id)s: %(exc)s',
2879                             {'exc': exc, 'volume_id': bdm.volume_id},
2880                             instance=instance)
2881             except cinder_exception.ClientException as exc:
2882                 LOG.warning('Ignoring unknown cinder exception for '
2883                             'volume %(volume_id)s: %(exc)s',
2884                             {'exc': exc, 'volume_id': bdm.volume_id},
2885                             instance=instance)
2886             except Exception as exc:
2887                 LOG.warning('Ignoring unknown exception for '
2888                             'volume %(volume_id)s: %(exc)s',
2889                             {'exc': exc, 'volume_id': bdm.volume_id},
2890                             instance=instance)
2891         if vol_bdms:
2892             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2893                      'for instance.',
2894                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2895                      instance=instance)
2896 
2897         if notify:
2898             self._notify_about_instance_usage(context, instance,
2899                                               "shutdown.end")
2900             compute_utils.notify_about_instance_action(context, instance,
2901                     self.host, action=fields.NotificationAction.SHUTDOWN,
2902                     phase=fields.NotificationPhase.END, bdms=bdms)
2903 
2904     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2905                          detach=True):
2906         exc_info = None
2907         for bdm in bdms:
2908             if detach and bdm.volume_id:
2909                 try:
2910                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2911                               instance_uuid=instance.uuid)
2912                     destroy = bdm.delete_on_termination
2913                     self._detach_volume(context, bdm, instance,
2914                                         destroy_bdm=destroy)
2915                 except Exception as exc:
2916                     exc_info = sys.exc_info()
2917                     LOG.warning('Failed to detach volume: %(volume_id)s '
2918                                 'due to %(exc)s',
2919                                 {'volume_id': bdm.volume_id, 'exc': exc})
2920 
2921             if bdm.volume_id and bdm.delete_on_termination:
2922                 try:
2923                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2924                               instance_uuid=instance.uuid)
2925                     self.volume_api.delete(context, bdm.volume_id)
2926                 except Exception as exc:
2927                     exc_info = sys.exc_info()
2928                     LOG.warning('Failed to delete volume: %(volume_id)s '
2929                                 'due to %(exc)s',
2930                                 {'volume_id': bdm.volume_id, 'exc': exc})
2931         if exc_info is not None and raise_exc:
2932             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2933 
2934     @hooks.add_hook("delete_instance")
2935     def _delete_instance(self, context, instance, bdms):
2936         """Delete an instance on this host.
2937 
2938         :param context: nova request context
2939         :param instance: nova.objects.instance.Instance object
2940         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2941         """
2942         events = self.instance_events.clear_events_for_instance(instance)
2943         if events:
2944             LOG.debug('Events pending at deletion: %(events)s',
2945                       {'events': ','.join(events.keys())},
2946                       instance=instance)
2947         self._notify_about_instance_usage(context, instance,
2948                                           "delete.start")
2949         compute_utils.notify_about_instance_action(context, instance,
2950                 self.host, action=fields.NotificationAction.DELETE,
2951                 phase=fields.NotificationPhase.START, bdms=bdms)
2952 
2953         self._shutdown_instance(context, instance, bdms)
2954 
2955         # NOTE(vish): We have already deleted the instance, so we have
2956         #             to ignore problems cleaning up the volumes. It
2957         #             would be nice to let the user know somehow that
2958         #             the volume deletion failed, but it is not
2959         #             acceptable to have an instance that can not be
2960         #             deleted. Perhaps this could be reworked in the
2961         #             future to set an instance fault the first time
2962         #             and to only ignore the failure if the instance
2963         #             is already in ERROR.
2964 
2965         # NOTE(ameeda): The volumes already detached during the above
2966         #               _shutdown_instance() call and this is why
2967         #               detach is not requested from _cleanup_volumes()
2968         #               in this case
2969 
2970         self._cleanup_volumes(context, instance, bdms,
2971                 raise_exc=False, detach=False)
2972         # if a delete task succeeded, always update vm state and task
2973         # state without expecting task state to be DELETING
2974         instance.vm_state = vm_states.DELETED
2975         instance.task_state = None
2976         instance.power_state = power_state.NOSTATE
2977         instance.terminated_at = timeutils.utcnow()
2978         instance.save()
2979 
2980         self._complete_deletion(context, instance)
2981         # only destroy the instance in the db if the _complete_deletion
2982         # doesn't raise and therefore allocation is successfully
2983         # deleted in placement
2984         instance.destroy()
2985 
2986         self._notify_about_instance_usage(context, instance, "delete.end")
2987         compute_utils.notify_about_instance_action(context, instance,
2988                 self.host, action=fields.NotificationAction.DELETE,
2989                 phase=fields.NotificationPhase.END, bdms=bdms)
2990 
2991     @wrap_exception()
2992     @reverts_task_state
2993     @wrap_instance_event(prefix='compute')
2994     @wrap_instance_fault
2995     def terminate_instance(self, context, instance, bdms):
2996         """Terminate an instance on this host."""
2997         @utils.synchronized(instance.uuid)
2998         def do_terminate_instance(instance, bdms):
2999             # NOTE(mriedem): If we are deleting the instance while it was
3000             # booting from volume, we could be racing with a database update of
3001             # the BDM volume_id. Since the compute API passes the BDMs over RPC
3002             # to compute here, the BDMs may be stale at this point. So check
3003             # for any volume BDMs that don't have volume_id set and if we
3004             # detect that, we need to refresh the BDM list before proceeding.
3005             # TODO(mriedem): Move this into _delete_instance and make the bdms
3006             # parameter optional.
3007             for bdm in list(bdms):
3008                 if bdm.is_volume and not bdm.volume_id:
3009                     LOG.debug('There are potentially stale BDMs during '
3010                               'delete, refreshing the BlockDeviceMappingList.',
3011                               instance=instance)
3012                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3013                         context, instance.uuid)
3014                     break
3015             try:
3016                 self._delete_instance(context, instance, bdms)
3017             except exception.InstanceNotFound:
3018                 LOG.info("Instance disappeared during terminate",
3019                          instance=instance)
3020             except Exception:
3021                 # As we're trying to delete always go to Error if something
3022                 # goes wrong that _delete_instance can't handle.
3023                 with excutils.save_and_reraise_exception():
3024                     LOG.exception('Setting instance vm_state to ERROR',
3025                                   instance=instance)
3026                     self._set_instance_obj_error_state(context, instance)
3027 
3028         do_terminate_instance(instance, bdms)
3029 
3030     # NOTE(johannes): This is probably better named power_off_instance
3031     # so it matches the driver method, but because of other issues, we
3032     # can't use that name in grizzly.
3033     @wrap_exception()
3034     @reverts_task_state
3035     @wrap_instance_event(prefix='compute')
3036     @wrap_instance_fault
3037     def stop_instance(self, context, instance, clean_shutdown):
3038         """Stopping an instance on this host."""
3039 
3040         @utils.synchronized(instance.uuid)
3041         def do_stop_instance():
3042             current_power_state = self._get_power_state(context, instance)
3043             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
3044                       'current task_state: %(task_state)s, current DB '
3045                       'power_state: %(db_power_state)s, current VM '
3046                       'power_state: %(current_power_state)s',
3047                       {'vm_state': instance.vm_state,
3048                        'task_state': instance.task_state,
3049                        'db_power_state': instance.power_state,
3050                        'current_power_state': current_power_state},
3051                       instance_uuid=instance.uuid)
3052 
3053             # NOTE(mriedem): If the instance is already powered off, we are
3054             # possibly tearing down and racing with other operations, so we can
3055             # expect the task_state to be None if something else updates the
3056             # instance and we're not locking it.
3057             expected_task_state = [task_states.POWERING_OFF]
3058             # The list of power states is from _sync_instance_power_state.
3059             if current_power_state in (power_state.NOSTATE,
3060                                        power_state.SHUTDOWN,
3061                                        power_state.CRASHED):
3062                 LOG.info('Instance is already powered off in the '
3063                          'hypervisor when stop is called.',
3064                          instance=instance)
3065                 expected_task_state.append(None)
3066 
3067             self._notify_about_instance_usage(context, instance,
3068                                               "power_off.start")
3069 
3070             compute_utils.notify_about_instance_action(context, instance,
3071                         self.host, action=fields.NotificationAction.POWER_OFF,
3072                         phase=fields.NotificationPhase.START)
3073 
3074             self._power_off_instance(context, instance, clean_shutdown)
3075             instance.power_state = self._get_power_state(context, instance)
3076             instance.vm_state = vm_states.STOPPED
3077             instance.task_state = None
3078             instance.save(expected_task_state=expected_task_state)
3079             self._notify_about_instance_usage(context, instance,
3080                                               "power_off.end")
3081 
3082             compute_utils.notify_about_instance_action(context, instance,
3083                         self.host, action=fields.NotificationAction.POWER_OFF,
3084                         phase=fields.NotificationPhase.END)
3085 
3086         do_stop_instance()
3087 
3088     def _power_on(self, context, instance):
3089         network_info = self.network_api.get_instance_nw_info(context, instance)
3090         block_device_info = self._get_instance_block_device_info(context,
3091                                                                  instance)
3092         self.driver.power_on(context, instance,
3093                              network_info,
3094                              block_device_info)
3095 
3096     def _delete_snapshot_of_shelved_instance(self, context, instance,
3097                                              snapshot_id):
3098         """Delete snapshot of shelved instance."""
3099         try:
3100             self.image_api.delete(context, snapshot_id)
3101         except (exception.ImageNotFound,
3102                 exception.ImageNotAuthorized) as exc:
3103             LOG.warning("Failed to delete snapshot "
3104                         "from shelved instance (%s).",
3105                         exc.format_message(), instance=instance)
3106         except Exception:
3107             LOG.exception("Something wrong happened when trying to "
3108                           "delete snapshot from shelved instance.",
3109                           instance=instance)
3110 
3111     # NOTE(johannes): This is probably better named power_on_instance
3112     # so it matches the driver method, but because of other issues, we
3113     # can't use that name in grizzly.
3114     @wrap_exception()
3115     @reverts_task_state
3116     @wrap_instance_event(prefix='compute')
3117     @wrap_instance_fault
3118     def start_instance(self, context, instance):
3119         """Starting an instance on this host."""
3120         self._notify_about_instance_usage(context, instance, "power_on.start")
3121         compute_utils.notify_about_instance_action(context, instance,
3122             self.host, action=fields.NotificationAction.POWER_ON,
3123             phase=fields.NotificationPhase.START)
3124         self._power_on(context, instance)
3125         instance.power_state = self._get_power_state(context, instance)
3126         instance.vm_state = vm_states.ACTIVE
3127         instance.task_state = None
3128 
3129         # Delete an image(VM snapshot) for a shelved instance
3130         snapshot_id = instance.system_metadata.get('shelved_image_id')
3131         if snapshot_id:
3132             self._delete_snapshot_of_shelved_instance(context, instance,
3133                                                       snapshot_id)
3134 
3135         # Delete system_metadata for a shelved instance
3136         compute_utils.remove_shelved_keys_from_system_metadata(instance)
3137 
3138         instance.save(expected_task_state=task_states.POWERING_ON)
3139         self._notify_about_instance_usage(context, instance, "power_on.end")
3140         compute_utils.notify_about_instance_action(context, instance,
3141             self.host, action=fields.NotificationAction.POWER_ON,
3142             phase=fields.NotificationPhase.END)
3143 
3144     @messaging.expected_exceptions(NotImplementedError,
3145                                    exception.TriggerCrashDumpNotSupported,
3146                                    exception.InstanceNotRunning)
3147     @wrap_exception()
3148     @wrap_instance_event(prefix='compute')
3149     @wrap_instance_fault
3150     def trigger_crash_dump(self, context, instance):
3151         """Trigger crash dump in an instance."""
3152 
3153         self._notify_about_instance_usage(context, instance,
3154                                           "trigger_crash_dump.start")
3155         compute_utils.notify_about_instance_action(context, instance,
3156                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3157                 phase=fields.NotificationPhase.START)
3158 
3159         # This method does not change task_state and power_state because the
3160         # effect of a trigger depends on user's configuration.
3161         self.driver.trigger_crash_dump(instance)
3162 
3163         self._notify_about_instance_usage(context, instance,
3164                                           "trigger_crash_dump.end")
3165         compute_utils.notify_about_instance_action(context, instance,
3166                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3167                 phase=fields.NotificationPhase.END)
3168 
3169     @wrap_exception()
3170     @reverts_task_state
3171     @wrap_instance_event(prefix='compute')
3172     @wrap_instance_fault
3173     def soft_delete_instance(self, context, instance):
3174         """Soft delete an instance on this host."""
3175         with compute_utils.notify_about_instance_delete(
3176                 self.notifier, context, instance, 'soft_delete',
3177                 source=fields.NotificationSource.COMPUTE):
3178             try:
3179                 self.driver.soft_delete(instance)
3180             except NotImplementedError:
3181                 # Fallback to just powering off the instance if the
3182                 # hypervisor doesn't implement the soft_delete method
3183                 self.driver.power_off(instance)
3184             instance.power_state = self._get_power_state(context, instance)
3185             instance.vm_state = vm_states.SOFT_DELETED
3186             instance.task_state = None
3187             instance.save(expected_task_state=[task_states.SOFT_DELETING])
3188 
3189     @wrap_exception()
3190     @reverts_task_state
3191     @wrap_instance_event(prefix='compute')
3192     @wrap_instance_fault
3193     def restore_instance(self, context, instance):
3194         """Restore a soft-deleted instance on this host."""
3195         self._notify_about_instance_usage(context, instance, "restore.start")
3196         compute_utils.notify_about_instance_action(context, instance,
3197             self.host, action=fields.NotificationAction.RESTORE,
3198             phase=fields.NotificationPhase.START)
3199         try:
3200             self.driver.restore(instance)
3201         except NotImplementedError:
3202             # Fallback to just powering on the instance if the hypervisor
3203             # doesn't implement the restore method
3204             self._power_on(context, instance)
3205         instance.power_state = self._get_power_state(context, instance)
3206         instance.vm_state = vm_states.ACTIVE
3207         instance.task_state = None
3208         instance.save(expected_task_state=task_states.RESTORING)
3209         self._notify_about_instance_usage(context, instance, "restore.end")
3210         compute_utils.notify_about_instance_action(context, instance,
3211             self.host, action=fields.NotificationAction.RESTORE,
3212             phase=fields.NotificationPhase.END)
3213 
3214     @staticmethod
3215     def _set_migration_status(migration, status):
3216         """Set the status, and guard against a None being passed in.
3217 
3218         This is useful as some of the compute RPC calls will not pass
3219         a migration object in older versions. The check can be removed when
3220         we move past 4.x major version of the RPC API.
3221         """
3222         if migration:
3223             migration.status = status
3224             migration.save()
3225 
3226     def _rebuild_default_impl(self, context, instance, image_meta,
3227                               injected_files, admin_password, allocations,
3228                               bdms, detach_block_devices, attach_block_devices,
3229                               network_info=None,
3230                               evacuate=False, block_device_info=None,
3231                               preserve_ephemeral=False):
3232         if preserve_ephemeral:
3233             # The default code path does not support preserving ephemeral
3234             # partitions.
3235             raise exception.PreserveEphemeralNotSupported()
3236 
3237         if evacuate:
3238             detach_block_devices(context, bdms)
3239         else:
3240             self._power_off_instance(context, instance, clean_shutdown=True)
3241             detach_block_devices(context, bdms)
3242             self.driver.destroy(context, instance,
3243                                 network_info=network_info,
3244                                 block_device_info=block_device_info)
3245 
3246         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
3247         instance.save(expected_task_state=[task_states.REBUILDING])
3248 
3249         new_block_device_info = attach_block_devices(context, instance, bdms)
3250 
3251         instance.task_state = task_states.REBUILD_SPAWNING
3252         instance.save(
3253             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
3254 
3255         with instance.mutated_migration_context():
3256             self.driver.spawn(context, instance, image_meta, injected_files,
3257                               admin_password, allocations,
3258                               network_info=network_info,
3259                               block_device_info=new_block_device_info)
3260 
3261     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
3262         tb = traceback.format_exc()
3263         self._notify_about_instance_usage(context, instance,
3264                                           'rebuild.error', fault=error)
3265         compute_utils.notify_about_instance_rebuild(
3266             context, instance, self.host,
3267             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms,
3268             tb=tb)
3269 
3270     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
3271     @wrap_exception()
3272     @reverts_task_state
3273     @wrap_instance_event(prefix='compute')
3274     @wrap_instance_fault
3275     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
3276                          injected_files, new_pass, orig_sys_metadata,
3277                          bdms, recreate, on_shared_storage,
3278                          preserve_ephemeral, migration,
3279                          scheduled_node, limits, request_spec):
3280         """Destroy and re-make this instance.
3281 
3282         A 'rebuild' effectively purges all existing data from the system and
3283         remakes the VM with given 'metadata' and 'personalities'.
3284 
3285         :param context: `nova.RequestContext` object
3286         :param instance: Instance object
3287         :param orig_image_ref: Original image_ref before rebuild
3288         :param image_ref: New image_ref for rebuild
3289         :param injected_files: Files to inject
3290         :param new_pass: password to set on rebuilt instance
3291         :param orig_sys_metadata: instance system metadata from pre-rebuild
3292         :param bdms: block-device-mappings to use for rebuild
3293         :param recreate: True if the instance is being evacuated (e.g. the
3294             hypervisor it was on failed) - cleanup of old state will be
3295             skipped.
3296         :param on_shared_storage: True if instance files on shared storage.
3297                                   If not provided then information from the
3298                                   driver will be used to decide if the instance
3299                                   files are available or not on the target host
3300         :param preserve_ephemeral: True if the default ephemeral storage
3301                                    partition must be preserved on rebuild
3302         :param migration: a Migration object if one was created for this
3303                           rebuild operation (if it's a part of evacuate)
3304         :param scheduled_node: A node of the host chosen by the scheduler. If a
3305                                host was specified by the user, this will be
3306                                None
3307         :param limits: Overcommit limits set by the scheduler. If a host was
3308                        specified by the user, this will be None
3309         :param request_spec: a RequestSpec object used to schedule the instance
3310 
3311         """
3312         # recreate=True means the instance is being evacuated from a failed
3313         # host to a new destination host (this host). The 'recreate' variable
3314         # name is confusing, so rename it to evacuate here at the top, which
3315         # is simpler than renaming a parameter in an RPC versioned method.
3316         evacuate = recreate
3317         context = context.elevated()
3318 
3319         if evacuate:
3320             LOG.info("Evacuating instance", instance=instance)
3321         else:
3322             LOG.info("Rebuilding instance", instance=instance)
3323 
3324         if evacuate:
3325             # This is an evacuation to a new host, so we need to perform a
3326             # resource claim.
3327             rebuild_claim = self.rt.rebuild_claim
3328         else:
3329             # This is a rebuild to the same host, so we don't need to make
3330             # a claim since the instance is already on this host.
3331             rebuild_claim = claims.NopClaim
3332 
3333         if image_ref:
3334             image_meta = objects.ImageMeta.from_image_ref(
3335                 context, self.image_api, image_ref)
3336         elif evacuate:
3337             # For evacuate the API does not send down the image_ref since the
3338             # image does not change so just get it from what was stashed in
3339             # the instance system_metadata when the instance was created (or
3340             # last rebuilt). This also works for volume-backed instances.
3341             image_meta = instance.image_meta
3342         else:
3343             image_meta = objects.ImageMeta()
3344 
3345         # NOTE(mriedem): On an evacuate, we need to update
3346         # the instance's host and node properties to reflect it's
3347         # destination node for the evacuate.
3348         if not scheduled_node:
3349             if evacuate:
3350                 try:
3351                     compute_node = self._get_compute_info(context, self.host)
3352                     scheduled_node = compute_node.hypervisor_hostname
3353                 except exception.ComputeHostNotFound:
3354                     LOG.exception('Failed to get compute_info for %s',
3355                                   self.host)
3356             else:
3357                 scheduled_node = instance.node
3358 
3359         allocs = self.reportclient.get_allocations_for_consumer(
3360                     context, instance.uuid)
3361 
3362         # If the resource claim or group policy validation fails before we
3363         # do anything to the guest or its networking/volumes we want to keep
3364         # the current status rather than put the instance into ERROR status.
3365         instance_state = instance.vm_state
3366         with self._error_out_instance_on_exception(
3367                 context, instance, instance_state=instance_state):
3368             try:
3369                 self._do_rebuild_instance_with_claim(
3370                     context, instance, orig_image_ref,
3371                     image_meta, injected_files, new_pass, orig_sys_metadata,
3372                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3373                     migration, request_spec, allocs, rebuild_claim,
3374                     scheduled_node, limits)
3375             except (exception.ComputeResourcesUnavailable,
3376                     exception.RescheduledException) as e:
3377                 if isinstance(e, exception.ComputeResourcesUnavailable):
3378                     LOG.debug("Could not rebuild instance on this host, not "
3379                               "enough resources available.", instance=instance)
3380                 else:
3381                     # RescheduledException is raised by the late server group
3382                     # policy check during evacuation if a parallel scheduling
3383                     # violated the policy.
3384                     # We catch the RescheduledException here but we don't have
3385                     # the plumbing to do an actual reschedule so we abort the
3386                     # operation.
3387                     LOG.debug("Could not rebuild instance on this host, "
3388                               "late server group check failed.",
3389                               instance=instance)
3390                 # NOTE(ndipanov): We just abort the build for now and leave a
3391                 # migration record for potential cleanup later
3392                 self._set_migration_status(migration, 'failed')
3393                 # Since the claim failed, we need to remove the allocation
3394                 # created against the destination node. Note that we can only
3395                 # get here when evacuating to a destination node. Rebuilding
3396                 # on the same host (not evacuate) uses the NopClaim which will
3397                 # not raise ComputeResourcesUnavailable.
3398                 self.rt.delete_allocation_for_evacuated_instance(
3399                     context, instance, scheduled_node, node_type='destination')
3400                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3401                 # Wrap this in InstanceFaultRollback so that the
3402                 # _error_out_instance_on_exception context manager keeps the
3403                 # vm_state unchanged.
3404                 raise exception.InstanceFaultRollback(
3405                     inner_exception=exception.BuildAbortException(
3406                         instance_uuid=instance.uuid,
3407                         reason=e.format_message()))
3408             except (exception.InstanceNotFound,
3409                     exception.UnexpectedDeletingTaskStateError) as e:
3410                 LOG.debug('Instance was deleted while rebuilding',
3411                           instance=instance)
3412                 self._set_migration_status(migration, 'failed')
3413                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3414             except Exception as e:
3415                 self._set_migration_status(migration, 'failed')
3416                 if evacuate or scheduled_node is not None:
3417                     self.rt.delete_allocation_for_evacuated_instance(
3418                         context, instance, scheduled_node,
3419                         node_type='destination')
3420                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3421                 raise
3422             else:
3423                 instance.apply_migration_context()
3424                 # NOTE (ndipanov): This save will now update the host and node
3425                 # attributes making sure that next RT pass is consistent since
3426                 # it will be based on the instance and not the migration DB
3427                 # entry.
3428                 instance.host = self.host
3429                 instance.node = scheduled_node
3430                 instance.save()
3431                 instance.drop_migration_context()
3432 
3433                 # NOTE (ndipanov): Mark the migration as done only after we
3434                 # mark the instance as belonging to this host.
3435                 self._set_migration_status(migration, 'done')
3436 
3437     def _do_rebuild_instance_with_claim(
3438             self, context, instance, orig_image_ref, image_meta,
3439             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3440             on_shared_storage, preserve_ephemeral, migration, request_spec,
3441             allocations, rebuild_claim, scheduled_node, limits):
3442         """Helper to avoid deep nesting in the top-level method."""
3443 
3444         request_group_resource_providers_mapping = None
3445         if evacuate:
3446             request_group_resource_providers_mapping = \
3447                 self._get_request_group_mapping(request_spec)
3448 
3449             if request_group_resource_providers_mapping:
3450                 compute_utils.\
3451                     update_pci_request_spec_with_allocated_interface_name(
3452                         context, self.reportclient, instance,
3453                         request_group_resource_providers_mapping)
3454 
3455         claim_context = rebuild_claim(
3456             context, instance, scheduled_node, allocations,
3457             limits=limits, image_meta=image_meta, migration=migration)
3458 
3459         with claim_context:
3460             self._do_rebuild_instance(
3461                 context, instance, orig_image_ref, image_meta, injected_files,
3462                 new_pass, orig_sys_metadata, bdms, evacuate, on_shared_storage,
3463                 preserve_ephemeral, migration, request_spec, allocations,
3464                 request_group_resource_providers_mapping)
3465 
3466     @staticmethod
3467     def _get_image_name(image_meta):
3468         if image_meta.obj_attr_is_set("name"):
3469             return image_meta.name
3470         else:
3471             return ''
3472 
3473     def _do_rebuild_instance(self, context, instance, orig_image_ref,
3474                              image_meta, injected_files, new_pass,
3475                              orig_sys_metadata, bdms, evacuate,
3476                              on_shared_storage, preserve_ephemeral,
3477                              migration, request_spec, allocations,
3478                              request_group_resource_providers_mapping):
3479         orig_vm_state = instance.vm_state
3480 
3481         if evacuate:
3482             if request_spec:
3483                 # NOTE(gibi): Do a late check of server group policy as
3484                 # parallel scheduling could violate such policy. This will
3485                 # cause the evacuate to fail as rebuild does not implement
3486                 # reschedule.
3487                 hints = self._get_scheduler_hints({}, request_spec)
3488                 dest_node = migration.dest_node
3489                 self._validate_instance_group_policy(context, instance,
3490                                                      hints, dest_node)
3491 
3492             if not self.driver.capabilities.get("supports_evacuate", False):
3493                 raise exception.InstanceEvacuateNotSupported
3494 
3495             self._check_instance_exists(context, instance)
3496 
3497             if on_shared_storage is None:
3498                 LOG.debug('on_shared_storage is not provided, using driver '
3499                           'information to decide if the instance needs to '
3500                           'be evacuated')
3501                 on_shared_storage = self.driver.instance_on_disk(instance)
3502 
3503             elif (on_shared_storage !=
3504                     self.driver.instance_on_disk(instance)):
3505                 # To cover case when admin expects that instance files are
3506                 # on shared storage, but not accessible and vice versa
3507                 raise exception.InvalidSharedStorage(
3508                         _("Invalid state of instance files on shared"
3509                             " storage"))
3510 
3511             if on_shared_storage:
3512                 LOG.info('disk on shared storage, evacuating using'
3513                          ' existing disk')
3514             elif instance.image_ref:
3515                 orig_image_ref = instance.image_ref
3516                 LOG.info("disk not on shared storage, evacuating from "
3517                          "image: '%s'", str(orig_image_ref))
3518             else:
3519                 LOG.info('disk on volume, evacuating using existing '
3520                          'volume')
3521 
3522         # We check trusted certs capabilities for both evacuate (rebuild on
3523         # another host) and rebuild (rebuild on the same host) because for
3524         # evacuate we need to make sure an instance with trusted certs can
3525         # have the image verified with those certs during rebuild, and for
3526         # rebuild we could be rebuilding a server that started out with no
3527         # trusted certs on this host, and then was rebuilt with trusted certs
3528         # for a new image, in which case we need to validate that new image
3529         # with the trusted certs during the rebuild.
3530         self._check_trusted_certs(instance)
3531 
3532         # This instance.exists message should contain the original
3533         # image_ref, not the new one.  Since the DB has been updated
3534         # to point to the new one... we have to override it.
3535         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3536                                                                context)
3537         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3538         compute_utils.notify_usage_exists(
3539                 self.notifier, context, instance, self.host,
3540                 current_period=True, system_metadata=orig_sys_metadata,
3541                 extra_usage_info=extra_usage_info)
3542 
3543         # This message should contain the new image_ref
3544         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3545         self._notify_about_instance_usage(context, instance,
3546                 "rebuild.start", extra_usage_info=extra_usage_info)
3547         # NOTE: image_name is not included in the versioned notification
3548         # because we already provide the image_uuid in the notification
3549         # payload and the image details can be looked up via the uuid.
3550         compute_utils.notify_about_instance_rebuild(
3551             context, instance, self.host,
3552             phase=fields.NotificationPhase.START,
3553             bdms=bdms)
3554 
3555         instance.power_state = self._get_power_state(context, instance)
3556         instance.task_state = task_states.REBUILDING
3557         instance.save(expected_task_state=[task_states.REBUILDING])
3558 
3559         if evacuate:
3560             self.network_api.setup_networks_on_host(
3561                     context, instance, self.host)
3562             # For nova-network this is needed to move floating IPs
3563             # For neutron this updates the host in the port binding
3564             # TODO(cfriesen): this network_api call and the one above
3565             # are so similar, we should really try to unify them.
3566             self.network_api.setup_instance_network_on_host(
3567                 context, instance, self.host, migration,
3568                 provider_mappings=request_group_resource_providers_mapping)
3569             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3570             # with @base_api.refresh_cache and then we wouldn't need this
3571             # explicit call to get_instance_nw_info.
3572             network_info = self.network_api.get_instance_nw_info(context,
3573                                                                  instance)
3574         else:
3575             network_info = instance.get_network_info()
3576 
3577         if bdms is None:
3578             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3579                     context, instance.uuid)
3580 
3581         block_device_info = \
3582             self._get_instance_block_device_info(
3583                     context, instance, bdms=bdms)
3584 
3585         def detach_block_devices(context, bdms):
3586             for bdm in bdms:
3587                 if bdm.is_volume:
3588                     # NOTE (ildikov): Having the attachment_id set in the BDM
3589                     # means that it's the new Cinder attach/detach flow
3590                     # (available from v3.44). In that case we explicitly
3591                     # attach and detach the volumes through attachment level
3592                     # operations. In this scenario _detach_volume will delete
3593                     # the existing attachment which would make the volume
3594                     # status change to 'available' if we don't pre-create
3595                     # another empty attachment before deleting the old one.
3596                     attachment_id = None
3597                     if bdm.attachment_id:
3598                         attachment_id = self.volume_api.attachment_create(
3599                             context, bdm['volume_id'], instance.uuid)['id']
3600                     self._detach_volume(context, bdm, instance,
3601                                         destroy_bdm=False)
3602                     if attachment_id:
3603                         bdm.attachment_id = attachment_id
3604                         bdm.save()
3605 
3606         files = self._decode_files(injected_files)
3607 
3608         kwargs = dict(
3609             context=context,
3610             instance=instance,
3611             image_meta=image_meta,
3612             injected_files=files,
3613             admin_password=new_pass,
3614             allocations=allocations,
3615             bdms=bdms,
3616             detach_block_devices=detach_block_devices,
3617             attach_block_devices=self._prep_block_device,
3618             block_device_info=block_device_info,
3619             network_info=network_info,
3620             preserve_ephemeral=preserve_ephemeral,
3621             evacuate=evacuate)
3622         try:
3623             with instance.mutated_migration_context():
3624                 self.driver.rebuild(**kwargs)
3625         except NotImplementedError:
3626             # NOTE(rpodolyaka): driver doesn't provide specialized version
3627             # of rebuild, fall back to the default implementation
3628             self._rebuild_default_impl(**kwargs)
3629         self._update_instance_after_spawn(context, instance)
3630         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3631 
3632         if orig_vm_state == vm_states.STOPPED:
3633             LOG.info("bringing vm to original state: '%s'",
3634                      orig_vm_state, instance=instance)
3635             instance.vm_state = vm_states.ACTIVE
3636             instance.task_state = task_states.POWERING_OFF
3637             instance.progress = 0
3638             instance.save()
3639             self.stop_instance(context, instance, False)
3640         # TODO(melwitt): We should clean up instance console tokens here in the
3641         # case of evacuate. The instance is on a new host and will need to
3642         # establish a new console connection.
3643         self._update_scheduler_instance_info(context, instance)
3644         self._notify_about_instance_usage(
3645                 context, instance, "rebuild.end",
3646                 network_info=network_info,
3647                 extra_usage_info=extra_usage_info)
3648         compute_utils.notify_about_instance_rebuild(
3649             context, instance, self.host,
3650             phase=fields.NotificationPhase.END,
3651             bdms=bdms)
3652 
3653     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3654                                      block_device_info):
3655         """Handle cases where the virt-layer had to detach non-working volumes
3656         in order to complete an operation.
3657         """
3658         for bdm in block_device_info['block_device_mapping']:
3659             if bdm.get('mount_device') in bad_devices:
3660                 try:
3661                     volume_id = bdm['connection_info']['data']['volume_id']
3662                 except KeyError:
3663                     continue
3664 
3665                 # NOTE(sirp): ideally we'd just call
3666                 # `compute_api.detach_volume` here but since that hits the
3667                 # DB directly, that's off limits from within the
3668                 # compute-manager.
3669                 #
3670                 # API-detach
3671                 LOG.info("Detaching from volume api: %s", volume_id)
3672                 self.volume_api.begin_detaching(context, volume_id)
3673 
3674                 # Manager-detach
3675                 self.detach_volume(context, volume_id, instance)
3676 
3677     @wrap_exception()
3678     @reverts_task_state
3679     @wrap_instance_event(prefix='compute')
3680     @wrap_instance_fault
3681     def reboot_instance(self, context, instance, block_device_info,
3682                         reboot_type):
3683         @utils.synchronized(instance.uuid)
3684         def do_reboot_instance(context, instance, block_device_info,
3685                                reboot_type):
3686             self._reboot_instance(context, instance, block_device_info,
3687                                   reboot_type)
3688         do_reboot_instance(context, instance, block_device_info, reboot_type)
3689 
3690     def _reboot_instance(self, context, instance, block_device_info,
3691                          reboot_type):
3692         """Reboot an instance on this host."""
3693         # acknowledge the request made it to the manager
3694         if reboot_type == "SOFT":
3695             instance.task_state = task_states.REBOOT_PENDING
3696             expected_states = task_states.soft_reboot_states
3697         else:
3698             instance.task_state = task_states.REBOOT_PENDING_HARD
3699             expected_states = task_states.hard_reboot_states
3700 
3701         context = context.elevated()
3702         LOG.info("Rebooting instance", instance=instance)
3703 
3704         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3705             context, instance.uuid)
3706         block_device_info = self._get_instance_block_device_info(
3707             context, instance, bdms=bdms)
3708 
3709         network_info = self.network_api.get_instance_nw_info(context, instance)
3710 
3711         self._notify_about_instance_usage(context, instance, "reboot.start")
3712         compute_utils.notify_about_instance_action(
3713             context, instance, self.host,
3714             action=fields.NotificationAction.REBOOT,
3715             phase=fields.NotificationPhase.START,
3716             bdms=bdms
3717         )
3718 
3719         instance.power_state = self._get_power_state(context, instance)
3720         instance.save(expected_task_state=expected_states)
3721 
3722         if instance.power_state != power_state.RUNNING:
3723             state = instance.power_state
3724             running = power_state.RUNNING
3725             LOG.warning('trying to reboot a non-running instance:'
3726                         ' (state: %(state)s expected: %(running)s)',
3727                         {'state': state, 'running': running},
3728                         instance=instance)
3729 
3730         def bad_volumes_callback(bad_devices):
3731             self._handle_bad_volumes_detached(
3732                     context, instance, bad_devices, block_device_info)
3733 
3734         try:
3735             # Don't change it out of rescue mode
3736             if instance.vm_state == vm_states.RESCUED:
3737                 new_vm_state = vm_states.RESCUED
3738             else:
3739                 new_vm_state = vm_states.ACTIVE
3740             new_power_state = None
3741             if reboot_type == "SOFT":
3742                 instance.task_state = task_states.REBOOT_STARTED
3743                 expected_state = task_states.REBOOT_PENDING
3744             else:
3745                 instance.task_state = task_states.REBOOT_STARTED_HARD
3746                 expected_state = task_states.REBOOT_PENDING_HARD
3747             instance.save(expected_task_state=expected_state)
3748             self.driver.reboot(context, instance,
3749                                network_info,
3750                                reboot_type,
3751                                block_device_info=block_device_info,
3752                                bad_volumes_callback=bad_volumes_callback)
3753 
3754         except Exception as error:
3755             with excutils.save_and_reraise_exception() as ctxt:
3756                 exc_info = sys.exc_info()
3757                 # if the reboot failed but the VM is running don't
3758                 # put it into an error state
3759                 new_power_state = self._get_power_state(context, instance)
3760                 if new_power_state == power_state.RUNNING:
3761                     LOG.warning('Reboot failed but instance is running',
3762                                 instance=instance)
3763                     compute_utils.add_instance_fault_from_exc(context,
3764                             instance, error, exc_info)
3765                     self._notify_about_instance_usage(context, instance,
3766                             'reboot.error', fault=error)
3767                     tb = traceback.format_exc()
3768                     compute_utils.notify_about_instance_action(
3769                         context, instance, self.host,
3770                         action=fields.NotificationAction.REBOOT,
3771                         phase=fields.NotificationPhase.ERROR,
3772                         exception=error, bdms=bdms, tb=tb
3773                     )
3774                     ctxt.reraise = False
3775                 else:
3776                     LOG.error('Cannot reboot instance: %s', error,
3777                               instance=instance)
3778                     self._set_instance_obj_error_state(context, instance)
3779 
3780         if not new_power_state:
3781             new_power_state = self._get_power_state(context, instance)
3782         try:
3783             instance.power_state = new_power_state
3784             instance.vm_state = new_vm_state
3785             instance.task_state = None
3786             instance.save()
3787         except exception.InstanceNotFound:
3788             LOG.warning("Instance disappeared during reboot",
3789                         instance=instance)
3790 
3791         self._notify_about_instance_usage(context, instance, "reboot.end")
3792         compute_utils.notify_about_instance_action(
3793             context, instance, self.host,
3794             action=fields.NotificationAction.REBOOT,
3795             phase=fields.NotificationPhase.END,
3796             bdms=bdms
3797         )
3798 
3799     @delete_image_on_error
3800     def _do_snapshot_instance(self, context, image_id, instance):
3801         self._snapshot_instance(context, image_id, instance,
3802                                 task_states.IMAGE_BACKUP)
3803 
3804     @wrap_exception()
3805     @reverts_task_state
3806     @wrap_instance_event(prefix='compute')
3807     @wrap_instance_fault
3808     def backup_instance(self, context, image_id, instance, backup_type,
3809                         rotation):
3810         """Backup an instance on this host.
3811 
3812         :param backup_type: daily | weekly
3813         :param rotation: int representing how many backups to keep around
3814         """
3815         self._do_snapshot_instance(context, image_id, instance)
3816         self._rotate_backups(context, instance, backup_type, rotation)
3817 
3818     @wrap_exception()
3819     @reverts_task_state
3820     @wrap_instance_event(prefix='compute')
3821     @wrap_instance_fault
3822     @delete_image_on_error
3823     def snapshot_instance(self, context, image_id, instance):
3824         """Snapshot an instance on this host.
3825 
3826         :param context: security context
3827         :param image_id: glance.db.sqlalchemy.models.Image.Id
3828         :param instance: a nova.objects.instance.Instance object
3829         """
3830         # NOTE(dave-mcnally) the task state will already be set by the api
3831         # but if the compute manager has crashed/been restarted prior to the
3832         # request getting here the task state may have been cleared so we set
3833         # it again and things continue normally
3834         try:
3835             instance.task_state = task_states.IMAGE_SNAPSHOT
3836             instance.save(
3837                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3838         except exception.InstanceNotFound:
3839             # possibility instance no longer exists, no point in continuing
3840             LOG.debug("Instance not found, could not set state %s "
3841                       "for instance.",
3842                       task_states.IMAGE_SNAPSHOT, instance=instance)
3843             return
3844 
3845         except exception.UnexpectedDeletingTaskStateError:
3846             LOG.debug("Instance being deleted, snapshot cannot continue",
3847                       instance=instance)
3848             return
3849 
3850         self._snapshot_instance(context, image_id, instance,
3851                                 task_states.IMAGE_SNAPSHOT)
3852 
3853     def _snapshot_instance(self, context, image_id, instance,
3854                            expected_task_state):
3855         context = context.elevated()
3856 
3857         instance.power_state = self._get_power_state(context, instance)
3858         try:
3859             instance.save()
3860 
3861             LOG.info('instance snapshotting', instance=instance)
3862 
3863             if instance.power_state != power_state.RUNNING:
3864                 state = instance.power_state
3865                 running = power_state.RUNNING
3866                 LOG.warning('trying to snapshot a non-running instance: '
3867                             '(state: %(state)s expected: %(running)s)',
3868                             {'state': state, 'running': running},
3869                             instance=instance)
3870 
3871             self._notify_about_instance_usage(
3872                 context, instance, "snapshot.start")
3873             compute_utils.notify_about_instance_snapshot(context, instance,
3874                 self.host, phase=fields.NotificationPhase.START,
3875                 snapshot_image_id=image_id)
3876 
3877             def update_task_state(task_state,
3878                                   expected_state=expected_task_state):
3879                 instance.task_state = task_state
3880                 instance.save(expected_task_state=expected_state)
3881 
3882             with timeutils.StopWatch() as timer:
3883                 self.driver.snapshot(context, instance, image_id,
3884                                      update_task_state)
3885             LOG.info('Took %0.2f seconds to snapshot the instance on '
3886                      'the hypervisor.', timer.elapsed(), instance=instance)
3887 
3888             instance.task_state = None
3889             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3890 
3891             self._notify_about_instance_usage(context, instance,
3892                                               "snapshot.end")
3893             compute_utils.notify_about_instance_snapshot(context, instance,
3894                 self.host, phase=fields.NotificationPhase.END,
3895                 snapshot_image_id=image_id)
3896         except (exception.InstanceNotFound,
3897                 exception.InstanceNotRunning,
3898                 exception.UnexpectedDeletingTaskStateError):
3899             # the instance got deleted during the snapshot
3900             # Quickly bail out of here
3901             msg = 'Instance disappeared during snapshot'
3902             LOG.debug(msg, instance=instance)
3903             try:
3904                 image = self.image_api.get(context, image_id)
3905                 if image['status'] != 'active':
3906                     self.image_api.delete(context, image_id)
3907             except exception.ImageNotFound:
3908                 LOG.debug('Image not found during clean up %s', image_id)
3909             except Exception:
3910                 LOG.warning("Error while trying to clean up image %s",
3911                             image_id, instance=instance)
3912         except exception.ImageNotFound:
3913             instance.task_state = None
3914             instance.save()
3915             LOG.warning("Image not found during snapshot", instance=instance)
3916 
3917     def _post_interrupted_snapshot_cleanup(self, context, instance):
3918         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3919 
3920     @messaging.expected_exceptions(NotImplementedError)
3921     @wrap_exception()
3922     def volume_snapshot_create(self, context, instance, volume_id,
3923                                create_info):
3924         try:
3925             self.driver.volume_snapshot_create(context, instance, volume_id,
3926                                                create_info)
3927         except exception.InstanceNotRunning:
3928             # Libvirt driver can raise this exception
3929             LOG.debug('Instance disappeared during volume snapshot create',
3930                       instance=instance)
3931 
3932     @messaging.expected_exceptions(NotImplementedError)
3933     @wrap_exception()
3934     def volume_snapshot_delete(self, context, instance, volume_id,
3935                                snapshot_id, delete_info):
3936         try:
3937             self.driver.volume_snapshot_delete(context, instance, volume_id,
3938                                                snapshot_id, delete_info)
3939         except exception.InstanceNotRunning:
3940             # Libvirt driver can raise this exception
3941             LOG.debug('Instance disappeared during volume snapshot delete',
3942                       instance=instance)
3943 
3944     @wrap_instance_fault
3945     def _rotate_backups(self, context, instance, backup_type, rotation):
3946         """Delete excess backups associated to an instance.
3947 
3948         Instances are allowed a fixed number of backups (the rotation number);
3949         this method deletes the oldest backups that exceed the rotation
3950         threshold.
3951 
3952         :param context: security context
3953         :param instance: Instance dict
3954         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3955         :param rotation: int representing how many backups to keep around;
3956             None if rotation shouldn't be used (as in the case of snapshots)
3957         """
3958         filters = {'property-image_type': 'backup',
3959                    'property-backup_type': backup_type,
3960                    'property-instance_uuid': instance.uuid}
3961 
3962         images = self.image_api.get_all(context, filters=filters,
3963                                         sort_key='created_at', sort_dir='desc')
3964         num_images = len(images)
3965         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3966                   {'num_images': num_images, 'rotation': rotation},
3967                   instance=instance)
3968 
3969         if num_images > rotation:
3970             # NOTE(sirp): this deletes all backups that exceed the rotation
3971             # limit
3972             excess = len(images) - rotation
3973             LOG.debug("Rotating out %d backups", excess,
3974                       instance=instance)
3975             for i in range(excess):
3976                 image = images.pop()
3977                 image_id = image['id']
3978                 LOG.debug("Deleting image %s", image_id,
3979                           instance=instance)
3980                 try:
3981                     self.image_api.delete(context, image_id)
3982                 except exception.ImageNotFound:
3983                     LOG.info("Failed to find image %(image_id)s to "
3984                              "delete", {'image_id': image_id},
3985                              instance=instance)
3986                 except (exception.ImageDeleteConflict, Exception) as exc:
3987                     LOG.info("Failed to delete image %(image_id)s during "
3988                              "deleting excess backups. "
3989                              "Continuing for next image.. %(exc)s",
3990                              {'image_id': image_id, 'exc': exc},
3991                              instance=instance)
3992 
3993     @wrap_exception()
3994     @reverts_task_state
3995     @wrap_instance_event(prefix='compute')
3996     @wrap_instance_fault
3997     def set_admin_password(self, context, instance, new_pass):
3998         """Set the root/admin password for an instance on this host.
3999 
4000         This is generally only called by API password resets after an
4001         image has been built.
4002 
4003         @param context: Nova auth context.
4004         @param instance: Nova instance object.
4005         @param new_pass: The admin password for the instance.
4006         """
4007 
4008         context = context.elevated()
4009         current_power_state = self._get_power_state(context, instance)
4010         expected_state = power_state.RUNNING
4011 
4012         if current_power_state != expected_state:
4013             instance.task_state = None
4014             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
4015             _msg = _('instance %s is not running') % instance.uuid
4016             raise exception.InstancePasswordSetFailed(
4017                 instance=instance.uuid, reason=_msg)
4018 
4019         try:
4020             self.driver.set_admin_password(instance, new_pass)
4021             LOG.info("Admin password set", instance=instance)
4022             instance.task_state = None
4023             instance.save(
4024                 expected_task_state=task_states.UPDATING_PASSWORD)
4025         except exception.InstanceAgentNotEnabled:
4026             with excutils.save_and_reraise_exception():
4027                 LOG.debug('Guest agent is not enabled for the instance.',
4028                           instance=instance)
4029                 instance.task_state = None
4030                 instance.save(
4031                     expected_task_state=task_states.UPDATING_PASSWORD)
4032         except exception.SetAdminPasswdNotSupported:
4033             with excutils.save_and_reraise_exception():
4034                 LOG.info('set_admin_password is not supported '
4035                          'by this driver or guest instance.',
4036                          instance=instance)
4037                 instance.task_state = None
4038                 instance.save(
4039                     expected_task_state=task_states.UPDATING_PASSWORD)
4040         except NotImplementedError:
4041             LOG.warning('set_admin_password is not implemented '
4042                         'by this driver or guest instance.',
4043                         instance=instance)
4044             instance.task_state = None
4045             instance.save(
4046                 expected_task_state=task_states.UPDATING_PASSWORD)
4047             raise NotImplementedError(_('set_admin_password is not '
4048                                         'implemented by this driver or guest '
4049                                         'instance.'))
4050         except exception.UnexpectedTaskStateError:
4051             # interrupted by another (most likely delete) task
4052             # do not retry
4053             raise
4054         except Exception:
4055             # Catch all here because this could be anything.
4056             LOG.exception('set_admin_password failed', instance=instance)
4057             # We create a new exception here so that we won't
4058             # potentially reveal password information to the
4059             # API caller.  The real exception is logged above
4060             _msg = _('error setting admin password')
4061             raise exception.InstancePasswordSetFailed(
4062                 instance=instance.uuid, reason=_msg)
4063 
4064     @wrap_exception()
4065     @reverts_task_state
4066     @wrap_instance_fault
4067     def inject_file(self, context, path, file_contents, instance):
4068         """Write a file to the specified path in an instance on this host."""
4069         # NOTE(russellb) Remove this method, as well as the underlying virt
4070         # driver methods, when the compute rpc interface is bumped to 4.x
4071         # as it is no longer used.
4072         context = context.elevated()
4073         current_power_state = self._get_power_state(context, instance)
4074         expected_state = power_state.RUNNING
4075         if current_power_state != expected_state:
4076             LOG.warning('trying to inject a file into a non-running '
4077                         '(state: %(current_state)s expected: '
4078                         '%(expected_state)s)',
4079                         {'current_state': current_power_state,
4080                          'expected_state': expected_state},
4081                         instance=instance)
4082         LOG.info('injecting file to %s', path, instance=instance)
4083         self.driver.inject_file(instance, path, file_contents)
4084 
4085     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
4086         """Determine what image should be used to boot the rescue VM."""
4087         # 1. If rescue_image_ref is passed in, use that for rescue.
4088         # 2. Else, use the base image associated with instance's current image.
4089         #       The idea here is to provide the customer with a rescue
4090         #       environment which they are familiar with.
4091         #       So, if they built their instance off of a Debian image,
4092         #       their rescue VM will also be Debian.
4093         # 3. As a last resort, use instance's current image.
4094         if not rescue_image_ref:
4095             system_meta = utils.instance_sys_meta(instance)
4096             rescue_image_ref = system_meta.get('image_base_image_ref')
4097 
4098         if not rescue_image_ref:
4099             LOG.warning('Unable to find a different image to use for '
4100                         'rescue VM, using instance\'s current image',
4101                         instance=instance)
4102             rescue_image_ref = instance.image_ref
4103 
4104         return objects.ImageMeta.from_image_ref(
4105             context, self.image_api, rescue_image_ref)
4106 
4107     @wrap_exception()
4108     @reverts_task_state
4109     @wrap_instance_event(prefix='compute')
4110     @wrap_instance_fault
4111     def rescue_instance(self, context, instance, rescue_password,
4112                         rescue_image_ref, clean_shutdown):
4113         context = context.elevated()
4114         LOG.info('Rescuing', instance=instance)
4115 
4116         admin_password = (rescue_password if rescue_password else
4117                       utils.generate_password())
4118 
4119         network_info = self.network_api.get_instance_nw_info(context, instance)
4120 
4121         rescue_image_meta = self._get_rescue_image(context, instance,
4122                                                    rescue_image_ref)
4123 
4124         extra_usage_info = {'rescue_image_name':
4125                             self._get_image_name(rescue_image_meta)}
4126         self._notify_about_instance_usage(context, instance,
4127                 "rescue.start", extra_usage_info=extra_usage_info,
4128                 network_info=network_info)
4129         compute_utils.notify_about_instance_rescue_action(
4130             context, instance, self.host, rescue_image_ref,
4131             phase=fields.NotificationPhase.START)
4132 
4133         try:
4134             self._power_off_instance(context, instance, clean_shutdown)
4135 
4136             self.driver.rescue(context, instance,
4137                                network_info,
4138                                rescue_image_meta, admin_password)
4139         except Exception as e:
4140             LOG.exception("Error trying to Rescue Instance",
4141                           instance=instance)
4142             self._set_instance_obj_error_state(context, instance)
4143             raise exception.InstanceNotRescuable(
4144                 instance_id=instance.uuid,
4145                 reason=_("Driver Error: %s") % e)
4146 
4147         compute_utils.notify_usage_exists(self.notifier, context, instance,
4148                                           self.host, current_period=True)
4149 
4150         instance.vm_state = vm_states.RESCUED
4151         instance.task_state = None
4152         instance.power_state = self._get_power_state(context, instance)
4153         instance.launched_at = timeutils.utcnow()
4154         instance.save(expected_task_state=task_states.RESCUING)
4155 
4156         self._notify_about_instance_usage(context, instance,
4157                 "rescue.end", extra_usage_info=extra_usage_info,
4158                 network_info=network_info)
4159         compute_utils.notify_about_instance_rescue_action(
4160             context, instance, self.host, rescue_image_ref,
4161             phase=fields.NotificationPhase.END)
4162 
4163     @wrap_exception()
4164     @reverts_task_state
4165     @wrap_instance_event(prefix='compute')
4166     @wrap_instance_fault
4167     def unrescue_instance(self, context, instance):
4168         context = context.elevated()
4169         LOG.info('Unrescuing', instance=instance)
4170 
4171         network_info = self.network_api.get_instance_nw_info(context, instance)
4172         self._notify_about_instance_usage(context, instance,
4173                 "unrescue.start", network_info=network_info)
4174         compute_utils.notify_about_instance_action(context, instance,
4175             self.host, action=fields.NotificationAction.UNRESCUE,
4176             phase=fields.NotificationPhase.START)
4177 
4178         with self._error_out_instance_on_exception(context, instance):
4179             self.driver.unrescue(instance,
4180                                  network_info)
4181 
4182         instance.vm_state = vm_states.ACTIVE
4183         instance.task_state = None
4184         instance.power_state = self._get_power_state(context, instance)
4185         instance.save(expected_task_state=task_states.UNRESCUING)
4186 
4187         self._notify_about_instance_usage(context,
4188                                           instance,
4189                                           "unrescue.end",
4190                                           network_info=network_info)
4191         compute_utils.notify_about_instance_action(context, instance,
4192             self.host, action=fields.NotificationAction.UNRESCUE,
4193             phase=fields.NotificationPhase.END)
4194 
4195     @wrap_exception()
4196     @wrap_instance_fault
4197     def change_instance_metadata(self, context, diff, instance):
4198         """Update the metadata published to the instance."""
4199         LOG.debug("Changing instance metadata according to %r",
4200                   diff, instance=instance)
4201         self.driver.change_instance_metadata(context, instance, diff)
4202 
4203     @wrap_exception()
4204     @wrap_instance_event(prefix='compute')
4205     @errors_out_migration
4206     @wrap_instance_fault
4207     def confirm_resize(self, context, instance, migration):
4208         """Confirms a migration/resize and deletes the 'old' instance.
4209 
4210         This is called from the API and runs on the source host.
4211 
4212         Nothing needs to happen on the destination host at this point since
4213         the instance is already running there. This routine just cleans up the
4214         source host.
4215         """
4216         @utils.synchronized(instance.uuid)
4217         def do_confirm_resize(context, instance, migration_id):
4218             # NOTE(wangpan): Get the migration status from db, if it has been
4219             #                confirmed, we do nothing and return here
4220             LOG.debug("Going to confirm migration %s", migration_id,
4221                       instance=instance)
4222             try:
4223                 # TODO(russellb) Why are we sending the migration object just
4224                 # to turn around and look it up from the db again?
4225                 migration = objects.Migration.get_by_id(
4226                                     context.elevated(), migration_id)
4227             except exception.MigrationNotFound:
4228                 LOG.error("Migration %s is not found during confirmation",
4229                           migration_id, instance=instance)
4230                 return
4231 
4232             if migration.status == 'confirmed':
4233                 LOG.info("Migration %s is already confirmed",
4234                          migration_id, instance=instance)
4235                 return
4236             elif migration.status not in ('finished', 'confirming'):
4237                 LOG.warning("Unexpected confirmation status '%(status)s' "
4238                             "of migration %(id)s, exit confirmation process",
4239                             {"status": migration.status, "id": migration_id},
4240                             instance=instance)
4241                 return
4242 
4243             # NOTE(wangpan): Get the instance from db, if it has been
4244             #                deleted, we do nothing and return here
4245             expected_attrs = ['metadata', 'system_metadata', 'flavor']
4246             try:
4247                 instance = objects.Instance.get_by_uuid(
4248                         context, instance.uuid,
4249                         expected_attrs=expected_attrs)
4250             except exception.InstanceNotFound:
4251                 LOG.info("Instance is not found during confirmation",
4252                          instance=instance)
4253                 return
4254 
4255             with self._error_out_instance_on_exception(context, instance):
4256                 try:
4257                     self._confirm_resize(
4258                         context, instance, migration=migration)
4259                 except Exception:
4260                     # Something failed when cleaning up the source host so
4261                     # log a traceback and leave a hint about hard rebooting
4262                     # the server to correct its state in the DB.
4263                     with excutils.save_and_reraise_exception(logger=LOG):
4264                         LOG.exception(
4265                             'Confirm resize failed on source host %s. '
4266                             'Resource allocations in the placement service '
4267                             'will be removed regardless because the instance '
4268                             'is now on the destination host %s. You can try '
4269                             'hard rebooting the instance to correct its '
4270                             'state.', self.host, migration.dest_compute,
4271                             instance=instance)
4272                 finally:
4273                     # Whether an error occurred or not, at this point the
4274                     # instance is on the dest host so to avoid leaking
4275                     # allocations in placement, delete them here.
4276                     self._delete_allocation_after_move(
4277                         context, instance, migration)
4278 
4279         do_confirm_resize(context, instance, migration.id)
4280 
4281     def _get_updated_nw_info_with_pci_mapping(self, nw_info, pci_mapping):
4282         # NOTE(adrianc): This method returns a copy of nw_info if modifications
4283         # are made else it returns the original nw_info.
4284         updated_nw_info = nw_info
4285         if nw_info and pci_mapping:
4286             updated_nw_info = copy.deepcopy(nw_info)
4287             for vif in updated_nw_info:
4288                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
4289                     try:
4290                         vif_pci_addr = vif['profile']['pci_slot']
4291                         new_addr = pci_mapping[vif_pci_addr].address
4292                         vif['profile']['pci_slot'] = new_addr
4293                         LOG.debug("Updating VIF's PCI address for VIF %(id)s. "
4294                                   "Original value %(orig_val)s, "
4295                                   "new value %(new_val)s",
4296                                   {'id': vif['id'],
4297                                    'orig_val': vif_pci_addr,
4298                                    'new_val': new_addr})
4299                     except (KeyError, AttributeError):
4300                         with excutils.save_and_reraise_exception():
4301                             # NOTE(adrianc): This should never happen. If we
4302                             # get here it means there is some inconsistency
4303                             # with either 'nw_info' or 'pci_mapping'.
4304                             LOG.error("Unexpected error when updating network "
4305                                       "information with PCI mapping.")
4306         return updated_nw_info
4307 
4308     def _confirm_resize(self, context, instance, migration=None):
4309         """Destroys the source instance."""
4310         self._notify_about_instance_usage(context, instance,
4311                                           "resize.confirm.start")
4312         compute_utils.notify_about_instance_action(context, instance,
4313             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4314             phase=fields.NotificationPhase.START)
4315 
4316         # NOTE(danms): delete stashed migration information
4317         old_instance_type = instance.old_flavor
4318         instance.old_flavor = None
4319         instance.new_flavor = None
4320         instance.system_metadata.pop('old_vm_state', None)
4321         instance.save()
4322 
4323         # NOTE(tr3buchet): tear down networks on source host
4324         self.network_api.setup_networks_on_host(context, instance,
4325                            migration.source_compute, teardown=True)
4326         network_info = self.network_api.get_instance_nw_info(context,
4327                                                              instance)
4328 
4329         # NOTE(adrianc): Populate old PCI device in VIF profile
4330         # to allow virt driver to properly unplug it from Hypervisor.
4331         pci_mapping = (instance.migration_context.
4332                        get_pci_mapping_for_migration(True))
4333         network_info = self._get_updated_nw_info_with_pci_mapping(
4334             network_info, pci_mapping)
4335 
4336         self.driver.confirm_migration(context, migration, instance,
4337                                       network_info)
4338 
4339         migration.status = 'confirmed'
4340         migration.save()
4341 
4342         # NOTE(mriedem): drop_move_claim relies on
4343         # instance.migration_context so make sure to not call
4344         # instance.drop_migration_context() until after drop_move_claim
4345         # is called.
4346         self.rt.drop_move_claim(context, instance, migration.source_node,
4347                                 old_instance_type, prefix='old_')
4348         instance.drop_migration_context()
4349 
4350         # NOTE(mriedem): The old_vm_state could be STOPPED but the user
4351         # might have manually powered up the instance to confirm the
4352         # resize/migrate, so we need to check the current power state
4353         # on the instance and set the vm_state appropriately. We default
4354         # to ACTIVE because if the power state is not SHUTDOWN, we
4355         # assume _sync_instance_power_state will clean it up.
4356         p_state = instance.power_state
4357         vm_state = None
4358         if p_state == power_state.SHUTDOWN:
4359             vm_state = vm_states.STOPPED
4360             LOG.debug("Resized/migrated instance is powered off. "
4361                       "Setting vm_state to '%s'.", vm_state,
4362                       instance=instance)
4363         else:
4364             vm_state = vm_states.ACTIVE
4365 
4366         instance.vm_state = vm_state
4367         instance.task_state = None
4368         instance.save(expected_task_state=[None, task_states.DELETING,
4369                                            task_states.SOFT_DELETING])
4370 
4371         self._notify_about_instance_usage(
4372             context, instance, "resize.confirm.end",
4373             network_info=network_info)
4374         compute_utils.notify_about_instance_action(context, instance,
4375                self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4376                phase=fields.NotificationPhase.END)
4377 
4378     def _delete_allocation_after_move(self, context, instance, migration):
4379         """Deletes resource allocations held by the migration record against
4380         the source compute node resource provider after a confirmed cold /
4381         successful live migration.
4382         """
4383         try:
4384             # NOTE(danms): We're finishing on the source node, so try
4385             # to delete the allocation based on the migration uuid
4386             self.reportclient.delete_allocation_for_instance(
4387                 context, migration.uuid, consumer_type='migration')
4388         except exception.AllocationDeleteFailed:
4389             LOG.error('Deleting allocation in placement for migration '
4390                       '%(migration_uuid)s failed. The instance '
4391                       '%(instance_uuid)s will be put to ERROR state '
4392                       'but the allocation held by the migration is '
4393                       'leaked.',
4394                       {'instance_uuid': instance.uuid,
4395                        'migration_uuid': migration.uuid})
4396             raise
4397 
4398     @wrap_exception()
4399     @wrap_instance_event(prefix='compute')
4400     @errors_out_migration
4401     @wrap_instance_fault
4402     def confirm_snapshot_based_resize_at_source(
4403             self, ctxt, instance, migration):
4404         """Confirms a snapshot-based resize on the source host.
4405 
4406         Cleans the guest from the source hypervisor including disks and drops
4407         the MoveClaim which will free up "old_flavor" usage from the
4408         ResourceTracker.
4409 
4410         Deletes the allocations held by the migration consumer against the
4411         source compute node resource provider.
4412 
4413         :param ctxt: nova auth request context targeted at the source cell
4414         :param instance: Instance object being resized which should have the
4415             "old_flavor" attribute set
4416         :param migration: Migration object for the resize operation
4417         """
4418 
4419         @utils.synchronized(instance.uuid)
4420         def do_confirm():
4421             LOG.info('Confirming resize on source host.', instance=instance)
4422             with self._error_out_instance_on_exception(ctxt, instance):
4423                 # TODO(mriedem): Could probably make this try/except/finally
4424                 # a context manager to share with confirm_resize().
4425                 try:
4426                     self._confirm_snapshot_based_resize_at_source(
4427                         ctxt, instance, migration)
4428                 except Exception:
4429                     # Something failed when cleaning up the source host so
4430                     # log a traceback and leave a hint about hard rebooting
4431                     # the server to correct its state in the DB.
4432                     with excutils.save_and_reraise_exception(logger=LOG):
4433                         LOG.exception(
4434                             'Confirm resize failed on source host %s. '
4435                             'Resource allocations in the placement service '
4436                             'will be removed regardless because the instance '
4437                             'is now on the destination host %s. You can try '
4438                             'hard rebooting the instance to correct its '
4439                             'state.', self.host, migration.dest_compute,
4440                             instance=instance)
4441                 finally:
4442                     # Whether an error occurred or not, at this point the
4443                     # instance is on the dest host so to avoid leaking
4444                     # allocations in placement, delete them here.
4445                     # TODO(mriedem): Should we catch and just log
4446                     # AllocationDeleteFailed? What is the user's recourse if
4447                     # we got this far but this fails? At this point the
4448                     # instance is on the target host and the allocations
4449                     # could just be manually cleaned up by the operator.
4450                     self._delete_allocation_after_move(ctxt, instance,
4451                                                        migration)
4452         do_confirm()
4453 
4454     def _confirm_snapshot_based_resize_at_source(
4455             self, ctxt, instance, migration):
4456         """Private version of confirm_snapshot_based_resize_at_source
4457 
4458         This allows the main method to be decorated with error handlers.
4459 
4460         :param ctxt: nova auth request context targeted at the source cell
4461         :param instance: Instance object being resized which should have the
4462             "old_flavor" attribute set
4463         :param migration: Migration object for the resize operation
4464         """
4465         # Cleanup the guest from the hypervisor including local disks.
4466         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4467         LOG.debug('Cleaning up guest from source hypervisor including disks.',
4468                   instance=instance)
4469 
4470         # FIXME(mriedem): Per bug 1809095, _confirm_resize calls
4471         # _get_updated_nw_info_with_pci_mapping here prior to unplugging
4472         # VIFs on the source, but in our case we have already unplugged
4473         # VIFs during prep_snapshot_based_resize_at_source, so what do we
4474         # need to do about those kinds of ports? Do we need to wait to unplug
4475         # VIFs until confirm like normal resize?
4476 
4477         # Note that prep_snapshot_based_resize_at_source already destroyed the
4478         # guest which disconnected volumes and unplugged VIFs but did not
4479         # destroy disks in case something failed during the resize and the
4480         # instance needed to be rebooted or rebuilt on the source host. Now
4481         # that we are confirming the resize we want to cleanup the disks left
4482         # on the source host. We call cleanup() instead of destroy() to avoid
4483         # any InstanceNotFound confusion from the driver since the guest was
4484         # already destroyed on this host. block_device_info=None and
4485         # destroy_vifs=False means cleanup() will not try to disconnect volumes
4486         # or unplug VIFs.
4487         self.driver.cleanup(
4488             ctxt, instance, network_info, block_device_info=None,
4489             destroy_disks=True, destroy_vifs=False)
4490 
4491         # Delete port bindings for the source host.
4492         self._confirm_snapshot_based_resize_delete_port_bindings(
4493             ctxt, instance, migration)
4494 
4495         # Delete volume attachments for the source host.
4496         self._delete_volume_attachments(ctxt, instance.get_bdms())
4497 
4498         # Free up the old_flavor usage from the resource tracker for this host.
4499         self.rt.drop_move_claim(
4500             ctxt, instance, migration.source_node, instance.old_flavor,
4501             prefix='old_')
4502         instance.drop_migration_context()
4503 
4504         migration.status = 'confirmed'
4505         migration.save()
4506 
4507     def _confirm_snapshot_based_resize_delete_port_bindings(
4508             self, ctxt, instance, migration):
4509         """Delete port bindings for the source host when confirming
4510         snapshot-based resize on the source host."
4511 
4512         :param ctxt: nova auth RequestContext
4513         :param instance: Instance object that was resized/cold migrated
4514         :param migration: Migration object for the resize/cold migrate
4515         """
4516         # setup_networks_on_host relies on the instance.host not being the same
4517         # as the host we pass in, so we have to mutate the instance to
4518         # effectively trick this code.
4519         with utils.temporary_mutation(instance, host=migration.dest_compute):
4520             LOG.debug('Deleting port bindings for source host.',
4521                       instance=instance)
4522             try:
4523                 self.network_api.setup_networks_on_host(
4524                     ctxt, instance, host=self.host, teardown=True)
4525             except exception.PortBindingDeletionFailed as e:
4526                 # Do not let this stop us from cleaning up since the guest
4527                 # is already gone.
4528                 LOG.error('Failed to delete port bindings from source host. '
4529                           'Error: %s', six.text_type(e), instance=instance)
4530 
4531     def _delete_volume_attachments(self, ctxt, bdms):
4532         """Deletes volume attachment records for the given bdms.
4533 
4534         This method will log but not re-raise any exceptions if the volume
4535         attachment delete fails.
4536 
4537         :param ctxt: nova auth request context used to make
4538             DELETE /attachments/{attachment_id} requests to cinder.
4539         :param bdms: objects.BlockDeviceMappingList representing volume
4540             attachments to delete based on BlockDeviceMapping.attachment_id.
4541         """
4542         for bdm in bdms:
4543             if bdm.attachment_id:
4544                 try:
4545                     self.volume_api.attachment_delete(ctxt, bdm.attachment_id)
4546                 except Exception as e:
4547                     LOG.error('Failed to delete volume attachment with ID %s. '
4548                               'Error: %s', bdm.attachment_id, six.text_type(e),
4549                               instance_uuid=bdm.instance_uuid)
4550 
4551     @wrap_exception()
4552     @reverts_task_state
4553     @wrap_instance_event(prefix='compute')
4554     @errors_out_migration
4555     @wrap_instance_fault
4556     def revert_snapshot_based_resize_at_dest(self, ctxt, instance, migration):
4557         """Reverts a snapshot-based resize at the destination host.
4558 
4559         Cleans the guest from the destination compute service host hypervisor
4560         and related resources (ports, volumes) and frees resource usage from
4561         the compute service on that host.
4562 
4563         :param ctxt: nova auth request context targeted at the target cell
4564         :param instance: Instance object whose vm_state is "resized" and
4565             task_state is "resize_reverting".
4566         :param migration: Migration object whose status is "reverting".
4567         """
4568         # A resize revert is essentially a resize back to the old size, so we
4569         # need to send a usage event here.
4570         compute_utils.notify_usage_exists(
4571             self.notifier, ctxt, instance, self.host, current_period=True)
4572 
4573         @utils.synchronized(instance.uuid)
4574         def do_revert():
4575             LOG.info('Reverting resize on destination host.',
4576                      instance=instance)
4577             with self._error_out_instance_on_exception(ctxt, instance):
4578                 self._revert_snapshot_based_resize_at_dest(
4579                     ctxt, instance, migration)
4580         do_revert()
4581 
4582         # Broadcast to all schedulers that the instance is no longer on
4583         # this host and clear any waiting callback events. This is best effort
4584         # so if anything fails just log it.
4585         try:
4586             self._delete_scheduler_instance_info(ctxt, instance.uuid)
4587             self.instance_events.clear_events_for_instance(instance)
4588         except Exception as e:
4589             LOG.warning('revert_snapshot_based_resize_at_dest failed during '
4590                         'post-processing. Error: %s', e, instance=instance)
4591 
4592     def _revert_snapshot_based_resize_at_dest(
4593             self, ctxt, instance, migration):
4594         """Private version of revert_snapshot_based_resize_at_dest.
4595 
4596         This allows the main method to be decorated with error handlers.
4597 
4598         :param ctxt: nova auth request context targeted at the target cell
4599         :param instance: Instance object whose vm_state is "resized" and
4600             task_state is "resize_reverting".
4601         :param migration: Migration object whose status is "reverting".
4602         """
4603         # Cleanup the guest from the hypervisor including local disks.
4604         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4605         bdms = instance.get_bdms()
4606         block_device_info = self._get_instance_block_device_info(
4607             ctxt, instance, bdms=bdms)
4608         LOG.debug('Destroying guest from destination hypervisor including '
4609                   'disks.', instance=instance)
4610         self.driver.destroy(
4611             ctxt, instance, network_info, block_device_info=block_device_info)
4612 
4613         # Activate source host port bindings. We need to do this before
4614         # deleting the (active) dest host port bindings in
4615         # setup_networks_on_host otherwise the ports will be unbound and
4616         # finish on the source will fail.
4617         # migrate_instance_start uses migration.dest_compute for the port
4618         # binding host and since we want to activate the source host port
4619         # bindings, we need to temporarily mutate the migration object.
4620         with utils.temporary_mutation(
4621                 migration, dest_compute=migration.source_compute):
4622             LOG.debug('Activating port bindings for source host %s.',
4623                       migration.source_compute, instance=instance)
4624             # TODO(mriedem): https://review.opendev.org/#/c/594139/ would allow
4625             # us to remove this and make setup_networks_on_host do it.
4626             # TODO(mriedem): Should we try/except/log any errors but continue?
4627             self.network_api.migrate_instance_start(
4628                 ctxt, instance, migration)
4629 
4630         # Delete port bindings for the target host. This relies on the
4631         # instance.host not being the same as the host we pass in, so we
4632         # have to mutate the instance to effectively trick this code.
4633         with utils.temporary_mutation(instance, host=migration.source_compute):
4634             LOG.debug('Deleting port bindings for target host %s.',
4635                       self.host, instance=instance)
4636             try:
4637                 # Note that deleting the destination host port bindings does
4638                 # not automatically activate the source host port bindings.
4639                 self.network_api.setup_networks_on_host(
4640                     ctxt, instance, host=self.host, teardown=True)
4641             except exception.PortBindingDeletionFailed as e:
4642                 # Do not let this stop us from cleaning up since the guest
4643                 # is already gone.
4644                 LOG.error('Failed to delete port bindings from target host. '
4645                           'Error: %s', six.text_type(e), instance=instance)
4646 
4647         # Delete any volume attachments remaining for this target host.
4648         LOG.debug('Deleting volume attachments for target host.',
4649                   instance=instance)
4650         self._delete_volume_attachments(ctxt, bdms)
4651 
4652         # Free up the new_flavor usage from the resource tracker for this host.
4653         instance.revert_migration_context()
4654         instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4655         self.rt.drop_move_claim(ctxt, instance, instance.node,
4656                                 instance_type=instance.new_flavor)
4657 
4658     @wrap_exception()
4659     @reverts_task_state
4660     @wrap_instance_event(prefix='compute')
4661     @errors_out_migration
4662     @wrap_instance_fault
4663     def finish_revert_snapshot_based_resize_at_source(
4664             self, ctxt, instance, migration):
4665         """Reverts a snapshot-based resize at the source host.
4666 
4667         Spawn the guest and re-connect volumes/VIFs on the source host and
4668         revert the instance to use the old_flavor for resource usage reporting.
4669 
4670         Updates allocations in the placement service to move the source node
4671         allocations, held by the migration record, to the instance and drop
4672         the allocations held by the instance on the destination node.
4673 
4674         :param ctxt: nova auth request context targeted at the target cell
4675         :param instance: Instance object whose vm_state is "resized" and
4676             task_state is "resize_reverting".
4677         :param migration: Migration object whose status is "reverting".
4678         """
4679 
4680         @utils.synchronized(instance.uuid)
4681         def do_revert():
4682             LOG.info('Reverting resize on source host.', instance=instance)
4683             with self._error_out_instance_on_exception(ctxt, instance):
4684                 self._finish_revert_snapshot_based_resize_at_source(
4685                     ctxt, instance, migration)
4686         do_revert()
4687 
4688         # Broadcast to all schedulers that the instance is on this host.
4689         # This is best effort so if anything fails just log it.
4690         try:
4691             self._update_scheduler_instance_info(ctxt, instance)
4692         except Exception as e:
4693             LOG.warning('finish_revert_snapshot_based_resize_at_source failed '
4694                         'during post-processing. Error: %s', e,
4695                         instance=instance)
4696 
4697     def _finish_revert_snapshot_based_resize_at_source(
4698             self, ctxt, instance, migration):
4699         """Private version of finish_revert_snapshot_based_resize_at_source.
4700 
4701         This allows the main method to be decorated with error handlers.
4702 
4703         :param ctxt: nova auth request context targeted at the source cell
4704         :param instance: Instance object whose vm_state is "resized" and
4705             task_state is "resize_reverting".
4706         :param migration: Migration object whose status is "reverting".
4707         """
4708         # Delete stashed old_vm_state information. We will use this to
4709         # determine if the guest should be powered on when we spawn it.
4710         old_vm_state = instance.system_metadata.pop(
4711             'old_vm_state', vm_states.ACTIVE)
4712 
4713         # Update instance host/node and flavor-related fields. After this
4714         # if anything fails the instance will get rebuilt/rebooted on this
4715         # host.
4716         self._finish_revert_resize_update_instance_flavor_host_node(
4717             instance, migration)
4718 
4719         # Move the allocations against the source compute node resource
4720         # provider, held by the migration, to the instance which will drop
4721         # the destination compute node resource provider allocations held by
4722         # the instance. This puts the allocations against the source node
4723         # back to the old_flavor and owned by the instance.
4724         try:
4725             self._revert_allocation(ctxt, instance, migration)
4726         except exception.AllocationMoveFailed:
4727             # Log the error but do not re-raise because we want to continue to
4728             # process ports and volumes below.
4729             LOG.error('Reverting allocation in placement for migration '
4730                       '%(migration_uuid)s failed. You may need to manually '
4731                       'remove the allocations for the migration consumer '
4732                       'against the source node resource provider '
4733                       '%(source_provider)s and the allocations for the '
4734                       'instance consumer against the destination node '
4735                       'resource provider %(dest_provider)s and then run the '
4736                       '"nova-manage placement heal_allocations" command.',
4737                       {'instance_uuid': instance.uuid,
4738                        'migration_uuid': migration.uuid,
4739                        'source_provider': migration.source_node,
4740                        'dest_provider': migration.dest_node},
4741                       instance=instance)
4742 
4743         bdms = instance.get_bdms()
4744         # prep_snapshot_based_resize_at_source created empty volume attachments
4745         # that we need to update here to get the connection_info before calling
4746         # driver.finish_revert_migration which will connect the volumes to this
4747         # host.
4748         LOG.debug('Updating volume attachments for target host %s.',
4749                   self.host, instance=instance)
4750         # TODO(mriedem): We should probably make _update_volume_attachments
4751         # (optionally) graceful to errors so we (1) try to process all
4752         # attachments and (2) continue to process networking below.
4753         self._update_volume_attachments(ctxt, instance, bdms)
4754 
4755         LOG.debug('Updating port bindings for source host %s.',
4756                   self.host, instance=instance)
4757         # TODO(mriedem): Calculate provider mappings when we support
4758         # cross-cell resize/migrate with ports having resource requests.
4759         self._finish_revert_resize_network_migrate_finish(
4760             ctxt, instance, migration, provider_mappings=None)
4761         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4762 
4763         # Remember that prep_snapshot_based_resize_at_source destroyed the
4764         # guest but left the disks intact so we cannot call spawn() here but
4765         # finish_revert_migration should do the job.
4766         block_device_info = self._get_instance_block_device_info(
4767             ctxt, instance, bdms=bdms)
4768         power_on = old_vm_state == vm_states.ACTIVE
4769         driver_error = None
4770         try:
4771             self.driver.finish_revert_migration(
4772                 ctxt, instance, network_info, migration,
4773                 block_device_info=block_device_info, power_on=power_on)
4774         except Exception as e:
4775             driver_error = e
4776             # Leave a hint about hard rebooting the guest and reraise so the
4777             # instance is put into ERROR state.
4778             with excutils.save_and_reraise_exception(logger=LOG):
4779                 LOG.error('An error occurred during finish_revert_migration. '
4780                           'The instance may need to be hard rebooted. Error: '
4781                           '%s', driver_error, instance=instance)
4782         else:
4783             # Perform final cleanup of the instance in the database.
4784             instance.drop_migration_context()
4785             # If the original vm_state was STOPPED, set it back to STOPPED.
4786             vm_state = vm_states.ACTIVE if power_on else vm_states.STOPPED
4787             self._update_instance_after_spawn(
4788                 ctxt, instance, vm_state=vm_state)
4789             instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4790         finally:
4791             # Complete any volume attachments so the volumes are in-use. We
4792             # do this regardless of finish_revert_migration failing because
4793             # the instance is back on this host now and we do not want to leave
4794             # the volumes in a pending state in case the instance is hard
4795             # rebooted.
4796             LOG.debug('Completing volume attachments for instance on source '
4797                       'host.', instance=instance)
4798             with excutils.save_and_reraise_exception(
4799                     reraise=driver_error is not None, logger=LOG):
4800                 self._complete_volume_attachments(ctxt, bdms)
4801 
4802         migration.status = 'reverted'
4803         migration.save()
4804 
4805     @wrap_exception()
4806     @reverts_task_state
4807     @wrap_instance_event(prefix='compute')
4808     @errors_out_migration
4809     @wrap_instance_fault
4810     def revert_resize(self, context, instance, migration, request_spec=None):
4811         """Destroys the new instance on the destination machine.
4812 
4813         Reverts the model changes, and powers on the old instance on the
4814         source machine.
4815 
4816         """
4817         # NOTE(comstud): A revert_resize is essentially a resize back to
4818         # the old size, so we need to send a usage event here.
4819         compute_utils.notify_usage_exists(self.notifier, context, instance,
4820                                           self.host, current_period=True)
4821 
4822         with self._error_out_instance_on_exception(context, instance):
4823             # NOTE(tr3buchet): tear down networks on destination host
4824             self.network_api.setup_networks_on_host(context, instance,
4825                                                     teardown=True)
4826 
4827             self.network_api.migrate_instance_start(context,
4828                                                     instance,
4829                                                     migration)
4830 
4831             network_info = self.network_api.get_instance_nw_info(context,
4832                                                                  instance)
4833             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4834                     context, instance.uuid)
4835             block_device_info = self._get_instance_block_device_info(
4836                                 context, instance, bdms=bdms)
4837 
4838             destroy_disks = not self._is_instance_storage_shared(
4839                 context, instance, host=migration.source_compute)
4840             self.driver.destroy(context, instance, network_info,
4841                                 block_device_info, destroy_disks)
4842 
4843             self._terminate_volume_connections(context, instance, bdms)
4844 
4845             migration.status = 'reverted'
4846             migration.save()
4847 
4848             # NOTE(ndipanov): We need to do this here because dropping the
4849             # claim means we lose the migration_context data. We really should
4850             # fix this by moving the drop_move_claim call to the
4851             # finish_revert_resize method as this is racy (revert is dropped,
4852             # but instance resources will be tracked with the new flavor until
4853             # it gets rolled back in finish_revert_resize, which is
4854             # potentially wrong for a period of time).
4855             instance.revert_migration_context()
4856             instance.save()
4857 
4858             self.rt.drop_move_claim(context, instance, instance.node)
4859 
4860             # RPC cast back to the source host to finish the revert there.
4861             self.compute_rpcapi.finish_revert_resize(context, instance,
4862                     migration, migration.source_compute, request_spec)
4863 
4864     def _finish_revert_resize_network_migrate_finish(
4865             self, context, instance, migration, provider_mappings):
4866         """Causes port binding to be updated. In some Neutron or port
4867         configurations - see NetworkModel.get_bind_time_events() - we
4868         expect the vif-plugged event from Neutron immediately and wait for it.
4869         The rest of the time, the event is expected further along in the
4870         virt driver, so we don't wait here.
4871 
4872         :param context: The request context.
4873         :param instance: The instance undergoing the revert resize.
4874         :param migration: The Migration object of the resize being reverted.
4875         :param provider_mappings: a dict of list of resource provider uuids
4876             keyed by port uuid
4877         :raises: eventlet.timeout.Timeout or
4878                  exception.VirtualInterfacePlugException.
4879         """
4880         network_info = instance.get_network_info()
4881         events = []
4882         deadline = CONF.vif_plugging_timeout
4883         if deadline and utils.is_neutron() and network_info:
4884             events = network_info.get_bind_time_events(migration)
4885             if events:
4886                 LOG.debug('Will wait for bind-time events: %s', events)
4887         error_cb = self._neutron_failed_migration_callback
4888         try:
4889             with self.virtapi.wait_for_instance_event(instance, events,
4890                                                       deadline=deadline,
4891                                                       error_callback=error_cb):
4892                 # NOTE(hanrong): we need to change migration.dest_compute to
4893                 # source host temporarily.
4894                 # "network_api.migrate_instance_finish" will setup the network
4895                 # for the instance on the destination host. For revert resize,
4896                 # the instance will back to the source host, the setup of the
4897                 # network for instance should be on the source host. So set
4898                 # the migration.dest_compute to source host at here.
4899                 with utils.temporary_mutation(
4900                         migration, dest_compute=migration.source_compute):
4901                     self.network_api.migrate_instance_finish(
4902                         context, instance, migration, provider_mappings)
4903         except eventlet.timeout.Timeout:
4904             with excutils.save_and_reraise_exception():
4905                 LOG.error('Timeout waiting for Neutron events: %s', events,
4906                           instance=instance)
4907 
4908     def _finish_revert_resize_update_instance_flavor_host_node(self, instance,
4909                                                                migration):
4910         """Updates host/node and flavor-related fields on the instance.
4911 
4912         This is used when finish the revert resize operation on the source
4913         host and updates the instance flavor-related fields back to the old
4914         flavor and then nulls out the old/new_flavor fields.
4915 
4916         The instance host/node fields are also set back to the source compute
4917         host/node.
4918 
4919         :param instance: Instance object
4920         :param migration: Migration object
4921         """
4922         self._set_instance_info(instance, instance.old_flavor)
4923         instance.old_flavor = None
4924         instance.new_flavor = None
4925         instance.host = migration.source_compute
4926         instance.node = migration.source_node
4927         instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4928 
4929     @wrap_exception()
4930     @reverts_task_state
4931     @wrap_instance_event(prefix='compute')
4932     @errors_out_migration
4933     @wrap_instance_fault
4934     def finish_revert_resize(
4935             self, context, instance, migration, request_spec=None):
4936         """Finishes the second half of reverting a resize on the source host.
4937 
4938         Bring the original source instance state back (active/shutoff) and
4939         revert the resized attributes in the database.
4940 
4941         """
4942         with self._error_out_instance_on_exception(context, instance):
4943             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4944                 context, instance.uuid)
4945             self._notify_about_instance_usage(
4946                     context, instance, "resize.revert.start")
4947             compute_utils.notify_about_instance_action(context, instance,
4948                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4949                     phase=fields.NotificationPhase.START, bdms=bdms)
4950 
4951             # NOTE(mriedem): delete stashed old_vm_state information; we
4952             # default to ACTIVE for backwards compatibility if old_vm_state
4953             # is not set
4954             old_vm_state = instance.system_metadata.pop('old_vm_state',
4955                                                         vm_states.ACTIVE)
4956 
4957             self._finish_revert_resize_update_instance_flavor_host_node(
4958                 instance, migration)
4959 
4960             try:
4961                 source_allocations = self._revert_allocation(
4962                     context, instance, migration)
4963             except exception.AllocationMoveFailed:
4964                 LOG.error('Reverting allocation in placement for migration '
4965                           '%(migration_uuid)s failed. The instance '
4966                           '%(instance_uuid)s will be put into ERROR state but '
4967                           'the allocation held by the migration is leaked.',
4968                           {'instance_uuid': instance.uuid,
4969                            'migration_uuid': migration.uuid})
4970                 raise
4971 
4972             provider_mappings = self._fill_provider_mapping_based_on_allocs(
4973                 context, source_allocations, request_spec)
4974 
4975             self.network_api.setup_networks_on_host(context, instance,
4976                                                     migration.source_compute)
4977             self._finish_revert_resize_network_migrate_finish(
4978                 context, instance, migration, provider_mappings)
4979             network_info = self.network_api.get_instance_nw_info(context,
4980                                                                  instance)
4981 
4982             # revert_resize deleted any volume attachments for the instance
4983             # and created new ones to be used on this host, but we
4984             # have to update those attachments with the host connector so the
4985             # BDM.connection_info will get set in the call to
4986             # _get_instance_block_device_info below with refresh_conn_info=True
4987             # and then the volumes can be re-connected via the driver on this
4988             # host.
4989             self._update_volume_attachments(context, instance, bdms)
4990 
4991             block_device_info = self._get_instance_block_device_info(
4992                     context, instance, refresh_conn_info=True, bdms=bdms)
4993 
4994             power_on = old_vm_state != vm_states.STOPPED
4995             self.driver.finish_revert_migration(
4996                 context, instance, network_info, migration, block_device_info,
4997                 power_on)
4998 
4999             instance.drop_migration_context()
5000             instance.launched_at = timeutils.utcnow()
5001             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
5002 
5003             # Complete any volume attachments so the volumes are in-use.
5004             self._complete_volume_attachments(context, bdms)
5005 
5006             # if the original vm state was STOPPED, set it back to STOPPED
5007             LOG.info("Updating instance to original state: '%s'",
5008                      old_vm_state, instance=instance)
5009             if power_on:
5010                 instance.vm_state = vm_states.ACTIVE
5011                 instance.task_state = None
5012                 instance.save()
5013             else:
5014                 instance.task_state = task_states.POWERING_OFF
5015                 instance.save()
5016                 self.stop_instance(context, instance=instance,
5017                                    clean_shutdown=True)
5018 
5019             self._notify_about_instance_usage(
5020                     context, instance, "resize.revert.end")
5021             compute_utils.notify_about_instance_action(context, instance,
5022                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
5023                     phase=fields.NotificationPhase.END, bdms=bdms)
5024 
5025     def _fill_provider_mapping_based_on_allocs(
5026             self, context, allocations, request_spec):
5027         """Fills and returns the request group - resource provider mapping
5028         based on the allocation passed in.
5029 
5030         :param context: The security context
5031         :param allocation: allocation dict keyed by RP UUID.
5032         :param request_spec: The RequestSpec object associated with the
5033             operation
5034         :returns: None if the request_spec is None. Otherwise a mapping
5035             between RequestGroup requester_id, currently Neutron port_id,
5036             and a list of resource provider UUIDs providing resource for
5037             that RequestGroup.
5038         """
5039         if request_spec:
5040             # NOTE(gibi): We need to re-calculate the resource provider -
5041             # port mapping as we have to have the neutron ports allocate
5042             # from the source compute after revert.
5043             scheduler_utils.fill_provider_mapping_based_on_allocation(
5044                 context, self.reportclient, request_spec, allocations)
5045             provider_mappings = self._get_request_group_mapping(
5046                 request_spec)
5047         else:
5048             # NOTE(gibi): The compute RPC is pinned to be older than 5.2
5049             # and therefore request_spec is not sent. We cannot calculate
5050             # the provider mappings. If the instance has ports with
5051             # resource request then the port update will fail in
5052             # _update_port_binding_for_instance() called via
5053             # _finish_revert_resize_network_migrate_finish() in
5054             # finish_revert_resize.
5055             provider_mappings = None
5056         return provider_mappings
5057 
5058     def _revert_allocation(self, context, instance, migration):
5059         """Revert an allocation that is held by migration to our instance."""
5060 
5061         # Fetch the original allocation that the instance had on the source
5062         # node, which are now held by the migration
5063         orig_alloc = self.reportclient.get_allocations_for_consumer(
5064             context, migration.uuid)
5065         if not orig_alloc:
5066             LOG.error('Did not find resource allocations for migration '
5067                       '%s on source node %s. Unable to revert source node '
5068                       'allocations back to the instance.',
5069                       migration.uuid, migration.source_node, instance=instance)
5070             return False
5071 
5072         LOG.info('Swapping old allocation on %(rp_uuids)s held by migration '
5073                  '%(mig)s for instance',
5074                  {'rp_uuids': orig_alloc.keys(), 'mig': migration.uuid},
5075                  instance=instance)
5076         # FIXME(gibi): This method is flawed in that it does not handle
5077         # allocations against sharing providers in any special way. This leads
5078         # to duplicate allocations against the sharing provider during
5079         # migration.
5080         # TODO(cdent): Should we be doing anything with return values here?
5081         self.reportclient.move_allocations(context, migration.uuid,
5082                                            instance.uuid)
5083         return orig_alloc
5084 
5085     def _prep_resize(self, context, image, instance, instance_type,
5086                      filter_properties, node, migration, request_spec,
5087                      clean_shutdown=True):
5088 
5089         if not filter_properties:
5090             filter_properties = {}
5091 
5092         if not instance.host:
5093             self._set_instance_obj_error_state(context, instance)
5094             msg = _('Instance has no source host')
5095             raise exception.MigrationError(reason=msg)
5096 
5097         same_host = instance.host == self.host
5098         # if the flavor IDs match, it's migrate; otherwise resize
5099         if same_host and instance_type.id == instance['instance_type_id']:
5100             # check driver whether support migrate to same host
5101             if not self.driver.capabilities.get(
5102                     'supports_migrate_to_same_host', False):
5103                 # Raise InstanceFaultRollback so that the
5104                 # _error_out_instance_on_exception context manager in
5105                 # prep_resize will set the instance.vm_state properly.
5106                 raise exception.InstanceFaultRollback(
5107                     inner_exception=exception.UnableToMigrateToSelf(
5108                         instance_id=instance.uuid, host=self.host))
5109 
5110         # NOTE(danms): Stash the new instance_type to avoid having to
5111         # look it up in the database later
5112         instance.new_flavor = instance_type
5113         # NOTE(mriedem): Stash the old vm_state so we can set the
5114         # resized/reverted instance back to the same state later.
5115         vm_state = instance.vm_state
5116         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
5117         instance.system_metadata['old_vm_state'] = vm_state
5118         instance.save()
5119 
5120         if not isinstance(request_spec, objects.RequestSpec):
5121             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
5122             # version of the request spec to compute and since Stein compute
5123             # could be sending that back to conductor on reschedule, so if we
5124             # got a dict convert it to an object.
5125             # TODO(mriedem): We can drop this compat code when we only support
5126             # compute RPC API >=6.0.
5127             request_spec = objects.RequestSpec.from_primitives(
5128                 context, request_spec, filter_properties)
5129             # We don't have to set the new flavor on the request spec because
5130             # if we got here it was due to a reschedule from the compute and
5131             # the request spec would already have the new flavor in it from the
5132             # else block below.
5133 
5134         request_group_resource_providers_mapping = \
5135             self._get_request_group_mapping(request_spec)
5136 
5137         if request_group_resource_providers_mapping:
5138             try:
5139                 compute_utils.\
5140                     update_pci_request_spec_with_allocated_interface_name(
5141                         context, self.reportclient, instance,
5142                         request_group_resource_providers_mapping)
5143             except (exception.AmbiguousResourceProviderForPCIRequest,
5144                     exception.UnexpectedResourceProviderNameForPCIRequest
5145                     ) as e:
5146                 raise exception.BuildAbortException(
5147                     reason=six.text_type(e), instance_uuid=instance.uuid)
5148 
5149         limits = filter_properties.get('limits', {})
5150         allocs = self.reportclient.get_allocations_for_consumer(
5151             context, instance.uuid)
5152         with self.rt.resize_claim(context, instance, instance_type, node,
5153                                   migration, allocs, image_meta=image,
5154                                   limits=limits) as claim:
5155             LOG.info('Migrating', instance=instance)
5156             # RPC cast to the source host to start the actual resize/migration.
5157             self.compute_rpcapi.resize_instance(
5158                     context, instance, claim.migration, image,
5159                     instance_type, request_spec, clean_shutdown)
5160 
5161     def _send_prep_resize_notifications(
5162             self, context, instance, phase, flavor):
5163         """Send "resize.prep.*" notifications.
5164 
5165         :param context: nova auth request context
5166         :param instance: The instance being resized
5167         :param phase: The phase of the action (NotificationPhase enum)
5168         :param flavor: The (new) flavor for the resize (same as existing
5169             instance.flavor for a cold migration)
5170         """
5171         # Only send notify_usage_exists if it's the "start" phase.
5172         if phase == fields.NotificationPhase.START:
5173             compute_utils.notify_usage_exists(
5174                 self.notifier, context, instance, self.host,
5175                 current_period=True)
5176 
5177         # Send extra usage info about the flavor if it's the "end" phase for
5178         # the legacy unversioned notification.
5179         extra_usage_info = None
5180         if phase == fields.NotificationPhase.END:
5181             extra_usage_info = dict(
5182                 new_instance_type=flavor.name,
5183                 new_instance_type_id=flavor.id)
5184         self._notify_about_instance_usage(
5185             context, instance, "resize.prep.%s" % phase,
5186             extra_usage_info=extra_usage_info)
5187 
5188         # Send the versioned notification.
5189         compute_utils.notify_about_resize_prep_instance(
5190             context, instance, self.host, phase, flavor)
5191 
5192     @wrap_exception()
5193     @reverts_task_state
5194     @wrap_instance_event(prefix='compute')
5195     @wrap_instance_fault
5196     def prep_resize(self, context, image, instance, instance_type,
5197                     request_spec, filter_properties, node,
5198                     clean_shutdown, migration, host_list):
5199         """Initiates the process of moving a running instance to another host.
5200 
5201         Possibly changes the VCPU, RAM and disk size in the process.
5202 
5203         This is initiated from conductor and runs on the destination host.
5204 
5205         The main purpose of this method is performing some checks on the
5206         destination host and making a claim for resources. If the claim fails
5207         then a reschedule to another host may be attempted which involves
5208         calling back to conductor to start the process over again.
5209         """
5210         if node is None:
5211             node = self._get_nodename(instance, refresh=True)
5212 
5213         # Pass instance_state=instance.vm_state because we can resize
5214         # a STOPPED server and we don't want to set it back to ACTIVE
5215         # in case _prep_resize fails.
5216         instance_state = instance.vm_state
5217         with self._error_out_instance_on_exception(
5218                 context, instance, instance_state=instance_state),\
5219                 errors_out_migration_ctxt(migration):
5220             self._send_prep_resize_notifications(
5221                 context, instance, fields.NotificationPhase.START,
5222                 instance_type)
5223             try:
5224                 self._prep_resize(context, image, instance,
5225                                   instance_type, filter_properties,
5226                                   node, migration, request_spec,
5227                                   clean_shutdown)
5228             except exception.BuildAbortException:
5229                 # NOTE(gibi): We failed
5230                 # update_pci_request_spec_with_allocated_interface_name so
5231                 # there is no reason to re-schedule. Just revert the allocation
5232                 # and fail the migration.
5233                 with excutils.save_and_reraise_exception():
5234                     self._revert_allocation(context, instance, migration)
5235             except Exception:
5236                 # Since we hit a failure, we're either rescheduling or dead
5237                 # and either way we need to cleanup any allocations created
5238                 # by the scheduler for the destination node.
5239                 self._revert_allocation(context, instance, migration)
5240                 # try to re-schedule the resize elsewhere:
5241                 exc_info = sys.exc_info()
5242                 self._reschedule_resize_or_reraise(context, instance,
5243                         exc_info, instance_type, request_spec,
5244                         filter_properties, host_list)
5245             finally:
5246                 self._send_prep_resize_notifications(
5247                     context, instance, fields.NotificationPhase.END,
5248                     instance_type)
5249 
5250     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
5251             instance_type, request_spec, filter_properties, host_list):
5252         """Try to re-schedule the resize or re-raise the original error to
5253         error out the instance.
5254         """
5255         if not filter_properties:
5256             filter_properties = {}
5257 
5258         rescheduled = False
5259         instance_uuid = instance.uuid
5260 
5261         try:
5262             retry = filter_properties.get('retry')
5263             if retry:
5264                 LOG.debug('Rescheduling, attempt %d', retry['num_attempts'],
5265                           instance_uuid=instance_uuid)
5266 
5267                 # reset the task state
5268                 task_state = task_states.RESIZE_PREP
5269                 self._instance_update(context, instance, task_state=task_state)
5270 
5271                 if exc_info:
5272                     # stringify to avoid circular ref problem in json
5273                     # serialization
5274                     retry['exc'] = traceback.format_exception_only(
5275                         exc_info[0], exc_info[1])
5276 
5277                 scheduler_hint = {'filter_properties': filter_properties}
5278 
5279                 self.compute_task_api.resize_instance(
5280                     context, instance, scheduler_hint, instance_type,
5281                     request_spec=request_spec, host_list=host_list)
5282 
5283                 rescheduled = True
5284             else:
5285                 # no retry information, do not reschedule.
5286                 LOG.debug('Retry info not present, will not reschedule',
5287                           instance_uuid=instance_uuid)
5288                 rescheduled = False
5289         except Exception as error:
5290             rescheduled = False
5291             LOG.exception("Error trying to reschedule",
5292                           instance_uuid=instance_uuid)
5293             compute_utils.add_instance_fault_from_exc(context,
5294                     instance, error,
5295                     exc_info=sys.exc_info())
5296             self._notify_about_instance_usage(context, instance,
5297                     'resize.error', fault=error)
5298             compute_utils.notify_about_instance_action(
5299                 context, instance, self.host,
5300                 action=fields.NotificationAction.RESIZE,
5301                 phase=fields.NotificationPhase.ERROR,
5302                 exception=error,
5303                 tb=','.join(traceback.format_exception(*exc_info)))
5304 
5305         if rescheduled:
5306             self._log_original_error(exc_info, instance_uuid)
5307             compute_utils.add_instance_fault_from_exc(context,
5308                     instance, exc_info[1], exc_info=exc_info)
5309             self._notify_about_instance_usage(context, instance,
5310                     'resize.error', fault=exc_info[1])
5311             compute_utils.notify_about_instance_action(
5312                 context, instance, self.host,
5313                 action=fields.NotificationAction.RESIZE,
5314                 phase=fields.NotificationPhase.ERROR,
5315                 exception=exc_info[1],
5316                 tb=','.join(traceback.format_exception(*exc_info)))
5317         else:
5318             # not re-scheduling
5319             six.reraise(*exc_info)
5320 
5321     @messaging.expected_exceptions(exception.MigrationPreCheckError)
5322     @wrap_exception()
5323     @wrap_instance_event(prefix='compute')
5324     @wrap_instance_fault
5325     def prep_snapshot_based_resize_at_dest(
5326             self, ctxt, instance, flavor, nodename, migration, limits,
5327             request_spec):
5328         """Performs pre-cross-cell resize resource claim on the dest host.
5329 
5330         This runs on the destination host in a cross-cell resize operation
5331         before the resize is actually started.
5332 
5333         Performs a resize_claim for resources that are not claimed in placement
5334         like PCI devices and NUMA topology.
5335 
5336         Note that this is different from same-cell prep_resize in that this:
5337 
5338         * Does not RPC cast to the source compute, that is orchestrated from
5339           conductor.
5340         * This does not reschedule on failure, conductor handles that since
5341           conductor is synchronously RPC calling this method. As such, the
5342           reverts_task_state decorator is not used on this method.
5343 
5344         :param ctxt: user auth request context
5345         :param instance: the instance being resized
5346         :param flavor: the flavor being resized to (unchanged for cold migrate)
5347         :param nodename: Name of the target compute node
5348         :param migration: nova.objects.Migration object for the operation
5349         :param limits: nova.objects.SchedulerLimits object of resource limits
5350         :param request_spec: nova.objects.RequestSpec object for the operation
5351         :returns: nova.objects.MigrationContext; the migration context created
5352             on the destination host during the resize_claim.
5353         :raises: nova.exception.MigrationPreCheckError if the pre-check
5354             validation fails for the given host selection
5355         """
5356         LOG.debug('Checking if we can cross-cell migrate instance to this '
5357                   'host (%s).', self.host, instance=instance)
5358         self._send_prep_resize_notifications(
5359             ctxt, instance, fields.NotificationPhase.START, flavor)
5360         # TODO(mriedem): update_pci_request_spec_with_allocated_interface_name
5361         # should be called here if the request spec has request group mappings,
5362         # e.g. for things like QoS ports with resource requests. Do it outside
5363         # the try/except so if it raises BuildAbortException we do not attempt
5364         # to reschedule.
5365         try:
5366             # Get the allocations within the try/except block in case we get
5367             # an error so MigrationPreCheckError is raised up.
5368             allocations = self.reportclient.get_allocs_for_consumer(
5369                 ctxt, instance.uuid)['allocations']
5370             # Claim resources on this target host using the new flavor which
5371             # will create the MigrationContext object. Note that in the future
5372             # if we want to do other validation here we should do it within
5373             # the MoveClaim context so we can drop the claim if anything fails.
5374             self.rt.resize_claim(
5375                 ctxt, instance, flavor, nodename, migration, allocations,
5376                 image_meta=instance.image_meta, limits=limits)
5377         except Exception as ex:
5378             err = six.text_type(ex)
5379             LOG.warning(
5380                 'Cross-cell resize pre-checks failed for this host (%s). '
5381                 'Cleaning up. Failure: %s', self.host, err,
5382                 instance=instance, exc_info=True)
5383             raise exception.MigrationPreCheckError(
5384                 reason=(_("Pre-checks failed on host '%(host)s'. "
5385                           "Error: %(error)s") %
5386                         {'host': self.host, 'error': err}))
5387         finally:
5388             self._send_prep_resize_notifications(
5389                 ctxt, instance, fields.NotificationPhase.END, flavor)
5390 
5391         # ResourceTracker.resize_claim() sets instance.migration_context.
5392         return instance.migration_context
5393 
5394     @messaging.expected_exceptions(exception.InstancePowerOffFailure)
5395     @wrap_exception()
5396     @reverts_task_state
5397     @wrap_instance_event(prefix='compute')
5398     @errors_out_migration
5399     @wrap_instance_fault
5400     def prep_snapshot_based_resize_at_source(
5401             self, ctxt, instance, migration, snapshot_id=None):
5402         """Prepares the instance at the source host for cross-cell resize
5403 
5404         Performs actions like powering off the guest, upload snapshot data if
5405         the instance is not volume-backed, disconnecting volumes, unplugging
5406         VIFs and activating the destination host port bindings.
5407 
5408         :param ctxt: user auth request context targeted at source cell
5409         :param instance: nova.objects.Instance; the instance being resized.
5410             The expected instance.task_state is "resize_migrating" when calling
5411             this method, and the expected task_state upon successful completion
5412             is "resize_migrated".
5413         :param migration: nova.objects.Migration object for the operation.
5414             The expected migration.status is "pre-migrating" when calling this
5415             method and the expected status upon successful completion is
5416             "post-migrating".
5417         :param snapshot_id: ID of the image snapshot to upload if not a
5418             volume-backed instance
5419         :raises: nova.exception.InstancePowerOffFailure if stopping the
5420             instance fails
5421         """
5422         # Note that if anything fails here, the migration-based allocations
5423         # created in conductor should be reverted by conductor as well,
5424         # see MigrationTask.rollback.
5425         self._prep_snapshot_based_resize_at_source(
5426             ctxt, instance, migration, snapshot_id=snapshot_id)
5427 
5428     @delete_image_on_error
5429     def _snapshot_for_resize(self, ctxt, image_id, instance):
5430         """Uploads snapshot for the instance during a snapshot-based resize
5431 
5432         If the snapshot operation fails the image will be deleted.
5433 
5434         :param ctxt: the nova auth request context for the resize operation
5435         :param image_id: the snapshot image ID
5436         :param instance: the instance to snapshot/resize
5437         """
5438         LOG.debug('Uploading snapshot data for image %s', image_id,
5439                   instance=instance)
5440         # Note that we do not track the snapshot phase task states
5441         # during resize since we do not want to reflect those into the
5442         # actual instance.task_state.
5443         update_task_state = lambda *args, **kwargs: None
5444         with timeutils.StopWatch() as timer:
5445             self.driver.snapshot(ctxt, instance, image_id, update_task_state)
5446             LOG.debug('Took %0.2f seconds to snapshot the instance on '
5447                       'the hypervisor.', timer.elapsed(), instance=instance)
5448 
5449     def _prep_snapshot_based_resize_at_source(
5450             self, ctxt, instance, migration, snapshot_id=None):
5451         """Private method for prep_snapshot_based_resize_at_source so calling
5452         code can handle errors and perform rollbacks as necessary.
5453         """
5454         # Fetch and update the instance.info_cache.
5455         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5456         # Get the BDMs attached to this instance on this source host.
5457         bdms = instance.get_bdms()
5458         # Send the resize.start notification.
5459         self._send_resize_instance_notifications(
5460             ctxt, instance, bdms, network_info, fields.NotificationPhase.START)
5461         # Update the migration status from "pre-migrating" to "migrating".
5462         migration.status = 'migrating'
5463         migration.save()
5464 
5465         # Since the instance is going to be left on the source host during the
5466         # resize, we need to power it off so we do not have the instance
5467         # potentially running in two places.
5468         LOG.debug('Stopping instance', instance=instance)
5469         try:
5470             self._power_off_instance(ctxt, instance)
5471         except Exception as e:
5472             LOG.exception('Failed to power off instance.', instance=instance)
5473             raise exception.InstancePowerOffFailure(reason=six.text_type(e))
5474         instance.power_state = self._get_power_state(ctxt, instance)
5475 
5476         # If a snapshot image ID was provided, we need to snapshot the guest
5477         # disk image and upload it to the image service.
5478         if snapshot_id:
5479             self._snapshot_for_resize(ctxt, snapshot_id, instance)
5480 
5481         block_device_info = self._get_instance_block_device_info(
5482             ctxt, instance, bdms=bdms)
5483 
5484         # If something fails at this point the instance must go to ERROR
5485         # status for operator intervention or to reboot/rebuild the instance.
5486         with self._error_out_instance_on_exception(
5487                 ctxt, instance, instance_state=vm_states.ERROR):
5488 
5489             # Destroy the guest on the source host which will disconnect
5490             # volumes and unplug VIFs. Note that we DO NOT destroy disks since
5491             # we want to leave those on the source host in case of a later
5492             # failure and disks are needed to recover the guest or in case the
5493             # resize is reverted.
5494             LOG.debug('Destroying guest on source host but retaining disks.',
5495                       instance=instance)
5496             self.driver.destroy(
5497                 ctxt, instance, network_info,
5498                 block_device_info=block_device_info, destroy_disks=False)
5499 
5500             # At this point the volumes are disconnected from this source host.
5501             # Delete the old volume attachment records and create new empty
5502             # ones which will be used later if the resize is reverted.
5503             LOG.debug('Deleting volume attachments for the source host.',
5504                       instance=instance)
5505             self._terminate_volume_connections(ctxt, instance, bdms)
5506 
5507             # At this point the VIFs are unplugged from this source host.
5508             # Activate the dest host port bindings created by conductor.
5509             self.network_api.migrate_instance_start(ctxt, instance, migration)
5510 
5511             # Update the migration status from "migrating" to "post-migrating".
5512             migration.status = 'post-migrating'
5513             migration.save()
5514 
5515             # At this point, the traditional resize_instance would update the
5516             # instance host/node values to point at the dest host/node because
5517             # that is where the disk is transferred during resize_instance, but
5518             # with cross-cell resize the instance is not yet at the dest host
5519             # so we do not make that update here.
5520             instance.task_state = task_states.RESIZE_MIGRATED
5521             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5522 
5523         self._send_resize_instance_notifications(
5524             ctxt, instance, bdms, network_info,
5525             fields.NotificationPhase.END)
5526         self.instance_events.clear_events_for_instance(instance)
5527 
5528     @wrap_exception()
5529     @reverts_task_state
5530     @wrap_instance_event(prefix='compute')
5531     @wrap_instance_fault
5532     def resize_instance(self, context, instance, image,
5533                         migration, instance_type, clean_shutdown,
5534                         request_spec=None):
5535         """Starts the migration of a running instance to another host.
5536 
5537         This is initiated from the destination host's ``prep_resize`` routine
5538         and runs on the source host.
5539         """
5540         try:
5541             self._resize_instance(context, instance, image, migration,
5542                                   instance_type, clean_shutdown, request_spec)
5543         except Exception:
5544             with excutils.save_and_reraise_exception():
5545                 self._revert_allocation(context, instance, migration)
5546 
5547     def _resize_instance(self, context, instance, image,
5548                          migration, instance_type, clean_shutdown,
5549                          request_spec):
5550         # Pass instance_state=instance.vm_state because we can resize
5551         # a STOPPED server and we don't want to set it back to ACTIVE
5552         # in case migrate_disk_and_power_off raises InstanceFaultRollback.
5553         instance_state = instance.vm_state
5554         with self._error_out_instance_on_exception(
5555                 context, instance, instance_state=instance_state), \
5556              errors_out_migration_ctxt(migration):
5557             network_info = self.network_api.get_instance_nw_info(context,
5558                                                                  instance)
5559 
5560             migration.status = 'migrating'
5561             migration.save()
5562 
5563             instance.task_state = task_states.RESIZE_MIGRATING
5564             instance.save(expected_task_state=task_states.RESIZE_PREP)
5565 
5566             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5567                     context, instance.uuid)
5568             self._send_resize_instance_notifications(
5569                 context, instance, bdms, network_info,
5570                 fields.NotificationPhase.START)
5571 
5572             block_device_info = self._get_instance_block_device_info(
5573                                 context, instance, bdms=bdms)
5574 
5575             timeout, retry_interval = self._get_power_off_values(context,
5576                                             instance, clean_shutdown)
5577             disk_info = self.driver.migrate_disk_and_power_off(
5578                     context, instance, migration.dest_host,
5579                     instance_type, network_info,
5580                     block_device_info,
5581                     timeout, retry_interval)
5582 
5583             self._terminate_volume_connections(context, instance, bdms)
5584 
5585             self.network_api.migrate_instance_start(context,
5586                                                     instance,
5587                                                     migration)
5588 
5589             migration.status = 'post-migrating'
5590             migration.save()
5591 
5592             instance.host = migration.dest_compute
5593             instance.node = migration.dest_node
5594             instance.task_state = task_states.RESIZE_MIGRATED
5595             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5596 
5597             # RPC cast to the destination host to finish the resize/migration.
5598             self.compute_rpcapi.finish_resize(context, instance,
5599                 migration, image, disk_info, migration.dest_compute,
5600                 request_spec)
5601 
5602         self._send_resize_instance_notifications(
5603             context, instance, bdms, network_info,
5604             fields.NotificationPhase.END)
5605         self.instance_events.clear_events_for_instance(instance)
5606 
5607     def _send_resize_instance_notifications(
5608             self, context, instance, bdms, network_info, phase):
5609         """Send "resize.(start|end)" notifications.
5610 
5611         :param context: nova auth request context
5612         :param instance: The instance being resized
5613         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5614             instance
5615         :param network_info: NetworkInfo for the instance info cache of ports
5616         :param phase: The phase of the action (NotificationPhase enum, either
5617             ``start`` or ``end``)
5618         """
5619         action = fields.NotificationAction.RESIZE
5620         # Send the legacy unversioned notification.
5621         self._notify_about_instance_usage(
5622             context, instance, "%s.%s" % (action, phase),
5623             network_info=network_info)
5624         # Send the versioned notification.
5625         compute_utils.notify_about_instance_action(
5626             context, instance, self.host, action=action, phase=phase,
5627             bdms=bdms)
5628 
5629     def _terminate_volume_connections(self, context, instance, bdms):
5630         connector = None
5631         for bdm in bdms:
5632             if bdm.is_volume:
5633                 if bdm.attachment_id:
5634                     # NOTE(jdg): So here's the thing, the idea behind the new
5635                     # attach API's was to have a new code fork/path that we
5636                     # followed, we're not going to do that so we have to do
5637                     # some extra work in here to make it *behave* just like the
5638                     # old code. Cinder doesn't allow disconnect/reconnect (you
5639                     # just delete the attachment and get a new one)
5640                     # attachments in the new attach code so we have to do
5641                     # a delete and create without a connector (reserve),
5642                     # in other words, beware
5643                     attachment_id = self.volume_api.attachment_create(
5644                         context, bdm.volume_id, instance.uuid)['id']
5645                     self.volume_api.attachment_delete(context,
5646                                                       bdm.attachment_id)
5647                     bdm.attachment_id = attachment_id
5648                     bdm.save()
5649 
5650                 else:
5651                     if connector is None:
5652                         connector = self.driver.get_volume_connector(instance)
5653                     self.volume_api.terminate_connection(context,
5654                                                          bdm.volume_id,
5655                                                          connector)
5656 
5657     @staticmethod
5658     def _set_instance_info(instance, instance_type):
5659         instance.instance_type_id = instance_type.id
5660         instance.memory_mb = instance_type.memory_mb
5661         instance.vcpus = instance_type.vcpus
5662         instance.root_gb = instance_type.root_gb
5663         instance.ephemeral_gb = instance_type.ephemeral_gb
5664         instance.flavor = instance_type
5665 
5666     def _update_volume_attachments(self, context, instance, bdms):
5667         """Updates volume attachments using the virt driver host connector.
5668 
5669         :param context: nova.context.RequestContext - user request context
5670         :param instance: nova.objects.Instance
5671         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5672                      device mappings for the given instance
5673         """
5674         if bdms:
5675             connector = None
5676             for bdm in bdms:
5677                 if bdm.is_volume and bdm.attachment_id:
5678                     if connector is None:
5679                         connector = self.driver.get_volume_connector(instance)
5680                     self.volume_api.attachment_update(
5681                         context, bdm.attachment_id, connector, bdm.device_name)
5682 
5683     def _complete_volume_attachments(self, context, bdms):
5684         """Completes volume attachments for the instance
5685 
5686         :param context: nova.context.RequestContext - user request context
5687         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5688                      device mappings for the given instance
5689         """
5690         if bdms:
5691             for bdm in bdms:
5692                 if bdm.is_volume and bdm.attachment_id:
5693                     self.volume_api.attachment_complete(
5694                         context, bdm.attachment_id)
5695 
5696     def _finish_resize(self, context, instance, migration, disk_info,
5697                        image_meta, bdms, request_spec):
5698         resize_instance = False  # indicates disks have been resized
5699         old_instance_type_id = migration['old_instance_type_id']
5700         new_instance_type_id = migration['new_instance_type_id']
5701         old_flavor = instance.flavor  # the current flavor is now old
5702         # NOTE(mriedem): Get the old_vm_state so we know if we should
5703         # power on the instance. If old_vm_state is not set we need to default
5704         # to ACTIVE for backwards compatibility
5705         old_vm_state = instance.system_metadata.get('old_vm_state',
5706                                                     vm_states.ACTIVE)
5707         instance.old_flavor = old_flavor
5708 
5709         if old_instance_type_id != new_instance_type_id:
5710             new_flavor = instance.new_flavor  # this is set in _prep_resize
5711             # Set the flavor-related fields on the instance object including
5712             # making instance.flavor = new_flavor.
5713             self._set_instance_info(instance, new_flavor)
5714             for key in ('root_gb', 'swap', 'ephemeral_gb'):
5715                 if old_flavor[key] != new_flavor[key]:
5716                     resize_instance = True
5717                     break
5718         instance.apply_migration_context()
5719 
5720         # NOTE(tr3buchet): setup networks on destination host
5721         self.network_api.setup_networks_on_host(context, instance,
5722                                                 migration.dest_compute)
5723         provider_mappings = self._get_request_group_mapping(request_spec)
5724 
5725         # For neutron, migrate_instance_finish updates port bindings for this
5726         # host including any PCI devices claimed for SR-IOV ports.
5727         self.network_api.migrate_instance_finish(
5728             context, instance, migration, provider_mappings)
5729 
5730         network_info = self.network_api.get_instance_nw_info(context, instance)
5731 
5732         instance.task_state = task_states.RESIZE_FINISH
5733         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5734 
5735         self._send_finish_resize_notifications(
5736             context, instance, bdms, network_info,
5737             fields.NotificationPhase.START)
5738 
5739         # We need to update any volume attachments using the destination
5740         # host connector so that we can update the BDM.connection_info
5741         # before calling driver.finish_migration otherwise the driver
5742         # won't know how to connect the volumes to this host.
5743         # Note that _get_instance_block_device_info with
5744         # refresh_conn_info=True will update the BDM.connection_info value
5745         # in the database so we must do this before calling that method.
5746         self._update_volume_attachments(context, instance, bdms)
5747 
5748         block_device_info = self._get_instance_block_device_info(
5749             context, instance, refresh_conn_info=True, bdms=bdms)
5750 
5751         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
5752         # automatically power on the instance after it's migrated
5753         power_on = old_vm_state != vm_states.STOPPED
5754 
5755         try:
5756             self.driver.finish_migration(context, migration, instance,
5757                                          disk_info,
5758                                          network_info,
5759                                          image_meta, resize_instance,
5760                                          block_device_info, power_on)
5761         except Exception:
5762             # Note that we do not rollback port bindings to the source host
5763             # because resize_instance (on the source host) updated the
5764             # instance.host to point to *this* host (the destination host)
5765             # so the port bindings pointing at this host are correct even
5766             # though we failed to create the guest.
5767             with excutils.save_and_reraise_exception():
5768                 # If we failed to create the guest on this host, reset the
5769                 # instance flavor-related fields to the old flavor. An
5770                 # error handler like reverts_task_state will save the changes.
5771                 if old_instance_type_id != new_instance_type_id:
5772                     self._set_instance_info(instance, old_flavor)
5773 
5774         # Now complete any volume attachments that were previously updated.
5775         self._complete_volume_attachments(context, bdms)
5776 
5777         migration.status = 'finished'
5778         migration.save()
5779 
5780         instance.vm_state = vm_states.RESIZED
5781         instance.task_state = None
5782         instance.launched_at = timeutils.utcnow()
5783         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5784 
5785         return network_info
5786 
5787     @wrap_exception()
5788     @reverts_task_state
5789     @wrap_instance_event(prefix='compute')
5790     @errors_out_migration
5791     @wrap_instance_fault
5792     def finish_resize(self, context, disk_info, image, instance,
5793                       migration, request_spec=None):
5794         """Completes the migration process.
5795 
5796         Sets up the newly transferred disk and turns on the instance at its
5797         new host machine.
5798 
5799         """
5800         try:
5801             self._finish_resize_helper(context, disk_info, image, instance,
5802                                        migration, request_spec)
5803         except Exception:
5804             with excutils.save_and_reraise_exception():
5805                 # At this point, resize_instance (which runs on the source) has
5806                 # already updated the instance host/node values to point to
5807                 # this (the dest) compute, so we need to leave the allocations
5808                 # against the dest node resource provider intact and drop the
5809                 # allocations against the source node resource provider. If the
5810                 # user tries to recover the server by hard rebooting it, it
5811                 # will happen on this host so that's where the allocations
5812                 # should go. Note that this is the same method called from
5813                 # confirm_resize to cleanup the source node allocations held
5814                 # by the migration record.
5815                 LOG.info('Deleting allocations for old flavor on source node '
5816                          '%s after finish_resize failure. You may be able to '
5817                          'recover the instance by hard rebooting it.',
5818                          migration.source_compute, instance=instance)
5819                 self._delete_allocation_after_move(
5820                     context, instance, migration)
5821 
5822     def _finish_resize_helper(self, context, disk_info, image, instance,
5823                               migration, request_spec):
5824         """Completes the migration process.
5825 
5826         The caller must revert the instance's allocations if the migration
5827         process failed.
5828         """
5829         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5830             context, instance.uuid)
5831 
5832         with self._error_out_instance_on_exception(context, instance):
5833             image_meta = objects.ImageMeta.from_dict(image)
5834             network_info = self._finish_resize(context, instance, migration,
5835                                                disk_info, image_meta, bdms,
5836                                                request_spec)
5837 
5838         # TODO(melwitt): We should clean up instance console tokens here. The
5839         # instance is on a new host and will need to establish a new console
5840         # connection.
5841         self._update_scheduler_instance_info(context, instance)
5842         self._send_finish_resize_notifications(
5843             context, instance, bdms, network_info,
5844             fields.NotificationPhase.END)
5845 
5846     def _send_finish_resize_notifications(
5847             self, context, instance, bdms, network_info, phase):
5848         """Send notifications for the finish_resize flow.
5849 
5850         :param context: nova auth request context
5851         :param instance: The instance being resized
5852         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5853             instance
5854         :param network_info: NetworkInfo for the instance info cache of ports
5855         :param phase: The phase of the action (NotificationPhase enum, either
5856             ``start`` or ``end``)
5857         """
5858         # Send the legacy unversioned notification.
5859         self._notify_about_instance_usage(
5860             context, instance, "finish_resize.%s" % phase,
5861             network_info=network_info)
5862         # Send the versioned notification.
5863         compute_utils.notify_about_instance_action(
5864             context, instance, self.host,
5865             action=fields.NotificationAction.RESIZE_FINISH, phase=phase,
5866             bdms=bdms)
5867 
5868     @wrap_exception()
5869     @reverts_task_state
5870     @wrap_instance_event(prefix='compute')
5871     @errors_out_migration
5872     @wrap_instance_fault
5873     def finish_snapshot_based_resize_at_dest(
5874             self, ctxt, instance, migration, snapshot_id, request_spec):
5875         """Finishes the snapshot-based resize at the destination compute.
5876 
5877         Sets up block devices and networking on the destination compute and
5878         spawns the guest.
5879 
5880         :param ctxt: nova auth request context targeted at the target cell DB
5881         :param instance: The Instance object being resized with the
5882             ``migration_context`` field set. Upon successful completion of this
5883             method the vm_state should be "resized", the task_state should be
5884             None, and migration context, host/node and flavor-related fields
5885             should be set on the instance.
5886         :param migration: The Migration object for this resize operation. Upon
5887             successful completion of this method the migration status should
5888             be "finished".
5889         :param snapshot_id: ID of the image snapshot created for a
5890             non-volume-backed instance, else None.
5891         :param request_spec: nova.objects.RequestSpec object for the operation
5892         """
5893         LOG.info('Finishing snapshot based resize on destination host %s.',
5894                  self.host, instance=instance)
5895         with self._error_out_instance_on_exception(ctxt, instance):
5896             # Note that if anything fails here, the migration-based allocations
5897             # created in conductor should be reverted by conductor as well,
5898             # see MigrationTask.rollback.
5899             self._finish_snapshot_based_resize_at_dest(
5900                 ctxt, instance, migration, snapshot_id)
5901 
5902     def _finish_snapshot_based_resize_at_dest(
5903             self, ctxt, instance, migration, snapshot_id):
5904         """Private variant of finish_snapshot_based_resize_at_dest so the
5905         caller can handle reverting resource allocations on failure and perform
5906         other generic error handling.
5907         """
5908         # Figure out the image metadata to use when spawning the guest.
5909         if snapshot_id:
5910             image_meta = objects.ImageMeta.from_image_ref(
5911                 ctxt, self.image_api, snapshot_id)
5912         else:
5913             # Just use what is already on the volume-backed instance.
5914             image_meta = instance.image_meta
5915 
5916         resize = migration.migration_type == 'resize'
5917         instance.old_flavor = instance.flavor
5918         if resize:
5919             flavor = instance.new_flavor
5920             # If we are resizing to a new flavor we need to set the
5921             # flavor-related fields on the instance.
5922             # NOTE(mriedem): This is likely where storing old/new_flavor on
5923             # the MigrationContext would make this cleaner.
5924             self._set_instance_info(instance, flavor)
5925 
5926         instance.apply_migration_context()
5927         instance.task_state = task_states.RESIZE_FINISH
5928         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5929 
5930         # This seems a bit late to be sending the start notification but
5931         # it is what traditional resize has always done as well and it does
5932         # contain the changes to the instance with the new_flavor and
5933         # task_state.
5934         bdms = instance.get_bdms()
5935         network_info = instance.get_network_info()
5936         self._send_finish_resize_notifications(
5937             ctxt, instance, bdms, network_info,
5938             fields.NotificationPhase.START)
5939 
5940         # Setup volumes and networking and spawn the guest in the hypervisor.
5941         self._finish_snapshot_based_resize_at_dest_spawn(
5942             ctxt, instance, migration, image_meta, bdms)
5943 
5944         # If we spawned from a temporary snapshot image we can delete that now,
5945         # similar to how unshelve works.
5946         if snapshot_id:
5947             # FIXME(mriedem): Need to deal with bug 1653953 for libvirt with
5948             # the rbd image backend. I think the cleanest thing we can do is
5949             # from the driver check to see if instance.migration_context is not
5950             # None and if so, get the Migration record for that context
5951             # (instance.migration_context.migration_id) and from that check the
5952             # Migration.cross_cell_move flag and if True, then flatten the
5953             # image.
5954             compute_utils.delete_image(
5955                 ctxt, instance, self.image_api, snapshot_id)
5956 
5957         migration.status = 'finished'
5958         migration.save()
5959 
5960         self._update_instance_after_spawn(
5961             ctxt, instance, vm_state=vm_states.RESIZED)
5962         # Setting the host/node values will make the ResourceTracker continue
5963         # to track usage for this instance on this host.
5964         instance.host = migration.dest_compute
5965         instance.node = migration.dest_node
5966         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5967 
5968         # Broadcast to all schedulers that the instance is on this host.
5969         self._update_scheduler_instance_info(ctxt, instance)
5970         self._send_finish_resize_notifications(
5971             ctxt, instance, bdms, network_info,
5972             fields.NotificationPhase.END)
5973 
5974     def _finish_snapshot_based_resize_at_dest_spawn(
5975             self, ctxt, instance, migration, image_meta, bdms):
5976         """Sets up volumes and networking and spawns the guest on the dest host
5977 
5978         If the instance was stopped when the resize was initiated the guest
5979         will be created but remain in a shutdown power state.
5980 
5981         If the spawn fails, port bindings are rolled back to the source host
5982         and volume connections are terminated for this dest host.
5983 
5984         :param ctxt: nova auth request context
5985         :param instance: Instance object being migrated
5986         :param migration: Migration object for the operation
5987         :param image_meta: ImageMeta object used during driver.spawn
5988         :param bdms: BlockDeviceMappingList of BDMs for the instance
5989         """
5990         # Update the volume attachments using this host's connector.
5991         # That will update the BlockDeviceMapping.connection_info which
5992         # will be used to connect the volumes on this host during spawn().
5993         block_device_info = self._prep_block_device(ctxt, instance, bdms)
5994 
5995         allocations = self.reportclient.get_allocations_for_consumer(
5996             ctxt, instance.uuid)
5997 
5998         # We do not call self.network_api.setup_networks_on_host here because
5999         # for neutron that sets up the port migration profile which is only
6000         # used during live migration with DVR. Yes it is gross knowing what
6001         # that method does internally. We could change this when bug 1814837
6002         # is fixed if setup_networks_on_host is made smarter by passing the
6003         # migration record and the method checks the migration_type.
6004 
6005         # Activate the port bindings for this host.
6006         # FIXME(mriedem): We're going to have the same issue as bug 1813789
6007         # here because this will update the port bindings and send the
6008         # network-vif-plugged event and that means when driver.spawn waits for
6009         # it we might have already gotten the event and neutron won't send
6010         # another one so we could timeout.
6011         # TODO(mriedem): Calculate provider mappings when we support cross-cell
6012         # resize/migrate with ports having resource requests.
6013         self.network_api.migrate_instance_finish(
6014             ctxt, instance, migration, provider_mappings=None)
6015         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6016 
6017         # If the original vm_state was STOPPED, we do not automatically
6018         # power on the instance after it is migrated.
6019         power_on = instance.system_metadata['old_vm_state'] == vm_states.ACTIVE
6020         try:
6021             # NOTE(mriedem): If this instance uses a config drive, it will get
6022             # rebuilt here which means any personality files will be lost,
6023             # similar to unshelve. If the instance is not using a config drive
6024             # and getting metadata from the metadata API service, personality
6025             # files would be lost regardless of the move operation.
6026             self.driver.spawn(
6027                 ctxt, instance, image_meta, injected_files=[],
6028                 admin_password=None, allocations=allocations,
6029                 network_info=network_info, block_device_info=block_device_info,
6030                 power_on=power_on)
6031         except Exception:
6032             with excutils.save_and_reraise_exception(logger=LOG):
6033                 # Rollback port bindings to the source host.
6034                 try:
6035                     # This is gross but migrate_instance_start looks at the
6036                     # migration.dest_compute to determine where to activate the
6037                     # port bindings and we want the source compute port
6038                     # bindings to be re-activated. Remember at this point the
6039                     # instance.host is still pointing at the source compute.
6040                     # TODO(mriedem): Maybe we should be calling
6041                     # setup_instance_network_on_host here to deal with pci
6042                     # devices?
6043                     with utils.temporary_mutation(
6044                             migration, dest_compute=migration.source_compute):
6045                         self.network_api.migrate_instance_start(
6046                             ctxt, instance, migration)
6047                 except Exception:
6048                     LOG.exception(
6049                         'Failed to activate port bindings on the source '
6050                         'host: %s', migration.source_compute,
6051                         instance=instance)
6052 
6053                 # Rollback volume connections on this host.
6054                 for bdm in bdms:
6055                     if bdm.is_volume:
6056                         try:
6057                             self._remove_volume_connection(
6058                                 ctxt, bdm, instance, delete_attachment=True)
6059                         except Exception:
6060                             LOG.exception('Failed to remove volume connection '
6061                                           'on this host %s for volume %s.',
6062                                           self.host, bdm.volume_id,
6063                                           instance=instance)
6064 
6065     @wrap_exception()
6066     @wrap_instance_fault
6067     def add_fixed_ip_to_instance(self, context, network_id, instance):
6068         """Calls network_api to add new fixed_ip to instance
6069         then injects the new network info and resets instance networking.
6070 
6071         """
6072         self._notify_about_instance_usage(
6073                 context, instance, "create_ip.start")
6074 
6075         network_info = self.network_api.add_fixed_ip_to_instance(context,
6076                                                                  instance,
6077                                                                  network_id)
6078         self._inject_network_info(context, instance, network_info)
6079         self.reset_network(context, instance)
6080 
6081         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6082         instance.updated_at = timeutils.utcnow()
6083         instance.save()
6084 
6085         self._notify_about_instance_usage(
6086             context, instance, "create_ip.end", network_info=network_info)
6087 
6088     @wrap_exception()
6089     @wrap_instance_fault
6090     def remove_fixed_ip_from_instance(self, context, address, instance):
6091         """Calls network_api to remove existing fixed_ip from instance
6092         by injecting the altered network info and resetting
6093         instance networking.
6094         """
6095         self._notify_about_instance_usage(
6096                 context, instance, "delete_ip.start")
6097 
6098         network_info = self.network_api.remove_fixed_ip_from_instance(context,
6099                                                                       instance,
6100                                                                       address)
6101         self._inject_network_info(context, instance, network_info)
6102         self.reset_network(context, instance)
6103 
6104         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6105         instance.updated_at = timeutils.utcnow()
6106         instance.save()
6107 
6108         self._notify_about_instance_usage(
6109             context, instance, "delete_ip.end", network_info=network_info)
6110 
6111     @wrap_exception()
6112     @reverts_task_state
6113     @wrap_instance_event(prefix='compute')
6114     @wrap_instance_fault
6115     def pause_instance(self, context, instance):
6116         """Pause an instance on this host."""
6117         context = context.elevated()
6118         LOG.info('Pausing', instance=instance)
6119         self._notify_about_instance_usage(context, instance, 'pause.start')
6120         compute_utils.notify_about_instance_action(context, instance,
6121                self.host, action=fields.NotificationAction.PAUSE,
6122                phase=fields.NotificationPhase.START)
6123         self.driver.pause(instance)
6124         instance.power_state = self._get_power_state(context, instance)
6125         instance.vm_state = vm_states.PAUSED
6126         instance.task_state = None
6127         instance.save(expected_task_state=task_states.PAUSING)
6128         self._notify_about_instance_usage(context, instance, 'pause.end')
6129         compute_utils.notify_about_instance_action(context, instance,
6130                self.host, action=fields.NotificationAction.PAUSE,
6131                phase=fields.NotificationPhase.END)
6132 
6133     @wrap_exception()
6134     @reverts_task_state
6135     @wrap_instance_event(prefix='compute')
6136     @wrap_instance_fault
6137     def unpause_instance(self, context, instance):
6138         """Unpause a paused instance on this host."""
6139         context = context.elevated()
6140         LOG.info('Unpausing', instance=instance)
6141         self._notify_about_instance_usage(context, instance, 'unpause.start')
6142         compute_utils.notify_about_instance_action(context, instance,
6143             self.host, action=fields.NotificationAction.UNPAUSE,
6144             phase=fields.NotificationPhase.START)
6145         self.driver.unpause(instance)
6146         instance.power_state = self._get_power_state(context, instance)
6147         instance.vm_state = vm_states.ACTIVE
6148         instance.task_state = None
6149         instance.save(expected_task_state=task_states.UNPAUSING)
6150         self._notify_about_instance_usage(context, instance, 'unpause.end')
6151         compute_utils.notify_about_instance_action(context, instance,
6152             self.host, action=fields.NotificationAction.UNPAUSE,
6153             phase=fields.NotificationPhase.END)
6154 
6155     @wrap_exception()
6156     def host_power_action(self, context, action):
6157         """Reboots, shuts down or powers up the host."""
6158         return self.driver.host_power_action(action)
6159 
6160     @wrap_exception()
6161     def host_maintenance_mode(self, context, host, mode):
6162         """Start/Stop host maintenance window. On start, it triggers
6163         guest VMs evacuation.
6164         """
6165         return self.driver.host_maintenance_mode(host, mode)
6166 
6167     def _update_compute_provider_status(self, context, enabled):
6168         """Adds or removes the COMPUTE_STATUS_DISABLED trait for this host.
6169 
6170         For each ComputeNode managed by this service, adds or removes the
6171         COMPUTE_STATUS_DISABLED traits to/from the associated resource provider
6172         in Placement.
6173 
6174         :param context: nova auth RequestContext
6175         :param enabled: True if the node is enabled in which case the trait
6176             would be removed, False if the node is disabled in which case
6177             the trait would be added.
6178         :raises: ComputeHostNotFound if there are no compute nodes found in
6179             the ResourceTracker for this service.
6180         """
6181         # Get the compute node(s) on this host. Remember that ironic can be
6182         # managing more than one compute node.
6183         nodes = self.rt.compute_nodes.values()
6184         if not nodes:
6185             raise exception.ComputeHostNotFound(host=self.host)
6186         # For each node, we want to add (or remove) the COMPUTE_STATUS_DISABLED
6187         # trait on the related resource provider in placement so the scheduler
6188         # (pre-)filters the provider based on its status.
6189         for node in nodes:
6190             try:
6191                 self.virtapi.update_compute_provider_status(
6192                     context, node.uuid, enabled)
6193             except (exception.ResourceProviderTraitRetrievalFailed,
6194                     exception.ResourceProviderUpdateConflict,
6195                     exception.ResourceProviderUpdateFailed,
6196                     exception.TraitRetrievalFailed) as e:
6197                 # This is best effort so just log a warning and continue.
6198                 LOG.warning('An error occurred while updating '
6199                             'COMPUTE_STATUS_DISABLED trait on compute node '
6200                             'resource provider %s. The trait will be '
6201                             'synchronized when the update_available_resource '
6202                             'periodic task runs. Error: %s',
6203                             node.uuid, e.format_message())
6204             except Exception:
6205                 LOG.exception('An error occurred while updating '
6206                               'COMPUTE_STATUS_DISABLED trait on compute node '
6207                               'resource provider %s. The trait will be '
6208                               'synchronized when the '
6209                               'update_available_resource periodic task runs.',
6210                               node.uuid)
6211 
6212     @wrap_exception()
6213     def set_host_enabled(self, context, enabled):
6214         """Sets the specified host's ability to accept new instances.
6215 
6216         This method will add or remove the COMPUTE_STATUS_DISABLED trait
6217         to/from the associated compute node resource provider(s) for this
6218         compute service.
6219         """
6220         try:
6221             self._update_compute_provider_status(context, enabled)
6222         except exception.ComputeHostNotFound:
6223             LOG.warning('Unable to add/remove trait COMPUTE_STATUS_DISABLED. '
6224                         'No ComputeNode(s) found for host: %s', self.host)
6225 
6226         try:
6227             return self.driver.set_host_enabled(enabled)
6228         except NotImplementedError:
6229             # Only the xenapi driver implements set_host_enabled but we don't
6230             # want NotImplementedError to get raised back to the API. We still
6231             # need to honor the compute RPC API contract and return 'enabled'
6232             # or 'disabled' though.
6233             return 'enabled' if enabled else 'disabled'
6234 
6235     @wrap_exception()
6236     def get_host_uptime(self, context):
6237         """Returns the result of calling "uptime" on the target host."""
6238         return self.driver.get_host_uptime()
6239 
6240     @wrap_exception()
6241     @wrap_instance_fault
6242     def get_diagnostics(self, context, instance):
6243         """Retrieve diagnostics for an instance on this host."""
6244         current_power_state = self._get_power_state(context, instance)
6245         if current_power_state == power_state.RUNNING:
6246             LOG.info("Retrieving diagnostics", instance=instance)
6247             return self.driver.get_diagnostics(instance)
6248         else:
6249             raise exception.InstanceInvalidState(
6250                 attr='power state',
6251                 instance_uuid=instance.uuid,
6252                 state=power_state.STATE_MAP[instance.power_state],
6253                 method='get_diagnostics')
6254 
6255     @wrap_exception()
6256     @wrap_instance_fault
6257     def get_instance_diagnostics(self, context, instance):
6258         """Retrieve diagnostics for an instance on this host."""
6259         current_power_state = self._get_power_state(context, instance)
6260         if current_power_state == power_state.RUNNING:
6261             LOG.info("Retrieving diagnostics", instance=instance)
6262             return self.driver.get_instance_diagnostics(instance)
6263         else:
6264             raise exception.InstanceInvalidState(
6265                 attr='power state',
6266                 instance_uuid=instance.uuid,
6267                 state=power_state.STATE_MAP[instance.power_state],
6268                 method='get_diagnostics')
6269 
6270     @wrap_exception()
6271     @reverts_task_state
6272     @wrap_instance_event(prefix='compute')
6273     @wrap_instance_fault
6274     def suspend_instance(self, context, instance):
6275         """Suspend the given instance."""
6276         context = context.elevated()
6277 
6278         # Store the old state
6279         instance.system_metadata['old_vm_state'] = instance.vm_state
6280         self._notify_about_instance_usage(context, instance, 'suspend.start')
6281         compute_utils.notify_about_instance_action(context, instance,
6282                 self.host, action=fields.NotificationAction.SUSPEND,
6283                 phase=fields.NotificationPhase.START)
6284         with self._error_out_instance_on_exception(context, instance,
6285              instance_state=instance.vm_state):
6286             self.driver.suspend(context, instance)
6287         instance.power_state = self._get_power_state(context, instance)
6288         instance.vm_state = vm_states.SUSPENDED
6289         instance.task_state = None
6290         instance.save(expected_task_state=task_states.SUSPENDING)
6291         self._notify_about_instance_usage(context, instance, 'suspend.end')
6292         compute_utils.notify_about_instance_action(context, instance,
6293                 self.host, action=fields.NotificationAction.SUSPEND,
6294                 phase=fields.NotificationPhase.END)
6295 
6296     @wrap_exception()
6297     @reverts_task_state
6298     @wrap_instance_event(prefix='compute')
6299     @wrap_instance_fault
6300     def resume_instance(self, context, instance):
6301         """Resume the given suspended instance."""
6302         context = context.elevated()
6303         LOG.info('Resuming', instance=instance)
6304 
6305         self._notify_about_instance_usage(context, instance, 'resume.start')
6306 
6307         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6308             context, instance.uuid)
6309         block_device_info = self._get_instance_block_device_info(
6310             context, instance, bdms=bdms)
6311 
6312         compute_utils.notify_about_instance_action(context, instance,
6313             self.host, action=fields.NotificationAction.RESUME,
6314             phase=fields.NotificationPhase.START, bdms=bdms)
6315 
6316         network_info = self.network_api.get_instance_nw_info(context, instance)
6317 
6318         with self._error_out_instance_on_exception(context, instance,
6319              instance_state=instance.vm_state):
6320             self.driver.resume(context, instance, network_info,
6321                                block_device_info)
6322 
6323         instance.power_state = self._get_power_state(context, instance)
6324 
6325         # We default to the ACTIVE state for backwards compatibility
6326         instance.vm_state = instance.system_metadata.pop('old_vm_state',
6327                                                          vm_states.ACTIVE)
6328 
6329         instance.task_state = None
6330         instance.save(expected_task_state=task_states.RESUMING)
6331         self._notify_about_instance_usage(context, instance, 'resume.end')
6332         compute_utils.notify_about_instance_action(context, instance,
6333             self.host, action=fields.NotificationAction.RESUME,
6334             phase=fields.NotificationPhase.END, bdms=bdms)
6335 
6336     @wrap_exception()
6337     @reverts_task_state
6338     @wrap_instance_event(prefix='compute')
6339     @wrap_instance_fault
6340     def shelve_instance(self, context, instance, image_id,
6341                         clean_shutdown):
6342         """Shelve an instance.
6343 
6344         This should be used when you want to take a snapshot of the instance.
6345         It also adds system_metadata that can be used by a periodic task to
6346         offload the shelved instance after a period of time.
6347 
6348         :param context: request context
6349         :param instance: an Instance object
6350         :param image_id: an image id to snapshot to.
6351         :param clean_shutdown: give the GuestOS a chance to stop
6352         """
6353 
6354         @utils.synchronized(instance.uuid)
6355         def do_shelve_instance():
6356             self._shelve_instance(context, instance, image_id, clean_shutdown)
6357         do_shelve_instance()
6358 
6359     def _shelve_instance(self, context, instance, image_id,
6360                          clean_shutdown):
6361         LOG.info('Shelving', instance=instance)
6362         offload = CONF.shelved_offload_time == 0
6363         if offload:
6364             # Get the BDMs early so we can pass them into versioned
6365             # notifications since _shelve_offload_instance needs the
6366             # BDMs anyway.
6367             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6368                 context, instance.uuid)
6369         else:
6370             bdms = None
6371         compute_utils.notify_usage_exists(self.notifier, context, instance,
6372                                           self.host, current_period=True)
6373         self._notify_about_instance_usage(context, instance, 'shelve.start')
6374         compute_utils.notify_about_instance_action(context, instance,
6375                 self.host, action=fields.NotificationAction.SHELVE,
6376                 phase=fields.NotificationPhase.START, bdms=bdms)
6377 
6378         def update_task_state(task_state, expected_state=task_states.SHELVING):
6379             shelving_state_map = {
6380                     task_states.IMAGE_PENDING_UPLOAD:
6381                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
6382                     task_states.IMAGE_UPLOADING:
6383                         task_states.SHELVING_IMAGE_UPLOADING,
6384                     task_states.SHELVING: task_states.SHELVING}
6385             task_state = shelving_state_map[task_state]
6386             expected_state = shelving_state_map[expected_state]
6387             instance.task_state = task_state
6388             instance.save(expected_task_state=expected_state)
6389         # Do not attempt a clean shutdown of a paused guest since some
6390         # hypervisors will fail the clean shutdown if the guest is not
6391         # running.
6392         if instance.power_state == power_state.PAUSED:
6393             clean_shutdown = False
6394         self._power_off_instance(context, instance, clean_shutdown)
6395         self.driver.snapshot(context, instance, image_id, update_task_state)
6396 
6397         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
6398         instance.system_metadata['shelved_image_id'] = image_id
6399         instance.system_metadata['shelved_host'] = self.host
6400         instance.vm_state = vm_states.SHELVED
6401         instance.task_state = None
6402         if CONF.shelved_offload_time == 0:
6403             instance.task_state = task_states.SHELVING_OFFLOADING
6404         instance.power_state = self._get_power_state(context, instance)
6405         instance.save(expected_task_state=[
6406                 task_states.SHELVING,
6407                 task_states.SHELVING_IMAGE_UPLOADING])
6408 
6409         self._notify_about_instance_usage(context, instance, 'shelve.end')
6410         compute_utils.notify_about_instance_action(context, instance,
6411                 self.host, action=fields.NotificationAction.SHELVE,
6412                 phase=fields.NotificationPhase.END, bdms=bdms)
6413 
6414         if offload:
6415             self._shelve_offload_instance(context, instance,
6416                                           clean_shutdown=False, bdms=bdms)
6417 
6418     @wrap_exception()
6419     @reverts_task_state
6420     @wrap_instance_event(prefix='compute')
6421     @wrap_instance_fault
6422     def shelve_offload_instance(self, context, instance, clean_shutdown):
6423         """Remove a shelved instance from the hypervisor.
6424 
6425         This frees up those resources for use by other instances, but may lead
6426         to slower unshelve times for this instance.  This method is used by
6427         volume backed instances since restoring them doesn't involve the
6428         potentially large download of an image.
6429 
6430         :param context: request context
6431         :param instance: nova.objects.instance.Instance
6432         :param clean_shutdown: give the GuestOS a chance to stop
6433         """
6434 
6435         @utils.synchronized(instance.uuid)
6436         def do_shelve_offload_instance():
6437             self._shelve_offload_instance(context, instance, clean_shutdown)
6438         do_shelve_offload_instance()
6439 
6440     def _shelve_offload_instance(self, context, instance, clean_shutdown,
6441                                  bdms=None):
6442         LOG.info('Shelve offloading', instance=instance)
6443         if bdms is None:
6444             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6445                 context, instance.uuid)
6446         self._notify_about_instance_usage(context, instance,
6447                 'shelve_offload.start')
6448         compute_utils.notify_about_instance_action(context, instance,
6449                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6450                 phase=fields.NotificationPhase.START, bdms=bdms)
6451 
6452         self._power_off_instance(context, instance, clean_shutdown)
6453         current_power_state = self._get_power_state(context, instance)
6454 
6455         self.network_api.cleanup_instance_network_on_host(context, instance,
6456                                                           instance.host)
6457         network_info = self.network_api.get_instance_nw_info(context, instance)
6458 
6459         block_device_info = self._get_instance_block_device_info(context,
6460                                                                  instance,
6461                                                                  bdms=bdms)
6462         self.driver.destroy(context, instance, network_info,
6463                 block_device_info)
6464 
6465         # the instance is going to be removed from the host so we want to
6466         # terminate all the connections with the volume server and the host
6467         self._terminate_volume_connections(context, instance, bdms)
6468 
6469         # Free up the resource allocations in the placement service.
6470         # This should happen *before* the vm_state is changed to
6471         # SHELVED_OFFLOADED in case client-side code is polling the API to
6472         # schedule more instances (or unshelve) once this server is offloaded.
6473         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
6474                                                                 instance)
6475 
6476         instance.power_state = current_power_state
6477         # NOTE(mriedem): The vm_state has to be set before updating the
6478         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
6479         # values cannot be nulled out until after updating the resource tracker
6480         # though.
6481         instance.vm_state = vm_states.SHELVED_OFFLOADED
6482         instance.task_state = None
6483         instance.save(expected_task_state=[task_states.SHELVING,
6484                                            task_states.SHELVING_OFFLOADING])
6485 
6486         # NOTE(ndipanov): Free resources from the resource tracker
6487         self._update_resource_tracker(context, instance)
6488 
6489         # NOTE(sfinucan): RPC calls should no longer be attempted against this
6490         # instance, so ensure any calls result in errors
6491         self._nil_out_instance_obj_host_and_node(instance)
6492         instance.save(expected_task_state=None)
6493 
6494         # TODO(melwitt): We should clean up instance console tokens here. The
6495         # instance has no host at this point and will need to establish a new
6496         # console connection in the future after it is unshelved.
6497         self._delete_scheduler_instance_info(context, instance.uuid)
6498         self._notify_about_instance_usage(context, instance,
6499                 'shelve_offload.end')
6500         compute_utils.notify_about_instance_action(context, instance,
6501                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6502                 phase=fields.NotificationPhase.END, bdms=bdms)
6503 
6504     @wrap_exception()
6505     @reverts_task_state
6506     @wrap_instance_event(prefix='compute')
6507     @wrap_instance_fault
6508     def unshelve_instance(self, context, instance, image,
6509                           filter_properties, node, request_spec=None):
6510         """Unshelve the instance.
6511 
6512         :param context: request context
6513         :param instance: a nova.objects.instance.Instance object
6514         :param image: an image to build from.  If None we assume a
6515             volume backed instance.
6516         :param filter_properties: dict containing limits, retry info etc.
6517         :param node: target compute node
6518         :param request_spec: the RequestSpec object used to schedule the
6519             instance
6520         """
6521         if filter_properties is None:
6522             filter_properties = {}
6523 
6524         @utils.synchronized(instance.uuid)
6525         def do_unshelve_instance():
6526             self._unshelve_instance(context, instance, image,
6527                                     filter_properties, node)
6528         do_unshelve_instance()
6529 
6530     def _unshelve_instance_key_scrub(self, instance):
6531         """Remove data from the instance that may cause side effects."""
6532         cleaned_keys = dict(
6533                 key_data=instance.key_data,
6534                 auto_disk_config=instance.auto_disk_config)
6535         instance.key_data = None
6536         instance.auto_disk_config = False
6537         return cleaned_keys
6538 
6539     def _unshelve_instance_key_restore(self, instance, keys):
6540         """Restore previously scrubbed keys before saving the instance."""
6541         instance.update(keys)
6542 
6543     def _unshelve_instance(self, context, instance, image, filter_properties,
6544                            node):
6545         LOG.info('Unshelving', instance=instance)
6546         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6547                 context, instance.uuid)
6548 
6549         self._notify_about_instance_usage(context, instance, 'unshelve.start')
6550         compute_utils.notify_about_instance_action(context, instance,
6551                 self.host, action=fields.NotificationAction.UNSHELVE,
6552                 phase=fields.NotificationPhase.START, bdms=bdms)
6553 
6554         instance.task_state = task_states.SPAWNING
6555         instance.save()
6556 
6557         block_device_info = self._prep_block_device(context, instance, bdms)
6558         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
6559 
6560         if node is None:
6561             node = self._get_nodename(instance)
6562 
6563         limits = filter_properties.get('limits', {})
6564 
6565         allocations = self.reportclient.get_allocations_for_consumer(
6566             context, instance.uuid)
6567 
6568         shelved_image_ref = instance.image_ref
6569         if image:
6570             instance.image_ref = image['id']
6571             image_meta = objects.ImageMeta.from_dict(image)
6572         else:
6573             image_meta = objects.ImageMeta.from_dict(
6574                 utils.get_image_from_system_metadata(
6575                     instance.system_metadata))
6576 
6577         self.network_api.setup_instance_network_on_host(context, instance,
6578                                                         self.host)
6579         network_info = self.network_api.get_instance_nw_info(context, instance)
6580         try:
6581             with self.rt.instance_claim(context, instance, node, allocations,
6582                                         limits):
6583                 self.driver.spawn(context, instance, image_meta,
6584                                   injected_files=[],
6585                                   admin_password=None,
6586                                   allocations=allocations,
6587                                   network_info=network_info,
6588                                   block_device_info=block_device_info)
6589         except Exception:
6590             with excutils.save_and_reraise_exception(logger=LOG):
6591                 LOG.exception('Instance failed to spawn',
6592                               instance=instance)
6593                 # Cleanup allocations created by the scheduler on this host
6594                 # since we failed to spawn the instance. We do this both if
6595                 # the instance claim failed with ComputeResourcesUnavailable
6596                 # or if we did claim but the spawn failed, because aborting the
6597                 # instance claim will not remove the allocations.
6598                 self.reportclient.delete_allocation_for_instance(context,
6599                                                                  instance.uuid)
6600                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
6601                 self._terminate_volume_connections(context, instance, bdms)
6602                 # The reverts_task_state decorator on unshelve_instance will
6603                 # eventually save these updates.
6604                 self._nil_out_instance_obj_host_and_node(instance)
6605 
6606         if image:
6607             instance.image_ref = shelved_image_ref
6608             self._delete_snapshot_of_shelved_instance(context, instance,
6609                                                       image['id'])
6610 
6611         self._unshelve_instance_key_restore(instance, scrubbed_keys)
6612         self._update_instance_after_spawn(context, instance)
6613         # Delete system_metadata for a shelved instance
6614         compute_utils.remove_shelved_keys_from_system_metadata(instance)
6615 
6616         instance.save(expected_task_state=task_states.SPAWNING)
6617         self._update_scheduler_instance_info(context, instance)
6618         self._notify_about_instance_usage(context, instance, 'unshelve.end')
6619         compute_utils.notify_about_instance_action(context, instance,
6620                 self.host, action=fields.NotificationAction.UNSHELVE,
6621                 phase=fields.NotificationPhase.END, bdms=bdms)
6622 
6623     @messaging.expected_exceptions(NotImplementedError)
6624     @wrap_instance_fault
6625     def reset_network(self, context, instance):
6626         """Reset networking on the given instance."""
6627         LOG.debug('Reset network', instance=instance)
6628         self.driver.reset_network(instance)
6629 
6630     def _inject_network_info(self, context, instance, network_info):
6631         """Inject network info for the given instance."""
6632         LOG.debug('Inject network info', instance=instance)
6633         LOG.debug('network_info to inject: |%s|', network_info,
6634                   instance=instance)
6635 
6636         self.driver.inject_network_info(instance,
6637                                         network_info)
6638 
6639     @wrap_instance_fault
6640     def inject_network_info(self, context, instance):
6641         """Inject network info, but don't return the info."""
6642         network_info = self.network_api.get_instance_nw_info(context, instance)
6643         self._inject_network_info(context, instance, network_info)
6644 
6645     @messaging.expected_exceptions(NotImplementedError,
6646                                    exception.ConsoleNotAvailable,
6647                                    exception.InstanceNotFound)
6648     @wrap_exception()
6649     @wrap_instance_fault
6650     def get_console_output(self, context, instance, tail_length):
6651         """Send the console output for the given instance."""
6652         context = context.elevated()
6653         LOG.info("Get console output", instance=instance)
6654         output = self.driver.get_console_output(context, instance)
6655 
6656         if type(output) is six.text_type:
6657             output = six.b(output)
6658 
6659         if tail_length is not None:
6660             output = self._tail_log(output, tail_length)
6661 
6662         return output.decode('ascii', 'replace')
6663 
6664     def _tail_log(self, log, length):
6665         try:
6666             length = int(length)
6667         except ValueError:
6668             length = 0
6669 
6670         if length == 0:
6671             return b''
6672         else:
6673             return b'\n'.join(log.split(b'\n')[-int(length):])
6674 
6675     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6676                                    exception.InstanceNotReady,
6677                                    exception.InstanceNotFound,
6678                                    exception.ConsoleTypeUnavailable,
6679                                    NotImplementedError)
6680     @wrap_exception()
6681     @wrap_instance_fault
6682     def get_vnc_console(self, context, console_type, instance):
6683         """Return connection information for a vnc console."""
6684         context = context.elevated()
6685         LOG.debug("Getting vnc console", instance=instance)
6686 
6687         if not CONF.vnc.enabled:
6688             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6689 
6690         if console_type == 'novnc':
6691             # For essex, novncproxy_base_url must include the full path
6692             # including the html file (like http://myhost/vnc_auto.html)
6693             access_url_base = CONF.vnc.novncproxy_base_url
6694         elif console_type == 'xvpvnc':
6695             access_url_base = CONF.vnc.xvpvncproxy_base_url
6696         else:
6697             raise exception.ConsoleTypeInvalid(console_type=console_type)
6698 
6699         try:
6700             # Retrieve connect info from driver, and then decorate with our
6701             # access info token
6702             console = self.driver.get_vnc_console(context, instance)
6703             console_auth = objects.ConsoleAuthToken(
6704                 context=context,
6705                 console_type=console_type,
6706                 host=console.host,
6707                 port=console.port,
6708                 internal_access_path=console.internal_access_path,
6709                 instance_uuid=instance.uuid,
6710                 access_url_base=access_url_base,
6711             )
6712             console_auth.authorize(CONF.consoleauth.token_ttl)
6713             connect_info = console.get_connection_info(
6714                 console_auth.token, console_auth.access_url)
6715 
6716         except exception.InstanceNotFound:
6717             if instance.vm_state != vm_states.BUILDING:
6718                 raise
6719             raise exception.InstanceNotReady(instance_id=instance.uuid)
6720 
6721         return connect_info
6722 
6723     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6724                                    exception.InstanceNotReady,
6725                                    exception.InstanceNotFound,
6726                                    exception.ConsoleTypeUnavailable,
6727                                    NotImplementedError)
6728     @wrap_exception()
6729     @wrap_instance_fault
6730     def get_spice_console(self, context, console_type, instance):
6731         """Return connection information for a spice console."""
6732         context = context.elevated()
6733         LOG.debug("Getting spice console", instance=instance)
6734 
6735         if not CONF.spice.enabled:
6736             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6737 
6738         if console_type != 'spice-html5':
6739             raise exception.ConsoleTypeInvalid(console_type=console_type)
6740 
6741         try:
6742             # Retrieve connect info from driver, and then decorate with our
6743             # access info token
6744             console = self.driver.get_spice_console(context, instance)
6745             console_auth = objects.ConsoleAuthToken(
6746                 context=context,
6747                 console_type=console_type,
6748                 host=console.host,
6749                 port=console.port,
6750                 internal_access_path=console.internal_access_path,
6751                 instance_uuid=instance.uuid,
6752                 access_url_base=CONF.spice.html5proxy_base_url,
6753             )
6754             console_auth.authorize(CONF.consoleauth.token_ttl)
6755             connect_info = console.get_connection_info(
6756                 console_auth.token, console_auth.access_url)
6757 
6758         except exception.InstanceNotFound:
6759             if instance.vm_state != vm_states.BUILDING:
6760                 raise
6761             raise exception.InstanceNotReady(instance_id=instance.uuid)
6762 
6763         return connect_info
6764 
6765     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6766                                    exception.InstanceNotReady,
6767                                    exception.InstanceNotFound,
6768                                    exception.ConsoleTypeUnavailable,
6769                                    NotImplementedError)
6770     @wrap_exception()
6771     @wrap_instance_fault
6772     def get_rdp_console(self, context, console_type, instance):
6773         """Return connection information for a RDP console."""
6774         context = context.elevated()
6775         LOG.debug("Getting RDP console", instance=instance)
6776 
6777         if not CONF.rdp.enabled:
6778             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6779 
6780         if console_type != 'rdp-html5':
6781             raise exception.ConsoleTypeInvalid(console_type=console_type)
6782 
6783         try:
6784             # Retrieve connect info from driver, and then decorate with our
6785             # access info token
6786             console = self.driver.get_rdp_console(context, instance)
6787             console_auth = objects.ConsoleAuthToken(
6788                 context=context,
6789                 console_type=console_type,
6790                 host=console.host,
6791                 port=console.port,
6792                 internal_access_path=console.internal_access_path,
6793                 instance_uuid=instance.uuid,
6794                 access_url_base=CONF.rdp.html5_proxy_base_url,
6795             )
6796             console_auth.authorize(CONF.consoleauth.token_ttl)
6797             connect_info = console.get_connection_info(
6798                 console_auth.token, console_auth.access_url)
6799 
6800         except exception.InstanceNotFound:
6801             if instance.vm_state != vm_states.BUILDING:
6802                 raise
6803             raise exception.InstanceNotReady(instance_id=instance.uuid)
6804 
6805         return connect_info
6806 
6807     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6808                                    exception.InstanceNotReady,
6809                                    exception.InstanceNotFound,
6810                                    exception.ConsoleTypeUnavailable,
6811                                    NotImplementedError)
6812     @wrap_exception()
6813     @wrap_instance_fault
6814     def get_mks_console(self, context, console_type, instance):
6815         """Return connection information for a MKS console."""
6816         context = context.elevated()
6817         LOG.debug("Getting MKS console", instance=instance)
6818 
6819         if not CONF.mks.enabled:
6820             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6821 
6822         if console_type != 'webmks':
6823             raise exception.ConsoleTypeInvalid(console_type=console_type)
6824 
6825         try:
6826             # Retrieve connect info from driver, and then decorate with our
6827             # access info token
6828             console = self.driver.get_mks_console(context, instance)
6829             console_auth = objects.ConsoleAuthToken(
6830                 context=context,
6831                 console_type=console_type,
6832                 host=console.host,
6833                 port=console.port,
6834                 internal_access_path=console.internal_access_path,
6835                 instance_uuid=instance.uuid,
6836                 access_url_base=CONF.mks.mksproxy_base_url,
6837             )
6838             console_auth.authorize(CONF.consoleauth.token_ttl)
6839             connect_info = console.get_connection_info(
6840                 console_auth.token, console_auth.access_url)
6841 
6842         except exception.InstanceNotFound:
6843             if instance.vm_state != vm_states.BUILDING:
6844                 raise
6845             raise exception.InstanceNotReady(instance_id=instance.uuid)
6846 
6847         return connect_info
6848 
6849     @messaging.expected_exceptions(
6850         exception.ConsoleTypeInvalid,
6851         exception.InstanceNotReady,
6852         exception.InstanceNotFound,
6853         exception.ConsoleTypeUnavailable,
6854         exception.SocketPortRangeExhaustedException,
6855         exception.ImageSerialPortNumberInvalid,
6856         exception.ImageSerialPortNumberExceedFlavorValue,
6857         NotImplementedError)
6858     @wrap_exception()
6859     @wrap_instance_fault
6860     def get_serial_console(self, context, console_type, instance):
6861         """Returns connection information for a serial console."""
6862 
6863         LOG.debug("Getting serial console", instance=instance)
6864 
6865         if not CONF.serial_console.enabled:
6866             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6867 
6868         context = context.elevated()
6869 
6870         try:
6871             # Retrieve connect info from driver, and then decorate with our
6872             # access info token
6873             console = self.driver.get_serial_console(context, instance)
6874             console_auth = objects.ConsoleAuthToken(
6875                 context=context,
6876                 console_type=console_type,
6877                 host=console.host,
6878                 port=console.port,
6879                 internal_access_path=console.internal_access_path,
6880                 instance_uuid=instance.uuid,
6881                 access_url_base=CONF.serial_console.base_url,
6882             )
6883             console_auth.authorize(CONF.consoleauth.token_ttl)
6884             connect_info = console.get_connection_info(
6885                 console_auth.token, console_auth.access_url)
6886 
6887         except exception.InstanceNotFound:
6888             if instance.vm_state != vm_states.BUILDING:
6889                 raise
6890             raise exception.InstanceNotReady(instance_id=instance.uuid)
6891 
6892         return connect_info
6893 
6894     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6895                                    exception.InstanceNotReady,
6896                                    exception.InstanceNotFound)
6897     @wrap_exception()
6898     @wrap_instance_fault
6899     def validate_console_port(self, ctxt, instance, port, console_type):
6900         if console_type == "spice-html5":
6901             console_info = self.driver.get_spice_console(ctxt, instance)
6902         elif console_type == "rdp-html5":
6903             console_info = self.driver.get_rdp_console(ctxt, instance)
6904         elif console_type == "serial":
6905             console_info = self.driver.get_serial_console(ctxt, instance)
6906         elif console_type == "webmks":
6907             console_info = self.driver.get_mks_console(ctxt, instance)
6908         else:
6909             console_info = self.driver.get_vnc_console(ctxt, instance)
6910 
6911         # Some drivers may return an int on console_info.port but the port
6912         # variable in this method is a string, so cast to be sure we are
6913         # comparing the correct types.
6914         return str(console_info.port) == port
6915 
6916     @wrap_exception()
6917     @reverts_task_state
6918     @wrap_instance_fault
6919     def reserve_block_device_name(self, context, instance, device,
6920                                   volume_id, disk_bus, device_type, tag,
6921                                   multiattach):
6922         if (tag and not
6923                 self.driver.capabilities.get('supports_tagged_attach_volume',
6924                                              False)):
6925             raise exception.VolumeTaggedAttachNotSupported()
6926 
6927         if (multiattach and not
6928                 self.driver.capabilities.get('supports_multiattach', False)):
6929             raise exception.MultiattachNotSupportedByVirtDriver(
6930                 volume_id=volume_id)
6931 
6932         @utils.synchronized(instance.uuid)
6933         def do_reserve():
6934             bdms = (
6935                 objects.BlockDeviceMappingList.get_by_instance_uuid(
6936                     context, instance.uuid))
6937 
6938             # NOTE(ndipanov): We need to explicitly set all the fields on the
6939             #                 object so that obj_load_attr does not fail
6940             new_bdm = objects.BlockDeviceMapping(
6941                     context=context,
6942                     source_type='volume', destination_type='volume',
6943                     instance_uuid=instance.uuid, boot_index=None,
6944                     volume_id=volume_id,
6945                     device_name=device, guest_format=None,
6946                     disk_bus=disk_bus, device_type=device_type, tag=tag)
6947 
6948             new_bdm.device_name = self._get_device_name_for_instance(
6949                     instance, bdms, new_bdm)
6950 
6951             # NOTE(vish): create bdm here to avoid race condition
6952             new_bdm.create()
6953             return new_bdm
6954 
6955         return do_reserve()
6956 
6957     @wrap_exception()
6958     @wrap_instance_event(prefix='compute')
6959     @wrap_instance_fault
6960     def attach_volume(self, context, instance, bdm):
6961         """Attach a volume to an instance."""
6962         driver_bdm = driver_block_device.convert_volume(bdm)
6963 
6964         @utils.synchronized(instance.uuid)
6965         def do_attach_volume(context, instance, driver_bdm):
6966             try:
6967                 return self._attach_volume(context, instance, driver_bdm)
6968             except Exception:
6969                 with excutils.save_and_reraise_exception():
6970                     bdm.destroy()
6971 
6972         do_attach_volume(context, instance, driver_bdm)
6973 
6974     def _attach_volume(self, context, instance, bdm):
6975         context = context.elevated()
6976         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
6977                  {'volume_id': bdm.volume_id,
6978                   'mountpoint': bdm['mount_device']},
6979                  instance=instance)
6980         compute_utils.notify_about_volume_attach_detach(
6981             context, instance, self.host,
6982             action=fields.NotificationAction.VOLUME_ATTACH,
6983             phase=fields.NotificationPhase.START,
6984             volume_id=bdm.volume_id)
6985         try:
6986             bdm.attach(context, instance, self.volume_api, self.driver,
6987                        do_driver_attach=True)
6988         except Exception as e:
6989             with excutils.save_and_reraise_exception():
6990                 LOG.exception("Failed to attach %(volume_id)s "
6991                               "at %(mountpoint)s",
6992                               {'volume_id': bdm.volume_id,
6993                                'mountpoint': bdm['mount_device']},
6994                               instance=instance)
6995                 if bdm['attachment_id']:
6996                     # Try to delete the attachment to make the volume
6997                     # available again. Note that DriverVolumeBlockDevice
6998                     # may have already deleted the attachment so ignore
6999                     # VolumeAttachmentNotFound.
7000                     try:
7001                         self.volume_api.attachment_delete(
7002                             context, bdm['attachment_id'])
7003                     except exception.VolumeAttachmentNotFound as exc:
7004                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
7005                                   exc, instance=instance)
7006                 else:
7007                     self.volume_api.unreserve_volume(context, bdm.volume_id)
7008                 tb = traceback.format_exc()
7009                 compute_utils.notify_about_volume_attach_detach(
7010                     context, instance, self.host,
7011                     action=fields.NotificationAction.VOLUME_ATTACH,
7012                     phase=fields.NotificationPhase.ERROR,
7013                     exception=e,
7014                     volume_id=bdm.volume_id, tb=tb)
7015 
7016         info = {'volume_id': bdm.volume_id}
7017         self._notify_about_instance_usage(
7018             context, instance, "volume.attach", extra_usage_info=info)
7019         compute_utils.notify_about_volume_attach_detach(
7020             context, instance, self.host,
7021             action=fields.NotificationAction.VOLUME_ATTACH,
7022             phase=fields.NotificationPhase.END,
7023             volume_id=bdm.volume_id)
7024 
7025     def _notify_volume_usage_detach(self, context, instance, bdm):
7026         if CONF.volume_usage_poll_interval <= 0:
7027             return
7028 
7029         mp = bdm.device_name
7030         # Handle bootable volumes which will not contain /dev/
7031         if '/dev/' in mp:
7032             mp = mp[5:]
7033         try:
7034             vol_stats = self.driver.block_stats(instance, mp)
7035             if vol_stats is None:
7036                 return
7037         except NotImplementedError:
7038             return
7039 
7040         LOG.debug("Updating volume usage cache with totals", instance=instance)
7041         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
7042         vol_usage = objects.VolumeUsage(context)
7043         vol_usage.volume_id = bdm.volume_id
7044         vol_usage.instance_uuid = instance.uuid
7045         vol_usage.project_id = instance.project_id
7046         vol_usage.user_id = instance.user_id
7047         vol_usage.availability_zone = instance.availability_zone
7048         vol_usage.curr_reads = rd_req
7049         vol_usage.curr_read_bytes = rd_bytes
7050         vol_usage.curr_writes = wr_req
7051         vol_usage.curr_write_bytes = wr_bytes
7052         vol_usage.save(update_totals=True)
7053         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
7054         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
7055 
7056     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
7057                        attachment_id=None):
7058         """Detach a volume from an instance.
7059 
7060         :param context: security context
7061         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
7062         :param instance: the Instance object to detach the volume from
7063         :param destroy_bdm: if True, the corresponding BDM entry will be marked
7064                             as deleted. Disabling this is useful for operations
7065                             like rebuild, when we don't want to destroy BDM
7066         :param attachment_id: The volume attachment_id for the given instance
7067                               and volume.
7068         """
7069         volume_id = bdm.volume_id
7070         compute_utils.notify_about_volume_attach_detach(
7071             context, instance, self.host,
7072             action=fields.NotificationAction.VOLUME_DETACH,
7073             phase=fields.NotificationPhase.START,
7074             volume_id=volume_id)
7075 
7076         self._notify_volume_usage_detach(context, instance, bdm)
7077 
7078         LOG.info('Detaching volume %(volume_id)s',
7079                  {'volume_id': volume_id}, instance=instance)
7080 
7081         driver_bdm = driver_block_device.convert_volume(bdm)
7082         driver_bdm.detach(context, instance, self.volume_api, self.driver,
7083                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
7084 
7085         info = dict(volume_id=volume_id)
7086         self._notify_about_instance_usage(
7087             context, instance, "volume.detach", extra_usage_info=info)
7088         compute_utils.notify_about_volume_attach_detach(
7089             context, instance, self.host,
7090             action=fields.NotificationAction.VOLUME_DETACH,
7091             phase=fields.NotificationPhase.END,
7092             volume_id=volume_id)
7093 
7094         if 'tag' in bdm and bdm.tag:
7095             self._delete_disk_metadata(instance, bdm)
7096         if destroy_bdm:
7097             bdm.destroy()
7098 
7099     def _delete_disk_metadata(self, instance, bdm):
7100         for device in instance.device_metadata.devices:
7101             if isinstance(device, objects.DiskMetadata):
7102                 if 'serial' in device:
7103                     if device.serial == bdm.volume_id:
7104                         instance.device_metadata.devices.remove(device)
7105                         instance.save()
7106                         break
7107                 else:
7108                     # NOTE(artom) We log the entire device object because all
7109                     # fields are nullable and may not be set
7110                     LOG.warning('Unable to determine whether to clean up '
7111                                 'device metadata for disk %s', device,
7112                                 instance=instance)
7113 
7114     @wrap_exception()
7115     @wrap_instance_event(prefix='compute')
7116     @wrap_instance_fault
7117     def detach_volume(self, context, volume_id, instance, attachment_id):
7118         """Detach a volume from an instance.
7119 
7120         :param context: security context
7121         :param volume_id: the volume id
7122         :param instance: the Instance object to detach the volume from
7123         :param attachment_id: The volume attachment_id for the given instance
7124                               and volume.
7125 
7126         """
7127         @utils.synchronized(instance.uuid)
7128         def do_detach_volume(context, volume_id, instance, attachment_id):
7129             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7130                     context, volume_id, instance.uuid)
7131             self._detach_volume(context, bdm, instance,
7132                                 attachment_id=attachment_id)
7133 
7134         do_detach_volume(context, volume_id, instance, attachment_id)
7135 
7136     def _init_volume_connection(self, context, new_volume,
7137                                 old_volume_id, connector, bdm,
7138                                 new_attachment_id, mountpoint):
7139         new_volume_id = new_volume['id']
7140         if new_attachment_id is None:
7141             # We're dealing with an old-style attachment so initialize the
7142             # connection so we can get the connection_info.
7143             new_cinfo = self.volume_api.initialize_connection(context,
7144                                                               new_volume_id,
7145                                                               connector)
7146         else:
7147             # Check for multiattach on the new volume and if True, check to
7148             # see if the virt driver supports multiattach.
7149             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
7150             # and should be consolidated into some common code at some point.
7151             vol_multiattach = new_volume.get('multiattach', False)
7152             virt_multiattach = self.driver.capabilities.get(
7153                 'supports_multiattach', False)
7154             if vol_multiattach and not virt_multiattach:
7155                 raise exception.MultiattachNotSupportedByVirtDriver(
7156                     volume_id=new_volume_id)
7157 
7158             # This is a new style attachment and the API created the new
7159             # volume attachment and passed the id to the compute over RPC.
7160             # At this point we need to update the new volume attachment with
7161             # the host connector, which will give us back the new attachment
7162             # connection_info.
7163             new_cinfo = self.volume_api.attachment_update(
7164                 context, new_attachment_id, connector,
7165                 mountpoint)['connection_info']
7166 
7167             if vol_multiattach:
7168                 # This will be used by the volume driver to determine the
7169                 # proper disk configuration.
7170                 new_cinfo['multiattach'] = True
7171 
7172         old_cinfo = jsonutils.loads(bdm['connection_info'])
7173         if old_cinfo and 'serial' not in old_cinfo:
7174             old_cinfo['serial'] = old_volume_id
7175         # NOTE(lyarwood): serial is not always present in the returned
7176         # connection_info so set it if it is missing as we do in
7177         # DriverVolumeBlockDevice.attach().
7178         if 'serial' not in new_cinfo:
7179             new_cinfo['serial'] = new_volume_id
7180         return (old_cinfo, new_cinfo)
7181 
7182     def _swap_volume(self, context, instance, bdm, connector,
7183                      old_volume_id, new_volume, resize_to,
7184                      new_attachment_id, is_cinder_migration):
7185         new_volume_id = new_volume['id']
7186         mountpoint = bdm['device_name']
7187         failed = False
7188         new_cinfo = None
7189         try:
7190             old_cinfo, new_cinfo = self._init_volume_connection(
7191                 context, new_volume, old_volume_id, connector,
7192                 bdm, new_attachment_id, mountpoint)
7193             # NOTE(lyarwood): The Libvirt driver, the only virt driver
7194             # currently implementing swap_volume, will modify the contents of
7195             # new_cinfo when connect_volume is called. This is then saved to
7196             # the BDM in swap_volume for future use outside of this flow.
7197             msg = ("swap_volume: Calling driver volume swap with "
7198                    "connection infos: new: %(new_cinfo)s; "
7199                    "old: %(old_cinfo)s" %
7200                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
7201             # Both new and old info might contain password
7202             LOG.debug(strutils.mask_password(msg), instance=instance)
7203 
7204             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
7205                                     mountpoint, resize_to)
7206             if new_attachment_id:
7207                 self.volume_api.attachment_complete(context, new_attachment_id)
7208             msg = ("swap_volume: Driver volume swap returned, new "
7209                    "connection_info is now : %(new_cinfo)s" %
7210                    {'new_cinfo': new_cinfo})
7211             LOG.debug(strutils.mask_password(msg))
7212         except Exception as ex:
7213             failed = True
7214             with excutils.save_and_reraise_exception():
7215                 tb = traceback.format_exc()
7216                 compute_utils.notify_about_volume_swap(
7217                     context, instance, self.host,
7218                     fields.NotificationPhase.ERROR,
7219                     old_volume_id, new_volume_id, ex, tb)
7220                 if new_cinfo:
7221                     msg = ("Failed to swap volume %(old_volume_id)s "
7222                            "for %(new_volume_id)s")
7223                     LOG.exception(msg, {'old_volume_id': old_volume_id,
7224                                         'new_volume_id': new_volume_id},
7225                                   instance=instance)
7226                 else:
7227                     msg = ("Failed to connect to volume %(volume_id)s "
7228                            "with volume at %(mountpoint)s")
7229                     LOG.exception(msg, {'volume_id': new_volume_id,
7230                                         'mountpoint': bdm['device_name']},
7231                                   instance=instance)
7232 
7233                 # The API marked the volume as 'detaching' for the old volume
7234                 # so we need to roll that back so the volume goes back to
7235                 # 'in-use' state.
7236                 self.volume_api.roll_detaching(context, old_volume_id)
7237 
7238                 if new_attachment_id is None:
7239                     # The API reserved the new volume so it would be in
7240                     # 'attaching' status, so we need to unreserve it so it
7241                     # goes back to 'available' status.
7242                     self.volume_api.unreserve_volume(context, new_volume_id)
7243                 else:
7244                     # This is a new style attachment for the new volume, which
7245                     # was created in the API. We just need to delete it here
7246                     # to put the new volume back into 'available' status.
7247                     self.volume_api.attachment_delete(
7248                         context, new_attachment_id)
7249         finally:
7250             # TODO(mriedem): This finally block is terribly confusing and is
7251             # trying to do too much. We should consider removing the finally
7252             # block and move whatever needs to happen on success and failure
7253             # into the blocks above for clarity, even if it means a bit of
7254             # redundant code.
7255             conn_volume = new_volume_id if failed else old_volume_id
7256             if new_cinfo:
7257                 LOG.debug("swap_volume: removing Cinder connection "
7258                           "for volume %(volume)s", {'volume': conn_volume},
7259                           instance=instance)
7260                 if bdm.attachment_id is None:
7261                     # This is the pre-3.44 flow for new-style volume
7262                     # attachments so just terminate the connection.
7263                     self.volume_api.terminate_connection(context,
7264                                                          conn_volume,
7265                                                          connector)
7266                 else:
7267                     # This is a new style volume attachment. If we failed, then
7268                     # the new attachment was already deleted above in the
7269                     # exception block and we have nothing more to do here. If
7270                     # swap_volume was successful in the driver, then we need to
7271                     # "detach" the original attachment by deleting it.
7272                     if not failed:
7273                         self.volume_api.attachment_delete(
7274                             context, bdm.attachment_id)
7275 
7276             # Need to make some decisions based on whether this was
7277             # a Cinder initiated migration or not. The callback to
7278             # migration completion isn't needed in the case of a
7279             # nova initiated simple swap of two volume
7280             # "volume-update" call so skip that. The new attachment
7281             # scenarios will give us a new attachment record and
7282             # that's what we want.
7283             if bdm.attachment_id and not is_cinder_migration:
7284                 # we don't callback to cinder
7285                 comp_ret = {'save_volume_id': new_volume_id}
7286             else:
7287                 # NOTE(lyarwood): The following call to
7288                 # os-migrate-volume-completion returns a dict containing
7289                 # save_volume_id, this volume id has two possible values :
7290                 # 1. old_volume_id if we are migrating (retyping) volumes
7291                 # 2. new_volume_id if we are swapping between two existing
7292                 #    volumes
7293                 # This volume id is later used to update the volume_id and
7294                 # connection_info['serial'] of the BDM.
7295                 comp_ret = self.volume_api.migrate_volume_completion(
7296                                                           context,
7297                                                           old_volume_id,
7298                                                           new_volume_id,
7299                                                           error=failed)
7300                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
7301                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
7302                           instance=instance)
7303 
7304         return (comp_ret, new_cinfo)
7305 
7306     @wrap_exception()
7307     @wrap_instance_event(prefix='compute')
7308     @wrap_instance_fault
7309     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
7310                     new_attachment_id):
7311         """Swap volume for an instance."""
7312         context = context.elevated()
7313 
7314         compute_utils.notify_about_volume_swap(
7315             context, instance, self.host,
7316             fields.NotificationPhase.START,
7317             old_volume_id, new_volume_id)
7318 
7319         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7320                 context, old_volume_id, instance.uuid)
7321         connector = self.driver.get_volume_connector(instance)
7322 
7323         resize_to = 0
7324         old_volume = self.volume_api.get(context, old_volume_id)
7325         # Yes this is a tightly-coupled state check of what's going on inside
7326         # cinder, but we need this while we still support old (v1/v2) and
7327         # new style attachments (v3.44). Once we drop support for old style
7328         # attachments we could think about cleaning up the cinder-initiated
7329         # swap volume API flows.
7330         is_cinder_migration = False
7331         if 'migration_status' in old_volume:
7332             is_cinder_migration = old_volume['migration_status'] == 'migrating'
7333         old_vol_size = old_volume['size']
7334         new_volume = self.volume_api.get(context, new_volume_id)
7335         new_vol_size = new_volume['size']
7336         if new_vol_size > old_vol_size:
7337             resize_to = new_vol_size
7338 
7339         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
7340                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
7341                  instance=instance)
7342         comp_ret, new_cinfo = self._swap_volume(context,
7343                                                 instance,
7344                                                 bdm,
7345                                                 connector,
7346                                                 old_volume_id,
7347                                                 new_volume,
7348                                                 resize_to,
7349                                                 new_attachment_id,
7350                                                 is_cinder_migration)
7351 
7352         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
7353         # correct volume_id returned by Cinder.
7354         save_volume_id = comp_ret['save_volume_id']
7355         new_cinfo['serial'] = save_volume_id
7356         values = {
7357             'connection_info': jsonutils.dumps(new_cinfo),
7358             'source_type': 'volume',
7359             'destination_type': 'volume',
7360             'snapshot_id': None,
7361             'volume_id': save_volume_id,
7362             'no_device': None}
7363 
7364         if resize_to:
7365             values['volume_size'] = resize_to
7366 
7367         if new_attachment_id is not None:
7368             # This was a volume swap for a new-style attachment so we
7369             # need to update the BDM attachment_id for the new attachment.
7370             values['attachment_id'] = new_attachment_id
7371 
7372         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
7373                   "%(updates)s", {'volume_id': bdm.volume_id,
7374                                   'updates': values},
7375                   instance=instance)
7376         bdm.update(values)
7377         bdm.save()
7378 
7379         compute_utils.notify_about_volume_swap(
7380             context, instance, self.host,
7381             fields.NotificationPhase.END,
7382             old_volume_id, new_volume_id)
7383 
7384     @wrap_exception()
7385     def remove_volume_connection(self, context, volume_id, instance):
7386         """Remove the volume connection on this host
7387 
7388         Detach the volume from this instance on this host, and if this is
7389         the cinder v2 flow, call cinder to terminate the connection.
7390         """
7391         try:
7392             # NOTE(mriedem): If the BDM was just passed directly we would not
7393             # need to do this DB query, but this is an RPC interface so
7394             # changing that requires some care.
7395             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7396                     context, volume_id, instance.uuid)
7397             # NOTE(mriedem): Normally we would pass delete_attachment=True to
7398             # _remove_volume_connection to delete a v3 style volume attachment,
7399             # but this method is RPC called from _rollback_live_migration which
7400             # already deletes the attachment, so because of that tight coupling
7401             # we cannot simply delete a v3 style attachment here without
7402             # needing to do some behavior modification of that
7403             # _rollback_live_migration flow which gets messy.
7404             self._remove_volume_connection(context, bdm, instance)
7405         except exception.NotFound:
7406             pass
7407 
7408     def _remove_volume_connection(self, context, bdm, instance,
7409                                   delete_attachment=False):
7410         """Remove the volume connection on this host
7411 
7412         Detach the volume from this instance on this host.
7413 
7414         :param context: nova auth request context
7415         :param bdm: BlockDeviceMapping object for a volume attached to the
7416             instance
7417         :param instance: Instance object with a volume attached represented
7418             by ``bdm``
7419         :param delete_attachment: If ``bdm.attachment_id`` is not None the
7420             attachment was made as a cinder v3 style attachment and if True,
7421             then deletes the volume attachment, otherwise just terminates
7422             the connection for a cinder legacy style connection.
7423         """
7424         driver_bdm = driver_block_device.convert_volume(bdm)
7425         driver_bdm.driver_detach(context, instance,
7426                                  self.volume_api, self.driver)
7427         if bdm.attachment_id is None:
7428             # cinder v2 api flow
7429             connector = self.driver.get_volume_connector(instance)
7430             self.volume_api.terminate_connection(context, bdm.volume_id,
7431                                                  connector)
7432         elif delete_attachment:
7433             # cinder v3 api flow
7434             self.volume_api.attachment_delete(context, bdm.attachment_id)
7435 
7436     def _deallocate_port_for_instance(self, context, instance, port_id,
7437                                       raise_on_failure=False):
7438         try:
7439             result = self.network_api.deallocate_port_for_instance(
7440                 context, instance, port_id)
7441             __, port_allocation = result
7442         except Exception as ex:
7443             with excutils.save_and_reraise_exception(
7444                     reraise=raise_on_failure):
7445                 LOG.warning('Failed to deallocate port %(port_id)s '
7446                             'for instance. Error: %(error)s',
7447                             {'port_id': port_id, 'error': ex},
7448                             instance=instance)
7449         else:
7450             if port_allocation:
7451                 # Deallocate the resources in placement that were used by the
7452                 # detached port.
7453                 try:
7454                     client = self.reportclient
7455                     client.remove_resources_from_instance_allocation(
7456                         context, instance.uuid, port_allocation)
7457                 except Exception as ex:
7458                     # We always raise here as it is not a race condition where
7459                     # somebody has already deleted the port we want to cleanup.
7460                     # Here we see that the port exists, the allocation exists,
7461                     # but we cannot clean it up so we will actually leak
7462                     # allocations.
7463                     with excutils.save_and_reraise_exception():
7464                         LOG.warning('Failed to remove resource allocation '
7465                                     'of port %(port_id)s for instance. Error: '
7466                                     '%(error)s',
7467                                     {'port_id': port_id, 'error': ex},
7468                                     instance=instance)
7469 
7470     # TODO(mriedem): There are likely race failures which can result in
7471     # NotFound and QuotaError exceptions getting traced as well.
7472     @messaging.expected_exceptions(
7473         # Do not log a traceback for user errors. We use Invalid generically
7474         # since this method can raise lots of different exceptions:
7475         # AttachInterfaceNotSupported
7476         # NetworkInterfaceTaggedAttachNotSupported
7477         # NetworkAmbiguous
7478         # PortNotUsable
7479         # PortInUse
7480         # PortNotUsableDNS
7481         # AttachSRIOVPortNotSupported
7482         # NetworksWithQoSPolicyNotSupported
7483         exception.Invalid)
7484     @wrap_exception()
7485     @wrap_instance_event(prefix='compute')
7486     @wrap_instance_fault
7487     def attach_interface(self, context, instance, network_id, port_id,
7488                          requested_ip, tag):
7489         """Use hotplug to add an network adapter to an instance."""
7490         if not self.driver.capabilities.get('supports_attach_interface',
7491                                             False):
7492             raise exception.AttachInterfaceNotSupported(
7493                 instance_uuid=instance.uuid)
7494         if (tag and not
7495             self.driver.capabilities.get('supports_tagged_attach_interface',
7496                                          False)):
7497             raise exception.NetworkInterfaceTaggedAttachNotSupported()
7498 
7499         compute_utils.notify_about_instance_action(
7500             context, instance, self.host,
7501             action=fields.NotificationAction.INTERFACE_ATTACH,
7502             phase=fields.NotificationPhase.START)
7503 
7504         bind_host_id = self.driver.network_binding_host_id(context, instance)
7505         network_info = self.network_api.allocate_port_for_instance(
7506             context, instance, port_id, network_id, requested_ip,
7507             bind_host_id=bind_host_id, tag=tag)
7508         if len(network_info) != 1:
7509             LOG.error('allocate_port_for_instance returned %(ports)s '
7510                       'ports', {'ports': len(network_info)})
7511             # TODO(elod.illes): an instance.interface_attach.error notification
7512             # should be sent here
7513             raise exception.InterfaceAttachFailed(
7514                     instance_uuid=instance.uuid)
7515         image_meta = objects.ImageMeta.from_instance(instance)
7516 
7517         try:
7518             self.driver.attach_interface(context, instance, image_meta,
7519                                          network_info[0])
7520         except exception.NovaException as ex:
7521             port_id = network_info[0].get('id')
7522             LOG.warning("attach interface failed , try to deallocate "
7523                         "port %(port_id)s, reason: %(msg)s",
7524                         {'port_id': port_id, 'msg': ex},
7525                         instance=instance)
7526             self._deallocate_port_for_instance(context, instance, port_id)
7527 
7528             tb = traceback.format_exc()
7529             compute_utils.notify_about_instance_action(
7530                 context, instance, self.host,
7531                 action=fields.NotificationAction.INTERFACE_ATTACH,
7532                 phase=fields.NotificationPhase.ERROR,
7533                 exception=ex, tb=tb)
7534 
7535             raise exception.InterfaceAttachFailed(
7536                 instance_uuid=instance.uuid)
7537 
7538         compute_utils.notify_about_instance_action(
7539             context, instance, self.host,
7540             action=fields.NotificationAction.INTERFACE_ATTACH,
7541             phase=fields.NotificationPhase.END)
7542 
7543         return network_info[0]
7544 
7545     @wrap_exception()
7546     @wrap_instance_event(prefix='compute')
7547     @wrap_instance_fault
7548     def detach_interface(self, context, instance, port_id):
7549         """Detach a network adapter from an instance."""
7550         network_info = instance.info_cache.network_info
7551         condemned = None
7552         for vif in network_info:
7553             if vif['id'] == port_id:
7554                 condemned = vif
7555                 break
7556         if condemned is None:
7557             raise exception.PortNotFound(_("Port %s is not "
7558                                            "attached") % port_id)
7559 
7560         compute_utils.notify_about_instance_action(
7561             context, instance, self.host,
7562             action=fields.NotificationAction.INTERFACE_DETACH,
7563             phase=fields.NotificationPhase.START)
7564 
7565         try:
7566             self.driver.detach_interface(context, instance, condemned)
7567         except exception.NovaException as ex:
7568             # If the instance was deleted before the interface was detached,
7569             # just log it at debug.
7570             log_level = (logging.DEBUG
7571                          if isinstance(ex, exception.InstanceNotFound)
7572                          else logging.WARNING)
7573             LOG.log(log_level,
7574                     "Detach interface failed, port_id=%(port_id)s, reason: "
7575                     "%(msg)s", {'port_id': port_id, 'msg': ex},
7576                     instance=instance)
7577             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
7578         else:
7579             self._deallocate_port_for_instance(
7580                 context, instance, port_id, raise_on_failure=True)
7581 
7582         compute_utils.notify_about_instance_action(
7583             context, instance, self.host,
7584             action=fields.NotificationAction.INTERFACE_DETACH,
7585             phase=fields.NotificationPhase.END)
7586 
7587     def _get_compute_info(self, context, host):
7588         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
7589             context, host)
7590 
7591     @wrap_exception()
7592     def check_instance_shared_storage(self, ctxt, instance, data):
7593         """Check if the instance files are shared
7594 
7595         :param ctxt: security context
7596         :param instance: dict of instance data
7597         :param data: result of driver.check_instance_shared_storage_local
7598 
7599         Returns True if instance disks located on shared storage and
7600         False otherwise.
7601         """
7602         return self.driver.check_instance_shared_storage_remote(ctxt, data)
7603 
7604     def _dest_can_numa_live_migrate(self, dest_check_data, migration):
7605         # TODO(artom) If we have a libvirt driver we expect it to set
7606         # dst_supports_numa_live_migration, but we have to remove it if we
7607         # did not get a migration from the conductor, indicating that it
7608         # cannot send RPC 5.3. This check can be removed in RPC 6.0.
7609         if ('dst_supports_numa_live_migration' in dest_check_data and
7610                 dest_check_data.dst_supports_numa_live_migration and
7611                 not migration):
7612             delattr(dest_check_data, 'dst_supports_numa_live_migration')
7613         return dest_check_data
7614 
7615     @wrap_exception()
7616     @wrap_instance_event(prefix='compute')
7617     @wrap_instance_fault
7618     def check_can_live_migrate_destination(self, ctxt, instance,
7619                                            block_migration, disk_over_commit,
7620                                            migration=None, limits=None,
7621                                            request_spec=None):
7622         """Check if it is possible to execute live migration.
7623 
7624         This runs checks on the destination host, and then calls
7625         back to the source host to check the results.
7626 
7627         :param context: security context
7628         :param instance: dict of instance data
7629         :param block_migration: if true, prepare for block migration
7630                                 if None, calculate it in driver
7631         :param disk_over_commit: if true, allow disk over commit
7632                                  if None, ignore disk usage checking
7633         :param migration: objects.Migration object for this live migration.
7634         :param limits: objects.SchedulerLimits object for this live migration.
7635         :param request_spec: A RequestSpec object
7636         :returns: a LiveMigrateData object (hypervisor-dependent)
7637         """
7638         src_compute_info = obj_base.obj_to_primitive(
7639             self._get_compute_info(ctxt, instance.host))
7640         dst_compute_info = obj_base.obj_to_primitive(
7641             self._get_compute_info(ctxt, CONF.host))
7642 
7643         if request_spec:
7644             hints = self._get_scheduler_hints({}, request_spec)
7645             self._validate_instance_group_policy(
7646                 ctxt, instance, hints, dst_compute_info['hypervisor_hostname'])
7647 
7648         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
7649             instance, src_compute_info, dst_compute_info,
7650             block_migration, disk_over_commit)
7651         dest_check_data = self._dest_can_numa_live_migrate(dest_check_data,
7652                                                            migration)
7653         LOG.debug('destination check data is %s', dest_check_data)
7654         try:
7655             migrate_data = self.compute_rpcapi.check_can_live_migrate_source(
7656                 ctxt, instance, dest_check_data)
7657             if ('src_supports_numa_live_migration' in migrate_data and
7658                     migrate_data.src_supports_numa_live_migration):
7659                 migrate_data = self._live_migration_claim(
7660                     ctxt, instance, migrate_data, migration, limits)
7661             elif 'dst_supports_numa_live_migration' in dest_check_data:
7662                 LOG.info('Destination was ready for NUMA live migration, '
7663                          'but source is either too old, or is set to an '
7664                          'older upgrade level.', instance=instance)
7665             # Create migrate_data vifs
7666             migrate_data.vifs = \
7667                 migrate_data_obj.VIFMigrateData.create_skeleton_migrate_vifs(
7668                     instance.get_network_info())
7669             # Claim PCI devices for VIFs on destination (if needed)
7670             port_id_to_pci = self._claim_pci_for_instance_vifs(ctxt, instance)
7671             # Update migrate VIFs with the newly claimed PCI devices
7672             self._update_migrate_vifs_profile_with_pci(migrate_data.vifs,
7673                                                        port_id_to_pci)
7674         finally:
7675             self.driver.cleanup_live_migration_destination_check(ctxt,
7676                     dest_check_data)
7677         return migrate_data
7678 
7679     def _live_migration_claim(self, ctxt, instance, migrate_data,
7680                               migration, limits):
7681         """Runs on the destination and does a resources claim, if necessary.
7682         Currently, only NUMA live migrations require it.
7683 
7684         :param ctxt: Request context
7685         :param instance: The Instance being live migrated
7686         :param migrate_data: The MigrateData object for this live migration
7687         :param migration: The Migration object for this live migration
7688         :param limits: The SchedulerLimits object for this live migration
7689         :returns: migrate_data with dst_numa_info set if necessary
7690         """
7691         try:
7692             # NOTE(artom) We might have gotten here from _find_destination() in
7693             # the conductor live migrate task. At that point,
7694             # migration.dest_node is not set yet (nor should it be, we're still
7695             # looking for a destination, after all). Therefore, we cannot use
7696             # migration.dest_node here and must use self._get_nodename().
7697             claim = self.rt.live_migration_claim(
7698                 ctxt, instance, self._get_nodename(instance), migration,
7699                 limits)
7700             LOG.debug('Created live migration claim.', instance=instance)
7701         except exception.ComputeResourcesUnavailable as e:
7702             raise exception.MigrationPreCheckError(
7703                 reason=e.format_message())
7704         return self.driver.post_claim_migrate_data(ctxt, instance,
7705                                                    migrate_data, claim)
7706 
7707     def _source_can_numa_live_migrate(self, ctxt, dest_check_data,
7708                                       source_check_data):
7709         # TODO(artom) Our virt driver may have told us that it supports NUMA
7710         # live migration. However, the following other conditions must be met
7711         # for a NUMA live migration to happen:
7712         # 1. We got a True dst_supports_numa_live_migration in
7713         #    dest_check_data, indicating that the dest virt driver supports
7714         #    NUMA live migration and that the conductor can send RPC 5.3 and
7715         #    that the destination compute manager can receive it.
7716         # 2. Ourselves, the source, can send RPC 5.3. There's no
7717         #    sentinel/parameter for this, so we just ask our rpcapi directly.
7718         # If any of these are not met, we need to remove the
7719         # src_supports_numa_live_migration flag from source_check_data to avoid
7720         # incorrectly initiating a NUMA live migration.
7721         # All of this can be removed in RPC 6.0/objects 2.0.
7722         can_numa_live_migrate = (
7723             'dst_supports_numa_live_migration' in dest_check_data and
7724             dest_check_data.dst_supports_numa_live_migration and
7725             self.compute_rpcapi.supports_numa_live_migration(ctxt))
7726         if ('src_supports_numa_live_migration' in source_check_data and
7727                 source_check_data.src_supports_numa_live_migration and
7728                 not can_numa_live_migrate):
7729             delattr(source_check_data, 'src_supports_numa_live_migration')
7730         return source_check_data
7731 
7732     @wrap_exception()
7733     @wrap_instance_event(prefix='compute')
7734     @wrap_instance_fault
7735     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
7736         """Check if it is possible to execute live migration.
7737 
7738         This checks if the live migration can succeed, based on the
7739         results from check_can_live_migrate_destination.
7740 
7741         :param ctxt: security context
7742         :param instance: dict of instance data
7743         :param dest_check_data: result of check_can_live_migrate_destination
7744         :returns: a LiveMigrateData object
7745         """
7746         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7747             ctxt, instance.uuid)
7748         is_volume_backed = compute_utils.is_volume_backed_instance(
7749             ctxt, instance, bdms)
7750         dest_check_data.is_volume_backed = is_volume_backed
7751         block_device_info = self._get_instance_block_device_info(
7752                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
7753         result = self.driver.check_can_live_migrate_source(ctxt, instance,
7754                                                            dest_check_data,
7755                                                            block_device_info)
7756         result = self._source_can_numa_live_migrate(ctxt, dest_check_data,
7757                                                     result)
7758         LOG.debug('source check data is %s', result)
7759         return result
7760 
7761     # TODO(mriedem): Remove the block_migration argument in v6.0 of the compute
7762     # RPC API.
7763     @wrap_exception()
7764     @wrap_instance_event(prefix='compute')
7765     @wrap_instance_fault
7766     def pre_live_migration(self, context, instance, block_migration, disk,
7767                            migrate_data):
7768         """Preparations for live migration at dest host.
7769 
7770         :param context: security context
7771         :param instance: dict of instance data
7772         :param block_migration: if true, prepare for block migration
7773         :param disk: disk info of instance
7774         :param migrate_data: A dict or LiveMigrateData object holding data
7775                              required for live migration without shared
7776                              storage.
7777         :returns: migrate_data containing additional migration info
7778         """
7779         LOG.debug('pre_live_migration data is %s', migrate_data)
7780 
7781         migrate_data.old_vol_attachment_ids = {}
7782         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7783             context, instance.uuid)
7784         network_info = self.network_api.get_instance_nw_info(context, instance)
7785         self._notify_about_instance_usage(
7786             context, instance, "live_migration.pre.start",
7787             network_info=network_info)
7788         compute_utils.notify_about_instance_action(
7789             context, instance, self.host,
7790             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7791             phase=fields.NotificationPhase.START, bdms=bdms)
7792 
7793         connector = self.driver.get_volume_connector(instance)
7794         try:
7795             for bdm in bdms:
7796                 if bdm.is_volume and bdm.attachment_id is not None:
7797                     # This bdm uses the new cinder v3.44 API.
7798                     # We will create a new attachment for this
7799                     # volume on this migration destination host. The old
7800                     # attachment will be deleted on the source host
7801                     # when the migration succeeds. The old attachment_id
7802                     # is stored in dict with the key being the bdm.volume_id
7803                     # so it can be restored on rollback.
7804                     #
7805                     # Also note that attachment_update is not needed as we
7806                     # are providing the connector in the create call.
7807                     attach_ref = self.volume_api.attachment_create(
7808                         context, bdm.volume_id, bdm.instance_uuid,
7809                         connector=connector, mountpoint=bdm.device_name)
7810 
7811                     # save current attachment so we can detach it on success,
7812                     # or restore it on a rollback.
7813                     # NOTE(mdbooth): This data is no longer used by the source
7814                     # host since change Ibe9215c0. We can't remove it until we
7815                     # are sure the source host has been upgraded.
7816                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
7817                         bdm.attachment_id
7818 
7819                     # update the bdm with the new attachment_id.
7820                     bdm.attachment_id = attach_ref['id']
7821                     bdm.save()
7822 
7823             block_device_info = self._get_instance_block_device_info(
7824                                 context, instance, refresh_conn_info=True,
7825                                 bdms=bdms)
7826 
7827             # The driver pre_live_migration will plug vifs on the host. We call
7828             # plug_vifs before calling ensure_filtering_rules_for_instance, to
7829             # ensure bridge is set up.
7830             migrate_data = self.driver.pre_live_migration(context,
7831                                            instance,
7832                                            block_device_info,
7833                                            network_info,
7834                                            disk,
7835                                            migrate_data)
7836             LOG.debug('driver pre_live_migration data is %s', migrate_data)
7837             # driver.pre_live_migration is what plugs vifs on the destination
7838             # host so now we can set the wait_for_vif_plugged flag in the
7839             # migrate_data object which the source compute will use to
7840             # determine if it should wait for a 'network-vif-plugged' event
7841             # from neutron before starting the actual guest transfer in the
7842             # hypervisor
7843             migrate_data.wait_for_vif_plugged = (
7844                 CONF.compute.live_migration_wait_for_vif_plug)
7845 
7846             # NOTE(tr3buchet): setup networks on destination host
7847             self.network_api.setup_networks_on_host(context, instance,
7848                                                              self.host)
7849 
7850             # Creating filters to hypervisors and firewalls.
7851             # An example is that nova-instance-instance-xxx,
7852             # which is written to libvirt.xml(Check "virsh nwfilter-list")
7853             # This nwfilter is necessary on the destination host.
7854             # In addition, this method is creating filtering rule
7855             # onto destination host.
7856             self.driver.ensure_filtering_rules_for_instance(instance,
7857                                                 network_info)
7858         except Exception:
7859             # If we raise, migrate_data with the updated attachment ids
7860             # will not be returned to the source host for rollback.
7861             # So we need to rollback new attachments here.
7862             with excutils.save_and_reraise_exception():
7863                 old_attachments = migrate_data.old_vol_attachment_ids
7864                 for bdm in bdms:
7865                     if (bdm.is_volume and bdm.attachment_id is not None and
7866                             bdm.volume_id in old_attachments):
7867                         self.volume_api.attachment_delete(context,
7868                                                           bdm.attachment_id)
7869                         bdm.attachment_id = old_attachments[bdm.volume_id]
7870                         bdm.save()
7871 
7872         # Volume connections are complete, tell cinder that all the
7873         # attachments have completed.
7874         for bdm in bdms:
7875             if bdm.is_volume and bdm.attachment_id is not None:
7876                 self.volume_api.attachment_complete(context,
7877                                                     bdm.attachment_id)
7878 
7879         self._notify_about_instance_usage(
7880                      context, instance, "live_migration.pre.end",
7881                      network_info=network_info)
7882         compute_utils.notify_about_instance_action(
7883             context, instance, self.host,
7884             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7885             phase=fields.NotificationPhase.END, bdms=bdms)
7886 
7887         LOG.debug('pre_live_migration result data is %s', migrate_data)
7888         return migrate_data
7889 
7890     @staticmethod
7891     def _neutron_failed_migration_callback(event_name, instance):
7892         msg = ('Neutron reported failure during migration '
7893                'with %(event)s for instance %(uuid)s')
7894         msg_args = {'event': event_name, 'uuid': instance.uuid}
7895         if CONF.vif_plugging_is_fatal:
7896             raise exception.VirtualInterfacePlugException(msg % msg_args)
7897         LOG.error(msg, msg_args)
7898 
7899     @staticmethod
7900     def _get_neutron_events_for_live_migration(instance):
7901         # We don't generate events if CONF.vif_plugging_timeout=0
7902         # meaning that the operator disabled using them.
7903         if CONF.vif_plugging_timeout and utils.is_neutron():
7904             return [('network-vif-plugged', vif['id'])
7905                     for vif in instance.get_network_info()]
7906         else:
7907             return []
7908 
7909     def _cleanup_pre_live_migration(self, context, dest, instance,
7910                                     migration, migrate_data, source_bdms):
7911         """Helper method for when pre_live_migration fails
7912 
7913         Sets the migration status to "error" and rolls back the live migration
7914         setup on the destination host.
7915 
7916         :param context: The user request context.
7917         :type context: nova.context.RequestContext
7918         :param dest: The live migration destination hostname.
7919         :type dest: str
7920         :param instance: The instance being live migrated.
7921         :type instance: nova.objects.Instance
7922         :param migration: The migration record tracking this live migration.
7923         :type migration: nova.objects.Migration
7924         :param migrate_data: Data about the live migration, populated from
7925                              the destination host.
7926         :type migrate_data: Subclass of nova.objects.LiveMigrateData
7927         :param source_bdms: BDMs prior to modification by the destination
7928                             compute host. Set by _do_live_migration and not
7929                             part of the callback interface, so this is never
7930                             None
7931         """
7932         self._set_migration_status(migration, 'error')
7933         # Make sure we set this for _rollback_live_migration()
7934         # so it can find it, as expected if it was called later
7935         migrate_data.migration = migration
7936         self._rollback_live_migration(context, instance, dest,
7937                                       migrate_data=migrate_data,
7938                                       source_bdms=source_bdms)
7939 
7940     def _do_pre_live_migration_from_source(self, context, dest, instance,
7941                                            block_migration, migration,
7942                                            migrate_data, source_bdms):
7943         """Prepares for pre-live-migration on the source host and calls dest
7944 
7945         Will setup a callback networking event handler (if configured) and
7946         then call the dest host's pre_live_migration method to prepare the
7947         dest host for live migration (plugs vifs, connect volumes, etc).
7948 
7949         _rollback_live_migration (on the source) will be called if
7950         pre_live_migration (on the dest) fails.
7951 
7952         :param context: nova auth request context for this operation
7953         :param dest: name of the destination compute service host
7954         :param instance: Instance object being live migrated
7955         :param block_migration: If true, prepare for block migration.
7956         :param migration: Migration object tracking this operation
7957         :param migrate_data: MigrateData object for this operation populated
7958             by the destination host compute driver as part of the
7959             check_can_live_migrate_destination call.
7960         :param source_bdms: BlockDeviceMappingList of BDMs currently attached
7961             to the instance from the source host.
7962         :returns: MigrateData object which is a modified version of the
7963             ``migrate_data`` argument from the compute driver on the dest
7964             host during the ``pre_live_migration`` call.
7965         :raises: MigrationError if waiting for the network-vif-plugged event
7966             timed out and is fatal.
7967         """
7968         class _BreakWaitForInstanceEvent(Exception):
7969             """Used as a signal to stop waiting for the network-vif-plugged
7970             event when we discover that
7971             [compute]/live_migration_wait_for_vif_plug is not set on the
7972             destination.
7973             """
7974             pass
7975 
7976         events = self._get_neutron_events_for_live_migration(instance)
7977         try:
7978             if ('block_migration' in migrate_data and
7979                     migrate_data.block_migration):
7980                 block_device_info = self._get_instance_block_device_info(
7981                     context, instance, bdms=source_bdms)
7982                 disk = self.driver.get_instance_disk_info(
7983                     instance, block_device_info=block_device_info)
7984             else:
7985                 disk = None
7986 
7987             deadline = CONF.vif_plugging_timeout
7988             error_cb = self._neutron_failed_migration_callback
7989             # In order to avoid a race with the vif plugging that the virt
7990             # driver does on the destination host, we register our events
7991             # to wait for before calling pre_live_migration. Then if the
7992             # dest host reports back that we shouldn't wait, we can break
7993             # out of the context manager using _BreakWaitForInstanceEvent.
7994             with self.virtapi.wait_for_instance_event(
7995                     instance, events, deadline=deadline,
7996                     error_callback=error_cb):
7997                 with timeutils.StopWatch() as timer:
7998                     # TODO(mriedem): The "block_migration" parameter passed
7999                     # here is not actually used in pre_live_migration but it
8000                     # is not optional in the RPC interface either.
8001                     migrate_data = self.compute_rpcapi.pre_live_migration(
8002                         context, instance,
8003                         block_migration, disk, dest, migrate_data)
8004                 LOG.info('Took %0.2f seconds for pre_live_migration on '
8005                          'destination host %s.',
8006                          timer.elapsed(), dest, instance=instance)
8007                 wait_for_vif_plugged = (
8008                     'wait_for_vif_plugged' in migrate_data and
8009                     migrate_data.wait_for_vif_plugged)
8010                 if events and not wait_for_vif_plugged:
8011                     raise _BreakWaitForInstanceEvent
8012         except _BreakWaitForInstanceEvent:
8013             if events:
8014                 LOG.debug('Not waiting for events after pre_live_migration: '
8015                           '%s. ', events, instance=instance)
8016             # This is a bit weird, but we need to clear sys.exc_info() so that
8017             # oslo.log formatting does not inadvertently use it later if an
8018             # error message is logged without an explicit exc_info. This is
8019             # only a problem with python 2.
8020             if six.PY2:
8021                 sys.exc_clear()
8022         except exception.VirtualInterfacePlugException:
8023             with excutils.save_and_reraise_exception():
8024                 LOG.exception('Failed waiting for network virtual interfaces '
8025                               'to be plugged on the destination host %s.',
8026                               dest, instance=instance)
8027                 self._cleanup_pre_live_migration(
8028                     context, dest, instance, migration, migrate_data,
8029                     source_bdms)
8030         except eventlet.timeout.Timeout:
8031             # We only get here if wait_for_vif_plugged is True which means
8032             # live_migration_wait_for_vif_plug=True on the destination host.
8033             msg = (
8034                 'Timed out waiting for events: %(events)s. If these timeouts '
8035                 'are a persistent issue it could mean the networking backend '
8036                 'on host %(dest)s does not support sending these events '
8037                 'unless there are port binding host changes which does not '
8038                 'happen at this point in the live migration process. You may '
8039                 'need to disable the live_migration_wait_for_vif_plug option '
8040                 'on host %(dest)s.')
8041             subs = {'events': events, 'dest': dest}
8042             LOG.warning(msg, subs, instance=instance)
8043             if CONF.vif_plugging_is_fatal:
8044                 self._cleanup_pre_live_migration(
8045                     context, dest, instance, migration, migrate_data,
8046                     source_bdms)
8047                 raise exception.MigrationError(reason=msg % subs)
8048         except Exception:
8049             with excutils.save_and_reraise_exception():
8050                 LOG.exception('Pre live migration failed at %s',
8051                               dest, instance=instance)
8052                 self._cleanup_pre_live_migration(
8053                     context, dest, instance, migration, migrate_data,
8054                     source_bdms)
8055         return migrate_data
8056 
8057     def _do_live_migration(self, context, dest, instance, block_migration,
8058                            migration, migrate_data):
8059         # NOTE(danms): We should enhance the RT to account for migrations
8060         # and use the status field to denote when the accounting has been
8061         # done on source/destination. For now, this is just here for status
8062         # reporting
8063         self._set_migration_status(migration, 'preparing')
8064         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8065                 context, instance.uuid)
8066 
8067         migrate_data = self._do_pre_live_migration_from_source(
8068             context, dest, instance, block_migration, migration, migrate_data,
8069             source_bdms)
8070 
8071         # Set migrate_data.migration because that is how _post_live_migration
8072         # and _rollback_live_migration get the migration object for cleanup.
8073         # Yes this is gross but changing the _post_live_migration and
8074         # _rollback_live_migration interfaces would also mean changing how the
8075         # virt drivers call them from the driver.live_migration method, i.e.
8076         # we would have to pass the migration object through the driver (or
8077         # consider using a partial but some do not like that pattern).
8078         migrate_data.migration = migration
8079 
8080         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
8081         # if it exist in the queue, then we are good to moving on, if
8082         # not, some other process must have aborted it, then we should
8083         # rollback.
8084         try:
8085             self._waiting_live_migrations.pop(instance.uuid)
8086         except KeyError:
8087             LOG.debug('Migration %s aborted by another process, rollback.',
8088                       migration.uuid, instance=instance)
8089             self._rollback_live_migration(context, instance, dest,
8090                                           migrate_data, 'cancelled',
8091                                           source_bdms=source_bdms)
8092             self._notify_live_migrate_abort_end(context, instance)
8093             return
8094 
8095         self._set_migration_status(migration, 'running')
8096 
8097         # NOTE(mdbooth): pre_live_migration will update connection_info and
8098         # attachment_id on all volume BDMS to reflect the new destination
8099         # host attachment. We fetch BDMs before that to retain connection_info
8100         # and attachment_id relating to the source host for post migration
8101         # cleanup.
8102         post_live_migration = functools.partial(self._post_live_migration,
8103                                                 source_bdms=source_bdms)
8104         rollback_live_migration = functools.partial(
8105             self._rollback_live_migration, source_bdms=source_bdms)
8106 
8107         LOG.debug('live_migration data is %s', migrate_data)
8108         try:
8109             self.driver.live_migration(context, instance, dest,
8110                                        post_live_migration,
8111                                        rollback_live_migration,
8112                                        block_migration, migrate_data)
8113         except Exception:
8114             LOG.exception('Live migration failed.', instance=instance)
8115             with excutils.save_and_reraise_exception():
8116                 # Put instance and migration into error state,
8117                 # as its almost certainly too late to rollback
8118                 self._set_migration_status(migration, 'error')
8119                 # first refresh instance as it may have got updated by
8120                 # post_live_migration_at_destination
8121                 instance.refresh()
8122                 self._set_instance_obj_error_state(context, instance,
8123                                                    clean_task_state=True)
8124 
8125     @wrap_exception()
8126     @wrap_instance_event(prefix='compute')
8127     @errors_out_migration
8128     @wrap_instance_fault
8129     def live_migration(self, context, dest, instance, block_migration,
8130                        migration, migrate_data):
8131         """Executing live migration.
8132 
8133         :param context: security context
8134         :param dest: destination host
8135         :param instance: a nova.objects.instance.Instance object
8136         :param block_migration: if true, prepare for block migration
8137         :param migration: an nova.objects.Migration object
8138         :param migrate_data: implementation specific params
8139 
8140         """
8141         self._set_migration_status(migration, 'queued')
8142         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
8143         # put the returned Future object into dict mapped with migration.uuid
8144         # in order to be able to track and abort it in the future.
8145         self._waiting_live_migrations[instance.uuid] = (None, None)
8146         try:
8147             future = self._live_migration_executor.submit(
8148                 self._do_live_migration, context, dest, instance,
8149                 block_migration, migration, migrate_data)
8150             self._waiting_live_migrations[instance.uuid] = (migration, future)
8151         except RuntimeError:
8152             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
8153             # pool is shutdown, which happens in
8154             # _cleanup_live_migrations_in_pool.
8155             LOG.info('Migration %s failed to submit as the compute service '
8156                      'is shutting down.', migration.uuid, instance=instance)
8157             raise exception.LiveMigrationNotSubmitted(
8158                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
8159 
8160     @wrap_exception()
8161     @wrap_instance_event(prefix='compute')
8162     @wrap_instance_fault
8163     def live_migration_force_complete(self, context, instance):
8164         """Force live migration to complete.
8165 
8166         :param context: Security context
8167         :param instance: The instance that is being migrated
8168         """
8169 
8170         self._notify_about_instance_usage(
8171             context, instance, 'live.migration.force.complete.start')
8172         compute_utils.notify_about_instance_action(
8173             context, instance, self.host,
8174             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8175             phase=fields.NotificationPhase.START)
8176         self.driver.live_migration_force_complete(instance)
8177         self._notify_about_instance_usage(
8178             context, instance, 'live.migration.force.complete.end')
8179         compute_utils.notify_about_instance_action(
8180             context, instance, self.host,
8181             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8182             phase=fields.NotificationPhase.END)
8183 
8184     def _notify_live_migrate_abort_end(self, context, instance):
8185         self._notify_about_instance_usage(
8186             context, instance, 'live.migration.abort.end')
8187         compute_utils.notify_about_instance_action(
8188             context, instance, self.host,
8189             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8190             phase=fields.NotificationPhase.END)
8191 
8192     @wrap_exception()
8193     @wrap_instance_event(prefix='compute')
8194     @wrap_instance_fault
8195     def live_migration_abort(self, context, instance, migration_id):
8196         """Abort an in-progress live migration.
8197 
8198         :param context: Security context
8199         :param instance: The instance that is being migrated
8200         :param migration_id: ID of in-progress live migration
8201 
8202         """
8203         self._notify_about_instance_usage(
8204             context, instance, 'live.migration.abort.start')
8205         compute_utils.notify_about_instance_action(
8206             context, instance, self.host,
8207             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8208             phase=fields.NotificationPhase.START)
8209         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
8210         # lead to 3 scenarios:
8211         # 1. The selected migration is still in queue, and the future.cancel()
8212         #    succeed, then the abort action is succeed, mark the migration
8213         #    status to 'cancelled'.
8214         # 2. The selected migration is still in queue, but the future.cancel()
8215         #    failed, then the _do_live_migration() has started executing, and
8216         #    the migration status is 'preparing', then we just pop it from the
8217         #    queue, and the migration process will handle it later. And the
8218         #    migration status couldn't be 'running' in this scenario because
8219         #    if _do_live_migration has started executing and we've already
8220         #    popped it from the queue and set the migration status to
8221         #    'running' at this point, popping it here will raise KeyError at
8222         #    which point we check if it's running and if so, we abort the old
8223         #    way.
8224         # 3. The selected migration is not in the queue, then the migration
8225         #    status is 'running', let the driver handle it.
8226         try:
8227             migration, future = (
8228                 self._waiting_live_migrations.pop(instance.uuid))
8229             if future and future.cancel():
8230                 # If we got here, we've successfully aborted the queued
8231                 # migration and _do_live_migration won't run so we need
8232                 # to set the migration status to cancelled and send the
8233                 # notification. If Future.cancel() fails, it means
8234                 # _do_live_migration is running and the migration status
8235                 # is preparing, and _do_live_migration() itself will attempt
8236                 # to pop the queued migration, hit a KeyError, and rollback,
8237                 # set the migration to cancelled and send the
8238                 # live.migration.abort.end notification.
8239                 self._set_migration_status(migration, 'cancelled')
8240         except KeyError:
8241             migration = objects.Migration.get_by_id(context, migration_id)
8242             if migration.status != 'running':
8243                 raise exception.InvalidMigrationState(
8244                     migration_id=migration_id, instance_uuid=instance.uuid,
8245                     state=migration.status, method='abort live migration')
8246             self.driver.live_migration_abort(instance)
8247         self._notify_live_migrate_abort_end(context, instance)
8248 
8249     def _live_migration_cleanup_flags(self, migrate_data):
8250         """Determine whether disks or instance path need to be cleaned up after
8251         live migration (at source on success, at destination on rollback)
8252 
8253         Block migration needs empty image at destination host before migration
8254         starts, so if any failure occurs, any empty images has to be deleted.
8255 
8256         Also Volume backed live migration w/o shared storage needs to delete
8257         newly created instance-xxx dir on the destination as a part of its
8258         rollback process
8259 
8260         :param migrate_data: implementation specific data
8261         :returns: (bool, bool) -- do_cleanup, destroy_disks
8262         """
8263         # NOTE(pkoniszewski): block migration specific params are set inside
8264         # migrate_data objects for drivers that expose block live migration
8265         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
8266         # cleanup is not needed.
8267         do_cleanup = False
8268         destroy_disks = False
8269         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
8270             # No instance booting at source host, but instance dir
8271             # must be deleted for preparing next block migration
8272             # must be deleted for preparing next live migration w/o shared
8273             # storage
8274             do_cleanup = not migrate_data.is_shared_instance_path
8275             destroy_disks = not migrate_data.is_shared_block_storage
8276         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
8277             do_cleanup = migrate_data.block_migration
8278             destroy_disks = migrate_data.block_migration
8279         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
8280             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
8281             do_cleanup = True
8282             destroy_disks = not migrate_data.is_shared_instance_path
8283 
8284         return (do_cleanup, destroy_disks)
8285 
8286     def _post_live_migration_remove_source_vol_connections(
8287             self, context, instance, source_bdms):
8288         """Disconnect volume connections from the source host during
8289         _post_live_migration.
8290 
8291         :param context: nova auth RequestContext
8292         :param instance: Instance object being live migrated
8293         :param source_bdms: BlockDeviceMappingList representing the attached
8294             volumes with connection_info set for the source host
8295         """
8296         # Detaching volumes.
8297         connector = self.driver.get_volume_connector(instance)
8298         for bdm in source_bdms:
8299             if bdm.is_volume:
8300                 # Detaching volumes is a call to an external API that can fail.
8301                 # If it does, we need to handle it gracefully so that the call
8302                 # to post_live_migration_at_destination - where we set instance
8303                 # host and task state - still happens. We need to rethink the
8304                 # current approach of setting instance host and task state
8305                 # AFTER a whole bunch of things that could fail in unhandled
8306                 # ways, but that is left as a TODO(artom).
8307                 try:
8308                     if bdm.attachment_id is None:
8309                         # Prior to cinder v3.44:
8310                         # We don't want to actually mark the volume detached,
8311                         # or delete the bdm, just remove the connection from
8312                         # this host.
8313                         #
8314                         # remove the volume connection without detaching from
8315                         # hypervisor because the instance is not running
8316                         # anymore on the current host
8317                         self.volume_api.terminate_connection(context,
8318                                                              bdm.volume_id,
8319                                                              connector)
8320                     else:
8321                         # cinder v3.44 api flow - delete the old attachment
8322                         # for the source host
8323                         self.volume_api.attachment_delete(context,
8324                                                           bdm.attachment_id)
8325 
8326                 except Exception as e:
8327                     if bdm.attachment_id is None:
8328                         LOG.error('Connection for volume %s not terminated on '
8329                                   'source host %s during post_live_migration: '
8330                                   '%s', bdm.volume_id, self.host,
8331                                   six.text_type(e), instance=instance)
8332                     else:
8333                         LOG.error('Volume attachment %s not deleted on source '
8334                                   'host %s during post_live_migration: %s',
8335                                   bdm.attachment_id, self.host,
8336                                   six.text_type(e), instance=instance)
8337 
8338     @wrap_exception()
8339     @wrap_instance_fault
8340     def _post_live_migration(self, ctxt, instance, dest,
8341                              block_migration=False, migrate_data=None,
8342                              source_bdms=None):
8343         """Post operations for live migration.
8344 
8345         This method is called from live_migration
8346         and mainly updating database record.
8347 
8348         :param ctxt: security context
8349         :param instance: instance dict
8350         :param dest: destination host
8351         :param block_migration: if true, prepare for block migration
8352         :param migrate_data: if not None, it is a dict which has data
8353         :param source_bdms: BDMs prior to modification by the destination
8354                             compute host. Set by _do_live_migration and not
8355                             part of the callback interface, so this is never
8356                             None
8357         required for live migration without shared storage
8358 
8359         """
8360         LOG.info('_post_live_migration() is started..',
8361                  instance=instance)
8362 
8363         # Cleanup source host post live-migration
8364         block_device_info = self._get_instance_block_device_info(
8365                             ctxt, instance, bdms=source_bdms)
8366         self.driver.post_live_migration(ctxt, instance, block_device_info,
8367                                         migrate_data)
8368 
8369         # Disconnect volumes from this (the source) host.
8370         self._post_live_migration_remove_source_vol_connections(
8371             ctxt, instance, source_bdms)
8372 
8373         # Releasing vlan.
8374         # (not necessary in current implementation?)
8375 
8376         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
8377 
8378         self._notify_about_instance_usage(ctxt, instance,
8379                                           "live_migration._post.start",
8380                                           network_info=network_info)
8381         compute_utils.notify_about_instance_action(
8382             ctxt, instance, self.host,
8383             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8384             phase=fields.NotificationPhase.START)
8385         # Releasing security group ingress rule.
8386         LOG.debug('Calling driver.unfilter_instance from _post_live_migration',
8387                   instance=instance)
8388         self.driver.unfilter_instance(instance,
8389                                       network_info)
8390 
8391         migration = {'source_compute': self.host,
8392                      'dest_compute': dest, }
8393         # For neutron, migrate_instance_start will activate the destination
8394         # host port bindings, if there are any created by conductor before live
8395         # migration started.
8396         self.network_api.migrate_instance_start(ctxt,
8397                                                 instance,
8398                                                 migration)
8399 
8400         destroy_vifs = False
8401         try:
8402             # It's possible that the vif type changed on the destination
8403             # host and is already bound and active, so we need to use the
8404             # stashed source vifs in migrate_data.vifs (if present) to unplug
8405             # on the source host.
8406             unplug_nw_info = network_info
8407             if migrate_data and 'vifs' in migrate_data:
8408                 nw_info = []
8409                 for migrate_vif in migrate_data.vifs:
8410                     nw_info.append(migrate_vif.source_vif)
8411                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
8412                 LOG.debug('Calling driver.post_live_migration_at_source '
8413                           'with original source VIFs from migrate_data: %s',
8414                           unplug_nw_info, instance=instance)
8415             self.driver.post_live_migration_at_source(ctxt, instance,
8416                                                       unplug_nw_info)
8417         except NotImplementedError as ex:
8418             LOG.debug(ex, instance=instance)
8419             # For all hypervisors other than libvirt, there is a possibility
8420             # they are unplugging networks from source node in the cleanup
8421             # method
8422             destroy_vifs = True
8423 
8424         # Free instance allocations on source before claims are allocated on
8425         # destination node
8426         self.rt.free_pci_device_allocations_for_instance(ctxt, instance)
8427         # NOTE(danms): Save source node before calling post method on
8428         # destination, which will update it
8429         source_node = instance.node
8430 
8431         # Define domain at destination host, without doing it,
8432         # pause/suspend/terminate do not work.
8433         post_at_dest_success = True
8434         try:
8435             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
8436                     instance, block_migration, dest)
8437         except Exception as error:
8438             post_at_dest_success = False
8439             # We don't want to break _post_live_migration() if
8440             # post_live_migration_at_destination() fails as it should never
8441             # affect cleaning up source node.
8442             LOG.exception("Post live migration at destination %s failed",
8443                           dest, instance=instance, error=error)
8444 
8445         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8446                 migrate_data)
8447 
8448         if do_cleanup:
8449             # NOTE(artom) By this time post_live_migration_at_destination()
8450             # will have applied the migration context and saved the instance,
8451             # writing a new instance NUMA topology in the process (if the
8452             # intance has one). Here on the source, some drivers will call
8453             # instance.save() in their cleanup() method, which would clobber
8454             # the new instance NUMA topology saved by the destination with the
8455             # old fields in our instance object. To prevent this, refresh our
8456             # instance.
8457             instance.refresh()
8458             LOG.debug('Calling driver.cleanup from _post_live_migration',
8459                       instance=instance)
8460             self.driver.cleanup(ctxt, instance, unplug_nw_info,
8461                                 destroy_disks=destroy_disks,
8462                                 migrate_data=migrate_data,
8463                                 destroy_vifs=destroy_vifs)
8464 
8465         self.instance_events.clear_events_for_instance(instance)
8466 
8467         # NOTE(timello): make sure we update available resources on source
8468         # host even before next periodic task.
8469         self.update_available_resource(ctxt)
8470 
8471         self._update_scheduler_instance_info(ctxt, instance)
8472         self._notify_about_instance_usage(ctxt, instance,
8473                                           "live_migration._post.end",
8474                                           network_info=network_info)
8475         compute_utils.notify_about_instance_action(
8476             ctxt, instance, self.host,
8477             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8478             phase=fields.NotificationPhase.END)
8479         if post_at_dest_success:
8480             LOG.info('Migrating instance to %s finished successfully.',
8481                      dest, instance=instance)
8482 
8483         self._clean_instance_console_tokens(ctxt, instance)
8484         if migrate_data and migrate_data.obj_attr_is_set('migration'):
8485             migrate_data.migration.status = 'completed'
8486             migrate_data.migration.save()
8487             self._delete_allocation_after_move(ctxt,
8488                                                instance,
8489                                                migrate_data.migration)
8490         else:
8491             # We didn't have data on a migration, which means we can't
8492             # look up to see if we had new-style migration-based
8493             # allocations. This should really only happen in cases of
8494             # a buggy virt driver. Log a warning so we know it happened.
8495             LOG.warning('Live migration ended with no migrate_data '
8496                         'record. Unable to clean up migration-based '
8497                         'allocations for node %s which is almost certainly '
8498                         'not an expected situation.', source_node,
8499                         instance=instance)
8500 
8501     def _consoles_enabled(self):
8502         """Returns whether a console is enable."""
8503         return (CONF.vnc.enabled or CONF.spice.enabled or
8504                 CONF.rdp.enabled or CONF.serial_console.enabled or
8505                 CONF.mks.enabled)
8506 
8507     def _clean_instance_console_tokens(self, ctxt, instance):
8508         """Clean console tokens stored for an instance."""
8509         # If the database backend isn't in use, don't bother trying to clean
8510         # tokens.
8511         if self._consoles_enabled():
8512             objects.ConsoleAuthToken.\
8513                 clean_console_auths_for_instance(ctxt, instance.uuid)
8514 
8515     @wrap_exception()
8516     @wrap_instance_event(prefix='compute')
8517     @wrap_instance_fault
8518     def post_live_migration_at_destination(self, context, instance,
8519                                            block_migration):
8520         """Post operations for live migration .
8521 
8522         :param context: security context
8523         :param instance: Instance dict
8524         :param block_migration: if true, prepare for block migration
8525 
8526         """
8527         LOG.info('Post operation of migration started',
8528                  instance=instance)
8529 
8530         # NOTE(tr3buchet): setup networks on destination host
8531         #                  this is called a second time because
8532         #                  multi_host does not create the bridge in
8533         #                  plug_vifs
8534         # NOTE(mriedem): This is a no-op for neutron.
8535         self.network_api.setup_networks_on_host(context, instance,
8536                                                          self.host)
8537         migration = objects.Migration(source_compute=instance.host,
8538                                       dest_compute=self.host,
8539                                       migration_type='live-migration')
8540         # TODO(gibi): calculate and pass resource_provider_mapping
8541         self.network_api.migrate_instance_finish(
8542             context, instance, migration, provider_mappings=None)
8543 
8544         network_info = self.network_api.get_instance_nw_info(context, instance)
8545         self._notify_about_instance_usage(
8546                      context, instance, "live_migration.post.dest.start",
8547                      network_info=network_info)
8548         compute_utils.notify_about_instance_action(context, instance,
8549                 self.host,
8550                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8551                 phase=fields.NotificationPhase.START)
8552         block_device_info = self._get_instance_block_device_info(context,
8553                                                                  instance)
8554         # Allocate the claimed PCI resources at destination.
8555         self.rt.allocate_pci_devices_for_instance(context, instance)
8556 
8557         try:
8558             self.driver.post_live_migration_at_destination(
8559                 context, instance, network_info, block_migration,
8560                 block_device_info)
8561         except Exception:
8562             with excutils.save_and_reraise_exception():
8563                 instance.vm_state = vm_states.ERROR
8564                 LOG.error('Unexpected error during post live migration at '
8565                           'destination host.', instance=instance)
8566         finally:
8567             # Restore instance state and update host
8568             current_power_state = self._get_power_state(context, instance)
8569             node_name = None
8570             prev_host = instance.host
8571             try:
8572                 compute_node = self._get_compute_info(context, self.host)
8573                 node_name = compute_node.hypervisor_hostname
8574             except exception.ComputeHostNotFound:
8575                 LOG.exception('Failed to get compute_info for %s', self.host)
8576             finally:
8577                 # NOTE(artom) We need to apply the migration context here
8578                 # regardless of whether the driver's
8579                 # post_live_migration_at_destination succeeded or not: the
8580                 # instance is on the destination, potentially with a new NUMA
8581                 # topology and resource usage. We need to persist that.
8582                 # NOTE(artom) Apply followed by drop looks weird, but apply
8583                 # just saves the new fields while drop actually removes the
8584                 # migration context from the instance.
8585                 instance.apply_migration_context()
8586                 instance.drop_migration_context()
8587                 instance.host = self.host
8588                 instance.power_state = current_power_state
8589                 instance.task_state = None
8590                 instance.node = node_name
8591                 instance.progress = 0
8592                 instance.save(expected_task_state=task_states.MIGRATING)
8593 
8594         # NOTE(tr3buchet): tear down networks on source host (nova-net)
8595         # NOTE(mriedem): For neutron, this will delete any inactive source
8596         # host port bindings.
8597         try:
8598             self.network_api.setup_networks_on_host(context, instance,
8599                                                     prev_host, teardown=True)
8600         except exception.PortBindingDeletionFailed as e:
8601             # Removing the inactive port bindings from the source host is not
8602             # critical so just log an error but don't fail.
8603             LOG.error('Network cleanup failed for source host %s during post '
8604                       'live migration. You may need to manually clean up '
8605                       'resources in the network service. Error: %s',
8606                       prev_host, six.text_type(e))
8607         # NOTE(vish): this is necessary to update dhcp for nova-network
8608         # NOTE(mriedem): This is a no-op for neutron.
8609         self.network_api.setup_networks_on_host(context, instance, self.host)
8610         self._notify_about_instance_usage(
8611                      context, instance, "live_migration.post.dest.end",
8612                      network_info=network_info)
8613         compute_utils.notify_about_instance_action(context, instance,
8614                 self.host,
8615                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8616                 phase=fields.NotificationPhase.END)
8617 
8618     def _remove_remote_volume_connections(self, context, dest, bdms, instance):
8619         """Rollback remote volume connections on the dest"""
8620         for bdm in bdms:
8621             try:
8622                 # remove the connection on the destination host
8623                 # NOTE(lyarwood): This actually calls the cinderv2
8624                 # os-terminate_connection API if required.
8625                 self.compute_rpcapi.remove_volume_connection(
8626                         context, instance, bdm.volume_id, dest)
8627             except Exception:
8628                 LOG.warning("Ignoring exception while attempting "
8629                             "to rollback volume connections for "
8630                             "volume %s on host %s.", bdm.volume_id,
8631                             dest, instance=instance)
8632 
8633     def _rollback_volume_bdms(self, context, bdms, original_bdms, instance):
8634         """Rollback the connection_info and attachment_id for each bdm"""
8635         original_bdms_by_volid = {bdm.volume_id: bdm for bdm in original_bdms
8636                                   if bdm.is_volume}
8637         for bdm in bdms:
8638             try:
8639                 original_bdm = original_bdms_by_volid[bdm.volume_id]
8640                 if bdm.attachment_id and original_bdm.attachment_id:
8641                     # NOTE(lyarwood): 3.44 cinder api flow. Delete the
8642                     # attachment used by the bdm and reset it to that of
8643                     # the original bdm.
8644                     self.volume_api.attachment_delete(context,
8645                                                       bdm.attachment_id)
8646                     bdm.attachment_id = original_bdm.attachment_id
8647                 # NOTE(lyarwood): Reset the connection_info to the original
8648                 bdm.connection_info = original_bdm.connection_info
8649                 bdm.save()
8650             except cinder_exception.ClientException:
8651                 LOG.warning("Ignoring cinderclient exception when "
8652                             "attempting to delete attachment %s for volume "
8653                             "%s while rolling back volume bdms.",
8654                             bdm.attachment_id, bdm.volume_id,
8655                             instance=instance)
8656             except Exception:
8657                 with excutils.save_and_reraise_exception():
8658                     LOG.exception("Exception while attempting to rollback "
8659                                   "BDM for volume %s.", bdm.volume_id,
8660                                   instance=instance)
8661 
8662     @wrap_exception()
8663     @wrap_instance_fault
8664     def _rollback_live_migration(self, context, instance,
8665                                  dest, migrate_data=None,
8666                                  migration_status='error',
8667                                  source_bdms=None):
8668         """Recovers Instance/volume state from migrating -> running.
8669 
8670         :param context: security context
8671         :param instance: nova.objects.instance.Instance object
8672         :param dest:
8673             This method is called from live migration src host.
8674             This param specifies destination host.
8675         :param migrate_data:
8676             if not none, contains implementation specific data.
8677         :param migration_status:
8678             Contains the status we want to set for the migration object
8679         :param source_bdms: BDMs prior to modification by the destination
8680                             compute host. Set by _do_live_migration and not
8681                             part of the callback interface, so this is never
8682                             None
8683 
8684         """
8685         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
8686               migrate_data.obj_attr_is_set('migration')):
8687             migration = migrate_data.migration
8688         else:
8689             migration = None
8690 
8691         if migration:
8692             # Remove allocations created in Placement for the dest node.
8693             # If migration is None, the virt driver didn't pass it which is
8694             # a bug.
8695             self._revert_allocation(context, instance, migration)
8696         else:
8697             LOG.error('Unable to revert allocations during live migration '
8698                       'rollback; compute driver did not provide migrate_data',
8699                       instance=instance)
8700 
8701         # TODO(artom) drop_move_claim_at_destination() is new in RPC 5.3, only
8702         # call it if we performed a NUMA-aware live migration (which implies us
8703         # being able to send RPC 5.3). To check this, we can use the
8704         # src_supports_numa_live_migration flag, as it will be set if and only
8705         # if:
8706         # - dst_supports_numa_live_migration made its way to the source
8707         #   (meaning both dest and source are new and conductor can speak
8708         #   RPC 5.3)
8709         # - src_supports_numa_live_migration was set by the source driver and
8710         #   passed the send-RPC-5.3 check.
8711         # This check can be removed in RPC 6.0.
8712         if ('src_supports_numa_live_migration' in migrate_data and
8713                 migrate_data.src_supports_numa_live_migration):
8714             LOG.debug('Calling destination to drop move claim.',
8715                       instance=instance)
8716             self.compute_rpcapi.drop_move_claim_at_destination(context,
8717                                                                instance, dest)
8718         instance.task_state = None
8719         instance.progress = 0
8720         instance.drop_migration_context()
8721         instance.save(expected_task_state=[task_states.MIGRATING])
8722 
8723         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
8724         #                  for nova-network)
8725         # NOTE(mriedem): This is a no-op for neutron.
8726         self.network_api.setup_networks_on_host(context, instance, self.host)
8727         self.driver.rollback_live_migration_at_source(context, instance,
8728                                                       migrate_data)
8729 
8730         # NOTE(lyarwood): Fetch the current list of BDMs, disconnect any
8731         # connected volumes from the dest and delete any volume attachments
8732         # used by the destination host before rolling back to the original
8733         # still valid source host volume attachments.
8734         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8735                 context, instance.uuid)
8736         # TODO(lyarwood): Turn the following into a lookup method within
8737         # BlockDeviceMappingList.
8738         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
8739         self._remove_remote_volume_connections(context, dest, vol_bdms,
8740                                                instance)
8741         self._rollback_volume_bdms(context, vol_bdms, source_bdms, instance)
8742 
8743         self._notify_about_instance_usage(context, instance,
8744                                           "live_migration._rollback.start")
8745         compute_utils.notify_about_instance_action(context, instance,
8746                 self.host,
8747                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8748                 phase=fields.NotificationPhase.START,
8749                 bdms=bdms)
8750 
8751         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8752                 migrate_data)
8753 
8754         if do_cleanup:
8755             self.compute_rpcapi.rollback_live_migration_at_destination(
8756                     context, instance, dest, destroy_disks=destroy_disks,
8757                     migrate_data=migrate_data)
8758         elif utils.is_neutron():
8759             # The port binding profiles need to be cleaned up.
8760             with errors_out_migration_ctxt(migration):
8761                 try:
8762                     # This call will delete any inactive destination host
8763                     # port bindings.
8764                     self.network_api.setup_networks_on_host(
8765                         context, instance, host=dest, teardown=True)
8766                 except exception.PortBindingDeletionFailed as e:
8767                     # Removing the inactive port bindings from the destination
8768                     # host is not critical so just log an error but don't fail.
8769                     LOG.error(
8770                         'Network cleanup failed for destination host %s '
8771                         'during live migration rollback. You may need to '
8772                         'manually clean up resources in the network service. '
8773                         'Error: %s', dest, six.text_type(e))
8774                 except Exception:
8775                     with excutils.save_and_reraise_exception():
8776                         LOG.exception(
8777                             'An error occurred while cleaning up networking '
8778                             'during live migration rollback.',
8779                             instance=instance)
8780 
8781         self._notify_about_instance_usage(context, instance,
8782                                           "live_migration._rollback.end")
8783         compute_utils.notify_about_instance_action(context, instance,
8784                 self.host,
8785                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8786                 phase=fields.NotificationPhase.END,
8787                 bdms=bdms)
8788 
8789         self._set_migration_status(migration, migration_status)
8790 
8791     @wrap_exception()
8792     @wrap_instance_fault
8793     def drop_move_claim_at_destination(self, context, instance):
8794         """Called by the source of a live migration during rollback to ask the
8795         destination to drop the MoveClaim object that was created for the live
8796         migration on the destination.
8797         """
8798         nodename = self._get_nodename(instance)
8799         LOG.debug('Dropping live migration resource claim on destination '
8800                   'node %s', nodename, instance=instance)
8801         self.rt.drop_move_claim(
8802             context, instance, nodename, instance_type=instance.flavor)
8803 
8804     @wrap_exception()
8805     @wrap_instance_event(prefix='compute')
8806     @wrap_instance_fault
8807     def rollback_live_migration_at_destination(self, context, instance,
8808                                                destroy_disks,
8809                                                migrate_data):
8810         """Cleaning up image directory that is created pre_live_migration.
8811 
8812         :param context: security context
8813         :param instance: a nova.objects.instance.Instance object sent over rpc
8814         :param destroy_disks: whether to destroy volumes or not
8815         :param migrate_data: contains migration info
8816         """
8817         network_info = self.network_api.get_instance_nw_info(context, instance)
8818         self._notify_about_instance_usage(
8819                       context, instance, "live_migration.rollback.dest.start",
8820                       network_info=network_info)
8821         compute_utils.notify_about_instance_action(
8822             context, instance, self.host,
8823             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8824             phase=fields.NotificationPhase.START)
8825         try:
8826             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
8827             # NOTE(mriedem): For neutron, this call will delete any
8828             # destination host port bindings.
8829             # TODO(mriedem): We should eventually remove this call from
8830             # this method (rollback_live_migration_at_destination) since this
8831             # method is only called conditionally based on whether or not the
8832             # instance is running on shared storage. _rollback_live_migration
8833             # already calls this method for neutron if we are running on
8834             # shared storage.
8835             self.network_api.setup_networks_on_host(context, instance,
8836                                                     self.host, teardown=True)
8837         except exception.PortBindingDeletionFailed as e:
8838             # Removing the inactive port bindings from the destination
8839             # host is not critical so just log an error but don't fail.
8840             LOG.error(
8841                 'Network cleanup failed for destination host %s '
8842                 'during live migration rollback. You may need to '
8843                 'manually clean up resources in the network service. '
8844                 'Error: %s', self.host, six.text_type(e))
8845         except Exception:
8846             with excutils.save_and_reraise_exception():
8847                 # NOTE(tdurakov): even if teardown networks fails driver
8848                 # should try to rollback live migration on destination.
8849                 LOG.exception('An error occurred while deallocating network.',
8850                               instance=instance)
8851         finally:
8852             # always run this even if setup_networks_on_host fails
8853             # NOTE(vish): The mapping is passed in so the driver can disconnect
8854             #             from remote volumes if necessary
8855             block_device_info = self._get_instance_block_device_info(context,
8856                                                                      instance)
8857             # free any instance PCI claims done on destination during
8858             # check_can_live_migrate_destination()
8859             self.rt.free_pci_device_claims_for_instance(context, instance)
8860 
8861             self.driver.rollback_live_migration_at_destination(
8862                 context, instance, network_info, block_device_info,
8863                 destroy_disks=destroy_disks, migrate_data=migrate_data)
8864 
8865         self._notify_about_instance_usage(
8866                         context, instance, "live_migration.rollback.dest.end",
8867                         network_info=network_info)
8868         compute_utils.notify_about_instance_action(
8869             context, instance, self.host,
8870             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8871             phase=fields.NotificationPhase.END)
8872 
8873     def _require_nw_info_update(self, context, instance):
8874         """Detect whether there is a mismatch in binding:host_id, or
8875         binding_failed or unbound binding:vif_type for any of the instances
8876         ports.
8877         """
8878         # Only update port bindings if compute manager does manage port
8879         # bindings instead of the compute driver. For example IronicDriver
8880         # manages the port binding for baremetal instance ports, hence,
8881         # external intervention with the binding is not desired.
8882         if (not utils.is_neutron() or
8883                 self.driver.manages_network_binding_host_id()):
8884             return False
8885 
8886         search_opts = {'device_id': instance.uuid,
8887                        'fields': ['binding:host_id', 'binding:vif_type']}
8888         ports = self.network_api.list_ports(context, **search_opts)
8889         for p in ports['ports']:
8890             if p.get('binding:host_id') != self.host:
8891                 return True
8892             vif_type = p.get('binding:vif_type')
8893             if (vif_type == network_model.VIF_TYPE_UNBOUND or
8894                     vif_type == network_model.VIF_TYPE_BINDING_FAILED):
8895                 return True
8896         return False
8897 
8898     @periodic_task.periodic_task(
8899         spacing=CONF.heal_instance_info_cache_interval)
8900     def _heal_instance_info_cache(self, context):
8901         """Called periodically.  On every call, try to update the
8902         info_cache's network information for another instance by
8903         calling to the network manager.
8904 
8905         This is implemented by keeping a cache of uuids of instances
8906         that live on this host.  On each call, we pop one off of a
8907         list, pull the DB record, and try the call to the network API.
8908         If anything errors don't fail, as it's possible the instance
8909         has been deleted, etc.
8910         """
8911         heal_interval = CONF.heal_instance_info_cache_interval
8912         if not heal_interval:
8913             return
8914 
8915         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
8916         instance = None
8917 
8918         LOG.debug('Starting heal instance info cache')
8919 
8920         if not instance_uuids:
8921             # The list of instances to heal is empty so rebuild it
8922             LOG.debug('Rebuilding the list of instances to heal')
8923             db_instances = objects.InstanceList.get_by_host(
8924                 context, self.host, expected_attrs=[], use_slave=True)
8925             for inst in db_instances:
8926                 # We don't want to refresh the cache for instances
8927                 # which are building or deleting so don't put them
8928                 # in the list. If they are building they will get
8929                 # added to the list next time we build it.
8930                 if (inst.vm_state == vm_states.BUILDING):
8931                     LOG.debug('Skipping network cache update for instance '
8932                               'because it is Building.', instance=inst)
8933                     continue
8934                 if (inst.task_state == task_states.DELETING):
8935                     LOG.debug('Skipping network cache update for instance '
8936                               'because it is being deleted.', instance=inst)
8937                     continue
8938 
8939                 if not instance:
8940                     # Save the first one we find so we don't
8941                     # have to get it again
8942                     instance = inst
8943                 else:
8944                     instance_uuids.append(inst['uuid'])
8945 
8946             self._instance_uuids_to_heal = instance_uuids
8947         else:
8948             # Find the next valid instance on the list
8949             while instance_uuids:
8950                 try:
8951                     inst = objects.Instance.get_by_uuid(
8952                             context, instance_uuids.pop(0),
8953                             expected_attrs=['system_metadata', 'info_cache',
8954                                             'flavor'],
8955                             use_slave=True)
8956                 except exception.InstanceNotFound:
8957                     # Instance is gone.  Try to grab another.
8958                     continue
8959 
8960                 # Check the instance hasn't been migrated
8961                 if inst.host != self.host:
8962                     LOG.debug('Skipping network cache update for instance '
8963                               'because it has been migrated to another '
8964                               'host.', instance=inst)
8965                 # Check the instance isn't being deleting
8966                 elif inst.task_state == task_states.DELETING:
8967                     LOG.debug('Skipping network cache update for instance '
8968                               'because it is being deleted.', instance=inst)
8969                 else:
8970                     instance = inst
8971                     break
8972 
8973         if instance:
8974             # We have an instance now to refresh
8975             try:
8976                 # Fix potential mismatch in port binding if evacuation failed
8977                 # after reassigning the port binding to the dest host but
8978                 # before the instance host is changed.
8979                 # Do this only when instance has no pending task.
8980                 if instance.task_state is None and \
8981                         self._require_nw_info_update(context, instance):
8982                     LOG.info("Updating ports in neutron", instance=instance)
8983                     self.network_api.setup_instance_network_on_host(
8984                         context, instance, self.host)
8985                 # Call to network API to get instance info.. this will
8986                 # force an update to the instance's info_cache
8987                 self.network_api.get_instance_nw_info(
8988                     context, instance, force_refresh=True)
8989                 LOG.debug('Updated the network info_cache for instance',
8990                           instance=instance)
8991             except exception.InstanceNotFound:
8992                 # Instance is gone.
8993                 LOG.debug('Instance no longer exists. Unable to refresh',
8994                           instance=instance)
8995                 return
8996             except exception.InstanceInfoCacheNotFound:
8997                 # InstanceInfoCache is gone.
8998                 LOG.debug('InstanceInfoCache no longer exists. '
8999                           'Unable to refresh', instance=instance)
9000             except Exception:
9001                 LOG.error('An error occurred while refreshing the network '
9002                           'cache.', instance=instance, exc_info=True)
9003         else:
9004             LOG.debug("Didn't find any instances for network info cache "
9005                       "update.")
9006 
9007     @periodic_task.periodic_task
9008     def _poll_rebooting_instances(self, context):
9009         if CONF.reboot_timeout > 0:
9010             filters = {'task_state':
9011                        [task_states.REBOOTING,
9012                         task_states.REBOOT_STARTED,
9013                         task_states.REBOOT_PENDING],
9014                        'host': self.host}
9015             rebooting = objects.InstanceList.get_by_filters(
9016                 context, filters, expected_attrs=[], use_slave=True)
9017 
9018             to_poll = []
9019             for instance in rebooting:
9020                 if timeutils.is_older_than(instance.updated_at,
9021                                            CONF.reboot_timeout):
9022                     to_poll.append(instance)
9023 
9024             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
9025 
9026     @periodic_task.periodic_task
9027     def _poll_rescued_instances(self, context):
9028         if CONF.rescue_timeout > 0:
9029             filters = {'vm_state': vm_states.RESCUED,
9030                        'host': self.host}
9031             rescued_instances = objects.InstanceList.get_by_filters(
9032                 context, filters, expected_attrs=["system_metadata"],
9033                 use_slave=True)
9034 
9035             to_unrescue = []
9036             for instance in rescued_instances:
9037                 if timeutils.is_older_than(instance.launched_at,
9038                                            CONF.rescue_timeout):
9039                     to_unrescue.append(instance)
9040 
9041             for instance in to_unrescue:
9042                 self.compute_api.unrescue(context, instance)
9043 
9044     @periodic_task.periodic_task
9045     def _poll_unconfirmed_resizes(self, context):
9046         if CONF.resize_confirm_window == 0:
9047             return
9048 
9049         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
9050                 context, CONF.resize_confirm_window, self.host,
9051                 use_slave=True)
9052 
9053         migrations_info = dict(migration_count=len(migrations),
9054                 confirm_window=CONF.resize_confirm_window)
9055 
9056         if migrations_info["migration_count"] > 0:
9057             LOG.info("Found %(migration_count)d unconfirmed migrations "
9058                      "older than %(confirm_window)d seconds",
9059                      migrations_info)
9060 
9061         def _set_migration_to_error(migration, reason, **kwargs):
9062             LOG.warning("Setting migration %(migration_id)s to error: "
9063                         "%(reason)s",
9064                         {'migration_id': migration['id'], 'reason': reason},
9065                         **kwargs)
9066             migration.status = 'error'
9067             migration.save()
9068 
9069         for migration in migrations:
9070             instance_uuid = migration.instance_uuid
9071             LOG.info("Automatically confirming migration "
9072                      "%(migration_id)s for instance %(instance_uuid)s",
9073                      {'migration_id': migration.id,
9074                       'instance_uuid': instance_uuid})
9075             expected_attrs = ['metadata', 'system_metadata']
9076             try:
9077                 instance = objects.Instance.get_by_uuid(context,
9078                             instance_uuid, expected_attrs=expected_attrs,
9079                             use_slave=True)
9080             except exception.InstanceNotFound:
9081                 reason = (_("Instance %s not found") %
9082                           instance_uuid)
9083                 _set_migration_to_error(migration, reason)
9084                 continue
9085             if instance.vm_state == vm_states.ERROR:
9086                 reason = _("In ERROR state")
9087                 _set_migration_to_error(migration, reason,
9088                                         instance=instance)
9089                 continue
9090             # race condition: The instance in DELETING state should not be
9091             # set the migration state to error, otherwise the instance in
9092             # to be deleted which is in RESIZED state
9093             # will not be able to confirm resize
9094             if instance.task_state in [task_states.DELETING,
9095                                        task_states.SOFT_DELETING]:
9096                 msg = ("Instance being deleted or soft deleted during resize "
9097                        "confirmation. Skipping.")
9098                 LOG.debug(msg, instance=instance)
9099                 continue
9100 
9101             # race condition: This condition is hit when this method is
9102             # called between the save of the migration record with a status of
9103             # finished and the save of the instance object with a state of
9104             # RESIZED. The migration record should not be set to error.
9105             if instance.task_state == task_states.RESIZE_FINISH:
9106                 msg = ("Instance still resizing during resize "
9107                        "confirmation. Skipping.")
9108                 LOG.debug(msg, instance=instance)
9109                 continue
9110 
9111             vm_state = instance.vm_state
9112             task_state = instance.task_state
9113             if vm_state != vm_states.RESIZED or task_state is not None:
9114                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
9115                            "RESIZED/None") %
9116                           {'vm_state': vm_state,
9117                            'task_state': task_state})
9118                 _set_migration_to_error(migration, reason,
9119                                         instance=instance)
9120                 continue
9121             try:
9122                 self.compute_api.confirm_resize(context, instance,
9123                                                 migration=migration)
9124             except Exception as e:
9125                 LOG.info("Error auto-confirming resize: %s. "
9126                          "Will retry later.", e, instance=instance)
9127 
9128     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
9129     def _poll_shelved_instances(self, context):
9130 
9131         if CONF.shelved_offload_time <= 0:
9132             return
9133 
9134         filters = {'vm_state': vm_states.SHELVED,
9135                    'task_state': None,
9136                    'host': self.host}
9137         shelved_instances = objects.InstanceList.get_by_filters(
9138             context, filters=filters, expected_attrs=['system_metadata'],
9139             use_slave=True)
9140 
9141         to_gc = []
9142         for instance in shelved_instances:
9143             sys_meta = instance.system_metadata
9144             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
9145             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
9146                 to_gc.append(instance)
9147 
9148         for instance in to_gc:
9149             try:
9150                 instance.task_state = task_states.SHELVING_OFFLOADING
9151                 instance.save(expected_task_state=(None,))
9152                 self.shelve_offload_instance(context, instance,
9153                                              clean_shutdown=False)
9154             except Exception:
9155                 LOG.exception('Periodic task failed to offload instance.',
9156                               instance=instance)
9157 
9158     @periodic_task.periodic_task
9159     def _instance_usage_audit(self, context):
9160         if not CONF.instance_usage_audit:
9161             return
9162 
9163         begin, end = utils.last_completed_audit_period()
9164         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
9165                                self.host):
9166             return
9167 
9168         instances = objects.InstanceList.get_active_by_window_joined(
9169             context, begin, end, host=self.host,
9170             expected_attrs=['system_metadata', 'info_cache', 'metadata',
9171                             'flavor'],
9172             use_slave=True)
9173         num_instances = len(instances)
9174         errors = 0
9175         successes = 0
9176         LOG.info("Running instance usage audit for host %(host)s "
9177                  "from %(begin_time)s to %(end_time)s. "
9178                  "%(number_instances)s instances.",
9179                  {'host': self.host,
9180                   'begin_time': begin,
9181                   'end_time': end,
9182                   'number_instances': num_instances})
9183         start_time = time.time()
9184         task_log = objects.TaskLog(context)
9185         task_log.task_name = 'instance_usage_audit'
9186         task_log.period_beginning = begin
9187         task_log.period_ending = end
9188         task_log.host = self.host
9189         task_log.task_items = num_instances
9190         task_log.message = 'Instance usage audit started...'
9191         task_log.begin_task()
9192         for instance in instances:
9193             try:
9194                 compute_utils.notify_usage_exists(
9195                     self.notifier, context, instance, self.host,
9196                     ignore_missing_network_data=False)
9197                 successes += 1
9198             except Exception:
9199                 LOG.exception('Failed to generate usage '
9200                               'audit for instance '
9201                               'on host %s', self.host,
9202                               instance=instance)
9203                 errors += 1
9204         task_log.errors = errors
9205         task_log.message = (
9206             'Instance usage audit ran for host %s, %s instances in %s seconds.'
9207             % (self.host, num_instances, time.time() - start_time))
9208         task_log.end_task()
9209 
9210     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
9211     def _poll_bandwidth_usage(self, context):
9212 
9213         if not self._bw_usage_supported:
9214             return
9215 
9216         prev_time, start_time = utils.last_completed_audit_period()
9217 
9218         curr_time = time.time()
9219         if (curr_time - self._last_bw_usage_poll >
9220                 CONF.bandwidth_poll_interval):
9221             self._last_bw_usage_poll = curr_time
9222             LOG.info("Updating bandwidth usage cache")
9223 
9224             instances = objects.InstanceList.get_by_host(context,
9225                                                               self.host,
9226                                                               use_slave=True)
9227             try:
9228                 bw_counters = self.driver.get_all_bw_counters(instances)
9229             except NotImplementedError:
9230                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
9231                 # implemented yet.  If they don't it doesn't break anything,
9232                 # they just don't get the info in the usage events.
9233                 # NOTE(PhilDay): Record that its not supported so we can
9234                 # skip fast on future calls rather than waste effort getting
9235                 # the list of instances.
9236                 LOG.info("Bandwidth usage not supported by %(driver)s.",
9237                          {'driver': CONF.compute_driver})
9238                 self._bw_usage_supported = False
9239                 return
9240 
9241             refreshed = timeutils.utcnow()
9242             for bw_ctr in bw_counters:
9243                 # Allow switching of greenthreads between queries.
9244                 greenthread.sleep(0)
9245                 bw_in = 0
9246                 bw_out = 0
9247                 last_ctr_in = None
9248                 last_ctr_out = None
9249                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
9250                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
9251                     start_period=start_time, use_slave=True)
9252                 if usage:
9253                     bw_in = usage.bw_in
9254                     bw_out = usage.bw_out
9255                     last_ctr_in = usage.last_ctr_in
9256                     last_ctr_out = usage.last_ctr_out
9257                 else:
9258                     usage = (objects.BandwidthUsage.
9259                              get_by_instance_uuid_and_mac(
9260                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
9261                         start_period=prev_time, use_slave=True))
9262                     if usage:
9263                         last_ctr_in = usage.last_ctr_in
9264                         last_ctr_out = usage.last_ctr_out
9265 
9266                 if last_ctr_in is not None:
9267                     if bw_ctr['bw_in'] < last_ctr_in:
9268                         # counter rollover
9269                         bw_in += bw_ctr['bw_in']
9270                     else:
9271                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
9272 
9273                 if last_ctr_out is not None:
9274                     if bw_ctr['bw_out'] < last_ctr_out:
9275                         # counter rollover
9276                         bw_out += bw_ctr['bw_out']
9277                     else:
9278                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
9279 
9280                 objects.BandwidthUsage(context=context).create(
9281                                               bw_ctr['uuid'],
9282                                               bw_ctr['mac_address'],
9283                                               bw_in,
9284                                               bw_out,
9285                                               bw_ctr['bw_in'],
9286                                               bw_ctr['bw_out'],
9287                                               start_period=start_time,
9288                                               last_refreshed=refreshed)
9289 
9290     def _get_host_volume_bdms(self, context, use_slave=False):
9291         """Return all block device mappings on a compute host."""
9292         compute_host_bdms = []
9293         instances = objects.InstanceList.get_by_host(context, self.host,
9294             use_slave=use_slave)
9295         for instance in instances:
9296             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9297                     context, instance.uuid, use_slave=use_slave)
9298             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
9299             compute_host_bdms.append(dict(instance=instance,
9300                                           instance_bdms=instance_bdms))
9301 
9302         return compute_host_bdms
9303 
9304     def _update_volume_usage_cache(self, context, vol_usages):
9305         """Updates the volume usage cache table with a list of stats."""
9306         for usage in vol_usages:
9307             # Allow switching of greenthreads between queries.
9308             greenthread.sleep(0)
9309             vol_usage = objects.VolumeUsage(context)
9310             vol_usage.volume_id = usage['volume']
9311             vol_usage.instance_uuid = usage['instance'].uuid
9312             vol_usage.project_id = usage['instance'].project_id
9313             vol_usage.user_id = usage['instance'].user_id
9314             vol_usage.availability_zone = usage['instance'].availability_zone
9315             vol_usage.curr_reads = usage['rd_req']
9316             vol_usage.curr_read_bytes = usage['rd_bytes']
9317             vol_usage.curr_writes = usage['wr_req']
9318             vol_usage.curr_write_bytes = usage['wr_bytes']
9319             vol_usage.save()
9320             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
9321             compute_utils.notify_about_volume_usage(context, vol_usage,
9322                                                     self.host)
9323 
9324     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
9325     def _poll_volume_usage(self, context):
9326         if CONF.volume_usage_poll_interval == 0:
9327             return
9328 
9329         compute_host_bdms = self._get_host_volume_bdms(context,
9330                                                        use_slave=True)
9331         if not compute_host_bdms:
9332             return
9333 
9334         LOG.debug("Updating volume usage cache")
9335         try:
9336             vol_usages = self.driver.get_all_volume_usage(context,
9337                                                           compute_host_bdms)
9338         except NotImplementedError:
9339             return
9340 
9341         self._update_volume_usage_cache(context, vol_usages)
9342 
9343     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
9344                                  run_immediately=True)
9345     def _sync_power_states(self, context):
9346         """Align power states between the database and the hypervisor.
9347 
9348         To sync power state data we make a DB call to get the number of
9349         virtual machines known by the hypervisor and if the number matches the
9350         number of virtual machines known by the database, we proceed in a lazy
9351         loop, one database record at a time, checking if the hypervisor has the
9352         same power state as is in the database.
9353         """
9354         db_instances = objects.InstanceList.get_by_host(context, self.host,
9355                                                         expected_attrs=[],
9356                                                         use_slave=True)
9357 
9358         try:
9359             num_vm_instances = self.driver.get_num_instances()
9360         except exception.VirtDriverNotReady as e:
9361             # If the virt driver is not ready, like ironic-api not being up
9362             # yet in the case of ironic, just log it and exit.
9363             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
9364             return
9365 
9366         num_db_instances = len(db_instances)
9367 
9368         if num_vm_instances != num_db_instances:
9369             LOG.warning("While synchronizing instance power states, found "
9370                         "%(num_db_instances)s instances in the database "
9371                         "and %(num_vm_instances)s instances on the "
9372                         "hypervisor.",
9373                         {'num_db_instances': num_db_instances,
9374                          'num_vm_instances': num_vm_instances})
9375 
9376         def _sync(db_instance):
9377             # NOTE(melwitt): This must be synchronized as we query state from
9378             #                two separate sources, the driver and the database.
9379             #                They are set (in stop_instance) and read, in sync.
9380             @utils.synchronized(db_instance.uuid)
9381             def query_driver_power_state_and_sync():
9382                 self._query_driver_power_state_and_sync(context, db_instance)
9383 
9384             try:
9385                 query_driver_power_state_and_sync()
9386             except Exception:
9387                 LOG.exception("Periodic sync_power_state task had an "
9388                               "error while processing an instance.",
9389                               instance=db_instance)
9390 
9391             self._syncs_in_progress.pop(db_instance.uuid)
9392 
9393         for db_instance in db_instances:
9394             # process syncs asynchronously - don't want instance locking to
9395             # block entire periodic task thread
9396             uuid = db_instance.uuid
9397             if uuid in self._syncs_in_progress:
9398                 LOG.debug('Sync already in progress for %s', uuid)
9399             else:
9400                 LOG.debug('Triggering sync for uuid %s', uuid)
9401                 self._syncs_in_progress[uuid] = True
9402                 self._sync_power_pool.spawn_n(_sync, db_instance)
9403 
9404     def _query_driver_power_state_and_sync(self, context, db_instance):
9405         if db_instance.task_state is not None:
9406             LOG.info("During sync_power_state the instance has a "
9407                      "pending task (%(task)s). Skip.",
9408                      {'task': db_instance.task_state}, instance=db_instance)
9409             return
9410         # No pending tasks. Now try to figure out the real vm_power_state.
9411         try:
9412             vm_instance = self.driver.get_info(db_instance)
9413             vm_power_state = vm_instance.state
9414         except exception.InstanceNotFound:
9415             vm_power_state = power_state.NOSTATE
9416         # Note(maoy): the above get_info call might take a long time,
9417         # for example, because of a broken libvirt driver.
9418         try:
9419             self._sync_instance_power_state(context,
9420                                             db_instance,
9421                                             vm_power_state,
9422                                             use_slave=True)
9423         except exception.InstanceNotFound:
9424             # NOTE(hanlind): If the instance gets deleted during sync,
9425             # silently ignore.
9426             pass
9427 
9428     def _stop_unexpected_shutdown_instance(self, context, vm_state,
9429                                            db_instance, orig_db_power_state):
9430         # this is an exceptional case; make sure our data is up
9431         # to date before slamming through a power off
9432         vm_instance = self.driver.get_info(db_instance,
9433                                            use_cache=False)
9434         vm_power_state = vm_instance.state
9435 
9436         # if it still looks off, go ahead and call stop()
9437         if vm_power_state in (power_state.SHUTDOWN,
9438                               power_state.CRASHED):
9439 
9440             LOG.warning("Instance shutdown by itself. Calling the "
9441                         "stop API. Current vm_state: %(vm_state)s, "
9442                         "current task_state: %(task_state)s, "
9443                         "original DB power_state: %(db_power_state)s, "
9444                         "current VM power_state: %(vm_power_state)s",
9445                         {'vm_state': vm_state,
9446                          'task_state': db_instance.task_state,
9447                          'db_power_state': orig_db_power_state,
9448                          'vm_power_state': vm_power_state},
9449                         instance=db_instance)
9450             try:
9451                 # Note(maoy): here we call the API instead of
9452                 # brutally updating the vm_state in the database
9453                 # to allow all the hooks and checks to be performed.
9454                 if db_instance.shutdown_terminate:
9455                     self.compute_api.delete(context, db_instance)
9456                 else:
9457                     self.compute_api.stop(context, db_instance)
9458             except Exception:
9459                 # Note(maoy): there is no need to propagate the error
9460                 # because the same power_state will be retrieved next
9461                 # time and retried.
9462                 # For example, there might be another task scheduled.
9463                 LOG.exception("error during stop() in sync_power_state.",
9464                               instance=db_instance)
9465 
9466     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
9467                                    use_slave=False):
9468         """Align instance power state between the database and hypervisor.
9469 
9470         If the instance is not found on the hypervisor, but is in the database,
9471         then a stop() API will be called on the instance.
9472         """
9473 
9474         # We re-query the DB to get the latest instance info to minimize
9475         # (not eliminate) race condition.
9476         db_instance.refresh(use_slave=use_slave)
9477         db_power_state = db_instance.power_state
9478         vm_state = db_instance.vm_state
9479 
9480         if self.host != db_instance.host:
9481             # on the sending end of nova-compute _sync_power_state
9482             # may have yielded to the greenthread performing a live
9483             # migration; this in turn has changed the resident-host
9484             # for the VM; However, the instance is still active, it
9485             # is just in the process of migrating to another host.
9486             # This implies that the compute source must relinquish
9487             # control to the compute destination.
9488             LOG.info("During the sync_power process the "
9489                      "instance has moved from "
9490                      "host %(src)s to host %(dst)s",
9491                      {'src': db_instance.host,
9492                       'dst': self.host},
9493                      instance=db_instance)
9494             return
9495         elif db_instance.task_state is not None:
9496             # on the receiving end of nova-compute, it could happen
9497             # that the DB instance already report the new resident
9498             # but the actual VM has not showed up on the hypervisor
9499             # yet. In this case, let's allow the loop to continue
9500             # and run the state sync in a later round
9501             LOG.info("During sync_power_state the instance has a "
9502                      "pending task (%(task)s). Skip.",
9503                      {'task': db_instance.task_state},
9504                      instance=db_instance)
9505             return
9506 
9507         orig_db_power_state = db_power_state
9508         if vm_power_state != db_power_state:
9509             LOG.info('During _sync_instance_power_state the DB '
9510                      'power_state (%(db_power_state)s) does not match '
9511                      'the vm_power_state from the hypervisor '
9512                      '(%(vm_power_state)s). Updating power_state in the '
9513                      'DB to match the hypervisor.',
9514                      {'db_power_state': db_power_state,
9515                       'vm_power_state': vm_power_state},
9516                      instance=db_instance)
9517             # power_state is always updated from hypervisor to db
9518             db_instance.power_state = vm_power_state
9519             db_instance.save()
9520             db_power_state = vm_power_state
9521 
9522         # Note(maoy): Now resolve the discrepancy between vm_state and
9523         # vm_power_state. We go through all possible vm_states.
9524         if vm_state in (vm_states.BUILDING,
9525                         vm_states.RESCUED,
9526                         vm_states.RESIZED,
9527                         vm_states.SUSPENDED,
9528                         vm_states.ERROR):
9529             # TODO(maoy): we ignore these vm_state for now.
9530             pass
9531         elif vm_state == vm_states.ACTIVE:
9532             # The only rational power state should be RUNNING
9533             if vm_power_state in (power_state.SHUTDOWN,
9534                                   power_state.CRASHED):
9535                 self._stop_unexpected_shutdown_instance(
9536                     context, vm_state, db_instance, orig_db_power_state)
9537             elif vm_power_state == power_state.SUSPENDED:
9538                 LOG.warning("Instance is suspended unexpectedly. Calling "
9539                             "the stop API.", instance=db_instance)
9540                 try:
9541                     self.compute_api.stop(context, db_instance)
9542                 except Exception:
9543                     LOG.exception("error during stop() in sync_power_state.",
9544                                   instance=db_instance)
9545             elif vm_power_state == power_state.PAUSED:
9546                 # Note(maoy): a VM may get into the paused state not only
9547                 # because the user request via API calls, but also
9548                 # due to (temporary) external instrumentations.
9549                 # Before the virt layer can reliably report the reason,
9550                 # we simply ignore the state discrepancy. In many cases,
9551                 # the VM state will go back to running after the external
9552                 # instrumentation is done. See bug 1097806 for details.
9553                 LOG.warning("Instance is paused unexpectedly. Ignore.",
9554                             instance=db_instance)
9555             elif vm_power_state == power_state.NOSTATE:
9556                 # Occasionally, depending on the status of the hypervisor,
9557                 # which could be restarting for example, an instance may
9558                 # not be found.  Therefore just log the condition.
9559                 LOG.warning("Instance is unexpectedly not found. Ignore.",
9560                             instance=db_instance)
9561         elif vm_state == vm_states.STOPPED:
9562             if vm_power_state not in (power_state.NOSTATE,
9563                                       power_state.SHUTDOWN,
9564                                       power_state.CRASHED):
9565                 LOG.warning("Instance is not stopped. Calling "
9566                             "the stop API. Current vm_state: %(vm_state)s,"
9567                             " current task_state: %(task_state)s, "
9568                             "original DB power_state: %(db_power_state)s, "
9569                             "current VM power_state: %(vm_power_state)s",
9570                             {'vm_state': vm_state,
9571                              'task_state': db_instance.task_state,
9572                              'db_power_state': orig_db_power_state,
9573                              'vm_power_state': vm_power_state},
9574                             instance=db_instance)
9575                 try:
9576                     # NOTE(russellb) Force the stop, because normally the
9577                     # compute API would not allow an attempt to stop a stopped
9578                     # instance.
9579                     self.compute_api.force_stop(context, db_instance)
9580                 except Exception:
9581                     LOG.exception("error during stop() in sync_power_state.",
9582                                   instance=db_instance)
9583         elif vm_state == vm_states.PAUSED:
9584             if vm_power_state in (power_state.SHUTDOWN,
9585                                   power_state.CRASHED):
9586                 LOG.warning("Paused instance shutdown by itself. Calling "
9587                             "the stop API.", instance=db_instance)
9588                 try:
9589                     self.compute_api.force_stop(context, db_instance)
9590                 except Exception:
9591                     LOG.exception("error during stop() in sync_power_state.",
9592                                   instance=db_instance)
9593         elif vm_state in (vm_states.SOFT_DELETED,
9594                           vm_states.DELETED):
9595             if vm_power_state not in (power_state.NOSTATE,
9596                                       power_state.SHUTDOWN):
9597                 # Note(maoy): this should be taken care of periodically in
9598                 # _cleanup_running_deleted_instances().
9599                 LOG.warning("Instance is not (soft-)deleted.",
9600                             instance=db_instance)
9601 
9602     @periodic_task.periodic_task
9603     def _reclaim_queued_deletes(self, context):
9604         """Reclaim instances that are queued for deletion."""
9605         interval = CONF.reclaim_instance_interval
9606         if interval <= 0:
9607             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
9608             return
9609 
9610         filters = {'vm_state': vm_states.SOFT_DELETED,
9611                    'task_state': None,
9612                    'host': self.host}
9613         instances = objects.InstanceList.get_by_filters(
9614             context, filters,
9615             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
9616             use_slave=True)
9617         for instance in instances:
9618             if self._deleted_old_enough(instance, interval):
9619                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9620                         context, instance.uuid)
9621                 LOG.info('Reclaiming deleted instance', instance=instance)
9622                 try:
9623                     self._delete_instance(context, instance, bdms)
9624                 except Exception as e:
9625                     LOG.warning("Periodic reclaim failed to delete "
9626                                 "instance: %s",
9627                                 e, instance=instance)
9628 
9629     def _get_nodename(self, instance, refresh=False):
9630         """Helper method to get the name of the first available node
9631         on this host. This method should not be used with any operations
9632         on ironic instances since it does not handle multiple nodes.
9633         """
9634         node = self.driver.get_available_nodes(refresh=refresh)[0]
9635         LOG.debug("No node specified, defaulting to %s", node,
9636                   instance=instance)
9637         return node
9638 
9639     def _update_available_resource_for_node(self, context, nodename,
9640                                             startup=False):
9641 
9642         try:
9643             self.rt.update_available_resource(context, nodename,
9644                                               startup=startup)
9645         except exception.ComputeHostNotFound:
9646             LOG.warning("Compute node '%s' not found in "
9647                         "update_available_resource.", nodename)
9648         except exception.ReshapeFailed:
9649             # We're only supposed to get here on startup, if a reshape was
9650             # needed, was attempted, and failed. We want to kill the service.
9651             with excutils.save_and_reraise_exception():
9652                 LOG.critical("Resource provider data migration failed "
9653                              "fatally during startup for node %s.", nodename)
9654         except exception.ReshapeNeeded:
9655             # This exception should only find its way here if the virt driver's
9656             # update_provider_tree raised it incorrectly: either
9657             # a) After the resource tracker already caught it once and
9658             # reinvoked update_provider_tree with allocations. At this point
9659             # the driver is just supposed to *do* the reshape, so if it raises
9660             # ReshapeNeeded, it's a bug, and we want to kill the compute
9661             # service.
9662             # b) On periodic rather than startup (we only allow reshapes to
9663             # happen on startup). In this case we'll just make the logs red and
9664             # go again at the next periodic interval, where the same thing may
9665             # or may not happen again. Depending on the previous and intended
9666             # shape of the providers/inventories, this may not actually cause
9667             # any immediately visible symptoms (in terms of scheduling, etc.)
9668             # If this becomes a problem, we may wish to make it pop immediately
9669             # (e.g. disable the service).
9670             with excutils.save_and_reraise_exception():
9671                 LOG.exception("ReshapeNeeded exception is unexpected here!")
9672         except Exception:
9673             LOG.exception("Error updating resources for node %(node)s.",
9674                           {'node': nodename})
9675 
9676     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
9677     def update_available_resource(self, context, startup=False):
9678         """See driver.get_available_resource()
9679 
9680         Periodic process that keeps that the compute host's understanding of
9681         resource availability and usage in sync with the underlying hypervisor.
9682 
9683         :param context: security context
9684         :param startup: True if this is being called when the nova-compute
9685             service is starting, False otherwise.
9686         """
9687 
9688         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
9689                                                             use_slave=True,
9690                                                             startup=startup)
9691         try:
9692             nodenames = set(self.driver.get_available_nodes())
9693         except exception.VirtDriverNotReady:
9694             LOG.warning("Virt driver is not ready.")
9695             return
9696 
9697         # Delete orphan compute node not reported by driver but still in db
9698         for cn in compute_nodes_in_db:
9699             if cn.hypervisor_hostname not in nodenames:
9700                 LOG.info("Deleting orphan compute node %(id)s "
9701                          "hypervisor host is %(hh)s, "
9702                          "nodes are %(nodes)s",
9703                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
9704                           'nodes': nodenames})
9705                 cn.destroy()
9706                 self.rt.remove_node(cn.hypervisor_hostname)
9707                 # Delete the corresponding resource provider in placement,
9708                 # along with any associated allocations.
9709                 try:
9710                     self.reportclient.delete_resource_provider(context, cn,
9711                                                                cascade=True)
9712                 except keystone_exception.ClientException as e:
9713                     LOG.error(
9714                         "Failed to delete compute node resource provider "
9715                         "for compute node %s: %s", cn.uuid, six.text_type(e))
9716 
9717         for nodename in nodenames:
9718             self._update_available_resource_for_node(context, nodename,
9719                                                      startup=startup)
9720 
9721     def _get_compute_nodes_in_db(self, context, use_slave=False,
9722                                  startup=False):
9723         try:
9724             return objects.ComputeNodeList.get_all_by_host(context, self.host,
9725                                                            use_slave=use_slave)
9726         except exception.NotFound:
9727             if startup:
9728                 LOG.warning(
9729                     "No compute node record found for host %s. If this is "
9730                     "the first time this service is starting on this "
9731                     "host, then you can ignore this warning.", self.host)
9732             else:
9733                 LOG.error("No compute node record for host %s", self.host)
9734             return []
9735 
9736     @periodic_task.periodic_task(
9737         spacing=CONF.running_deleted_instance_poll_interval,
9738         run_immediately=True)
9739     def _cleanup_running_deleted_instances(self, context):
9740         """Cleanup any instances which are erroneously still running after
9741         having been deleted.
9742 
9743         Valid actions to take are:
9744 
9745             1. noop - do nothing
9746             2. log - log which instances are erroneously running
9747             3. reap - shutdown and cleanup any erroneously running instances
9748             4. shutdown - power off *and disable* any erroneously running
9749                           instances
9750 
9751         The use-case for this cleanup task is: for various reasons, it may be
9752         possible for the database to show an instance as deleted but for that
9753         instance to still be running on a host machine (see bug
9754         https://bugs.launchpad.net/nova/+bug/911366).
9755 
9756         This cleanup task is a cross-hypervisor utility for finding these
9757         zombied instances and either logging the discrepancy (likely what you
9758         should do in production), or automatically reaping the instances (more
9759         appropriate for dev environments).
9760         """
9761         action = CONF.running_deleted_instance_action
9762 
9763         if action == "noop":
9764             return
9765 
9766         # NOTE(sirp): admin contexts don't ordinarily return deleted records
9767         with utils.temporary_mutation(context, read_deleted="yes"):
9768 
9769             try:
9770                 instances = self._running_deleted_instances(context)
9771             except exception.VirtDriverNotReady:
9772                 # Since this task runs immediately on startup, if the
9773                 # hypervisor is not yet ready handle it gracefully.
9774                 LOG.debug('Unable to check for running deleted instances '
9775                           'at this time since the hypervisor is not ready.')
9776                 return
9777 
9778             for instance in instances:
9779                 if action == "log":
9780                     LOG.warning("Detected instance with name label "
9781                                 "'%s' which is marked as "
9782                                 "DELETED but still present on host.",
9783                                 instance.name, instance=instance)
9784 
9785                 elif action == 'shutdown':
9786                     LOG.info("Powering off instance with name label "
9787                              "'%s' which is marked as "
9788                              "DELETED but still present on host.",
9789                              instance.name, instance=instance)
9790                     try:
9791                         try:
9792                             # disable starting the instance
9793                             self.driver.set_bootable(instance, False)
9794                         except NotImplementedError:
9795                             LOG.debug("set_bootable is not implemented "
9796                                       "for the current driver")
9797                         # and power it off
9798                         self.driver.power_off(instance)
9799                     except Exception:
9800                         LOG.warning("Failed to power off instance",
9801                                     instance=instance, exc_info=True)
9802 
9803                 elif action == 'reap':
9804                     LOG.info("Destroying instance with name label "
9805                              "'%s' which is marked as "
9806                              "DELETED but still present on host.",
9807                              instance.name, instance=instance)
9808                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9809                         context, instance.uuid, use_slave=True)
9810                     self.instance_events.clear_events_for_instance(instance)
9811                     try:
9812                         self._shutdown_instance(context, instance, bdms,
9813                                                 notify=False)
9814                         self._cleanup_volumes(context, instance, bdms,
9815                                               detach=False)
9816                     except Exception as e:
9817                         LOG.warning("Periodic cleanup failed to delete "
9818                                     "instance: %s",
9819                                     e, instance=instance)
9820                 else:
9821                     raise Exception(_("Unrecognized value '%s'"
9822                                       " for CONF.running_deleted_"
9823                                       "instance_action") % action)
9824 
9825     def _running_deleted_instances(self, context):
9826         """Returns a list of instances nova thinks is deleted,
9827         but the hypervisor thinks is still running.
9828         """
9829         timeout = CONF.running_deleted_instance_timeout
9830         filters = {'deleted': True,
9831                    'soft_deleted': False}
9832         instances = self._get_instances_on_driver(context, filters)
9833         return [i for i in instances if self._deleted_old_enough(i, timeout)]
9834 
9835     def _deleted_old_enough(self, instance, timeout):
9836         deleted_at = instance.deleted_at
9837         if deleted_at:
9838             deleted_at = deleted_at.replace(tzinfo=None)
9839         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
9840 
9841     @contextlib.contextmanager
9842     def _error_out_instance_on_exception(self, context, instance,
9843                                          instance_state=vm_states.ACTIVE):
9844         """Context manager to set instance.vm_state after some operation raises
9845 
9846         Used to handle NotImplementedError and InstanceFaultRollback errors
9847         and reset the instance vm_state and task_state. The vm_state is set
9848         to the $instance_state parameter and task_state is set to None.
9849         For all other types of exceptions, the vm_state is set to ERROR and
9850         the task_state is left unchanged (although most callers will have the
9851         @reverts_task_state decorator which will set the task_state to None).
9852 
9853         Re-raises the original exception *except* in the case of
9854         InstanceFaultRollback in which case the wrapped `inner_exception` is
9855         re-raised.
9856 
9857         :param context: The nova auth request context for the operation.
9858         :param instance: The instance to update. The vm_state will be set by
9859             this context manager when an exception is raised.
9860         :param instance_state: For NotImplementedError and
9861             InstanceFaultRollback this is the vm_state to set the instance to
9862             when handling one of those types of exceptions. By default the
9863             instance will be set to ACTIVE, but the caller should control this
9864             in case there have been no changes to the running state of the
9865             instance. For example, resizing a stopped server where prep_resize
9866             fails early and does not change the power state of the guest should
9867             not set the instance status to ACTIVE but remain STOPPED.
9868             This parameter is ignored for all other types of exceptions and the
9869             instance vm_state is set to ERROR.
9870         """
9871         # NOTE(mriedem): Why doesn't this method just save off the
9872         # original instance.vm_state here rather than use a parameter? Or use
9873         # instance_state=None as an override but default to the current
9874         # vm_state when rolling back.
9875         instance_uuid = instance.uuid
9876         try:
9877             yield
9878         except (NotImplementedError, exception.InstanceFaultRollback) as error:
9879             # Use reraise=False to determine if we want to raise the original
9880             # exception or something else.
9881             with excutils.save_and_reraise_exception(reraise=False) as ctxt:
9882                 LOG.info("Setting instance back to %(state)s after: %(error)s",
9883                          {'state': instance_state, 'error': error},
9884                          instance_uuid=instance_uuid)
9885                 self._instance_update(context, instance,
9886                                       vm_state=instance_state,
9887                                       task_state=None)
9888                 if isinstance(error, exception.InstanceFaultRollback):
9889                     # Raise the wrapped exception.
9890                     raise error.inner_exception
9891                 # Else re-raise the NotImplementedError.
9892                 ctxt.reraise = True
9893         except Exception:
9894             LOG.exception('Setting instance vm_state to ERROR',
9895                           instance_uuid=instance_uuid)
9896             with excutils.save_and_reraise_exception():
9897                 # NOTE(mriedem): Why don't we pass clean_task_state=True here?
9898                 self._set_instance_obj_error_state(context, instance)
9899 
9900     @wrap_exception()
9901     def add_aggregate_host(self, context, aggregate, host, slave_info):
9902         """Notify hypervisor of change (for hypervisor pools)."""
9903         try:
9904             self.driver.add_to_aggregate(context, aggregate, host,
9905                                          slave_info=slave_info)
9906         except NotImplementedError:
9907             LOG.debug('Hypervisor driver does not support '
9908                       'add_aggregate_host')
9909         except exception.AggregateError:
9910             with excutils.save_and_reraise_exception():
9911                 self.driver.undo_aggregate_operation(
9912                                     context,
9913                                     aggregate.delete_host,
9914                                     aggregate, host)
9915 
9916     @wrap_exception()
9917     def remove_aggregate_host(self, context, host, slave_info, aggregate):
9918         """Removes a host from a physical hypervisor pool."""
9919         try:
9920             self.driver.remove_from_aggregate(context, aggregate, host,
9921                                               slave_info=slave_info)
9922         except NotImplementedError:
9923             LOG.debug('Hypervisor driver does not support '
9924                       'remove_aggregate_host')
9925         except (exception.AggregateError,
9926                 exception.InvalidAggregateAction) as e:
9927             with excutils.save_and_reraise_exception():
9928                 self.driver.undo_aggregate_operation(
9929                                     context,
9930                                     aggregate.add_host,
9931                                     aggregate, host,
9932                                     isinstance(e, exception.AggregateError))
9933 
9934     def _process_instance_event(self, instance, event):
9935         _event = self.instance_events.pop_instance_event(instance, event)
9936         if _event:
9937             LOG.debug('Processing event %(event)s',
9938                       {'event': event.key}, instance=instance)
9939             _event.send(event)
9940         else:
9941             # If it's a network-vif-unplugged event and the instance is being
9942             # deleted or live migrated then we don't need to make this a
9943             # warning as it's expected. There are other expected things which
9944             # could trigger this event like detaching an interface, but we
9945             # don't have a task state for that.
9946             # TODO(mriedem): We have other move operations and things like
9947             # hard reboot (probably rebuild as well) which trigger this event
9948             # but nothing listens for network-vif-unplugged. We should either
9949             # handle those other known cases or consider just not logging a
9950             # warning if we get this event and the instance is undergoing some
9951             # task state transition.
9952             if (event.name == 'network-vif-unplugged' and
9953                     instance.task_state in (
9954                         task_states.DELETING, task_states.MIGRATING)):
9955                 LOG.debug('Received event %s for instance with task_state %s.',
9956                           event.key, instance.task_state, instance=instance)
9957             else:
9958                 LOG.warning('Received unexpected event %(event)s for '
9959                             'instance with vm_state %(vm_state)s and '
9960                             'task_state %(task_state)s.',
9961                             {'event': event.key,
9962                              'vm_state': instance.vm_state,
9963                              'task_state': instance.task_state},
9964                             instance=instance)
9965 
9966     def _process_instance_vif_deleted_event(self, context, instance,
9967                                             deleted_vif_id):
9968         # If an attached port is deleted by neutron, it needs to
9969         # be detached from the instance.
9970         # And info cache needs to be updated.
9971         network_info = instance.info_cache.network_info
9972         for index, vif in enumerate(network_info):
9973             if vif['id'] == deleted_vif_id:
9974                 LOG.info('Neutron deleted interface %(intf)s; '
9975                          'detaching it from the instance and '
9976                          'deleting it from the info cache',
9977                          {'intf': vif['id']},
9978                          instance=instance)
9979                 profile = vif.get('profile', {}) or {}  # profile can be None
9980                 if profile.get('allocation'):
9981                     LOG.error(
9982                         'The bound port %(port_id)s is deleted in Neutron but '
9983                         'the resource allocation on the resource provider '
9984                         '%(rp_uuid)s is leaked until the server '
9985                         '%(server_uuid)s is deleted.',
9986                         {'port_id': vif['id'],
9987                          'rp_uuid': vif['profile']['allocation'],
9988                          'server_uuid': instance.uuid})
9989 
9990                 del network_info[index]
9991                 base_net_api.update_instance_cache_with_nw_info(
9992                                  self.network_api, context,
9993                                  instance,
9994                                  nw_info=network_info)
9995                 try:
9996                     self.driver.detach_interface(context, instance, vif)
9997                 except NotImplementedError:
9998                     # Not all virt drivers support attach/detach of interfaces
9999                     # yet (like Ironic), so just ignore this.
10000                     pass
10001                 except exception.NovaException as ex:
10002                     # If the instance was deleted before the interface was
10003                     # detached, just log it at debug.
10004                     log_level = (logging.DEBUG
10005                                  if isinstance(ex, exception.InstanceNotFound)
10006                                  else logging.WARNING)
10007                     LOG.log(log_level,
10008                             "Detach interface failed, "
10009                             "port_id=%(port_id)s, reason: %(msg)s",
10010                             {'port_id': deleted_vif_id, 'msg': ex},
10011                             instance=instance)
10012                 break
10013 
10014     @wrap_instance_event(prefix='compute')
10015     @wrap_instance_fault
10016     def extend_volume(self, context, instance, extended_volume_id):
10017 
10018         # If an attached volume is extended by cinder, it needs to
10019         # be extended by virt driver so host can detect its new size.
10020         # And bdm needs to be updated.
10021         LOG.debug('Handling volume-extended event for volume %(vol)s',
10022                   {'vol': extended_volume_id}, instance=instance)
10023 
10024         try:
10025             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
10026                    context, extended_volume_id, instance.uuid)
10027         except exception.NotFound:
10028             LOG.warning('Extend volume failed, '
10029                         'volume %(vol)s is not attached to instance.',
10030                         {'vol': extended_volume_id},
10031                         instance=instance)
10032             return
10033 
10034         LOG.info('Cinder extended volume %(vol)s; '
10035                  'extending it to detect new size',
10036                  {'vol': extended_volume_id},
10037                  instance=instance)
10038         volume = self.volume_api.get(context, bdm.volume_id)
10039 
10040         if bdm.connection_info is None:
10041             LOG.warning('Extend volume failed, '
10042                         'attached volume %(vol)s has no connection_info',
10043                         {'vol': extended_volume_id},
10044                         instance=instance)
10045             return
10046 
10047         connection_info = jsonutils.loads(bdm.connection_info)
10048         bdm.volume_size = volume['size']
10049         bdm.save()
10050 
10051         if not self.driver.capabilities.get('supports_extend_volume', False):
10052             raise exception.ExtendVolumeNotSupported()
10053 
10054         try:
10055             self.driver.extend_volume(connection_info,
10056                                       instance,
10057                                       bdm.volume_size * units.Gi)
10058         except Exception as ex:
10059             LOG.warning('Extend volume failed, '
10060                         'volume_id=%(volume_id)s, reason: %(msg)s',
10061                         {'volume_id': extended_volume_id, 'msg': ex},
10062                         instance=instance)
10063             raise
10064 
10065     @staticmethod
10066     def _is_state_valid_for_power_update_event(instance, target_power_state):
10067         """Check if the current state of the instance allows it to be
10068         a candidate for the power-update event.
10069 
10070         :param instance: The nova instance object.
10071         :param target_power_state: The desired target power state; this should
10072                                    either be "POWER_ON" or "POWER_OFF".
10073         :returns Boolean: True if the instance can be subjected to the
10074                           power-update event.
10075         """
10076         if ((target_power_state == external_event_obj.POWER_ON and
10077                 instance.task_state is None and
10078                 instance.vm_state == vm_states.STOPPED and
10079                 instance.power_state == power_state.SHUTDOWN) or
10080             (target_power_state == external_event_obj.POWER_OFF and
10081                 instance.task_state is None and
10082                 instance.vm_state == vm_states.ACTIVE and
10083                 instance.power_state == power_state.RUNNING)):
10084             return True
10085         return False
10086 
10087     @wrap_exception()
10088     @reverts_task_state
10089     @wrap_instance_event(prefix='compute')
10090     @wrap_instance_fault
10091     def power_update(self, context, instance, target_power_state):
10092         """Power update of an instance prompted by an external event.
10093         :param context: The API request context.
10094         :param instance: The nova instance object.
10095         :param target_power_state: The desired target power state;
10096                                    this should either be "POWER_ON" or
10097                                    "POWER_OFF".
10098         """
10099 
10100         @utils.synchronized(instance.uuid)
10101         def do_power_update():
10102             LOG.debug('Handling power-update event with target_power_state %s '
10103                       'for instance', target_power_state, instance=instance)
10104             if not self._is_state_valid_for_power_update_event(
10105                     instance, target_power_state):
10106                 pow_state = fields.InstancePowerState.from_index(
10107                     instance.power_state)
10108                 LOG.info('The power-update %(tag)s event for instance '
10109                          '%(uuid)s is a no-op since the instance is in '
10110                          'vm_state %(vm_state)s, task_state '
10111                          '%(task_state)s and power_state '
10112                          '%(power_state)s.',
10113                          {'tag': target_power_state, 'uuid': instance.uuid,
10114                          'vm_state': instance.vm_state,
10115                          'task_state': instance.task_state,
10116                          'power_state': pow_state})
10117                 return
10118             LOG.debug("Trying to %s instance",
10119                       target_power_state, instance=instance)
10120             if target_power_state == external_event_obj.POWER_ON:
10121                 action = fields.NotificationAction.POWER_ON
10122                 notification_name = "power_on."
10123                 instance.task_state = task_states.POWERING_ON
10124             else:
10125                 # It's POWER_OFF
10126                 action = fields.NotificationAction.POWER_OFF
10127                 notification_name = "power_off."
10128                 instance.task_state = task_states.POWERING_OFF
10129                 instance.progress = 0
10130 
10131             try:
10132                 # Note that the task_state is set here rather than the API
10133                 # because this is a best effort operation and deferring
10134                 # updating the task_state until we get to the compute service
10135                 # avoids error handling in the API and needing to account for
10136                 # older compute services during rolling upgrades from Stein.
10137                 # If we lose a race, UnexpectedTaskStateError is handled
10138                 # below.
10139                 instance.save(expected_task_state=[None])
10140                 self._notify_about_instance_usage(context, instance,
10141                                                   notification_name + "start")
10142                 compute_utils.notify_about_instance_action(context, instance,
10143                     self.host, action=action,
10144                     phase=fields.NotificationPhase.START)
10145                 # UnexpectedTaskStateError raised from the driver will be
10146                 # handled below and not result in a fault, error notification
10147                 # or failure of the instance action. Other driver errors like
10148                 # NotImplementedError will be record a fault, send an error
10149                 # notification and mark the instance action as failed.
10150                 self.driver.power_update_event(instance, target_power_state)
10151                 self._notify_about_instance_usage(context, instance,
10152                                                   notification_name + "end")
10153                 compute_utils.notify_about_instance_action(context, instance,
10154                     self.host, action=action,
10155                     phase=fields.NotificationPhase.END)
10156             except exception.UnexpectedTaskStateError as e:
10157                 # Handling the power-update event is best effort and if we lost
10158                 # a race with some other action happening to the instance we
10159                 # just log it and return rather than fail the action.
10160                 LOG.info("The power-update event was possibly preempted: %s ",
10161                          e.format_message(), instance=instance)
10162                 return
10163         do_power_update()
10164 
10165     @wrap_exception()
10166     def external_instance_event(self, context, instances, events):
10167         # NOTE(danms): Some event types are handled by the manager, such
10168         # as when we're asked to update the instance's info_cache. If it's
10169         # not one of those, look for some thread(s) waiting for the event and
10170         # unblock them if so.
10171         for event in events:
10172             instance = [inst for inst in instances
10173                         if inst.uuid == event.instance_uuid][0]
10174             LOG.debug('Received event %(event)s',
10175                       {'event': event.key},
10176                       instance=instance)
10177             if event.name == 'network-changed':
10178                 try:
10179                     LOG.debug('Refreshing instance network info cache due to '
10180                               'event %s.', event.key, instance=instance)
10181                     self.network_api.get_instance_nw_info(
10182                         context, instance, refresh_vif_id=event.tag)
10183                 except exception.NotFound as e:
10184                     LOG.info('Failed to process external instance event '
10185                              '%(event)s due to: %(error)s',
10186                              {'event': event.key, 'error': six.text_type(e)},
10187                              instance=instance)
10188             elif event.name == 'network-vif-deleted':
10189                 try:
10190                     self._process_instance_vif_deleted_event(context,
10191                                                              instance,
10192                                                              event.tag)
10193                 except exception.NotFound as e:
10194                     LOG.info('Failed to process external instance event '
10195                              '%(event)s due to: %(error)s',
10196                              {'event': event.key, 'error': six.text_type(e)},
10197                              instance=instance)
10198             elif event.name == 'volume-extended':
10199                 self.extend_volume(context, instance, event.tag)
10200             elif event.name == 'power-update':
10201                 self.power_update(context, instance, event.tag)
10202             else:
10203                 self._process_instance_event(instance, event)
10204 
10205     @periodic_task.periodic_task(spacing=CONF.image_cache.manager_interval,
10206                                  external_process_ok=True)
10207     def _run_image_cache_manager_pass(self, context):
10208         """Run a single pass of the image cache manager."""
10209 
10210         if not self.driver.capabilities.get("has_imagecache", False):
10211             return
10212 
10213         # Determine what other nodes use this storage
10214         storage_users.register_storage_use(CONF.instances_path, CONF.host)
10215         nodes = storage_users.get_storage_users(CONF.instances_path)
10216 
10217         # Filter all_instances to only include those nodes which share this
10218         # storage path.
10219         # TODO(mikal): this should be further refactored so that the cache
10220         # cleanup code doesn't know what those instances are, just a remote
10221         # count, and then this logic should be pushed up the stack.
10222         filters = {'deleted': False,
10223                    'soft_deleted': True,
10224                    'host': nodes}
10225         filtered_instances = objects.InstanceList.get_by_filters(context,
10226                                  filters, expected_attrs=[], use_slave=True)
10227 
10228         self.driver.manage_image_cache(context, filtered_instances)
10229 
10230     def cache_images(self, context, image_ids):
10231         """Ask the virt driver to pre-cache a set of base images.
10232 
10233         :param context: The RequestContext
10234         :param image_ids: The image IDs to be cached
10235         :return: A dict, keyed by image-id where the values are one of:
10236                  'cached' if the image was downloaded,
10237                  'existing' if the image was already in the cache,
10238                  'unsupported' if the virt driver does not support caching,
10239                  'error' if the virt driver raised an exception.
10240         """
10241 
10242         results = {}
10243 
10244         LOG.info('Caching %i image(s) by request', len(image_ids))
10245         for image_id in image_ids:
10246             try:
10247                 cached = self.driver.cache_image(context, image_id)
10248                 if cached:
10249                     results[image_id] = 'cached'
10250                 else:
10251                     results[image_id] = 'existing'
10252             except NotImplementedError:
10253                 LOG.warning('Virt driver does not support image pre-caching;'
10254                             ' ignoring request')
10255                 # NOTE(danms): Yes, technically we could short-circuit here to
10256                 # avoid trying the rest of the images, but it's very cheap to
10257                 # just keep hitting the NotImplementedError to keep the logic
10258                 # clean.
10259                 results[image_id] = 'unsupported'
10260             except Exception as e:
10261                 results[image_id] = 'error'
10262                 LOG.error('Failed to cache image %(image_id)s: %(err)s',
10263                           {'image_id': image_id,
10264                            'err': e})
10265 
10266         return results
10267 
10268     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10269     def _run_pending_deletes(self, context):
10270         """Retry any pending instance file deletes."""
10271         LOG.debug('Cleaning up deleted instances')
10272         filters = {'deleted': True,
10273                    'soft_deleted': False,
10274                    'host': CONF.host,
10275                    'cleaned': False}
10276         attrs = ['system_metadata']
10277         with utils.temporary_mutation(context, read_deleted='yes'):
10278             instances = objects.InstanceList.get_by_filters(
10279                 context, filters, expected_attrs=attrs, use_slave=True)
10280         LOG.debug('There are %d instances to clean', len(instances))
10281 
10282         for instance in instances:
10283             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
10284             LOG.debug('Instance has had %(attempts)s of %(max)s '
10285                       'cleanup attempts',
10286                       {'attempts': attempts,
10287                        'max': CONF.maximum_instance_delete_attempts},
10288                       instance=instance)
10289             if attempts < CONF.maximum_instance_delete_attempts:
10290                 success = self.driver.delete_instance_files(instance)
10291 
10292                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
10293                 if success:
10294                     instance.cleaned = True
10295                 with utils.temporary_mutation(context, read_deleted='yes'):
10296                     instance.save()
10297 
10298     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10299     def _cleanup_incomplete_migrations(self, context):
10300         """Delete instance files on failed resize/revert-resize operation
10301 
10302         During resize/revert-resize operation, if that instance gets deleted
10303         in-between then instance files might remain either on source or
10304         destination compute node because of race condition.
10305         """
10306         LOG.debug('Cleaning up deleted instances with incomplete migration ')
10307         migration_filters = {'host': CONF.host,
10308                              'status': 'error'}
10309         migrations = objects.MigrationList.get_by_filters(context,
10310                                                           migration_filters)
10311 
10312         if not migrations:
10313             return
10314 
10315         inst_uuid_from_migrations = set([migration.instance_uuid for migration
10316                                          in migrations])
10317 
10318         inst_filters = {'deleted': True, 'soft_deleted': False,
10319                         'uuid': inst_uuid_from_migrations}
10320         attrs = ['info_cache', 'security_groups', 'system_metadata']
10321         with utils.temporary_mutation(context, read_deleted='yes'):
10322             instances = objects.InstanceList.get_by_filters(
10323                 context, inst_filters, expected_attrs=attrs, use_slave=True)
10324 
10325         for instance in instances:
10326             if instance.host != CONF.host:
10327                 for migration in migrations:
10328                     if instance.uuid == migration.instance_uuid:
10329                         # Delete instance files if not cleanup properly either
10330                         # from the source or destination compute nodes when
10331                         # the instance is deleted during resizing.
10332                         self.driver.delete_instance_files(instance)
10333                         try:
10334                             migration.status = 'failed'
10335                             migration.save()
10336                         except exception.MigrationNotFound:
10337                             LOG.warning("Migration %s is not found.",
10338                                         migration.id,
10339                                         instance=instance)
10340                         break
10341 
10342     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10343                                    exception.QemuGuestAgentNotEnabled,
10344                                    exception.NovaException,
10345                                    NotImplementedError)
10346     @wrap_exception()
10347     def quiesce_instance(self, context, instance):
10348         """Quiesce an instance on this host."""
10349         context = context.elevated()
10350         image_meta = objects.ImageMeta.from_instance(instance)
10351         self.driver.quiesce(context, instance, image_meta)
10352 
10353     def _wait_for_snapshots_completion(self, context, mapping):
10354         for mapping_dict in mapping:
10355             if mapping_dict.get('source_type') == 'snapshot':
10356 
10357                 def _wait_snapshot():
10358                     snapshot = self.volume_api.get_snapshot(
10359                         context, mapping_dict['snapshot_id'])
10360                     if snapshot.get('status') != 'creating':
10361                         raise loopingcall.LoopingCallDone()
10362 
10363                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
10364                 timer.start(interval=0.5).wait()
10365 
10366     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10367                                    exception.QemuGuestAgentNotEnabled,
10368                                    exception.NovaException,
10369                                    NotImplementedError)
10370     @wrap_exception()
10371     def unquiesce_instance(self, context, instance, mapping=None):
10372         """Unquiesce an instance on this host.
10373 
10374         If snapshots' image mapping is provided, it waits until snapshots are
10375         completed before unqueiscing.
10376         """
10377         context = context.elevated()
10378         if mapping:
10379             try:
10380                 self._wait_for_snapshots_completion(context, mapping)
10381             except Exception as error:
10382                 LOG.exception("Exception while waiting completion of "
10383                               "volume snapshots: %s",
10384                               error, instance=instance)
10385         image_meta = objects.ImageMeta.from_instance(instance)
10386         self.driver.unquiesce(context, instance, image_meta)
10387 
10388     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10389     def _cleanup_expired_console_auth_tokens(self, context):
10390         """Remove all expired console auth tokens.
10391 
10392         Console authorization tokens and their connection data are stored
10393         in the database when a user asks for a console connection to an
10394         instance. After a time they expire. We periodically remove any expired
10395         tokens from the database.
10396         """
10397         objects.ConsoleAuthToken.clean_expired_console_auths(context)
10398 
10399     def _claim_pci_for_instance_vifs(self, ctxt, instance):
10400         """Claim PCI devices for the instance's VIFs on the compute node
10401 
10402         :param ctxt: Context
10403         :param instance: Instance object
10404         :return: <port ID: PciDevice> mapping for the VIFs that yielded a
10405                 PCI claim on the compute node
10406         """
10407         pci_req_id_to_port_id = {}
10408         pci_reqs = []
10409         port_id_to_pci_dev = {}
10410 
10411         for vif in instance.get_network_info():
10412             pci_req = pci_req_module.get_instance_pci_request_from_vif(
10413                 ctxt,
10414                 instance,
10415                 vif)
10416             if pci_req:
10417                 pci_req_id_to_port_id[pci_req.request_id] = vif['id']
10418                 pci_reqs.append(pci_req)
10419 
10420         if pci_reqs:
10421             # Create PCI requests and claim against PCI resource tracker
10422             # NOTE(adrianc): We claim against the same requests as on the
10423             # source node.
10424             vif_pci_requests = objects.InstancePCIRequests(
10425                 requests=pci_reqs,
10426                 instance_uuid=instance.uuid)
10427 
10428             claimed_pci_devices_objs = self.rt.claim_pci_devices(
10429                 ctxt,
10430                 vif_pci_requests)
10431 
10432             # Update VIFMigrateData profile with the newly claimed PCI
10433             # device
10434             for pci_dev in claimed_pci_devices_objs:
10435                 LOG.debug("PCI device: %s Claimed on destination node",
10436                           pci_dev.address)
10437                 port_id = pci_req_id_to_port_id[pci_dev.request_id]
10438                 port_id_to_pci_dev[port_id] = pci_dev
10439 
10440         return port_id_to_pci_dev
10441 
10442     def _update_migrate_vifs_profile_with_pci(self,
10443                                               migrate_vifs,
10444                                               port_id_to_pci_dev):
10445         """Update migrate vifs profile with the claimed PCI devices
10446 
10447         :param migrate_vifs: list of VIFMigrateData objects
10448         :param port_id_to_pci_dev: a <port_id: PciDevice> mapping
10449         :return: None.
10450         """
10451         for mig_vif in migrate_vifs:
10452             port_id = mig_vif.port_id
10453             if port_id not in port_id_to_pci_dev:
10454                 continue
10455 
10456             pci_dev = port_id_to_pci_dev[port_id]
10457             profile = copy.deepcopy(mig_vif.source_vif['profile'])
10458             profile['pci_slot'] = pci_dev.address
10459             profile['pci_vendor_info'] = ':'.join([pci_dev.vendor_id,
10460                                                    pci_dev.product_id])
10461             mig_vif.profile = profile
10462             LOG.debug("Updating migrate VIF profile for port %(port_id)s:"
10463                       "%(profile)s", {'port_id': port_id,
10464                                       'profile': profile})
