Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import collections
29 from collections import deque
30 import contextlib
31 import errno
32 import functools
33 import glob
34 import itertools
35 import mmap
36 import operator
37 import os
38 import pwd
39 import shutil
40 import tempfile
41 import time
42 import uuid
43 
44 from castellan import key_manager
45 import eventlet
46 from eventlet import greenthread
47 from eventlet import tpool
48 from lxml import etree
49 from os_brick import encryptors
50 from os_brick import exception as brick_exception
51 from os_brick.initiator import connector
52 from oslo_concurrency import processutils
53 from oslo_log import log as logging
54 from oslo_serialization import jsonutils
55 from oslo_service import loopingcall
56 from oslo_utils import excutils
57 from oslo_utils import fileutils
58 from oslo_utils import importutils
59 from oslo_utils import strutils
60 from oslo_utils import timeutils
61 from oslo_utils import units
62 from oslo_utils import uuidutils
63 import six
64 from six.moves import range
65 
66 from nova.api.metadata import base as instance_metadata
67 from nova import block_device
68 from nova.compute import power_state
69 from nova.compute import task_states
70 from nova.compute import utils as compute_utils
71 import nova.conf
72 from nova.console import serial as serial_console
73 from nova.console import type as ctype
74 from nova import context as nova_context
75 from nova import exception
76 from nova.i18n import _
77 from nova import image
78 from nova.network import model as network_model
79 from nova import objects
80 from nova.objects import diagnostics as diagnostics_obj
81 from nova.objects import fields
82 from nova.objects import migrate_data as migrate_data_obj
83 from nova.pci import manager as pci_manager
84 from nova.pci import utils as pci_utils
85 import nova.privsep.libvirt
86 import nova.privsep.path
87 from nova import utils
88 from nova import version
89 from nova.virt import block_device as driver_block_device
90 from nova.virt import configdrive
91 from nova.virt.disk import api as disk_api
92 from nova.virt.disk.vfs import guestfs
93 from nova.virt import driver
94 from nova.virt import firewall
95 from nova.virt import hardware
96 from nova.virt.image import model as imgmodel
97 from nova.virt import images
98 from nova.virt.libvirt import blockinfo
99 from nova.virt.libvirt import config as vconfig
100 from nova.virt.libvirt import firewall as libvirt_firewall
101 from nova.virt.libvirt import guest as libvirt_guest
102 from nova.virt.libvirt import host
103 from nova.virt.libvirt import imagebackend
104 from nova.virt.libvirt import imagecache
105 from nova.virt.libvirt import instancejobtracker
106 from nova.virt.libvirt import migration as libvirt_migrate
107 from nova.virt.libvirt.storage import dmcrypt
108 from nova.virt.libvirt.storage import lvm
109 from nova.virt.libvirt.storage import rbd_utils
110 from nova.virt.libvirt import utils as libvirt_utils
111 from nova.virt.libvirt import vif as libvirt_vif
112 from nova.virt.libvirt.volume import mount
113 from nova.virt.libvirt.volume import remotefs
114 from nova.virt import netutils
115 from nova.volume import cinder
116 
117 libvirt = None
118 
119 uefi_logged = False
120 
121 LOG = logging.getLogger(__name__)
122 
123 CONF = nova.conf.CONF
124 
125 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
126     libvirt_firewall.__name__,
127     libvirt_firewall.IptablesFirewallDriver.__name__)
128 
129 DEFAULT_UEFI_LOADER_PATH = {
130     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
131     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
132 }
133 
134 MAX_CONSOLE_BYTES = 100 * units.Ki
135 
136 # The libvirt driver will prefix any disable reason codes with this string.
137 DISABLE_PREFIX = 'AUTO: '
138 # Disable reason for the service which was enabled or disabled without reason
139 DISABLE_REASON_UNDEFINED = None
140 
141 # Guest config console string
142 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
143 
144 GuestNumaConfig = collections.namedtuple(
145     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
146 
147 InjectionInfo = collections.namedtuple(
148     'InjectionInfo', ['network_info', 'files', 'admin_pass'])
149 
150 libvirt_volume_drivers = [
151     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
152     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
153     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
154     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
155     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
156     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
157     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
158     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
159     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
160     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
161     'fibre_channel='
162         'nova.virt.libvirt.volume.fibrechannel.'
163         'LibvirtFibreChannelVolumeDriver',
164     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
165     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
166     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
167     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
168     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
169     'vzstorage='
170         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
171     'veritas_hyperscale='
172         'nova.virt.libvirt.volume.vrtshyperscale.'
173         'LibvirtHyperScaleVolumeDriver',
174 ]
175 
176 
177 def patch_tpool_proxy():
178     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
179     or __repr__() calls. See bug #962840 for details.
180     We perform a monkey patch to replace those two instance methods.
181     """
182     def str_method(self):
183         return str(self._obj)
184 
185     def repr_method(self):
186         return repr(self._obj)
187 
188     tpool.Proxy.__str__ = str_method
189     tpool.Proxy.__repr__ = repr_method
190 
191 
192 patch_tpool_proxy()
193 
194 # For information about when MIN_LIBVIRT_VERSION and
195 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
196 #
197 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
198 #
199 # Currently this is effectively the min version for i686/x86_64
200 # + KVM/QEMU, as other architectures/hypervisors require newer
201 # versions. Over time, this will become a common min version
202 # for all architectures/hypervisors, as this value rises to
203 # meet them.
204 MIN_LIBVIRT_VERSION = (1, 2, 9)
205 MIN_QEMU_VERSION = (2, 1, 0)
206 # TODO(berrange): Re-evaluate this at start of each release cycle
207 # to decide if we want to plan a future min version bump.
208 # MIN_LIBVIRT_VERSION can be updated to match this after
209 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
210 # one cycle
211 NEXT_MIN_LIBVIRT_VERSION = (1, 3, 1)
212 NEXT_MIN_QEMU_VERSION = (2, 5, 0)
213 
214 # When the above version matches/exceeds this version
215 # delete it & corresponding code using it
216 # Libvirt version 1.2.17 is required for successful block live migration
217 # of vm booted from image with attached devices
218 MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION = (1, 2, 17)
219 # PowerPC based hosts that support NUMA using libvirt
220 MIN_LIBVIRT_NUMA_VERSION_PPC = (1, 2, 19)
221 # Versions of libvirt with known NUMA topology issues
222 # See bug #1449028
223 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
224 # Versions of libvirt with broken cpu pinning support. This excludes
225 # versions of libvirt with broken NUMA support since pinning needs
226 # NUMA
227 # See bug #1438226
228 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
229 
230 # Virtuozzo driver support
231 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
232 MIN_LIBVIRT_VIRTUOZZO_VERSION = (1, 2, 12)
233 
234 # Ability to set the user guest password with Qemu
235 MIN_LIBVIRT_SET_ADMIN_PASSWD = (1, 2, 16)
236 
237 # Ability to set the user guest password with parallels
238 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
239 
240 # s/390 & s/390x architectures with KVM
241 MIN_LIBVIRT_KVM_S390_VERSION = (1, 2, 13)
242 MIN_QEMU_S390_VERSION = (2, 3, 0)
243 
244 # libvirt < 1.3 reported virt_functions capability
245 # only when VFs are enabled.
246 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
247 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
248 
249 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
250 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
251 MIN_QEMU_VIRTLOGD = (2, 7, 0)
252 
253 # ppc64/ppc64le architectures with KVM
254 # NOTE(rfolco): Same levels for Libvirt/Qemu on Big Endian and Little
255 # Endian giving the nuance around guest vs host architectures
256 MIN_LIBVIRT_KVM_PPC64_VERSION = (1, 2, 12)
257 
258 # Names of the types that do not get compressed during migration
259 NO_COMPRESSION_TYPES = ('qcow2',)
260 
261 
262 # number of serial console limit
263 QEMU_MAX_SERIAL_PORTS = 4
264 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
265 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
266 
267 # realtime support
268 MIN_LIBVIRT_REALTIME_VERSION = (1, 2, 13)
269 
270 # libvirt postcopy support
271 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
272 
273 # qemu postcopy support
274 MIN_QEMU_POSTCOPY_VERSION = (2, 5, 0)
275 
276 MIN_LIBVIRT_OTHER_ARCH = {
277     fields.Architecture.S390: MIN_LIBVIRT_KVM_S390_VERSION,
278     fields.Architecture.S390X: MIN_LIBVIRT_KVM_S390_VERSION,
279     fields.Architecture.PPC: MIN_LIBVIRT_KVM_PPC64_VERSION,
280     fields.Architecture.PPC64: MIN_LIBVIRT_KVM_PPC64_VERSION,
281     fields.Architecture.PPC64LE: MIN_LIBVIRT_KVM_PPC64_VERSION,
282 }
283 
284 MIN_QEMU_OTHER_ARCH = {
285     fields.Architecture.S390: MIN_QEMU_S390_VERSION,
286     fields.Architecture.S390X: MIN_QEMU_S390_VERSION,
287 }
288 
289 # perf events support
290 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
291 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
292 
293 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
294                                 'mbml': 'mbm_local',
295                                 'mbmt': 'mbm_total',
296                                }
297 
298 
299 class LibvirtDriver(driver.ComputeDriver):
300     capabilities = {
301         "has_imagecache": True,
302         "supports_recreate": True,
303         "supports_migrate_to_same_host": False,
304         "supports_attach_interface": True,
305         "supports_device_tagging": True,
306         "supports_tagged_attach_interface": True,
307         "supports_tagged_attach_volume": True,
308         "supports_extend_volume": True,
309     }
310 
311     def __init__(self, virtapi, read_only=False):
312         super(LibvirtDriver, self).__init__(virtapi)
313 
314         global libvirt
315         if libvirt is None:
316             libvirt = importutils.import_module('libvirt')
317             libvirt_migrate.libvirt = libvirt
318 
319         self._host = host.Host(self._uri(), read_only,
320                                lifecycle_event_handler=self.emit_event,
321                                conn_event_handler=self._handle_conn_event)
322         self._initiator = None
323         self._fc_wwnns = None
324         self._fc_wwpns = None
325         self._caps = None
326         self._supported_perf_events = []
327         self.firewall_driver = firewall.load_driver(
328             DEFAULT_FIREWALL_DRIVER,
329             host=self._host)
330 
331         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
332 
333         # TODO(mriedem): Long-term we should load up the volume drivers on
334         # demand as needed rather than doing this on startup, as there might
335         # be unsupported volume drivers in this list based on the underlying
336         # platform.
337         self.volume_drivers = self._get_volume_drivers()
338 
339         self._disk_cachemode = None
340         self.image_cache_manager = imagecache.ImageCacheManager()
341         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
342 
343         self.disk_cachemodes = {}
344 
345         self.valid_cachemodes = ["default",
346                                  "none",
347                                  "writethrough",
348                                  "writeback",
349                                  "directsync",
350                                  "unsafe",
351                                 ]
352         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
353                                                                       'qemu')
354 
355         for mode_str in CONF.libvirt.disk_cachemodes:
356             disk_type, sep, cache_mode = mode_str.partition('=')
357             if cache_mode not in self.valid_cachemodes:
358                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
359                             'for disk type %(disk_type)s.',
360                             {'cache_mode': cache_mode, 'disk_type': disk_type})
361                 continue
362             self.disk_cachemodes[disk_type] = cache_mode
363 
364         self._volume_api = cinder.API()
365         self._image_api = image.API()
366 
367         sysinfo_serial_funcs = {
368             'none': lambda: None,
369             'hardware': self._get_host_sysinfo_serial_hardware,
370             'os': self._get_host_sysinfo_serial_os,
371             'auto': self._get_host_sysinfo_serial_auto,
372         }
373 
374         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
375             CONF.libvirt.sysinfo_serial)
376 
377         self.job_tracker = instancejobtracker.InstanceJobTracker()
378         self._remotefs = remotefs.RemoteFilesystem()
379 
380         self._live_migration_flags = self._block_migration_flags = 0
381         self.active_migrations = {}
382 
383         # Compute reserved hugepages from conf file at the very
384         # beginning to ensure any syntax error will be reported and
385         # avoid any re-calculation when computing resources.
386         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
387 
388     def _get_volume_drivers(self):
389         driver_registry = dict()
390 
391         for driver_str in libvirt_volume_drivers:
392             driver_type, _sep, driver = driver_str.partition('=')
393             driver_class = importutils.import_class(driver)
394             try:
395                 driver_registry[driver_type] = driver_class(self._host)
396             except brick_exception.InvalidConnectorProtocol:
397                 LOG.debug('Unable to load volume driver %s. It is not '
398                           'supported on this host.', driver)
399 
400         return driver_registry
401 
402     @property
403     def disk_cachemode(self):
404         if self._disk_cachemode is None:
405             # We prefer 'none' for consistent performance, host crash
406             # safety & migration correctness by avoiding host page cache.
407             # Some filesystems don't support O_DIRECT though. For those we
408             # fallback to 'writethrough' which gives host crash safety, and
409             # is safe for migration provided the filesystem is cache coherent
410             # (cluster filesystems typically are, but things like NFS are not).
411             self._disk_cachemode = "none"
412             if not self._supports_direct_io(CONF.instances_path):
413                 self._disk_cachemode = "writethrough"
414         return self._disk_cachemode
415 
416     def _set_cache_mode(self, conf):
417         """Set cache mode on LibvirtConfigGuestDisk object."""
418         try:
419             source_type = conf.source_type
420             driver_cache = conf.driver_cache
421         except AttributeError:
422             return
423 
424         cache_mode = self.disk_cachemodes.get(source_type,
425                                               driver_cache)
426         conf.driver_cache = cache_mode
427 
428     def _do_quality_warnings(self):
429         """Warn about untested driver configurations.
430 
431         This will log a warning message about untested driver or host arch
432         configurations to indicate to administrators that the quality is
433         unknown. Currently, only qemu or kvm on intel 32- or 64-bit systems
434         is tested upstream.
435         """
436         caps = self._host.get_capabilities()
437         hostarch = caps.host.cpu.arch
438         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
439             hostarch not in (fields.Architecture.I686,
440                              fields.Architecture.X86_64)):
441             LOG.warning('The libvirt driver is not tested on '
442                         '%(type)s/%(arch)s by the OpenStack project and '
443                         'thus its quality can not be ensured. For more '
444                         'information, see: https://docs.openstack.org/'
445                         'nova/latest/user/support-matrix.html',
446                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
447 
448     def _handle_conn_event(self, enabled, reason):
449         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
450                  {'enabled': enabled, 'reason': reason})
451         self._set_host_enabled(enabled, reason)
452 
453     def _version_to_string(self, version):
454         return '.'.join([str(x) for x in version])
455 
456     def init_host(self, host):
457         self._host.initialize()
458 
459         self._do_quality_warnings()
460 
461         self._parse_migration_flags()
462 
463         self._supported_perf_events = self._get_supported_perf_events()
464 
465         if (CONF.libvirt.virt_type == 'lxc' and
466                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
467             LOG.warning("Running libvirt-lxc without user namespaces is "
468                         "dangerous. Containers spawned by Nova will be run "
469                         "as the host's root user. It is highly suggested "
470                         "that user namespaces be used in a public or "
471                         "multi-tenant environment.")
472 
473         # Stop libguestfs using KVM unless we're also configured
474         # to use this. This solves problem where people need to
475         # stop Nova use of KVM because nested-virt is broken
476         if CONF.libvirt.virt_type != "kvm":
477             guestfs.force_tcg()
478 
479         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
480             raise exception.InternalError(
481                 _('Nova requires libvirt version %s or greater.') %
482                 self._version_to_string(MIN_LIBVIRT_VERSION))
483 
484         if CONF.libvirt.virt_type in ("qemu", "kvm"):
485             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
486                 # "qemu-img info" calls are version dependent, so we need to
487                 # store the version in the images module.
488                 images.QEMU_VERSION = self._host.get_connection().getVersion()
489             else:
490                 raise exception.InternalError(
491                     _('Nova requires QEMU version %s or greater.') %
492                     self._version_to_string(MIN_QEMU_VERSION))
493 
494         if CONF.libvirt.virt_type == 'parallels':
495             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
496                 raise exception.InternalError(
497                     _('Nova requires Virtuozzo version %s or greater.') %
498                     self._version_to_string(MIN_VIRTUOZZO_VERSION))
499             if not self._host.has_min_version(MIN_LIBVIRT_VIRTUOZZO_VERSION):
500                 raise exception.InternalError(
501                     _('Running Nova with parallels virt_type requires '
502                       'libvirt version %s') %
503                     self._version_to_string(MIN_LIBVIRT_VIRTUOZZO_VERSION))
504 
505         # Give the cloud admin a heads up if we are intending to
506         # change the MIN_LIBVIRT_VERSION in the next release.
507         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
508             LOG.warning('Running Nova with a libvirt version less than '
509                         '%(version)s is deprecated. The required minimum '
510                         'version of libvirt will be raised to %(version)s '
511                         'in the next release.',
512                         {'version': self._version_to_string(
513                             NEXT_MIN_LIBVIRT_VERSION)})
514         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
515             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
516             LOG.warning('Running Nova with a QEMU version less than '
517                         '%(version)s is deprecated. The required minimum '
518                         'version of QEMU will be raised to %(version)s '
519                         'in the next release.',
520                         {'version': self._version_to_string(
521                             NEXT_MIN_QEMU_VERSION)})
522 
523         kvm_arch = fields.Architecture.from_host()
524         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
525             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
526                 not self._host.has_min_version(
527                                         MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch),
528                                         MIN_QEMU_OTHER_ARCH.get(kvm_arch))):
529             if MIN_QEMU_OTHER_ARCH.get(kvm_arch):
530                 raise exception.InternalError(
531                     _('Running Nova with qemu/kvm virt_type on %(arch)s '
532                       'requires libvirt version %(libvirt_ver)s and '
533                       'qemu version %(qemu_ver)s, or greater') %
534                     {'arch': kvm_arch,
535                      'libvirt_ver': self._version_to_string(
536                          MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch)),
537                      'qemu_ver': self._version_to_string(
538                          MIN_QEMU_OTHER_ARCH.get(kvm_arch))})
539             # no qemu version in the error message
540             raise exception.InternalError(
541                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
542                   'requires libvirt version %(libvirt_ver)s or greater') %
543                 {'arch': kvm_arch,
544                  'libvirt_ver': self._version_to_string(
545                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
546 
547     def _prepare_migration_flags(self):
548         migration_flags = 0
549 
550         migration_flags |= libvirt.VIR_MIGRATE_LIVE
551 
552         # Adding p2p flag only if xen is not in use, because xen does not
553         # support p2p migrations
554         if CONF.libvirt.virt_type != 'xen':
555             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
556 
557         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
558         # instance will remain defined on the source host
559         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
560 
561         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
562         # destination host
563         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
564 
565         live_migration_flags = block_migration_flags = migration_flags
566 
567         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
568         # will be live-migrations instead
569         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
570 
571         return (live_migration_flags, block_migration_flags)
572 
573     def _handle_live_migration_tunnelled(self, migration_flags):
574         if (CONF.libvirt.live_migration_tunnelled is None or
575                 CONF.libvirt.live_migration_tunnelled):
576             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
577         return migration_flags
578 
579     def _is_post_copy_available(self):
580         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION,
581                                       hv_ver=MIN_QEMU_POSTCOPY_VERSION):
582             return True
583         return False
584 
585     def _is_virtlogd_available(self):
586         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
587                                           MIN_QEMU_VIRTLOGD)
588 
589     def _handle_live_migration_post_copy(self, migration_flags):
590         if CONF.libvirt.live_migration_permit_post_copy:
591             if self._is_post_copy_available():
592                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
593             else:
594                 LOG.info('The live_migration_permit_post_copy is set '
595                          'to True, but it is not supported.')
596         return migration_flags
597 
598     def _handle_live_migration_auto_converge(self, migration_flags):
599         if (self._is_post_copy_available() and
600                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
601             LOG.info('The live_migration_permit_post_copy is set to '
602                      'True and post copy live migration is available '
603                      'so auto-converge will not be in use.')
604         elif CONF.libvirt.live_migration_permit_auto_converge:
605             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
606         return migration_flags
607 
608     def _parse_migration_flags(self):
609         (live_migration_flags,
610             block_migration_flags) = self._prepare_migration_flags()
611 
612         live_migration_flags = self._handle_live_migration_tunnelled(
613             live_migration_flags)
614         block_migration_flags = self._handle_live_migration_tunnelled(
615             block_migration_flags)
616 
617         live_migration_flags = self._handle_live_migration_post_copy(
618             live_migration_flags)
619         block_migration_flags = self._handle_live_migration_post_copy(
620             block_migration_flags)
621 
622         live_migration_flags = self._handle_live_migration_auto_converge(
623             live_migration_flags)
624         block_migration_flags = self._handle_live_migration_auto_converge(
625             block_migration_flags)
626 
627         self._live_migration_flags = live_migration_flags
628         self._block_migration_flags = block_migration_flags
629 
630     # TODO(sahid): This method is targeted for removal when the tests
631     # have been updated to avoid its use
632     #
633     # All libvirt API calls on the libvirt.Connect object should be
634     # encapsulated by methods on the nova.virt.libvirt.host.Host
635     # object, rather than directly invoking the libvirt APIs. The goal
636     # is to avoid a direct dependency on the libvirt API from the
637     # driver.py file.
638     def _get_connection(self):
639         return self._host.get_connection()
640 
641     _conn = property(_get_connection)
642 
643     @staticmethod
644     def _uri():
645         if CONF.libvirt.virt_type == 'uml':
646             uri = CONF.libvirt.connection_uri or 'uml:///system'
647         elif CONF.libvirt.virt_type == 'xen':
648             uri = CONF.libvirt.connection_uri or 'xen:///'
649         elif CONF.libvirt.virt_type == 'lxc':
650             uri = CONF.libvirt.connection_uri or 'lxc:///'
651         elif CONF.libvirt.virt_type == 'parallels':
652             uri = CONF.libvirt.connection_uri or 'parallels:///system'
653         else:
654             uri = CONF.libvirt.connection_uri or 'qemu:///system'
655         return uri
656 
657     @staticmethod
658     def _live_migration_uri(dest):
659         uris = {
660             'kvm': 'qemu+%s://%s/system',
661             'qemu': 'qemu+%s://%s/system',
662             'xen': 'xenmigr://%s/system',
663             'parallels': 'parallels+tcp://%s/system',
664         }
665         virt_type = CONF.libvirt.virt_type
666         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
667         uri = CONF.libvirt.live_migration_uri
668         if uri:
669             return uri % dest
670 
671         uri = uris.get(virt_type)
672         if uri is None:
673             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
674 
675         str_format = (dest,)
676         if virt_type in ('kvm', 'qemu'):
677             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
678             str_format = (scheme, dest)
679         return uris.get(virt_type) % str_format
680 
681     @staticmethod
682     def _migrate_uri(dest):
683         uri = None
684         # Only QEMU live migrations supports migrate-uri parameter
685         virt_type = CONF.libvirt.virt_type
686         if virt_type in ('qemu', 'kvm'):
687             # QEMU accept two schemes: tcp and rdma.  By default
688             # libvirt build the URI using the remote hostname and the
689             # tcp schema.
690             uri = 'tcp://%s' % dest
691         # Because dest might be of type unicode, here we might return value of
692         # type unicode as well which is not acceptable by libvirt python
693         # binding when Python 2.7 is in use, so let's convert it explicitly
694         # back to string. When Python 3.x is in use, libvirt python binding
695         # accepts unicode type so it is completely fine to do a no-op str(uri)
696         # conversion which will return value of type unicode.
697         return uri and str(uri)
698 
699     def instance_exists(self, instance):
700         """Efficient override of base instance_exists method."""
701         try:
702             self._host.get_guest(instance)
703             return True
704         except (exception.InternalError, exception.InstanceNotFound):
705             return False
706 
707     def estimate_instance_overhead(self, instance_info):
708         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
709             instance_info)
710         if isinstance(instance_info, objects.Flavor):
711             # A flavor object is passed during case of migrate
712             # TODO(sahid): We do not have any way to retrieve the
713             # image meta related to the instance so if the cpu_policy
714             # has been set in image_meta we will get an
715             # exception. Until we fix it we specifically set the
716             # cpu_policy in dedicated in an ImageMeta object so if the
717             # emulator threads has been requested nothing is going to
718             # fail.
719             image_meta = objects.ImageMeta.from_dict({"properties": {
720                 "hw_cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
721             }})
722             if (hardware.get_emulator_threads_constraint(
723                     instance_info, image_meta)
724                 == fields.CPUEmulatorThreadsPolicy.ISOLATE):
725                 overhead['vcpus'] += 1
726         else:
727             # An instance object is passed during case of spawing or a
728             # dict is passed when computing resource for an instance
729             numa_topology = hardware.instance_topology_from_instance(
730                 instance_info)
731             if numa_topology and numa_topology.emulator_threads_isolated:
732                 overhead['vcpus'] += 1
733         return overhead
734 
735     def list_instances(self):
736         names = []
737         for guest in self._host.list_guests(only_running=False):
738             names.append(guest.name)
739 
740         return names
741 
742     def list_instance_uuids(self):
743         uuids = []
744         for guest in self._host.list_guests(only_running=False):
745             uuids.append(guest.uuid)
746 
747         return uuids
748 
749     def plug_vifs(self, instance, network_info):
750         """Plug VIFs into networks."""
751         for vif in network_info:
752             self.vif_driver.plug(instance, vif)
753 
754     def _unplug_vifs(self, instance, network_info, ignore_errors):
755         """Unplug VIFs from networks."""
756         for vif in network_info:
757             try:
758                 self.vif_driver.unplug(instance, vif)
759             except exception.NovaException:
760                 if not ignore_errors:
761                     raise
762 
763     def unplug_vifs(self, instance, network_info):
764         self._unplug_vifs(instance, network_info, False)
765 
766     def _teardown_container(self, instance):
767         inst_path = libvirt_utils.get_instance_path(instance)
768         container_dir = os.path.join(inst_path, 'rootfs')
769         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
770         LOG.debug('Attempting to teardown container at path %(dir)s with '
771                   'root device: %(rootfs_dev)s',
772                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
773                   instance=instance)
774         disk_api.teardown_container(container_dir, rootfs_dev)
775 
776     def _destroy(self, instance, attempt=1):
777         try:
778             guest = self._host.get_guest(instance)
779             if CONF.serial_console.enabled:
780                 # This method is called for several events: destroy,
781                 # rebuild, hard-reboot, power-off - For all of these
782                 # events we want to release the serial ports acquired
783                 # for the guest before destroying it.
784                 serials = self._get_serial_ports_from_guest(guest)
785                 for hostname, port in serials:
786                     serial_console.release_port(host=hostname, port=port)
787         except exception.InstanceNotFound:
788             guest = None
789 
790         # If the instance is already terminated, we're still happy
791         # Otherwise, destroy it
792         old_domid = -1
793         if guest is not None:
794             try:
795                 old_domid = guest.id
796                 guest.poweroff()
797 
798             except libvirt.libvirtError as e:
799                 is_okay = False
800                 errcode = e.get_error_code()
801                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
802                     # Domain already gone. This can safely be ignored.
803                     is_okay = True
804                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
805                     # If the instance is already shut off, we get this:
806                     # Code=55 Error=Requested operation is not valid:
807                     # domain is not running
808 
809                     state = guest.get_power_state(self._host)
810                     if state == power_state.SHUTDOWN:
811                         is_okay = True
812                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
813                     errmsg = e.get_error_message()
814                     if (CONF.libvirt.virt_type == 'lxc' and
815                         errmsg == 'internal error: '
816                                   'Some processes refused to die'):
817                         # Some processes in the container didn't die
818                         # fast enough for libvirt. The container will
819                         # eventually die. For now, move on and let
820                         # the wait_for_destroy logic take over.
821                         is_okay = True
822                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
823                     LOG.warning("Cannot destroy instance, operation time out",
824                                 instance=instance)
825                     reason = _("operation time out")
826                     raise exception.InstancePowerOffFailure(reason=reason)
827                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
828                     if e.get_int1() == errno.EBUSY:
829                         # NOTE(danpb): When libvirt kills a process it sends it
830                         # SIGTERM first and waits 10 seconds. If it hasn't gone
831                         # it sends SIGKILL and waits another 5 seconds. If it
832                         # still hasn't gone then you get this EBUSY error.
833                         # Usually when a QEMU process fails to go away upon
834                         # SIGKILL it is because it is stuck in an
835                         # uninterruptible kernel sleep waiting on I/O from
836                         # some non-responsive server.
837                         # Given the CPU load of the gate tests though, it is
838                         # conceivable that the 15 second timeout is too short,
839                         # particularly if the VM running tempest has a high
840                         # steal time from the cloud host. ie 15 wallclock
841                         # seconds may have passed, but the VM might have only
842                         # have a few seconds of scheduled run time.
843                         LOG.warning('Error from libvirt during destroy. '
844                                     'Code=%(errcode)s Error=%(e)s; '
845                                     'attempt %(attempt)d of 3',
846                                     {'errcode': errcode, 'e': e,
847                                      'attempt': attempt},
848                                     instance=instance)
849                         with excutils.save_and_reraise_exception() as ctxt:
850                             # Try up to 3 times before giving up.
851                             if attempt < 3:
852                                 ctxt.reraise = False
853                                 self._destroy(instance, attempt + 1)
854                                 return
855 
856                 if not is_okay:
857                     with excutils.save_and_reraise_exception():
858                         LOG.error('Error from libvirt during destroy. '
859                                   'Code=%(errcode)s Error=%(e)s',
860                                   {'errcode': errcode, 'e': e},
861                                   instance=instance)
862 
863         def _wait_for_destroy(expected_domid):
864             """Called at an interval until the VM is gone."""
865             # NOTE(vish): If the instance disappears during the destroy
866             #             we ignore it so the cleanup can still be
867             #             attempted because we would prefer destroy to
868             #             never fail.
869             try:
870                 dom_info = self.get_info(instance)
871                 state = dom_info.state
872                 new_domid = dom_info.internal_id
873             except exception.InstanceNotFound:
874                 LOG.debug("During wait destroy, instance disappeared.",
875                           instance=instance)
876                 state = power_state.SHUTDOWN
877 
878             if state == power_state.SHUTDOWN:
879                 LOG.info("Instance destroyed successfully.", instance=instance)
880                 raise loopingcall.LoopingCallDone()
881 
882             # NOTE(wangpan): If the instance was booted again after destroy,
883             #                this may be an endless loop, so check the id of
884             #                domain here, if it changed and the instance is
885             #                still running, we should destroy it again.
886             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
887             if new_domid != expected_domid:
888                 LOG.info("Instance may be started again.", instance=instance)
889                 kwargs['is_running'] = True
890                 raise loopingcall.LoopingCallDone()
891 
892         kwargs = {'is_running': False}
893         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
894                                                      old_domid)
895         timer.start(interval=0.5).wait()
896         if kwargs['is_running']:
897             LOG.info("Going to destroy instance again.", instance=instance)
898             self._destroy(instance)
899         else:
900             # NOTE(GuanQiang): teardown container to avoid resource leak
901             if CONF.libvirt.virt_type == 'lxc':
902                 self._teardown_container(instance)
903 
904     def destroy(self, context, instance, network_info, block_device_info=None,
905                 destroy_disks=True):
906         self._destroy(instance)
907         self.cleanup(context, instance, network_info, block_device_info,
908                      destroy_disks)
909 
910     def _undefine_domain(self, instance):
911         try:
912             guest = self._host.get_guest(instance)
913             try:
914                 support_uefi = self._has_uefi_support()
915                 guest.delete_configuration(support_uefi)
916             except libvirt.libvirtError as e:
917                 with excutils.save_and_reraise_exception() as ctxt:
918                     errcode = e.get_error_code()
919                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
920                         LOG.debug("Called undefine, but domain already gone.",
921                                   instance=instance)
922                         ctxt.reraise = False
923                     else:
924                         LOG.error('Error from libvirt during undefine. '
925                                   'Code=%(errcode)s Error=%(e)s',
926                                   {'errcode': errcode, 'e': e},
927                                   instance=instance)
928         except exception.InstanceNotFound:
929             pass
930 
931     def cleanup(self, context, instance, network_info, block_device_info=None,
932                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
933         if destroy_vifs:
934             self._unplug_vifs(instance, network_info, True)
935 
936         retry = True
937         while retry:
938             try:
939                 self.unfilter_instance(instance, network_info)
940             except libvirt.libvirtError as e:
941                 try:
942                     state = self.get_info(instance).state
943                 except exception.InstanceNotFound:
944                     state = power_state.SHUTDOWN
945 
946                 if state != power_state.SHUTDOWN:
947                     LOG.warning("Instance may be still running, destroy "
948                                 "it again.", instance=instance)
949                     self._destroy(instance)
950                 else:
951                     retry = False
952                     errcode = e.get_error_code()
953                     LOG.exception(_('Error from libvirt during unfilter. '
954                                     'Code=%(errcode)s Error=%(e)s'),
955                                   {'errcode': errcode, 'e': e},
956                                   instance=instance)
957                     reason = _("Error unfiltering instance.")
958                     raise exception.InstanceTerminationFailure(reason=reason)
959             except Exception:
960                 retry = False
961                 raise
962             else:
963                 retry = False
964 
965         # FIXME(wangpan): if the instance is booted again here, such as the
966         #                 soft reboot operation boot it here, it will become
967         #                 "running deleted", should we check and destroy it
968         #                 at the end of this method?
969 
970         # NOTE(vish): we disconnect from volumes regardless
971         block_device_mapping = driver.block_device_info_get_mapping(
972             block_device_info)
973         for vol in block_device_mapping:
974             connection_info = vol['connection_info']
975             disk_dev = vol['mount_device']
976             if disk_dev is not None:
977                 disk_dev = disk_dev.rpartition("/")[2]
978 
979             if ('data' in connection_info and
980                     'volume_id' in connection_info['data']):
981                 volume_id = connection_info['data']['volume_id']
982                 encryption = encryptors.get_encryption_metadata(
983                     context, self._volume_api, volume_id, connection_info)
984 
985                 if encryption:
986                     # The volume must be detached from the VM before
987                     # disconnecting it from its encryptor. Otherwise, the
988                     # encryptor may report that the volume is still in use.
989                     encryptor = self._get_volume_encryptor(connection_info,
990                                                            encryption)
991                     encryptor.detach_volume(**encryption)
992 
993             try:
994                 self._disconnect_volume(connection_info, disk_dev, instance)
995             except Exception as exc:
996                 with excutils.save_and_reraise_exception() as ctxt:
997                     if destroy_disks:
998                         # Don't block on Volume errors if we're trying to
999                         # delete the instance as we may be partially created
1000                         # or deleted
1001                         ctxt.reraise = False
1002                         LOG.warning(
1003                             "Ignoring Volume Error on vol %(vol_id)s "
1004                             "during delete %(exc)s",
1005                             {'vol_id': vol.get('volume_id'), 'exc': exc},
1006                             instance=instance)
1007 
1008         if destroy_disks:
1009             # NOTE(haomai): destroy volumes if needed
1010             if CONF.libvirt.images_type == 'lvm':
1011                 self._cleanup_lvm(instance, block_device_info)
1012             if CONF.libvirt.images_type == 'rbd':
1013                 self._cleanup_rbd(instance)
1014 
1015         is_shared_block_storage = False
1016         if migrate_data and 'is_shared_block_storage' in migrate_data:
1017             is_shared_block_storage = migrate_data.is_shared_block_storage
1018         if destroy_disks or is_shared_block_storage:
1019             attempts = int(instance.system_metadata.get('clean_attempts',
1020                                                         '0'))
1021             success = self.delete_instance_files(instance)
1022             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1023             # task in the compute manager. The tight coupling is not great...
1024             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1025             if success:
1026                 instance.cleaned = True
1027             instance.save()
1028 
1029         self._undefine_domain(instance)
1030 
1031     def _detach_encrypted_volumes(self, instance, block_device_info):
1032         """Detaches encrypted volumes attached to instance."""
1033         disks = self._get_instance_disk_info(instance, block_device_info)
1034         encrypted_volumes = filter(dmcrypt.is_encrypted,
1035                                    [disk['path'] for disk in disks])
1036         for path in encrypted_volumes:
1037             dmcrypt.delete_volume(path)
1038 
1039     def _get_serial_ports_from_guest(self, guest, mode=None):
1040         """Returns an iterator over serial port(s) configured on guest.
1041 
1042         :param mode: Should be a value in (None, bind, connect)
1043         """
1044         xml = guest.get_xml_desc()
1045         tree = etree.fromstring(xml)
1046 
1047         # The 'serial' device is the base for x86 platforms. Other platforms
1048         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1049         xpath_mode = "[@mode='%s']" % mode if mode else ""
1050         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1051         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1052 
1053         tcp_devices = tree.findall(serial_tcp)
1054         if len(tcp_devices) == 0:
1055             tcp_devices = tree.findall(console_tcp)
1056         for source in tcp_devices:
1057             yield (source.get("host"), int(source.get("service")))
1058 
1059     def _get_scsi_controller_max_unit(self, guest):
1060         """Returns the max disk unit used by scsi controller"""
1061         xml = guest.get_xml_desc()
1062         tree = etree.fromstring(xml)
1063         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1064 
1065         ret = []
1066         for obj in tree.findall(addrs):
1067             ret.append(int(obj.get('unit', 0)))
1068         return max(ret)
1069 
1070     @staticmethod
1071     def _get_rbd_driver():
1072         return rbd_utils.RBDDriver(
1073                 pool=CONF.libvirt.images_rbd_pool,
1074                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1075                 rbd_user=CONF.libvirt.rbd_user)
1076 
1077     def _cleanup_rbd(self, instance):
1078         # NOTE(nic): On revert_resize, the cleanup steps for the root
1079         # volume are handled with an "rbd snap rollback" command,
1080         # and none of this is needed (and is, in fact, harmful) so
1081         # filter out non-ephemerals from the list
1082         if instance.task_state == task_states.RESIZE_REVERTING:
1083             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1084                                       disk.endswith('disk.local'))
1085         else:
1086             filter_fn = lambda disk: disk.startswith(instance.uuid)
1087         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1088 
1089     def _cleanup_lvm(self, instance, block_device_info):
1090         """Delete all LVM disks for given instance object."""
1091         if instance.get('ephemeral_key_uuid') is not None:
1092             self._detach_encrypted_volumes(instance, block_device_info)
1093 
1094         disks = self._lvm_disks(instance)
1095         if disks:
1096             lvm.remove_volumes(disks)
1097 
1098     def _lvm_disks(self, instance):
1099         """Returns all LVM disks for given instance object."""
1100         if CONF.libvirt.images_volume_group:
1101             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1102             if not os.path.exists(vg):
1103                 return []
1104             pattern = '%s_' % instance.uuid
1105 
1106             def belongs_to_instance(disk):
1107                 return disk.startswith(pattern)
1108 
1109             def fullpath(name):
1110                 return os.path.join(vg, name)
1111 
1112             logical_volumes = lvm.list_volumes(vg)
1113 
1114             disks = [fullpath(disk) for disk in logical_volumes
1115                      if belongs_to_instance(disk)]
1116             return disks
1117         return []
1118 
1119     def get_volume_connector(self, instance):
1120         root_helper = utils.get_root_helper()
1121         return connector.get_connector_properties(
1122             root_helper, CONF.my_block_storage_ip,
1123             CONF.libvirt.volume_use_multipath,
1124             enforce_multipath=True,
1125             host=CONF.host)
1126 
1127     def _cleanup_resize(self, instance, network_info):
1128         inst_base = libvirt_utils.get_instance_path(instance)
1129         target = inst_base + '_resize'
1130 
1131         if os.path.exists(target):
1132             # Deletion can fail over NFS, so retry the deletion as required.
1133             # Set maximum attempt as 5, most test can remove the directory
1134             # for the second time.
1135             utils.execute('rm', '-rf', target, delay_on_retry=True,
1136                           attempts=5)
1137 
1138         root_disk = self.image_backend.by_name(instance, 'disk')
1139         # TODO(nic): Set ignore_errors=False in a future release.
1140         # It is set to True here to avoid any upgrade issues surrounding
1141         # instances being in pending resize state when the software is updated;
1142         # in that case there will be no snapshot to remove.  Once it can be
1143         # reasonably assumed that no such instances exist in the wild
1144         # anymore, it should be set back to False (the default) so it will
1145         # throw errors, like it should.
1146         if root_disk.exists():
1147             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1148                                   ignore_errors=True)
1149 
1150         # NOTE(mjozefcz):
1151         # self.image_backend.image for some backends recreates instance
1152         # directory and image disk.info - remove it here if exists
1153         if os.path.exists(inst_base) and not root_disk.exists():
1154             try:
1155                 shutil.rmtree(inst_base)
1156             except OSError as e:
1157                 if e.errno != errno.ENOENT:
1158                     raise
1159 
1160         if instance.host != CONF.host:
1161             self._undefine_domain(instance)
1162             self.unplug_vifs(instance, network_info)
1163             self.unfilter_instance(instance, network_info)
1164 
1165     def _get_volume_driver(self, connection_info):
1166         driver_type = connection_info.get('driver_volume_type')
1167         if driver_type not in self.volume_drivers:
1168             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1169         return self.volume_drivers[driver_type]
1170 
1171     def _connect_volume(self, connection_info, disk_info, instance):
1172         vol_driver = self._get_volume_driver(connection_info)
1173         vol_driver.connect_volume(connection_info, disk_info, instance)
1174 
1175     def _disconnect_volume(self, connection_info, disk_dev, instance):
1176         vol_driver = self._get_volume_driver(connection_info)
1177         vol_driver.disconnect_volume(connection_info, disk_dev, instance)
1178 
1179     def _extend_volume(self, connection_info, instance):
1180         vol_driver = self._get_volume_driver(connection_info)
1181         return vol_driver.extend_volume(connection_info, instance)
1182 
1183     def _get_volume_config(self, connection_info, disk_info):
1184         vol_driver = self._get_volume_driver(connection_info)
1185         conf = vol_driver.get_config(connection_info, disk_info)
1186         self._set_cache_mode(conf)
1187         return conf
1188 
1189     def _get_volume_encryptor(self, connection_info, encryption):
1190         root_helper = utils.get_root_helper()
1191         return encryptors.get_volume_encryptor(root_helper=root_helper,
1192                                                keymgr=key_manager.API(CONF),
1193                                                connection_info=connection_info,
1194                                                **encryption)
1195 
1196     def _check_discard_for_attach_volume(self, conf, instance):
1197         """Perform some checks for volumes configured for discard support.
1198 
1199         If discard is configured for the volume, and the guest is using a
1200         configuration known to not work, we will log a message explaining
1201         the reason why.
1202         """
1203         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1204             LOG.debug('Attempting to attach volume %(id)s with discard '
1205                       'support enabled to an instance using an '
1206                       'unsupported configuration. target_bus = '
1207                       '%(bus)s. Trim commands will not be issued to '
1208                       'the storage device.',
1209                       {'bus': conf.target_bus,
1210                        'id': conf.serial},
1211                       instance=instance)
1212 
1213     def attach_volume(self, context, connection_info, instance, mountpoint,
1214                       disk_bus=None, device_type=None, encryption=None):
1215         guest = self._host.get_guest(instance)
1216 
1217         disk_dev = mountpoint.rpartition("/")[2]
1218         bdm = {
1219             'device_name': disk_dev,
1220             'disk_bus': disk_bus,
1221             'device_type': device_type}
1222 
1223         # Note(cfb): If the volume has a custom block size, check that
1224         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1225         #            presence of a block size is considered mandatory by
1226         #            cinder so we fail if we can't honor the request.
1227         data = {}
1228         if ('data' in connection_info):
1229             data = connection_info['data']
1230         if ('logical_block_size' in data or 'physical_block_size' in data):
1231             if ((CONF.libvirt.virt_type != "kvm" and
1232                  CONF.libvirt.virt_type != "qemu")):
1233                 msg = _("Volume sets block size, but the current "
1234                         "libvirt hypervisor '%s' does not support custom "
1235                         "block size") % CONF.libvirt.virt_type
1236                 raise exception.InvalidHypervisorType(msg)
1237 
1238         disk_info = blockinfo.get_info_from_bdm(
1239             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1240         self._connect_volume(connection_info, disk_info, instance)
1241         if disk_info['bus'] == 'scsi':
1242             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1243 
1244         conf = self._get_volume_config(connection_info, disk_info)
1245 
1246         self._check_discard_for_attach_volume(conf, instance)
1247 
1248         try:
1249             state = guest.get_power_state(self._host)
1250             live = state in (power_state.RUNNING, power_state.PAUSED)
1251 
1252             if encryption:
1253                 encryptor = self._get_volume_encryptor(connection_info,
1254                                                        encryption)
1255                 encryptor.attach_volume(context, **encryption)
1256 
1257             guest.attach_device(conf, persistent=True, live=live)
1258             # NOTE(artom) If we're attaching with a device role tag, we need to
1259             # rebuild device_metadata. If we're attaching without a role
1260             # tag, we're rebuilding it here needlessly anyways. This isn't a
1261             # massive deal, and it helps reduce code complexity by not having
1262             # to indicate to the virt driver that the attach is tagged. The
1263             # really important optimization of not calling the database unless
1264             # device_metadata has actually changed is done for us by
1265             # instance.save().
1266             instance.device_metadata = self._build_device_metadata(
1267                 context, instance)
1268             instance.save()
1269         except Exception:
1270             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1271                           mountpoint, instance=instance)
1272             with excutils.save_and_reraise_exception():
1273                 self._disconnect_volume(connection_info, disk_dev, instance)
1274 
1275     def _swap_volume(self, guest, disk_path, conf, resize_to):
1276         """Swap existing disk with a new block device."""
1277         dev = guest.get_block_device(disk_path)
1278 
1279         # Save a copy of the domain's persistent XML file
1280         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1281 
1282         # Abort is an idempotent operation, so make sure any block
1283         # jobs which may have failed are ended.
1284         try:
1285             dev.abort_job()
1286         except Exception:
1287             pass
1288 
1289         try:
1290             # NOTE (rmk): blockRebase cannot be executed on persistent
1291             #             domains, so we need to temporarily undefine it.
1292             #             If any part of this block fails, the domain is
1293             #             re-defined regardless.
1294             if guest.has_persistent_configuration():
1295                 support_uefi = self._has_uefi_support()
1296                 guest.delete_configuration(support_uefi)
1297 
1298             try:
1299                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1300                 # allow writing to existing external volume file. Use
1301                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1302                 # make sure XML is generated correctly (bug 1691195)
1303                 copy_dev = conf.source_type == 'block'
1304                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1305                            copy_dev=copy_dev)
1306                 while not dev.is_job_complete():
1307                     time.sleep(0.5)
1308 
1309                 dev.abort_job(pivot=True)
1310 
1311             except Exception as exc:
1312                 LOG.exception("Failure rebasing volume %(new_path)s on "
1313                     "%(old_path)s.", {'new_path': conf.source_path,
1314                                       'old_path': disk_path})
1315                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1316 
1317             if resize_to:
1318                 dev.resize(resize_to * units.Gi / units.Ki)
1319         finally:
1320             self._host.write_instance_config(xml)
1321 
1322     def swap_volume(self, old_connection_info,
1323                     new_connection_info, instance, mountpoint, resize_to):
1324 
1325         guest = self._host.get_guest(instance)
1326 
1327         disk_dev = mountpoint.rpartition("/")[2]
1328         if not guest.get_disk(disk_dev):
1329             raise exception.DiskNotFound(location=disk_dev)
1330         disk_info = {
1331             'dev': disk_dev,
1332             'bus': blockinfo.get_disk_bus_for_disk_dev(
1333                 CONF.libvirt.virt_type, disk_dev),
1334             'type': 'disk',
1335             }
1336         # NOTE (lyarwood): new_connection_info will be modified by the
1337         # following _connect_volume call down into the volume drivers. The
1338         # majority of the volume drivers will add a device_path that is in turn
1339         # used by _get_volume_config to set the source_path of the
1340         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1341         # this to the BDM here as the upper compute swap_volume method will
1342         # eventually do this for us.
1343         self._connect_volume(new_connection_info, disk_info, instance)
1344         conf = self._get_volume_config(new_connection_info, disk_info)
1345         if not conf.source_path:
1346             self._disconnect_volume(new_connection_info, disk_dev, instance)
1347             raise NotImplementedError(_("Swap only supports host devices"))
1348 
1349         try:
1350             self._swap_volume(guest, disk_dev, conf, resize_to)
1351         except exception.VolumeRebaseFailed:
1352             with excutils.save_and_reraise_exception():
1353                 self._disconnect_volume(new_connection_info, disk_dev,
1354                                         instance)
1355 
1356         self._disconnect_volume(old_connection_info, disk_dev, instance)
1357 
1358     def _get_existing_domain_xml(self, instance, network_info,
1359                                  block_device_info=None):
1360         try:
1361             guest = self._host.get_guest(instance)
1362             xml = guest.get_xml_desc()
1363         except exception.InstanceNotFound:
1364             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1365                                                 instance,
1366                                                 instance.image_meta,
1367                                                 block_device_info)
1368             xml = self._get_guest_xml(nova_context.get_admin_context(),
1369                                       instance, network_info, disk_info,
1370                                       instance.image_meta,
1371                                       block_device_info=block_device_info)
1372         return xml
1373 
1374     def detach_volume(self, connection_info, instance, mountpoint,
1375                       encryption=None):
1376         disk_dev = mountpoint.rpartition("/")[2]
1377         try:
1378             guest = self._host.get_guest(instance)
1379 
1380             state = guest.get_power_state(self._host)
1381             live = state in (power_state.RUNNING, power_state.PAUSED)
1382 
1383             # The volume must be detached from the VM before disconnecting it
1384             # from its encryptor. Otherwise, the encryptor may report that the
1385             # volume is still in use.
1386             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1387                                                              disk_dev,
1388                                                              live=live)
1389             wait_for_detach()
1390 
1391             if encryption:
1392                 encryptor = self._get_volume_encryptor(connection_info,
1393                                                        encryption)
1394                 encryptor.detach_volume(**encryption)
1395 
1396         except exception.InstanceNotFound:
1397             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1398             #                will throw InstanceNotFound exception. Need to
1399             #                disconnect volume under this circumstance.
1400             LOG.warning("During detach_volume, instance disappeared.",
1401                         instance=instance)
1402         except exception.DeviceNotFound:
1403             raise exception.DiskNotFound(location=disk_dev)
1404         except libvirt.libvirtError as ex:
1405             # NOTE(vish): This is called to cleanup volumes after live
1406             #             migration, so we should still disconnect even if
1407             #             the instance doesn't exist here anymore.
1408             error_code = ex.get_error_code()
1409             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1410                 # NOTE(vish):
1411                 LOG.warning("During detach_volume, instance disappeared.",
1412                             instance=instance)
1413             else:
1414                 raise
1415 
1416         self._disconnect_volume(connection_info, disk_dev, instance)
1417 
1418     def extend_volume(self, connection_info, instance):
1419         try:
1420             new_size = self._extend_volume(connection_info, instance)
1421         except NotImplementedError:
1422             raise exception.ExtendVolumeNotSupported()
1423 
1424         # Resize the device in QEMU so its size is updated and
1425         # detected by the instance without rebooting.
1426         try:
1427             guest = self._host.get_guest(instance)
1428             state = guest.get_power_state(self._host)
1429             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1430             if active_state:
1431                 disk_path = connection_info['data']['device_path']
1432                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1433                           {'dev': disk_path, 'size': new_size})
1434                 dev = guest.get_block_device(disk_path)
1435                 dev.resize(new_size // units.Ki)
1436             else:
1437                 LOG.debug('Skipping block device resize, guest is not running',
1438                           instance=instance)
1439         except exception.InstanceNotFound:
1440             with excutils.save_and_reraise_exception():
1441                 LOG.warning('During extend_volume, instance disappeared.',
1442                             instance=instance)
1443         except libvirt.libvirtError:
1444             with excutils.save_and_reraise_exception():
1445                 LOG.exception('resizing block device failed.',
1446                               instance=instance)
1447 
1448     def attach_interface(self, context, instance, image_meta, vif):
1449         guest = self._host.get_guest(instance)
1450 
1451         self.vif_driver.plug(instance, vif)
1452         self.firewall_driver.setup_basic_filtering(instance, [vif])
1453         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1454                                          instance.flavor,
1455                                          CONF.libvirt.virt_type,
1456                                          self._host)
1457         try:
1458             state = guest.get_power_state(self._host)
1459             live = state in (power_state.RUNNING, power_state.PAUSED)
1460             guest.attach_device(cfg, persistent=True, live=live)
1461         except libvirt.libvirtError:
1462             LOG.error('attaching network adapter failed.',
1463                       instance=instance, exc_info=True)
1464             self.vif_driver.unplug(instance, vif)
1465             raise exception.InterfaceAttachFailed(
1466                     instance_uuid=instance.uuid)
1467         try:
1468             # NOTE(artom) If we're attaching with a device role tag, we need to
1469             # rebuild device_metadata. If we're attaching without a role
1470             # tag, we're rebuilding it here needlessly anyways. This isn't a
1471             # massive deal, and it helps reduce code complexity by not having
1472             # to indicate to the virt driver that the attach is tagged. The
1473             # really important optimization of not calling the database unless
1474             # device_metadata has actually changed is done for us by
1475             # instance.save().
1476             instance.device_metadata = self._build_device_metadata(
1477                 context, instance)
1478             instance.save()
1479         except Exception:
1480             # NOTE(artom) If we fail here it means the interface attached
1481             # successfully but building and/or saving the device metadata
1482             # failed. Just unplugging the vif is therefore not enough cleanup,
1483             # we need to detach the interface.
1484             with excutils.save_and_reraise_exception(reraise=False):
1485                 LOG.error('Interface attached successfully but building '
1486                           'and/or saving device metadata failed.',
1487                           instance=instance, exc_info=True)
1488                 self.detach_interface(context, instance, vif)
1489                 raise exception.InterfaceAttachFailed(
1490                     instance_uuid=instance.uuid)
1491 
1492     def detach_interface(self, context, instance, vif):
1493         guest = self._host.get_guest(instance)
1494         cfg = self.vif_driver.get_config(instance, vif,
1495                                          instance.image_meta,
1496                                          instance.flavor,
1497                                          CONF.libvirt.virt_type, self._host)
1498         interface = guest.get_interface_by_cfg(cfg)
1499         try:
1500             self.vif_driver.unplug(instance, vif)
1501             # NOTE(mriedem): When deleting an instance and using Neutron,
1502             # we can be racing against Neutron deleting the port and
1503             # sending the vif-deleted event which then triggers a call to
1504             # detach the interface, so if the interface is not found then
1505             # we can just log it as a warning.
1506             if not interface:
1507                 mac = vif.get('address')
1508                 # The interface is gone so just log it as a warning.
1509                 LOG.warning('Detaching interface %(mac)s failed because '
1510                             'the device is no longer found on the guest.',
1511                             {'mac': mac}, instance=instance)
1512                 return
1513 
1514             state = guest.get_power_state(self._host)
1515             live = state in (power_state.RUNNING, power_state.PAUSED)
1516             # Now we are going to loop until the interface is detached or we
1517             # timeout.
1518             wait_for_detach = guest.detach_device_with_retry(
1519                 guest.get_interface_by_cfg, cfg, live=live,
1520                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1521             wait_for_detach()
1522         except exception.DeviceDetachFailed:
1523             # We failed to detach the device even with the retry loop, so let's
1524             # dump some debug information to the logs before raising back up.
1525             with excutils.save_and_reraise_exception():
1526                 devname = self.vif_driver.get_vif_devname(vif)
1527                 interface = guest.get_interface_by_cfg(cfg)
1528                 if interface:
1529                     LOG.warning(
1530                         'Failed to detach interface %(devname)s after '
1531                         'repeated attempts. Final interface xml:\n'
1532                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1533                         {'devname': devname,
1534                          'interface_xml': interface.to_xml(),
1535                          'guest_xml': guest.get_xml_desc()},
1536                         instance=instance)
1537         except exception.DeviceNotFound:
1538             # The interface is gone so just log it as a warning.
1539             LOG.warning('Detaching interface %(mac)s failed because '
1540                         'the device is no longer found on the guest.',
1541                         {'mac': vif.get('address')}, instance=instance)
1542         except libvirt.libvirtError as ex:
1543             error_code = ex.get_error_code()
1544             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1545                 LOG.warning("During detach_interface, instance disappeared.",
1546                             instance=instance)
1547             else:
1548                 # NOTE(mriedem): When deleting an instance and using Neutron,
1549                 # we can be racing against Neutron deleting the port and
1550                 # sending the vif-deleted event which then triggers a call to
1551                 # detach the interface, so we might have failed because the
1552                 # network device no longer exists. Libvirt will fail with
1553                 # "operation failed: no matching network device was found"
1554                 # which unfortunately does not have a unique error code so we
1555                 # need to look up the interface by config and if it's not found
1556                 # then we can just log it as a warning rather than tracing an
1557                 # error.
1558                 mac = vif.get('address')
1559                 interface = guest.get_interface_by_cfg(cfg)
1560                 if interface:
1561                     LOG.error('detaching network adapter failed.',
1562                               instance=instance, exc_info=True)
1563                     raise exception.InterfaceDetachFailed(
1564                             instance_uuid=instance.uuid)
1565 
1566                 # The interface is gone so just log it as a warning.
1567                 LOG.warning('Detaching interface %(mac)s failed because '
1568                             'the device is no longer found on the guest.',
1569                             {'mac': mac}, instance=instance)
1570 
1571     def _create_snapshot_metadata(self, image_meta, instance,
1572                                   img_fmt, snp_name):
1573         metadata = {'is_public': False,
1574                     'status': 'active',
1575                     'name': snp_name,
1576                     'properties': {
1577                                    'kernel_id': instance.kernel_id,
1578                                    'image_location': 'snapshot',
1579                                    'image_state': 'available',
1580                                    'owner_id': instance.project_id,
1581                                    'ramdisk_id': instance.ramdisk_id,
1582                                    }
1583                     }
1584         if instance.os_type:
1585             metadata['properties']['os_type'] = instance.os_type
1586 
1587         # NOTE(vish): glance forces ami disk format to be ami
1588         if image_meta.disk_format == 'ami':
1589             metadata['disk_format'] = 'ami'
1590         else:
1591             metadata['disk_format'] = img_fmt
1592 
1593         if image_meta.obj_attr_is_set("container_format"):
1594             metadata['container_format'] = image_meta.container_format
1595         else:
1596             metadata['container_format'] = "bare"
1597 
1598         return metadata
1599 
1600     def snapshot(self, context, instance, image_id, update_task_state):
1601         """Create snapshot from a running VM instance.
1602 
1603         This command only works with qemu 0.14+
1604         """
1605         try:
1606             guest = self._host.get_guest(instance)
1607 
1608             # TODO(sahid): We are converting all calls from a
1609             # virDomain object to use nova.virt.libvirt.Guest.
1610             # We should be able to remove virt_dom at the end.
1611             virt_dom = guest._domain
1612         except exception.InstanceNotFound:
1613             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1614 
1615         snapshot = self._image_api.get(context, image_id)
1616 
1617         # source_format is an on-disk format
1618         # source_type is a backend type
1619         disk_path, source_format = libvirt_utils.find_disk(guest)
1620         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1621 
1622         # We won't have source_type for raw or qcow2 disks, because we can't
1623         # determine that from the path. We should have it from the libvirt
1624         # xml, though.
1625         if source_type is None:
1626             source_type = source_format
1627         # For lxc instances we won't have it either from libvirt xml
1628         # (because we just gave libvirt the mounted filesystem), or the path,
1629         # so source_type is still going to be None. In this case,
1630         # root_disk is going to default to CONF.libvirt.images_type
1631         # below, which is still safe.
1632 
1633         image_format = CONF.libvirt.snapshot_image_format or source_type
1634 
1635         # NOTE(bfilippov): save lvm and rbd as raw
1636         if image_format == 'lvm' or image_format == 'rbd':
1637             image_format = 'raw'
1638 
1639         metadata = self._create_snapshot_metadata(instance.image_meta,
1640                                                   instance,
1641                                                   image_format,
1642                                                   snapshot['name'])
1643 
1644         snapshot_name = uuidutils.generate_uuid(dashed=False)
1645 
1646         state = guest.get_power_state(self._host)
1647 
1648         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1649         #               cold snapshots. Currently, checking for encryption is
1650         #               redundant because LVM supports only cold snapshots.
1651         #               It is necessary in case this situation changes in the
1652         #               future.
1653         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1654              and source_type not in ('lvm')
1655              and not CONF.ephemeral_storage_encryption.enabled
1656              and not CONF.workarounds.disable_libvirt_livesnapshot):
1657             live_snapshot = True
1658             # Abort is an idempotent operation, so make sure any block
1659             # jobs which may have failed are ended. This operation also
1660             # confirms the running instance, as opposed to the system as a
1661             # whole, has a new enough version of the hypervisor (bug 1193146).
1662             try:
1663                 guest.get_block_device(disk_path).abort_job()
1664             except libvirt.libvirtError as ex:
1665                 error_code = ex.get_error_code()
1666                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1667                     live_snapshot = False
1668                 else:
1669                     pass
1670         else:
1671             live_snapshot = False
1672 
1673         # NOTE(rmk): We cannot perform live snapshots when a managedSave
1674         #            file is present, so we will use the cold/legacy method
1675         #            for instances which are shutdown.
1676         if state == power_state.SHUTDOWN:
1677             live_snapshot = False
1678 
1679         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1680                                           instance)
1681 
1682         root_disk = self.image_backend.by_libvirt_path(
1683             instance, disk_path, image_type=source_type)
1684 
1685         if live_snapshot:
1686             LOG.info("Beginning live snapshot process", instance=instance)
1687         else:
1688             LOG.info("Beginning cold snapshot process", instance=instance)
1689 
1690         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1691 
1692         try:
1693             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1694                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1695             metadata['location'] = root_disk.direct_snapshot(
1696                 context, snapshot_name, image_format, image_id,
1697                 instance.image_ref)
1698             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1699                                   instance)
1700             self._image_api.update(context, image_id, metadata,
1701                                    purge_props=False)
1702         except (NotImplementedError, exception.ImageUnacceptable,
1703                 exception.Forbidden) as e:
1704             if type(e) != NotImplementedError:
1705                 LOG.warning('Performing standard snapshot because direct '
1706                             'snapshot failed: %(error)s', {'error': e})
1707             failed_snap = metadata.pop('location', None)
1708             if failed_snap:
1709                 failed_snap = {'url': str(failed_snap)}
1710             root_disk.cleanup_direct_snapshot(failed_snap,
1711                                                   also_destroy_volume=True,
1712                                                   ignore_errors=True)
1713             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1714                               expected_state=task_states.IMAGE_UPLOADING)
1715 
1716             # TODO(nic): possibly abstract this out to the root_disk
1717             if source_type == 'rbd' and live_snapshot:
1718                 # Standard snapshot uses qemu-img convert from RBD which is
1719                 # not safe to run with live_snapshot.
1720                 live_snapshot = False
1721                 # Suspend the guest, so this is no longer a live snapshot
1722                 self._prepare_domain_for_snapshot(context, live_snapshot,
1723                                                   state, instance)
1724 
1725             snapshot_directory = CONF.libvirt.snapshots_directory
1726             fileutils.ensure_tree(snapshot_directory)
1727             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1728                 try:
1729                     out_path = os.path.join(tmpdir, snapshot_name)
1730                     if live_snapshot:
1731                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1732                         os.chmod(tmpdir, 0o701)
1733                         self._live_snapshot(context, instance, guest,
1734                                             disk_path, out_path, source_format,
1735                                             image_format, instance.image_meta)
1736                     else:
1737                         root_disk.snapshot_extract(out_path, image_format)
1738                 finally:
1739                     self._snapshot_domain(context, live_snapshot, virt_dom,
1740                                           state, instance)
1741                     LOG.info("Snapshot extracted, beginning image upload",
1742                              instance=instance)
1743 
1744                 # Upload that image to the image service
1745                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1746                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1747                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
1748                     self._image_api.update(context,
1749                                            image_id,
1750                                            metadata,
1751                                            image_file)
1752         except Exception:
1753             with excutils.save_and_reraise_exception():
1754                 LOG.exception(_("Failed to snapshot image"))
1755                 failed_snap = metadata.pop('location', None)
1756                 if failed_snap:
1757                     failed_snap = {'url': str(failed_snap)}
1758                 root_disk.cleanup_direct_snapshot(
1759                         failed_snap, also_destroy_volume=True,
1760                         ignore_errors=True)
1761 
1762         LOG.info("Snapshot image upload complete", instance=instance)
1763 
1764     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1765                                      instance):
1766         # NOTE(dkang): managedSave does not work for LXC
1767         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1768             if state == power_state.RUNNING or state == power_state.PAUSED:
1769                 self.suspend(context, instance)
1770 
1771     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1772                          instance):
1773         guest = None
1774         # NOTE(dkang): because previous managedSave is not called
1775         #              for LXC, _create_domain must not be called.
1776         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1777             if state == power_state.RUNNING:
1778                 guest = self._create_domain(domain=virt_dom)
1779             elif state == power_state.PAUSED:
1780                 guest = self._create_domain(domain=virt_dom, pause=True)
1781 
1782             if guest is not None:
1783                 self._attach_pci_devices(
1784                     guest, pci_manager.get_instance_pci_devs(instance))
1785                 self._attach_direct_passthrough_ports(
1786                     context, instance, guest)
1787 
1788     def _can_set_admin_password(self, image_meta):
1789 
1790         if CONF.libvirt.virt_type == 'parallels':
1791             if not self._host.has_min_version(
1792                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
1793                 raise exception.SetAdminPasswdNotSupported()
1794         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
1795             if not self._host.has_min_version(
1796                    MIN_LIBVIRT_SET_ADMIN_PASSWD):
1797                 raise exception.SetAdminPasswdNotSupported()
1798             if not image_meta.properties.get('hw_qemu_guest_agent', False):
1799                 raise exception.QemuGuestAgentNotEnabled()
1800         else:
1801             raise exception.SetAdminPasswdNotSupported()
1802 
1803     def set_admin_password(self, instance, new_pass):
1804         self._can_set_admin_password(instance.image_meta)
1805 
1806         guest = self._host.get_guest(instance)
1807         user = instance.image_meta.properties.get("os_admin_user")
1808         if not user:
1809             if instance.os_type == "windows":
1810                 user = "Administrator"
1811             else:
1812                 user = "root"
1813         try:
1814             guest.set_user_password(user, new_pass)
1815         except libvirt.libvirtError as ex:
1816             error_code = ex.get_error_code()
1817             msg = (_('Error from libvirt while set password for username '
1818                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
1819                    % {'user': user, 'error_code': error_code, 'ex': ex})
1820             raise exception.InternalError(msg)
1821 
1822     def _can_quiesce(self, instance, image_meta):
1823         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
1824             raise exception.InstanceQuiesceNotSupported(
1825                 instance_id=instance.uuid)
1826 
1827         if not image_meta.properties.get('hw_qemu_guest_agent', False):
1828             raise exception.QemuGuestAgentNotEnabled()
1829 
1830     def _requires_quiesce(self, image_meta):
1831         return image_meta.properties.get('os_require_quiesce', False)
1832 
1833     def _set_quiesced(self, context, instance, image_meta, quiesced):
1834         self._can_quiesce(instance, image_meta)
1835         try:
1836             guest = self._host.get_guest(instance)
1837             if quiesced:
1838                 guest.freeze_filesystems()
1839             else:
1840                 guest.thaw_filesystems()
1841         except libvirt.libvirtError as ex:
1842             error_code = ex.get_error_code()
1843             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
1844                      '[Error Code %(error_code)s] %(ex)s')
1845                    % {'instance_name': instance.name,
1846                       'error_code': error_code, 'ex': ex})
1847             raise exception.InternalError(msg)
1848 
1849     def quiesce(self, context, instance, image_meta):
1850         """Freeze the guest filesystems to prepare for snapshot.
1851 
1852         The qemu-guest-agent must be setup to execute fsfreeze.
1853         """
1854         self._set_quiesced(context, instance, image_meta, True)
1855 
1856     def unquiesce(self, context, instance, image_meta):
1857         """Thaw the guest filesystems after snapshot."""
1858         self._set_quiesced(context, instance, image_meta, False)
1859 
1860     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
1861                        source_format, image_format, image_meta):
1862         """Snapshot an instance without downtime."""
1863         dev = guest.get_block_device(disk_path)
1864 
1865         # Save a copy of the domain's persistent XML file
1866         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1867 
1868         # Abort is an idempotent operation, so make sure any block
1869         # jobs which may have failed are ended.
1870         try:
1871             dev.abort_job()
1872         except Exception:
1873             pass
1874 
1875         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
1876         #             in QEMU 1.3. In order to do this, we need to create
1877         #             a destination image with the original backing file
1878         #             and matching size of the instance root disk.
1879         src_disk_size = libvirt_utils.get_disk_size(disk_path,
1880                                                     format=source_format)
1881         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
1882                                                         format=source_format,
1883                                                         basename=False)
1884         disk_delta = out_path + '.delta'
1885         libvirt_utils.create_cow_image(src_back_path, disk_delta,
1886                                        src_disk_size)
1887 
1888         quiesced = False
1889         try:
1890             self._set_quiesced(context, instance, image_meta, True)
1891             quiesced = True
1892         except exception.NovaException as err:
1893             if self._requires_quiesce(image_meta):
1894                 raise
1895             LOG.info('Skipping quiescing instance: %(reason)s.',
1896                      {'reason': err}, instance=instance)
1897 
1898         try:
1899             # NOTE (rmk): blockRebase cannot be executed on persistent
1900             #             domains, so we need to temporarily undefine it.
1901             #             If any part of this block fails, the domain is
1902             #             re-defined regardless.
1903             if guest.has_persistent_configuration():
1904                 support_uefi = self._has_uefi_support()
1905                 guest.delete_configuration(support_uefi)
1906 
1907             # NOTE (rmk): Establish a temporary mirror of our root disk and
1908             #             issue an abort once we have a complete copy.
1909             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
1910 
1911             while not dev.is_job_complete():
1912                 time.sleep(0.5)
1913 
1914             dev.abort_job()
1915             nova.privsep.path.chown(disk_delta, uid=os.getuid())
1916         finally:
1917             self._host.write_instance_config(xml)
1918             if quiesced:
1919                 self._set_quiesced(context, instance, image_meta, False)
1920 
1921         # Convert the delta (CoW) image with a backing file to a flat
1922         # image with no backing file.
1923         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
1924                                        out_path, image_format)
1925 
1926     def _volume_snapshot_update_status(self, context, snapshot_id, status):
1927         """Send a snapshot status update to Cinder.
1928 
1929         This method captures and logs exceptions that occur
1930         since callers cannot do anything useful with these exceptions.
1931 
1932         Operations on the Cinder side waiting for this will time out if
1933         a failure occurs sending the update.
1934 
1935         :param context: security context
1936         :param snapshot_id: id of snapshot being updated
1937         :param status: new status value
1938 
1939         """
1940 
1941         try:
1942             self._volume_api.update_snapshot_status(context,
1943                                                     snapshot_id,
1944                                                     status)
1945         except Exception:
1946             LOG.exception(_('Failed to send updated snapshot status '
1947                             'to volume service.'))
1948 
1949     def _volume_snapshot_create(self, context, instance, guest,
1950                                 volume_id, new_file):
1951         """Perform volume snapshot.
1952 
1953            :param guest: VM that volume is attached to
1954            :param volume_id: volume UUID to snapshot
1955            :param new_file: relative path to new qcow2 file present on share
1956 
1957         """
1958         xml = guest.get_xml_desc()
1959         xml_doc = etree.fromstring(xml)
1960 
1961         device_info = vconfig.LibvirtConfigGuest()
1962         device_info.parse_dom(xml_doc)
1963 
1964         disks_to_snap = []          # to be snapshotted by libvirt
1965         network_disks_to_snap = []  # network disks (netfs, etc.)
1966         disks_to_skip = []          # local disks not snapshotted
1967 
1968         for guest_disk in device_info.devices:
1969             if (guest_disk.root_name != 'disk'):
1970                 continue
1971 
1972             if (guest_disk.target_dev is None):
1973                 continue
1974 
1975             if (guest_disk.serial is None or guest_disk.serial != volume_id):
1976                 disks_to_skip.append(guest_disk.target_dev)
1977                 continue
1978 
1979             # disk is a Cinder volume with the correct volume_id
1980 
1981             disk_info = {
1982                 'dev': guest_disk.target_dev,
1983                 'serial': guest_disk.serial,
1984                 'current_file': guest_disk.source_path,
1985                 'source_protocol': guest_disk.source_protocol,
1986                 'source_name': guest_disk.source_name,
1987                 'source_hosts': guest_disk.source_hosts,
1988                 'source_ports': guest_disk.source_ports
1989             }
1990 
1991             # Determine path for new_file based on current path
1992             if disk_info['current_file'] is not None:
1993                 current_file = disk_info['current_file']
1994                 new_file_path = os.path.join(os.path.dirname(current_file),
1995                                              new_file)
1996                 disks_to_snap.append((current_file, new_file_path))
1997             # NOTE(mriedem): This used to include a check for gluster in
1998             # addition to netfs since they were added together. Support for
1999             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2000             # however, if other volume drivers rely on the netfs disk source
2001             # protocol.
2002             elif disk_info['source_protocol'] == 'netfs':
2003                 network_disks_to_snap.append((disk_info, new_file))
2004 
2005         if not disks_to_snap and not network_disks_to_snap:
2006             msg = _('Found no disk to snapshot.')
2007             raise exception.InternalError(msg)
2008 
2009         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2010 
2011         for current_name, new_filename in disks_to_snap:
2012             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2013             snap_disk.name = current_name
2014             snap_disk.source_path = new_filename
2015             snap_disk.source_type = 'file'
2016             snap_disk.snapshot = 'external'
2017             snap_disk.driver_name = 'qcow2'
2018 
2019             snapshot.add_disk(snap_disk)
2020 
2021         for disk_info, new_filename in network_disks_to_snap:
2022             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2023             snap_disk.name = disk_info['dev']
2024             snap_disk.source_type = 'network'
2025             snap_disk.source_protocol = disk_info['source_protocol']
2026             snap_disk.snapshot = 'external'
2027             snap_disk.source_path = new_filename
2028             old_dir = disk_info['source_name'].split('/')[0]
2029             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2030             snap_disk.source_hosts = disk_info['source_hosts']
2031             snap_disk.source_ports = disk_info['source_ports']
2032 
2033             snapshot.add_disk(snap_disk)
2034 
2035         for dev in disks_to_skip:
2036             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2037             snap_disk.name = dev
2038             snap_disk.snapshot = 'no'
2039 
2040             snapshot.add_disk(snap_disk)
2041 
2042         snapshot_xml = snapshot.to_xml()
2043         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2044 
2045         image_meta = instance.image_meta
2046         try:
2047             # Check to see if we can quiesce the guest before taking the
2048             # snapshot.
2049             self._can_quiesce(instance, image_meta)
2050             try:
2051                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2052                                reuse_ext=True, quiesce=True)
2053                 return
2054             except libvirt.libvirtError:
2055                 # If the image says that quiesce is required then we fail.
2056                 if self._requires_quiesce(image_meta):
2057                     raise
2058                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2059                                 'attempting again with quiescing disabled.'),
2060                               instance=instance)
2061         except (exception.InstanceQuiesceNotSupported,
2062                 exception.QemuGuestAgentNotEnabled) as err:
2063             # If the image says that quiesce is required then we need to fail.
2064             if self._requires_quiesce(image_meta):
2065                 raise
2066             LOG.info('Skipping quiescing instance: %(reason)s.',
2067                      {'reason': err}, instance=instance)
2068 
2069         try:
2070             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2071                            reuse_ext=True, quiesce=False)
2072         except libvirt.libvirtError:
2073             LOG.exception(_('Unable to create VM snapshot, '
2074                             'failing volume_snapshot operation.'),
2075                           instance=instance)
2076 
2077             raise
2078 
2079     def _volume_refresh_connection_info(self, context, instance, volume_id):
2080         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2081                   context, volume_id, instance.uuid)
2082 
2083         driver_bdm = driver_block_device.convert_volume(bdm)
2084         if driver_bdm:
2085             driver_bdm.refresh_connection_info(context, instance,
2086                                                self._volume_api, self)
2087 
2088     def volume_snapshot_create(self, context, instance, volume_id,
2089                                create_info):
2090         """Create snapshots of a Cinder volume via libvirt.
2091 
2092         :param instance: VM instance object reference
2093         :param volume_id: id of volume being snapshotted
2094         :param create_info: dict of information used to create snapshots
2095                      - snapshot_id : ID of snapshot
2096                      - type : qcow2 / <other>
2097                      - new_file : qcow2 file created by Cinder which
2098                      becomes the VM's active image after
2099                      the snapshot is complete
2100         """
2101 
2102         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2103                   {'c_info': create_info}, instance=instance)
2104 
2105         try:
2106             guest = self._host.get_guest(instance)
2107         except exception.InstanceNotFound:
2108             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2109 
2110         if create_info['type'] != 'qcow2':
2111             msg = _('Unknown type: %s') % create_info['type']
2112             raise exception.InternalError(msg)
2113 
2114         snapshot_id = create_info.get('snapshot_id', None)
2115         if snapshot_id is None:
2116             msg = _('snapshot_id required in create_info')
2117             raise exception.InternalError(msg)
2118 
2119         try:
2120             self._volume_snapshot_create(context, instance, guest,
2121                                          volume_id, create_info['new_file'])
2122         except Exception:
2123             with excutils.save_and_reraise_exception():
2124                 LOG.exception(_('Error occurred during '
2125                                 'volume_snapshot_create, '
2126                                 'sending error status to Cinder.'),
2127                               instance=instance)
2128                 self._volume_snapshot_update_status(
2129                     context, snapshot_id, 'error')
2130 
2131         self._volume_snapshot_update_status(
2132             context, snapshot_id, 'creating')
2133 
2134         def _wait_for_snapshot():
2135             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2136 
2137             if snapshot.get('status') != 'creating':
2138                 self._volume_refresh_connection_info(context, instance,
2139                                                      volume_id)
2140                 raise loopingcall.LoopingCallDone()
2141 
2142         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2143         timer.start(interval=0.5).wait()
2144 
2145     @staticmethod
2146     def _rebase_with_qemu_img(guest, device, active_disk_object,
2147                               rebase_base):
2148         """Rebase a device tied to a guest using qemu-img.
2149 
2150         :param guest:the Guest which owns the device being rebased
2151         :type guest: nova.virt.libvirt.guest.Guest
2152         :param device: the guest block device to rebase
2153         :type device: nova.virt.libvirt.guest.BlockDevice
2154         :param active_disk_object: the guest block device to rebase
2155         :type active_disk_object: nova.virt.libvirt.config.\
2156                                     LibvirtConfigGuestDisk
2157         :param rebase_base: the new parent in the backing chain
2158         :type rebase_base: None or string
2159         """
2160 
2161         # It's unsure how well qemu-img handles network disks for
2162         # every protocol. So let's be safe.
2163         active_protocol = active_disk_object.source_protocol
2164         if active_protocol is not None:
2165             msg = _("Something went wrong when deleting a volume snapshot: "
2166                     "rebasing a %(protocol)s network disk using qemu-img "
2167                     "has not been fully tested") % {'protocol':
2168                     active_protocol}
2169             LOG.error(msg)
2170             raise exception.InternalError(msg)
2171 
2172         if rebase_base is None:
2173             # If backing_file is specified as "" (the empty string), then
2174             # the image is rebased onto no backing file (i.e. it will exist
2175             # independently of any backing file).
2176             backing_file = ""
2177             qemu_img_extra_arg = []
2178         else:
2179             # If the rebased image is going to have a backing file then
2180             # explicitly set the backing file format to avoid any security
2181             # concerns related to file format auto detection.
2182             backing_file = rebase_base
2183             b_file_fmt = images.qemu_img_info(backing_file).file_format
2184             qemu_img_extra_arg = ['-F', b_file_fmt]
2185 
2186         qemu_img_extra_arg.append(active_disk_object.source_path)
2187         utils.execute("qemu-img", "rebase", "-b", backing_file,
2188                       *qemu_img_extra_arg)
2189 
2190     def _volume_snapshot_delete(self, context, instance, volume_id,
2191                                 snapshot_id, delete_info=None):
2192         """Note:
2193             if file being merged into == active image:
2194                 do a blockRebase (pull) operation
2195             else:
2196                 do a blockCommit operation
2197             Files must be adjacent in snap chain.
2198 
2199         :param instance: instance object reference
2200         :param volume_id: volume UUID
2201         :param snapshot_id: snapshot UUID (unused currently)
2202         :param delete_info: {
2203             'type':              'qcow2',
2204             'file_to_merge':     'a.img',
2205             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2206                                                   active image)
2207           }
2208         """
2209 
2210         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2211                   instance=instance)
2212 
2213         if delete_info['type'] != 'qcow2':
2214             msg = _('Unknown delete_info type %s') % delete_info['type']
2215             raise exception.InternalError(msg)
2216 
2217         try:
2218             guest = self._host.get_guest(instance)
2219         except exception.InstanceNotFound:
2220             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2221 
2222         # Find dev name
2223         my_dev = None
2224         active_disk = None
2225 
2226         xml = guest.get_xml_desc()
2227         xml_doc = etree.fromstring(xml)
2228 
2229         device_info = vconfig.LibvirtConfigGuest()
2230         device_info.parse_dom(xml_doc)
2231 
2232         active_disk_object = None
2233 
2234         for guest_disk in device_info.devices:
2235             if (guest_disk.root_name != 'disk'):
2236                 continue
2237 
2238             if (guest_disk.target_dev is None or guest_disk.serial is None):
2239                 continue
2240 
2241             if guest_disk.serial == volume_id:
2242                 my_dev = guest_disk.target_dev
2243 
2244                 active_disk = guest_disk.source_path
2245                 active_protocol = guest_disk.source_protocol
2246                 active_disk_object = guest_disk
2247                 break
2248 
2249         if my_dev is None or (active_disk is None and active_protocol is None):
2250             LOG.debug('Domain XML: %s', xml, instance=instance)
2251             msg = (_('Disk with id: %s not found attached to instance.')
2252                    % volume_id)
2253             raise exception.InternalError(msg)
2254 
2255         LOG.debug("found device at %s", my_dev, instance=instance)
2256 
2257         def _get_snap_dev(filename, backing_store):
2258             if filename is None:
2259                 msg = _('filename cannot be None')
2260                 raise exception.InternalError(msg)
2261 
2262             # libgfapi delete
2263             LOG.debug("XML: %s", xml)
2264 
2265             LOG.debug("active disk object: %s", active_disk_object)
2266 
2267             # determine reference within backing store for desired image
2268             filename_to_merge = filename
2269             matched_name = None
2270             b = backing_store
2271             index = None
2272 
2273             current_filename = active_disk_object.source_name.split('/')[1]
2274             if current_filename == filename_to_merge:
2275                 return my_dev + '[0]'
2276 
2277             while b is not None:
2278                 source_filename = b.source_name.split('/')[1]
2279                 if source_filename == filename_to_merge:
2280                     LOG.debug('found match: %s', b.source_name)
2281                     matched_name = b.source_name
2282                     index = b.index
2283                     break
2284 
2285                 b = b.backing_store
2286 
2287             if matched_name is None:
2288                 msg = _('no match found for %s') % (filename_to_merge)
2289                 raise exception.InternalError(msg)
2290 
2291             LOG.debug('index of match (%s) is %s', b.source_name, index)
2292 
2293             my_snap_dev = '%s[%s]' % (my_dev, index)
2294             return my_snap_dev
2295 
2296         if delete_info['merge_target_file'] is None:
2297             # pull via blockRebase()
2298 
2299             # Merge the most recent snapshot into the active image
2300 
2301             rebase_disk = my_dev
2302             rebase_base = delete_info['file_to_merge']  # often None
2303             if (active_protocol is not None) and (rebase_base is not None):
2304                 rebase_base = _get_snap_dev(rebase_base,
2305                                             active_disk_object.backing_store)
2306 
2307             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2308             # and when available this flag _must_ be used to ensure backing
2309             # paths are maintained relative by qemu.
2310             #
2311             # If _RELATIVE flag not found, continue with old behaviour
2312             # (relative backing path seems to work for this case)
2313             try:
2314                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2315                 relative = rebase_base is not None
2316             except AttributeError:
2317                 LOG.warning(
2318                     "Relative blockrebase support was not detected. "
2319                     "Continuing with old behaviour.")
2320                 relative = False
2321 
2322             LOG.debug(
2323                 'disk: %(disk)s, base: %(base)s, '
2324                 'bw: %(bw)s, relative: %(relative)s',
2325                 {'disk': rebase_disk,
2326                  'base': rebase_base,
2327                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2328                  'relative': str(relative)}, instance=instance)
2329 
2330             dev = guest.get_block_device(rebase_disk)
2331             if guest.is_active():
2332                 result = dev.rebase(rebase_base, relative=relative)
2333                 if result == 0:
2334                     LOG.debug('blockRebase started successfully',
2335                               instance=instance)
2336 
2337                 while not dev.is_job_complete():
2338                     LOG.debug('waiting for blockRebase job completion',
2339                               instance=instance)
2340                     time.sleep(0.5)
2341 
2342             # If the guest is not running libvirt won't do a blockRebase.
2343             # In that case, let's ask qemu-img to rebase the disk.
2344             else:
2345                 LOG.debug('Guest is not running so doing a block rebase '
2346                           'using "qemu-img rebase"', instance=instance)
2347                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2348                                            rebase_base)
2349 
2350         else:
2351             # commit with blockCommit()
2352             my_snap_base = None
2353             my_snap_top = None
2354             commit_disk = my_dev
2355 
2356             if active_protocol is not None:
2357                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2358                                              active_disk_object.backing_store)
2359                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2360                                             active_disk_object.backing_store)
2361 
2362             commit_base = my_snap_base or delete_info['merge_target_file']
2363             commit_top = my_snap_top or delete_info['file_to_merge']
2364 
2365             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2366                       'commit_base=%(commit_base)s '
2367                       'commit_top=%(commit_top)s ',
2368                       {'commit_disk': commit_disk,
2369                        'commit_base': commit_base,
2370                        'commit_top': commit_top}, instance=instance)
2371 
2372             dev = guest.get_block_device(commit_disk)
2373             result = dev.commit(commit_base, commit_top, relative=True)
2374 
2375             if result == 0:
2376                 LOG.debug('blockCommit started successfully',
2377                           instance=instance)
2378 
2379             while not dev.is_job_complete():
2380                 LOG.debug('waiting for blockCommit job completion',
2381                           instance=instance)
2382                 time.sleep(0.5)
2383 
2384     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2385                                delete_info):
2386         try:
2387             self._volume_snapshot_delete(context, instance, volume_id,
2388                                          snapshot_id, delete_info=delete_info)
2389         except Exception:
2390             with excutils.save_and_reraise_exception():
2391                 LOG.exception(_('Error occurred during '
2392                                 'volume_snapshot_delete, '
2393                                 'sending error status to Cinder.'),
2394                               instance=instance)
2395                 self._volume_snapshot_update_status(
2396                     context, snapshot_id, 'error_deleting')
2397 
2398         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2399         self._volume_refresh_connection_info(context, instance, volume_id)
2400 
2401     def reboot(self, context, instance, network_info, reboot_type,
2402                block_device_info=None, bad_volumes_callback=None):
2403         """Reboot a virtual machine, given an instance reference."""
2404         if reboot_type == 'SOFT':
2405             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2406             try:
2407                 soft_reboot_success = self._soft_reboot(instance)
2408             except libvirt.libvirtError as e:
2409                 LOG.debug("Instance soft reboot failed: %s", e,
2410                           instance=instance)
2411                 soft_reboot_success = False
2412 
2413             if soft_reboot_success:
2414                 LOG.info("Instance soft rebooted successfully.",
2415                          instance=instance)
2416                 return
2417             else:
2418                 LOG.warning("Failed to soft reboot instance. "
2419                             "Trying hard reboot.",
2420                             instance=instance)
2421         return self._hard_reboot(context, instance, network_info,
2422                                  block_device_info)
2423 
2424     def _soft_reboot(self, instance):
2425         """Attempt to shutdown and restart the instance gracefully.
2426 
2427         We use shutdown and create here so we can return if the guest
2428         responded and actually rebooted. Note that this method only
2429         succeeds if the guest responds to acpi. Therefore we return
2430         success or failure so we can fall back to a hard reboot if
2431         necessary.
2432 
2433         :returns: True if the reboot succeeded
2434         """
2435         guest = self._host.get_guest(instance)
2436 
2437         state = guest.get_power_state(self._host)
2438         old_domid = guest.id
2439         # NOTE(vish): This check allows us to reboot an instance that
2440         #             is already shutdown.
2441         if state == power_state.RUNNING:
2442             guest.shutdown()
2443         # NOTE(vish): This actually could take slightly longer than the
2444         #             FLAG defines depending on how long the get_info
2445         #             call takes to return.
2446         self._prepare_pci_devices_for_use(
2447             pci_manager.get_instance_pci_devs(instance, 'all'))
2448         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2449             guest = self._host.get_guest(instance)
2450 
2451             state = guest.get_power_state(self._host)
2452             new_domid = guest.id
2453 
2454             # NOTE(ivoks): By checking domain IDs, we make sure we are
2455             #              not recreating domain that's already running.
2456             if old_domid != new_domid:
2457                 if state in [power_state.SHUTDOWN,
2458                              power_state.CRASHED]:
2459                     LOG.info("Instance shutdown successfully.",
2460                              instance=instance)
2461                     self._create_domain(domain=guest._domain)
2462                     timer = loopingcall.FixedIntervalLoopingCall(
2463                         self._wait_for_running, instance)
2464                     timer.start(interval=0.5).wait()
2465                     return True
2466                 else:
2467                     LOG.info("Instance may have been rebooted during soft "
2468                              "reboot, so return now.", instance=instance)
2469                     return True
2470             greenthread.sleep(1)
2471         return False
2472 
2473     def _hard_reboot(self, context, instance, network_info,
2474                      block_device_info=None):
2475         """Reboot a virtual machine, given an instance reference.
2476 
2477         Performs a Libvirt reset (if supported) on the domain.
2478 
2479         If Libvirt reset is unavailable this method actually destroys and
2480         re-creates the domain to ensure the reboot happens, as the guest
2481         OS cannot ignore this action.
2482         """
2483         # NOTE(lyarwood): Start by fully destroying the instance, undefining
2484         # the domain and detaching vifs, volumes and encryptors.
2485         self.destroy(context, instance, network_info, destroy_disks=False,
2486                      block_device_info=block_device_info)
2487 
2488         # Convert the system metadata to image metadata
2489         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2490         #                https://bugs.launchpad.net/nova/+bug/1349978
2491         instance_dir = libvirt_utils.get_instance_path(instance)
2492         fileutils.ensure_tree(instance_dir)
2493 
2494         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2495                                             instance,
2496                                             instance.image_meta,
2497                                             block_device_info)
2498         # NOTE(vish): This could generate the wrong device_format if we are
2499         #             using the raw backend and the images don't exist yet.
2500         #             The create_images_and_backing below doesn't properly
2501         #             regenerate raw backend images, however, so when it
2502         #             does we need to (re)generate the xml after the images
2503         #             are in place.
2504         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2505                                   instance.image_meta,
2506                                   block_device_info=block_device_info)
2507 
2508         # NOTE(mdbooth): context.auth_token will not be set when we call
2509         #                _hard_reboot from resume_state_on_host_boot()
2510         if context.auth_token is not None:
2511             # NOTE (rmk): Re-populate any missing backing files.
2512             config = vconfig.LibvirtConfigGuest()
2513             config.parse_str(xml)
2514             backing_disk_info = self._get_instance_disk_info_from_config(
2515                 config, block_device_info)
2516             self._create_images_and_backing(context, instance, instance_dir,
2517                                             backing_disk_info)
2518 
2519         # Initialize all the necessary networking, block devices and
2520         # start the instance.
2521         self._create_domain_and_network(context, xml, instance, network_info,
2522                                         block_device_info=block_device_info)
2523         self._prepare_pci_devices_for_use(
2524             pci_manager.get_instance_pci_devs(instance, 'all'))
2525 
2526         def _wait_for_reboot():
2527             """Called at an interval until the VM is running again."""
2528             state = self.get_info(instance).state
2529 
2530             if state == power_state.RUNNING:
2531                 LOG.info("Instance rebooted successfully.",
2532                          instance=instance)
2533                 raise loopingcall.LoopingCallDone()
2534 
2535         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2536         timer.start(interval=0.5).wait()
2537 
2538     def pause(self, instance):
2539         """Pause VM instance."""
2540         self._host.get_guest(instance).pause()
2541 
2542     def unpause(self, instance):
2543         """Unpause paused VM instance."""
2544         guest = self._host.get_guest(instance)
2545         guest.resume()
2546         guest.sync_guest_time()
2547 
2548     def _clean_shutdown(self, instance, timeout, retry_interval):
2549         """Attempt to shutdown the instance gracefully.
2550 
2551         :param instance: The instance to be shutdown
2552         :param timeout: How long to wait in seconds for the instance to
2553                         shutdown
2554         :param retry_interval: How often in seconds to signal the instance
2555                                to shutdown while waiting
2556 
2557         :returns: True if the shutdown succeeded
2558         """
2559 
2560         # List of states that represent a shutdown instance
2561         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2562                            power_state.CRASHED]
2563 
2564         try:
2565             guest = self._host.get_guest(instance)
2566         except exception.InstanceNotFound:
2567             # If the instance has gone then we don't need to
2568             # wait for it to shutdown
2569             return True
2570 
2571         state = guest.get_power_state(self._host)
2572         if state in SHUTDOWN_STATES:
2573             LOG.info("Instance already shutdown.", instance=instance)
2574             return True
2575 
2576         LOG.debug("Shutting down instance from state %s", state,
2577                   instance=instance)
2578         guest.shutdown()
2579         retry_countdown = retry_interval
2580 
2581         for sec in range(timeout):
2582 
2583             guest = self._host.get_guest(instance)
2584             state = guest.get_power_state(self._host)
2585 
2586             if state in SHUTDOWN_STATES:
2587                 LOG.info("Instance shutdown successfully after %d seconds.",
2588                          sec, instance=instance)
2589                 return True
2590 
2591             # Note(PhilD): We can't assume that the Guest was able to process
2592             #              any previous shutdown signal (for example it may
2593             #              have still been startingup, so within the overall
2594             #              timeout we re-trigger the shutdown every
2595             #              retry_interval
2596             if retry_countdown == 0:
2597                 retry_countdown = retry_interval
2598                 # Instance could shutdown at any time, in which case we
2599                 # will get an exception when we call shutdown
2600                 try:
2601                     LOG.debug("Instance in state %s after %d seconds - "
2602                               "resending shutdown", state, sec,
2603                               instance=instance)
2604                     guest.shutdown()
2605                 except libvirt.libvirtError:
2606                     # Assume this is because its now shutdown, so loop
2607                     # one more time to clean up.
2608                     LOG.debug("Ignoring libvirt exception from shutdown "
2609                               "request.", instance=instance)
2610                     continue
2611             else:
2612                 retry_countdown -= 1
2613 
2614             time.sleep(1)
2615 
2616         LOG.info("Instance failed to shutdown in %d seconds.",
2617                  timeout, instance=instance)
2618         return False
2619 
2620     def power_off(self, instance, timeout=0, retry_interval=0):
2621         """Power off the specified instance."""
2622         if timeout:
2623             self._clean_shutdown(instance, timeout, retry_interval)
2624         self._destroy(instance)
2625 
2626     def power_on(self, context, instance, network_info,
2627                  block_device_info=None):
2628         """Power on the specified instance."""
2629         # We use _hard_reboot here to ensure that all backing files,
2630         # network, and block device connections, etc. are established
2631         # and available before we attempt to start the instance.
2632         self._hard_reboot(context, instance, network_info, block_device_info)
2633 
2634     def trigger_crash_dump(self, instance):
2635 
2636         """Trigger crash dump by injecting an NMI to the specified instance."""
2637         try:
2638             self._host.get_guest(instance).inject_nmi()
2639         except libvirt.libvirtError as ex:
2640             error_code = ex.get_error_code()
2641 
2642             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2643                 raise exception.TriggerCrashDumpNotSupported()
2644             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2645                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2646 
2647             LOG.exception(_('Error from libvirt while injecting an NMI to '
2648                             '%(instance_uuid)s: '
2649                             '[Error Code %(error_code)s] %(ex)s'),
2650                           {'instance_uuid': instance.uuid,
2651                            'error_code': error_code, 'ex': ex})
2652             raise
2653 
2654     def suspend(self, context, instance):
2655         """Suspend the specified instance."""
2656         guest = self._host.get_guest(instance)
2657 
2658         self._detach_pci_devices(guest,
2659             pci_manager.get_instance_pci_devs(instance))
2660         self._detach_direct_passthrough_ports(context, instance, guest)
2661         guest.save_memory_state()
2662 
2663     def resume(self, context, instance, network_info, block_device_info=None):
2664         """resume the specified instance."""
2665         xml = self._get_existing_domain_xml(instance, network_info,
2666                                             block_device_info)
2667         guest = self._create_domain_and_network(context, xml, instance,
2668                            network_info, block_device_info=block_device_info,
2669                            vifs_already_plugged=True)
2670         self._attach_pci_devices(guest,
2671             pci_manager.get_instance_pci_devs(instance))
2672         self._attach_direct_passthrough_ports(
2673             context, instance, guest, network_info)
2674         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2675                                                      instance)
2676         timer.start(interval=0.5).wait()
2677         guest.sync_guest_time()
2678 
2679     def resume_state_on_host_boot(self, context, instance, network_info,
2680                                   block_device_info=None):
2681         """resume guest state when a host is booted."""
2682         # Check if the instance is running already and avoid doing
2683         # anything if it is.
2684         try:
2685             guest = self._host.get_guest(instance)
2686             state = guest.get_power_state(self._host)
2687 
2688             ignored_states = (power_state.RUNNING,
2689                               power_state.SUSPENDED,
2690                               power_state.NOSTATE,
2691                               power_state.PAUSED)
2692 
2693             if state in ignored_states:
2694                 return
2695         except (exception.InternalError, exception.InstanceNotFound):
2696             pass
2697 
2698         # Instance is not up and could be in an unknown state.
2699         # Be as absolute as possible about getting it back into
2700         # a known and running state.
2701         self._hard_reboot(context, instance, network_info, block_device_info)
2702 
2703     def rescue(self, context, instance, network_info, image_meta,
2704                rescue_password):
2705         """Loads a VM using rescue images.
2706 
2707         A rescue is normally performed when something goes wrong with the
2708         primary images and data needs to be corrected/recovered. Rescuing
2709         should not edit or over-ride the original image, only allow for
2710         data recovery.
2711 
2712         """
2713         instance_dir = libvirt_utils.get_instance_path(instance)
2714         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2715         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2716         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2717 
2718         rescue_image_id = None
2719         if image_meta.obj_attr_is_set("id"):
2720             rescue_image_id = image_meta.id
2721 
2722         rescue_images = {
2723             'image_id': (rescue_image_id or
2724                         CONF.libvirt.rescue_image_id or instance.image_ref),
2725             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2726                           instance.kernel_id),
2727             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2728                            instance.ramdisk_id),
2729         }
2730         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2731                                             instance,
2732                                             image_meta,
2733                                             rescue=True)
2734         injection_info = InjectionInfo(network_info=network_info,
2735                                        admin_pass=rescue_password,
2736                                        files=None)
2737         gen_confdrive = functools.partial(self._create_configdrive,
2738                                           context, instance, injection_info,
2739                                           rescue=True)
2740         self._create_image(context, instance, disk_info['mapping'],
2741                            injection_info=injection_info, suffix='.rescue',
2742                            disk_images=rescue_images)
2743         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2744                                   image_meta, rescue=rescue_images)
2745         self._destroy(instance)
2746         self._create_domain(xml, post_xml_callback=gen_confdrive)
2747 
2748     def unrescue(self, instance, network_info):
2749         """Reboot the VM which is being rescued back into primary images.
2750         """
2751         instance_dir = libvirt_utils.get_instance_path(instance)
2752         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2753         xml = libvirt_utils.load_file(unrescue_xml_path)
2754         guest = self._host.get_guest(instance)
2755 
2756         # TODO(sahid): We are converting all calls from a
2757         # virDomain object to use nova.virt.libvirt.Guest.
2758         # We should be able to remove virt_dom at the end.
2759         virt_dom = guest._domain
2760         self._destroy(instance)
2761         self._create_domain(xml, virt_dom)
2762         os.unlink(unrescue_xml_path)
2763         rescue_files = os.path.join(instance_dir, "*.rescue")
2764         for rescue_file in glob.iglob(rescue_files):
2765             if os.path.isdir(rescue_file):
2766                 shutil.rmtree(rescue_file)
2767             else:
2768                 os.unlink(rescue_file)
2769         # cleanup rescue volume
2770         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2771                                 if lvmdisk.endswith('.rescue')])
2772         if CONF.libvirt.images_type == 'rbd':
2773             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
2774                                       disk.endswith('.rescue'))
2775             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
2776 
2777     def poll_rebooting_instances(self, timeout, instances):
2778         pass
2779 
2780     # NOTE(ilyaalekseyev): Implementation like in multinics
2781     # for xenapi(tr3buchet)
2782     def spawn(self, context, instance, image_meta, injected_files,
2783               admin_password, network_info=None, block_device_info=None):
2784         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2785                                             instance,
2786                                             image_meta,
2787                                             block_device_info)
2788         injection_info = InjectionInfo(network_info=network_info,
2789                                        files=injected_files,
2790                                        admin_pass=admin_password)
2791         gen_confdrive = functools.partial(self._create_configdrive,
2792                                           context, instance,
2793                                           injection_info)
2794         self._create_image(context, instance, disk_info['mapping'],
2795                            injection_info=injection_info,
2796                            block_device_info=block_device_info)
2797 
2798         # Required by Quobyte CI
2799         self._ensure_console_log_for_instance(instance)
2800 
2801         xml = self._get_guest_xml(context, instance, network_info,
2802                                   disk_info, image_meta,
2803                                   block_device_info=block_device_info)
2804         self._create_domain_and_network(
2805             context, xml, instance, network_info,
2806             block_device_info=block_device_info,
2807             post_xml_callback=gen_confdrive,
2808             destroy_disks_on_failure=True)
2809         LOG.debug("Instance is running", instance=instance)
2810 
2811         def _wait_for_boot():
2812             """Called at an interval until the VM is running."""
2813             state = self.get_info(instance).state
2814 
2815             if state == power_state.RUNNING:
2816                 LOG.info("Instance spawned successfully.", instance=instance)
2817                 raise loopingcall.LoopingCallDone()
2818 
2819         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
2820         timer.start(interval=0.5).wait()
2821 
2822     def _get_console_output_file(self, instance, console_log):
2823         bytes_to_read = MAX_CONSOLE_BYTES
2824         log_data = b""  # The last N read bytes
2825         i = 0  # in case there is a log rotation (like "virtlogd")
2826         path = console_log
2827 
2828         while bytes_to_read > 0 and os.path.exists(path):
2829             read_log_data, remaining = nova.privsep.libvirt.last_bytes(
2830                                         path, bytes_to_read)
2831             # We need the log file content in chronological order,
2832             # that's why we *prepend* the log data.
2833             log_data = read_log_data + log_data
2834 
2835             # Prep to read the next file in the chain
2836             bytes_to_read -= len(read_log_data)
2837             path = console_log + "." + str(i)
2838             i += 1
2839 
2840             if remaining > 0:
2841                 LOG.info('Truncated console log returned, '
2842                          '%d bytes ignored', remaining, instance=instance)
2843         return log_data
2844 
2845     def get_console_output(self, context, instance):
2846         guest = self._host.get_guest(instance)
2847 
2848         xml = guest.get_xml_desc()
2849         tree = etree.fromstring(xml)
2850 
2851         # If the guest has a console logging to a file prefer to use that
2852         file_consoles = tree.findall("./devices/console[@type='file']")
2853         if file_consoles:
2854             for file_console in file_consoles:
2855                 source_node = file_console.find('./source')
2856                 if source_node is None:
2857                     continue
2858                 path = source_node.get("path")
2859                 if not path:
2860                     continue
2861 
2862                 if not os.path.exists(path):
2863                     LOG.info('Instance is configured with a file console, '
2864                              'but the backing file is not (yet?) present',
2865                              instance=instance)
2866                     return ""
2867 
2868                 return self._get_console_output_file(instance, path)
2869 
2870         # Try 'pty' types
2871         pty_consoles = tree.findall("./devices/console[@type='pty']")
2872         if pty_consoles:
2873             for pty_console in pty_consoles:
2874                 source_node = pty_console.find('./source')
2875                 if source_node is None:
2876                     continue
2877                 pty = source_node.get("path")
2878                 if not pty:
2879                     continue
2880                 break
2881             else:
2882                 raise exception.ConsoleNotAvailable()
2883         else:
2884             raise exception.ConsoleNotAvailable()
2885 
2886         console_log = self._get_console_log_path(instance)
2887         data = nova.privsep.libvirt.readpty(pty)
2888 
2889         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
2890         # which create a dedicated file device for the console logging.
2891         # Other virt_types like xen, lxc, uml, parallels depend on the
2892         # flush of that pty device into the "console.log" file to ensure
2893         # that a series of "get_console_output" calls return the complete
2894         # content even after rebooting a guest.
2895         nova.privsep.path.writefile(console_log, 'a+', data)
2896         return self._get_console_output_file(instance, console_log)
2897 
2898     def get_host_ip_addr(self):
2899         ips = compute_utils.get_machine_ips()
2900         if CONF.my_ip not in ips:
2901             LOG.warning('my_ip address (%(my_ip)s) was not found on '
2902                         'any of the interfaces: %(ifaces)s',
2903                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
2904         return CONF.my_ip
2905 
2906     def get_vnc_console(self, context, instance):
2907         def get_vnc_port_for_instance(instance_name):
2908             guest = self._host.get_guest(instance)
2909 
2910             xml = guest.get_xml_desc()
2911             xml_dom = etree.fromstring(xml)
2912 
2913             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
2914             if graphic is not None:
2915                 return graphic.get('port')
2916             # NOTE(rmk): We had VNC consoles enabled but the instance in
2917             # question is not actually listening for connections.
2918             raise exception.ConsoleTypeUnavailable(console_type='vnc')
2919 
2920         port = get_vnc_port_for_instance(instance.name)
2921         host = CONF.vnc.server_proxyclient_address
2922 
2923         return ctype.ConsoleVNC(host=host, port=port)
2924 
2925     def get_spice_console(self, context, instance):
2926         def get_spice_ports_for_instance(instance_name):
2927             guest = self._host.get_guest(instance)
2928 
2929             xml = guest.get_xml_desc()
2930             xml_dom = etree.fromstring(xml)
2931 
2932             graphic = xml_dom.find("./devices/graphics[@type='spice']")
2933             if graphic is not None:
2934                 return (graphic.get('port'), graphic.get('tlsPort'))
2935             # NOTE(rmk): We had Spice consoles enabled but the instance in
2936             # question is not actually listening for connections.
2937             raise exception.ConsoleTypeUnavailable(console_type='spice')
2938 
2939         ports = get_spice_ports_for_instance(instance.name)
2940         host = CONF.spice.server_proxyclient_address
2941 
2942         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
2943 
2944     def get_serial_console(self, context, instance):
2945         guest = self._host.get_guest(instance)
2946         for hostname, port in self._get_serial_ports_from_guest(
2947                 guest, mode='bind'):
2948             return ctype.ConsoleSerial(host=hostname, port=port)
2949         raise exception.ConsoleTypeUnavailable(console_type='serial')
2950 
2951     @staticmethod
2952     def _supports_direct_io(dirpath):
2953 
2954         if not hasattr(os, 'O_DIRECT'):
2955             LOG.debug("This python runtime does not support direct I/O")
2956             return False
2957 
2958         testfile = os.path.join(dirpath, ".directio.test")
2959 
2960         hasDirectIO = True
2961         fd = None
2962         try:
2963             fd = os.open(testfile, os.O_CREAT | os.O_WRONLY | os.O_DIRECT)
2964             # Check is the write allowed with 512 byte alignment
2965             align_size = 512
2966             m = mmap.mmap(-1, align_size)
2967             m.write(b"x" * align_size)
2968             os.write(fd, m)
2969             LOG.debug("Path '%(path)s' supports direct I/O",
2970                       {'path': dirpath})
2971         except OSError as e:
2972             if e.errno == errno.EINVAL:
2973                 LOG.debug("Path '%(path)s' does not support direct I/O: "
2974                           "'%(ex)s'", {'path': dirpath, 'ex': e})
2975                 hasDirectIO = False
2976             else:
2977                 with excutils.save_and_reraise_exception():
2978                     LOG.error("Error on '%(path)s' while checking "
2979                               "direct I/O: '%(ex)s'",
2980                               {'path': dirpath, 'ex': e})
2981         except Exception as e:
2982             with excutils.save_and_reraise_exception():
2983                 LOG.error("Error on '%(path)s' while checking direct I/O: "
2984                           "'%(ex)s'", {'path': dirpath, 'ex': e})
2985         finally:
2986             # ensure unlink(filepath) will actually remove the file by deleting
2987             # the remaining link to it in close(fd)
2988             if fd is not None:
2989                 os.close(fd)
2990 
2991             try:
2992                 os.unlink(testfile)
2993             except Exception:
2994                 pass
2995 
2996         return hasDirectIO
2997 
2998     @staticmethod
2999     def _create_ephemeral(target, ephemeral_size,
3000                           fs_label, os_type, is_block_dev=False,
3001                           context=None, specified_fs=None,
3002                           vm_mode=None):
3003         if not is_block_dev:
3004             if (CONF.libvirt.virt_type == "parallels" and
3005                     vm_mode == fields.VMMode.EXE):
3006 
3007                 libvirt_utils.create_ploop_image('expanded', target,
3008                                                  '%dG' % ephemeral_size,
3009                                                  specified_fs)
3010                 return
3011             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3012 
3013         # Run as root only for block devices.
3014         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3015                       specified_fs=specified_fs)
3016 
3017     @staticmethod
3018     def _create_swap(target, swap_mb, context=None):
3019         """Create a swap file of specified size."""
3020         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3021         utils.mkfs('swap', target)
3022 
3023     @staticmethod
3024     def _get_console_log_path(instance):
3025         return os.path.join(libvirt_utils.get_instance_path(instance),
3026                             'console.log')
3027 
3028     def _ensure_console_log_for_instance(self, instance):
3029         # NOTE(mdbooth): Although libvirt will create this file for us
3030         # automatically when it starts, it will initially create it with
3031         # root ownership and then chown it depending on the configuration of
3032         # the domain it is launching. Quobyte CI explicitly disables the
3033         # chown by setting dynamic_ownership=0 in libvirt's config.
3034         # Consequently when the domain starts it is unable to write to its
3035         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3036         #
3037         # To work around this, we create the file manually before starting
3038         # the domain so it has the same ownership as Nova. This works
3039         # for Quobyte CI because it is also configured to run qemu as the same
3040         # user as the Nova service. Installations which don't set
3041         # dynamic_ownership=0 are not affected because libvirt will always
3042         # correctly configure permissions regardless of initial ownership.
3043         #
3044         # Setting dynamic_ownership=0 is dubious and potentially broken in
3045         # more ways than console.log (see comment #22 on the above bug), so
3046         # Future Maintainer who finds this code problematic should check to see
3047         # if we still support it.
3048         console_file = self._get_console_log_path(instance)
3049         LOG.debug('Ensure instance console log exists: %s', console_file,
3050                   instance=instance)
3051         try:
3052             libvirt_utils.file_open(console_file, 'a').close()
3053         # NOTE(sfinucan): We can safely ignore permission issues here and
3054         # assume that it is libvirt that has taken ownership of this file.
3055         except IOError as ex:
3056             if ex.errno != errno.EACCES:
3057                 raise
3058             LOG.debug('Console file already exists: %s.', console_file)
3059 
3060     @staticmethod
3061     def _get_disk_config_image_type():
3062         # TODO(mikal): there is a bug here if images_type has
3063         # changed since creation of the instance, but I am pretty
3064         # sure that this bug already exists.
3065         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3066 
3067     @staticmethod
3068     def _is_booted_from_volume(block_device_info):
3069         """Determines whether the VM is booting from volume
3070 
3071         Determines whether the block device info indicates that the VM
3072         is booting from a volume.
3073         """
3074         block_device_mapping = driver.block_device_info_get_mapping(
3075             block_device_info)
3076         return bool(block_device.get_root_bdm(block_device_mapping))
3077 
3078     def _inject_data(self, disk, instance, injection_info):
3079         """Injects data in a disk image
3080 
3081         Helper used for injecting data in a disk image file system.
3082 
3083         :param disk: The disk we're injecting into (an Image object)
3084         :param instance: The instance we're injecting into
3085         :param injection_info: Injection info
3086         """
3087         # Handles the partition need to be used.
3088         LOG.debug('Checking root disk injection %(info)s',
3089                   info=str(injection_info), instance=instance)
3090         target_partition = None
3091         if not instance.kernel_id:
3092             target_partition = CONF.libvirt.inject_partition
3093             if target_partition == 0:
3094                 target_partition = None
3095         if CONF.libvirt.virt_type == 'lxc':
3096             target_partition = None
3097 
3098         # Handles the key injection.
3099         if CONF.libvirt.inject_key and instance.get('key_data'):
3100             key = str(instance.key_data)
3101         else:
3102             key = None
3103 
3104         # Handles the admin password injection.
3105         if not CONF.libvirt.inject_password:
3106             admin_pass = None
3107         else:
3108             admin_pass = injection_info.admin_pass
3109 
3110         # Handles the network injection.
3111         net = netutils.get_injected_network_template(
3112             injection_info.network_info,
3113             libvirt_virt_type=CONF.libvirt.virt_type)
3114 
3115         # Handles the metadata injection
3116         metadata = instance.get('metadata')
3117 
3118         if any((key, net, metadata, admin_pass, injection_info.files)):
3119             LOG.debug('Injecting %(info)s', info=str(injection_info),
3120                       instance=instance)
3121             img_id = instance.image_ref
3122             try:
3123                 disk_api.inject_data(disk.get_model(self._conn),
3124                                      key, net, metadata, admin_pass,
3125                                      injection_info.files,
3126                                      partition=target_partition,
3127                                      mandatory=('files',))
3128             except Exception as e:
3129                 with excutils.save_and_reraise_exception():
3130                     LOG.error('Error injecting data into image '
3131                               '%(img_id)s (%(e)s)',
3132                               {'img_id': img_id, 'e': e},
3133                               instance=instance)
3134 
3135     # NOTE(sileht): many callers of this method assume that this
3136     # method doesn't fail if an image already exists but instead
3137     # think that it will be reused (ie: (live)-migration/resize)
3138     def _create_image(self, context, instance,
3139                       disk_mapping, injection_info=None, suffix='',
3140                       disk_images=None, block_device_info=None,
3141                       fallback_from_host=None,
3142                       ignore_bdi_for_swap=False):
3143         booted_from_volume = self._is_booted_from_volume(block_device_info)
3144 
3145         def image(fname, image_type=CONF.libvirt.images_type):
3146             return self.image_backend.by_name(instance,
3147                                               fname + suffix, image_type)
3148 
3149         def raw(fname):
3150             return image(fname, image_type='raw')
3151 
3152         # ensure directories exist and are writable
3153         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3154 
3155         LOG.info('Creating image', instance=instance)
3156 
3157         inst_type = instance.get_flavor()
3158         swap_mb = 0
3159         if 'disk.swap' in disk_mapping:
3160             mapping = disk_mapping['disk.swap']
3161 
3162             if ignore_bdi_for_swap:
3163                 # This is a workaround to support legacy swap resizing,
3164                 # which does not touch swap size specified in bdm,
3165                 # but works with flavor specified size only.
3166                 # In this case we follow the legacy logic and ignore block
3167                 # device info completely.
3168                 # NOTE(ft): This workaround must be removed when a correct
3169                 # implementation of resize operation changing sizes in bdms is
3170                 # developed. Also at that stage we probably may get rid of
3171                 # the direct usage of flavor swap size here,
3172                 # leaving the work with bdm only.
3173                 swap_mb = inst_type['swap']
3174             else:
3175                 swap = driver.block_device_info_get_swap(block_device_info)
3176                 if driver.swap_is_usable(swap):
3177                     swap_mb = swap['swap_size']
3178                 elif (inst_type['swap'] > 0 and
3179                       not block_device.volume_in_mapping(
3180                         mapping['dev'], block_device_info)):
3181                     swap_mb = inst_type['swap']
3182 
3183             if swap_mb > 0:
3184                 if (CONF.libvirt.virt_type == "parallels" and
3185                         instance.vm_mode == fields.VMMode.EXE):
3186                     msg = _("Swap disk is not supported "
3187                             "for Virtuozzo container")
3188                     raise exception.Invalid(msg)
3189 
3190         if not disk_images:
3191             disk_images = {'image_id': instance.image_ref,
3192                            'kernel_id': instance.kernel_id,
3193                            'ramdisk_id': instance.ramdisk_id}
3194 
3195         if disk_images['kernel_id']:
3196             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3197             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3198                                 context=context,
3199                                 filename=fname,
3200                                 image_id=disk_images['kernel_id'])
3201             if disk_images['ramdisk_id']:
3202                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3203                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3204                                      context=context,
3205                                      filename=fname,
3206                                      image_id=disk_images['ramdisk_id'])
3207 
3208         if CONF.libvirt.virt_type == 'uml':
3209             # PONDERING(mikal): can I assume that root is UID zero in every
3210             # OS? Probably not.
3211             uid = pwd.getpwnam('root').pw_uid
3212             nova.privsep.path.chown(image('disk').path, uid=uid)
3213 
3214         self._create_and_inject_local_root(context, instance,
3215                                            booted_from_volume, suffix,
3216                                            disk_images, injection_info,
3217                                            fallback_from_host)
3218 
3219         # Lookup the filesystem type if required
3220         os_type_with_default = disk_api.get_fs_type_for_os_type(
3221             instance.os_type)
3222         # Generate a file extension based on the file system
3223         # type and the mkfs commands configured if any
3224         file_extension = disk_api.get_file_extension_for_os_type(
3225                                                           os_type_with_default)
3226 
3227         vm_mode = fields.VMMode.get_from_instance(instance)
3228         ephemeral_gb = instance.flavor.ephemeral_gb
3229         if 'disk.local' in disk_mapping:
3230             disk_image = image('disk.local')
3231             fn = functools.partial(self._create_ephemeral,
3232                                    fs_label='ephemeral0',
3233                                    os_type=instance.os_type,
3234                                    is_block_dev=disk_image.is_block_dev,
3235                                    vm_mode=vm_mode)
3236             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3237             size = ephemeral_gb * units.Gi
3238             disk_image.cache(fetch_func=fn,
3239                              context=context,
3240                              filename=fname,
3241                              size=size,
3242                              ephemeral_size=ephemeral_gb)
3243 
3244         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3245                 block_device_info)):
3246             disk_image = image(blockinfo.get_eph_disk(idx))
3247 
3248             specified_fs = eph.get('guest_format')
3249             if specified_fs and not self.is_supported_fs_format(specified_fs):
3250                 msg = _("%s format is not supported") % specified_fs
3251                 raise exception.InvalidBDMFormat(details=msg)
3252 
3253             fn = functools.partial(self._create_ephemeral,
3254                                    fs_label='ephemeral%d' % idx,
3255                                    os_type=instance.os_type,
3256                                    is_block_dev=disk_image.is_block_dev,
3257                                    vm_mode=vm_mode)
3258             size = eph['size'] * units.Gi
3259             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3260             disk_image.cache(fetch_func=fn,
3261                              context=context,
3262                              filename=fname,
3263                              size=size,
3264                              ephemeral_size=eph['size'],
3265                              specified_fs=specified_fs)
3266 
3267         if swap_mb > 0:
3268             size = swap_mb * units.Mi
3269             image('disk.swap').cache(fetch_func=self._create_swap,
3270                                      context=context,
3271                                      filename="swap_%s" % swap_mb,
3272                                      size=size,
3273                                      swap_mb=swap_mb)
3274 
3275     def _create_and_inject_local_root(self, context, instance,
3276                                       booted_from_volume, suffix, disk_images,
3277                                       injection_info, fallback_from_host):
3278         # File injection only if needed
3279         need_inject = (not configdrive.required_by(instance) and
3280                        injection_info is not None and
3281                        CONF.libvirt.inject_partition != -2)
3282 
3283         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3284         # currently happens only on rescue - we still don't want to
3285         # create a base image.
3286         if not booted_from_volume:
3287             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3288             size = instance.flavor.root_gb * units.Gi
3289 
3290             if size == 0 or suffix == '.rescue':
3291                 size = None
3292 
3293             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3294                                                  CONF.libvirt.images_type)
3295             if instance.task_state == task_states.RESIZE_FINISH:
3296                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3297             if backend.SUPPORTS_CLONE:
3298                 def clone_fallback_to_fetch(*args, **kwargs):
3299                     try:
3300                         backend.clone(context, disk_images['image_id'])
3301                     except exception.ImageUnacceptable:
3302                         libvirt_utils.fetch_image(*args, **kwargs)
3303                 fetch_func = clone_fallback_to_fetch
3304             else:
3305                 fetch_func = libvirt_utils.fetch_image
3306             self._try_fetch_image_cache(backend, fetch_func, context,
3307                                         root_fname, disk_images['image_id'],
3308                                         instance, size, fallback_from_host)
3309 
3310             if need_inject:
3311                 self._inject_data(backend, instance, injection_info)
3312 
3313         elif need_inject:
3314             LOG.warning('File injection into a boot from volume '
3315                         'instance is not supported', instance=instance)
3316 
3317     def _create_configdrive(self, context, instance, injection_info,
3318                             rescue=False):
3319         # As this method being called right after the definition of a
3320         # domain, but before its actual launch, device metadata will be built
3321         # and saved in the instance for it to be used by the config drive and
3322         # the metadata service.
3323         instance.device_metadata = self._build_device_metadata(context,
3324                                                                instance)
3325         if configdrive.required_by(instance):
3326             LOG.info('Using config drive', instance=instance)
3327 
3328             name = 'disk.config'
3329             if rescue:
3330                 name += '.rescue'
3331 
3332             config_disk = self.image_backend.by_name(
3333                 instance, name, self._get_disk_config_image_type())
3334 
3335             # Don't overwrite an existing config drive
3336             if not config_disk.exists():
3337                 extra_md = {}
3338                 if injection_info.admin_pass:
3339                     extra_md['admin_pass'] = injection_info.admin_pass
3340 
3341                 inst_md = instance_metadata.InstanceMetadata(
3342                     instance, content=injection_info.files, extra_md=extra_md,
3343                     network_info=injection_info.network_info,
3344                     request_context=context)
3345 
3346                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3347                 with cdb:
3348                     # NOTE(mdbooth): We're hardcoding here the path of the
3349                     # config disk when using the flat backend. This isn't
3350                     # good, but it's required because we need a local path we
3351                     # know we can write to in case we're subsequently
3352                     # importing into rbd. This will be cleaned up when we
3353                     # replace this with a call to create_from_func, but that
3354                     # can't happen until we've updated the backends and we
3355                     # teach them not to cache config disks. This isn't
3356                     # possible while we're still using cache() under the hood.
3357                     config_disk_local_path = os.path.join(
3358                         libvirt_utils.get_instance_path(instance), name)
3359                     LOG.info('Creating config drive at %(path)s',
3360                              {'path': config_disk_local_path},
3361                              instance=instance)
3362 
3363                     try:
3364                         cdb.make_drive(config_disk_local_path)
3365                     except processutils.ProcessExecutionError as e:
3366                         with excutils.save_and_reraise_exception():
3367                             LOG.error('Creating config drive failed with '
3368                                       'error: %s', e, instance=instance)
3369 
3370                 try:
3371                     config_disk.import_file(
3372                         instance, config_disk_local_path, name)
3373                 finally:
3374                     # NOTE(mikal): if the config drive was imported into RBD,
3375                     # then we no longer need the local copy
3376                     if CONF.libvirt.images_type == 'rbd':
3377                         LOG.info('Deleting local config drive %(path)s '
3378                                  'because it was imported into RBD.',
3379                                  {'path': config_disk_local_path},
3380                                  instance=instance)
3381                         os.unlink(config_disk_local_path)
3382 
3383     def _prepare_pci_devices_for_use(self, pci_devices):
3384         # kvm , qemu support managed mode
3385         # In managed mode, the configured device will be automatically
3386         # detached from the host OS drivers when the guest is started,
3387         # and then re-attached when the guest shuts down.
3388         if CONF.libvirt.virt_type != 'xen':
3389             # we do manual detach only for xen
3390             return
3391         try:
3392             for dev in pci_devices:
3393                 libvirt_dev_addr = dev['hypervisor_name']
3394                 libvirt_dev = \
3395                         self._host.device_lookup_by_name(libvirt_dev_addr)
3396                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3397                 # http://libvirt.org/html/libvirt-libvirt.html.
3398                 libvirt_dev.dettach()
3399 
3400             # Note(yjiang5): A reset of one PCI device may impact other
3401             # devices on the same bus, thus we need two separated loops
3402             # to detach and then reset it.
3403             for dev in pci_devices:
3404                 libvirt_dev_addr = dev['hypervisor_name']
3405                 libvirt_dev = \
3406                         self._host.device_lookup_by_name(libvirt_dev_addr)
3407                 libvirt_dev.reset()
3408 
3409         except libvirt.libvirtError as exc:
3410             raise exception.PciDevicePrepareFailed(id=dev['id'],
3411                                                    instance_uuid=
3412                                                    dev['instance_uuid'],
3413                                                    reason=six.text_type(exc))
3414 
3415     def _detach_pci_devices(self, guest, pci_devs):
3416         try:
3417             for dev in pci_devs:
3418                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3419                 # after detachDeviceFlags returned, we should check the dom to
3420                 # ensure the detaching is finished
3421                 xml = guest.get_xml_desc()
3422                 xml_doc = etree.fromstring(xml)
3423                 guest_config = vconfig.LibvirtConfigGuest()
3424                 guest_config.parse_dom(xml_doc)
3425 
3426                 for hdev in [d for d in guest_config.devices
3427                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3428                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3429                     dbsf = pci_utils.parse_address(dev.address)
3430                     if [int(x, 16) for x in hdbsf] ==\
3431                             [int(x, 16) for x in dbsf]:
3432                         raise exception.PciDeviceDetachFailed(reason=
3433                                                               "timeout",
3434                                                               dev=dev)
3435 
3436         except libvirt.libvirtError as ex:
3437             error_code = ex.get_error_code()
3438             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3439                 LOG.warning("Instance disappeared while detaching "
3440                             "a PCI device from it.")
3441             else:
3442                 raise
3443 
3444     def _attach_pci_devices(self, guest, pci_devs):
3445         try:
3446             for dev in pci_devs:
3447                 guest.attach_device(self._get_guest_pci_device(dev))
3448 
3449         except libvirt.libvirtError:
3450             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3451                       {'dev': pci_devs, 'dom': guest.id})
3452             raise
3453 
3454     @staticmethod
3455     def _has_direct_passthrough_port(network_info):
3456         for vif in network_info:
3457             if (vif['vnic_type'] in
3458                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3459                 return True
3460         return False
3461 
3462     def _attach_direct_passthrough_ports(
3463         self, context, instance, guest, network_info=None):
3464         if network_info is None:
3465             network_info = instance.info_cache.network_info
3466         if network_info is None:
3467             return
3468 
3469         if self._has_direct_passthrough_port(network_info):
3470             for vif in network_info:
3471                 if (vif['vnic_type'] in
3472                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3473                     cfg = self.vif_driver.get_config(instance,
3474                                                      vif,
3475                                                      instance.image_meta,
3476                                                      instance.flavor,
3477                                                      CONF.libvirt.virt_type,
3478                                                      self._host)
3479                     LOG.debug('Attaching direct passthrough port %(port)s '
3480                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3481                               instance=instance)
3482                     guest.attach_device(cfg)
3483 
3484     def _detach_direct_passthrough_ports(self, context, instance, guest):
3485         network_info = instance.info_cache.network_info
3486         if network_info is None:
3487             return
3488 
3489         if self._has_direct_passthrough_port(network_info):
3490             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3491             # pci request per direct passthrough port. Therefore we can trust
3492             # that pci_slot value in the vif is correct.
3493             direct_passthrough_pci_addresses = [
3494                 vif['profile']['pci_slot']
3495                 for vif in network_info
3496                 if (vif['vnic_type'] in
3497                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3498                     vif['profile'].get('pci_slot') is not None)
3499             ]
3500 
3501             # use detach_pci_devices to avoid failure in case of
3502             # multiple guest direct passthrough ports with the same MAC
3503             # (protection use-case, ports are on different physical
3504             # interfaces)
3505             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3506             direct_passthrough_pci_addresses = (
3507                 [pci_dev for pci_dev in pci_devs
3508                  if pci_dev.address in direct_passthrough_pci_addresses])
3509             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3510 
3511     def _set_host_enabled(self, enabled,
3512                           disable_reason=DISABLE_REASON_UNDEFINED):
3513         """Enables / Disables the compute service on this host.
3514 
3515            This doesn't override non-automatic disablement with an automatic
3516            setting; thereby permitting operators to keep otherwise
3517            healthy hosts out of rotation.
3518         """
3519 
3520         status_name = {True: 'disabled',
3521                        False: 'enabled'}
3522 
3523         disable_service = not enabled
3524 
3525         ctx = nova_context.get_admin_context()
3526         try:
3527             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3528 
3529             if service.disabled != disable_service:
3530                 # Note(jang): this is a quick fix to stop operator-
3531                 # disabled compute hosts from re-enabling themselves
3532                 # automatically. We prefix any automatic reason code
3533                 # with a fixed string. We only re-enable a host
3534                 # automatically if we find that string in place.
3535                 # This should probably be replaced with a separate flag.
3536                 if not service.disabled or (
3537                         service.disabled_reason and
3538                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3539                     service.disabled = disable_service
3540                     service.disabled_reason = (
3541                        DISABLE_PREFIX + disable_reason
3542                        if disable_service and disable_reason else
3543                            DISABLE_REASON_UNDEFINED)
3544                     service.save()
3545                     LOG.debug('Updating compute service status to %s',
3546                               status_name[disable_service])
3547                 else:
3548                     LOG.debug('Not overriding manual compute service '
3549                               'status with: %s',
3550                               status_name[disable_service])
3551         except exception.ComputeHostNotFound:
3552             LOG.warning('Cannot update service status on host "%s" '
3553                         'since it is not registered.', CONF.host)
3554         except Exception:
3555             LOG.warning('Cannot update service status on host "%s" '
3556                         'due to an unexpected exception.', CONF.host,
3557                         exc_info=True)
3558 
3559         if enabled:
3560             mount.get_manager().host_up(self._host)
3561         else:
3562             mount.get_manager().host_down()
3563 
3564     def _get_guest_cpu_model_config(self):
3565         mode = CONF.libvirt.cpu_mode
3566         model = CONF.libvirt.cpu_model
3567 
3568         if (CONF.libvirt.virt_type == "kvm" or
3569             CONF.libvirt.virt_type == "qemu"):
3570             if mode is None:
3571                 mode = "host-model"
3572             if mode == "none":
3573                 return vconfig.LibvirtConfigGuestCPU()
3574         else:
3575             if mode is None or mode == "none":
3576                 return None
3577 
3578         if ((CONF.libvirt.virt_type != "kvm" and
3579              CONF.libvirt.virt_type != "qemu")):
3580             msg = _("Config requested an explicit CPU model, but "
3581                     "the current libvirt hypervisor '%s' does not "
3582                     "support selecting CPU models") % CONF.libvirt.virt_type
3583             raise exception.Invalid(msg)
3584 
3585         if mode == "custom" and model is None:
3586             msg = _("Config requested a custom CPU model, but no "
3587                     "model name was provided")
3588             raise exception.Invalid(msg)
3589         elif mode != "custom" and model is not None:
3590             msg = _("A CPU model name should not be set when a "
3591                     "host CPU model is requested")
3592             raise exception.Invalid(msg)
3593 
3594         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3595                   {'mode': mode, 'model': (model or "")})
3596 
3597         cpu = vconfig.LibvirtConfigGuestCPU()
3598         cpu.mode = mode
3599         cpu.model = model
3600 
3601         return cpu
3602 
3603     def _get_guest_cpu_config(self, flavor, image_meta,
3604                               guest_cpu_numa_config, instance_numa_topology):
3605         cpu = self._get_guest_cpu_model_config()
3606 
3607         if cpu is None:
3608             return None
3609 
3610         topology = hardware.get_best_cpu_topology(
3611                 flavor, image_meta, numa_topology=instance_numa_topology)
3612 
3613         cpu.sockets = topology.sockets
3614         cpu.cores = topology.cores
3615         cpu.threads = topology.threads
3616         cpu.numa = guest_cpu_numa_config
3617 
3618         return cpu
3619 
3620     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3621                                image_type=None):
3622         disk_unit = None
3623         disk = self.image_backend.by_name(instance, name, image_type)
3624         if (name == 'disk.config' and image_type == 'rbd' and
3625                 not disk.exists()):
3626             # This is likely an older config drive that has not been migrated
3627             # to rbd yet. Try to fall back on 'flat' image type.
3628             # TODO(melwitt): Add online migration of some sort so we can
3629             # remove this fall back once we know all config drives are in rbd.
3630             # NOTE(vladikr): make sure that the flat image exist, otherwise
3631             # the image will be created after the domain definition.
3632             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3633             if flat_disk.exists():
3634                 disk = flat_disk
3635                 LOG.debug('Config drive not found in RBD, falling back to the '
3636                           'instance directory', instance=instance)
3637         disk_info = disk_mapping[name]
3638         if 'unit' in disk_mapping:
3639             disk_unit = disk_mapping['unit']
3640             disk_mapping['unit'] += 1  # Increments for the next disk added
3641         conf = disk.libvirt_info(disk_info['bus'],
3642                                  disk_info['dev'],
3643                                  disk_info['type'],
3644                                  self.disk_cachemode,
3645                                  inst_type['extra_specs'],
3646                                  self._host.get_version(),
3647                                  disk_unit=disk_unit)
3648         return conf
3649 
3650     def _get_guest_fs_config(self, instance, name, image_type=None):
3651         disk = self.image_backend.by_name(instance, name, image_type)
3652         return disk.libvirt_fs_info("/", "ploop")
3653 
3654     def _get_guest_storage_config(self, instance, image_meta,
3655                                   disk_info,
3656                                   rescue, block_device_info,
3657                                   inst_type, os_type):
3658         devices = []
3659         disk_mapping = disk_info['mapping']
3660 
3661         block_device_mapping = driver.block_device_info_get_mapping(
3662             block_device_info)
3663         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3664         scsi_controller = self._get_scsi_controller(image_meta)
3665 
3666         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3667             # The virtio-scsi can handle up to 256 devices but the
3668             # optional element "address" must be defined to describe
3669             # where the device is placed on the controller (see:
3670             # LibvirtConfigGuestDeviceAddressDrive).
3671             #
3672             # Note about why it's added in disk_mapping: It's not
3673             # possible to pass an 'int' by reference in Python, so we
3674             # use disk_mapping as container to keep reference of the
3675             # unit added and be able to increment it for each disk
3676             # added.
3677             disk_mapping['unit'] = 0
3678 
3679         def _get_ephemeral_devices():
3680             eph_devices = []
3681             for idx, eph in enumerate(
3682                 driver.block_device_info_get_ephemerals(
3683                     block_device_info)):
3684                 diskeph = self._get_guest_disk_config(
3685                     instance,
3686                     blockinfo.get_eph_disk(idx),
3687                     disk_mapping, inst_type)
3688                 eph_devices.append(diskeph)
3689             return eph_devices
3690 
3691         if mount_rootfs:
3692             fs = vconfig.LibvirtConfigGuestFilesys()
3693             fs.source_type = "mount"
3694             fs.source_dir = os.path.join(
3695                 libvirt_utils.get_instance_path(instance), 'rootfs')
3696             devices.append(fs)
3697         elif (os_type == fields.VMMode.EXE and
3698               CONF.libvirt.virt_type == "parallels"):
3699             if rescue:
3700                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3701                 devices.append(fsrescue)
3702 
3703                 fsos = self._get_guest_fs_config(instance, "disk")
3704                 fsos.target_dir = "/mnt/rescue"
3705                 devices.append(fsos)
3706             else:
3707                 if 'disk' in disk_mapping:
3708                     fs = self._get_guest_fs_config(instance, "disk")
3709                     devices.append(fs)
3710                 devices = devices + _get_ephemeral_devices()
3711         else:
3712 
3713             if rescue:
3714                 diskrescue = self._get_guest_disk_config(instance,
3715                                                          'disk.rescue',
3716                                                          disk_mapping,
3717                                                          inst_type)
3718                 devices.append(diskrescue)
3719 
3720                 diskos = self._get_guest_disk_config(instance,
3721                                                      'disk',
3722                                                      disk_mapping,
3723                                                      inst_type)
3724                 devices.append(diskos)
3725             else:
3726                 if 'disk' in disk_mapping:
3727                     diskos = self._get_guest_disk_config(instance,
3728                                                          'disk',
3729                                                          disk_mapping,
3730                                                          inst_type)
3731                     devices.append(diskos)
3732 
3733                 if 'disk.local' in disk_mapping:
3734                     disklocal = self._get_guest_disk_config(instance,
3735                                                             'disk.local',
3736                                                             disk_mapping,
3737                                                             inst_type)
3738                     devices.append(disklocal)
3739                     instance.default_ephemeral_device = (
3740                         block_device.prepend_dev(disklocal.target_dev))
3741 
3742                 devices = devices + _get_ephemeral_devices()
3743 
3744                 if 'disk.swap' in disk_mapping:
3745                     diskswap = self._get_guest_disk_config(instance,
3746                                                            'disk.swap',
3747                                                            disk_mapping,
3748                                                            inst_type)
3749                     devices.append(diskswap)
3750                     instance.default_swap_device = (
3751                         block_device.prepend_dev(diskswap.target_dev))
3752 
3753             config_name = 'disk.config.rescue' if rescue else 'disk.config'
3754             if config_name in disk_mapping:
3755                 diskconfig = self._get_guest_disk_config(
3756                     instance, config_name, disk_mapping, inst_type,
3757                     self._get_disk_config_image_type())
3758                 devices.append(diskconfig)
3759 
3760         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3761                                                    mount_rootfs):
3762             connection_info = vol['connection_info']
3763             vol_dev = block_device.prepend_dev(vol['mount_device'])
3764             info = disk_mapping[vol_dev]
3765             self._connect_volume(connection_info, info, instance)
3766             if scsi_controller and scsi_controller.model == 'virtio-scsi':
3767                 info['unit'] = disk_mapping['unit']
3768                 disk_mapping['unit'] += 1
3769             cfg = self._get_volume_config(connection_info, info)
3770             devices.append(cfg)
3771             vol['connection_info'] = connection_info
3772             vol.save()
3773 
3774         if scsi_controller:
3775             devices.append(scsi_controller)
3776 
3777         return devices
3778 
3779     @staticmethod
3780     def _get_scsi_controller(image_meta):
3781         """Return scsi controller or None based on image meta"""
3782         # TODO(sahid): should raise an exception for an invalid controller
3783         if image_meta.properties.get('hw_scsi_model'):
3784             hw_scsi_model = image_meta.properties.hw_scsi_model
3785             scsi_controller = vconfig.LibvirtConfigGuestController()
3786             scsi_controller.type = 'scsi'
3787             scsi_controller.model = hw_scsi_model
3788             scsi_controller.index = 0
3789             return scsi_controller
3790 
3791     def _get_host_sysinfo_serial_hardware(self):
3792         """Get a UUID from the host hardware
3793 
3794         Get a UUID for the host hardware reported by libvirt.
3795         This is typically from the SMBIOS data, unless it has
3796         been overridden in /etc/libvirt/libvirtd.conf
3797         """
3798         caps = self._host.get_capabilities()
3799         return caps.host.uuid
3800 
3801     def _get_host_sysinfo_serial_os(self):
3802         """Get a UUID from the host operating system
3803 
3804         Get a UUID for the host operating system. Modern Linux
3805         distros based on systemd provide a /etc/machine-id
3806         file containing a UUID. This is also provided inside
3807         systemd based containers and can be provided by other
3808         init systems too, since it is just a plain text file.
3809         """
3810         if not os.path.exists("/etc/machine-id"):
3811             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
3812             raise exception.InternalError(msg)
3813 
3814         with open("/etc/machine-id") as f:
3815             # We want to have '-' in the right place
3816             # so we parse & reformat the value
3817             lines = f.read().split()
3818             if not lines:
3819                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
3820                 raise exception.InternalError(msg)
3821 
3822             return str(uuid.UUID(lines[0]))
3823 
3824     def _get_host_sysinfo_serial_auto(self):
3825         if os.path.exists("/etc/machine-id"):
3826             return self._get_host_sysinfo_serial_os()
3827         else:
3828             return self._get_host_sysinfo_serial_hardware()
3829 
3830     def _get_guest_config_sysinfo(self, instance):
3831         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
3832 
3833         sysinfo.system_manufacturer = version.vendor_string()
3834         sysinfo.system_product = version.product_string()
3835         sysinfo.system_version = version.version_string_with_package()
3836 
3837         sysinfo.system_serial = self._sysinfo_serial_func()
3838         sysinfo.system_uuid = instance.uuid
3839 
3840         sysinfo.system_family = "Virtual Machine"
3841 
3842         return sysinfo
3843 
3844     def _get_guest_pci_device(self, pci_device):
3845 
3846         dbsf = pci_utils.parse_address(pci_device.address)
3847         dev = vconfig.LibvirtConfigGuestHostdevPCI()
3848         dev.domain, dev.bus, dev.slot, dev.function = dbsf
3849 
3850         # only kvm support managed mode
3851         if CONF.libvirt.virt_type in ('xen', 'parallels',):
3852             dev.managed = 'no'
3853         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3854             dev.managed = 'yes'
3855 
3856         return dev
3857 
3858     def _get_guest_config_meta(self, instance):
3859         """Get metadata config for guest."""
3860 
3861         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
3862         meta.package = version.version_string_with_package()
3863         meta.name = instance.display_name
3864         meta.creationTime = time.time()
3865 
3866         if instance.image_ref not in ("", None):
3867             meta.roottype = "image"
3868             meta.rootid = instance.image_ref
3869 
3870         system_meta = instance.system_metadata
3871         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
3872         ometa.userid = instance.user_id
3873         ometa.username = system_meta.get('owner_user_name', 'N/A')
3874         ometa.projectid = instance.project_id
3875         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
3876         meta.owner = ometa
3877 
3878         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
3879         flavor = instance.flavor
3880         fmeta.name = flavor.name
3881         fmeta.memory = flavor.memory_mb
3882         fmeta.vcpus = flavor.vcpus
3883         fmeta.ephemeral = flavor.ephemeral_gb
3884         fmeta.disk = flavor.root_gb
3885         fmeta.swap = flavor.swap
3886 
3887         meta.flavor = fmeta
3888 
3889         return meta
3890 
3891     def _machine_type_mappings(self):
3892         mappings = {}
3893         for mapping in CONF.libvirt.hw_machine_type:
3894             host_arch, _, machine_type = mapping.partition('=')
3895             mappings[host_arch] = machine_type
3896         return mappings
3897 
3898     def _get_machine_type(self, image_meta, caps):
3899         # The underlying machine type can be set as an image attribute,
3900         # or otherwise based on some architecture specific defaults
3901 
3902         mach_type = None
3903 
3904         if image_meta.properties.get('hw_machine_type') is not None:
3905             mach_type = image_meta.properties.hw_machine_type
3906         else:
3907             # For ARM systems we will default to vexpress-a15 for armv7
3908             # and virt for aarch64
3909             if caps.host.cpu.arch == fields.Architecture.ARMV7:
3910                 mach_type = "vexpress-a15"
3911 
3912             if caps.host.cpu.arch == fields.Architecture.AARCH64:
3913                 mach_type = "virt"
3914 
3915             if caps.host.cpu.arch in (fields.Architecture.S390,
3916                                       fields.Architecture.S390X):
3917                 mach_type = 's390-ccw-virtio'
3918 
3919             # If set in the config, use that as the default.
3920             if CONF.libvirt.hw_machine_type:
3921                 mappings = self._machine_type_mappings()
3922                 mach_type = mappings.get(caps.host.cpu.arch)
3923 
3924         return mach_type
3925 
3926     @staticmethod
3927     def _create_idmaps(klass, map_strings):
3928         idmaps = []
3929         if len(map_strings) > 5:
3930             map_strings = map_strings[0:5]
3931             LOG.warning("Too many id maps, only included first five.")
3932         for map_string in map_strings:
3933             try:
3934                 idmap = klass()
3935                 values = [int(i) for i in map_string.split(":")]
3936                 idmap.start = values[0]
3937                 idmap.target = values[1]
3938                 idmap.count = values[2]
3939                 idmaps.append(idmap)
3940             except (ValueError, IndexError):
3941                 LOG.warning("Invalid value for id mapping %s", map_string)
3942         return idmaps
3943 
3944     def _get_guest_idmaps(self):
3945         id_maps = []
3946         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
3947             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
3948                                            CONF.libvirt.uid_maps)
3949             id_maps.extend(uid_maps)
3950         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
3951             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
3952                                            CONF.libvirt.gid_maps)
3953             id_maps.extend(gid_maps)
3954         return id_maps
3955 
3956     def _update_guest_cputune(self, guest, flavor, virt_type):
3957         is_able = self._host.is_cpu_control_policy_capable()
3958 
3959         cputuning = ['shares', 'period', 'quota']
3960         wants_cputune = any([k for k in cputuning
3961             if "quota:cpu_" + k in flavor.extra_specs.keys()])
3962 
3963         if wants_cputune and not is_able:
3964             raise exception.UnsupportedHostCPUControlPolicy()
3965 
3966         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
3967             return
3968 
3969         if guest.cputune is None:
3970             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
3971             # Setting the default cpu.shares value to be a value
3972             # dependent on the number of vcpus
3973         guest.cputune.shares = 1024 * guest.vcpus
3974 
3975         for name in cputuning:
3976             key = "quota:cpu_" + name
3977             if key in flavor.extra_specs:
3978                 setattr(guest.cputune, name,
3979                         int(flavor.extra_specs[key]))
3980 
3981     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
3982                                            wants_hugepages):
3983         if instance_numa_topology:
3984             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
3985             for instance_cell in instance_numa_topology.cells:
3986                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
3987                 guest_cell.id = instance_cell.id
3988                 guest_cell.cpus = instance_cell.cpuset
3989                 guest_cell.memory = instance_cell.memory * units.Ki
3990 
3991                 # The vhost-user network backend requires file backed
3992                 # guest memory (ie huge pages) to be marked as shared
3993                 # access, not private, so an external process can read
3994                 # and write the pages.
3995                 #
3996                 # You can't change the shared vs private flag for an
3997                 # already running guest, and since we can't predict what
3998                 # types of NIC may be hotplugged, we have no choice but
3999                 # to unconditionally turn on the shared flag. This has
4000                 # no real negative functional effect on the guest, so
4001                 # is a reasonable approach to take
4002                 if wants_hugepages:
4003                     guest_cell.memAccess = "shared"
4004                 guest_cpu_numa.cells.append(guest_cell)
4005             return guest_cpu_numa
4006 
4007     def _has_cpu_policy_support(self):
4008         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
4009             if self._host.has_version(ver):
4010                 ver_ = self._version_to_string(ver)
4011                 raise exception.CPUPinningNotSupported(reason=_(
4012                     'Invalid libvirt version %(version)s') % {'version': ver_})
4013         return True
4014 
4015     def _wants_hugepages(self, host_topology, instance_topology):
4016         """Determine if the guest / host topology implies the
4017            use of huge pages for guest RAM backing
4018         """
4019 
4020         if host_topology is None or instance_topology is None:
4021             return False
4022 
4023         avail_pagesize = [page.size_kb
4024                           for page in host_topology.cells[0].mempages]
4025         avail_pagesize.sort()
4026         # Remove smallest page size as that's not classed as a largepage
4027         avail_pagesize = avail_pagesize[1:]
4028 
4029         # See if we have page size set
4030         for cell in instance_topology.cells:
4031             if (cell.pagesize is not None and
4032                 cell.pagesize in avail_pagesize):
4033                 return True
4034 
4035         return False
4036 
4037     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4038                                allowed_cpus=None, image_meta=None):
4039         """Returns the config objects for the guest NUMA specs.
4040 
4041         Determines the CPUs that the guest can be pinned to if the guest
4042         specifies a cell topology and the host supports it. Constructs the
4043         libvirt XML config object representing the NUMA topology selected
4044         for the guest. Returns a tuple of:
4045 
4046             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4047 
4048         With the following caveats:
4049 
4050             a) If there is no specified guest NUMA topology, then
4051                all tuple elements except cpu_set shall be None. cpu_set
4052                will be populated with the chosen CPUs that the guest
4053                allowed CPUs fit within, which could be the supplied
4054                allowed_cpus value if the host doesn't support NUMA
4055                topologies.
4056 
4057             b) If there is a specified guest NUMA topology, then
4058                cpu_set will be None and guest_cpu_numa will be the
4059                LibvirtConfigGuestCPUNUMA object representing the guest's
4060                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4061                will contain a LibvirtConfigGuestCPUTune object representing
4062                the optimized chosen cells that match the host capabilities
4063                with the instance's requested topology. If the host does
4064                not support NUMA, then guest_cpu_tune and guest_numa_tune
4065                will be None.
4066         """
4067 
4068         if (not self._has_numa_support() and
4069                 instance_numa_topology is not None):
4070             # We should not get here, since we should have avoided
4071             # reporting NUMA topology from _get_host_numa_topology
4072             # in the first place. Just in case of a scheduler
4073             # mess up though, raise an exception
4074             raise exception.NUMATopologyUnsupported()
4075 
4076         topology = self._get_host_numa_topology()
4077 
4078         # We have instance NUMA so translate it to the config class
4079         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4080                 instance_numa_topology,
4081                 self._wants_hugepages(topology, instance_numa_topology))
4082 
4083         if not guest_cpu_numa_config:
4084             # No NUMA topology defined for instance - let the host kernel deal
4085             # with the NUMA effects.
4086             # TODO(ndipanov): Attempt to spread the instance
4087             # across NUMA nodes and expose the topology to the
4088             # instance as an optimisation
4089             return GuestNumaConfig(allowed_cpus, None, None, None)
4090         else:
4091             if topology:
4092                 # Now get the CpuTune configuration from the numa_topology
4093                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4094                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4095                 emupcpus = []
4096 
4097                 numa_mem = vconfig.LibvirtConfigGuestNUMATuneMemory()
4098                 numa_memnodes = [vconfig.LibvirtConfigGuestNUMATuneMemNode()
4099                                  for _ in guest_cpu_numa_config.cells]
4100 
4101                 emulator_threads_isolated = (
4102                     instance_numa_topology.emulator_threads_isolated)
4103 
4104                 vcpus_rt = set([])
4105                 wants_realtime = hardware.is_realtime_enabled(flavor)
4106                 if wants_realtime:
4107                     if not self._host.has_min_version(
4108                             MIN_LIBVIRT_REALTIME_VERSION):
4109                         raise exception.RealtimePolicyNotSupported()
4110                     # Prepare realtime config for libvirt
4111                     vcpus_rt = hardware.vcpus_realtime_topology(
4112                         flavor, image_meta)
4113                     vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4114                     vcpusched.vcpus = vcpus_rt
4115                     vcpusched.scheduler = "fifo"
4116                     vcpusched.priority = (
4117                         CONF.libvirt.realtime_scheduler_priority)
4118                     guest_cpu_tune.vcpusched.append(vcpusched)
4119 
4120                 # TODO(sahid): Defining domain topology should be
4121                 # refactored.
4122                 for host_cell in topology.cells:
4123                     for guest_node_id, guest_config_cell in enumerate(
4124                             guest_cpu_numa_config.cells):
4125                         if guest_config_cell.id == host_cell.id:
4126                             node = numa_memnodes[guest_node_id]
4127                             node.cellid = guest_node_id
4128                             node.nodeset = [host_cell.id]
4129                             node.mode = "strict"
4130 
4131                             numa_mem.nodeset.append(host_cell.id)
4132 
4133                             object_numa_cell = (
4134                                     instance_numa_topology.cells[guest_node_id]
4135                                 )
4136                             for cpu in guest_config_cell.cpus:
4137                                 pin_cpuset = (
4138                                     vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
4139                                 pin_cpuset.id = cpu
4140                                 # If there is pinning information in the cell
4141                                 # we pin to individual CPUs, otherwise we float
4142                                 # over the whole host NUMA node
4143 
4144                                 if (object_numa_cell.cpu_pinning and
4145                                         self._has_cpu_policy_support()):
4146                                     pcpu = object_numa_cell.cpu_pinning[cpu]
4147                                     pin_cpuset.cpuset = set([pcpu])
4148                                 else:
4149                                     pin_cpuset.cpuset = host_cell.cpuset
4150                                 if emulator_threads_isolated:
4151                                     emupcpus.extend(
4152                                         object_numa_cell.cpuset_reserved)
4153                                 elif not wants_realtime or cpu not in vcpus_rt:
4154                                     # - If realtime IS NOT enabled, the
4155                                     #   emulator threads are allowed to float
4156                                     #   across all the pCPUs associated with
4157                                     #   the guest vCPUs ("not wants_realtime"
4158                                     #   is true, so we add all pcpus)
4159                                     # - If realtime IS enabled, then at least
4160                                     #   1 vCPU is required to be set aside for
4161                                     #   non-realtime usage. The emulator
4162                                     #   threads are allowed to float acros the
4163                                     #   pCPUs that are associated with the
4164                                     #   non-realtime VCPUs (the "cpu not in
4165                                     #   vcpu_rt" check deals with this
4166                                     #   filtering)
4167                                     emupcpus.extend(pin_cpuset.cpuset)
4168                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4169 
4170                 # TODO(berrange) When the guest has >1 NUMA node, it will
4171                 # span multiple host NUMA nodes. By pinning emulator threads
4172                 # to the union of all nodes, we guarantee there will be
4173                 # cross-node memory access by the emulator threads when
4174                 # responding to guest I/O operations. The only way to avoid
4175                 # this would be to pin emulator threads to a single node and
4176                 # tell the guest OS to only do I/O from one of its virtual
4177                 # NUMA nodes. This is not even remotely practical.
4178                 #
4179                 # The long term solution is to make use of a new QEMU feature
4180                 # called "I/O Threads" which will let us configure an explicit
4181                 # I/O thread for each guest vCPU or guest NUMA node. It is
4182                 # still TBD how to make use of this feature though, especially
4183                 # how to associate IO threads with guest devices to eliminate
4184                 # cross NUMA node traffic. This is an area of investigation
4185                 # for QEMU community devs.
4186                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
4187                 emulatorpin.cpuset = set(emupcpus)
4188                 guest_cpu_tune.emulatorpin = emulatorpin
4189                 # Sort the vcpupin list per vCPU id for human-friendlier XML
4190                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4191 
4192                 guest_numa_tune.memory = numa_mem
4193                 guest_numa_tune.memnodes = numa_memnodes
4194 
4195                 # normalize cell.id
4196                 for i, (cell, memnode) in enumerate(
4197                                             zip(guest_cpu_numa_config.cells,
4198                                                 guest_numa_tune.memnodes)):
4199                     cell.id = i
4200                     memnode.cellid = i
4201 
4202                 return GuestNumaConfig(None, guest_cpu_tune,
4203                                        guest_cpu_numa_config,
4204                                        guest_numa_tune)
4205             else:
4206                 return GuestNumaConfig(allowed_cpus, None,
4207                                        guest_cpu_numa_config, None)
4208 
4209     def _get_guest_os_type(self, virt_type):
4210         """Returns the guest OS type based on virt type."""
4211         if virt_type == "lxc":
4212             ret = fields.VMMode.EXE
4213         elif virt_type == "uml":
4214             ret = fields.VMMode.UML
4215         elif virt_type == "xen":
4216             ret = fields.VMMode.XEN
4217         else:
4218             ret = fields.VMMode.HVM
4219         return ret
4220 
4221     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4222                               root_device_name):
4223         if rescue.get('kernel_id'):
4224             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4225             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4226             if virt_type == "qemu":
4227                 guest.os_cmdline += " no_timer_check"
4228         if rescue.get('ramdisk_id'):
4229             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4230 
4231     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4232                                 root_device_name, image_meta):
4233         guest.os_kernel = os.path.join(inst_path, "kernel")
4234         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4235         if virt_type == "qemu":
4236             guest.os_cmdline += " no_timer_check"
4237         if instance.ramdisk_id:
4238             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4239         # we only support os_command_line with images with an explicit
4240         # kernel set and don't want to break nova if there's an
4241         # os_command_line property without a specified kernel_id param
4242         if image_meta.properties.get("os_command_line"):
4243             guest.os_cmdline = image_meta.properties.os_command_line
4244 
4245     def _set_clock(self, guest, os_type, image_meta, virt_type):
4246         # NOTE(mikal): Microsoft Windows expects the clock to be in
4247         # "localtime". If the clock is set to UTC, then you can use a
4248         # registry key to let windows know, but Microsoft says this is
4249         # buggy in http://support.microsoft.com/kb/2687252
4250         clk = vconfig.LibvirtConfigGuestClock()
4251         if os_type == 'windows':
4252             LOG.info('Configuring timezone for windows instance to localtime')
4253             clk.offset = 'localtime'
4254         else:
4255             clk.offset = 'utc'
4256         guest.set_clock(clk)
4257 
4258         if virt_type == "kvm":
4259             self._set_kvm_timers(clk, os_type, image_meta)
4260 
4261     def _set_kvm_timers(self, clk, os_type, image_meta):
4262         # TODO(berrange) One day this should be per-guest
4263         # OS type configurable
4264         tmpit = vconfig.LibvirtConfigGuestTimer()
4265         tmpit.name = "pit"
4266         tmpit.tickpolicy = "delay"
4267 
4268         tmrtc = vconfig.LibvirtConfigGuestTimer()
4269         tmrtc.name = "rtc"
4270         tmrtc.tickpolicy = "catchup"
4271 
4272         clk.add_timer(tmpit)
4273         clk.add_timer(tmrtc)
4274 
4275         guestarch = libvirt_utils.get_arch(image_meta)
4276         if guestarch in (fields.Architecture.I686,
4277                          fields.Architecture.X86_64):
4278             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4279             # qemu -no-hpet is not supported on non-x86 targets.
4280             tmhpet = vconfig.LibvirtConfigGuestTimer()
4281             tmhpet.name = "hpet"
4282             tmhpet.present = False
4283             clk.add_timer(tmhpet)
4284 
4285         # Provide Windows guests with the paravirtualized hyperv timer source.
4286         # This is the windows equiv of kvm-clock, allowing Windows
4287         # guests to accurately keep time.
4288         if os_type == 'windows':
4289             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4290             tmhyperv.name = "hypervclock"
4291             tmhyperv.present = True
4292             clk.add_timer(tmhyperv)
4293 
4294     def _set_features(self, guest, os_type, caps, virt_type, image_meta):
4295         if virt_type == "xen":
4296             # PAE only makes sense in X86
4297             if caps.host.cpu.arch in (fields.Architecture.I686,
4298                                       fields.Architecture.X86_64):
4299                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4300 
4301         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4302                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4303             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4304             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4305 
4306         if (virt_type in ("qemu", "kvm") and
4307                 os_type == 'windows'):
4308             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4309             hv.relaxed = True
4310 
4311             hv.spinlocks = True
4312             # Increase spinlock retries - value recommended by
4313             # KVM maintainers who certify Windows guests
4314             # with Microsoft
4315             hv.spinlock_retries = 8191
4316             hv.vapic = True
4317             guest.features.append(hv)
4318 
4319         if (virt_type in ("qemu", "kvm") and
4320                 image_meta.properties.get('img_hide_hypervisor_id')):
4321             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4322 
4323     def _check_number_of_serial_console(self, num_ports):
4324         virt_type = CONF.libvirt.virt_type
4325         if (virt_type in ("kvm", "qemu") and
4326             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4327             raise exception.SerialPortNumberLimitExceeded(
4328                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4329 
4330     def _add_video_driver(self, guest, image_meta, flavor):
4331         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4332                                "xen", "qxl", "virtio")
4333         video = vconfig.LibvirtConfigGuestVideo()
4334         # NOTE(ldbragst): The following logic sets the video.type
4335         # depending on supported defaults given the architecture,
4336         # virtualization type, and features. The video.type attribute can
4337         # be overridden by the user with image_meta.properties, which
4338         # is carried out in the next if statement below this one.
4339         guestarch = libvirt_utils.get_arch(image_meta)
4340         if guest.os_type == fields.VMMode.XEN:
4341             video.type = 'xen'
4342         elif CONF.libvirt.virt_type == 'parallels':
4343             video.type = 'vga'
4344         elif guestarch in (fields.Architecture.PPC,
4345                            fields.Architecture.PPC64,
4346                            fields.Architecture.PPC64LE):
4347             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4348             # so use 'vga' instead when running on Power hardware.
4349             video.type = 'vga'
4350         elif guestarch in (fields.Architecture.AARCH64):
4351             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4352             # so use 'virtio' instead when running on AArch64 hardware.
4353             video.type = 'virtio'
4354         elif CONF.spice.enabled:
4355             video.type = 'qxl'
4356         if image_meta.properties.get('hw_video_model'):
4357             video.type = image_meta.properties.hw_video_model
4358             if (video.type not in VALID_VIDEO_DEVICES):
4359                 raise exception.InvalidVideoMode(model=video.type)
4360 
4361         # Set video memory, only if the flavor's limit is set
4362         video_ram = image_meta.properties.get('hw_video_ram', 0)
4363         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4364         if video_ram > max_vram:
4365             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4366                                                  max_vram=max_vram)
4367         if max_vram and video_ram:
4368             video.vram = video_ram * units.Mi / units.Ki
4369         guest.add_device(video)
4370 
4371     def _add_qga_device(self, guest, instance):
4372         qga = vconfig.LibvirtConfigGuestChannel()
4373         qga.type = "unix"
4374         qga.target_name = "org.qemu.guest_agent.0"
4375         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4376                           ("org.qemu.guest_agent.0", instance.name))
4377         guest.add_device(qga)
4378 
4379     def _add_rng_device(self, guest, flavor):
4380         rng_device = vconfig.LibvirtConfigGuestRng()
4381         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4382         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4383         if rate_bytes:
4384             rng_device.rate_bytes = int(rate_bytes)
4385             rng_device.rate_period = int(period)
4386         rng_path = CONF.libvirt.rng_dev_path
4387         if (rng_path and not os.path.exists(rng_path)):
4388             raise exception.RngDeviceNotExist(path=rng_path)
4389         rng_device.backend = rng_path
4390         guest.add_device(rng_device)
4391 
4392     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4393         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4394         if image_meta.properties.get('hw_qemu_guest_agent', False):
4395             LOG.debug("Qemu guest agent is enabled through image "
4396                       "metadata", instance=instance)
4397             self._add_qga_device(guest, instance)
4398         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4399         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4400         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4401         if rng_is_virtio and rng_allowed:
4402             self._add_rng_device(guest, flavor)
4403 
4404     def _get_guest_memory_backing_config(
4405             self, inst_topology, numatune, flavor):
4406         wantsmempages = False
4407         if inst_topology:
4408             for cell in inst_topology.cells:
4409                 if cell.pagesize:
4410                     wantsmempages = True
4411                     break
4412 
4413         wantsrealtime = hardware.is_realtime_enabled(flavor)
4414 
4415         membacking = None
4416         if wantsmempages:
4417             pages = self._get_memory_backing_hugepages_support(
4418                 inst_topology, numatune)
4419             if pages:
4420                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4421                 membacking.hugepages = pages
4422         if wantsrealtime:
4423             if not membacking:
4424                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4425             membacking.locked = True
4426             membacking.sharedpages = False
4427 
4428         return membacking
4429 
4430     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4431         if not self._has_numa_support():
4432             # We should not get here, since we should have avoided
4433             # reporting NUMA topology from _get_host_numa_topology
4434             # in the first place. Just in case of a scheduler
4435             # mess up though, raise an exception
4436             raise exception.MemoryPagesUnsupported()
4437 
4438         host_topology = self._get_host_numa_topology()
4439 
4440         if host_topology is None:
4441             # As above, we should not get here but just in case...
4442             raise exception.MemoryPagesUnsupported()
4443 
4444         # Currently libvirt does not support the smallest
4445         # pagesize set as a backend memory.
4446         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4447         avail_pagesize = [page.size_kb
4448                           for page in host_topology.cells[0].mempages]
4449         avail_pagesize.sort()
4450         smallest = avail_pagesize[0]
4451 
4452         pages = []
4453         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4454             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4455                 for memnode in numatune.memnodes:
4456                     if guest_cellid == memnode.cellid:
4457                         page = (
4458                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4459                         page.nodeset = [guest_cellid]
4460                         page.size_kb = inst_cell.pagesize
4461                         pages.append(page)
4462                         break  # Quit early...
4463         return pages
4464 
4465     def _get_flavor(self, ctxt, instance, flavor):
4466         if flavor is not None:
4467             return flavor
4468         return instance.flavor
4469 
4470     def _has_uefi_support(self):
4471         # This means that the host can support uefi booting for guests
4472         supported_archs = [fields.Architecture.X86_64,
4473                            fields.Architecture.AARCH64]
4474         caps = self._host.get_capabilities()
4475         return ((caps.host.cpu.arch in supported_archs) and
4476                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4477 
4478     def _get_supported_perf_events(self):
4479 
4480         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4481              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4482             return []
4483 
4484         supported_events = []
4485         host_cpu_info = self._get_cpu_info()
4486         for event in CONF.libvirt.enabled_perf_events:
4487             if self._supported_perf_event(event, host_cpu_info['features']):
4488                 supported_events.append(event)
4489         return supported_events
4490 
4491     def _supported_perf_event(self, event, cpu_features):
4492 
4493         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4494 
4495         if not hasattr(libvirt, libvirt_perf_event_name):
4496             LOG.warning("Libvirt doesn't support event type %s.", event)
4497             return False
4498 
4499         if (event in PERF_EVENTS_CPU_FLAG_MAPPING
4500             and PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features):
4501             LOG.warning("Host does not support event type %s.", event)
4502             return False
4503 
4504         return True
4505 
4506     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4507                                       image_meta, flavor, root_device_name):
4508         if virt_type == "xen":
4509             if guest.os_type == fields.VMMode.HVM:
4510                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4511             else:
4512                 guest.os_cmdline = CONSOLE
4513         elif virt_type in ("kvm", "qemu"):
4514             if caps.host.cpu.arch in (fields.Architecture.I686,
4515                                       fields.Architecture.X86_64):
4516                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4517                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4518             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4519             if hw_firmware_type == fields.FirmwareType.UEFI:
4520                 if self._has_uefi_support():
4521                     global uefi_logged
4522                     if not uefi_logged:
4523                         LOG.warning("uefi support is without some kind of "
4524                                     "functional testing and therefore "
4525                                     "considered experimental.")
4526                         uefi_logged = True
4527                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4528                         caps.host.cpu.arch]
4529                     guest.os_loader_type = "pflash"
4530                 else:
4531                     raise exception.UEFINotSupported()
4532             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4533             if image_meta.properties.get('hw_boot_menu') is None:
4534                 guest.os_bootmenu = strutils.bool_from_string(
4535                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4536             else:
4537                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4538 
4539         elif virt_type == "lxc":
4540             guest.os_init_path = "/sbin/init"
4541             guest.os_cmdline = CONSOLE
4542         elif virt_type == "uml":
4543             guest.os_kernel = "/usr/bin/linux"
4544             guest.os_root = root_device_name
4545         elif virt_type == "parallels":
4546             if guest.os_type == fields.VMMode.EXE:
4547                 guest.os_init_path = "/sbin/init"
4548 
4549     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4550                     instance, inst_path, image_meta, disk_info):
4551         if rescue:
4552             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4553                                        root_device_name)
4554         elif instance.kernel_id:
4555             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4556                                             virt_type, root_device_name,
4557                                             image_meta)
4558         else:
4559             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4560 
4561     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4562                          image_meta):
4563         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4564         # easy to lose track. Use this chart to figure out your case:
4565         #
4566         # case | is serial | has       | is qemu | resulting
4567         #      | enabled?  | virtlogd? | or kvm? | devices
4568         # --------------------------------------------------
4569         #    1 |        no |        no |     no  | pty*
4570         #    2 |        no |        no |     yes | file + pty
4571         #    3 |        no |       yes |      no | see case 1
4572         #    4 |        no |       yes |     yes | pty with logd
4573         #    5 |       yes |        no |      no | see case 1
4574         #    6 |       yes |        no |     yes | tcp + pty
4575         #    7 |       yes |       yes |      no | see case 1
4576         #    8 |       yes |       yes |     yes | tcp with logd
4577         #    * exception: virt_type "parallels" doesn't create a device
4578         if virt_type == 'parallels':
4579             pass
4580         elif virt_type not in ("qemu", "kvm"):
4581             log_path = self._get_console_log_path(instance)
4582             self._create_pty_device(guest_cfg,
4583                                     vconfig.LibvirtConfigGuestConsole,
4584                                     log_path=log_path)
4585         elif (virt_type in ("qemu", "kvm") and
4586                   self._is_s390x_guest(image_meta)):
4587             self._create_consoles_s390x(guest_cfg, instance,
4588                                         flavor, image_meta)
4589         elif virt_type in ("qemu", "kvm"):
4590             self._create_consoles_qemu_kvm(guest_cfg, instance,
4591                                         flavor, image_meta)
4592 
4593     def _is_s390x_guest(self, image_meta):
4594         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4595         return libvirt_utils.get_arch(image_meta) in s390x_archs
4596 
4597     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4598                                   image_meta):
4599         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4600         log_path = self._get_console_log_path(instance)
4601         if CONF.serial_console.enabled:
4602             if not self._serial_ports_already_defined(instance):
4603                 num_ports = hardware.get_number_of_serial_ports(flavor,
4604                                                                 image_meta)
4605                 self._check_number_of_serial_console(num_ports)
4606                 self._create_serial_consoles(guest_cfg, num_ports,
4607                                              char_dev_cls, log_path)
4608         else:
4609             self._create_file_device(guest_cfg, instance, char_dev_cls)
4610         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4611 
4612     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4613         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4614         log_path = self._get_console_log_path(instance)
4615         if CONF.serial_console.enabled:
4616             if not self._serial_ports_already_defined(instance):
4617                 num_ports = hardware.get_number_of_serial_ports(flavor,
4618                                                                 image_meta)
4619                 self._create_serial_consoles(guest_cfg, num_ports,
4620                                              char_dev_cls, log_path)
4621         else:
4622             self._create_file_device(guest_cfg, instance, char_dev_cls,
4623                                      "sclplm")
4624         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4625 
4626     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4627                            log_path=None):
4628         def _create_base_dev():
4629             consolepty = char_dev_cls()
4630             consolepty.target_type = target_type
4631             consolepty.type = "pty"
4632             return consolepty
4633 
4634         def _create_logd_dev():
4635             consolepty = _create_base_dev()
4636             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4637             log.file = log_path
4638             consolepty.log = log
4639             return consolepty
4640 
4641         if CONF.serial_console.enabled:
4642             if self._is_virtlogd_available():
4643                 return
4644             else:
4645                 # NOTE(markus_z): You may wonder why this is necessary and
4646                 # so do I. I'm certain that this is *not* needed in any
4647                 # real use case. It is, however, useful if you want to
4648                 # pypass the Nova API and use "virsh console <guest>" on
4649                 # an hypervisor, as this CLI command doesn't work with TCP
4650                 # devices (like the serial console is).
4651                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4652                 # Pypassing the Nova API however is a thing we don't want.
4653                 # Future changes should remove this and fix the unit tests
4654                 # which ask for the existence.
4655                 guest_cfg.add_device(_create_base_dev())
4656         else:
4657             if self._is_virtlogd_available():
4658                 guest_cfg.add_device(_create_logd_dev())
4659             else:
4660                 guest_cfg.add_device(_create_base_dev())
4661 
4662     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4663                             target_type=None):
4664         if self._is_virtlogd_available():
4665             return
4666 
4667         consolelog = char_dev_cls()
4668         consolelog.target_type = target_type
4669         consolelog.type = "file"
4670         consolelog.source_path = self._get_console_log_path(instance)
4671         guest_cfg.add_device(consolelog)
4672 
4673     def _serial_ports_already_defined(self, instance):
4674         try:
4675             guest = self._host.get_guest(instance)
4676             if list(self._get_serial_ports_from_guest(guest)):
4677                 # Serial port are already configured for instance that
4678                 # means we are in a context of migration.
4679                 return True
4680         except exception.InstanceNotFound:
4681             LOG.debug(
4682                 "Instance does not exist yet on libvirt, we can "
4683                 "safely pass on looking for already defined serial "
4684                 "ports in its domain XML", instance=instance)
4685         return False
4686 
4687     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4688                                 log_path):
4689         for port in six.moves.range(num_ports):
4690             console = char_dev_cls()
4691             console.port = port
4692             console.type = "tcp"
4693             console.listen_host = CONF.serial_console.proxyclient_address
4694             listen_port = serial_console.acquire_port(console.listen_host)
4695             console.listen_port = listen_port
4696             # NOTE: only the first serial console gets the boot messages,
4697             # that's why we attach the logd subdevice only to that.
4698             if port == 0 and self._is_virtlogd_available():
4699                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
4700                 log.file = log_path
4701                 console.log = log
4702             guest_cfg.add_device(console)
4703 
4704     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
4705         """Update VirtCPUModel object according to libvirt CPU config.
4706 
4707         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
4708                            instance's virtual cpu configuration.
4709         :param:vcpu_model: VirtCPUModel object. A new object will be created
4710                            if None.
4711 
4712         :return: Updated VirtCPUModel object, or None if cpu_config is None
4713 
4714         """
4715 
4716         if not cpu_config:
4717             return
4718         if not vcpu_model:
4719             vcpu_model = objects.VirtCPUModel()
4720 
4721         vcpu_model.arch = cpu_config.arch
4722         vcpu_model.vendor = cpu_config.vendor
4723         vcpu_model.model = cpu_config.model
4724         vcpu_model.mode = cpu_config.mode
4725         vcpu_model.match = cpu_config.match
4726 
4727         if cpu_config.sockets:
4728             vcpu_model.topology = objects.VirtCPUTopology(
4729                 sockets=cpu_config.sockets,
4730                 cores=cpu_config.cores,
4731                 threads=cpu_config.threads)
4732         else:
4733             vcpu_model.topology = None
4734 
4735         features = [objects.VirtCPUFeature(
4736             name=f.name,
4737             policy=f.policy) for f in cpu_config.features]
4738         vcpu_model.features = features
4739 
4740         return vcpu_model
4741 
4742     def _vcpu_model_to_cpu_config(self, vcpu_model):
4743         """Create libvirt CPU config according to VirtCPUModel object.
4744 
4745         :param:vcpu_model: VirtCPUModel object.
4746 
4747         :return: vconfig.LibvirtConfigGuestCPU.
4748 
4749         """
4750 
4751         cpu_config = vconfig.LibvirtConfigGuestCPU()
4752         cpu_config.arch = vcpu_model.arch
4753         cpu_config.model = vcpu_model.model
4754         cpu_config.mode = vcpu_model.mode
4755         cpu_config.match = vcpu_model.match
4756         cpu_config.vendor = vcpu_model.vendor
4757         if vcpu_model.topology:
4758             cpu_config.sockets = vcpu_model.topology.sockets
4759             cpu_config.cores = vcpu_model.topology.cores
4760             cpu_config.threads = vcpu_model.topology.threads
4761         if vcpu_model.features:
4762             for f in vcpu_model.features:
4763                 xf = vconfig.LibvirtConfigGuestCPUFeature()
4764                 xf.name = f.name
4765                 xf.policy = f.policy
4766                 cpu_config.features.add(xf)
4767         return cpu_config
4768 
4769     def _get_guest_config(self, instance, network_info, image_meta,
4770                           disk_info, rescue=None, block_device_info=None,
4771                           context=None):
4772         """Get config data for parameters.
4773 
4774         :param rescue: optional dictionary that should contain the key
4775             'ramdisk_id' if a ramdisk is needed for the rescue image and
4776             'kernel_id' if a kernel is needed for the rescue image.
4777         """
4778         flavor = instance.flavor
4779         inst_path = libvirt_utils.get_instance_path(instance)
4780         disk_mapping = disk_info['mapping']
4781 
4782         virt_type = CONF.libvirt.virt_type
4783         guest = vconfig.LibvirtConfigGuest()
4784         guest.virt_type = virt_type
4785         guest.name = instance.name
4786         guest.uuid = instance.uuid
4787         # We are using default unit for memory: KiB
4788         guest.memory = flavor.memory_mb * units.Ki
4789         guest.vcpus = flavor.vcpus
4790         allowed_cpus = hardware.get_vcpu_pin_set()
4791 
4792         guest_numa_config = self._get_guest_numa_config(
4793             instance.numa_topology, flavor, allowed_cpus, image_meta)
4794 
4795         guest.cpuset = guest_numa_config.cpuset
4796         guest.cputune = guest_numa_config.cputune
4797         guest.numatune = guest_numa_config.numatune
4798 
4799         guest.membacking = self._get_guest_memory_backing_config(
4800             instance.numa_topology,
4801             guest_numa_config.numatune,
4802             flavor)
4803 
4804         guest.metadata.append(self._get_guest_config_meta(instance))
4805         guest.idmaps = self._get_guest_idmaps()
4806 
4807         for event in self._supported_perf_events:
4808             guest.add_perf_event(event)
4809 
4810         self._update_guest_cputune(guest, flavor, virt_type)
4811 
4812         guest.cpu = self._get_guest_cpu_config(
4813             flavor, image_meta, guest_numa_config.numaconfig,
4814             instance.numa_topology)
4815 
4816         # Notes(yjiang5): we always sync the instance's vcpu model with
4817         # the corresponding config file.
4818         instance.vcpu_model = self._cpu_config_to_vcpu_model(
4819             guest.cpu, instance.vcpu_model)
4820 
4821         if 'root' in disk_mapping:
4822             root_device_name = block_device.prepend_dev(
4823                 disk_mapping['root']['dev'])
4824         else:
4825             root_device_name = None
4826 
4827         if root_device_name:
4828             # NOTE(yamahata):
4829             # for nova.api.ec2.cloud.CloudController.get_metadata()
4830             instance.root_device_name = root_device_name
4831 
4832         guest.os_type = (fields.VMMode.get_from_instance(instance) or
4833                 self._get_guest_os_type(virt_type))
4834         caps = self._host.get_capabilities()
4835 
4836         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
4837                                            image_meta, flavor,
4838                                            root_device_name)
4839         if virt_type not in ('lxc', 'uml'):
4840             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
4841                     instance, inst_path, image_meta, disk_info)
4842 
4843         self._set_features(guest, instance.os_type, caps, virt_type,
4844                            image_meta)
4845         self._set_clock(guest, instance.os_type, image_meta, virt_type)
4846 
4847         storage_configs = self._get_guest_storage_config(
4848                 instance, image_meta, disk_info, rescue, block_device_info,
4849                 flavor, guest.os_type)
4850         for config in storage_configs:
4851             guest.add_device(config)
4852 
4853         for vif in network_info:
4854             config = self.vif_driver.get_config(
4855                 instance, vif, image_meta,
4856                 flavor, virt_type, self._host)
4857             guest.add_device(config)
4858 
4859         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
4860 
4861         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
4862         if pointer:
4863             guest.add_device(pointer)
4864 
4865         self._guest_add_spice_channel(guest)
4866 
4867         if self._guest_add_video_device(guest):
4868             self._add_video_driver(guest, image_meta, flavor)
4869 
4870         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
4871         if virt_type in ('qemu', 'kvm'):
4872             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
4873 
4874         self._guest_add_pci_devices(guest, instance)
4875 
4876         self._guest_add_watchdog_action(guest, flavor, image_meta)
4877 
4878         self._guest_add_memory_balloon(guest)
4879 
4880         return guest
4881 
4882     @staticmethod
4883     def _guest_add_spice_channel(guest):
4884         if (CONF.spice.enabled and CONF.spice.agent_enabled
4885                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
4886             channel = vconfig.LibvirtConfigGuestChannel()
4887             channel.type = 'spicevmc'
4888             channel.target_name = "com.redhat.spice.0"
4889             guest.add_device(channel)
4890 
4891     @staticmethod
4892     def _guest_add_memory_balloon(guest):
4893         virt_type = guest.virt_type
4894         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
4895         if (virt_type in ('xen', 'qemu', 'kvm') and
4896                     CONF.libvirt.mem_stats_period_seconds > 0):
4897             balloon = vconfig.LibvirtConfigMemoryBalloon()
4898             if virt_type in ('qemu', 'kvm'):
4899                 balloon.model = 'virtio'
4900             else:
4901                 balloon.model = 'xen'
4902             balloon.period = CONF.libvirt.mem_stats_period_seconds
4903             guest.add_device(balloon)
4904 
4905     @staticmethod
4906     def _guest_add_watchdog_action(guest, flavor, image_meta):
4907         # image meta takes precedence over flavor extra specs; disable the
4908         # watchdog action by default
4909         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
4910                            or 'disabled')
4911         watchdog_action = image_meta.properties.get('hw_watchdog_action',
4912                                                     watchdog_action)
4913         # NB(sross): currently only actually supported by KVM/QEmu
4914         if watchdog_action != 'disabled':
4915             if watchdog_action in fields.WatchdogAction.ALL:
4916                 bark = vconfig.LibvirtConfigGuestWatchdog()
4917                 bark.action = watchdog_action
4918                 guest.add_device(bark)
4919             else:
4920                 raise exception.InvalidWatchdogAction(action=watchdog_action)
4921 
4922     def _guest_add_pci_devices(self, guest, instance):
4923         virt_type = guest.virt_type
4924         if virt_type in ('xen', 'qemu', 'kvm'):
4925             # Get all generic PCI devices (non-SR-IOV).
4926             for pci_dev in pci_manager.get_instance_pci_devs(instance):
4927                 guest.add_device(self._get_guest_pci_device(pci_dev))
4928         else:
4929             # PCI devices is only supported for hypervisors
4930             #  'xen', 'qemu' and 'kvm'.
4931             if pci_manager.get_instance_pci_devs(instance, 'all'):
4932                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
4933 
4934     @staticmethod
4935     def _guest_add_video_device(guest):
4936         # NB some versions of libvirt support both SPICE and VNC
4937         # at the same time. We're not trying to second guess which
4938         # those versions are. We'll just let libvirt report the
4939         # errors appropriately if the user enables both.
4940         add_video_driver = False
4941         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
4942             graphics = vconfig.LibvirtConfigGuestGraphics()
4943             graphics.type = "vnc"
4944             if CONF.vnc.keymap:
4945                 # TODO(stephenfin): There are some issues here that may
4946                 # necessitate deprecating this option entirely in the future.
4947                 # Refer to bug #1682020 for more information.
4948                 graphics.keymap = CONF.vnc.keymap
4949             graphics.listen = CONF.vnc.server_listen
4950             guest.add_device(graphics)
4951             add_video_driver = True
4952         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
4953             graphics = vconfig.LibvirtConfigGuestGraphics()
4954             graphics.type = "spice"
4955             if CONF.spice.keymap:
4956                 # TODO(stephenfin): There are some issues here that may
4957                 # necessitate deprecating this option entirely in the future.
4958                 # Refer to bug #1682020 for more information.
4959                 graphics.keymap = CONF.spice.keymap
4960             graphics.listen = CONF.spice.server_listen
4961             guest.add_device(graphics)
4962             add_video_driver = True
4963         return add_video_driver
4964 
4965     def _get_guest_pointer_model(self, os_type, image_meta):
4966         pointer_model = image_meta.properties.get(
4967             'hw_pointer_model', CONF.pointer_model)
4968         if pointer_model is None and CONF.libvirt.use_usb_tablet:
4969             # TODO(sahid): We set pointer_model to keep compatibility
4970             # until the next release O*. It means operators can continue
4971             # to use the deprecated option "use_usb_tablet" or set a
4972             # specific device to use
4973             pointer_model = "usbtablet"
4974             LOG.warning('The option "use_usb_tablet" has been '
4975                         'deprecated for Newton in favor of the more '
4976                         'generic "pointer_model". Please update '
4977                         'nova.conf to address this change.')
4978 
4979         if pointer_model == "usbtablet":
4980             # We want a tablet if VNC is enabled, or SPICE is enabled and
4981             # the SPICE agent is disabled. If the SPICE agent is enabled
4982             # it provides a paravirt mouse which drastically reduces
4983             # overhead (by eliminating USB polling).
4984             if CONF.vnc.enabled or (
4985                     CONF.spice.enabled and not CONF.spice.agent_enabled):
4986                 return self._get_guest_usb_tablet(os_type)
4987             else:
4988                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
4989                     # For backward compatibility We don't want to break
4990                     # process of booting an instance if host is configured
4991                     # to use USB tablet without VNC or SPICE and SPICE
4992                     # agent disable.
4993                     LOG.warning('USB tablet requested for guests by host '
4994                                 'configuration. In order to accept this '
4995                                 'request VNC should be enabled or SPICE '
4996                                 'and SPICE agent disabled on host.')
4997                 else:
4998                     raise exception.UnsupportedPointerModelRequested(
4999                         model="usbtablet")
5000 
5001     def _get_guest_usb_tablet(self, os_type):
5002         tablet = None
5003         if os_type == fields.VMMode.HVM:
5004             tablet = vconfig.LibvirtConfigGuestInput()
5005             tablet.type = "tablet"
5006             tablet.bus = "usb"
5007         else:
5008             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5009                 # For backward compatibility We don't want to break
5010                 # process of booting an instance if virtual machine mode
5011                 # is not configured as HVM.
5012                 LOG.warning('USB tablet requested for guests by host '
5013                             'configuration. In order to accept this '
5014                             'request the machine mode should be '
5015                             'configured as HVM.')
5016             else:
5017                 raise exception.UnsupportedPointerModelRequested(
5018                     model="usbtablet")
5019         return tablet
5020 
5021     def _get_guest_xml(self, context, instance, network_info, disk_info,
5022                        image_meta, rescue=None,
5023                        block_device_info=None):
5024         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5025         # this ahead of time so that we don't acquire it while also
5026         # holding the logging lock.
5027         network_info_str = str(network_info)
5028         msg = ('Start _get_guest_xml '
5029                'network_info=%(network_info)s '
5030                'disk_info=%(disk_info)s '
5031                'image_meta=%(image_meta)s rescue=%(rescue)s '
5032                'block_device_info=%(block_device_info)s' %
5033                {'network_info': network_info_str, 'disk_info': disk_info,
5034                 'image_meta': image_meta, 'rescue': rescue,
5035                 'block_device_info': block_device_info})
5036         # NOTE(mriedem): block_device_info can contain auth_password so we
5037         # need to sanitize the password in the message.
5038         LOG.debug(strutils.mask_password(msg), instance=instance)
5039         conf = self._get_guest_config(instance, network_info, image_meta,
5040                                       disk_info, rescue, block_device_info,
5041                                       context)
5042         xml = conf.to_xml()
5043 
5044         LOG.debug('End _get_guest_xml xml=%(xml)s',
5045                   {'xml': xml}, instance=instance)
5046         return xml
5047 
5048     def get_info(self, instance):
5049         """Retrieve information from libvirt for a specific instance.
5050 
5051         If a libvirt error is encountered during lookup, we might raise a
5052         NotFound exception or Error exception depending on how severe the
5053         libvirt error is.
5054 
5055         :param instance: nova.objects.instance.Instance object
5056         :returns: An InstanceInfo object
5057         """
5058         guest = self._host.get_guest(instance)
5059         # Kind of ugly but we need to pass host to get_info as for a
5060         # workaround, see libvirt/compat.py
5061         return guest.get_info(self._host)
5062 
5063     def _create_domain_setup_lxc(self, instance, image_meta,
5064                                  block_device_info):
5065         inst_path = libvirt_utils.get_instance_path(instance)
5066         block_device_mapping = driver.block_device_info_get_mapping(
5067             block_device_info)
5068         root_disk = block_device.get_root_bdm(block_device_mapping)
5069         if root_disk:
5070             disk_info = blockinfo.get_info_from_bdm(
5071                 instance, CONF.libvirt.virt_type, image_meta, root_disk)
5072             self._connect_volume(root_disk['connection_info'], disk_info,
5073                                  instance)
5074             disk_path = root_disk['connection_info']['data']['device_path']
5075 
5076             # NOTE(apmelton) - Even though the instance is being booted from a
5077             # cinder volume, it is still presented as a local block device.
5078             # LocalBlockImage is used here to indicate that the instance's
5079             # disk is backed by a local block device.
5080             image_model = imgmodel.LocalBlockImage(disk_path)
5081         else:
5082             root_disk = self.image_backend.by_name(instance, 'disk')
5083             image_model = root_disk.get_model(self._conn)
5084 
5085         container_dir = os.path.join(inst_path, 'rootfs')
5086         fileutils.ensure_tree(container_dir)
5087         rootfs_dev = disk_api.setup_container(image_model,
5088                                               container_dir=container_dir)
5089 
5090         try:
5091             # Save rootfs device to disconnect it when deleting the instance
5092             if rootfs_dev:
5093                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5094             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5095                 id_maps = self._get_guest_idmaps()
5096                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5097         except Exception:
5098             with excutils.save_and_reraise_exception():
5099                 self._create_domain_cleanup_lxc(instance)
5100 
5101     def _create_domain_cleanup_lxc(self, instance):
5102         inst_path = libvirt_utils.get_instance_path(instance)
5103         container_dir = os.path.join(inst_path, 'rootfs')
5104 
5105         try:
5106             state = self.get_info(instance).state
5107         except exception.InstanceNotFound:
5108             # The domain may not be present if the instance failed to start
5109             state = None
5110 
5111         if state == power_state.RUNNING:
5112             # NOTE(uni): Now the container is running with its own private
5113             # mount namespace and so there is no need to keep the container
5114             # rootfs mounted in the host namespace
5115             LOG.debug('Attempting to unmount container filesystem: %s',
5116                       container_dir, instance=instance)
5117             disk_api.clean_lxc_namespace(container_dir=container_dir)
5118         else:
5119             disk_api.teardown_container(container_dir=container_dir)
5120 
5121     @contextlib.contextmanager
5122     def _lxc_disk_handler(self, instance, image_meta, block_device_info):
5123         """Context manager to handle the pre and post instance boot,
5124            LXC specific disk operations.
5125 
5126            An image or a volume path will be prepared and setup to be
5127            used by the container, prior to starting it.
5128            The disk will be disconnected and unmounted if a container has
5129            failed to start.
5130         """
5131 
5132         if CONF.libvirt.virt_type != 'lxc':
5133             yield
5134             return
5135 
5136         self._create_domain_setup_lxc(instance, image_meta, block_device_info)
5137 
5138         try:
5139             yield
5140         finally:
5141             self._create_domain_cleanup_lxc(instance)
5142 
5143     # TODO(sahid): Consider renaming this to _create_guest.
5144     def _create_domain(self, xml=None, domain=None,
5145                        power_on=True, pause=False, post_xml_callback=None):
5146         """Create a domain.
5147 
5148         Either domain or xml must be passed in. If both are passed, then
5149         the domain definition is overwritten from the xml.
5150 
5151         :returns guest.Guest: Guest just created
5152         """
5153         if xml:
5154             guest = libvirt_guest.Guest.create(xml, self._host)
5155             if post_xml_callback is not None:
5156                 post_xml_callback()
5157         else:
5158             guest = libvirt_guest.Guest(domain)
5159 
5160         if power_on or pause:
5161             guest.launch(pause=pause)
5162 
5163         if not utils.is_neutron():
5164             guest.enable_hairpin()
5165 
5166         return guest
5167 
5168     def _neutron_failed_callback(self, event_name, instance):
5169         LOG.error('Neutron Reported failure on event '
5170                   '%(event)s for instance %(uuid)s',
5171                   {'event': event_name, 'uuid': instance.uuid},
5172                   instance=instance)
5173         if CONF.vif_plugging_is_fatal:
5174             raise exception.VirtualInterfaceCreateException()
5175 
5176     def _get_neutron_events(self, network_info):
5177         # NOTE(danms): We need to collect any VIFs that are currently
5178         # down that we expect a down->up event for. Anything that is
5179         # already up will not undergo that transition, and for
5180         # anything that might be stale (cache-wise) assume it's
5181         # already up so we don't block on it.
5182         return [('network-vif-plugged', vif['id'])
5183                 for vif in network_info if vif.get('active', True) is False]
5184 
5185     def _cleanup_failed_start(self, context, instance, network_info,
5186                               block_device_info, guest, destroy_disks):
5187         try:
5188             if guest and guest.is_active():
5189                 guest.poweroff()
5190         finally:
5191             self.cleanup(context, instance, network_info=network_info,
5192                          block_device_info=block_device_info,
5193                          destroy_disks=destroy_disks)
5194 
5195     def _create_domain_and_network(self, context, xml, instance, network_info,
5196                                    block_device_info=None,
5197                                    power_on=True, reboot=False,
5198                                    vifs_already_plugged=False,
5199                                    post_xml_callback=None,
5200                                    destroy_disks_on_failure=False):
5201 
5202         """Do required network setup and create domain."""
5203         block_device_mapping = driver.block_device_info_get_mapping(
5204             block_device_info)
5205 
5206         for vol in block_device_mapping:
5207             connection_info = vol['connection_info']
5208 
5209             if (not reboot and 'data' in connection_info and
5210                     'volume_id' in connection_info['data']):
5211                 volume_id = connection_info['data']['volume_id']
5212                 encryption = encryptors.get_encryption_metadata(
5213                     context, self._volume_api, volume_id, connection_info)
5214 
5215                 if encryption:
5216                     encryptor = self._get_volume_encryptor(connection_info,
5217                                                            encryption)
5218                     encryptor.attach_volume(context, **encryption)
5219 
5220         timeout = CONF.vif_plugging_timeout
5221         if (self._conn_supports_start_paused and
5222             utils.is_neutron() and not
5223             vifs_already_plugged and power_on and timeout):
5224             events = self._get_neutron_events(network_info)
5225         else:
5226             events = []
5227 
5228         pause = bool(events)
5229         guest = None
5230         try:
5231             with self.virtapi.wait_for_instance_event(
5232                     instance, events, deadline=timeout,
5233                     error_callback=self._neutron_failed_callback):
5234                 self.plug_vifs(instance, network_info)
5235                 self.firewall_driver.setup_basic_filtering(instance,
5236                                                            network_info)
5237                 self.firewall_driver.prepare_instance_filter(instance,
5238                                                              network_info)
5239                 with self._lxc_disk_handler(instance, instance.image_meta,
5240                                             block_device_info):
5241                     guest = self._create_domain(
5242                         xml, pause=pause, power_on=power_on,
5243                         post_xml_callback=post_xml_callback)
5244 
5245                 self.firewall_driver.apply_instance_filter(instance,
5246                                                            network_info)
5247         except exception.VirtualInterfaceCreateException:
5248             # Neutron reported failure and we didn't swallow it, so
5249             # bail here
5250             with excutils.save_and_reraise_exception():
5251                 self._cleanup_failed_start(context, instance, network_info,
5252                                            block_device_info, guest,
5253                                            destroy_disks_on_failure)
5254         except eventlet.timeout.Timeout:
5255             # We never heard from Neutron
5256             LOG.warning('Timeout waiting for vif plugging callback for '
5257                         'instance with vm_state %(vm_state)s and '
5258                         'task_state %(task_state)s.',
5259                         {'vm_state': instance.vm_state,
5260                          'task_state': instance.task_state},
5261                         instance=instance)
5262             if CONF.vif_plugging_is_fatal:
5263                 self._cleanup_failed_start(context, instance, network_info,
5264                                            block_device_info, guest,
5265                                            destroy_disks_on_failure)
5266                 raise exception.VirtualInterfaceCreateException()
5267         except Exception:
5268             # Any other error, be sure to clean up
5269             LOG.error('Failed to start libvirt guest', instance=instance)
5270             with excutils.save_and_reraise_exception():
5271                 self._cleanup_failed_start(context, instance, network_info,
5272                                            block_device_info, guest,
5273                                            destroy_disks_on_failure)
5274 
5275         # Resume only if domain has been paused
5276         if pause:
5277             guest.resume()
5278         return guest
5279 
5280     def _get_vcpu_total(self):
5281         """Get available vcpu number of physical computer.
5282 
5283         :returns: the number of cpu core instances can be used.
5284 
5285         """
5286         try:
5287             total_pcpus = self._host.get_cpu_count()
5288         except libvirt.libvirtError:
5289             LOG.warning("Cannot get the number of cpu, because this "
5290                         "function is not implemented for this platform. ")
5291             return 0
5292 
5293         if not CONF.vcpu_pin_set:
5294             return total_pcpus
5295 
5296         available_ids = hardware.get_vcpu_pin_set()
5297         # We get the list of online CPUs on the host and see if the requested
5298         # set falls under these. If not, we retain the old behavior.
5299         online_pcpus = None
5300         try:
5301             online_pcpus = self._host.get_online_cpus()
5302         except libvirt.libvirtError as ex:
5303             error_code = ex.get_error_code()
5304             LOG.warning(
5305                 "Couldn't retrieve the online CPUs due to a Libvirt "
5306                 "error: %(error)s with error code: %(error_code)s",
5307                 {'error': ex, 'error_code': error_code})
5308         if online_pcpus:
5309             if not (available_ids <= online_pcpus):
5310                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5311                          "specified cpuset is not online. Online cpuset(s): "
5312                          "%(online)s, requested cpuset(s): %(req)s"),
5313                        {'online': sorted(online_pcpus),
5314                         'req': sorted(available_ids)})
5315                 raise exception.Invalid(msg)
5316         elif sorted(available_ids)[-1] >= total_pcpus:
5317             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5318                                       "out of hypervisor cpu range."))
5319         return len(available_ids)
5320 
5321     @staticmethod
5322     def _get_local_gb_info():
5323         """Get local storage info of the compute node in GB.
5324 
5325         :returns: A dict containing:
5326              :total: How big the overall usable filesystem is (in gigabytes)
5327              :free: How much space is free (in gigabytes)
5328              :used: How much space is used (in gigabytes)
5329         """
5330 
5331         if CONF.libvirt.images_type == 'lvm':
5332             info = lvm.get_volume_group_info(
5333                                CONF.libvirt.images_volume_group)
5334         elif CONF.libvirt.images_type == 'rbd':
5335             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5336         else:
5337             info = libvirt_utils.get_fs_info(CONF.instances_path)
5338 
5339         for (k, v) in info.items():
5340             info[k] = v / units.Gi
5341 
5342         return info
5343 
5344     def _get_vcpu_used(self):
5345         """Get vcpu usage number of physical computer.
5346 
5347         :returns: The total number of vcpu(s) that are currently being used.
5348 
5349         """
5350 
5351         total = 0
5352 
5353         # Not all libvirt drivers will support the get_vcpus_info()
5354         #
5355         # For example, LXC does not have a concept of vCPUs, while
5356         # QEMU (TCG) traditionally handles all vCPUs in a single
5357         # thread. So both will report an exception when the vcpus()
5358         # API call is made. In such a case we should report the
5359         # guest as having 1 vCPU, since that lets us still do
5360         # CPU over commit calculations that apply as the total
5361         # guest count scales.
5362         #
5363         # It is also possible that we might see an exception if
5364         # the guest is just in middle of shutting down. Technically
5365         # we should report 0 for vCPU usage in this case, but we
5366         # we can't reliably distinguish the vcpu not supported
5367         # case from the just shutting down case. Thus we don't know
5368         # whether to report 1 or 0 for vCPU count.
5369         #
5370         # Under-reporting vCPUs is bad because it could conceivably
5371         # let the scheduler place too many guests on the host. Over-
5372         # reporting vCPUs is not a problem as it'll auto-correct on
5373         # the next refresh of usage data.
5374         #
5375         # Thus when getting an exception we always report 1 as the
5376         # vCPU count, as the least worst value.
5377         for guest in self._host.list_guests():
5378             try:
5379                 vcpus = guest.get_vcpus_info()
5380                 total += len(list(vcpus))
5381             except libvirt.libvirtError:
5382                 total += 1
5383             # NOTE(gtt116): give other tasks a chance.
5384             greenthread.sleep(0)
5385         return total
5386 
5387     def _get_instance_capabilities(self):
5388         """Get hypervisor instance capabilities
5389 
5390         Returns a list of tuples that describe instances the
5391         hypervisor is capable of hosting.  Each tuple consists
5392         of the triplet (arch, hypervisor_type, vm_mode).
5393 
5394         :returns: List of tuples describing instance capabilities
5395         """
5396         caps = self._host.get_capabilities()
5397         instance_caps = list()
5398         for g in caps.guests:
5399             for dt in g.domtype:
5400                 instance_cap = (
5401                     fields.Architecture.canonicalize(g.arch),
5402                     fields.HVType.canonicalize(dt),
5403                     fields.VMMode.canonicalize(g.ostype))
5404                 instance_caps.append(instance_cap)
5405 
5406         return instance_caps
5407 
5408     def _get_cpu_info(self):
5409         """Get cpuinfo information.
5410 
5411         Obtains cpu feature from virConnect.getCapabilities.
5412 
5413         :return: see above description
5414 
5415         """
5416 
5417         caps = self._host.get_capabilities()
5418         cpu_info = dict()
5419 
5420         cpu_info['arch'] = caps.host.cpu.arch
5421         cpu_info['model'] = caps.host.cpu.model
5422         cpu_info['vendor'] = caps.host.cpu.vendor
5423 
5424         topology = dict()
5425         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5426         topology['sockets'] = caps.host.cpu.sockets
5427         topology['cores'] = caps.host.cpu.cores
5428         topology['threads'] = caps.host.cpu.threads
5429         cpu_info['topology'] = topology
5430 
5431         features = set()
5432         for f in caps.host.cpu.features:
5433             features.add(f.name)
5434         cpu_info['features'] = features
5435         return cpu_info
5436 
5437     def _get_pcinet_info(self, vf_address):
5438         """Returns a dict of NET device."""
5439         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5440         if not devname:
5441             return
5442 
5443         virtdev = self._host.device_lookup_by_name(devname)
5444         xmlstr = virtdev.XMLDesc(0)
5445         cfgdev = vconfig.LibvirtConfigNodeDevice()
5446         cfgdev.parse_str(xmlstr)
5447         return {'name': cfgdev.name,
5448                 'capabilities': cfgdev.pci_capability.features}
5449 
5450     def _get_pcidev_info(self, devname):
5451         """Returns a dict of PCI device."""
5452 
5453         def _get_device_type(cfgdev, pci_address):
5454             """Get a PCI device's device type.
5455 
5456             An assignable PCI device can be a normal PCI device,
5457             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5458             Function (VF). Only normal PCI devices or SR-IOV VFs
5459             are assignable, while SR-IOV PFs are always owned by
5460             hypervisor.
5461             """
5462             for fun_cap in cfgdev.pci_capability.fun_capability:
5463                 if fun_cap.type == 'virt_functions':
5464                     return {
5465                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5466                     }
5467                 if (fun_cap.type == 'phys_function' and
5468                     len(fun_cap.device_addrs) != 0):
5469                     phys_address = "%04x:%02x:%02x.%01x" % (
5470                         fun_cap.device_addrs[0][0],
5471                         fun_cap.device_addrs[0][1],
5472                         fun_cap.device_addrs[0][2],
5473                         fun_cap.device_addrs[0][3])
5474                     return {
5475                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5476                         'parent_addr': phys_address,
5477                     }
5478 
5479             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5480             # only when VFs are enabled. The check below is a workaround
5481             # to get the correct report regardless of whether or not any
5482             # VFs are enabled for the device.
5483             if not self._host.has_min_version(
5484                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5485                 is_physical_function = pci_utils.is_physical_function(
5486                     *pci_utils.get_pci_address_fields(pci_address))
5487                 if is_physical_function:
5488                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5489 
5490             return {'dev_type': fields.PciDeviceType.STANDARD}
5491 
5492         def _get_device_capabilities(device, address):
5493             """Get PCI VF device's additional capabilities.
5494 
5495             If a PCI device is a virtual function, this function reads the PCI
5496             parent's network capabilities (must be always a NIC device) and
5497             appends this information to the device's dictionary.
5498             """
5499             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5500                 pcinet_info = self._get_pcinet_info(address)
5501                 if pcinet_info:
5502                     return {'capabilities':
5503                                 {'network': pcinet_info.get('capabilities')}}
5504             return {}
5505 
5506         virtdev = self._host.device_lookup_by_name(devname)
5507         xmlstr = virtdev.XMLDesc(0)
5508         cfgdev = vconfig.LibvirtConfigNodeDevice()
5509         cfgdev.parse_str(xmlstr)
5510 
5511         address = "%04x:%02x:%02x.%1x" % (
5512             cfgdev.pci_capability.domain,
5513             cfgdev.pci_capability.bus,
5514             cfgdev.pci_capability.slot,
5515             cfgdev.pci_capability.function)
5516 
5517         device = {
5518             "dev_id": cfgdev.name,
5519             "address": address,
5520             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5521             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5522             }
5523 
5524         device["numa_node"] = cfgdev.pci_capability.numa_node
5525 
5526         # requirement by DataBase Model
5527         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5528         device.update(_get_device_type(cfgdev, address))
5529         device.update(_get_device_capabilities(device, address))
5530         return device
5531 
5532     def _get_pci_passthrough_devices(self):
5533         """Get host PCI devices information.
5534 
5535         Obtains pci devices information from libvirt, and returns
5536         as a JSON string.
5537 
5538         Each device information is a dictionary, with mandatory keys
5539         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5540         'label' and other optional device specific information.
5541 
5542         Refer to the objects/pci_device.py for more idea of these keys.
5543 
5544         :returns: a JSON string containing a list of the assignable PCI
5545                   devices information
5546         """
5547         # Bail early if we know we can't support `listDevices` to avoid
5548         # repeated warnings within a periodic task
5549         if not getattr(self, '_list_devices_supported', True):
5550             return jsonutils.dumps([])
5551 
5552         try:
5553             dev_names = self._host.list_pci_devices() or []
5554         except libvirt.libvirtError as ex:
5555             error_code = ex.get_error_code()
5556             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5557                 self._list_devices_supported = False
5558                 LOG.warning("URI %(uri)s does not support "
5559                             "listDevices: %(error)s",
5560                             {'uri': self._uri(), 'error': ex})
5561                 return jsonutils.dumps([])
5562             else:
5563                 raise
5564 
5565         pci_info = []
5566         for name in dev_names:
5567             pci_info.append(self._get_pcidev_info(name))
5568 
5569         return jsonutils.dumps(pci_info)
5570 
5571     def _has_numa_support(self):
5572         # This means that the host can support LibvirtConfigGuestNUMATune
5573         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
5574         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
5575             if self._host.has_version(ver):
5576                 if not getattr(self, '_bad_libvirt_numa_version_warn', False):
5577                     LOG.warning('You are running with libvirt version %s '
5578                                 'which is known to have broken NUMA support. '
5579                                 'Consider patching or updating libvirt on '
5580                                 'this host if you need NUMA support.',
5581                                 self._version_to_string(ver))
5582                     self._bad_libvirt_numa_version_warn = True
5583                 return False
5584 
5585         caps = self._host.get_capabilities()
5586 
5587         if (caps.host.cpu.arch in (fields.Architecture.I686,
5588                                    fields.Architecture.X86_64,
5589                                    fields.Architecture.AARCH64) and
5590                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
5591             return True
5592         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
5593                                      fields.Architecture.PPC64LE) and
5594                 self._host.has_min_version(MIN_LIBVIRT_NUMA_VERSION_PPC,
5595                                            hv_type=host.HV_DRIVER_QEMU)):
5596             return True
5597 
5598         return False
5599 
5600     def _get_host_numa_topology(self):
5601         if not self._has_numa_support():
5602             return
5603 
5604         caps = self._host.get_capabilities()
5605         topology = caps.host.topology
5606 
5607         if topology is None or not topology.cells:
5608             return
5609 
5610         cells = []
5611         allowed_cpus = hardware.get_vcpu_pin_set()
5612         online_cpus = self._host.get_online_cpus()
5613         if allowed_cpus:
5614             allowed_cpus &= online_cpus
5615         else:
5616             allowed_cpus = online_cpus
5617 
5618         def _get_reserved_memory_for_cell(self, cell_id, page_size):
5619             cell = self._reserved_hugepages.get(cell_id, {})
5620             return cell.get(page_size, 0)
5621 
5622         for cell in topology.cells:
5623             cpuset = set(cpu.id for cpu in cell.cpus)
5624             siblings = sorted(map(set,
5625                                   set(tuple(cpu.siblings)
5626                                         if cpu.siblings else ()
5627                                       for cpu in cell.cpus)
5628                                   ))
5629             cpuset &= allowed_cpus
5630             siblings = [sib & allowed_cpus for sib in siblings]
5631             # Filter out singles and empty sibling sets that may be left
5632             siblings = [sib for sib in siblings if len(sib) > 1]
5633 
5634             mempages = [
5635                 objects.NUMAPagesTopology(
5636                     size_kb=pages.size,
5637                     total=pages.total,
5638                     used=0,
5639                     reserved=_get_reserved_memory_for_cell(
5640                         self, cell.id, pages.size))
5641                 for pages in cell.mempages]
5642 
5643             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
5644                                     memory=cell.memory / units.Ki,
5645                                     cpu_usage=0, memory_usage=0,
5646                                     siblings=siblings,
5647                                     pinned_cpus=set([]),
5648                                     mempages=mempages)
5649             cells.append(cell)
5650 
5651         return objects.NUMATopology(cells=cells)
5652 
5653     def get_all_volume_usage(self, context, compute_host_bdms):
5654         """Return usage info for volumes attached to vms on
5655            a given host.
5656         """
5657         vol_usage = []
5658 
5659         for instance_bdms in compute_host_bdms:
5660             instance = instance_bdms['instance']
5661 
5662             for bdm in instance_bdms['instance_bdms']:
5663                 mountpoint = bdm['device_name']
5664                 if mountpoint.startswith('/dev/'):
5665                     mountpoint = mountpoint[5:]
5666                 volume_id = bdm['volume_id']
5667 
5668                 LOG.debug("Trying to get stats for the volume %s",
5669                           volume_id, instance=instance)
5670                 vol_stats = self.block_stats(instance, mountpoint)
5671 
5672                 if vol_stats:
5673                     stats = dict(volume=volume_id,
5674                                  instance=instance,
5675                                  rd_req=vol_stats[0],
5676                                  rd_bytes=vol_stats[1],
5677                                  wr_req=vol_stats[2],
5678                                  wr_bytes=vol_stats[3])
5679                     LOG.debug(
5680                         "Got volume usage stats for the volume=%(volume)s,"
5681                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
5682                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
5683                         stats, instance=instance)
5684                     vol_usage.append(stats)
5685 
5686         return vol_usage
5687 
5688     def block_stats(self, instance, disk_id):
5689         """Note that this function takes an instance name."""
5690         try:
5691             guest = self._host.get_guest(instance)
5692 
5693             # TODO(sahid): We are converting all calls from a
5694             # virDomain object to use nova.virt.libvirt.Guest.
5695             # We should be able to remove domain at the end.
5696             domain = guest._domain
5697             return domain.blockStats(disk_id)
5698         except libvirt.libvirtError as e:
5699             errcode = e.get_error_code()
5700             LOG.info('Getting block stats failed, device might have '
5701                      'been detached. Instance=%(instance_name)s '
5702                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
5703                      {'instance_name': instance.name, 'disk': disk_id,
5704                       'errcode': errcode, 'e': e},
5705                      instance=instance)
5706         except exception.InstanceNotFound:
5707             LOG.info('Could not find domain in libvirt for instance %s. '
5708                      'Cannot get block stats for device', instance.name,
5709                      instance=instance)
5710 
5711     def get_console_pool_info(self, console_type):
5712         # TODO(mdragon): console proxy should be implemented for libvirt,
5713         #                in case someone wants to use it with kvm or
5714         #                such. For now return fake data.
5715         return {'address': '127.0.0.1',
5716                 'username': 'fakeuser',
5717                 'password': 'fakepassword'}
5718 
5719     def refresh_security_group_rules(self, security_group_id):
5720         self.firewall_driver.refresh_security_group_rules(security_group_id)
5721 
5722     def refresh_instance_security_rules(self, instance):
5723         self.firewall_driver.refresh_instance_security_rules(instance)
5724 
5725     def get_inventory(self, nodename):
5726         """Return a dict, keyed by resource class, of inventory information for
5727         the supplied node.
5728         """
5729         disk_gb = int(self._get_local_gb_info()['total'])
5730         memory_mb = int(self._host.get_memory_mb_total())
5731         vcpus = self._get_vcpu_total()
5732         # NOTE(jaypipes): We leave some fields like allocation_ratio and
5733         # reserved out of the returned dicts here because, for now at least,
5734         # the RT injects those values into the inventory dict based on the
5735         # compute_nodes record values.
5736         result = {
5737             fields.ResourceClass.VCPU: {
5738                 'total': vcpus,
5739                 'min_unit': 1,
5740                 'max_unit': vcpus,
5741                 'step_size': 1,
5742             },
5743             fields.ResourceClass.MEMORY_MB: {
5744                 'total': memory_mb,
5745                 'min_unit': 1,
5746                 'max_unit': memory_mb,
5747                 'step_size': 1,
5748             },
5749             fields.ResourceClass.DISK_GB: {
5750                 'total': disk_gb,
5751                 'min_unit': 1,
5752                 'max_unit': disk_gb,
5753                 'step_size': 1,
5754             },
5755         }
5756         return result
5757 
5758     def get_available_resource(self, nodename):
5759         """Retrieve resource information.
5760 
5761         This method is called when nova-compute launches, and
5762         as part of a periodic task that records the results in the DB.
5763 
5764         :param nodename: unused in this driver
5765         :returns: dictionary containing resource info
5766         """
5767 
5768         disk_info_dict = self._get_local_gb_info()
5769         data = {}
5770 
5771         # NOTE(dprince): calling capabilities before getVersion works around
5772         # an initialization issue with some versions of Libvirt (1.0.5.5).
5773         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
5774         # See: https://bugs.launchpad.net/nova/+bug/1215593
5775         data["supported_instances"] = self._get_instance_capabilities()
5776 
5777         data["vcpus"] = self._get_vcpu_total()
5778         data["memory_mb"] = self._host.get_memory_mb_total()
5779         data["local_gb"] = disk_info_dict['total']
5780         data["vcpus_used"] = self._get_vcpu_used()
5781         data["memory_mb_used"] = self._host.get_memory_mb_used()
5782         data["local_gb_used"] = disk_info_dict['used']
5783         data["hypervisor_type"] = self._host.get_driver_type()
5784         data["hypervisor_version"] = self._host.get_version()
5785         data["hypervisor_hostname"] = self._host.get_hostname()
5786         # TODO(berrange): why do we bother converting the
5787         # libvirt capabilities XML into a special JSON format ?
5788         # The data format is different across all the drivers
5789         # so we could just return the raw capabilities XML
5790         # which 'compare_cpu' could use directly
5791         #
5792         # That said, arch_filter.py now seems to rely on
5793         # the libvirt drivers format which suggests this
5794         # data format needs to be standardized across drivers
5795         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
5796 
5797         disk_free_gb = disk_info_dict['free']
5798         disk_over_committed = self._get_disk_over_committed_size_total()
5799         available_least = disk_free_gb * units.Gi - disk_over_committed
5800         data['disk_available_least'] = available_least / units.Gi
5801 
5802         data['pci_passthrough_devices'] = \
5803             self._get_pci_passthrough_devices()
5804 
5805         numa_topology = self._get_host_numa_topology()
5806         if numa_topology:
5807             data['numa_topology'] = numa_topology._to_json()
5808         else:
5809             data['numa_topology'] = None
5810 
5811         return data
5812 
5813     def check_instance_shared_storage_local(self, context, instance):
5814         """Check if instance files located on shared storage.
5815 
5816         This runs check on the destination host, and then calls
5817         back to the source host to check the results.
5818 
5819         :param context: security context
5820         :param instance: nova.objects.instance.Instance object
5821         :returns:
5822          - tempfile: A dict containing the tempfile info on the destination
5823                      host
5824          - None:
5825 
5826             1. If the instance path is not existing.
5827             2. If the image backend is shared block storage type.
5828         """
5829         if self.image_backend.backend().is_shared_block_storage():
5830             return None
5831 
5832         dirpath = libvirt_utils.get_instance_path(instance)
5833 
5834         if not os.path.exists(dirpath):
5835             return None
5836 
5837         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
5838         LOG.debug("Creating tmpfile %s to verify with other "
5839                   "compute node that the instance is on "
5840                   "the same shared storage.",
5841                   tmp_file, instance=instance)
5842         os.close(fd)
5843         return {"filename": tmp_file}
5844 
5845     def check_instance_shared_storage_remote(self, context, data):
5846         return os.path.exists(data['filename'])
5847 
5848     def check_instance_shared_storage_cleanup(self, context, data):
5849         fileutils.delete_if_exists(data["filename"])
5850 
5851     def check_can_live_migrate_destination(self, context, instance,
5852                                            src_compute_info, dst_compute_info,
5853                                            block_migration=False,
5854                                            disk_over_commit=False):
5855         """Check if it is possible to execute live migration.
5856 
5857         This runs checks on the destination host, and then calls
5858         back to the source host to check the results.
5859 
5860         :param context: security context
5861         :param instance: nova.db.sqlalchemy.models.Instance
5862         :param block_migration: if true, prepare for block migration
5863         :param disk_over_commit: if true, allow disk over commit
5864         :returns: a LibvirtLiveMigrateData object
5865         """
5866         disk_available_gb = dst_compute_info['disk_available_least']
5867         disk_available_mb = (
5868             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
5869 
5870         # Compare CPU
5871         if not instance.vcpu_model or not instance.vcpu_model.model:
5872             source_cpu_info = src_compute_info['cpu_info']
5873             self._compare_cpu(None, source_cpu_info, instance)
5874         else:
5875             self._compare_cpu(instance.vcpu_model, None, instance)
5876 
5877         # Create file on storage, to be checked on source host
5878         filename = self._create_shared_storage_test_file(instance)
5879 
5880         data = objects.LibvirtLiveMigrateData()
5881         data.filename = filename
5882         data.image_type = CONF.libvirt.images_type
5883         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
5884         data.graphics_listen_addr_spice = CONF.spice.server_listen
5885         if CONF.serial_console.enabled:
5886             data.serial_listen_addr = CONF.serial_console.proxyclient_address
5887         else:
5888             data.serial_listen_addr = None
5889         # Notes(eliqiao): block_migration and disk_over_commit are not
5890         # nullable, so just don't set them if they are None
5891         if block_migration is not None:
5892             data.block_migration = block_migration
5893         if disk_over_commit is not None:
5894             data.disk_over_commit = disk_over_commit
5895         data.disk_available_mb = disk_available_mb
5896         return data
5897 
5898     def cleanup_live_migration_destination_check(self, context,
5899                                                  dest_check_data):
5900         """Do required cleanup on dest host after check_can_live_migrate calls
5901 
5902         :param context: security context
5903         """
5904         filename = dest_check_data.filename
5905         self._cleanup_shared_storage_test_file(filename)
5906 
5907     def check_can_live_migrate_source(self, context, instance,
5908                                       dest_check_data,
5909                                       block_device_info=None):
5910         """Check if it is possible to execute live migration.
5911 
5912         This checks if the live migration can succeed, based on the
5913         results from check_can_live_migrate_destination.
5914 
5915         :param context: security context
5916         :param instance: nova.db.sqlalchemy.models.Instance
5917         :param dest_check_data: result of check_can_live_migrate_destination
5918         :param block_device_info: result of _get_instance_block_device_info
5919         :returns: a LibvirtLiveMigrateData object
5920         """
5921         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
5922             md_obj = objects.LibvirtLiveMigrateData()
5923             md_obj.from_legacy_dict(dest_check_data)
5924             dest_check_data = md_obj
5925 
5926         # Checking shared storage connectivity
5927         # if block migration, instances_path should not be on shared storage.
5928         source = CONF.host
5929 
5930         dest_check_data.is_shared_instance_path = (
5931             self._check_shared_storage_test_file(
5932                 dest_check_data.filename, instance))
5933 
5934         dest_check_data.is_shared_block_storage = (
5935             self._is_shared_block_storage(instance, dest_check_data,
5936                                           block_device_info))
5937 
5938         if 'block_migration' not in dest_check_data:
5939             dest_check_data.block_migration = (
5940                 not dest_check_data.is_on_shared_storage())
5941 
5942         if dest_check_data.block_migration:
5943             # TODO(eliqiao): Once block_migration flag is removed from the API
5944             # we can safely remove the if condition
5945             if dest_check_data.is_on_shared_storage():
5946                 reason = _("Block migration can not be used "
5947                            "with shared storage.")
5948                 raise exception.InvalidLocalStorage(reason=reason, path=source)
5949             if 'disk_over_commit' in dest_check_data:
5950                 self._assert_dest_node_has_enough_disk(context, instance,
5951                                         dest_check_data.disk_available_mb,
5952                                         dest_check_data.disk_over_commit,
5953                                         block_device_info)
5954             if block_device_info:
5955                 bdm = block_device_info.get('block_device_mapping')
5956                 # NOTE(pkoniszewski): libvirt from version 1.2.17 upwards
5957                 # supports selective block device migration. It means that it
5958                 # is possible to define subset of block devices to be copied
5959                 # during migration. If they are not specified - block devices
5960                 # won't be migrated. However, it does not work when live
5961                 # migration is tunnelled through libvirt.
5962                 if bdm and not self._host.has_min_version(
5963                         MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
5964                     # NOTE(stpierre): if this instance has mapped volumes,
5965                     # we can't do a block migration, since that will result
5966                     # in volumes being copied from themselves to themselves,
5967                     # which is a recipe for disaster.
5968                     ver = ".".join([str(x) for x in
5969                                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION])
5970                     msg = (_('Cannot block migrate instance %(uuid)s with'
5971                              ' mapped volumes. Selective block device'
5972                              ' migration feature requires libvirt version'
5973                              ' %(libvirt_ver)s') %
5974                            {'uuid': instance.uuid, 'libvirt_ver': ver})
5975                     LOG.error(msg, instance=instance)
5976                     raise exception.MigrationPreCheckError(reason=msg)
5977                 # NOTE(eliqiao): Selective disk migrations are not supported
5978                 # with tunnelled block migrations so we can block them early.
5979                 if (bdm and
5980                     (self._block_migration_flags &
5981                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
5982                     msg = (_('Cannot block migrate instance %(uuid)s with'
5983                              ' mapped volumes. Selective block device'
5984                              ' migration is not supported with tunnelled'
5985                              ' block migrations.') % {'uuid': instance.uuid})
5986                     LOG.error(msg, instance=instance)
5987                     raise exception.MigrationPreCheckError(reason=msg)
5988         elif not (dest_check_data.is_shared_block_storage or
5989                   dest_check_data.is_shared_instance_path):
5990             reason = _("Shared storage live-migration requires either shared "
5991                        "storage or boot-from-volume with no local disks.")
5992             raise exception.InvalidSharedStorage(reason=reason, path=source)
5993 
5994         # NOTE(mikal): include the instance directory name here because it
5995         # doesn't yet exist on the destination but we want to force that
5996         # same name to be used
5997         instance_path = libvirt_utils.get_instance_path(instance,
5998                                                         relative=True)
5999         dest_check_data.instance_relative_path = instance_path
6000 
6001         return dest_check_data
6002 
6003     def _is_shared_block_storage(self, instance, dest_check_data,
6004                                  block_device_info=None):
6005         """Check if all block storage of an instance can be shared
6006         between source and destination of a live migration.
6007 
6008         Returns true if the instance is volume backed and has no local disks,
6009         or if the image backend is the same on source and destination and the
6010         backend shares block storage between compute nodes.
6011 
6012         :param instance: nova.objects.instance.Instance object
6013         :param dest_check_data: dict with boolean fields image_type,
6014                                 is_shared_instance_path, and is_volume_backed
6015         """
6016         if (dest_check_data.obj_attr_is_set('image_type') and
6017                 CONF.libvirt.images_type == dest_check_data.image_type and
6018                 self.image_backend.backend().is_shared_block_storage()):
6019             # NOTE(dgenin): currently true only for RBD image backend
6020             return True
6021 
6022         if (dest_check_data.is_shared_instance_path and
6023                 self.image_backend.backend().is_file_in_instance_path()):
6024             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6025             # place block device files under the instance path
6026             return True
6027 
6028         if (dest_check_data.is_volume_backed and
6029                 not bool(self._get_instance_disk_info(instance,
6030                                                       block_device_info))):
6031             return True
6032 
6033         return False
6034 
6035     def _assert_dest_node_has_enough_disk(self, context, instance,
6036                                              available_mb, disk_over_commit,
6037                                              block_device_info):
6038         """Checks if destination has enough disk for block migration."""
6039         # Libvirt supports qcow2 disk format,which is usually compressed
6040         # on compute nodes.
6041         # Real disk image (compressed) may enlarged to "virtual disk size",
6042         # that is specified as the maximum disk size.
6043         # (See qemu-img -f path-to-disk)
6044         # Scheduler recognizes destination host still has enough disk space
6045         # if real disk size < available disk size
6046         # if disk_over_commit is True,
6047         #  otherwise virtual disk size < available disk size.
6048 
6049         available = 0
6050         if available_mb:
6051             available = available_mb * units.Mi
6052 
6053         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6054 
6055         necessary = 0
6056         if disk_over_commit:
6057             for info in disk_infos:
6058                 necessary += int(info['disk_size'])
6059         else:
6060             for info in disk_infos:
6061                 necessary += int(info['virt_disk_size'])
6062 
6063         # Check that available disk > necessary disk
6064         if (available - necessary) < 0:
6065             reason = (_('Unable to migrate %(instance_uuid)s: '
6066                         'Disk of instance is too large(available'
6067                         ' on destination host:%(available)s '
6068                         '< need:%(necessary)s)') %
6069                       {'instance_uuid': instance.uuid,
6070                        'available': available,
6071                        'necessary': necessary})
6072             raise exception.MigrationPreCheckError(reason=reason)
6073 
6074     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6075         """Check the host is compatible with the requested CPU
6076 
6077         :param guest_cpu: nova.objects.VirtCPUModel or None
6078         :param host_cpu_str: JSON from _get_cpu_info() method
6079 
6080         If the 'guest_cpu' parameter is not None, this will be
6081         validated for migration compatibility with the host.
6082         Otherwise the 'host_cpu_str' JSON string will be used for
6083         validation.
6084 
6085         :returns:
6086             None. if given cpu info is not compatible to this server,
6087             raise exception.
6088         """
6089 
6090         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6091         # guests (<domain type='qemu'>) should not matter -- in this
6092         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6093         # hardware acceleration, like KVM, is involved. So, skip the CPU
6094         # compatibility check for the QEMU domain type, and retain it for
6095         # KVM guests.
6096         if CONF.libvirt.virt_type not in ['kvm']:
6097             return
6098 
6099         if guest_cpu is None:
6100             info = jsonutils.loads(host_cpu_str)
6101             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6102             cpu = vconfig.LibvirtConfigCPU()
6103             cpu.arch = info['arch']
6104             cpu.model = info['model']
6105             cpu.vendor = info['vendor']
6106             cpu.sockets = info['topology']['sockets']
6107             cpu.cores = info['topology']['cores']
6108             cpu.threads = info['topology']['threads']
6109             for f in info['features']:
6110                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6111         else:
6112             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6113 
6114         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6115              "virCPUCompareResult")
6116         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6117         # unknown character exists in xml, then libvirt complains
6118         try:
6119             cpu_xml = cpu.to_xml()
6120             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6121             ret = self._host.compare_cpu(cpu_xml)
6122         except libvirt.libvirtError as e:
6123             error_code = e.get_error_code()
6124             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6125                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6126                           "It will be proceeded though. Error: %(error)s",
6127                           {'uri': self._uri(), 'error': e})
6128                 return
6129             else:
6130                 LOG.error(m, {'ret': e, 'u': u})
6131                 raise exception.MigrationPreCheckError(
6132                     reason=m % {'ret': e, 'u': u})
6133 
6134         if ret <= 0:
6135             LOG.error(m, {'ret': ret, 'u': u})
6136             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6137 
6138     def _create_shared_storage_test_file(self, instance):
6139         """Makes tmpfile under CONF.instances_path."""
6140         dirpath = CONF.instances_path
6141         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6142         LOG.debug("Creating tmpfile %s to notify to other "
6143                   "compute nodes that they should mount "
6144                   "the same storage.", tmp_file, instance=instance)
6145         os.close(fd)
6146         return os.path.basename(tmp_file)
6147 
6148     def _check_shared_storage_test_file(self, filename, instance):
6149         """Confirms existence of the tmpfile under CONF.instances_path.
6150 
6151         Cannot confirm tmpfile return False.
6152         """
6153         # NOTE(tpatzig): if instances_path is a shared volume that is
6154         # under heavy IO (many instances on many compute nodes),
6155         # then checking the existence of the testfile fails,
6156         # just because it takes longer until the client refreshes and new
6157         # content gets visible.
6158         # os.utime (like touch) on the directory forces the client to refresh.
6159         os.utime(CONF.instances_path, None)
6160 
6161         tmp_file = os.path.join(CONF.instances_path, filename)
6162         if not os.path.exists(tmp_file):
6163             exists = False
6164         else:
6165             exists = True
6166         LOG.debug('Check if temp file %s exists to indicate shared storage '
6167                   'is being used for migration. Exists? %s', tmp_file, exists,
6168                   instance=instance)
6169         return exists
6170 
6171     def _cleanup_shared_storage_test_file(self, filename):
6172         """Removes existence of the tmpfile under CONF.instances_path."""
6173         tmp_file = os.path.join(CONF.instances_path, filename)
6174         os.remove(tmp_file)
6175 
6176     def ensure_filtering_rules_for_instance(self, instance, network_info):
6177         """Ensure that an instance's filtering rules are enabled.
6178 
6179         When migrating an instance, we need the filtering rules to
6180         be configured on the destination host before starting the
6181         migration.
6182 
6183         Also, when restarting the compute service, we need to ensure
6184         that filtering rules exist for all running services.
6185         """
6186 
6187         self.firewall_driver.setup_basic_filtering(instance, network_info)
6188         self.firewall_driver.prepare_instance_filter(instance,
6189                 network_info)
6190 
6191         # nwfilters may be defined in a separate thread in the case
6192         # of libvirt non-blocking mode, so we wait for completion
6193         timeout_count = list(range(CONF.live_migration_retry_count))
6194         while timeout_count:
6195             if self.firewall_driver.instance_filter_exists(instance,
6196                                                            network_info):
6197                 break
6198             timeout_count.pop()
6199             if len(timeout_count) == 0:
6200                 msg = _('The firewall filter for %s does not exist')
6201                 raise exception.InternalError(msg % instance.name)
6202             greenthread.sleep(1)
6203 
6204     def filter_defer_apply_on(self):
6205         self.firewall_driver.filter_defer_apply_on()
6206 
6207     def filter_defer_apply_off(self):
6208         self.firewall_driver.filter_defer_apply_off()
6209 
6210     def live_migration(self, context, instance, dest,
6211                        post_method, recover_method, block_migration=False,
6212                        migrate_data=None):
6213         """Spawning live_migration operation for distributing high-load.
6214 
6215         :param context: security context
6216         :param instance:
6217             nova.db.sqlalchemy.models.Instance object
6218             instance object that is migrated.
6219         :param dest: destination host
6220         :param post_method:
6221             post operation method.
6222             expected nova.compute.manager._post_live_migration.
6223         :param recover_method:
6224             recovery method when any exception occurs.
6225             expected nova.compute.manager._rollback_live_migration.
6226         :param block_migration: if true, do block migration.
6227         :param migrate_data: a LibvirtLiveMigrateData object
6228 
6229         """
6230 
6231         # 'dest' will be substituted into 'migration_uri' so ensure
6232         # it does't contain any characters that could be used to
6233         # exploit the URI accepted by libivrt
6234         if not libvirt_utils.is_valid_hostname(dest):
6235             raise exception.InvalidHostname(hostname=dest)
6236 
6237         self._live_migration(context, instance, dest,
6238                              post_method, recover_method, block_migration,
6239                              migrate_data)
6240 
6241     def live_migration_abort(self, instance):
6242         """Aborting a running live-migration.
6243 
6244         :param instance: instance object that is in migration
6245 
6246         """
6247 
6248         guest = self._host.get_guest(instance)
6249         dom = guest._domain
6250 
6251         try:
6252             dom.abortJob()
6253         except libvirt.libvirtError as e:
6254             LOG.error("Failed to cancel migration %s", e, instance=instance)
6255             raise
6256 
6257     def _verify_serial_console_is_disabled(self):
6258         if CONF.serial_console.enabled:
6259 
6260             msg = _('Your destination node does not support'
6261                     ' retrieving listen addresses.  In order'
6262                     ' for live migration to work properly you'
6263                     ' must disable serial console.')
6264             raise exception.MigrationError(reason=msg)
6265 
6266     def _live_migration_operation(self, context, instance, dest,
6267                                   block_migration, migrate_data, guest,
6268                                   device_names):
6269         """Invoke the live migration operation
6270 
6271         :param context: security context
6272         :param instance:
6273             nova.db.sqlalchemy.models.Instance object
6274             instance object that is migrated.
6275         :param dest: destination host
6276         :param block_migration: if true, do block migration.
6277         :param migrate_data: a LibvirtLiveMigrateData object
6278         :param guest: the guest domain object
6279         :param device_names: list of device names that are being migrated with
6280             instance
6281 
6282         This method is intended to be run in a background thread and will
6283         block that thread until the migration is finished or failed.
6284         """
6285         try:
6286             if migrate_data.block_migration:
6287                 migration_flags = self._block_migration_flags
6288             else:
6289                 migration_flags = self._live_migration_flags
6290 
6291             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6292                 migrate_data)
6293             if not serial_listen_addr:
6294                 # In this context we want to ensure that serial console is
6295                 # disabled on source node. This is because nova couldn't
6296                 # retrieve serial listen address from destination node, so we
6297                 # consider that destination node might have serial console
6298                 # disabled as well.
6299                 self._verify_serial_console_is_disabled()
6300 
6301             # NOTE(aplanas) migrate_uri will have a value only in the
6302             # case that `live_migration_inbound_addr` parameter is
6303             # set, and we propose a non tunneled migration.
6304             migrate_uri = None
6305             if ('target_connect_addr' in migrate_data and
6306                     migrate_data.target_connect_addr is not None):
6307                 dest = migrate_data.target_connect_addr
6308                 if (migration_flags &
6309                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6310                     migrate_uri = self._migrate_uri(dest)
6311 
6312             params = None
6313             new_xml_str = None
6314             if CONF.libvirt.virt_type != "parallels":
6315                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
6316                     # TODO(sahid): It's not a really good idea to pass
6317                     # the method _get_volume_config and we should to find
6318                     # a way to avoid this in future.
6319                     guest, migrate_data, self._get_volume_config)
6320             if self._host.has_min_version(
6321                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6322                 params = {
6323                     'destination_xml': new_xml_str,
6324                     'migrate_disks': device_names,
6325                 }
6326                 # NOTE(pkoniszewski): Because of precheck which blocks
6327                 # tunnelled block live migration with mapped volumes we
6328                 # can safely remove migrate_disks when tunnelling is on.
6329                 # Otherwise we will block all tunnelled block migrations,
6330                 # even when an instance does not have volumes mapped.
6331                 # This is because selective disk migration is not
6332                 # supported in tunnelled block live migration. Also we
6333                 # cannot fallback to migrateToURI2 in this case because of
6334                 # bug #1398999
6335                 if (migration_flags &
6336                     libvirt.VIR_MIGRATE_TUNNELLED != 0):
6337                     params.pop('migrate_disks')
6338 
6339             # TODO(sahid): This should be in
6340             # post_live_migration_at_source but no way to retrieve
6341             # ports acquired on the host for the guest at this
6342             # step. Since the domain is going to be removed from
6343             # libvird on source host after migration, we backup the
6344             # serial ports to release them if all went well.
6345             serial_ports = []
6346             if CONF.serial_console.enabled:
6347                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6348 
6349             guest.migrate(self._live_migration_uri(dest),
6350                           migrate_uri=migrate_uri,
6351                           flags=migration_flags,
6352                           params=params,
6353                           domain_xml=new_xml_str,
6354                           bandwidth=CONF.libvirt.live_migration_bandwidth)
6355 
6356             for hostname, port in serial_ports:
6357                 serial_console.release_port(host=hostname, port=port)
6358         except Exception as e:
6359             with excutils.save_and_reraise_exception():
6360                 LOG.error("Live Migration failure: %s", e, instance=instance)
6361 
6362         # If 'migrateToURI' fails we don't know what state the
6363         # VM instances on each host are in. Possibilities include
6364         #
6365         #  1. src==running, dst==none
6366         #
6367         #     Migration failed & rolled back, or never started
6368         #
6369         #  2. src==running, dst==paused
6370         #
6371         #     Migration started but is still ongoing
6372         #
6373         #  3. src==paused,  dst==paused
6374         #
6375         #     Migration data transfer completed, but switchover
6376         #     is still ongoing, or failed
6377         #
6378         #  4. src==paused,  dst==running
6379         #
6380         #     Migration data transfer completed, switchover
6381         #     happened but cleanup on source failed
6382         #
6383         #  5. src==none,    dst==running
6384         #
6385         #     Migration fully succeeded.
6386         #
6387         # Libvirt will aim to complete any migration operation
6388         # or roll it back. So even if the migrateToURI call has
6389         # returned an error, if the migration was not finished
6390         # libvirt should clean up.
6391         #
6392         # So we take the error raise here with a pinch of salt
6393         # and rely on the domain job info status to figure out
6394         # what really happened to the VM, which is a much more
6395         # reliable indicator.
6396         #
6397         # In particular we need to try very hard to ensure that
6398         # Nova does not "forget" about the guest. ie leaving it
6399         # running on a different host to the one recorded in
6400         # the database, as that would be a serious resource leak
6401 
6402         LOG.debug("Migration operation thread has finished",
6403                   instance=instance)
6404 
6405     def _live_migration_copy_disk_paths(self, context, instance, guest):
6406         '''Get list of disks to copy during migration
6407 
6408         :param context: security context
6409         :param instance: the instance being migrated
6410         :param guest: the Guest instance being migrated
6411 
6412         Get the list of disks to copy during migration.
6413 
6414         :returns: a list of local source paths and a list of device names to
6415             copy
6416         '''
6417 
6418         disk_paths = []
6419         device_names = []
6420         block_devices = []
6421 
6422         # TODO(pkoniszewski): Remove version check when we bump min libvirt
6423         # version to >= 1.2.17.
6424         if (self._block_migration_flags &
6425                 libvirt.VIR_MIGRATE_TUNNELLED == 0 and
6426                 self._host.has_min_version(
6427                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION)):
6428             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
6429                 context, instance.uuid)
6430             block_device_info = driver.get_block_device_info(instance,
6431                                                              bdm_list)
6432 
6433             block_device_mappings = driver.block_device_info_get_mapping(
6434                 block_device_info)
6435             for bdm in block_device_mappings:
6436                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
6437                 block_devices.append(device_name)
6438 
6439         for dev in guest.get_all_disks():
6440             if dev.readonly or dev.shareable:
6441                 continue
6442             if dev.source_type not in ["file", "block"]:
6443                 continue
6444             if dev.target_dev in block_devices:
6445                 continue
6446             disk_paths.append(dev.source_path)
6447             device_names.append(dev.target_dev)
6448         return (disk_paths, device_names)
6449 
6450     def _live_migration_data_gb(self, instance, disk_paths):
6451         '''Calculate total amount of data to be transferred
6452 
6453         :param instance: the nova.objects.Instance being migrated
6454         :param disk_paths: list of disk paths that are being migrated
6455         with instance
6456 
6457         Calculates the total amount of data that needs to be
6458         transferred during the live migration. The actual
6459         amount copied will be larger than this, due to the
6460         guest OS continuing to dirty RAM while the migration
6461         is taking place. So this value represents the minimal
6462         data size possible.
6463 
6464         :returns: data size to be copied in GB
6465         '''
6466 
6467         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
6468         if ram_gb < 2:
6469             ram_gb = 2
6470 
6471         disk_gb = 0
6472         for path in disk_paths:
6473             try:
6474                 size = os.stat(path).st_size
6475                 size_gb = (size / units.Gi)
6476                 if size_gb < 2:
6477                     size_gb = 2
6478                 disk_gb += size_gb
6479             except OSError as e:
6480                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
6481                             {'disk': path, 'ex': e})
6482                 # Ignore error since we don't want to break
6483                 # the migration monitoring thread operation
6484 
6485         return ram_gb + disk_gb
6486 
6487     def _get_migration_flags(self, is_block_migration):
6488         if is_block_migration:
6489             return self._block_migration_flags
6490         return self._live_migration_flags
6491 
6492     def _live_migration_monitor(self, context, instance, guest,
6493                                 dest, post_method,
6494                                 recover_method, block_migration,
6495                                 migrate_data, finish_event,
6496                                 disk_paths):
6497         on_migration_failure = deque()
6498         data_gb = self._live_migration_data_gb(instance, disk_paths)
6499         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
6500         migration = migrate_data.migration
6501         curdowntime = None
6502 
6503         migration_flags = self._get_migration_flags(
6504                                   migrate_data.block_migration)
6505 
6506         n = 0
6507         start = time.time()
6508         progress_time = start
6509         progress_watermark = None
6510         previous_data_remaining = -1
6511         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
6512         while True:
6513             info = guest.get_job_info()
6514 
6515             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6516                 # Either still running, or failed or completed,
6517                 # lets untangle the mess
6518                 if not finish_event.ready():
6519                     LOG.debug("Operation thread is still running",
6520                               instance=instance)
6521                 else:
6522                     info.type = libvirt_migrate.find_job_type(guest, instance)
6523                     LOG.debug("Fixed incorrect job type to be %d",
6524                               info.type, instance=instance)
6525 
6526             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6527                 # Migration is not yet started
6528                 LOG.debug("Migration not running yet",
6529                           instance=instance)
6530             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
6531                 # Migration is still running
6532                 #
6533                 # This is where we wire up calls to change live
6534                 # migration status. eg change max downtime, cancel
6535                 # the operation, change max bandwidth
6536                 libvirt_migrate.run_tasks(guest, instance,
6537                                           self.active_migrations,
6538                                           on_migration_failure,
6539                                           migration,
6540                                           is_post_copy_enabled)
6541 
6542                 now = time.time()
6543                 elapsed = now - start
6544 
6545                 if ((progress_watermark is None) or
6546                     (progress_watermark == 0) or
6547                     (progress_watermark > info.data_remaining)):
6548                     progress_watermark = info.data_remaining
6549                     progress_time = now
6550 
6551                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
6552                 completion_timeout = int(
6553                     CONF.libvirt.live_migration_completion_timeout * data_gb)
6554                 if libvirt_migrate.should_abort(instance, now, progress_time,
6555                                                 progress_timeout, elapsed,
6556                                                 completion_timeout,
6557                                                 migration.status):
6558                     try:
6559                         guest.abort_job()
6560                     except libvirt.libvirtError as e:
6561                         LOG.warning("Failed to abort migration %s",
6562                                     e, instance=instance)
6563                         self._clear_empty_migration(instance)
6564                         raise
6565 
6566                 if (is_post_copy_enabled and
6567                     libvirt_migrate.should_switch_to_postcopy(
6568                     info.memory_iteration, info.data_remaining,
6569                     previous_data_remaining, migration.status)):
6570                     libvirt_migrate.trigger_postcopy_switch(guest,
6571                                                             instance,
6572                                                             migration)
6573                 previous_data_remaining = info.data_remaining
6574 
6575                 curdowntime = libvirt_migrate.update_downtime(
6576                     guest, instance, curdowntime,
6577                     downtime_steps, elapsed)
6578 
6579                 # We loop every 500ms, so don't log on every
6580                 # iteration to avoid spamming logs for long
6581                 # running migrations. Just once every 5 secs
6582                 # is sufficient for developers to debug problems.
6583                 # We log once every 30 seconds at info to help
6584                 # admins see slow running migration operations
6585                 # when debug logs are off.
6586                 if (n % 10) == 0:
6587                     # Ignoring memory_processed, as due to repeated
6588                     # dirtying of data, this can be way larger than
6589                     # memory_total. Best to just look at what's
6590                     # remaining to copy and ignore what's done already
6591                     #
6592                     # TODO(berrange) perhaps we could include disk
6593                     # transfer stats in the progress too, but it
6594                     # might make memory info more obscure as large
6595                     # disk sizes might dwarf memory size
6596                     remaining = 100
6597                     if info.memory_total != 0:
6598                         remaining = round(info.memory_remaining *
6599                                           100 / info.memory_total)
6600 
6601                     libvirt_migrate.save_stats(instance, migration,
6602                                                info, remaining)
6603 
6604                     lg = LOG.debug
6605                     if (n % 60) == 0:
6606                         lg = LOG.info
6607 
6608                     lg("Migration running for %(secs)d secs, "
6609                        "memory %(remaining)d%% remaining; "
6610                        "(bytes processed=%(processed_memory)d, "
6611                        "remaining=%(remaining_memory)d, "
6612                        "total=%(total_memory)d)",
6613                        {"secs": n / 2, "remaining": remaining,
6614                         "processed_memory": info.memory_processed,
6615                         "remaining_memory": info.memory_remaining,
6616                         "total_memory": info.memory_total}, instance=instance)
6617                     if info.data_remaining > progress_watermark:
6618                         lg("Data remaining %(remaining)d bytes, "
6619                            "low watermark %(watermark)d bytes "
6620                            "%(last)d seconds ago",
6621                            {"remaining": info.data_remaining,
6622                             "watermark": progress_watermark,
6623                             "last": (now - progress_time)}, instance=instance)
6624 
6625                 n = n + 1
6626             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
6627                 # Migration is all done
6628                 LOG.info("Migration operation has completed",
6629                          instance=instance)
6630                 post_method(context, instance, dest, block_migration,
6631                             migrate_data)
6632                 break
6633             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
6634                 # Migration did not succeed
6635                 LOG.error("Migration operation has aborted", instance=instance)
6636                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6637                                                   on_migration_failure)
6638                 recover_method(context, instance, dest, migrate_data)
6639                 break
6640             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
6641                 # Migration was stopped by admin
6642                 LOG.warning("Migration operation was cancelled",
6643                             instance=instance)
6644                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6645                                                   on_migration_failure)
6646                 recover_method(context, instance, dest, migrate_data,
6647                                migration_status='cancelled')
6648                 break
6649             else:
6650                 LOG.warning("Unexpected migration job type: %d",
6651                             info.type, instance=instance)
6652 
6653             time.sleep(0.5)
6654         self._clear_empty_migration(instance)
6655 
6656     def _clear_empty_migration(self, instance):
6657         try:
6658             del self.active_migrations[instance.uuid]
6659         except KeyError:
6660             LOG.warning("There are no records in active migrations "
6661                         "for instance", instance=instance)
6662 
6663     def _live_migration(self, context, instance, dest, post_method,
6664                         recover_method, block_migration,
6665                         migrate_data):
6666         """Do live migration.
6667 
6668         :param context: security context
6669         :param instance:
6670             nova.db.sqlalchemy.models.Instance object
6671             instance object that is migrated.
6672         :param dest: destination host
6673         :param post_method:
6674             post operation method.
6675             expected nova.compute.manager._post_live_migration.
6676         :param recover_method:
6677             recovery method when any exception occurs.
6678             expected nova.compute.manager._rollback_live_migration.
6679         :param block_migration: if true, do block migration.
6680         :param migrate_data: a LibvirtLiveMigrateData object
6681 
6682         This fires off a new thread to run the blocking migration
6683         operation, and then this thread monitors the progress of
6684         migration and controls its operation
6685         """
6686 
6687         guest = self._host.get_guest(instance)
6688 
6689         disk_paths = []
6690         device_names = []
6691         if (migrate_data.block_migration and
6692                 CONF.libvirt.virt_type != "parallels"):
6693             disk_paths, device_names = self._live_migration_copy_disk_paths(
6694                 context, instance, guest)
6695 
6696         opthread = utils.spawn(self._live_migration_operation,
6697                                      context, instance, dest,
6698                                      block_migration,
6699                                      migrate_data, guest,
6700                                      device_names)
6701 
6702         finish_event = eventlet.event.Event()
6703         self.active_migrations[instance.uuid] = deque()
6704 
6705         def thread_finished(thread, event):
6706             LOG.debug("Migration operation thread notification",
6707                       instance=instance)
6708             event.send()
6709         opthread.link(thread_finished, finish_event)
6710 
6711         # Let eventlet schedule the new thread right away
6712         time.sleep(0)
6713 
6714         try:
6715             LOG.debug("Starting monitoring of live migration",
6716                       instance=instance)
6717             self._live_migration_monitor(context, instance, guest, dest,
6718                                          post_method, recover_method,
6719                                          block_migration, migrate_data,
6720                                          finish_event, disk_paths)
6721         except Exception as ex:
6722             LOG.warning("Error monitoring migration: %(ex)s",
6723                         {"ex": ex}, instance=instance, exc_info=True)
6724             raise
6725         finally:
6726             LOG.debug("Live migration monitoring is all done",
6727                       instance=instance)
6728 
6729     def _is_post_copy_enabled(self, migration_flags):
6730         if self._is_post_copy_available():
6731             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
6732                 return True
6733         return False
6734 
6735     def live_migration_force_complete(self, instance):
6736         try:
6737             self.active_migrations[instance.uuid].append('force-complete')
6738         except KeyError:
6739             raise exception.NoActiveMigrationForInstance(
6740                 instance_id=instance.uuid)
6741 
6742     def _try_fetch_image(self, context, path, image_id, instance,
6743                          fallback_from_host=None):
6744         try:
6745             libvirt_utils.fetch_image(context, path, image_id)
6746         except exception.ImageNotFound:
6747             if not fallback_from_host:
6748                 raise
6749             LOG.debug("Image %(image_id)s doesn't exist anymore on "
6750                       "image service, attempting to copy image "
6751                       "from %(host)s",
6752                       {'image_id': image_id, 'host': fallback_from_host})
6753             libvirt_utils.copy_image(src=path, dest=path,
6754                                      host=fallback_from_host,
6755                                      receive=True)
6756 
6757     def _fetch_instance_kernel_ramdisk(self, context, instance,
6758                                        fallback_from_host=None):
6759         """Download kernel and ramdisk for instance in instance directory."""
6760         instance_dir = libvirt_utils.get_instance_path(instance)
6761         if instance.kernel_id:
6762             kernel_path = os.path.join(instance_dir, 'kernel')
6763             # NOTE(dsanders): only fetch image if it's not available at
6764             # kernel_path. This also avoids ImageNotFound exception if
6765             # the image has been deleted from glance
6766             if not os.path.exists(kernel_path):
6767                 self._try_fetch_image(context,
6768                                       kernel_path,
6769                                       instance.kernel_id,
6770                                       instance, fallback_from_host)
6771             if instance.ramdisk_id:
6772                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
6773                 # NOTE(dsanders): only fetch image if it's not available at
6774                 # ramdisk_path. This also avoids ImageNotFound exception if
6775                 # the image has been deleted from glance
6776                 if not os.path.exists(ramdisk_path):
6777                     self._try_fetch_image(context,
6778                                           ramdisk_path,
6779                                           instance.ramdisk_id,
6780                                           instance, fallback_from_host)
6781 
6782     def rollback_live_migration_at_destination(self, context, instance,
6783                                                network_info,
6784                                                block_device_info,
6785                                                destroy_disks=True,
6786                                                migrate_data=None):
6787         """Clean up destination node after a failed live migration."""
6788         try:
6789             self.destroy(context, instance, network_info, block_device_info,
6790                          destroy_disks)
6791         finally:
6792             # NOTE(gcb): Failed block live migration may leave instance
6793             # directory at destination node, ensure it is always deleted.
6794             is_shared_instance_path = True
6795             if migrate_data:
6796                 is_shared_instance_path = migrate_data.is_shared_instance_path
6797                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
6798                     and migrate_data.serial_listen_ports):
6799                     # Releases serial ports reserved.
6800                     for port in migrate_data.serial_listen_ports:
6801                         serial_console.release_port(
6802                             host=migrate_data.serial_listen_addr, port=port)
6803 
6804             if not is_shared_instance_path:
6805                 instance_dir = libvirt_utils.get_instance_path_at_destination(
6806                     instance, migrate_data)
6807                 if os.path.exists(instance_dir):
6808                     shutil.rmtree(instance_dir)
6809 
6810     def pre_live_migration(self, context, instance, block_device_info,
6811                            network_info, disk_info, migrate_data):
6812         """Preparation live migration."""
6813         if disk_info is not None:
6814             disk_info = jsonutils.loads(disk_info)
6815 
6816         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
6817                   instance=instance)
6818         is_shared_block_storage = migrate_data.is_shared_block_storage
6819         is_shared_instance_path = migrate_data.is_shared_instance_path
6820         is_block_migration = migrate_data.block_migration
6821 
6822         if not is_shared_instance_path:
6823             instance_dir = libvirt_utils.get_instance_path_at_destination(
6824                             instance, migrate_data)
6825 
6826             if os.path.exists(instance_dir):
6827                 raise exception.DestinationDiskExists(path=instance_dir)
6828 
6829             LOG.debug('Creating instance directory: %s', instance_dir,
6830                       instance=instance)
6831             os.mkdir(instance_dir)
6832 
6833             # Recreate the disk.info file and in doing so stop the
6834             # imagebackend from recreating it incorrectly by inspecting the
6835             # contents of each file when using the Raw backend.
6836             if disk_info:
6837                 image_disk_info = {}
6838                 for info in disk_info:
6839                     image_file = os.path.basename(info['path'])
6840                     image_path = os.path.join(instance_dir, image_file)
6841                     image_disk_info[image_path] = info['type']
6842 
6843                 LOG.debug('Creating disk.info with the contents: %s',
6844                           image_disk_info, instance=instance)
6845 
6846                 image_disk_info_path = os.path.join(instance_dir,
6847                                                     'disk.info')
6848                 libvirt_utils.write_to_file(image_disk_info_path,
6849                                             jsonutils.dumps(image_disk_info))
6850 
6851             if not is_shared_block_storage:
6852                 # Ensure images and backing files are present.
6853                 LOG.debug('Checking to make sure images and backing files are '
6854                           'present before live migration.', instance=instance)
6855                 self._create_images_and_backing(
6856                     context, instance, instance_dir, disk_info,
6857                     fallback_from_host=instance.host)
6858                 if (configdrive.required_by(instance) and
6859                         CONF.config_drive_format == 'iso9660'):
6860                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
6861                     # drive needs to be copied to destination prior to
6862                     # migration when instance path is not shared and block
6863                     # storage is not shared. Files that are already present
6864                     # on destination are excluded from a list of files that
6865                     # need to be copied to destination. If we don't do that
6866                     # live migration will fail on copying iso config drive to
6867                     # destination and writing to read-only device.
6868                     # Please see bug/1246201 for more details.
6869                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
6870                     self._remotefs.copy_file(src, instance_dir)
6871 
6872             if not is_block_migration:
6873                 # NOTE(angdraug): when block storage is shared between source
6874                 # and destination and instance path isn't (e.g. volume backed
6875                 # or rbd backed instance), instance path on destination has to
6876                 # be prepared
6877 
6878                 # Required by Quobyte CI
6879                 self._ensure_console_log_for_instance(instance)
6880 
6881                 # if image has kernel and ramdisk, just download
6882                 # following normal way.
6883                 self._fetch_instance_kernel_ramdisk(context, instance)
6884 
6885         # Establishing connection to volume server.
6886         block_device_mapping = driver.block_device_info_get_mapping(
6887             block_device_info)
6888 
6889         if len(block_device_mapping):
6890             LOG.debug('Connecting volumes before live migration.',
6891                       instance=instance)
6892 
6893         for bdm in block_device_mapping:
6894             connection_info = bdm['connection_info']
6895             disk_info = blockinfo.get_info_from_bdm(
6896                 instance, CONF.libvirt.virt_type,
6897                 instance.image_meta, bdm)
6898             self._connect_volume(connection_info, disk_info, instance)
6899 
6900         # We call plug_vifs before the compute manager calls
6901         # ensure_filtering_rules_for_instance, to ensure bridge is set up
6902         # Retry operation is necessary because continuously request comes,
6903         # concurrent request occurs to iptables, then it complains.
6904         LOG.debug('Plugging VIFs before live migration.', instance=instance)
6905         max_retry = CONF.live_migration_retry_count
6906         for cnt in range(max_retry):
6907             try:
6908                 self.plug_vifs(instance, network_info)
6909                 break
6910             except processutils.ProcessExecutionError:
6911                 if cnt == max_retry - 1:
6912                     raise
6913                 else:
6914                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
6915                                 '%(max_retry)d.',
6916                                 {'cnt': cnt, 'max_retry': max_retry},
6917                                 instance=instance)
6918                     greenthread.sleep(1)
6919 
6920         # Store server_listen and latest disk device info
6921         if not migrate_data:
6922             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
6923         else:
6924             migrate_data.bdms = []
6925         # Store live_migration_inbound_addr
6926         migrate_data.target_connect_addr = \
6927             CONF.libvirt.live_migration_inbound_addr
6928         migrate_data.supported_perf_events = self._supported_perf_events
6929 
6930         migrate_data.serial_listen_ports = []
6931         if CONF.serial_console.enabled:
6932             num_ports = hardware.get_number_of_serial_ports(
6933                 instance.flavor, instance.image_meta)
6934             for port in six.moves.range(num_ports):
6935                 migrate_data.serial_listen_ports.append(
6936                     serial_console.acquire_port(
6937                         migrate_data.serial_listen_addr))
6938 
6939         for vol in block_device_mapping:
6940             connection_info = vol['connection_info']
6941             if connection_info.get('serial'):
6942                 disk_info = blockinfo.get_info_from_bdm(
6943                     instance, CONF.libvirt.virt_type,
6944                     instance.image_meta, vol)
6945 
6946                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
6947                 bdmi.serial = connection_info['serial']
6948                 bdmi.connection_info = connection_info
6949                 bdmi.bus = disk_info['bus']
6950                 bdmi.dev = disk_info['dev']
6951                 bdmi.type = disk_info['type']
6952                 bdmi.format = disk_info.get('format')
6953                 bdmi.boot_index = disk_info.get('boot_index')
6954                 migrate_data.bdms.append(bdmi)
6955 
6956         return migrate_data
6957 
6958     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
6959                                image_id, instance, size,
6960                                fallback_from_host=None):
6961         try:
6962             image.cache(fetch_func=fetch_func,
6963                         context=context,
6964                         filename=filename,
6965                         image_id=image_id,
6966                         size=size)
6967         except exception.ImageNotFound:
6968             if not fallback_from_host:
6969                 raise
6970             LOG.debug("Image %(image_id)s doesn't exist anymore "
6971                       "on image service, attempting to copy "
6972                       "image from %(host)s",
6973                       {'image_id': image_id, 'host': fallback_from_host},
6974                       instance=instance)
6975 
6976             def copy_from_host(target):
6977                 libvirt_utils.copy_image(src=target,
6978                                          dest=target,
6979                                          host=fallback_from_host,
6980                                          receive=True)
6981             image.cache(fetch_func=copy_from_host,
6982                         filename=filename)
6983 
6984     def _create_images_and_backing(self, context, instance, instance_dir,
6985                                    disk_info, fallback_from_host=None):
6986         """:param context: security context
6987            :param instance:
6988                nova.db.sqlalchemy.models.Instance object
6989                instance object that is migrated.
6990            :param instance_dir:
6991                instance path to use, calculated externally to handle block
6992                migrating an instance with an old style instance path
6993            :param disk_info:
6994                disk info specified in _get_instance_disk_info_from_config
6995                (list of dicts)
6996            :param fallback_from_host:
6997                host where we can retrieve images if the glance images are
6998                not available.
6999 
7000         """
7001 
7002         # Virtuozzo containers don't use backing file
7003         if (CONF.libvirt.virt_type == "parallels" and
7004                 instance.vm_mode == fields.VMMode.EXE):
7005             return
7006 
7007         if not disk_info:
7008             disk_info = []
7009 
7010         for info in disk_info:
7011             base = os.path.basename(info['path'])
7012             # Get image type and create empty disk image, and
7013             # create backing file in case of qcow2.
7014             instance_disk = os.path.join(instance_dir, base)
7015             if not info['backing_file'] and not os.path.exists(instance_disk):
7016                 libvirt_utils.create_image(info['type'], instance_disk,
7017                                            info['virt_disk_size'])
7018             elif info['backing_file']:
7019                 # Creating backing file follows same way as spawning instances.
7020                 cache_name = os.path.basename(info['backing_file'])
7021 
7022                 disk = self.image_backend.by_name(instance, instance_disk,
7023                                                   CONF.libvirt.images_type)
7024                 if cache_name.startswith('ephemeral'):
7025                     # The argument 'size' is used by image.cache to
7026                     # validate disk size retrieved from cache against
7027                     # the instance disk size (should always return OK)
7028                     # and ephemeral_size is used by _create_ephemeral
7029                     # to build the image if the disk is not already
7030                     # cached.
7031                     disk.cache(
7032                         fetch_func=self._create_ephemeral,
7033                         fs_label=cache_name,
7034                         os_type=instance.os_type,
7035                         filename=cache_name,
7036                         size=info['virt_disk_size'],
7037                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7038                 elif cache_name.startswith('swap'):
7039                     inst_type = instance.get_flavor()
7040                     swap_mb = inst_type.swap
7041                     disk.cache(fetch_func=self._create_swap,
7042                                 filename="swap_%s" % swap_mb,
7043                                 size=swap_mb * units.Mi,
7044                                 swap_mb=swap_mb)
7045                 else:
7046                     self._try_fetch_image_cache(disk,
7047                                                 libvirt_utils.fetch_image,
7048                                                 context, cache_name,
7049                                                 instance.image_ref,
7050                                                 instance,
7051                                                 info['virt_disk_size'],
7052                                                 fallback_from_host)
7053 
7054         # if disk has kernel and ramdisk, just download
7055         # following normal way.
7056         self._fetch_instance_kernel_ramdisk(
7057             context, instance, fallback_from_host=fallback_from_host)
7058 
7059     def post_live_migration(self, context, instance, block_device_info,
7060                             migrate_data=None):
7061         # Disconnect from volume server
7062         block_device_mapping = driver.block_device_info_get_mapping(
7063                 block_device_info)
7064         connector = self.get_volume_connector(instance)
7065         volume_api = self._volume_api
7066         for vol in block_device_mapping:
7067             # Retrieve connection info from Cinder's initialize_connection API.
7068             # The info returned will be accurate for the source server.
7069             volume_id = vol['connection_info']['serial']
7070             connection_info = volume_api.initialize_connection(context,
7071                                                                volume_id,
7072                                                                connector)
7073 
7074             # TODO(leeantho) The following multipath_id logic is temporary
7075             # and will be removed in the future once os-brick is updated
7076             # to handle multipath for drivers in a more efficient way.
7077             # For now this logic is needed to ensure the connection info
7078             # data is correct.
7079 
7080             # Pull out multipath_id from the bdm information. The
7081             # multipath_id can be placed into the connection info
7082             # because it is based off of the volume and will be the
7083             # same on the source and destination hosts.
7084             if 'multipath_id' in vol['connection_info']['data']:
7085                 multipath_id = vol['connection_info']['data']['multipath_id']
7086                 connection_info['data']['multipath_id'] = multipath_id
7087 
7088             disk_dev = vol['mount_device'].rpartition("/")[2]
7089             self._disconnect_volume(connection_info, disk_dev, instance)
7090 
7091     def post_live_migration_at_source(self, context, instance, network_info):
7092         """Unplug VIFs from networks at source.
7093 
7094         :param context: security context
7095         :param instance: instance object reference
7096         :param network_info: instance network information
7097         """
7098         self.unplug_vifs(instance, network_info)
7099 
7100     def post_live_migration_at_destination(self, context,
7101                                            instance,
7102                                            network_info,
7103                                            block_migration=False,
7104                                            block_device_info=None):
7105         """Post operation of live migration at destination host.
7106 
7107         :param context: security context
7108         :param instance:
7109             nova.db.sqlalchemy.models.Instance object
7110             instance object that is migrated.
7111         :param network_info: instance network information
7112         :param block_migration: if true, post operation of block_migration.
7113         """
7114         guest = self._host.get_guest(instance)
7115 
7116         # TODO(sahid): In Ocata we have added the migration flag
7117         # VIR_MIGRATE_PERSIST_DEST to libvirt, which means that the
7118         # guest XML is going to be set in libvirtd on destination node
7119         # automatically. However we do not remove that part until P*
7120         # because during an upgrade, to ensure migrating instances
7121         # from node running Newton is still going to set the guest XML
7122         # in libvirtd on destination node.
7123 
7124         # Make sure we define the migrated instance in libvirt
7125         xml = guest.get_xml_desc()
7126         self._host.write_instance_config(xml)
7127 
7128     def _get_instance_disk_info_from_config(self, guest_config,
7129                                             block_device_info):
7130         """Get the non-volume disk information from the domain xml
7131 
7132         :param LibvirtConfigGuest guest_config: the libvirt domain config
7133                                                 for the instance
7134         :param dict block_device_info: block device info for BDMs
7135         :returns disk_info: list of dicts with keys:
7136 
7137           * 'type': the disk type (str)
7138           * 'path': the disk path (str)
7139           * 'virt_disk_size': the virtual disk size (int)
7140           * 'backing_file': backing file of a disk image (str)
7141           * 'disk_size': physical disk size (int)
7142           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7143         """
7144         block_device_mapping = driver.block_device_info_get_mapping(
7145             block_device_info)
7146 
7147         volume_devices = set()
7148         for vol in block_device_mapping:
7149             disk_dev = vol['mount_device'].rpartition("/")[2]
7150             volume_devices.add(disk_dev)
7151 
7152         disk_info = []
7153 
7154         if (guest_config.virt_type == 'parallels' and
7155                 guest_config.os_type == fields.VMMode.EXE):
7156             node_type = 'filesystem'
7157         else:
7158             node_type = 'disk'
7159 
7160         for device in guest_config.devices:
7161             if device.root_name != node_type:
7162                 continue
7163             disk_type = device.source_type
7164             if device.root_name == 'filesystem':
7165                 target = device.target_dir
7166                 if device.source_type == 'file':
7167                     path = device.source_file
7168                 elif device.source_type == 'block':
7169                     path = device.source_dev
7170                 else:
7171                     path = None
7172             else:
7173                 target = device.target_dev
7174                 path = device.source_path
7175 
7176             if not path:
7177                 LOG.debug('skipping disk for %s as it does not have a path',
7178                           guest_config.name)
7179                 continue
7180 
7181             if disk_type not in ['file', 'block']:
7182                 LOG.debug('skipping disk because it looks like a volume', path)
7183                 continue
7184 
7185             if target in volume_devices:
7186                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
7187                           'volume', {'path': path, 'target': target})
7188                 continue
7189 
7190             if device.root_name == 'filesystem':
7191                 driver_type = device.driver_type
7192             else:
7193                 driver_type = device.driver_format
7194             # get the real disk size or
7195             # raise a localized error if image is unavailable
7196             if disk_type == 'file':
7197                 if driver_type == 'ploop':
7198                     dk_size = 0
7199                     for dirpath, dirnames, filenames in os.walk(path):
7200                         for f in filenames:
7201                             fp = os.path.join(dirpath, f)
7202                             dk_size += os.path.getsize(fp)
7203                 else:
7204                     dk_size = int(os.path.getsize(path))
7205             elif disk_type == 'block' and block_device_info:
7206                 dk_size = lvm.get_volume_size(path)
7207             else:
7208                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7209                           'determine if volume',
7210                           {'path': path, 'target': target})
7211                 continue
7212 
7213             if driver_type in ("qcow2", "ploop"):
7214                 backing_file = libvirt_utils.get_disk_backing_file(path)
7215                 virt_size = disk_api.get_disk_size(path)
7216                 over_commit_size = int(virt_size) - dk_size
7217             else:
7218                 backing_file = ""
7219                 virt_size = dk_size
7220                 over_commit_size = 0
7221 
7222             disk_info.append({'type': driver_type,
7223                               'path': path,
7224                               'virt_disk_size': virt_size,
7225                               'backing_file': backing_file,
7226                               'disk_size': dk_size,
7227                               'over_committed_disk_size': over_commit_size})
7228         return disk_info
7229 
7230     def _get_instance_disk_info(self, instance, block_device_info):
7231         try:
7232             guest = self._host.get_guest(instance)
7233             config = guest.get_config()
7234         except libvirt.libvirtError as ex:
7235             error_code = ex.get_error_code()
7236             LOG.warning('Error from libvirt while getting description of '
7237                         '%(instance_name)s: [Error Code %(error_code)s] '
7238                         '%(ex)s',
7239                         {'instance_name': instance.name,
7240                          'error_code': error_code,
7241                          'ex': ex},
7242                         instance=instance)
7243             raise exception.InstanceNotFound(instance_id=instance.uuid)
7244 
7245         return self._get_instance_disk_info_from_config(config,
7246                                                         block_device_info)
7247 
7248     def get_instance_disk_info(self, instance,
7249                                block_device_info=None):
7250         return jsonutils.dumps(
7251             self._get_instance_disk_info(instance, block_device_info))
7252 
7253     def _get_disk_over_committed_size_total(self):
7254         """Return total over committed disk size for all instances."""
7255         # Disk size that all instance uses : virtual_size - disk_size
7256         disk_over_committed_size = 0
7257         instance_domains = self._host.list_instance_domains(only_running=False)
7258         if not instance_domains:
7259             return disk_over_committed_size
7260 
7261         # Get all instance uuids
7262         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7263         ctx = nova_context.get_admin_context()
7264         # Get instance object list by uuid filter
7265         filters = {'uuid': instance_uuids}
7266         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7267         # getting called twice one is here and another in the
7268         # _update_available_resource method of resource_tracker. Since
7269         # _update_available_resource method is synchronized, there is a
7270         # possibility the instances list retrieved here to calculate
7271         # disk_over_committed_size would differ to the list you would get
7272         # in _update_available_resource method for calculating usages based
7273         # on instance utilization.
7274         local_instance_list = objects.InstanceList.get_by_filters(
7275             ctx, filters, use_slave=True)
7276         # Convert instance list to dictionary with instance uuid as key.
7277         local_instances = {inst.uuid: inst for inst in local_instance_list}
7278 
7279         # Get bdms by instance uuids
7280         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7281             ctx, instance_uuids)
7282 
7283         for dom in instance_domains:
7284             try:
7285                 guest = libvirt_guest.Guest(dom)
7286                 config = guest.get_config()
7287 
7288                 block_device_info = None
7289                 if guest.uuid in local_instances \
7290                         and (bdms and guest.uuid in bdms):
7291                     # Get block device info for instance
7292                     block_device_info = driver.get_block_device_info(
7293                         local_instances[guest.uuid], bdms[guest.uuid])
7294 
7295                 disk_infos = self._get_instance_disk_info_from_config(
7296                     config, block_device_info)
7297                 if not disk_infos:
7298                     continue
7299 
7300                 for info in disk_infos:
7301                     disk_over_committed_size += int(
7302                         info['over_committed_disk_size'])
7303             except libvirt.libvirtError as ex:
7304                 error_code = ex.get_error_code()
7305                 LOG.warning(
7306                     'Error from libvirt while getting description of '
7307                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
7308                     {'instance_name': guest.name,
7309                      'error_code': error_code,
7310                      'ex': ex})
7311             except OSError as e:
7312                 if e.errno in (errno.ENOENT, errno.ESTALE):
7313                     LOG.warning('Periodic task is updating the host stat, '
7314                                 'it is trying to get disk %(i_name)s, '
7315                                 'but disk file was removed by concurrent '
7316                                 'operations such as resize.',
7317                                 {'i_name': guest.name})
7318                 elif e.errno == errno.EACCES:
7319                     LOG.warning('Periodic task is updating the host stat, '
7320                                 'it is trying to get disk %(i_name)s, '
7321                                 'but access is denied. It is most likely '
7322                                 'due to a VM that exists on the compute '
7323                                 'node but is not managed by Nova.',
7324                                 {'i_name': guest.name})
7325                 else:
7326                     raise
7327             except exception.VolumeBDMPathNotFound as e:
7328                 LOG.warning('Periodic task is updating the host stats, '
7329                             'it is trying to get disk info for %(i_name)s, '
7330                             'but the backing volume block device was removed '
7331                             'by concurrent operations such as resize. '
7332                             'Error: %(error)s',
7333                             {'i_name': guest.name, 'error': e})
7334             # NOTE(gtt116): give other tasks a chance.
7335             greenthread.sleep(0)
7336         return disk_over_committed_size
7337 
7338     def unfilter_instance(self, instance, network_info):
7339         """See comments of same method in firewall_driver."""
7340         self.firewall_driver.unfilter_instance(instance,
7341                                                network_info=network_info)
7342 
7343     def get_available_nodes(self, refresh=False):
7344         return [self._host.get_hostname()]
7345 
7346     def get_host_cpu_stats(self):
7347         """Return the current CPU state of the host."""
7348         return self._host.get_cpu_stats()
7349 
7350     def get_host_uptime(self):
7351         """Returns the result of calling "uptime"."""
7352         out, err = utils.execute('env', 'LANG=C', 'uptime')
7353         return out
7354 
7355     def manage_image_cache(self, context, all_instances):
7356         """Manage the local cache of images."""
7357         self.image_cache_manager.update(context, all_instances)
7358 
7359     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
7360                                   shared_storage=False):
7361         """Used only for cleanup in case migrate_disk_and_power_off fails."""
7362         try:
7363             if os.path.exists(inst_base_resize):
7364                 utils.execute('rm', '-rf', inst_base)
7365                 utils.execute('mv', inst_base_resize, inst_base)
7366                 if not shared_storage:
7367                     self._remotefs.remove_dir(dest, inst_base)
7368         except Exception:
7369             pass
7370 
7371     def _is_storage_shared_with(self, dest, inst_base):
7372         # NOTE (rmk): There are two methods of determining whether we are
7373         #             on the same filesystem: the source and dest IP are the
7374         #             same, or we create a file on the dest system via SSH
7375         #             and check whether the source system can also see it.
7376         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
7377         #                it will always be shared storage
7378         if CONF.libvirt.images_type == 'rbd':
7379             return True
7380         shared_storage = (dest == self.get_host_ip_addr())
7381         if not shared_storage:
7382             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
7383             tmp_path = os.path.join(inst_base, tmp_file)
7384 
7385             try:
7386                 self._remotefs.create_file(dest, tmp_path)
7387                 if os.path.exists(tmp_path):
7388                     shared_storage = True
7389                     os.unlink(tmp_path)
7390                 else:
7391                     self._remotefs.remove_file(dest, tmp_path)
7392             except Exception:
7393                 pass
7394         return shared_storage
7395 
7396     def migrate_disk_and_power_off(self, context, instance, dest,
7397                                    flavor, network_info,
7398                                    block_device_info=None,
7399                                    timeout=0, retry_interval=0):
7400         LOG.debug("Starting migrate_disk_and_power_off",
7401                    instance=instance)
7402 
7403         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
7404 
7405         # get_bdm_ephemeral_disk_size() will return 0 if the new
7406         # instance's requested block device mapping contain no
7407         # ephemeral devices. However, we still want to check if
7408         # the original instance's ephemeral_gb property was set and
7409         # ensure that the new requested flavor ephemeral size is greater
7410         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
7411                     instance.flavor.ephemeral_gb)
7412 
7413         # Checks if the migration needs a disk resize down.
7414         root_down = flavor.root_gb < instance.flavor.root_gb
7415         ephemeral_down = flavor.ephemeral_gb < eph_size
7416         booted_from_volume = self._is_booted_from_volume(block_device_info)
7417 
7418         if (root_down and not booted_from_volume) or ephemeral_down:
7419             reason = _("Unable to resize disk down.")
7420             raise exception.InstanceFaultRollback(
7421                 exception.ResizeError(reason=reason))
7422 
7423         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
7424         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
7425             reason = _("Migration is not supported for LVM backed instances")
7426             raise exception.InstanceFaultRollback(
7427                 exception.MigrationPreCheckError(reason=reason))
7428 
7429         # copy disks to destination
7430         # rename instance dir to +_resize at first for using
7431         # shared storage for instance dir (eg. NFS).
7432         inst_base = libvirt_utils.get_instance_path(instance)
7433         inst_base_resize = inst_base + "_resize"
7434         shared_storage = self._is_storage_shared_with(dest, inst_base)
7435 
7436         # try to create the directory on the remote compute node
7437         # if this fails we pass the exception up the stack so we can catch
7438         # failures here earlier
7439         if not shared_storage:
7440             try:
7441                 self._remotefs.create_dir(dest, inst_base)
7442             except processutils.ProcessExecutionError as e:
7443                 reason = _("not able to execute ssh command: %s") % e
7444                 raise exception.InstanceFaultRollback(
7445                     exception.ResizeError(reason=reason))
7446 
7447         self.power_off(instance, timeout, retry_interval)
7448 
7449         block_device_mapping = driver.block_device_info_get_mapping(
7450             block_device_info)
7451         for vol in block_device_mapping:
7452             connection_info = vol['connection_info']
7453             disk_dev = vol['mount_device'].rpartition("/")[2]
7454             self._disconnect_volume(connection_info, disk_dev, instance)
7455 
7456         disk_info = self._get_instance_disk_info(instance, block_device_info)
7457 
7458         try:
7459             utils.execute('mv', inst_base, inst_base_resize)
7460             # if we are migrating the instance with shared storage then
7461             # create the directory.  If it is a remote node the directory
7462             # has already been created
7463             if shared_storage:
7464                 dest = None
7465                 fileutils.ensure_tree(inst_base)
7466 
7467             on_execute = lambda process: \
7468                 self.job_tracker.add_job(instance, process.pid)
7469             on_completion = lambda process: \
7470                 self.job_tracker.remove_job(instance, process.pid)
7471 
7472             for info in disk_info:
7473                 # assume inst_base == dirname(info['path'])
7474                 img_path = info['path']
7475                 fname = os.path.basename(img_path)
7476                 from_path = os.path.join(inst_base_resize, fname)
7477 
7478                 # We will not copy over the swap disk here, and rely on
7479                 # finish_migration to re-create it for us. This is ok because
7480                 # the OS is shut down, and as recreating a swap disk is very
7481                 # cheap it is more efficient than copying either locally or
7482                 # over the network. This also means we don't have to resize it.
7483                 if fname == 'disk.swap':
7484                     continue
7485 
7486                 compression = info['type'] not in NO_COMPRESSION_TYPES
7487                 libvirt_utils.copy_image(from_path, img_path, host=dest,
7488                                          on_execute=on_execute,
7489                                          on_completion=on_completion,
7490                                          compression=compression)
7491 
7492             # Ensure disk.info is written to the new path to avoid disks being
7493             # reinspected and potentially changing format.
7494             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
7495             if os.path.exists(src_disk_info_path):
7496                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
7497                 libvirt_utils.copy_image(src_disk_info_path,
7498                                          dst_disk_info_path,
7499                                          host=dest, on_execute=on_execute,
7500                                          on_completion=on_completion)
7501         except Exception:
7502             with excutils.save_and_reraise_exception():
7503                 self._cleanup_remote_migration(dest, inst_base,
7504                                                inst_base_resize,
7505                                                shared_storage)
7506 
7507         return jsonutils.dumps(disk_info)
7508 
7509     def _wait_for_running(self, instance):
7510         state = self.get_info(instance).state
7511 
7512         if state == power_state.RUNNING:
7513             LOG.info("Instance running successfully.", instance=instance)
7514             raise loopingcall.LoopingCallDone()
7515 
7516     @staticmethod
7517     def _disk_raw_to_qcow2(path):
7518         """Converts a raw disk to qcow2."""
7519         path_qcow = path + '_qcow'
7520         utils.execute('qemu-img', 'convert', '-f', 'raw',
7521                       '-O', 'qcow2', path, path_qcow)
7522         utils.execute('mv', path_qcow, path)
7523 
7524     @staticmethod
7525     def _disk_qcow2_to_raw(path):
7526         """Converts a qcow2 disk to raw."""
7527         path_raw = path + '_raw'
7528         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
7529                       '-O', 'raw', path, path_raw)
7530         utils.execute('mv', path_raw, path)
7531 
7532     def finish_migration(self, context, migration, instance, disk_info,
7533                          network_info, image_meta, resize_instance,
7534                          block_device_info=None, power_on=True):
7535         LOG.debug("Starting finish_migration", instance=instance)
7536 
7537         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7538                                                   instance,
7539                                                   image_meta,
7540                                                   block_device_info)
7541         # assume _create_image does nothing if a target file exists.
7542         # NOTE: This has the intended side-effect of fetching a missing
7543         # backing file.
7544         self._create_image(context, instance, block_disk_info['mapping'],
7545                            block_device_info=block_device_info,
7546                            ignore_bdi_for_swap=True,
7547                            fallback_from_host=migration.source_compute)
7548 
7549         # Required by Quobyte CI
7550         self._ensure_console_log_for_instance(instance)
7551 
7552         gen_confdrive = functools.partial(
7553             self._create_configdrive, context, instance,
7554             InjectionInfo(admin_pass=None, network_info=network_info,
7555                           files=None))
7556 
7557         # Convert raw disks to qcow2 if migrating to host which uses
7558         # qcow2 from host which uses raw.
7559         disk_info = jsonutils.loads(disk_info)
7560         for info in disk_info:
7561             path = info['path']
7562             disk_name = os.path.basename(path)
7563 
7564             # NOTE(mdbooth): The code below looks wrong, but is actually
7565             # required to prevent a security hole when migrating from a host
7566             # with use_cow_images=False to one with use_cow_images=True.
7567             # Imagebackend uses use_cow_images to select between the
7568             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
7569             # writes to disk.info, but does not read it as it assumes qcow2.
7570             # Therefore if we don't convert raw to qcow2 here, a raw disk will
7571             # be incorrectly assumed to be qcow2, which is a severe security
7572             # flaw. The reverse is not true, because the atrociously-named-Raw
7573             # backend supports both qcow2 and raw disks, and will choose
7574             # appropriately between them as long as disk.info exists and is
7575             # correctly populated, which it is because Qcow2 writes to
7576             # disk.info.
7577             #
7578             # In general, we do not yet support format conversion during
7579             # migration. For example:
7580             #   * Converting from use_cow_images=True to use_cow_images=False
7581             #     isn't handled. This isn't a security bug, but is almost
7582             #     certainly buggy in other cases, as the 'Raw' backend doesn't
7583             #     expect a backing file.
7584             #   * Converting to/from lvm and rbd backends is not supported.
7585             #
7586             # This behaviour is inconsistent, and therefore undesirable for
7587             # users. It is tightly-coupled to implementation quirks of 2
7588             # out of 5 backends in imagebackend and defends against a severe
7589             # security flaw which is not at all obvious without deep analysis,
7590             # and is therefore undesirable to developers. We should aim to
7591             # remove it. This will not be possible, though, until we can
7592             # represent the storage layout of a specific instance
7593             # independent of the default configuration of the local compute
7594             # host.
7595 
7596             # Config disks are hard-coded to be raw even when
7597             # use_cow_images=True (see _get_disk_config_image_type),so don't
7598             # need to be converted.
7599             if (disk_name != 'disk.config' and
7600                         info['type'] == 'raw' and CONF.use_cow_images):
7601                 self._disk_raw_to_qcow2(info['path'])
7602 
7603         xml = self._get_guest_xml(context, instance, network_info,
7604                                   block_disk_info, image_meta,
7605                                   block_device_info=block_device_info)
7606         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
7607         # or not we've migrated to another host, because we unplug VIFs locally
7608         # and the status change in the port might go undetected by the neutron
7609         # L2 agent (or neutron server) so neutron may not know that the VIF was
7610         # unplugged in the first place and never send an event.
7611         guest = self._create_domain_and_network(context, xml, instance,
7612                                         network_info,
7613                                         block_device_info=block_device_info,
7614                                         power_on=power_on,
7615                                         vifs_already_plugged=True,
7616                                         post_xml_callback=gen_confdrive)
7617         if power_on:
7618             timer = loopingcall.FixedIntervalLoopingCall(
7619                                                     self._wait_for_running,
7620                                                     instance)
7621             timer.start(interval=0.5).wait()
7622 
7623             # Sync guest time after migration.
7624             guest.sync_guest_time()
7625 
7626         LOG.debug("finish_migration finished successfully.", instance=instance)
7627 
7628     def _cleanup_failed_migration(self, inst_base):
7629         """Make sure that a failed migrate doesn't prevent us from rolling
7630         back in a revert.
7631         """
7632         try:
7633             shutil.rmtree(inst_base)
7634         except OSError as e:
7635             if e.errno != errno.ENOENT:
7636                 raise
7637 
7638     def finish_revert_migration(self, context, instance, network_info,
7639                                 block_device_info=None, power_on=True):
7640         LOG.debug("Starting finish_revert_migration",
7641                   instance=instance)
7642 
7643         inst_base = libvirt_utils.get_instance_path(instance)
7644         inst_base_resize = inst_base + "_resize"
7645 
7646         # NOTE(danms): if we're recovering from a failed migration,
7647         # make sure we don't have a left-over same-host base directory
7648         # that would conflict. Also, don't fail on the rename if the
7649         # failure happened early.
7650         if os.path.exists(inst_base_resize):
7651             self._cleanup_failed_migration(inst_base)
7652             utils.execute('mv', inst_base_resize, inst_base)
7653 
7654         root_disk = self.image_backend.by_name(instance, 'disk')
7655         # Once we rollback, the snapshot is no longer needed, so remove it
7656         # TODO(nic): Remove the try/except/finally in a future release
7657         # To avoid any upgrade issues surrounding instances being in pending
7658         # resize state when the software is updated, this portion of the
7659         # method logs exceptions rather than failing on them.  Once it can be
7660         # reasonably assumed that no such instances exist in the wild
7661         # anymore, the try/except/finally should be removed,
7662         # and ignore_errors should be set back to False (the default) so
7663         # that problems throw errors, like they should.
7664         if root_disk.exists():
7665             try:
7666                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
7667             except exception.SnapshotNotFound:
7668                 LOG.warning("Failed to rollback snapshot (%s)",
7669                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
7670             finally:
7671                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
7672                                       ignore_errors=True)
7673 
7674         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7675                                             instance,
7676                                             instance.image_meta,
7677                                             block_device_info)
7678         xml = self._get_guest_xml(context, instance, network_info, disk_info,
7679                                   instance.image_meta,
7680                                   block_device_info=block_device_info)
7681         self._create_domain_and_network(context, xml, instance, network_info,
7682                                         block_device_info=block_device_info,
7683                                         power_on=power_on,
7684                                         vifs_already_plugged=True)
7685 
7686         if power_on:
7687             timer = loopingcall.FixedIntervalLoopingCall(
7688                                                     self._wait_for_running,
7689                                                     instance)
7690             timer.start(interval=0.5).wait()
7691 
7692         LOG.debug("finish_revert_migration finished successfully.",
7693                   instance=instance)
7694 
7695     def confirm_migration(self, context, migration, instance, network_info):
7696         """Confirms a resize, destroying the source VM."""
7697         self._cleanup_resize(instance, network_info)
7698 
7699     @staticmethod
7700     def _get_io_devices(xml_doc):
7701         """get the list of io devices from the xml document."""
7702         result = {"volumes": [], "ifaces": []}
7703         try:
7704             doc = etree.fromstring(xml_doc)
7705         except Exception:
7706             return result
7707         blocks = [('./devices/disk', 'volumes'),
7708             ('./devices/interface', 'ifaces')]
7709         for block, key in blocks:
7710             section = doc.findall(block)
7711             for node in section:
7712                 for child in node.getchildren():
7713                     if child.tag == 'target' and child.get('dev'):
7714                         result[key].append(child.get('dev'))
7715         return result
7716 
7717     def get_diagnostics(self, instance):
7718         guest = self._host.get_guest(instance)
7719 
7720         # TODO(sahid): We are converting all calls from a
7721         # virDomain object to use nova.virt.libvirt.Guest.
7722         # We should be able to remove domain at the end.
7723         domain = guest._domain
7724         output = {}
7725         # get cpu time, might launch an exception if the method
7726         # is not supported by the underlying hypervisor being
7727         # used by libvirt
7728         try:
7729             for vcpu in guest.get_vcpus_info():
7730                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
7731         except libvirt.libvirtError:
7732             pass
7733         # get io status
7734         xml = guest.get_xml_desc()
7735         dom_io = LibvirtDriver._get_io_devices(xml)
7736         for guest_disk in dom_io["volumes"]:
7737             try:
7738                 # blockStats might launch an exception if the method
7739                 # is not supported by the underlying hypervisor being
7740                 # used by libvirt
7741                 stats = domain.blockStats(guest_disk)
7742                 output[guest_disk + "_read_req"] = stats[0]
7743                 output[guest_disk + "_read"] = stats[1]
7744                 output[guest_disk + "_write_req"] = stats[2]
7745                 output[guest_disk + "_write"] = stats[3]
7746                 output[guest_disk + "_errors"] = stats[4]
7747             except libvirt.libvirtError:
7748                 pass
7749         for interface in dom_io["ifaces"]:
7750             try:
7751                 # interfaceStats might launch an exception if the method
7752                 # is not supported by the underlying hypervisor being
7753                 # used by libvirt
7754                 stats = domain.interfaceStats(interface)
7755                 output[interface + "_rx"] = stats[0]
7756                 output[interface + "_rx_packets"] = stats[1]
7757                 output[interface + "_rx_errors"] = stats[2]
7758                 output[interface + "_rx_drop"] = stats[3]
7759                 output[interface + "_tx"] = stats[4]
7760                 output[interface + "_tx_packets"] = stats[5]
7761                 output[interface + "_tx_errors"] = stats[6]
7762                 output[interface + "_tx_drop"] = stats[7]
7763             except libvirt.libvirtError:
7764                 pass
7765         output["memory"] = domain.maxMemory()
7766         # memoryStats might launch an exception if the method
7767         # is not supported by the underlying hypervisor being
7768         # used by libvirt
7769         try:
7770             mem = domain.memoryStats()
7771             for key in mem.keys():
7772                 output["memory-" + key] = mem[key]
7773         except (libvirt.libvirtError, AttributeError):
7774             pass
7775         return output
7776 
7777     def get_instance_diagnostics(self, instance):
7778         guest = self._host.get_guest(instance)
7779 
7780         # TODO(sahid): We are converting all calls from a
7781         # virDomain object to use nova.virt.libvirt.Guest.
7782         # We should be able to remove domain at the end.
7783         domain = guest._domain
7784 
7785         xml = guest.get_xml_desc()
7786         xml_doc = etree.fromstring(xml)
7787 
7788         # TODO(sahid): Needs to use get_info but more changes have to
7789         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
7790         # needed.
7791         (state, max_mem, mem, num_cpu, cpu_time) = \
7792             guest._get_domain_info(self._host)
7793         config_drive = configdrive.required_by(instance)
7794         launched_at = timeutils.normalize_time(instance.launched_at)
7795         uptime = timeutils.delta_seconds(launched_at,
7796                                          timeutils.utcnow())
7797         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
7798                                         driver='libvirt',
7799                                         config_drive=config_drive,
7800                                         hypervisor=CONF.libvirt.virt_type,
7801                                         hypervisor_os='linux',
7802                                         uptime=uptime)
7803         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
7804             maximum=max_mem / units.Mi,
7805             used=mem / units.Mi)
7806 
7807         # get cpu time, might launch an exception if the method
7808         # is not supported by the underlying hypervisor being
7809         # used by libvirt
7810         try:
7811             for vcpu in guest.get_vcpus_info():
7812                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
7813         except libvirt.libvirtError:
7814             pass
7815         # get io status
7816         dom_io = LibvirtDriver._get_io_devices(xml)
7817         for guest_disk in dom_io["volumes"]:
7818             try:
7819                 # blockStats might launch an exception if the method
7820                 # is not supported by the underlying hypervisor being
7821                 # used by libvirt
7822                 stats = domain.blockStats(guest_disk)
7823                 diags.add_disk(read_bytes=stats[1],
7824                                read_requests=stats[0],
7825                                write_bytes=stats[3],
7826                                write_requests=stats[2],
7827                                errors_count=stats[4])
7828             except libvirt.libvirtError:
7829                 pass
7830         for interface in dom_io["ifaces"]:
7831             try:
7832                 # interfaceStats might launch an exception if the method
7833                 # is not supported by the underlying hypervisor being
7834                 # used by libvirt
7835                 stats = domain.interfaceStats(interface)
7836                 diags.add_nic(rx_octets=stats[0],
7837                               rx_errors=stats[2],
7838                               rx_drop=stats[3],
7839                               rx_packets=stats[1],
7840                               tx_octets=stats[4],
7841                               tx_errors=stats[6],
7842                               tx_drop=stats[7],
7843                               tx_packets=stats[5])
7844             except libvirt.libvirtError:
7845                 pass
7846 
7847         # Update mac addresses of interface if stats have been reported
7848         if diags.nic_details:
7849             nodes = xml_doc.findall('./devices/interface/mac')
7850             for index, node in enumerate(nodes):
7851                 diags.nic_details[index].mac_address = node.get('address')
7852         return diags
7853 
7854     @staticmethod
7855     def _prepare_device_bus(dev):
7856         """Determines the device bus and its hypervisor assigned address
7857         """
7858         bus = None
7859         address = (dev.device_addr.format_address() if
7860                    dev.device_addr else None)
7861         if isinstance(dev.device_addr,
7862                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
7863             bus = objects.PCIDeviceBus()
7864         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7865             if dev.target_bus == 'scsi':
7866                 bus = objects.SCSIDeviceBus()
7867             elif dev.target_bus == 'ide':
7868                 bus = objects.IDEDeviceBus()
7869             elif dev.target_bus == 'usb':
7870                 bus = objects.USBDeviceBus()
7871         if address is not None and bus is not None:
7872             bus.address = address
7873         return bus
7874 
7875     def _build_device_metadata(self, context, instance):
7876         """Builds a metadata object for instance devices, that maps the user
7877            provided tag to the hypervisor assigned device address.
7878         """
7879         def _get_device_name(bdm):
7880             return block_device.strip_dev(bdm.device_name)
7881 
7882         network_info = instance.info_cache.network_info
7883         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
7884         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
7885                                                                  instance.uuid)
7886         vifs_to_expose = {vif.address: vif for vif in vifs
7887                           if ('tag' in vif and vif.tag) or
7888                              vlans_by_mac.get(vif.address)}
7889         # TODO(mriedem): We should be able to avoid the DB query here by using
7890         # block_device_info['block_device_mapping'] which is passed into most
7891         # methods that call this function.
7892         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7893             context, instance.uuid)
7894         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
7895 
7896         devices = []
7897         guest = self._host.get_guest(instance)
7898         xml = guest.get_xml_desc()
7899         xml_dom = etree.fromstring(xml)
7900         guest_config = vconfig.LibvirtConfigGuest()
7901         guest_config.parse_dom(xml_dom)
7902 
7903         for dev in guest_config.devices:
7904             # Build network interfaces related metadata
7905             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
7906                 vif = vifs_to_expose.get(dev.mac_addr)
7907                 if not vif:
7908                     continue
7909                 bus = self._prepare_device_bus(dev)
7910                 device = objects.NetworkInterfaceMetadata(mac=vif.address)
7911                 if 'tag' in vif and vif.tag:
7912                     device.tags = [vif.tag]
7913                 if bus:
7914                     device.bus = bus
7915                 vlan = vlans_by_mac.get(vif.address)
7916                 if vlan:
7917                     device.vlan = int(vlan)
7918                 devices.append(device)
7919 
7920             # Build disks related metadata
7921             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7922                 bdm = tagged_bdms.get(dev.target_dev)
7923                 if not bdm:
7924                     continue
7925                 bus = self._prepare_device_bus(dev)
7926                 device = objects.DiskMetadata(tags=[bdm.tag])
7927                 # NOTE(artom) Setting the serial (which corresponds to
7928                 # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
7929                 # find the disks's BlockDeviceMapping object when we detach the
7930                 # volume and want to clean up its metadata.
7931                 device.serial = bdm.volume_id
7932                 if bus:
7933                     device.bus = bus
7934                 devices.append(device)
7935         if devices:
7936             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
7937             return dev_meta
7938 
7939     def instance_on_disk(self, instance):
7940         # ensure directories exist and are writable
7941         instance_path = libvirt_utils.get_instance_path(instance)
7942         LOG.debug('Checking instance files accessibility %s', instance_path,
7943                   instance=instance)
7944         shared_instance_path = os.access(instance_path, os.W_OK)
7945         # NOTE(flwang): For shared block storage scenario, the file system is
7946         # not really shared by the two hosts, but the volume of evacuated
7947         # instance is reachable.
7948         shared_block_storage = (self.image_backend.backend().
7949                                 is_shared_block_storage())
7950         return shared_instance_path or shared_block_storage
7951 
7952     def inject_network_info(self, instance, nw_info):
7953         self.firewall_driver.setup_basic_filtering(instance, nw_info)
7954 
7955     def delete_instance_files(self, instance):
7956         target = libvirt_utils.get_instance_path(instance)
7957         # A resize may be in progress
7958         target_resize = target + '_resize'
7959         # Other threads may attempt to rename the path, so renaming the path
7960         # to target + '_del' (because it is atomic) and iterating through
7961         # twice in the unlikely event that a concurrent rename occurs between
7962         # the two rename attempts in this method. In general this method
7963         # should be fairly thread-safe without these additional checks, since
7964         # other operations involving renames are not permitted when the task
7965         # state is not None and the task state should be set to something
7966         # other than None by the time this method is invoked.
7967         target_del = target + '_del'
7968         for i in range(2):
7969             try:
7970                 utils.execute('mv', target, target_del)
7971                 break
7972             except Exception:
7973                 pass
7974             try:
7975                 utils.execute('mv', target_resize, target_del)
7976                 break
7977             except Exception:
7978                 pass
7979         # Either the target or target_resize path may still exist if all
7980         # rename attempts failed.
7981         remaining_path = None
7982         for p in (target, target_resize):
7983             if os.path.exists(p):
7984                 remaining_path = p
7985                 break
7986 
7987         # A previous delete attempt may have been interrupted, so target_del
7988         # may exist even if all rename attempts during the present method
7989         # invocation failed due to the absence of both target and
7990         # target_resize.
7991         if not remaining_path and os.path.exists(target_del):
7992             self.job_tracker.terminate_jobs(instance)
7993 
7994             LOG.info('Deleting instance files %s', target_del,
7995                      instance=instance)
7996             remaining_path = target_del
7997             try:
7998                 shutil.rmtree(target_del)
7999             except OSError as e:
8000                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8001                           {'target': target_del, 'e': e}, instance=instance)
8002 
8003         # It is possible that the delete failed, if so don't mark the instance
8004         # as cleaned.
8005         if remaining_path and os.path.exists(remaining_path):
8006             LOG.info('Deletion of %s failed', remaining_path,
8007                      instance=instance)
8008             return False
8009 
8010         LOG.info('Deletion of %s complete', target_del, instance=instance)
8011         return True
8012 
8013     @property
8014     def need_legacy_block_device_info(self):
8015         return False
8016 
8017     def default_root_device_name(self, instance, image_meta, root_bdm):
8018         disk_bus = blockinfo.get_disk_bus_for_device_type(
8019             instance, CONF.libvirt.virt_type, image_meta, "disk")
8020         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8021             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8022         root_info = blockinfo.get_root_info(
8023             instance, CONF.libvirt.virt_type, image_meta,
8024             root_bdm, disk_bus, cdrom_bus)
8025         return block_device.prepend_dev(root_info['dev'])
8026 
8027     def default_device_names_for_instance(self, instance, root_device_name,
8028                                           *block_device_lists):
8029         block_device_mapping = list(itertools.chain(*block_device_lists))
8030         # NOTE(ndipanov): Null out the device names so that blockinfo code
8031         #                 will assign them
8032         for bdm in block_device_mapping:
8033             if bdm.device_name is not None:
8034                 LOG.warning(
8035                     "Ignoring supplied device name: %(device_name)s. "
8036                     "Libvirt can't honour user-supplied dev names",
8037                     {'device_name': bdm.device_name}, instance=instance)
8038                 bdm.device_name = None
8039         block_device_info = driver.get_block_device_info(instance,
8040                                                          block_device_mapping)
8041 
8042         blockinfo.default_device_names(CONF.libvirt.virt_type,
8043                                        nova_context.get_admin_context(),
8044                                        instance,
8045                                        block_device_info,
8046                                        instance.image_meta)
8047 
8048     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8049         block_device_info = driver.get_block_device_info(instance, bdms)
8050         instance_info = blockinfo.get_disk_info(
8051                 CONF.libvirt.virt_type, instance,
8052                 instance.image_meta, block_device_info=block_device_info)
8053 
8054         suggested_dev_name = block_device_obj.device_name
8055         if suggested_dev_name is not None:
8056             LOG.warning(
8057                 'Ignoring supplied device name: %(suggested_dev)s',
8058                 {'suggested_dev': suggested_dev_name}, instance=instance)
8059 
8060         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8061         #                 only when it's actually not set on the bd object
8062         block_device_obj.device_name = None
8063         disk_info = blockinfo.get_info_from_bdm(
8064             instance, CONF.libvirt.virt_type, instance.image_meta,
8065             block_device_obj, mapping=instance_info['mapping'])
8066         return block_device.prepend_dev(disk_info['dev'])
8067 
8068     def is_supported_fs_format(self, fs_type):
8069         return fs_type in [disk_api.FS_FORMAT_EXT2, disk_api.FS_FORMAT_EXT3,
8070                            disk_api.FS_FORMAT_EXT4, disk_api.FS_FORMAT_XFS]
