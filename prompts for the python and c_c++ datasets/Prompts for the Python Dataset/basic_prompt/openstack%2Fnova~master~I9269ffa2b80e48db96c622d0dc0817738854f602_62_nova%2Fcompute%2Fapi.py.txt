Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import base64
23 import collections
24 import copy
25 import functools
26 import re
27 import string
28 
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_serialization import jsonutils
33 from oslo_utils import excutils
34 from oslo_utils import strutils
35 from oslo_utils import timeutils
36 from oslo_utils import units
37 from oslo_utils import uuidutils
38 import six
39 from six.moves import range
40 
41 from nova import availability_zones
42 from nova import block_device
43 from nova.cells import opts as cells_opts
44 from nova.compute import flavors
45 from nova.compute import instance_actions
46 from nova.compute import power_state
47 from nova.compute import rpcapi as compute_rpcapi
48 from nova.compute import task_states
49 from nova.compute import utils as compute_utils
50 from nova.compute import vm_states
51 from nova import conductor
52 import nova.conf
53 from nova.consoleauth import rpcapi as consoleauth_rpcapi
54 from nova import context as nova_context
55 from nova import crypto
56 from nova.db import base
57 from nova import exception
58 from nova import exception_wrapper
59 from nova import hooks
60 from nova.i18n import _
61 from nova import image
62 from nova import keymgr
63 from nova import network
64 from nova.network import model as network_model
65 from nova.network.security_group import openstack_driver
66 from nova.network.security_group import security_group_base
67 from nova import objects
68 from nova.objects import base as obj_base
69 from nova.objects import block_device as block_device_obj
70 from nova.objects import fields as fields_obj
71 from nova.objects import keypair as keypair_obj
72 from nova.objects import quotas as quotas_obj
73 from nova.pci import request as pci_request
74 import nova.policy
75 from nova import profiler
76 from nova import rpc
77 from nova.scheduler import client as scheduler_client
78 from nova.scheduler import utils as scheduler_utils
79 from nova import servicegroup
80 from nova import utils
81 from nova.virt import hardware
82 from nova.volume import cinder
83 
84 LOG = logging.getLogger(__name__)
85 
86 get_notifier = functools.partial(rpc.get_notifier, service='compute')
87 # NOTE(gibi): legacy notification used compute as a service but these
88 # calls still run on the client side of the compute service which is
89 # nova-api. By setting the binary to nova-api below, we can make sure
90 # that the new versioned notifications has the right publisher_id but the
91 # legacy notifications does not change.
92 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
93                                    get_notifier=get_notifier,
94                                    binary='nova-api')
95 CONF = nova.conf.CONF
96 
97 MAX_USERDATA_SIZE = 65535
98 RO_SECURITY_GROUPS = ['default']
99 
100 AGGREGATE_ACTION_UPDATE = 'Update'
101 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
102 AGGREGATE_ACTION_DELETE = 'Delete'
103 AGGREGATE_ACTION_ADD = 'Add'
104 BFV_RESERVE_MIN_COMPUTE_VERSION = 17
105 
106 # FIXME(danms): Keep a global cache of the cells we find the
107 # first time we look. This needs to be refreshed on a timer or
108 # trigger.
109 CELLS = []
110 
111 
112 def check_instance_state(vm_state=None, task_state=(None,),
113                          must_have_launched=True):
114     """Decorator to check VM and/or task state before entry to API functions.
115 
116     If the instance is in the wrong state, or has not been successfully
117     started at least once the wrapper will raise an exception.
118     """
119 
120     if vm_state is not None and not isinstance(vm_state, set):
121         vm_state = set(vm_state)
122     if task_state is not None and not isinstance(task_state, set):
123         task_state = set(task_state)
124 
125     def outer(f):
126         @six.wraps(f)
127         def inner(self, context, instance, *args, **kw):
128             if vm_state is not None and instance.vm_state not in vm_state:
129                 raise exception.InstanceInvalidState(
130                     attr='vm_state',
131                     instance_uuid=instance.uuid,
132                     state=instance.vm_state,
133                     method=f.__name__)
134             if (task_state is not None and
135                     instance.task_state not in task_state):
136                 raise exception.InstanceInvalidState(
137                     attr='task_state',
138                     instance_uuid=instance.uuid,
139                     state=instance.task_state,
140                     method=f.__name__)
141             if must_have_launched and not instance.launched_at:
142                 raise exception.InstanceInvalidState(
143                     attr='launched_at',
144                     instance_uuid=instance.uuid,
145                     state=instance.launched_at,
146                     method=f.__name__)
147 
148             return f(self, context, instance, *args, **kw)
149         return inner
150     return outer
151 
152 
153 def _set_or_none(q):
154     return q if q is None or isinstance(q, set) else set(q)
155 
156 
157 def reject_instance_state(vm_state=None, task_state=None):
158     """Decorator.  Raise InstanceInvalidState if instance is in any of the
159     given states.
160     """
161 
162     vm_state = _set_or_none(vm_state)
163     task_state = _set_or_none(task_state)
164 
165     def outer(f):
166         @six.wraps(f)
167         def inner(self, context, instance, *args, **kw):
168             _InstanceInvalidState = functools.partial(
169                 exception.InstanceInvalidState,
170                 instance_uuid=instance.uuid,
171                 method=f.__name__)
172 
173             if vm_state is not None and instance.vm_state in vm_state:
174                 raise _InstanceInvalidState(
175                     attr='vm_state', state=instance.vm_state)
176 
177             if task_state is not None and instance.task_state in task_state:
178                 raise _InstanceInvalidState(
179                     attr='task_state', state=instance.task_state)
180 
181             return f(self, context, instance, *args, **kw)
182         return inner
183     return outer
184 
185 
186 def check_instance_host(function):
187     @six.wraps(function)
188     def wrapped(self, context, instance, *args, **kwargs):
189         if not instance.host:
190             raise exception.InstanceNotReady(instance_id=instance.uuid)
191         return function(self, context, instance, *args, **kwargs)
192     return wrapped
193 
194 
195 def check_instance_lock(function):
196     @six.wraps(function)
197     def inner(self, context, instance, *args, **kwargs):
198         if instance.locked and not context.is_admin:
199             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
200         return function(self, context, instance, *args, **kwargs)
201     return inner
202 
203 
204 def check_instance_cell(fn):
205     @six.wraps(fn)
206     def _wrapped(self, context, instance, *args, **kwargs):
207         self._validate_cell(instance)
208         return fn(self, context, instance, *args, **kwargs)
209     return _wrapped
210 
211 
212 def _diff_dict(orig, new):
213     """Return a dict describing how to change orig to new.  The keys
214     correspond to values that have changed; the value will be a list
215     of one or two elements.  The first element of the list will be
216     either '+' or '-', indicating whether the key was updated or
217     deleted; if the key was updated, the list will contain a second
218     element, giving the updated value.
219     """
220     # Figure out what keys went away
221     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
222     # Compute the updates
223     for key, value in new.items():
224         if key not in orig or value != orig[key]:
225             result[key] = ['+', value]
226     return result
227 
228 
229 def load_cells():
230     global CELLS
231     if not CELLS:
232         CELLS = objects.CellMappingList.get_all(
233             nova_context.get_admin_context())
234         LOG.debug('Found %(count)i cells: %(cells)s',
235                   dict(count=len(CELLS),
236                        cells=','.join([c.identity for c in CELLS])))
237 
238     if not CELLS:
239         LOG.error('No cells are configured, unable to continue')
240 
241 
242 @profiler.trace_cls("compute_api")
243 class API(base.Base):
244     """API for interacting with the compute manager."""
245 
246     def __init__(self, image_api=None, network_api=None, volume_api=None,
247                  security_group_api=None, **kwargs):
248         self.image_api = image_api or image.API()
249         self.network_api = network_api or network.API()
250         self.volume_api = volume_api or cinder.API()
251         self.security_group_api = (security_group_api or
252             openstack_driver.get_openstack_security_group_driver())
253         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
254         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
255         self.compute_task_api = conductor.ComputeTaskAPI()
256         self.servicegroup_api = servicegroup.API()
257         self.notifier = rpc.get_notifier('compute', CONF.host)
258         if CONF.ephemeral_storage_encryption.enabled:
259             self.key_manager = keymgr.API()
260 
261         super(API, self).__init__(**kwargs)
262 
263     @property
264     def cell_type(self):
265         try:
266             return getattr(self, '_cell_type')
267         except AttributeError:
268             self._cell_type = cells_opts.get_cell_type()
269             return self._cell_type
270 
271     def _validate_cell(self, instance):
272         if self.cell_type != 'api':
273             return
274         cell_name = instance.cell_name
275         if not cell_name:
276             raise exception.InstanceUnknownCell(
277                     instance_uuid=instance.uuid)
278 
279     def _record_action_start(self, context, instance, action):
280         objects.InstanceAction.action_start(context, instance.uuid,
281                                             action, want_result=False)
282 
283     def _check_injected_file_quota(self, context, injected_files):
284         """Enforce quota limits on injected files.
285 
286         Raises a QuotaError if any limit is exceeded.
287         """
288         if injected_files is None:
289             return
290 
291         # Check number of files first
292         try:
293             objects.Quotas.limit_check(context,
294                                        injected_files=len(injected_files))
295         except exception.OverQuota:
296             raise exception.OnsetFileLimitExceeded()
297 
298         # OK, now count path and content lengths; we're looking for
299         # the max...
300         max_path = 0
301         max_content = 0
302         for path, content in injected_files:
303             max_path = max(max_path, len(path))
304             max_content = max(max_content, len(content))
305 
306         try:
307             objects.Quotas.limit_check(context,
308                                        injected_file_path_bytes=max_path,
309                                        injected_file_content_bytes=max_content)
310         except exception.OverQuota as exc:
311             # Favor path limit over content limit for reporting
312             # purposes
313             if 'injected_file_path_bytes' in exc.kwargs['overs']:
314                 raise exception.OnsetFilePathLimitExceeded()
315             else:
316                 raise exception.OnsetFileContentLimitExceeded()
317 
318     def _check_metadata_properties_quota(self, context, metadata=None):
319         """Enforce quota limits on metadata properties."""
320         if not metadata:
321             metadata = {}
322         if not isinstance(metadata, dict):
323             msg = (_("Metadata type should be dict."))
324             raise exception.InvalidMetadata(reason=msg)
325         num_metadata = len(metadata)
326         try:
327             objects.Quotas.limit_check(context, metadata_items=num_metadata)
328         except exception.OverQuota as exc:
329             quota_metadata = exc.kwargs['quotas']['metadata_items']
330             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
331 
332         # Because metadata is stored in the DB, we hard-code the size limits
333         # In future, we may support more variable length strings, so we act
334         #  as if this is quota-controlled for forwards compatibility.
335         # Those are only used in V2 API, from V2.1 API, those checks are
336         # validated at API layer schema validation.
337         for k, v in metadata.items():
338             try:
339                 utils.check_string_length(v)
340                 utils.check_string_length(k, min_length=1)
341             except exception.InvalidInput as e:
342                 raise exception.InvalidMetadata(reason=e.format_message())
343 
344             if len(k) > 255:
345                 msg = _("Metadata property key greater than 255 characters")
346                 raise exception.InvalidMetadataSize(reason=msg)
347             if len(v) > 255:
348                 msg = _("Metadata property value greater than 255 characters")
349                 raise exception.InvalidMetadataSize(reason=msg)
350 
351     def _check_requested_secgroups(self, context, secgroups):
352         """Check if the security group requested exists and belongs to
353         the project.
354 
355         :param context: The nova request context.
356         :type context: nova.context.RequestContext
357         :param secgroups: list of requested security group names, or uuids in
358             the case of Neutron.
359         :type secgroups: list
360         :returns: list of requested security group names unmodified if using
361             nova-network. If using Neutron, the list returned is all uuids.
362             Note that 'default' is a special case and will be unmodified if
363             it's requested.
364         """
365         security_groups = []
366         for secgroup in secgroups:
367             # NOTE(sdague): default is handled special
368             if secgroup == "default":
369                 security_groups.append(secgroup)
370                 continue
371             secgroup_dict = self.security_group_api.get(context, secgroup)
372             if not secgroup_dict:
373                 raise exception.SecurityGroupNotFoundForProject(
374                     project_id=context.project_id, security_group_id=secgroup)
375 
376             # Check to see if it's a nova-network or neutron type.
377             if isinstance(secgroup_dict['id'], int):
378                 # This is nova-network so just return the requested name.
379                 security_groups.append(secgroup)
380             else:
381                 # The id for neutron is a uuid, so we return the id (uuid).
382                 security_groups.append(secgroup_dict['id'])
383 
384         return security_groups
385 
386     def _check_requested_networks(self, context, requested_networks,
387                                   max_count):
388         """Check if the networks requested belongs to the project
389         and the fixed IP address for each network provided is within
390         same the network block
391         """
392         if requested_networks is not None:
393             if requested_networks.no_allocate:
394                 # If the network request was specifically 'none' meaning don't
395                 # allocate any networks, we just return the number of requested
396                 # instances since quotas don't change at all.
397                 return max_count
398 
399             # NOTE(danms): Temporary transition
400             requested_networks = requested_networks.as_tuples()
401 
402         return self.network_api.validate_networks(context, requested_networks,
403                                                   max_count)
404 
405     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
406                                    image):
407         """Choose kernel and ramdisk appropriate for the instance.
408 
409         The kernel and ramdisk can be chosen in one of three ways:
410 
411             1. Passed in with create-instance request.
412 
413             2. Inherited from image.
414 
415             3. Forced to None by using `null_kernel` FLAG.
416         """
417         # Inherit from image if not specified
418         image_properties = image.get('properties', {})
419 
420         if kernel_id is None:
421             kernel_id = image_properties.get('kernel_id')
422 
423         if ramdisk_id is None:
424             ramdisk_id = image_properties.get('ramdisk_id')
425 
426         # Force to None if using null_kernel
427         if kernel_id == str(CONF.null_kernel):
428             kernel_id = None
429             ramdisk_id = None
430 
431         # Verify kernel and ramdisk exist (fail-fast)
432         if kernel_id is not None:
433             kernel_image = self.image_api.get(context, kernel_id)
434             # kernel_id could have been a URI, not a UUID, so to keep behaviour
435             # from before, which leaked that implementation detail out to the
436             # caller, we return the image UUID of the kernel image and ramdisk
437             # image (below) and not any image URIs that might have been
438             # supplied.
439             # TODO(jaypipes): Get rid of this silliness once we move to a real
440             # Image object and hide all of that stuff within nova.image.api.
441             kernel_id = kernel_image['id']
442 
443         if ramdisk_id is not None:
444             ramdisk_image = self.image_api.get(context, ramdisk_id)
445             ramdisk_id = ramdisk_image['id']
446 
447         return kernel_id, ramdisk_id
448 
449     @staticmethod
450     def parse_availability_zone(context, availability_zone):
451         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
452         #             via az using az:host:node. It might be nice to expose an
453         #             api to specify specific hosts to force onto, but for
454         #             now it just supports this legacy hack.
455         # NOTE(deva): It is also possible to specify az::node, in which case
456         #             the host manager will determine the correct host.
457         forced_host = None
458         forced_node = None
459         if availability_zone and ':' in availability_zone:
460             c = availability_zone.count(':')
461             if c == 1:
462                 availability_zone, forced_host = availability_zone.split(':')
463             elif c == 2:
464                 if '::' in availability_zone:
465                     availability_zone, forced_node = \
466                             availability_zone.split('::')
467                 else:
468                     availability_zone, forced_host, forced_node = \
469                             availability_zone.split(':')
470             else:
471                 raise exception.InvalidInput(
472                         reason="Unable to parse availability_zone")
473 
474         if not availability_zone:
475             availability_zone = CONF.default_schedule_zone
476 
477         return availability_zone, forced_host, forced_node
478 
479     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
480                                           auto_disk_config, image):
481         auto_disk_config_disabled = \
482                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
483         if auto_disk_config_disabled and auto_disk_config:
484             raise exception.AutoDiskConfigDisabledByImage(image=image)
485 
486     def _inherit_properties_from_image(self, image, auto_disk_config):
487         image_properties = image.get('properties', {})
488         auto_disk_config_img = \
489                 utils.get_auto_disk_config_from_image_props(image_properties)
490         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
491                                                auto_disk_config,
492                                                image.get("id"))
493         if auto_disk_config is None:
494             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
495 
496         return {
497             'os_type': image_properties.get('os_type'),
498             'architecture': image_properties.get('architecture'),
499             'vm_mode': image_properties.get('vm_mode'),
500             'auto_disk_config': auto_disk_config
501         }
502 
503     def _new_instance_name_from_template(self, uuid, display_name, index):
504         params = {
505             'uuid': uuid,
506             'name': display_name,
507             'count': index + 1,
508         }
509         try:
510             new_name = (CONF.multi_instance_display_name_template %
511                         params)
512         except (KeyError, TypeError):
513             LOG.exception('Failed to set instance name using '
514                           'multi_instance_display_name_template.')
515             new_name = display_name
516         return new_name
517 
518     def _apply_instance_name_template(self, context, instance, index):
519         original_name = instance.display_name
520         new_name = self._new_instance_name_from_template(instance.uuid,
521                 instance.display_name, index)
522         instance.display_name = new_name
523         if not instance.get('hostname', None):
524             if utils.sanitize_hostname(original_name) == "":
525                 instance.hostname = self._default_host_name(instance.uuid)
526             else:
527                 instance.hostname = utils.sanitize_hostname(new_name)
528         return instance
529 
530     def _check_config_drive(self, config_drive):
531         if config_drive:
532             try:
533                 bool_val = strutils.bool_from_string(config_drive,
534                                                      strict=True)
535             except ValueError:
536                 raise exception.ConfigDriveInvalidValue(option=config_drive)
537         else:
538             bool_val = False
539         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
540         # but this is because the config drive column is a String.  False
541         # is represented by using an empty string.  And for whatever
542         # reason, we rely on the DB to cast True to a String.
543         return True if bool_val else ''
544 
545     def _check_requested_image(self, context, image_id, image,
546                                instance_type, root_bdm):
547         if not image:
548             return
549 
550         if image['status'] != 'active':
551             raise exception.ImageNotActive(image_id=image_id)
552 
553         image_properties = image.get('properties', {})
554         config_drive_option = image_properties.get(
555             'img_config_drive', 'optional')
556         if config_drive_option not in ['optional', 'mandatory']:
557             raise exception.InvalidImageConfigDrive(
558                 config_drive=config_drive_option)
559 
560         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
561             raise exception.FlavorMemoryTooSmall()
562 
563         # Image min_disk is in gb, size is in bytes. For sanity, have them both
564         # in bytes.
565         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
566         image_size = int(image.get('size') or 0)
567 
568         # Target disk is a volume. Don't check flavor disk size because it
569         # doesn't make sense, and check min_disk against the volume size.
570         if (root_bdm is not None and root_bdm.is_volume):
571             # There are 2 possibilities here: either the target volume already
572             # exists, or it doesn't, in which case the bdm will contain the
573             # intended volume size.
574             #
575             # Cinder does its own check against min_disk, so if the target
576             # volume already exists this has already been done and we don't
577             # need to check it again here. In this case, volume_size may not be
578             # set on the bdm.
579             #
580             # If we're going to create the volume, the bdm will contain
581             # volume_size. Therefore we should check it if it exists. This will
582             # still be checked again by cinder when the volume is created, but
583             # that will not happen until the request reaches a host. By
584             # checking it here, the user gets an immediate and useful failure
585             # indication.
586             #
587             # The third possibility is that we have failed to consider
588             # something, and there are actually more than 2 possibilities. In
589             # this case cinder will still do the check at volume creation time.
590             # The behaviour will still be correct, but the user will not get an
591             # immediate failure from the api, and will instead have to
592             # determine why the instance is in an error state with a task of
593             # block_device_mapping.
594             #
595             # We could reasonably refactor this check into _validate_bdm at
596             # some future date, as the various size logic is already split out
597             # in there.
598             dest_size = root_bdm.volume_size
599             if dest_size is not None:
600                 dest_size *= units.Gi
601 
602                 if image_min_disk > dest_size:
603                     raise exception.VolumeSmallerThanMinDisk(
604                         volume_size=dest_size, image_min_disk=image_min_disk)
605 
606         # Target disk is a local disk whose size is taken from the flavor
607         else:
608             dest_size = instance_type['root_gb'] * units.Gi
609 
610             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
611             # since libvirt interpreted the value differently than other
612             # drivers. A value of 0 means don't check size.
613             if dest_size != 0:
614                 if image_size > dest_size:
615                     raise exception.FlavorDiskSmallerThanImage(
616                         flavor_size=dest_size, image_size=image_size)
617 
618                 if image_min_disk > dest_size:
619                     raise exception.FlavorDiskSmallerThanMinDisk(
620                         flavor_size=dest_size, image_min_disk=image_min_disk)
621 
622     def _get_image_defined_bdms(self, instance_type, image_meta,
623                                 root_device_name):
624         image_properties = image_meta.get('properties', {})
625 
626         # Get the block device mappings defined by the image.
627         image_defined_bdms = image_properties.get('block_device_mapping', [])
628         legacy_image_defined = not image_properties.get('bdm_v2', False)
629 
630         image_mapping = image_properties.get('mappings', [])
631 
632         if legacy_image_defined:
633             image_defined_bdms = block_device.from_legacy_mapping(
634                 image_defined_bdms, None, root_device_name)
635         else:
636             image_defined_bdms = list(map(block_device.BlockDeviceDict,
637                                           image_defined_bdms))
638 
639         if image_mapping:
640             image_mapping = self._prepare_image_mapping(instance_type,
641                                                         image_mapping)
642             image_defined_bdms = self._merge_bdms_lists(
643                 image_mapping, image_defined_bdms)
644 
645         return image_defined_bdms
646 
647     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
648         flavor_defined_bdms = []
649 
650         have_ephemeral_bdms = any(filter(
651             block_device.new_format_is_ephemeral, block_device_mapping))
652         have_swap_bdms = any(filter(
653             block_device.new_format_is_swap, block_device_mapping))
654 
655         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
656             flavor_defined_bdms.append(
657                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
658         if instance_type.get('swap') and not have_swap_bdms:
659             flavor_defined_bdms.append(
660                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
661 
662         return flavor_defined_bdms
663 
664     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
665         """Override any block devices from the first list by device name
666 
667         :param overridable_mappings: list which items are overridden
668         :param overrider_mappings: list which items override
669 
670         :returns: A merged list of bdms
671         """
672         device_names = set(bdm['device_name'] for bdm in overrider_mappings
673                            if bdm['device_name'])
674         return (overrider_mappings +
675                 [bdm for bdm in overridable_mappings
676                  if bdm['device_name'] not in device_names])
677 
678     def _check_and_transform_bdm(self, context, base_options, instance_type,
679                                  image_meta, min_count, max_count,
680                                  block_device_mapping, legacy_bdm):
681         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
682         #                  It's needed for legacy conversion to work.
683         root_device_name = (base_options.get('root_device_name') or 'vda')
684         image_ref = base_options.get('image_ref', '')
685         # If the instance is booted by image and has a volume attached,
686         # the volume cannot have the same device name as root_device_name
687         if image_ref:
688             for bdm in block_device_mapping:
689                 if (bdm.get('destination_type') == 'volume' and
690                     block_device.strip_dev(bdm.get(
691                     'device_name')) == root_device_name):
692                     msg = _('The volume cannot be assigned the same device'
693                             ' name as the root device %s') % root_device_name
694                     raise exception.InvalidRequest(msg)
695 
696         image_defined_bdms = self._get_image_defined_bdms(
697             instance_type, image_meta, root_device_name)
698         root_in_image_bdms = (
699             block_device.get_root_bdm(image_defined_bdms) is not None)
700 
701         if legacy_bdm:
702             block_device_mapping = block_device.from_legacy_mapping(
703                 block_device_mapping, image_ref, root_device_name,
704                 no_root=root_in_image_bdms)
705         elif root_in_image_bdms:
706             # NOTE (ndipanov): client will insert an image mapping into the v2
707             # block_device_mapping, but if there is a bootable device in image
708             # mappings - we need to get rid of the inserted image
709             # NOTE (gibi): another case is when a server is booted with an
710             # image to bdm mapping where the image only contains a bdm to a
711             # snapshot. In this case the other image to bdm mapping
712             # contains an unnecessary device with boot_index == 0.
713             # Also in this case the image_ref is None as we are booting from
714             # an image to volume bdm.
715             def not_image_and_root_bdm(bdm):
716                 return not (bdm.get('boot_index') == 0 and
717                             bdm.get('source_type') == 'image')
718 
719             block_device_mapping = list(
720                 filter(not_image_and_root_bdm, block_device_mapping))
721 
722         block_device_mapping = self._merge_bdms_lists(
723             image_defined_bdms, block_device_mapping)
724 
725         if min_count > 1 or max_count > 1:
726             if any(map(lambda bdm: bdm['source_type'] == 'volume',
727                        block_device_mapping)):
728                 msg = _('Cannot attach one or more volumes to multiple'
729                         ' instances')
730                 raise exception.InvalidRequest(msg)
731 
732         block_device_mapping += self._get_flavor_defined_bdms(
733             instance_type, block_device_mapping)
734 
735         return block_device_obj.block_device_make_list_from_dicts(
736                 context, block_device_mapping)
737 
738     def _get_image(self, context, image_href):
739         if not image_href:
740             return None, {}
741 
742         image = self.image_api.get(context, image_href)
743         return image['id'], image
744 
745     def _checks_for_create_and_rebuild(self, context, image_id, image,
746                                        instance_type, metadata,
747                                        files_to_inject, root_bdm):
748         self._check_metadata_properties_quota(context, metadata)
749         self._check_injected_file_quota(context, files_to_inject)
750         self._check_requested_image(context, image_id, image,
751                                     instance_type, root_bdm)
752 
753     def _validate_and_build_base_options(self, context, instance_type,
754                                          boot_meta, image_href, image_id,
755                                          kernel_id, ramdisk_id, display_name,
756                                          display_description, key_name,
757                                          key_data, security_groups,
758                                          availability_zone, user_data,
759                                          metadata, access_ip_v4, access_ip_v6,
760                                          requested_networks, config_drive,
761                                          auto_disk_config, reservation_id,
762                                          max_count):
763         """Verify all the input parameters regardless of the provisioning
764         strategy being performed.
765         """
766         if instance_type['disabled']:
767             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
768 
769         if user_data:
770             l = len(user_data)
771             if l > MAX_USERDATA_SIZE:
772                 # NOTE(mikal): user_data is stored in a text column, and
773                 # the database might silently truncate if its over length.
774                 raise exception.InstanceUserDataTooLarge(
775                     length=l, maxsize=MAX_USERDATA_SIZE)
776 
777             try:
778                 base64utils.decode_as_bytes(user_data)
779             except (base64.binascii.Error, TypeError):
780                 # TODO(harlowja): reduce the above exceptions caught to
781                 # only type error once we get a new oslo.serialization
782                 # release that captures and makes only one be output.
783                 #
784                 # We can eliminate the capture of `binascii.Error` when:
785                 #
786                 # https://review.openstack.org/#/c/418066/ is released.
787                 raise exception.InstanceUserDataMalformed()
788 
789         # When using Neutron, _check_requested_secgroups will translate and
790         # return any requested security group names to uuids.
791         security_groups = (
792             self._check_requested_secgroups(context, security_groups))
793 
794         # Note:  max_count is the number of instances requested by the user,
795         # max_network_count is the maximum number of instances taking into
796         # account any network quotas
797         max_network_count = self._check_requested_networks(context,
798                                      requested_networks, max_count)
799 
800         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
801                 context, kernel_id, ramdisk_id, boot_meta)
802 
803         config_drive = self._check_config_drive(config_drive)
804 
805         if key_data is None and key_name is not None:
806             key_pair = objects.KeyPair.get_by_name(context,
807                                                    context.user_id,
808                                                    key_name)
809             key_data = key_pair.public_key
810         else:
811             key_pair = None
812 
813         root_device_name = block_device.prepend_dev(
814                 block_device.properties_root_device_name(
815                     boot_meta.get('properties', {})))
816 
817         try:
818             image_meta = objects.ImageMeta.from_dict(boot_meta)
819         except ValueError as e:
820             # there must be invalid values in the image meta properties so
821             # consider this an invalid request
822             msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
823             raise exception.InvalidRequest(msg)
824         numa_topology = hardware.numa_get_constraints(
825                 instance_type, image_meta)
826 
827         system_metadata = {}
828 
829         # PCI requests come from two sources: instance flavor and
830         # requested_networks. The first call in below returns an
831         # InstancePCIRequests object which is a list of InstancePCIRequest
832         # objects. The second call in below creates an InstancePCIRequest
833         # object for each SR-IOV port, and append it to the list in the
834         # InstancePCIRequests object
835         pci_request_info = pci_request.get_pci_requests_from_flavor(
836             instance_type)
837         self.network_api.create_pci_requests_for_sriov_ports(context,
838             pci_request_info, requested_networks)
839 
840         base_options = {
841             'reservation_id': reservation_id,
842             'image_ref': image_href,
843             'kernel_id': kernel_id or '',
844             'ramdisk_id': ramdisk_id or '',
845             'power_state': power_state.NOSTATE,
846             'vm_state': vm_states.BUILDING,
847             'config_drive': config_drive,
848             'user_id': context.user_id,
849             'project_id': context.project_id,
850             'instance_type_id': instance_type['id'],
851             'memory_mb': instance_type['memory_mb'],
852             'vcpus': instance_type['vcpus'],
853             'root_gb': instance_type['root_gb'],
854             'ephemeral_gb': instance_type['ephemeral_gb'],
855             'display_name': display_name,
856             'display_description': display_description,
857             'user_data': user_data,
858             'key_name': key_name,
859             'key_data': key_data,
860             'locked': False,
861             'metadata': metadata or {},
862             'access_ip_v4': access_ip_v4,
863             'access_ip_v6': access_ip_v6,
864             'availability_zone': availability_zone,
865             'root_device_name': root_device_name,
866             'progress': 0,
867             'pci_requests': pci_request_info,
868             'numa_topology': numa_topology,
869             'system_metadata': system_metadata}
870 
871         options_from_image = self._inherit_properties_from_image(
872                 boot_meta, auto_disk_config)
873 
874         base_options.update(options_from_image)
875 
876         # return the validated options and maximum number of instances allowed
877         # by the network quotas
878         return base_options, max_network_count, key_pair, security_groups
879 
880     def _provision_instances(self, context, instance_type, min_count,
881             max_count, base_options, boot_meta, security_groups,
882             block_device_mapping, shutdown_terminate,
883             instance_group, check_server_group_quota, filter_properties,
884             key_pair):
885         # Check quotas
886         num_instances = compute_utils.check_num_instances_quota(
887                 context, instance_type, min_count, max_count)
888         security_groups = self.security_group_api.populate_security_groups(
889                 security_groups)
890         self.security_group_api.ensure_default(context)
891         LOG.debug("Going to run %s instances...", num_instances)
892         instances_to_build = []
893         try:
894             for i in range(num_instances):
895                 # Create a uuid for the instance so we can store the
896                 # RequestSpec before the instance is created.
897                 instance_uuid = uuidutils.generate_uuid()
898                 # Store the RequestSpec that will be used for scheduling.
899                 req_spec = objects.RequestSpec.from_components(context,
900                         instance_uuid, boot_meta, instance_type,
901                         base_options['numa_topology'],
902                         base_options['pci_requests'], filter_properties,
903                         instance_group, base_options['availability_zone'],
904                         security_groups=security_groups)
905                 # NOTE(danms): We need to record num_instances on the request
906                 # spec as this is how the conductor knows how many were in this
907                 # batch.
908                 req_spec.num_instances = num_instances
909                 req_spec.create()
910 
911                 # Create an instance object, but do not store in db yet.
912                 instance = objects.Instance(context=context)
913                 instance.uuid = instance_uuid
914                 instance.update(base_options)
915                 instance.keypairs = objects.KeyPairList(objects=[])
916                 if key_pair:
917                     instance.keypairs.objects.append(key_pair)
918                 instance = self.create_db_entry_for_new_instance(context,
919                         instance_type, boot_meta, instance, security_groups,
920                         block_device_mapping, num_instances, i,
921                         shutdown_terminate, create_instance=False)
922                 block_device_mapping = (
923                     self._bdm_validate_set_size_and_instance(context,
924                         instance, instance_type, block_device_mapping))
925 
926                 build_request = objects.BuildRequest(context,
927                         instance=instance, instance_uuid=instance.uuid,
928                         project_id=instance.project_id,
929                         block_device_mappings=block_device_mapping)
930                 build_request.create()
931 
932                 # Create an instance_mapping.  The null cell_mapping indicates
933                 # that the instance doesn't yet exist in a cell, and lookups
934                 # for it need to instead look for the RequestSpec.
935                 # cell_mapping will be populated after scheduling, with a
936                 # scheduling failure using the cell_mapping for the special
937                 # cell0.
938                 inst_mapping = objects.InstanceMapping(context=context)
939                 inst_mapping.instance_uuid = instance_uuid
940                 inst_mapping.project_id = context.project_id
941                 inst_mapping.cell_mapping = None
942                 inst_mapping.create()
943 
944                 instances_to_build.append(
945                     (req_spec, build_request, inst_mapping))
946 
947                 if instance_group:
948                     if check_server_group_quota:
949                         try:
950                             objects.Quotas.check_deltas(
951                                 context, {'server_group_members': 1},
952                                 instance_group, context.user_id)
953                         except exception.OverQuota:
954                             msg = _("Quota exceeded, too many servers in "
955                                     "group")
956                             raise exception.QuotaError(msg)
957 
958                     members = objects.InstanceGroup.add_members(
959                         context, instance_group.uuid, [instance.uuid])
960 
961                     # NOTE(melwitt): We recheck the quota after creating the
962                     # object to prevent users from allocating more resources
963                     # than their allowed quota in the event of a race. This is
964                     # configurable because it can be expensive if strict quota
965                     # limits are not required in a deployment.
966                     if CONF.quota.recheck_quota and check_server_group_quota:
967                         try:
968                             objects.Quotas.check_deltas(
969                                 context, {'server_group_members': 0},
970                                 instance_group, context.user_id)
971                         except exception.OverQuota:
972                             objects.InstanceGroup._remove_members_in_db(
973                                 context, instance_group.id, [instance.uuid])
974                             msg = _("Quota exceeded, too many servers in "
975                                     "group")
976                             raise exception.QuotaError(msg)
977                     # list of members added to servers group in this iteration
978                     # is needed to check quota of server group during add next
979                     # instance
980                     instance_group.members.extend(members)
981 
982         # In the case of any exceptions, attempt DB cleanup
983         except Exception:
984             with excutils.save_and_reraise_exception():
985                 for rs, br, im in instances_to_build:
986                     try:
987                         rs.destroy()
988                     except exception.RequestSpecNotFound:
989                         pass
990                     try:
991                         im.destroy()
992                     except exception.InstanceMappingNotFound:
993                         pass
994                     try:
995                         br.destroy()
996                     except exception.BuildRequestNotFound:
997                         pass
998 
999         return instances_to_build
1000 
1001     def _get_bdm_image_metadata(self, context, block_device_mapping,
1002                                 legacy_bdm=True):
1003         """If we are booting from a volume, we need to get the
1004         volume details from Cinder and make sure we pass the
1005         metadata back accordingly.
1006         """
1007         if not block_device_mapping:
1008             return {}
1009 
1010         for bdm in block_device_mapping:
1011             if (legacy_bdm and
1012                     block_device.get_device_letter(
1013                        bdm.get('device_name', '')) != 'a'):
1014                 continue
1015             elif not legacy_bdm and bdm.get('boot_index') != 0:
1016                 continue
1017 
1018             volume_id = bdm.get('volume_id')
1019             snapshot_id = bdm.get('snapshot_id')
1020             if snapshot_id:
1021                 # NOTE(alaski): A volume snapshot inherits metadata from the
1022                 # originating volume, but the API does not expose metadata
1023                 # on the snapshot itself.  So we query the volume for it below.
1024                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1025                 volume_id = snapshot['volume_id']
1026 
1027             if bdm.get('image_id'):
1028                 try:
1029                     image_id = bdm['image_id']
1030                     image_meta = self.image_api.get(context, image_id)
1031                     return image_meta
1032                 except Exception:
1033                     raise exception.InvalidBDMImage(id=image_id)
1034             elif volume_id:
1035                 try:
1036                     volume = self.volume_api.get(context, volume_id)
1037                 except exception.CinderConnectionFailed:
1038                     raise
1039                 except Exception:
1040                     raise exception.InvalidBDMVolume(id=volume_id)
1041 
1042                 if not volume.get('bootable', True):
1043                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1044 
1045                 return utils.get_image_metadata_from_volume(volume)
1046         return {}
1047 
1048     @staticmethod
1049     def _get_requested_instance_group(context, filter_properties):
1050         if (not filter_properties or
1051                 not filter_properties.get('scheduler_hints')):
1052             return
1053 
1054         group_hint = filter_properties.get('scheduler_hints').get('group')
1055         if not group_hint:
1056             return
1057 
1058         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1059 
1060     def _create_instance(self, context, instance_type,
1061                image_href, kernel_id, ramdisk_id,
1062                min_count, max_count,
1063                display_name, display_description,
1064                key_name, key_data, security_groups,
1065                availability_zone, user_data, metadata, injected_files,
1066                admin_password, access_ip_v4, access_ip_v6,
1067                requested_networks, config_drive,
1068                block_device_mapping, auto_disk_config, filter_properties,
1069                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1070                check_server_group_quota=False):
1071         """Verify all the input parameters regardless of the provisioning
1072         strategy being performed and schedule the instance(s) for
1073         creation.
1074         """
1075 
1076         # Normalize and setup some parameters
1077         if reservation_id is None:
1078             reservation_id = utils.generate_uid('r')
1079         security_groups = security_groups or ['default']
1080         min_count = min_count or 1
1081         max_count = max_count or min_count
1082         block_device_mapping = block_device_mapping or []
1083 
1084         if image_href:
1085             image_id, boot_meta = self._get_image(context, image_href)
1086         else:
1087             image_id = None
1088             boot_meta = self._get_bdm_image_metadata(
1089                 context, block_device_mapping, legacy_bdm)
1090 
1091         self._check_auto_disk_config(image=boot_meta,
1092                                      auto_disk_config=auto_disk_config)
1093 
1094         base_options, max_net_count, key_pair, security_groups = \
1095                 self._validate_and_build_base_options(
1096                     context, instance_type, boot_meta, image_href, image_id,
1097                     kernel_id, ramdisk_id, display_name, display_description,
1098                     key_name, key_data, security_groups, availability_zone,
1099                     user_data, metadata, access_ip_v4, access_ip_v6,
1100                     requested_networks, config_drive, auto_disk_config,
1101                     reservation_id, max_count)
1102 
1103         # max_net_count is the maximum number of instances requested by the
1104         # user adjusted for any network quota constraints, including
1105         # consideration of connections to each requested network
1106         if max_net_count < min_count:
1107             raise exception.PortLimitExceeded()
1108         elif max_net_count < max_count:
1109             LOG.info("max count reduced from %(max_count)d to "
1110                      "%(max_net_count)d due to network port quota",
1111                      {'max_count': max_count,
1112                       'max_net_count': max_net_count})
1113             max_count = max_net_count
1114 
1115         block_device_mapping = self._check_and_transform_bdm(context,
1116             base_options, instance_type, boot_meta, min_count, max_count,
1117             block_device_mapping, legacy_bdm)
1118 
1119         # We can't do this check earlier because we need bdms from all sources
1120         # to have been merged in order to get the root bdm.
1121         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1122                 instance_type, metadata, injected_files,
1123                 block_device_mapping.root_bdm())
1124 
1125         instance_group = self._get_requested_instance_group(context,
1126                                    filter_properties)
1127 
1128         instances_to_build = self._provision_instances(context, instance_type,
1129                 min_count, max_count, base_options, boot_meta, security_groups,
1130                 block_device_mapping, shutdown_terminate,
1131                 instance_group, check_server_group_quota, filter_properties,
1132                 key_pair)
1133 
1134         instances = []
1135         request_specs = []
1136         build_requests = []
1137         for rs, build_request, im in instances_to_build:
1138             build_requests.append(build_request)
1139             instance = build_request.get_new_instance(context)
1140             instances.append(instance)
1141             request_specs.append(rs)
1142 
1143         if CONF.cells.enable:
1144             # NOTE(danms): CellsV1 can't do the new thing, so we
1145             # do the old thing here. We can remove this path once
1146             # we stop supporting v1.
1147             for instance in instances:
1148                 instance.create()
1149             # NOTE(melwitt): We recheck the quota after creating the objects
1150             # to prevent users from allocating more resources than their
1151             # allowed quota in the event of a race. This is configurable
1152             # because it can be expensive if strict quota limits are not
1153             # required in a deployment.
1154             if CONF.quota.recheck_quota:
1155                 try:
1156                     compute_utils.check_num_instances_quota(
1157                         context, instance_type, 0, 0,
1158                         orig_num_req=len(instances))
1159                 except exception.TooManyInstances:
1160                     with excutils.save_and_reraise_exception():
1161                         # Need to clean up all the instances we created
1162                         # along with the build requests, request specs,
1163                         # and instance mappings.
1164                         self._cleanup_build_artifacts(instances,
1165                                                       instances_to_build)
1166 
1167             self.compute_task_api.build_instances(context,
1168                 instances=instances, image=boot_meta,
1169                 filter_properties=filter_properties,
1170                 admin_password=admin_password,
1171                 injected_files=injected_files,
1172                 requested_networks=requested_networks,
1173                 security_groups=security_groups,
1174                 block_device_mapping=block_device_mapping,
1175                 legacy_bdm=False)
1176         else:
1177             self.compute_task_api.schedule_and_build_instances(
1178                 context,
1179                 build_requests=build_requests,
1180                 request_spec=request_specs,
1181                 image=boot_meta,
1182                 admin_password=admin_password,
1183                 injected_files=injected_files,
1184                 requested_networks=requested_networks,
1185                 block_device_mapping=block_device_mapping)
1186 
1187         return (instances, reservation_id)
1188 
1189     @staticmethod
1190     def _cleanup_build_artifacts(instances, instances_to_build):
1191         # instances_to_build is a list of tuples:
1192         # (RequestSpec, BuildRequest, InstanceMapping)
1193         for instance in instances:
1194             instance.destroy()
1195         for rs, build_request, im in instances_to_build:
1196             rs.destroy()
1197             build_request.destroy()
1198             im.destroy()
1199 
1200     @staticmethod
1201     def _volume_size(instance_type, bdm):
1202         size = bdm.get('volume_size')
1203         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1204         if (size is None and bdm.get('source_type') == 'blank' and
1205                 bdm.get('destination_type') == 'local'):
1206             if bdm.get('guest_format') == 'swap':
1207                 size = instance_type.get('swap', 0)
1208             else:
1209                 size = instance_type.get('ephemeral_gb', 0)
1210         return size
1211 
1212     def _prepare_image_mapping(self, instance_type, mappings):
1213         """Extract and format blank devices from image mappings."""
1214 
1215         prepared_mappings = []
1216 
1217         for bdm in block_device.mappings_prepend_dev(mappings):
1218             LOG.debug("Image bdm %s", bdm)
1219 
1220             virtual_name = bdm['virtual']
1221             if virtual_name == 'ami' or virtual_name == 'root':
1222                 continue
1223 
1224             if not block_device.is_swap_or_ephemeral(virtual_name):
1225                 continue
1226 
1227             guest_format = bdm.get('guest_format')
1228             if virtual_name == 'swap':
1229                 guest_format = 'swap'
1230             if not guest_format:
1231                 guest_format = CONF.default_ephemeral_format
1232 
1233             values = block_device.BlockDeviceDict({
1234                 'device_name': bdm['device'],
1235                 'source_type': 'blank',
1236                 'destination_type': 'local',
1237                 'device_type': 'disk',
1238                 'guest_format': guest_format,
1239                 'delete_on_termination': True,
1240                 'boot_index': -1})
1241 
1242             values['volume_size'] = self._volume_size(
1243                 instance_type, values)
1244             if values['volume_size'] == 0:
1245                 continue
1246 
1247             prepared_mappings.append(values)
1248 
1249         return prepared_mappings
1250 
1251     def _bdm_validate_set_size_and_instance(self, context, instance,
1252                                             instance_type,
1253                                             block_device_mapping):
1254         """Ensure the bdms are valid, then set size and associate with instance
1255 
1256         Because this method can be called multiple times when more than one
1257         instance is booted in a single request it makes a copy of the bdm list.
1258         """
1259         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1260                   instance_uuid=instance.uuid)
1261         self._validate_bdm(
1262             context, instance, instance_type, block_device_mapping)
1263         instance_block_device_mapping = block_device_mapping.obj_clone()
1264         for bdm in instance_block_device_mapping:
1265             bdm.volume_size = self._volume_size(instance_type, bdm)
1266             bdm.instance_uuid = instance.uuid
1267         return instance_block_device_mapping
1268 
1269     def _create_block_device_mapping(self, block_device_mapping):
1270         # Copy the block_device_mapping because this method can be called
1271         # multiple times when more than one instance is booted in a single
1272         # request. This avoids 'id' being set and triggering the object dupe
1273         # detection
1274         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1275         # Create the BlockDeviceMapping objects in the db.
1276         for bdm in db_block_device_mapping:
1277             # TODO(alaski): Why is this done?
1278             if bdm.volume_size == 0:
1279                 continue
1280 
1281             bdm.update_or_create()
1282 
1283     def _validate_bdm(self, context, instance, instance_type,
1284                       block_device_mappings):
1285         def _subsequent_list(l):
1286             # Each device which is capable of being used as boot device should
1287             # be given a unique boot index, starting from 0 in ascending order.
1288             return all(el + 1 == l[i + 1] for i, el in enumerate(l[:-1]))
1289 
1290         # Make sure that the boot indexes make sense.
1291         # Setting a negative value or None indicates that the device should not
1292         # be used for booting.
1293         boot_indexes = sorted([bdm.boot_index
1294                                for bdm in block_device_mappings
1295                                if bdm.boot_index is not None
1296                                and bdm.boot_index >= 0])
1297 
1298         if 0 not in boot_indexes or not _subsequent_list(boot_indexes):
1299             # Convert the BlockDeviceMappingList to a list for repr details.
1300             LOG.debug('Invalid block device mapping boot sequence for '
1301                       'instance: %s', list(block_device_mappings),
1302                       instance=instance)
1303             raise exception.InvalidBDMBootSequence()
1304 
1305         for bdm in block_device_mappings:
1306             # NOTE(vish): For now, just make sure the volumes are accessible.
1307             # Additionally, check that the volume can be attached to this
1308             # instance.
1309             snapshot_id = bdm.snapshot_id
1310             volume_id = bdm.volume_id
1311             image_id = bdm.image_id
1312             if (image_id is not None and
1313                     image_id != instance.get('image_ref')):
1314                 try:
1315                     self._get_image(context, image_id)
1316                 except Exception:
1317                     raise exception.InvalidBDMImage(id=image_id)
1318                 if (bdm.source_type == 'image' and
1319                         bdm.destination_type == 'volume' and
1320                         not bdm.volume_size):
1321                     raise exception.InvalidBDM(message=_("Images with "
1322                         "destination_type 'volume' need to have a non-zero "
1323                         "size specified"))
1324             elif volume_id is not None:
1325                 min_compute_version = objects.Service.get_minimum_version(
1326                     context, 'nova-compute')
1327                 try:
1328                     # NOTE(ildikov): The boot from volume operation did not
1329                     # reserve the volume before Pike and as the older computes
1330                     # are running 'check_attach' which will fail if the volume
1331                     # is in 'attaching' state; if the compute service version
1332                     # is not high enough we will just perform the old check as
1333                     # opposed to reserving the volume here.
1334                     if (min_compute_version >=
1335                         BFV_RESERVE_MIN_COMPUTE_VERSION):
1336                         volume = self._check_attach_and_reserve_volume(
1337                             context, volume_id, instance)
1338                     else:
1339                         # NOTE(ildikov): This call is here only for backward
1340                         # compatibility can be removed after Ocata EOL.
1341                         volume = self._check_attach(context, volume_id,
1342                                                     instance)
1343                     bdm.volume_size = volume.get('size')
1344                 except (exception.CinderConnectionFailed,
1345                         exception.InvalidVolume):
1346                     raise
1347                 except exception.InvalidInput as exc:
1348                     raise exception.InvalidVolume(reason=exc.format_message())
1349                 except Exception:
1350                     raise exception.InvalidBDMVolume(id=volume_id)
1351             elif snapshot_id is not None:
1352                 try:
1353                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1354                     bdm.volume_size = bdm.volume_size or snap.get('size')
1355                 except exception.CinderConnectionFailed:
1356                     raise
1357                 except Exception:
1358                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1359             elif (bdm.source_type == 'blank' and
1360                     bdm.destination_type == 'volume' and
1361                     not bdm.volume_size):
1362                 raise exception.InvalidBDM(message=_("Blank volumes "
1363                     "(source: 'blank', dest: 'volume') need to have non-zero "
1364                     "size"))
1365 
1366         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1367                 for bdm in block_device_mappings
1368                 if block_device.new_format_is_ephemeral(bdm))
1369         if ephemeral_size > instance_type['ephemeral_gb']:
1370             raise exception.InvalidBDMEphemeralSize()
1371 
1372         # There should be only one swap
1373         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1374         if len(swap_list) > 1:
1375             msg = _("More than one swap drive requested.")
1376             raise exception.InvalidBDMFormat(details=msg)
1377 
1378         if swap_list:
1379             swap_size = swap_list[0].volume_size or 0
1380             if swap_size > instance_type['swap']:
1381                 raise exception.InvalidBDMSwapSize()
1382 
1383         max_local = CONF.max_local_block_devices
1384         if max_local >= 0:
1385             num_local = len([bdm for bdm in block_device_mappings
1386                              if bdm.destination_type == 'local'])
1387             if num_local > max_local:
1388                 raise exception.InvalidBDMLocalsLimit()
1389 
1390     def _check_attach(self, context, volume_id, instance):
1391         # TODO(ildikov): This check_attach code is kept only for backward
1392         # compatibility and should be removed after Ocata EOL.
1393         volume = self.volume_api.get(context, volume_id)
1394         if volume['status'] != 'available':
1395             msg = _("volume '%(vol)s' status must be 'available'. Currently "
1396                     "in '%(status)s'") % {'vol': volume['id'],
1397                                           'status': volume['status']}
1398             raise exception.InvalidVolume(reason=msg)
1399         if volume['attach_status'] == 'attached':
1400             msg = _("volume %s already attached") % volume['id']
1401             raise exception.InvalidVolume(reason=msg)
1402         self.volume_api.check_availability_zone(context, volume,
1403                                                 instance=instance)
1404 
1405         return volume
1406 
1407     def _populate_instance_names(self, instance, num_instances):
1408         """Populate instance display_name and hostname."""
1409         display_name = instance.get('display_name')
1410         if instance.obj_attr_is_set('hostname'):
1411             hostname = instance.get('hostname')
1412         else:
1413             hostname = None
1414 
1415         # NOTE(mriedem): This is only here for test simplicity since a server
1416         # name is required in the REST API.
1417         if display_name is None:
1418             display_name = self._default_display_name(instance.uuid)
1419             instance.display_name = display_name
1420 
1421         if hostname is None and num_instances == 1:
1422             # NOTE(russellb) In the multi-instance case, we're going to
1423             # overwrite the display_name using the
1424             # multi_instance_display_name_template.  We need the default
1425             # display_name set so that it can be used in the template, though.
1426             # Only set the hostname here if we're only creating one instance.
1427             # Otherwise, it will be built after the template based
1428             # display_name.
1429             hostname = display_name
1430             default_hostname = self._default_host_name(instance.uuid)
1431             instance.hostname = utils.sanitize_hostname(hostname,
1432                                                         default_hostname)
1433 
1434     def _default_display_name(self, instance_uuid):
1435         return "Server %s" % instance_uuid
1436 
1437     def _default_host_name(self, instance_uuid):
1438         return "Server-%s" % instance_uuid
1439 
1440     def _populate_instance_for_create(self, context, instance, image,
1441                                       index, security_groups, instance_type,
1442                                       num_instances, shutdown_terminate):
1443         """Build the beginning of a new instance."""
1444 
1445         instance.launch_index = index
1446         instance.vm_state = vm_states.BUILDING
1447         instance.task_state = task_states.SCHEDULING
1448         info_cache = objects.InstanceInfoCache()
1449         info_cache.instance_uuid = instance.uuid
1450         info_cache.network_info = network_model.NetworkInfo()
1451         instance.info_cache = info_cache
1452         instance.flavor = instance_type
1453         instance.old_flavor = None
1454         instance.new_flavor = None
1455         if CONF.ephemeral_storage_encryption.enabled:
1456             # NOTE(kfarr): dm-crypt expects the cipher in a
1457             # hyphenated format: cipher-chainmode-ivmode
1458             # (ex: aes-xts-plain64). The algorithm needs
1459             # to be parsed out to pass to the key manager (ex: aes).
1460             cipher = CONF.ephemeral_storage_encryption.cipher
1461             algorithm = cipher.split('-')[0] if cipher else None
1462             instance.ephemeral_key_uuid = self.key_manager.create_key(
1463                 context,
1464                 algorithm=algorithm,
1465                 length=CONF.ephemeral_storage_encryption.key_size)
1466         else:
1467             instance.ephemeral_key_uuid = None
1468 
1469         # Store image properties so we can use them later
1470         # (for notifications, etc).  Only store what we can.
1471         if not instance.obj_attr_is_set('system_metadata'):
1472             instance.system_metadata = {}
1473         # Make sure we have the dict form that we need for instance_update.
1474         instance.system_metadata = utils.instance_sys_meta(instance)
1475 
1476         system_meta = utils.get_system_metadata_from_image(
1477             image, instance_type)
1478 
1479         # In case we couldn't find any suitable base_image
1480         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1481 
1482         system_meta['owner_user_name'] = context.user_name
1483         system_meta['owner_project_name'] = context.project_name
1484 
1485         instance.system_metadata.update(system_meta)
1486 
1487         if CONF.use_neutron:
1488             # For Neutron we don't actually store anything in the database, we
1489             # proxy the security groups on the instance from the ports
1490             # attached to the instance.
1491             instance.security_groups = objects.SecurityGroupList()
1492         else:
1493             instance.security_groups = security_groups
1494 
1495         self._populate_instance_names(instance, num_instances)
1496         instance.shutdown_terminate = shutdown_terminate
1497         if num_instances > 1 and self.cell_type != 'api':
1498             instance = self._apply_instance_name_template(context, instance,
1499                                                           index)
1500 
1501         return instance
1502 
1503     # This method remains because cellsv1 uses it in the scheduler
1504     def create_db_entry_for_new_instance(self, context, instance_type, image,
1505             instance, security_group, block_device_mapping, num_instances,
1506             index, shutdown_terminate=False, create_instance=True):
1507         """Create an entry in the DB for this new instance,
1508         including any related table updates (such as security group,
1509         etc).
1510 
1511         This is called by the scheduler after a location for the
1512         instance has been determined.
1513 
1514         :param create_instance: Determines if the instance is created here or
1515             just populated for later creation. This is done so that this code
1516             can be shared with cellsv1 which needs the instance creation to
1517             happen here. It should be removed and this method cleaned up when
1518             cellsv1 is a distant memory.
1519         """
1520         self._populate_instance_for_create(context, instance, image, index,
1521                                            security_group, instance_type,
1522                                            num_instances, shutdown_terminate)
1523 
1524         if create_instance:
1525             instance.create()
1526 
1527         return instance
1528 
1529     def _check_multiple_instances_with_neutron_ports(self,
1530                                                      requested_networks):
1531         """Check whether multiple instances are created from port id(s)."""
1532         for requested_net in requested_networks:
1533             if requested_net.port_id:
1534                 msg = _("Unable to launch multiple instances with"
1535                         " a single configured port ID. Please launch your"
1536                         " instance one by one with different ports.")
1537                 raise exception.MultiplePortsNotApplicable(reason=msg)
1538 
1539     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1540         """Check whether multiple instances are created with specified ip."""
1541 
1542         for requested_net in requested_networks:
1543             if requested_net.network_id and requested_net.address:
1544                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1545                         "is specified.")
1546                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1547 
1548     @hooks.add_hook("create_instance")
1549     def create(self, context, instance_type,
1550                image_href, kernel_id=None, ramdisk_id=None,
1551                min_count=None, max_count=None,
1552                display_name=None, display_description=None,
1553                key_name=None, key_data=None, security_groups=None,
1554                availability_zone=None, forced_host=None, forced_node=None,
1555                user_data=None, metadata=None, injected_files=None,
1556                admin_password=None, block_device_mapping=None,
1557                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1558                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1559                legacy_bdm=True, shutdown_terminate=False,
1560                check_server_group_quota=False):
1561         """Provision instances, sending instance information to the
1562         scheduler.  The scheduler will determine where the instance(s)
1563         go and will handle creating the DB entries.
1564 
1565         Returns a tuple of (instances, reservation_id)
1566         """
1567         if requested_networks and max_count is not None and max_count > 1:
1568             self._check_multiple_instances_with_specified_ip(
1569                 requested_networks)
1570             if utils.is_neutron():
1571                 self._check_multiple_instances_with_neutron_ports(
1572                     requested_networks)
1573 
1574         if availability_zone:
1575             available_zones = availability_zones.\
1576                 get_availability_zones(context.elevated(), True)
1577             if forced_host is None and availability_zone not in \
1578                     available_zones:
1579                 msg = _('The requested availability zone is not available')
1580                 raise exception.InvalidRequest(msg)
1581 
1582         filter_properties = scheduler_utils.build_filter_properties(
1583                 scheduler_hints, forced_host, forced_node, instance_type)
1584 
1585         return self._create_instance(
1586                        context, instance_type,
1587                        image_href, kernel_id, ramdisk_id,
1588                        min_count, max_count,
1589                        display_name, display_description,
1590                        key_name, key_data, security_groups,
1591                        availability_zone, user_data, metadata,
1592                        injected_files, admin_password,
1593                        access_ip_v4, access_ip_v6,
1594                        requested_networks, config_drive,
1595                        block_device_mapping, auto_disk_config,
1596                        filter_properties=filter_properties,
1597                        legacy_bdm=legacy_bdm,
1598                        shutdown_terminate=shutdown_terminate,
1599                        check_server_group_quota=check_server_group_quota)
1600 
1601     def _check_auto_disk_config(self, instance=None, image=None,
1602                                 **extra_instance_updates):
1603         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1604         if auto_disk_config is None:
1605             return
1606         if not image and not instance:
1607             return
1608 
1609         if image:
1610             image_props = image.get("properties", {})
1611             auto_disk_config_img = \
1612                 utils.get_auto_disk_config_from_image_props(image_props)
1613             image_ref = image.get("id")
1614         else:
1615             sys_meta = utils.instance_sys_meta(instance)
1616             image_ref = sys_meta.get('image_base_image_ref')
1617             auto_disk_config_img = \
1618                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1619 
1620         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1621                                                auto_disk_config,
1622                                                image_ref)
1623 
1624     def _lookup_instance(self, context, uuid):
1625         '''Helper method for pulling an instance object from a database.
1626 
1627         During the transition to cellsv2 there is some complexity around
1628         retrieving an instance from the database which this method hides. If
1629         there is an instance mapping then query the cell for the instance, if
1630         no mapping exists then query the configured nova database.
1631 
1632         Once we are past the point that all deployments can be assumed to be
1633         migrated to cellsv2 this method can go away.
1634         '''
1635         inst_map = None
1636         try:
1637             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1638                 context, uuid)
1639         except exception.InstanceMappingNotFound:
1640             # TODO(alaski): This exception block can be removed once we're
1641             # guaranteed everyone is using cellsv2.
1642             pass
1643 
1644         if (inst_map is None or inst_map.cell_mapping is None or
1645                 CONF.cells.enable):
1646             # If inst_map is None then the deployment has not migrated to
1647             # cellsv2 yet.
1648             # If inst_map.cell_mapping is None then the instance is not in a
1649             # cell yet. Until instance creation moves to the conductor the
1650             # instance can be found in the configured database, so attempt
1651             # to look it up.
1652             # If we're on cellsv1, we can't yet short-circuit the cells
1653             # messaging path
1654             cell = None
1655             try:
1656                 instance = objects.Instance.get_by_uuid(context, uuid)
1657             except exception.InstanceNotFound:
1658                 # If we get here then the conductor is in charge of writing the
1659                 # instance to the database and hasn't done that yet. It's up to
1660                 # the caller of this method to determine what to do with that
1661                 # information.
1662                 return None, None
1663         else:
1664             cell = inst_map.cell_mapping
1665             with nova_context.target_cell(context, cell) as cctxt:
1666                 try:
1667                     instance = objects.Instance.get_by_uuid(cctxt, uuid)
1668                 except exception.InstanceNotFound:
1669                     # Since the cell_mapping exists we know the instance is in
1670                     # the cell, however InstanceNotFound means it's already
1671                     # deleted.
1672                     return None, None
1673         return cell, instance
1674 
1675     def _delete_while_booting(self, context, instance):
1676         """Handle deletion if the instance has not reached a cell yet
1677 
1678         Deletion before an instance reaches a cell needs to be handled
1679         differently. What we're attempting to do is delete the BuildRequest
1680         before the api level conductor does.  If we succeed here then the boot
1681         request stops before reaching a cell.  If not then the instance will
1682         need to be looked up in a cell db and the normal delete path taken.
1683         """
1684         deleted = self._attempt_delete_of_buildrequest(context, instance)
1685 
1686         # After service version 15 deletion of the BuildRequest will halt the
1687         # build process in the conductor. In that case run the rest of this
1688         # method and consider the instance deleted. If we have not yet reached
1689         # service version 15 then just return False so the rest of the delete
1690         # process will proceed usually.
1691         service_version = objects.Service.get_minimum_version(
1692             context, 'nova-osapi_compute')
1693         if service_version < 15:
1694             return False
1695 
1696         if deleted:
1697             # If we've reached this block the successful deletion of the
1698             # buildrequest indicates that the build process should be halted by
1699             # the conductor.
1700 
1701             # NOTE(alaski): Though the conductor halts the build process it
1702             # does not currently delete the instance record. This is
1703             # because in the near future the instance record will not be
1704             # created if the buildrequest has been deleted here. For now we
1705             # ensure the instance has been set to deleted at this point.
1706             # Yes this directly contradicts the comment earlier in this
1707             # method, but this is a temporary measure.
1708             # Look up the instance because the current instance object was
1709             # stashed on the buildrequest and therefore not complete enough
1710             # to run .destroy().
1711             try:
1712                 instance_uuid = instance.uuid
1713                 cell, instance = self._lookup_instance(context, instance_uuid)
1714                 if instance is not None:
1715                     # If instance is None it has already been deleted.
1716                     if cell:
1717                         with nova_context.target_cell(context, cell) as cctxt:
1718                             # FIXME: When the instance context is targeted,
1719                             # we can remove this
1720                             with compute_utils.notify_about_instance_delete(
1721                                     self.notifier, cctxt, instance):
1722                                 instance.destroy()
1723                     else:
1724                         instance.destroy()
1725             except exception.InstanceNotFound:
1726                 pass
1727 
1728             return True
1729         return False
1730 
1731     def _attempt_delete_of_buildrequest(self, context, instance):
1732         # If there is a BuildRequest then the instance may not have been
1733         # written to a cell db yet. Delete the BuildRequest here, which
1734         # will indicate that the Instance build should not proceed.
1735         try:
1736             build_req = objects.BuildRequest.get_by_instance_uuid(
1737                 context, instance.uuid)
1738             build_req.destroy()
1739         except exception.BuildRequestNotFound:
1740             # This means that conductor has deleted the BuildRequest so the
1741             # instance is now in a cell and the delete needs to proceed
1742             # normally.
1743             return False
1744         return True
1745 
1746     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1747         if instance.disable_terminate:
1748             LOG.info('instance termination disabled', instance=instance)
1749             return
1750 
1751         cell = None
1752         # If there is an instance.host (or the instance is shelved-offloaded),
1753         # the instance has been scheduled and sent to a cell/compute which
1754         # means it was pulled from the cell db.
1755         # Normal delete should be attempted.
1756         if not (instance.host or
1757                 instance.vm_state == vm_states.SHELVED_OFFLOADED):
1758             try:
1759                 if self._delete_while_booting(context, instance):
1760                     return
1761                 # If instance.host was not set it's possible that the Instance
1762                 # object here was pulled from a BuildRequest object and is not
1763                 # fully populated. Notably it will be missing an 'id' field
1764                 # which will prevent instance.destroy from functioning
1765                 # properly. A lookup is attempted which will either return a
1766                 # full Instance or None if not found. If not found then it's
1767                 # acceptable to skip the rest of the delete processing.
1768                 cell, instance = self._lookup_instance(context, instance.uuid)
1769                 if cell and instance:
1770                     try:
1771                         # Now destroy the instance from the cell it lives in.
1772                         with compute_utils.notify_about_instance_delete(
1773                                 self.notifier, context, instance):
1774                             instance.destroy()
1775                     except exception.InstanceNotFound:
1776                         pass
1777                     # The instance was deleted or is already gone.
1778                     return
1779                 if not instance:
1780                     # Instance is already deleted.
1781                     return
1782             except exception.ObjectActionError:
1783                 # NOTE(melwitt): This means the instance.host changed
1784                 # under us indicating the instance became scheduled
1785                 # during the destroy(). Refresh the instance from the DB and
1786                 # continue on with the delete logic for a scheduled instance.
1787                 # NOTE(danms): If instance.host is set, we should be able to
1788                 # do the following lookup. If not, there's not much we can
1789                 # do to recover.
1790                 cell, instance = self._lookup_instance(context, instance.uuid)
1791                 if not instance:
1792                     # Instance is already deleted
1793                     return
1794 
1795         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1796                 context, instance.uuid)
1797 
1798         # At these states an instance has a snapshot associate.
1799         if instance.vm_state in (vm_states.SHELVED,
1800                                  vm_states.SHELVED_OFFLOADED):
1801             snapshot_id = instance.system_metadata.get('shelved_image_id')
1802             LOG.info("Working on deleting snapshot %s "
1803                      "from shelved instance...",
1804                      snapshot_id, instance=instance)
1805             try:
1806                 self.image_api.delete(context, snapshot_id)
1807             except (exception.ImageNotFound,
1808                     exception.ImageNotAuthorized) as exc:
1809                 LOG.warning("Failed to delete snapshot "
1810                             "from shelved instance (%s).",
1811                             exc.format_message(), instance=instance)
1812             except Exception:
1813                 LOG.exception("Something wrong happened when trying to "
1814                               "delete snapshot from shelved instance.",
1815                               instance=instance)
1816 
1817         original_task_state = instance.task_state
1818         try:
1819             # NOTE(maoy): no expected_task_state needs to be set
1820             instance.update(instance_attrs)
1821             instance.progress = 0
1822             instance.save()
1823 
1824             # NOTE(dtp): cells.enable = False means "use cells v2".
1825             # Run everywhere except v1 compute cells.
1826             if not CONF.cells.enable or self.cell_type == 'api':
1827                 self.consoleauth_rpcapi.delete_tokens_for_instance(
1828                     context, instance.uuid)
1829 
1830             if self.cell_type == 'api':
1831                 # NOTE(comstud): If we're in the API cell, we need to
1832                 # skip all remaining logic and just call the callback,
1833                 # which will cause a cast to the child cell.
1834                 cb(context, instance, bdms)
1835                 return
1836             shelved_offloaded = (instance.vm_state
1837                                  == vm_states.SHELVED_OFFLOADED)
1838             if not instance.host and not shelved_offloaded:
1839                 try:
1840                     compute_utils.notify_about_instance_usage(
1841                             self.notifier, context, instance,
1842                             "%s.start" % delete_type)
1843                     instance.destroy()
1844                     compute_utils.notify_about_instance_usage(
1845                             self.notifier, context, instance,
1846                             "%s.end" % delete_type,
1847                             system_metadata=instance.system_metadata)
1848                     LOG.info('Instance deleted and does not have host '
1849                              'field, its vm_state is %(state)s.',
1850                              {'state': instance.vm_state},
1851                               instance=instance)
1852                     return
1853                 except exception.ObjectActionError:
1854                     instance.refresh()
1855 
1856             if instance.vm_state == vm_states.RESIZED:
1857                 self._confirm_resize_on_deleting(context, instance)
1858 
1859             is_local_delete = True
1860             try:
1861                 if not shelved_offloaded:
1862                     service = objects.Service.get_by_compute_host(
1863                         context.elevated(), instance.host)
1864                     is_local_delete = not self.servicegroup_api.service_is_up(
1865                         service)
1866                 if not is_local_delete:
1867                     if original_task_state in (task_states.DELETING,
1868                                                   task_states.SOFT_DELETING):
1869                         LOG.info('Instance is already in deleting state, '
1870                                  'ignoring this request',
1871                                  instance=instance)
1872                         return
1873                     self._record_action_start(context, instance,
1874                                               instance_actions.DELETE)
1875 
1876                     cb(context, instance, bdms)
1877             except exception.ComputeHostNotFound:
1878                 pass
1879 
1880             if is_local_delete:
1881                 # If instance is in shelved_offloaded state or compute node
1882                 # isn't up, delete instance from db and clean bdms info and
1883                 # network info
1884                 if cell is None:
1885                     # NOTE(danms): If we didn't get our cell from one of the
1886                     # paths above, look it up now.
1887                     try:
1888                         im = objects.InstanceMapping.get_by_instance_uuid(
1889                             context, instance.uuid)
1890                         cell = im.cell_mapping
1891                     except exception.InstanceMappingNotFound:
1892                         LOG.warning('During local delete, failed to find '
1893                                     'instance mapping', instance=instance)
1894                         return
1895 
1896                 LOG.debug('Doing local delete in cell %s', cell.identity,
1897                           instance=instance)
1898                 with nova_context.target_cell(context, cell) as cctxt:
1899                     self._local_delete(cctxt, instance, bdms, delete_type, cb)
1900 
1901         except exception.InstanceNotFound:
1902             # NOTE(comstud): Race condition. Instance already gone.
1903             pass
1904 
1905     def _confirm_resize_on_deleting(self, context, instance):
1906         # If in the middle of a resize, use confirm_resize to
1907         # ensure the original instance is cleaned up too
1908         migration = None
1909         for status in ('finished', 'confirming'):
1910             try:
1911                 migration = objects.Migration.get_by_instance_and_status(
1912                         context.elevated(), instance.uuid, status)
1913                 LOG.info('Found an unconfirmed migration during delete, '
1914                          'id: %(id)s, status: %(status)s',
1915                          {'id': migration.id,
1916                           'status': migration.status},
1917                          instance=instance)
1918                 break
1919             except exception.MigrationNotFoundByStatus:
1920                 pass
1921 
1922         if not migration:
1923             LOG.info('Instance may have been confirmed during delete',
1924                      instance=instance)
1925             return
1926 
1927         src_host = migration.source_compute
1928 
1929         self._record_action_start(context, instance,
1930                                   instance_actions.CONFIRM_RESIZE)
1931 
1932         self.compute_rpcapi.confirm_resize(context,
1933                 instance, migration, src_host, cast=False)
1934 
1935     def _get_stashed_volume_connector(self, bdm, instance):
1936         """Lookup a connector dict from the bdm.connection_info if set
1937 
1938         Gets the stashed connector dict out of the bdm.connection_info if set
1939         and the connector host matches the instance host.
1940 
1941         :param bdm: nova.objects.block_device.BlockDeviceMapping
1942         :param instance: nova.objects.instance.Instance
1943         :returns: volume connector dict or None
1944         """
1945         if 'connection_info' in bdm and bdm.connection_info is not None:
1946             # NOTE(mriedem): We didn't start stashing the connector in the
1947             # bdm.connection_info until Mitaka so it might not be there on old
1948             # attachments. Also, if the volume was attached when the instance
1949             # was in shelved_offloaded state and it hasn't been unshelved yet
1950             # we don't have the attachment/connection information either.
1951             connector = jsonutils.loads(bdm.connection_info).get('connector')
1952             if connector:
1953                 if connector.get('host') == instance.host:
1954                     return connector
1955                 LOG.debug('Found stashed volume connector for instance but '
1956                           'connector host %(connector_host)s does not match '
1957                           'the instance host %(instance_host)s.',
1958                           {'connector_host': connector.get('host'),
1959                            'instance_host': instance.host}, instance=instance)
1960 
1961     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
1962         """The method deletes the bdm records and, if a bdm is a volume, call
1963         the terminate connection and the detach volume via the Volume API.
1964         """
1965         elevated = context.elevated()
1966         for bdm in bdms:
1967             if bdm.is_volume:
1968                 try:
1969                     if bdm.attachment_id:
1970                         self.volume_api.attachment_delete(context,
1971                                                           bdm.attachment_id)
1972                     else:
1973                         connector = self._get_stashed_volume_connector(
1974                             bdm, instance)
1975                         if connector:
1976                             self.volume_api.terminate_connection(context,
1977                                                                  bdm.volume_id,
1978                                                                  connector)
1979                         else:
1980                             LOG.debug('Unable to find connector for volume %s,'
1981                                       ' not attempting terminate_connection.',
1982                                       bdm.volume_id, instance=instance)
1983                         # Attempt to detach the volume. If there was no
1984                         # connection made in the first place this is just
1985                         # cleaning up the volume state in the Cinder DB.
1986                         self.volume_api.detach(elevated, bdm.volume_id,
1987                                                instance.uuid)
1988 
1989                     if bdm.delete_on_termination:
1990                         self.volume_api.delete(context, bdm.volume_id)
1991                 except Exception as exc:
1992                     LOG.warning("Ignoring volume cleanup failure due to %s",
1993                                 exc, instance=instance)
1994             bdm.destroy()
1995 
1996     def _local_delete(self, context, instance, bdms, delete_type, cb):
1997         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
1998             LOG.info("instance is in SHELVED_OFFLOADED state, cleanup"
1999                      " the instance's info from database.",
2000                      instance=instance)
2001         else:
2002             LOG.warning("instance's host %s is down, deleting from "
2003                         "database", instance.host, instance=instance)
2004         compute_utils.notify_about_instance_usage(
2005             self.notifier, context, instance, "%s.start" % delete_type)
2006 
2007         elevated = context.elevated()
2008         if self.cell_type != 'api':
2009             # NOTE(liusheng): In nova-network multi_host scenario,deleting
2010             # network info of the instance may need instance['host'] as
2011             # destination host of RPC call. If instance in SHELVED_OFFLOADED
2012             # state, instance['host'] is None, here, use shelved_host as host
2013             # to deallocate network info and reset instance['host'] after that.
2014             # Here we shouldn't use instance.save(), because this will mislead
2015             # user who may think the instance's host has been changed, and
2016             # actually, the instance.host is always None.
2017             orig_host = instance.host
2018             try:
2019                 if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2020                     sysmeta = getattr(instance,
2021                                       obj_base.get_attrname('system_metadata'))
2022                     instance.host = sysmeta.get('shelved_host')
2023                 self.network_api.deallocate_for_instance(elevated,
2024                                                          instance)
2025             finally:
2026                 instance.host = orig_host
2027 
2028         # cleanup volumes
2029         self._local_cleanup_bdm_volumes(bdms, instance, context)
2030         cb(context, instance, bdms, local=True)
2031         sys_meta = instance.system_metadata
2032         instance.destroy()
2033         compute_utils.notify_about_instance_usage(
2034             self.notifier, context, instance, "%s.end" % delete_type,
2035             system_metadata=sys_meta)
2036 
2037     def _do_delete(self, context, instance, bdms, local=False):
2038         if local:
2039             instance.vm_state = vm_states.DELETED
2040             instance.task_state = None
2041             instance.terminated_at = timeutils.utcnow()
2042             instance.save()
2043         else:
2044             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2045                                                    delete_type='delete')
2046 
2047     def _do_force_delete(self, context, instance, bdms, local=False):
2048         if local:
2049             instance.vm_state = vm_states.DELETED
2050             instance.task_state = None
2051             instance.terminated_at = timeutils.utcnow()
2052             instance.save()
2053         else:
2054             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2055                                                    delete_type='force_delete')
2056 
2057     def _do_soft_delete(self, context, instance, bdms, local=False):
2058         if local:
2059             instance.vm_state = vm_states.SOFT_DELETED
2060             instance.task_state = None
2061             instance.terminated_at = timeutils.utcnow()
2062             instance.save()
2063         else:
2064             self.compute_rpcapi.soft_delete_instance(context, instance)
2065 
2066     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2067     @check_instance_lock
2068     @check_instance_cell
2069     @check_instance_state(vm_state=None, task_state=None,
2070                           must_have_launched=True)
2071     def soft_delete(self, context, instance):
2072         """Terminate an instance."""
2073         LOG.debug('Going to try to soft delete instance',
2074                   instance=instance)
2075 
2076         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2077                      task_state=task_states.SOFT_DELETING,
2078                      deleted_at=timeutils.utcnow())
2079 
2080     def _delete_instance(self, context, instance):
2081         self._delete(context, instance, 'delete', self._do_delete,
2082                      task_state=task_states.DELETING)
2083 
2084     @check_instance_lock
2085     @check_instance_cell
2086     @check_instance_state(vm_state=None, task_state=None,
2087                           must_have_launched=False)
2088     def delete(self, context, instance):
2089         """Terminate an instance."""
2090         LOG.debug("Going to try to terminate instance", instance=instance)
2091         self._delete_instance(context, instance)
2092 
2093     @check_instance_lock
2094     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2095     def restore(self, context, instance):
2096         """Restore a previously deleted (but not reclaimed) instance."""
2097         # Check quotas
2098         # TODO(melwitt): We're not rechecking for strict quota here to guard
2099         # against going over quota during a race at this time because the
2100         # resource consumption for this operation is written to the database
2101         # by compute.
2102         flavor = instance.get_flavor()
2103         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2104         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
2105                 project_id=project_id, user_id=user_id)
2106 
2107         self._record_action_start(context, instance, instance_actions.RESTORE)
2108 
2109         if instance.host:
2110             instance.task_state = task_states.RESTORING
2111             instance.deleted_at = None
2112             instance.save(expected_task_state=[None])
2113             self.compute_rpcapi.restore_instance(context, instance)
2114         else:
2115             instance.vm_state = vm_states.ACTIVE
2116             instance.task_state = None
2117             instance.deleted_at = None
2118             instance.save(expected_task_state=[None])
2119 
2120     @check_instance_lock
2121     @check_instance_state(must_have_launched=False)
2122     def force_delete(self, context, instance):
2123         """Force delete an instance in any vm_state/task_state."""
2124         self._delete(context, instance, 'force_delete', self._do_force_delete,
2125                      task_state=task_states.DELETING)
2126 
2127     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2128         LOG.debug("Going to try to stop instance", instance=instance)
2129 
2130         instance.task_state = task_states.POWERING_OFF
2131         instance.progress = 0
2132         instance.save(expected_task_state=[None])
2133 
2134         self._record_action_start(context, instance, instance_actions.STOP)
2135 
2136         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2137                                           clean_shutdown=clean_shutdown)
2138 
2139     @check_instance_lock
2140     @check_instance_host
2141     @check_instance_cell
2142     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2143     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2144         """Stop an instance."""
2145         self.force_stop(context, instance, do_cast, clean_shutdown)
2146 
2147     @check_instance_lock
2148     @check_instance_host
2149     @check_instance_cell
2150     @check_instance_state(vm_state=[vm_states.STOPPED])
2151     def start(self, context, instance):
2152         """Start an instance."""
2153         LOG.debug("Going to try to start instance", instance=instance)
2154 
2155         instance.task_state = task_states.POWERING_ON
2156         instance.save(expected_task_state=[None])
2157 
2158         self._record_action_start(context, instance, instance_actions.START)
2159         # TODO(yamahata): injected_files isn't supported right now.
2160         #                 It is used only for osapi. not for ec2 api.
2161         #                 availability_zone isn't used by run_instance.
2162         self.compute_rpcapi.start_instance(context, instance)
2163 
2164     @check_instance_lock
2165     @check_instance_host
2166     @check_instance_cell
2167     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2168     def trigger_crash_dump(self, context, instance):
2169         """Trigger crash dump in an instance."""
2170         LOG.debug("Try to trigger crash dump", instance=instance)
2171 
2172         self._record_action_start(context, instance,
2173                                   instance_actions.TRIGGER_CRASH_DUMP)
2174 
2175         self.compute_rpcapi.trigger_crash_dump(context, instance)
2176 
2177     def _get_instance_map_or_none(self, context, instance_uuid):
2178         try:
2179             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2180                     context, instance_uuid)
2181         except exception.InstanceMappingNotFound:
2182             # InstanceMapping should always be found generally. This exception
2183             # may be raised if a deployment has partially migrated the nova-api
2184             # services.
2185             inst_map = None
2186         return inst_map
2187 
2188     def _get_instance(self, context, instance_uuid, expected_attrs):
2189         # Before service version 15 the BuildRequest is not cleaned up during
2190         # a delete request so there is no reason to look it up here as we can't
2191         # trust that it's not referencing a deleted instance. Also even if
2192         # there is an instance mapping we don't need to honor it for older
2193         # service versions.
2194         service_version = objects.Service.get_minimum_version(
2195             context, 'nova-osapi_compute')
2196         # If we're on cellsv1, we also need to consult the top-level
2197         # merged replica instead of the cell directly, so fall through
2198         # here in that case as well.
2199         if service_version < 15 or CONF.cells.enable:
2200             return objects.Instance.get_by_uuid(context, instance_uuid,
2201                                                 expected_attrs=expected_attrs)
2202         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2203         if inst_map and (inst_map.cell_mapping is not None):
2204             nova_context.set_target_cell(context, inst_map.cell_mapping)
2205             instance = objects.Instance.get_by_uuid(
2206                 context, instance_uuid, expected_attrs=expected_attrs)
2207         elif inst_map and (inst_map.cell_mapping is None):
2208             # This means the instance has not been scheduled and put in
2209             # a cell yet. For now it also may mean that the deployer
2210             # has not created their cell(s) yet.
2211             try:
2212                 build_req = objects.BuildRequest.get_by_instance_uuid(
2213                         context, instance_uuid)
2214                 instance = build_req.instance
2215             except exception.BuildRequestNotFound:
2216                 # Instance was mapped and the BuildRequest was deleted
2217                 # while fetching. Try again.
2218                 inst_map = self._get_instance_map_or_none(context,
2219                                                           instance_uuid)
2220                 if inst_map and (inst_map.cell_mapping is not None):
2221                     nova_context.set_target_cell(context,
2222                                                  inst_map.cell_mapping)
2223                     instance = objects.Instance.get_by_uuid(
2224                         context, instance_uuid,
2225                         expected_attrs=expected_attrs)
2226                 else:
2227                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2228         else:
2229             raise exception.InstanceNotFound(instance_id=instance_uuid)
2230 
2231         return instance
2232 
2233     def get(self, context, instance_id, expected_attrs=None):
2234         """Get a single instance with the given instance_id."""
2235         if not expected_attrs:
2236             expected_attrs = []
2237         expected_attrs.extend(['metadata', 'system_metadata',
2238                                'security_groups', 'info_cache'])
2239         # NOTE(ameade): we still need to support integer ids for ec2
2240         try:
2241             if uuidutils.is_uuid_like(instance_id):
2242                 LOG.debug("Fetching instance by UUID",
2243                            instance_uuid=instance_id)
2244 
2245                 instance = self._get_instance(context, instance_id,
2246                                               expected_attrs)
2247             else:
2248                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2249                 raise exception.InstanceNotFound(instance_id=instance_id)
2250         except exception.InvalidID:
2251             LOG.debug("Invalid instance id %s", instance_id)
2252             raise exception.InstanceNotFound(instance_id=instance_id)
2253 
2254         return instance
2255 
2256     def get_all(self, context, search_opts=None, limit=None, marker=None,
2257                 expected_attrs=None, sort_keys=None, sort_dirs=None):
2258         """Get all instances filtered by one of the given parameters.
2259 
2260         If there is no filter and the context is an admin, it will retrieve
2261         all instances in the system.
2262 
2263         Deleted instances will be returned by default, unless there is a
2264         search option that says otherwise.
2265 
2266         The results will be sorted based on the list of sort keys in the
2267         'sort_keys' parameter (first value is primary sort key, second value is
2268         secondary sort ket, etc.). For each sort key, the associated sort
2269         direction is based on the list of sort directions in the 'sort_dirs'
2270         parameter.
2271         """
2272         if search_opts is None:
2273             search_opts = {}
2274 
2275         LOG.debug("Searching by: %s", str(search_opts))
2276 
2277         # Fixups for the DB call
2278         filters = {}
2279 
2280         def _remap_flavor_filter(flavor_id):
2281             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2282             filters['instance_type_id'] = flavor.id
2283 
2284         def _remap_fixed_ip_filter(fixed_ip):
2285             # Turn fixed_ip into a regexp match. Since '.' matches
2286             # any character, we need to use regexp escaping for it.
2287             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2288 
2289         def _remap_metadata_filter(metadata):
2290             filters['metadata'] = jsonutils.loads(metadata)
2291 
2292         def _remap_system_metadata_filter(metadata):
2293             filters['system_metadata'] = jsonutils.loads(metadata)
2294 
2295         # search_option to filter_name mapping.
2296         filter_mapping = {
2297                 'image': 'image_ref',
2298                 'name': 'display_name',
2299                 'tenant_id': 'project_id',
2300                 'flavor': _remap_flavor_filter,
2301                 'fixed_ip': _remap_fixed_ip_filter,
2302                 'metadata': _remap_metadata_filter,
2303                 'system_metadata': _remap_system_metadata_filter}
2304 
2305         # copy from search_opts, doing various remappings as necessary
2306         for opt, value in search_opts.items():
2307             # Do remappings.
2308             # Values not in the filter_mapping table are copied as-is.
2309             # If remapping is None, option is not copied
2310             # If the remapping is a string, it is the filter_name to use
2311             try:
2312                 remap_object = filter_mapping[opt]
2313             except KeyError:
2314                 filters[opt] = value
2315             else:
2316                 # Remaps are strings to translate to, or functions to call
2317                 # to do the translating as defined by the table above.
2318                 if isinstance(remap_object, six.string_types):
2319                     filters[remap_object] = value
2320                 else:
2321                     try:
2322                         remap_object(value)
2323 
2324                     # We already know we can't match the filter, so
2325                     # return an empty list
2326                     except ValueError:
2327                         return objects.InstanceList()
2328 
2329         # IP address filtering cannot be applied at the DB layer, remove any DB
2330         # limit so that it can be applied after the IP filter.
2331         filter_ip = 'ip6' in filters or 'ip' in filters
2332         orig_limit = limit
2333         if filter_ip and limit:
2334             LOG.debug('Removing limit for DB query due to IP filter')
2335             limit = None
2336 
2337         # The ordering of instances will be
2338         # [sorted instances with no host] + [sorted instances with host].
2339         # This means BuildRequest and cell0 instances first, then cell
2340         # instances
2341         build_requests = objects.BuildRequestList.get_by_filters(
2342             context, filters, limit=limit, marker=marker, sort_keys=sort_keys,
2343             sort_dirs=sort_dirs)
2344         build_req_instances = objects.InstanceList(
2345             objects=[build_req.instance for build_req in build_requests])
2346         # Only subtract from limit if it is not None
2347         limit = (limit - len(build_req_instances)) if limit else limit
2348 
2349         try:
2350             cell0_mapping = objects.CellMapping.get_by_uuid(context,
2351                 objects.CellMapping.CELL0_UUID)
2352         except exception.CellMappingNotFound:
2353             cell0_instances = objects.InstanceList(objects=[])
2354         else:
2355             with nova_context.target_cell(context, cell0_mapping) as cctxt:
2356                 try:
2357                     cell0_instances = self._get_instances_by_filters(
2358                         cctxt, filters, limit=limit, marker=marker,
2359                         expected_attrs=expected_attrs, sort_keys=sort_keys,
2360                         sort_dirs=sort_dirs)
2361                     # If we found the marker in cell0 we need to set it to None
2362                     # so we don't expect to find it in the cells below.
2363                     marker = None
2364                 except exception.MarkerNotFound:
2365                     # We can ignore this since we need to look in the cell DB
2366                     cell0_instances = objects.InstanceList(objects=[])
2367         # Only subtract from limit if it is not None
2368         limit = (limit - len(cell0_instances)) if limit else limit
2369 
2370         # There is only planned support for a single cell here. Multiple cell
2371         # instance lists should be proxied to project Searchlight, or a similar
2372         # alternative.
2373         if limit is None or limit > 0:
2374             if not CONF.cells.enable:
2375                 cell_instances = self._get_instances_by_filters_all_cells(
2376                         context, filters,
2377                         limit=limit, marker=marker,
2378                         expected_attrs=expected_attrs, sort_keys=sort_keys,
2379                         sort_dirs=sort_dirs)
2380             else:
2381                 # NOTE(melwitt): If we're on cells v1, we need to read
2382                 # instances from the top-level database because reading from
2383                 # cells results in changed behavior, because of the syncing.
2384                 # We can remove this path once we stop supporting cells v1.
2385                 cell_instances = self._get_instances_by_filters(
2386                     context, filters, limit=limit, marker=marker,
2387                     expected_attrs=expected_attrs, sort_keys=sort_keys,
2388                     sort_dirs=sort_dirs)
2389         else:
2390             LOG.debug('Limit excludes any results from real cells')
2391             cell_instances = objects.InstanceList(objects=[])
2392 
2393         def _get_unique_filter_method():
2394             seen_uuids = set()
2395 
2396             def _filter(instance):
2397                 if instance.uuid in seen_uuids:
2398                     return False
2399                 seen_uuids.add(instance.uuid)
2400                 return True
2401 
2402             return _filter
2403 
2404         filter_method = _get_unique_filter_method()
2405         # Only subtract from limit if it is not None
2406         limit = (limit - len(cell_instances)) if limit else limit
2407         # TODO(alaski): Clean up the objects concatenation when List objects
2408         # support it natively.
2409         instances = objects.InstanceList(
2410             objects=list(filter(filter_method,
2411                            build_req_instances.objects +
2412                            cell0_instances.objects +
2413                            cell_instances.objects)))
2414 
2415         if filter_ip:
2416             instances = self._ip_filter(instances, filters, orig_limit)
2417 
2418         return instances
2419 
2420     @staticmethod
2421     def _ip_filter(inst_models, filters, limit):
2422         ipv4_f = re.compile(str(filters.get('ip')))
2423         ipv6_f = re.compile(str(filters.get('ip6')))
2424 
2425         def _match_instance(instance):
2426             nw_info = instance.get_network_info()
2427             for vif in nw_info:
2428                 for fixed_ip in vif.fixed_ips():
2429                     address = fixed_ip.get('address')
2430                     if not address:
2431                         continue
2432                     version = fixed_ip.get('version')
2433                     if ((version == 4 and ipv4_f.match(address)) or
2434                         (version == 6 and ipv6_f.match(address))):
2435                         return True
2436             return False
2437 
2438         result_objs = []
2439         for instance in inst_models:
2440             if _match_instance(instance):
2441                 result_objs.append(instance)
2442                 if limit and len(result_objs) == limit:
2443                     break
2444         return objects.InstanceList(objects=result_objs)
2445 
2446     def _get_instances_by_filters_all_cells(self, context, *args, **kwargs):
2447         """This is just a wrapper that iterates (non-zero) cells."""
2448         load_cells()
2449         if len(CELLS) == 1:
2450             # We always expect at least two cells; one for cell0 and one for at
2451             # least a single main cell. If there is only one cell it indicates
2452             # that nova-api was started before all of the cells were mapped and
2453             # we should provide a warning to the operator.
2454             LOG.warning('At least two cells are expected but only one '
2455                         'was found (%s). cell0 and the initial main cell '
2456                         'should be created before starting nova-api since '
2457                         'the cells are cached in each worker. When you '
2458                         'create more cells, you will need to restart the '
2459                         'nova-api service to reset the cache.',
2460                         CELLS[0].identity)
2461 
2462         limit = kwargs.pop('limit', None)
2463 
2464         instances = []
2465         for cell in CELLS:
2466             if cell.uuid == objects.CellMapping.CELL0_UUID:
2467                 LOG.debug('Skipping already-collected cell0 list')
2468                 continue
2469             LOG.debug('Listing %s instances in cell %s',
2470                       limit or 'all', cell.identity)
2471             with nova_context.target_cell(context, cell) as ccontext:
2472                 try:
2473                     cell_insts = self._get_instances_by_filters(ccontext,
2474                                                                 *args,
2475                                                                 limit=limit,
2476                                                                 **kwargs)
2477                 except exception.MarkerNotFound:
2478                     # NOTE(danms): We need to keep looking through the
2479                     # later cells to find the marker
2480                     continue
2481                 instances.extend(cell_insts)
2482                 # NOTE(danms): We must have found a marker if we had one,
2483                 # so make sure we don't require a marker in the next cell
2484                 kwargs['marker'] = None
2485                 if limit:
2486                     limit -= len(cell_insts)
2487                     if limit <= 0:
2488                         break
2489 
2490         marker = kwargs.get('marker')
2491         if marker is not None and len(instances) == 0:
2492             # NOTE(danms): If we did not find the marker in any cell,
2493             # mimic the db_api behavior here.
2494             raise exception.MarkerNotFound(marker=marker)
2495 
2496         return objects.InstanceList(objects=instances)
2497 
2498     def _get_instances_by_filters(self, context, filters,
2499                                   limit=None, marker=None, expected_attrs=None,
2500                                   sort_keys=None, sort_dirs=None):
2501         fields = ['metadata', 'system_metadata', 'info_cache',
2502                   'security_groups']
2503         if expected_attrs:
2504             fields.extend(expected_attrs)
2505         return objects.InstanceList.get_by_filters(
2506             context, filters=filters, limit=limit, marker=marker,
2507             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2508 
2509     def update_instance(self, context, instance, updates):
2510         """Updates a single Instance object with some updates dict.
2511 
2512         Returns the updated instance.
2513         """
2514 
2515         # NOTE(sbauza): Given we only persist the Instance object after we
2516         # create the BuildRequest, we are sure that if the Instance object
2517         # has an ID field set, then it was persisted in the right Cell DB.
2518         if instance.obj_attr_is_set('id'):
2519             instance.update(updates)
2520             # Instance has been scheduled and the BuildRequest has been deleted
2521             # we can directly write the update down to the right cell.
2522             inst_map = self._get_instance_map_or_none(context, instance.uuid)
2523             # If we have a cell_mapping and we're not on cells v1, then
2524             # look up the instance in the cell database
2525             if inst_map and (inst_map.cell_mapping is not None) and (
2526                     not CONF.cells.enable):
2527                 with nova_context.target_cell(context,
2528                                               inst_map.cell_mapping) as cctxt:
2529                     with instance.obj_alternate_context(cctxt):
2530                         instance.save()
2531             else:
2532                 # If inst_map.cell_mapping does not point at a cell then cell
2533                 # migration has not happened yet.
2534                 # TODO(alaski): Make this a failure case after we put in
2535                 # a block that requires migrating to cellsv2.
2536                 instance.save()
2537         else:
2538             # Instance is not yet mapped to a cell, so we need to update
2539             # BuildRequest instead
2540             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2541             # could be deleted because of either a concurrent instance delete
2542             # or because the scheduler just returned a destination right
2543             # after we called the instance in the API.
2544             try:
2545                 build_req = objects.BuildRequest.get_by_instance_uuid(
2546                     context, instance.uuid)
2547                 instance = build_req.instance
2548                 instance.update(updates)
2549                 # FIXME(sbauza): Here we are updating the current
2550                 # thread-related BuildRequest object. Given that another worker
2551                 # could have looking up at that BuildRequest in the API, it
2552                 # means that it could pass it down to the conductor without
2553                 # making sure that it's not updated, we could have some race
2554                 # condition where it would missing the updated fields, but
2555                 # that's something we could discuss once the instance record
2556                 # is persisted by the conductor.
2557                 build_req.save()
2558             except exception.BuildRequestNotFound:
2559                 # Instance was mapped and the BuildRequest was deleted
2560                 # while fetching (and possibly the instance could have been
2561                 # deleted as well). We need to lookup again the Instance object
2562                 # in order to correctly update it.
2563                 # TODO(sbauza): Figure out a good way to know the expected
2564                 # attributes by checking which fields are set or not.
2565                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2566                                   'tags', 'metadata', 'system_metadata',
2567                                   'security_groups', 'info_cache']
2568                 inst_map = self._get_instance_map_or_none(context,
2569                                                           instance.uuid)
2570                 if inst_map and (inst_map.cell_mapping is not None):
2571                     with nova_context.target_cell(
2572                             context,
2573                             inst_map.cell_mapping) as cctxt:
2574                         instance = objects.Instance.get_by_uuid(
2575                             cctxt, instance.uuid,
2576                             expected_attrs=expected_attrs)
2577                         instance.update(updates)
2578                         instance.save()
2579                 else:
2580                     # If inst_map.cell_mapping does not point at a cell then
2581                     # cell migration has not happened yet.
2582                     # TODO(alaski): Make this a failure case after we put in
2583                     # a block that requires migrating to cellsv2.
2584                     instance = objects.Instance.get_by_uuid(
2585                         context, instance.uuid, expected_attrs=expected_attrs)
2586                     instance.update(updates)
2587                     instance.save()
2588         return instance
2589 
2590     # NOTE(melwitt): We don't check instance lock for backup because lock is
2591     #                intended to prevent accidental change/delete of instances
2592     @check_instance_cell
2593     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2594                                     vm_states.PAUSED, vm_states.SUSPENDED])
2595     def backup(self, context, instance, name, backup_type, rotation,
2596                extra_properties=None):
2597         """Backup the given instance
2598 
2599         :param instance: nova.objects.instance.Instance object
2600         :param name: name of the backup
2601         :param backup_type: 'daily' or 'weekly'
2602         :param rotation: int representing how many backups to keep around;
2603             None if rotation shouldn't be used (as in the case of snapshots)
2604         :param extra_properties: dict of extra image properties to include
2605                                  when creating the image.
2606         :returns: A dict containing image metadata
2607         """
2608         props_copy = dict(extra_properties, backup_type=backup_type)
2609 
2610         if compute_utils.is_volume_backed_instance(context, instance):
2611             LOG.info("It's not supported to backup volume backed "
2612                      "instance.", instance=instance)
2613             raise exception.InvalidRequest(
2614                 _('Backup is not supported for volume-backed instances.'))
2615         else:
2616             image_meta = self._create_image(context, instance,
2617                                             name, 'backup',
2618                                             extra_properties=props_copy)
2619 
2620         # NOTE(comstud): Any changes to this method should also be made
2621         # to the backup_instance() method in nova/cells/messaging.py
2622 
2623         instance.task_state = task_states.IMAGE_BACKUP
2624         instance.save(expected_task_state=[None])
2625 
2626         self.compute_rpcapi.backup_instance(context, instance,
2627                                             image_meta['id'],
2628                                             backup_type,
2629                                             rotation)
2630         return image_meta
2631 
2632     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2633     #                intended to prevent accidental change/delete of instances
2634     @check_instance_cell
2635     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2636                                     vm_states.PAUSED, vm_states.SUSPENDED])
2637     def snapshot(self, context, instance, name, extra_properties=None):
2638         """Snapshot the given instance.
2639 
2640         :param instance: nova.objects.instance.Instance object
2641         :param name: name of the snapshot
2642         :param extra_properties: dict of extra image properties to include
2643                                  when creating the image.
2644         :returns: A dict containing image metadata
2645         """
2646         image_meta = self._create_image(context, instance, name,
2647                                         'snapshot',
2648                                         extra_properties=extra_properties)
2649 
2650         # NOTE(comstud): Any changes to this method should also be made
2651         # to the snapshot_instance() method in nova/cells/messaging.py
2652         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2653         try:
2654             instance.save(expected_task_state=[None])
2655         except (exception.InstanceNotFound,
2656                 exception.UnexpectedDeletingTaskStateError) as ex:
2657             # Changing the instance task state to use in raising the
2658             # InstanceInvalidException below
2659             LOG.debug('Instance disappeared during snapshot.',
2660                       instance=instance)
2661             try:
2662                 image_id = image_meta['id']
2663                 self.image_api.delete(context, image_id)
2664                 LOG.info('Image %s deleted because instance '
2665                          'deleted before snapshot started.',
2666                          image_id, instance=instance)
2667             except exception.ImageNotFound:
2668                 pass
2669             except Exception as exc:
2670                 LOG.warning("Error while trying to clean up image %(img_id)s: "
2671                             "%(error_msg)s",
2672                             {"img_id": image_meta['id'],
2673                              "error_msg": six.text_type(exc)})
2674             attr = 'task_state'
2675             state = task_states.DELETING
2676             if type(ex) == exception.InstanceNotFound:
2677                 attr = 'vm_state'
2678                 state = vm_states.DELETED
2679             raise exception.InstanceInvalidState(attr=attr,
2680                                            instance_uuid=instance.uuid,
2681                                            state=state,
2682                                            method='snapshot')
2683 
2684         self.compute_rpcapi.snapshot_instance(context, instance,
2685                                               image_meta['id'])
2686 
2687         return image_meta
2688 
2689     def _create_image(self, context, instance, name, image_type,
2690                       extra_properties=None):
2691         """Create new image entry in the image service.  This new image
2692         will be reserved for the compute manager to upload a snapshot
2693         or backup.
2694 
2695         :param context: security context
2696         :param instance: nova.objects.instance.Instance object
2697         :param name: string for name of the snapshot
2698         :param image_type: snapshot | backup
2699         :param extra_properties: dict of extra image properties to include
2700 
2701         """
2702         properties = {
2703             'instance_uuid': instance.uuid,
2704             'user_id': str(context.user_id),
2705             'image_type': image_type,
2706         }
2707         properties.update(extra_properties or {})
2708 
2709         image_meta = self._initialize_instance_snapshot_metadata(
2710             instance, name, properties)
2711         # if we're making a snapshot, omit the disk and container formats,
2712         # since the image may have been converted to another format, and the
2713         # original values won't be accurate.  The driver will populate these
2714         # with the correct values later, on image upload.
2715         if image_type == 'snapshot':
2716             image_meta.pop('disk_format', None)
2717             image_meta.pop('container_format', None)
2718         return self.image_api.create(context, image_meta)
2719 
2720     def _initialize_instance_snapshot_metadata(self, instance, name,
2721                                                extra_properties=None):
2722         """Initialize new metadata for a snapshot of the given instance.
2723 
2724         :param instance: nova.objects.instance.Instance object
2725         :param name: string for name of the snapshot
2726         :param extra_properties: dict of extra metadata properties to include
2727 
2728         :returns: the new instance snapshot metadata
2729         """
2730         image_meta = utils.get_image_from_system_metadata(
2731             instance.system_metadata)
2732         image_meta.update({'name': name,
2733                            'is_public': False})
2734 
2735         # Delete properties that are non-inheritable
2736         properties = image_meta['properties']
2737         for key in CONF.non_inheritable_image_properties:
2738             properties.pop(key, None)
2739 
2740         # The properties in extra_properties have precedence
2741         properties.update(extra_properties or {})
2742 
2743         return image_meta
2744 
2745     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2746     #                intended to prevent accidental change/delete of instances
2747     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2748                                     vm_states.SUSPENDED])
2749     def snapshot_volume_backed(self, context, instance, name,
2750                                extra_properties=None):
2751         """Snapshot the given volume-backed instance.
2752 
2753         :param instance: nova.objects.instance.Instance object
2754         :param name: name of the backup or snapshot
2755         :param extra_properties: dict of extra image properties to include
2756 
2757         :returns: the new image metadata
2758         """
2759         image_meta = self._initialize_instance_snapshot_metadata(
2760             instance, name, extra_properties)
2761         # the new image is simply a bucket of properties (particularly the
2762         # block device mapping, kernel and ramdisk IDs) with no image data,
2763         # hence the zero size
2764         image_meta['size'] = 0
2765         for attr in ('container_format', 'disk_format'):
2766             image_meta.pop(attr, None)
2767         properties = image_meta['properties']
2768         # clean properties before filling
2769         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
2770             properties.pop(key, None)
2771         if instance.root_device_name:
2772             properties['root_device_name'] = instance.root_device_name
2773 
2774         quiesced = False
2775         if instance.vm_state == vm_states.ACTIVE:
2776             try:
2777                 self.compute_rpcapi.quiesce_instance(context, instance)
2778                 quiesced = True
2779             except (exception.InstanceQuiesceNotSupported,
2780                     exception.QemuGuestAgentNotEnabled,
2781                     exception.NovaException, NotImplementedError) as err:
2782                 if strutils.bool_from_string(instance.system_metadata.get(
2783                         'image_os_require_quiesce')):
2784                     raise
2785                 else:
2786                     LOG.info('Skipping quiescing instance: %(reason)s.',
2787                              {'reason': err},
2788                              instance=instance)
2789 
2790         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2791                 context, instance.uuid)
2792 
2793         mapping = []
2794         for bdm in bdms:
2795             if bdm.no_device:
2796                 continue
2797 
2798             if bdm.is_volume:
2799                 # create snapshot based on volume_id
2800                 volume = self.volume_api.get(context, bdm.volume_id)
2801                 # NOTE(yamahata): Should we wait for snapshot creation?
2802                 #                 Linux LVM snapshot creation completes in
2803                 #                 short time, it doesn't matter for now.
2804                 name = _('snapshot for %s') % image_meta['name']
2805                 LOG.debug('Creating snapshot from volume %s.', volume['id'],
2806                           instance=instance)
2807                 snapshot = self.volume_api.create_snapshot_force(
2808                     context, volume['id'], name, volume['display_description'])
2809                 mapping_dict = block_device.snapshot_from_bdm(snapshot['id'],
2810                                                               bdm)
2811                 mapping_dict = mapping_dict.get_image_mapping()
2812             else:
2813                 mapping_dict = bdm.get_image_mapping()
2814 
2815             mapping.append(mapping_dict)
2816 
2817         if quiesced:
2818             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
2819 
2820         if mapping:
2821             properties['block_device_mapping'] = mapping
2822             properties['bdm_v2'] = True
2823 
2824         return self.image_api.create(context, image_meta)
2825 
2826     @check_instance_lock
2827     def reboot(self, context, instance, reboot_type):
2828         """Reboot the given instance."""
2829         if reboot_type == 'SOFT':
2830             self._soft_reboot(context, instance)
2831         else:
2832             self._hard_reboot(context, instance)
2833 
2834     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
2835                           task_state=[None])
2836     def _soft_reboot(self, context, instance):
2837         expected_task_state = [None]
2838         instance.task_state = task_states.REBOOTING
2839         instance.save(expected_task_state=expected_task_state)
2840 
2841         self._record_action_start(context, instance, instance_actions.REBOOT)
2842 
2843         self.compute_rpcapi.reboot_instance(context, instance=instance,
2844                                             block_device_info=None,
2845                                             reboot_type='SOFT')
2846 
2847     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
2848                           task_state=task_states.ALLOW_REBOOT)
2849     def _hard_reboot(self, context, instance):
2850         instance.task_state = task_states.REBOOTING_HARD
2851         expected_task_state = [None,
2852                                task_states.REBOOTING,
2853                                task_states.REBOOT_PENDING,
2854                                task_states.REBOOT_STARTED,
2855                                task_states.REBOOTING_HARD,
2856                                task_states.RESUMING,
2857                                task_states.UNPAUSING,
2858                                task_states.SUSPENDING]
2859         instance.save(expected_task_state = expected_task_state)
2860 
2861         self._record_action_start(context, instance, instance_actions.REBOOT)
2862 
2863         self.compute_rpcapi.reboot_instance(context, instance=instance,
2864                                             block_device_info=None,
2865                                             reboot_type='HARD')
2866 
2867     @check_instance_lock
2868     @check_instance_cell
2869     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2870                                     vm_states.ERROR])
2871     def rebuild(self, context, instance, image_href, admin_password,
2872                 files_to_inject=None, **kwargs):
2873         """Rebuild the given instance with the provided attributes."""
2874         orig_image_ref = instance.image_ref or ''
2875         files_to_inject = files_to_inject or []
2876         metadata = kwargs.get('metadata', {})
2877         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
2878         auto_disk_config = kwargs.get('auto_disk_config')
2879 
2880         image_id, image = self._get_image(context, image_href)
2881         self._check_auto_disk_config(image=image, **kwargs)
2882 
2883         flavor = instance.get_flavor()
2884         root_bdm = compute_utils.get_root_bdm(context, instance)
2885         self._checks_for_create_and_rebuild(context, image_id, image,
2886                 flavor, metadata, files_to_inject, root_bdm)
2887 
2888         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
2889                 context, None, None, image)
2890 
2891         def _reset_image_metadata():
2892             """Remove old image properties that we're storing as instance
2893             system metadata.  These properties start with 'image_'.
2894             Then add the properties for the new image.
2895             """
2896             # FIXME(comstud): There's a race condition here in that if
2897             # the system_metadata for this instance is updated after
2898             # we do the previous save() and before we update.. those
2899             # other updates will be lost. Since this problem exists in
2900             # a lot of other places, I think it should be addressed in
2901             # a DB layer overhaul.
2902 
2903             orig_sys_metadata = dict(instance.system_metadata)
2904             # Remove the old keys
2905             for key in list(instance.system_metadata.keys()):
2906                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
2907                     del instance.system_metadata[key]
2908 
2909             # Add the new ones
2910             new_sys_metadata = utils.get_system_metadata_from_image(
2911                 image, flavor)
2912 
2913             instance.system_metadata.update(new_sys_metadata)
2914             instance.save()
2915             return orig_sys_metadata
2916 
2917         # Since image might have changed, we may have new values for
2918         # os_type, vm_mode, etc
2919         options_from_image = self._inherit_properties_from_image(
2920                 image, auto_disk_config)
2921         instance.update(options_from_image)
2922 
2923         instance.task_state = task_states.REBUILDING
2924         instance.image_ref = image_href
2925         instance.kernel_id = kernel_id or ""
2926         instance.ramdisk_id = ramdisk_id or ""
2927         instance.progress = 0
2928         instance.update(kwargs)
2929         instance.save(expected_task_state=[None])
2930 
2931         # On a rebuild, since we're potentially changing images, we need to
2932         # wipe out the old image properties that we're storing as instance
2933         # system metadata... and copy in the properties for the new image.
2934         orig_sys_metadata = _reset_image_metadata()
2935 
2936         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2937                 context, instance.uuid)
2938 
2939         self._record_action_start(context, instance, instance_actions.REBUILD)
2940 
2941         # NOTE(sbauza): The migration script we provided in Newton should make
2942         # sure that all our instances are currently migrated to have an
2943         # attached RequestSpec object but let's consider that the operator only
2944         # half migrated all their instances in the meantime.
2945         try:
2946             request_spec = objects.RequestSpec.get_by_instance_uuid(
2947                 context, instance.uuid)
2948         except exception.RequestSpecNotFound:
2949             # Some old instances can still have no RequestSpec object attached
2950             # to them, we need to support the old way
2951             request_spec = None
2952 
2953         self.compute_task_api.rebuild_instance(context, instance=instance,
2954                 new_pass=admin_password, injected_files=files_to_inject,
2955                 image_ref=image_href, orig_image_ref=orig_image_ref,
2956                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
2957                 preserve_ephemeral=preserve_ephemeral, host=instance.host,
2958                 request_spec=request_spec,
2959                 kwargs=kwargs)
2960 
2961     @staticmethod
2962     def _check_quota_for_upsize(context, instance, current_flavor, new_flavor):
2963         project_id, user_id = quotas_obj.ids_from_instance(context,
2964                                                            instance)
2965         deltas = compute_utils.upsize_quota_delta(context, new_flavor,
2966                                                   current_flavor)
2967         if deltas:
2968             try:
2969                 res_deltas = {'cores': deltas.get('cores', 0),
2970                               'ram': deltas.get('ram', 0)}
2971                 objects.Quotas.check_deltas(context, res_deltas,
2972                                             project_id, user_id=user_id,
2973                                             check_project_id=project_id,
2974                                             check_user_id=user_id)
2975             except exception.OverQuota as exc:
2976                 quotas = exc.kwargs['quotas']
2977                 overs = exc.kwargs['overs']
2978                 usages = exc.kwargs['usages']
2979                 headroom = compute_utils.get_headroom(quotas, usages,
2980                                                       deltas)
2981                 (overs, reqs, total_alloweds,
2982                  useds) = compute_utils.get_over_quota_detail(headroom,
2983                                                               overs,
2984                                                               quotas,
2985                                                               deltas)
2986                 LOG.warning("%(overs)s quota exceeded for %(pid)s,"
2987                             " tried to resize instance.",
2988                             {'overs': overs, 'pid': context.project_id})
2989                 raise exception.TooManyInstances(overs=overs,
2990                                                  req=reqs,
2991                                                  used=useds,
2992                                                  allowed=total_alloweds)
2993 
2994     @check_instance_lock
2995     @check_instance_cell
2996     @check_instance_state(vm_state=[vm_states.RESIZED])
2997     def revert_resize(self, context, instance):
2998         """Reverts a resize, deleting the 'new' instance in the process."""
2999         elevated = context.elevated()
3000         migration = objects.Migration.get_by_instance_and_status(
3001             elevated, instance.uuid, 'finished')
3002 
3003         # If this is a resize down, a revert might go over quota.
3004         # TODO(melwitt): We're not rechecking for strict quota here to guard
3005         # against going over quota during a race at this time because the
3006         # resource consumption for this operation is written to the database
3007         # by compute.
3008         self._check_quota_for_upsize(context, instance, instance.flavor,
3009                                      instance.old_flavor)
3010 
3011         instance.task_state = task_states.RESIZE_REVERTING
3012         instance.save(expected_task_state=[None])
3013 
3014         migration.status = 'reverting'
3015         migration.save()
3016 
3017         self._record_action_start(context, instance,
3018                                   instance_actions.REVERT_RESIZE)
3019 
3020         self.compute_rpcapi.revert_resize(context, instance,
3021                                           migration,
3022                                           migration.dest_compute)
3023 
3024     @check_instance_lock
3025     @check_instance_cell
3026     @check_instance_state(vm_state=[vm_states.RESIZED])
3027     def confirm_resize(self, context, instance, migration=None):
3028         """Confirms a migration/resize and deletes the 'old' instance."""
3029         elevated = context.elevated()
3030         if migration is None:
3031             migration = objects.Migration.get_by_instance_and_status(
3032                 elevated, instance.uuid, 'finished')
3033 
3034         migration.status = 'confirming'
3035         migration.save()
3036 
3037         self._record_action_start(context, instance,
3038                                   instance_actions.CONFIRM_RESIZE)
3039 
3040         self.compute_rpcapi.confirm_resize(context,
3041                                            instance,
3042                                            migration,
3043                                            migration.source_compute)
3044 
3045     @staticmethod
3046     def _resize_cells_support(context, instance,
3047                               current_instance_type, new_instance_type):
3048         """Special API cell logic for resize."""
3049         # NOTE(johannes/comstud): The API cell needs a local migration
3050         # record for later resize_confirm and resize_reverts.
3051         # We don't need source and/or destination
3052         # information, just the old and new flavors. Status is set to
3053         # 'finished' since nothing else will update the status along
3054         # the way.
3055         mig = objects.Migration(context=context.elevated())
3056         mig.instance_uuid = instance.uuid
3057         mig.old_instance_type_id = current_instance_type['id']
3058         mig.new_instance_type_id = new_instance_type['id']
3059         mig.status = 'finished'
3060         mig.migration_type = (
3061             mig.old_instance_type_id != mig.new_instance_type_id and
3062             'resize' or 'migration')
3063         mig.create()
3064 
3065     @check_instance_lock
3066     @check_instance_cell
3067     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3068     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3069                **extra_instance_updates):
3070         """Resize (ie, migrate) a running instance.
3071 
3072         If flavor_id is None, the process is considered a migration, keeping
3073         the original flavor_id. If flavor_id is not None, the instance should
3074         be migrated to a new host and resized to the new flavor_id.
3075         """
3076         self._check_auto_disk_config(instance, **extra_instance_updates)
3077 
3078         current_instance_type = instance.get_flavor()
3079 
3080         # If flavor_id is not provided, only migrate the instance.
3081         if not flavor_id:
3082             LOG.debug("flavor_id is None. Assuming migration.",
3083                       instance=instance)
3084             new_instance_type = current_instance_type
3085         else:
3086             new_instance_type = flavors.get_flavor_by_flavor_id(
3087                     flavor_id, read_deleted="no")
3088             if (new_instance_type.get('root_gb') == 0 and
3089                 current_instance_type.get('root_gb') != 0 and
3090                 not compute_utils.is_volume_backed_instance(context,
3091                     instance)):
3092                 reason = _('Resize to zero disk flavor is not allowed.')
3093                 raise exception.CannotResizeDisk(reason=reason)
3094 
3095         if not new_instance_type:
3096             raise exception.FlavorNotFound(flavor_id=flavor_id)
3097 
3098         current_instance_type_name = current_instance_type['name']
3099         new_instance_type_name = new_instance_type['name']
3100         LOG.debug("Old instance type %(current_instance_type_name)s, "
3101                   "new instance type %(new_instance_type_name)s",
3102                   {'current_instance_type_name': current_instance_type_name,
3103                    'new_instance_type_name': new_instance_type_name},
3104                   instance=instance)
3105 
3106         same_instance_type = (current_instance_type['id'] ==
3107                               new_instance_type['id'])
3108 
3109         # NOTE(sirp): We don't want to force a customer to change their flavor
3110         # when Ops is migrating off of a failed host.
3111         if not same_instance_type and new_instance_type.get('disabled'):
3112             raise exception.FlavorNotFound(flavor_id=flavor_id)
3113 
3114         if same_instance_type and flavor_id and self.cell_type != 'compute':
3115             raise exception.CannotResizeToSameFlavor()
3116 
3117         # ensure there is sufficient headroom for upsizes
3118         # TODO(melwitt): We're not rechecking for strict quota here to guard
3119         # against going over quota during a race at this time because the
3120         # resource consumption for this operation is written to the database
3121         # by compute.
3122         if flavor_id:
3123             self._check_quota_for_upsize(context, instance,
3124                                          current_instance_type,
3125                                          new_instance_type)
3126 
3127         instance.task_state = task_states.RESIZE_PREP
3128         instance.progress = 0
3129         instance.update(extra_instance_updates)
3130         instance.save(expected_task_state=[None])
3131 
3132         filter_properties = {'ignore_hosts': []}
3133 
3134         if not CONF.allow_resize_to_same_host:
3135             filter_properties['ignore_hosts'].append(instance.host)
3136 
3137         if self.cell_type == 'api':
3138             # Create migration record.
3139             self._resize_cells_support(context, instance,
3140                                        current_instance_type,
3141                                        new_instance_type)
3142 
3143         if not flavor_id:
3144             self._record_action_start(context, instance,
3145                                       instance_actions.MIGRATE)
3146         else:
3147             self._record_action_start(context, instance,
3148                                       instance_actions.RESIZE)
3149 
3150         # NOTE(sbauza): The migration script we provided in Newton should make
3151         # sure that all our instances are currently migrated to have an
3152         # attached RequestSpec object but let's consider that the operator only
3153         # half migrated all their instances in the meantime.
3154         try:
3155             request_spec = objects.RequestSpec.get_by_instance_uuid(
3156                 context, instance.uuid)
3157             request_spec.ignore_hosts = filter_properties['ignore_hosts']
3158         except exception.RequestSpecNotFound:
3159             # Some old instances can still have no RequestSpec object attached
3160             # to them, we need to support the old way
3161             request_spec = None
3162 
3163         scheduler_hint = {'filter_properties': filter_properties}
3164         self.compute_task_api.resize_instance(context, instance,
3165                 extra_instance_updates, scheduler_hint=scheduler_hint,
3166                 flavor=new_instance_type,
3167                 clean_shutdown=clean_shutdown,
3168                 request_spec=request_spec)
3169 
3170     @check_instance_lock
3171     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3172                                     vm_states.PAUSED, vm_states.SUSPENDED])
3173     def shelve(self, context, instance, clean_shutdown=True):
3174         """Shelve an instance.
3175 
3176         Shuts down an instance and frees it up to be removed from the
3177         hypervisor.
3178         """
3179         instance.task_state = task_states.SHELVING
3180         instance.save(expected_task_state=[None])
3181 
3182         self._record_action_start(context, instance, instance_actions.SHELVE)
3183 
3184         if not compute_utils.is_volume_backed_instance(context, instance):
3185             name = '%s-shelved' % instance.display_name
3186             image_meta = self._create_image(context, instance, name,
3187                     'snapshot')
3188             image_id = image_meta['id']
3189             self.compute_rpcapi.shelve_instance(context, instance=instance,
3190                     image_id=image_id, clean_shutdown=clean_shutdown)
3191         else:
3192             self.compute_rpcapi.shelve_offload_instance(context,
3193                     instance=instance, clean_shutdown=clean_shutdown)
3194 
3195     @check_instance_lock
3196     @check_instance_state(vm_state=[vm_states.SHELVED])
3197     def shelve_offload(self, context, instance, clean_shutdown=True):
3198         """Remove a shelved instance from the hypervisor."""
3199         instance.task_state = task_states.SHELVING_OFFLOADING
3200         instance.save(expected_task_state=[None])
3201 
3202         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3203             clean_shutdown=clean_shutdown)
3204 
3205     @check_instance_lock
3206     @check_instance_state(vm_state=[vm_states.SHELVED,
3207         vm_states.SHELVED_OFFLOADED])
3208     def unshelve(self, context, instance):
3209         """Restore a shelved instance."""
3210         instance.task_state = task_states.UNSHELVING
3211         instance.save(expected_task_state=[None])
3212 
3213         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3214 
3215         try:
3216             request_spec = objects.RequestSpec.get_by_instance_uuid(
3217                 context, instance.uuid)
3218         except exception.RequestSpecNotFound:
3219             # Some old instances can still have no RequestSpec object attached
3220             # to them, we need to support the old way
3221             request_spec = None
3222         self.compute_task_api.unshelve_instance(context, instance,
3223                                                 request_spec)
3224 
3225     @check_instance_lock
3226     def add_fixed_ip(self, context, instance, network_id):
3227         """Add fixed_ip from specified network to given instance."""
3228         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3229                 instance=instance, network_id=network_id)
3230 
3231     @check_instance_lock
3232     def remove_fixed_ip(self, context, instance, address):
3233         """Remove fixed_ip from specified network to given instance."""
3234         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3235                 instance=instance, address=address)
3236 
3237     @check_instance_lock
3238     @check_instance_cell
3239     @check_instance_state(vm_state=[vm_states.ACTIVE])
3240     def pause(self, context, instance):
3241         """Pause the given instance."""
3242         instance.task_state = task_states.PAUSING
3243         instance.save(expected_task_state=[None])
3244         self._record_action_start(context, instance, instance_actions.PAUSE)
3245         self.compute_rpcapi.pause_instance(context, instance)
3246 
3247     @check_instance_lock
3248     @check_instance_cell
3249     @check_instance_state(vm_state=[vm_states.PAUSED])
3250     def unpause(self, context, instance):
3251         """Unpause the given instance."""
3252         instance.task_state = task_states.UNPAUSING
3253         instance.save(expected_task_state=[None])
3254         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3255         self.compute_rpcapi.unpause_instance(context, instance)
3256 
3257     @check_instance_host
3258     def get_diagnostics(self, context, instance):
3259         """Retrieve diagnostics for the given instance."""
3260         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3261 
3262     @check_instance_host
3263     def get_instance_diagnostics(self, context, instance):
3264         """Retrieve diagnostics for the given instance."""
3265         return self.compute_rpcapi.get_instance_diagnostics(context,
3266                                                             instance=instance)
3267 
3268     @check_instance_lock
3269     @check_instance_cell
3270     @check_instance_state(vm_state=[vm_states.ACTIVE])
3271     def suspend(self, context, instance):
3272         """Suspend the given instance."""
3273         instance.task_state = task_states.SUSPENDING
3274         instance.save(expected_task_state=[None])
3275         self._record_action_start(context, instance, instance_actions.SUSPEND)
3276         self.compute_rpcapi.suspend_instance(context, instance)
3277 
3278     @check_instance_lock
3279     @check_instance_cell
3280     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3281     def resume(self, context, instance):
3282         """Resume the given instance."""
3283         instance.task_state = task_states.RESUMING
3284         instance.save(expected_task_state=[None])
3285         self._record_action_start(context, instance, instance_actions.RESUME)
3286         self.compute_rpcapi.resume_instance(context, instance)
3287 
3288     @check_instance_lock
3289     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3290                                     vm_states.ERROR])
3291     def rescue(self, context, instance, rescue_password=None,
3292                rescue_image_ref=None, clean_shutdown=True):
3293         """Rescue the given instance."""
3294 
3295         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3296                     context, instance.uuid)
3297         for bdm in bdms:
3298             if bdm.volume_id:
3299                 vol = self.volume_api.get(context, bdm.volume_id)
3300                 self.volume_api.check_attached(context, vol)
3301         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3302             reason = _("Cannot rescue a volume-backed instance")
3303             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3304                                                  reason=reason)
3305 
3306         instance.task_state = task_states.RESCUING
3307         instance.save(expected_task_state=[None])
3308 
3309         self._record_action_start(context, instance, instance_actions.RESCUE)
3310 
3311         self.compute_rpcapi.rescue_instance(context, instance=instance,
3312             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3313             clean_shutdown=clean_shutdown)
3314 
3315     @check_instance_lock
3316     @check_instance_state(vm_state=[vm_states.RESCUED])
3317     def unrescue(self, context, instance):
3318         """Unrescue the given instance."""
3319         instance.task_state = task_states.UNRESCUING
3320         instance.save(expected_task_state=[None])
3321 
3322         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3323 
3324         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3325 
3326     @check_instance_lock
3327     @check_instance_cell
3328     @check_instance_state(vm_state=[vm_states.ACTIVE])
3329     def set_admin_password(self, context, instance, password=None):
3330         """Set the root/admin password for the given instance.
3331 
3332         @param context: Nova auth context.
3333         @param instance: Nova instance object.
3334         @param password: The admin password for the instance.
3335         """
3336         instance.task_state = task_states.UPDATING_PASSWORD
3337         instance.save(expected_task_state=[None])
3338 
3339         self._record_action_start(context, instance,
3340                                   instance_actions.CHANGE_PASSWORD)
3341 
3342         self.compute_rpcapi.set_admin_password(context,
3343                                                instance=instance,
3344                                                new_pass=password)
3345 
3346     @check_instance_host
3347     @reject_instance_state(
3348         task_state=[task_states.DELETING, task_states.MIGRATING])
3349     def get_vnc_console(self, context, instance, console_type):
3350         """Get a url to an instance Console."""
3351         connect_info = self.compute_rpcapi.get_vnc_console(context,
3352                 instance=instance, console_type=console_type)
3353 
3354         self.consoleauth_rpcapi.authorize_console(context,
3355                 connect_info['token'], console_type,
3356                 connect_info['host'], connect_info['port'],
3357                 connect_info['internal_access_path'], instance.uuid,
3358                 access_url=connect_info['access_url'])
3359 
3360         return {'url': connect_info['access_url']}
3361 
3362     @check_instance_host
3363     def get_vnc_connect_info(self, context, instance, console_type):
3364         """Used in a child cell to get console info."""
3365         connect_info = self.compute_rpcapi.get_vnc_console(context,
3366                 instance=instance, console_type=console_type)
3367         return connect_info
3368 
3369     @check_instance_host
3370     @reject_instance_state(
3371         task_state=[task_states.DELETING, task_states.MIGRATING])
3372     def get_spice_console(self, context, instance, console_type):
3373         """Get a url to an instance Console."""
3374         connect_info = self.compute_rpcapi.get_spice_console(context,
3375                 instance=instance, console_type=console_type)
3376         self.consoleauth_rpcapi.authorize_console(context,
3377                 connect_info['token'], console_type,
3378                 connect_info['host'], connect_info['port'],
3379                 connect_info['internal_access_path'], instance.uuid,
3380                 access_url=connect_info['access_url'])
3381 
3382         return {'url': connect_info['access_url']}
3383 
3384     @check_instance_host
3385     def get_spice_connect_info(self, context, instance, console_type):
3386         """Used in a child cell to get console info."""
3387         connect_info = self.compute_rpcapi.get_spice_console(context,
3388                 instance=instance, console_type=console_type)
3389         return connect_info
3390 
3391     @check_instance_host
3392     @reject_instance_state(
3393         task_state=[task_states.DELETING, task_states.MIGRATING])
3394     def get_rdp_console(self, context, instance, console_type):
3395         """Get a url to an instance Console."""
3396         connect_info = self.compute_rpcapi.get_rdp_console(context,
3397                 instance=instance, console_type=console_type)
3398         self.consoleauth_rpcapi.authorize_console(context,
3399                 connect_info['token'], console_type,
3400                 connect_info['host'], connect_info['port'],
3401                 connect_info['internal_access_path'], instance.uuid,
3402                 access_url=connect_info['access_url'])
3403 
3404         return {'url': connect_info['access_url']}
3405 
3406     @check_instance_host
3407     def get_rdp_connect_info(self, context, instance, console_type):
3408         """Used in a child cell to get console info."""
3409         connect_info = self.compute_rpcapi.get_rdp_console(context,
3410                 instance=instance, console_type=console_type)
3411         return connect_info
3412 
3413     @check_instance_host
3414     @reject_instance_state(
3415         task_state=[task_states.DELETING, task_states.MIGRATING])
3416     def get_serial_console(self, context, instance, console_type):
3417         """Get a url to a serial console."""
3418         connect_info = self.compute_rpcapi.get_serial_console(context,
3419                 instance=instance, console_type=console_type)
3420 
3421         self.consoleauth_rpcapi.authorize_console(context,
3422                 connect_info['token'], console_type,
3423                 connect_info['host'], connect_info['port'],
3424                 connect_info['internal_access_path'], instance.uuid,
3425                 access_url=connect_info['access_url'])
3426         return {'url': connect_info['access_url']}
3427 
3428     @check_instance_host
3429     def get_serial_console_connect_info(self, context, instance, console_type):
3430         """Used in a child cell to get serial console."""
3431         connect_info = self.compute_rpcapi.get_serial_console(context,
3432                 instance=instance, console_type=console_type)
3433         return connect_info
3434 
3435     @check_instance_host
3436     @reject_instance_state(
3437         task_state=[task_states.DELETING, task_states.MIGRATING])
3438     def get_mks_console(self, context, instance, console_type):
3439         """Get a url to a MKS console."""
3440         connect_info = self.compute_rpcapi.get_mks_console(context,
3441                 instance=instance, console_type=console_type)
3442         self.consoleauth_rpcapi.authorize_console(context,
3443                 connect_info['token'], console_type,
3444                 connect_info['host'], connect_info['port'],
3445                 connect_info['internal_access_path'], instance.uuid,
3446                 access_url=connect_info['access_url'])
3447         return {'url': connect_info['access_url']}
3448 
3449     @check_instance_host
3450     def get_console_output(self, context, instance, tail_length=None):
3451         """Get console output for an instance."""
3452         return self.compute_rpcapi.get_console_output(context,
3453                 instance=instance, tail_length=tail_length)
3454 
3455     def lock(self, context, instance):
3456         """Lock the given instance."""
3457         # Only update the lock if we are an admin (non-owner)
3458         is_owner = instance.project_id == context.project_id
3459         if instance.locked and is_owner:
3460             return
3461 
3462         context = context.elevated()
3463         LOG.debug('Locking', instance=instance)
3464         instance.locked = True
3465         instance.locked_by = 'owner' if is_owner else 'admin'
3466         instance.save()
3467 
3468     def is_expected_locked_by(self, context, instance):
3469         is_owner = instance.project_id == context.project_id
3470         expect_locked_by = 'owner' if is_owner else 'admin'
3471         locked_by = instance.locked_by
3472         if locked_by and locked_by != expect_locked_by:
3473             return False
3474         return True
3475 
3476     def unlock(self, context, instance):
3477         """Unlock the given instance."""
3478         context = context.elevated()
3479         LOG.debug('Unlocking', instance=instance)
3480         instance.locked = False
3481         instance.locked_by = None
3482         instance.save()
3483 
3484     @check_instance_lock
3485     @check_instance_cell
3486     def reset_network(self, context, instance):
3487         """Reset networking on the instance."""
3488         self.compute_rpcapi.reset_network(context, instance=instance)
3489 
3490     @check_instance_lock
3491     @check_instance_cell
3492     def inject_network_info(self, context, instance):
3493         """Inject network info for the instance."""
3494         self.compute_rpcapi.inject_network_info(context, instance=instance)
3495 
3496     def _create_volume_bdm(self, context, instance, device, volume_id,
3497                            disk_bus, device_type, is_local_creation=False,
3498                            tag=None):
3499         if is_local_creation:
3500             # when the creation is done locally we can't specify the device
3501             # name as we do not have a way to check that the name specified is
3502             # a valid one.
3503             # We leave the setting of that value when the actual attach
3504             # happens on the compute manager
3505             # NOTE(artom) Local attach (to a shelved-offload instance) cannot
3506             # support device tagging because we have no way to call the compute
3507             # manager to check that it supports device tagging. In fact, we
3508             # don't even know which computer manager the instance will
3509             # eventually end up on when it's unshelved.
3510             volume_bdm = objects.BlockDeviceMapping(
3511                 context=context,
3512                 source_type='volume', destination_type='volume',
3513                 instance_uuid=instance.uuid, boot_index=None,
3514                 volume_id=volume_id,
3515                 device_name=None, guest_format=None,
3516                 disk_bus=disk_bus, device_type=device_type)
3517             volume_bdm.create()
3518         else:
3519             # NOTE(vish): This is done on the compute host because we want
3520             #             to avoid a race where two devices are requested at
3521             #             the same time. When db access is removed from
3522             #             compute, the bdm will be created here and we will
3523             #             have to make sure that they are assigned atomically.
3524             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3525                 context, instance, device, volume_id, disk_bus=disk_bus,
3526                 device_type=device_type, tag=tag)
3527         return volume_bdm
3528 
3529     def _check_attach_and_reserve_volume(self, context, volume_id, instance):
3530         volume = self.volume_api.get(context, volume_id)
3531         self.volume_api.check_availability_zone(context, volume,
3532                                                 instance=instance)
3533         self.volume_api.reserve_volume(context, volume_id)
3534 
3535         return volume
3536 
3537     def _attach_volume(self, context, instance, volume_id, device,
3538                        disk_bus, device_type, tag=None):
3539         """Attach an existing volume to an existing instance.
3540 
3541         This method is separated to make it possible for cells version
3542         to override it.
3543         """
3544         volume_bdm = self._create_volume_bdm(
3545             context, instance, device, volume_id, disk_bus=disk_bus,
3546             device_type=device_type, tag=tag)
3547         try:
3548             self._check_attach_and_reserve_volume(context, volume_id, instance)
3549             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
3550         except Exception:
3551             with excutils.save_and_reraise_exception():
3552                 volume_bdm.destroy()
3553 
3554         return volume_bdm.device_name
3555 
3556     def _attach_volume_shelved_offloaded(self, context, instance, volume_id,
3557                                          device, disk_bus, device_type):
3558         """Attach an existing volume to an instance in shelved offloaded state.
3559 
3560         Attaching a volume for an instance in shelved offloaded state requires
3561         to perform the regular check to see if we can attach and reserve the
3562         volume then we need to call the attach method on the volume API
3563         to mark the volume as 'in-use'.
3564         The instance at this stage is not managed by a compute manager
3565         therefore the actual attachment will be performed once the
3566         instance will be unshelved.
3567         """
3568 
3569         volume_bdm = self._create_volume_bdm(
3570             context, instance, device, volume_id, disk_bus=disk_bus,
3571             device_type=device_type, is_local_creation=True)
3572         try:
3573             self._check_attach_and_reserve_volume(context, volume_id, instance)
3574             self.volume_api.attach(context,
3575                                    volume_id,
3576                                    instance.uuid,
3577                                    device)
3578         except Exception:
3579             with excutils.save_and_reraise_exception():
3580                 volume_bdm.destroy()
3581 
3582         return volume_bdm.device_name
3583 
3584     @check_instance_lock
3585     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3586                                     vm_states.STOPPED, vm_states.RESIZED,
3587                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3588                                     vm_states.SHELVED_OFFLOADED])
3589     def attach_volume(self, context, instance, volume_id, device=None,
3590                       disk_bus=None, device_type=None, tag=None):
3591         """Attach an existing volume to an existing instance."""
3592         # NOTE(vish): Fail fast if the device is not going to pass. This
3593         #             will need to be removed along with the test if we
3594         #             change the logic in the manager for what constitutes
3595         #             a valid device.
3596         if device and not block_device.match_device(device):
3597             raise exception.InvalidDevicePath(path=device)
3598 
3599         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
3600         if is_shelved_offloaded:
3601             if tag:
3602                 # NOTE(artom) Local attach (to a shelved-offload instance)
3603                 # cannot support device tagging because we have no way to call
3604                 # the compute manager to check that it supports device tagging.
3605                 # In fact, we don't even know which computer manager the
3606                 # instance will eventually end up on when it's unshelved.
3607                 raise exception.VolumeTaggedAttachToShelvedNotSupported()
3608             return self._attach_volume_shelved_offloaded(context,
3609                                                          instance,
3610                                                          volume_id,
3611                                                          device,
3612                                                          disk_bus,
3613                                                          device_type)
3614 
3615         return self._attach_volume(context, instance, volume_id, device,
3616                                    disk_bus, device_type, tag=tag)
3617 
3618     def _check_and_begin_detach(self, context, volume, instance):
3619         self.volume_api.check_detach(context, volume, instance=instance)
3620         self.volume_api.begin_detaching(context, volume['id'])
3621 
3622     def _detach_volume(self, context, instance, volume):
3623         """Detach volume from instance.
3624 
3625         This method is separated to make it easier for cells version
3626         to override.
3627         """
3628         self._check_and_begin_detach(context, volume, instance)
3629         attachments = volume.get('attachments', {})
3630         attachment_id = None
3631         if attachments and instance.uuid in attachments:
3632             attachment_id = attachments[instance.uuid]['attachment_id']
3633         self.compute_rpcapi.detach_volume(context, instance=instance,
3634                 volume_id=volume['id'], attachment_id=attachment_id)
3635 
3636     def _detach_volume_shelved_offloaded(self, context, instance, volume):
3637         """Detach a volume from an instance in shelved offloaded state.
3638 
3639         If the instance is shelved offloaded we just need to cleanup volume
3640         calling the volume api detach, the volume api terminate_connection
3641         and delete the bdm record.
3642         If the volume has delete_on_termination option set then we call the
3643         volume api delete as well.
3644         """
3645         self._check_and_begin_detach(context, volume, instance)
3646         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
3647                 context, volume['id'], instance.uuid)]
3648         self._local_cleanup_bdm_volumes(bdms, instance, context)
3649 
3650     @check_instance_lock
3651     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3652                                     vm_states.STOPPED, vm_states.RESIZED,
3653                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3654                                     vm_states.SHELVED_OFFLOADED])
3655     def detach_volume(self, context, instance, volume):
3656         """Detach a volume from an instance."""
3657         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
3658             self._detach_volume_shelved_offloaded(context, instance, volume)
3659         else:
3660             self._detach_volume(context, instance, volume)
3661 
3662     @check_instance_lock
3663     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3664                                     vm_states.SUSPENDED, vm_states.STOPPED,
3665                                     vm_states.RESIZED, vm_states.SOFT_DELETED])
3666     def swap_volume(self, context, instance, old_volume, new_volume):
3667         """Swap volume attached to an instance."""
3668         if old_volume['attach_status'] == 'detached':
3669             raise exception.VolumeUnattached(volume_id=old_volume['id'])
3670         # The caller likely got the instance from volume['attachments']
3671         # in the first place, but let's sanity check.
3672         if not old_volume.get('attachments', {}).get(instance.uuid):
3673             msg = _("Old volume is attached to a different instance.")
3674             raise exception.InvalidVolume(reason=msg)
3675         if new_volume['attach_status'] == 'attached':
3676             msg = _("New volume must be detached in order to swap.")
3677             raise exception.InvalidVolume(reason=msg)
3678         if int(new_volume['size']) < int(old_volume['size']):
3679             msg = _("New volume must be the same size or larger.")
3680             raise exception.InvalidVolume(reason=msg)
3681         self.volume_api.check_detach(context, old_volume)
3682         self.volume_api.check_availability_zone(context, new_volume,
3683                                                 instance=instance)
3684         self.volume_api.begin_detaching(context, old_volume['id'])
3685         self.volume_api.reserve_volume(context, new_volume['id'])
3686         try:
3687             self.compute_rpcapi.swap_volume(
3688                     context, instance=instance,
3689                     old_volume_id=old_volume['id'],
3690                     new_volume_id=new_volume['id'])
3691         except Exception:
3692             with excutils.save_and_reraise_exception():
3693                 self.volume_api.roll_detaching(context, old_volume['id'])
3694                 self.volume_api.unreserve_volume(context, new_volume['id'])
3695 
3696     @check_instance_lock
3697     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3698                                     vm_states.STOPPED],
3699                           task_state=[None])
3700     def attach_interface(self, context, instance, network_id, port_id,
3701                          requested_ip, tag=None):
3702         """Use hotplug to add an network adapter to an instance."""
3703         return self.compute_rpcapi.attach_interface(context,
3704             instance=instance, network_id=network_id, port_id=port_id,
3705             requested_ip=requested_ip, tag=tag)
3706 
3707     @check_instance_lock
3708     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3709                                     vm_states.STOPPED],
3710                           task_state=[None])
3711     def detach_interface(self, context, instance, port_id):
3712         """Detach an network adapter from an instance."""
3713         self.compute_rpcapi.detach_interface(context, instance=instance,
3714             port_id=port_id)
3715 
3716     def get_instance_metadata(self, context, instance):
3717         """Get all metadata associated with an instance."""
3718         return self.db.instance_metadata_get(context, instance.uuid)
3719 
3720     def get_all_instance_metadata(self, context, search_filts):
3721         return self._get_all_instance_metadata(
3722             context, search_filts, metadata_type='metadata')
3723 
3724     def get_all_system_metadata(self, context, search_filts):
3725         return self._get_all_instance_metadata(
3726             context, search_filts, metadata_type='system_metadata')
3727 
3728     def _get_all_instance_metadata(self, context, search_filts, metadata_type):
3729         """Get all metadata."""
3730         instances = self._get_instances_by_filters(context, filters={},
3731                                                    sort_keys=['created_at'],
3732                                                    sort_dirs=['desc'])
3733         return utils.filter_and_format_resource_metadata('instance', instances,
3734                 search_filts, metadata_type)
3735 
3736     @check_instance_lock
3737     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3738                                     vm_states.SUSPENDED, vm_states.STOPPED],
3739                           task_state=None)
3740     def delete_instance_metadata(self, context, instance, key):
3741         """Delete the given metadata item from an instance."""
3742         instance.delete_metadata_key(key)
3743         self.compute_rpcapi.change_instance_metadata(context,
3744                                                      instance=instance,
3745                                                      diff={key: ['-']})
3746 
3747     @check_instance_lock
3748     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3749                                     vm_states.SUSPENDED, vm_states.STOPPED],
3750                           task_state=None)
3751     def update_instance_metadata(self, context, instance,
3752                                  metadata, delete=False):
3753         """Updates or creates instance metadata.
3754 
3755         If delete is True, metadata items that are not specified in the
3756         `metadata` argument will be deleted.
3757 
3758         """
3759         orig = dict(instance.metadata)
3760         if delete:
3761             _metadata = metadata
3762         else:
3763             _metadata = dict(instance.metadata)
3764             _metadata.update(metadata)
3765 
3766         self._check_metadata_properties_quota(context, _metadata)
3767         instance.metadata = _metadata
3768         instance.save()
3769         diff = _diff_dict(orig, instance.metadata)
3770         self.compute_rpcapi.change_instance_metadata(context,
3771                                                      instance=instance,
3772                                                      diff=diff)
3773         return _metadata
3774 
3775     @check_instance_lock
3776     @check_instance_cell
3777     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
3778     def live_migrate(self, context, instance, block_migration,
3779                      disk_over_commit, host_name, force=None, async=False):
3780         """Migrate a server lively to a new host."""
3781         LOG.debug("Going to try to live migrate instance to %s",
3782                   host_name or "another host", instance=instance)
3783 
3784         instance.task_state = task_states.MIGRATING
3785         instance.save(expected_task_state=[None])
3786 
3787         self._record_action_start(context, instance,
3788                                   instance_actions.LIVE_MIGRATION)
3789 
3790         self.consoleauth_rpcapi.delete_tokens_for_instance(
3791             context, instance.uuid)
3792 
3793         try:
3794             request_spec = objects.RequestSpec.get_by_instance_uuid(
3795                 context, instance.uuid)
3796         except exception.RequestSpecNotFound:
3797             # Some old instances can still have no RequestSpec object attached
3798             # to them, we need to support the old way
3799             request_spec = None
3800 
3801         # NOTE(sbauza): Force is a boolean by the new related API version
3802         if force is False and host_name:
3803             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
3804             # NOTE(sbauza): Unset the host to make sure we call the scheduler
3805             host_name = None
3806             # FIXME(sbauza): Since only Ironic driver uses more than one
3807             # compute per service but doesn't support live migrations,
3808             # let's provide the first one.
3809             target = nodes[0]
3810             if request_spec:
3811                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
3812                 # having a request spec attached to them (particularly true for
3813                 # cells v1). For the moment, let's keep the same behaviour for
3814                 # all the instances but provide the destination only if a spec
3815                 # is found.
3816                 destination = objects.Destination(
3817                     host=target.host,
3818                     node=target.hypervisor_hostname
3819                 )
3820                 request_spec.requested_destination = destination
3821 
3822         try:
3823             self.compute_task_api.live_migrate_instance(context, instance,
3824                 host_name, block_migration=block_migration,
3825                 disk_over_commit=disk_over_commit,
3826                 request_spec=request_spec, async=async)
3827         except oslo_exceptions.MessagingTimeout as messaging_timeout:
3828             with excutils.save_and_reraise_exception():
3829                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
3830                 # occurs, but LM will still be in progress, so write
3831                 # instance fault to database
3832                 compute_utils.add_instance_fault_from_exc(context,
3833                                                           instance,
3834                                                           messaging_timeout)
3835 
3836     @check_instance_lock
3837     @check_instance_cell
3838     @check_instance_state(vm_state=[vm_states.ACTIVE],
3839                           task_state=[task_states.MIGRATING])
3840     def live_migrate_force_complete(self, context, instance, migration_id):
3841         """Force live migration to complete.
3842 
3843         :param context: Security context
3844         :param instance: The instance that is being migrated
3845         :param migration_id: ID of ongoing migration
3846 
3847         """
3848         LOG.debug("Going to try to force live migration to complete",
3849                   instance=instance)
3850 
3851         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
3852         # live migration for particular instance. Also pass migration id to
3853         # compute to double check and avoid possible race condition.
3854         migration = objects.Migration.get_by_id_and_instance(
3855             context, migration_id, instance.uuid)
3856         if migration.status != 'running':
3857             raise exception.InvalidMigrationState(migration_id=migration_id,
3858                                                   instance_uuid=instance.uuid,
3859                                                   state=migration.status,
3860                                                   method='force complete')
3861 
3862         self._record_action_start(
3863             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
3864 
3865         self.compute_rpcapi.live_migration_force_complete(
3866             context, instance, migration)
3867 
3868     @check_instance_lock
3869     @check_instance_cell
3870     @check_instance_state(task_state=[task_states.MIGRATING])
3871     def live_migrate_abort(self, context, instance, migration_id):
3872         """Abort an in-progress live migration.
3873 
3874         :param context: Security context
3875         :param instance: The instance that is being migrated
3876         :param migration_id: ID of in-progress live migration
3877 
3878         """
3879         migration = objects.Migration.get_by_id_and_instance(context,
3880                     migration_id, instance.uuid)
3881         LOG.debug("Going to cancel live migration %s",
3882                   migration.id, instance=instance)
3883 
3884         if migration.status != 'running':
3885             raise exception.InvalidMigrationState(migration_id=migration_id,
3886                     instance_uuid=instance.uuid,
3887                     state=migration.status,
3888                     method='abort live migration')
3889         self._record_action_start(context, instance,
3890                                   instance_actions.LIVE_MIGRATION_CANCEL)
3891 
3892         self.compute_rpcapi.live_migration_abort(context,
3893                 instance, migration.id)
3894 
3895     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3896                                     vm_states.ERROR])
3897     def evacuate(self, context, instance, host, on_shared_storage,
3898                  admin_password=None, force=None):
3899         """Running evacuate to target host.
3900 
3901         Checking vm compute host state, if the host not in expected_state,
3902         raising an exception.
3903 
3904         :param instance: The instance to evacuate
3905         :param host: Target host. if not set, the scheduler will pick up one
3906         :param on_shared_storage: True if instance files on shared storage
3907         :param admin_password: password to set on rebuilt instance
3908         :param force: Force the evacuation to the specific host target
3909 
3910         """
3911         LOG.debug('vm evacuation scheduled', instance=instance)
3912         inst_host = instance.host
3913         service = objects.Service.get_by_compute_host(context, inst_host)
3914         if self.servicegroup_api.service_is_up(service):
3915             LOG.error('Instance compute service state on %s '
3916                       'expected to be down, but it was up.', inst_host)
3917             raise exception.ComputeServiceInUse(host=inst_host)
3918 
3919         instance.task_state = task_states.REBUILDING
3920         instance.save(expected_task_state=[None])
3921         self._record_action_start(context, instance, instance_actions.EVACUATE)
3922 
3923         # NOTE(danms): Create this as a tombstone for the source compute
3924         # to find and cleanup. No need to pass it anywhere else.
3925         migration = objects.Migration(context,
3926                                       source_compute=instance.host,
3927                                       source_node=instance.node,
3928                                       instance_uuid=instance.uuid,
3929                                       status='accepted',
3930                                       migration_type='evacuation')
3931         if host:
3932             migration.dest_compute = host
3933         migration.create()
3934 
3935         compute_utils.notify_about_instance_usage(
3936             self.notifier, context, instance, "evacuate")
3937 
3938         try:
3939             request_spec = objects.RequestSpec.get_by_instance_uuid(
3940                 context, instance.uuid)
3941         except exception.RequestSpecNotFound:
3942             # Some old instances can still have no RequestSpec object attached
3943             # to them, we need to support the old way
3944             request_spec = None
3945 
3946         # NOTE(sbauza): Force is a boolean by the new related API version
3947         if force is False and host:
3948             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
3949             # NOTE(sbauza): Unset the host to make sure we call the scheduler
3950             host = None
3951             # FIXME(sbauza): Since only Ironic driver uses more than one
3952             # compute per service but doesn't support evacuations,
3953             # let's provide the first one.
3954             target = nodes[0]
3955             if request_spec:
3956                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
3957                 # having a request spec attached to them (particularly true for
3958                 # cells v1). For the moment, let's keep the same behaviour for
3959                 # all the instances but provide the destination only if a spec
3960                 # is found.
3961                 destination = objects.Destination(
3962                     host=target.host,
3963                     node=target.hypervisor_hostname
3964                 )
3965                 request_spec.requested_destination = destination
3966 
3967         return self.compute_task_api.rebuild_instance(context,
3968                        instance=instance,
3969                        new_pass=admin_password,
3970                        injected_files=None,
3971                        image_ref=None,
3972                        orig_image_ref=None,
3973                        orig_sys_metadata=None,
3974                        bdms=None,
3975                        recreate=True,
3976                        on_shared_storage=on_shared_storage,
3977                        host=host,
3978                        request_spec=request_spec,
3979                        )
3980 
3981     def get_migrations(self, context, filters):
3982         """Get all migrations for the given filters."""
3983         load_cells()
3984 
3985         migrations = []
3986         for cell in CELLS:
3987             if cell.uuid == objects.CellMapping.CELL0_UUID:
3988                 continue
3989             with nova_context.target_cell(context, cell) as cctxt:
3990                 migrations.extend(objects.MigrationList.get_by_filters(
3991                     cctxt, filters).objects)
3992         return objects.MigrationList(objects=migrations)
3993 
3994     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
3995                                                migration_type=None):
3996         """Get all migrations of an instance in progress."""
3997         return objects.MigrationList.get_in_progress_by_instance(
3998                 context, instance_uuid, migration_type)
3999 
4000     def get_migration_by_id_and_instance(self, context,
4001                                          migration_id, instance_uuid):
4002         """Get the migration of an instance by id."""
4003         return objects.Migration.get_by_id_and_instance(
4004                 context, migration_id, instance_uuid)
4005 
4006     def volume_snapshot_create(self, context, volume_id, create_info):
4007         bdm = objects.BlockDeviceMapping.get_by_volume(
4008                 context, volume_id, expected_attrs=['instance'])
4009 
4010         # We allow creating the snapshot in any vm_state as long as there is
4011         # no task being performed on the instance and it has a host.
4012         @check_instance_host
4013         @check_instance_state(vm_state=None)
4014         def do_volume_snapshot_create(self, context, instance):
4015             self.compute_rpcapi.volume_snapshot_create(context, instance,
4016                     volume_id, create_info)
4017             snapshot = {
4018                 'snapshot': {
4019                     'id': create_info.get('id'),
4020                     'volumeId': volume_id
4021                 }
4022             }
4023             return snapshot
4024 
4025         return do_volume_snapshot_create(self, context, bdm.instance)
4026 
4027     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
4028                                delete_info):
4029         bdm = objects.BlockDeviceMapping.get_by_volume(
4030                 context, volume_id, expected_attrs=['instance'])
4031 
4032         # We allow deleting the snapshot in any vm_state as long as there is
4033         # no task being performed on the instance and it has a host.
4034         @check_instance_host
4035         @check_instance_state(vm_state=None)
4036         def do_volume_snapshot_delete(self, context, instance):
4037             self.compute_rpcapi.volume_snapshot_delete(context, instance,
4038                     volume_id, snapshot_id, delete_info)
4039 
4040         do_volume_snapshot_delete(self, context, bdm.instance)
4041 
4042     def external_instance_event(self, context, instances, mappings, events):
4043         # NOTE(danms): The external API consumer just provides events,
4044         # but doesn't know where they go. We need to collate lists
4045         # by the host the affected instance is on and dispatch them
4046         # according to host
4047         instances_by_host = collections.defaultdict(list)
4048         events_by_host = collections.defaultdict(list)
4049         hosts_by_instance = collections.defaultdict(list)
4050         for instance in instances:
4051             for host in self._get_relevant_hosts(context, instance):
4052                 instances_by_host[host].append(instance)
4053                 hosts_by_instance[instance.uuid].append(host)
4054 
4055         for event in events:
4056             for host in hosts_by_instance[event.instance_uuid]:
4057                 events_by_host[host].append(event)
4058 
4059         for host in instances_by_host:
4060             # NOTE(danms): All instances on a host must have the same
4061             # mapping, so just use that
4062             cell_mapping = mappings[instances_by_host[host][0].uuid]
4063 
4064             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
4065             # in order to ensure that a failure in processing events on a host
4066             # will not prevent processing events on other hosts
4067             with nova_context.target_cell(context, cell_mapping) as cctxt:
4068                 self.compute_rpcapi.external_instance_event(
4069                     cctxt, instances_by_host[host], events_by_host[host],
4070                     host=host)
4071 
4072     def _get_relevant_hosts(self, context, instance):
4073         hosts = set()
4074         hosts.add(instance.host)
4075         if instance.migration_context is not None:
4076             migration_id = instance.migration_context.migration_id
4077             migration = objects.Migration.get_by_id(context, migration_id)
4078             hosts.add(migration.dest_compute)
4079             hosts.add(migration.source_compute)
4080             LOG.debug('Instance %(instance)s is migrating, '
4081                       'copying events to all relevant hosts: '
4082                       '%(hosts)s', {'instance': instance.uuid,
4083                                     'hosts': hosts})
4084         return hosts
4085 
4086     def get_instance_host_status(self, instance):
4087         if instance.host:
4088             try:
4089                 service = [service for service in instance.services if
4090                            service.binary == 'nova-compute'][0]
4091                 if service.forced_down:
4092                     host_status = fields_obj.HostStatus.DOWN
4093                 elif service.disabled:
4094                     host_status = fields_obj.HostStatus.MAINTENANCE
4095                 else:
4096                     alive = self.servicegroup_api.service_is_up(service)
4097                     host_status = ((alive and fields_obj.HostStatus.UP) or
4098                                    fields_obj.HostStatus.UNKNOWN)
4099             except IndexError:
4100                 host_status = fields_obj.HostStatus.NONE
4101         else:
4102             host_status = fields_obj.HostStatus.NONE
4103         return host_status
4104 
4105     def get_instances_host_statuses(self, instance_list):
4106         host_status_dict = dict()
4107         host_statuses = dict()
4108         for instance in instance_list:
4109             if instance.host:
4110                 if instance.host not in host_status_dict:
4111                     host_status = self.get_instance_host_status(instance)
4112                     host_status_dict[instance.host] = host_status
4113                 else:
4114                     host_status = host_status_dict[instance.host]
4115             else:
4116                 host_status = fields_obj.HostStatus.NONE
4117             host_statuses[instance.uuid] = host_status
4118         return host_statuses
4119 
4120 
4121 def target_host_cell(fn):
4122     """Target a host-based function to a cell.
4123 
4124     Expects to wrap a function of signature:
4125 
4126        func(self, context, host, ...)
4127     """
4128 
4129     @functools.wraps(fn)
4130     def targeted(self, context, host, *args, **kwargs):
4131         mapping = objects.HostMapping.get_by_host(context, host)
4132         nova_context.set_target_cell(context, mapping.cell_mapping)
4133         return fn(self, context, host, *args, **kwargs)
4134     return targeted
4135 
4136 
4137 def _find_service_in_cell(context, service_id=None, service_host=None):
4138     """Find a service by id or hostname by searching all cells.
4139 
4140     If one matching service is found, return it. If none or multiple
4141     are found, raise an exception.
4142 
4143     :param context: A context.RequestContext
4144     :param service_id: If not none, the DB ID of the service to find
4145     :param service_host: If not None, the hostname of the service to find
4146     :returns: An objects.Service
4147     :raises: ServiceNotUnique if multiple matching IDs are found
4148     :raises: NotFound if no matches are found
4149     :raises: NovaException if called with neither search option
4150     """
4151 
4152     load_cells()
4153     service = None
4154     found_in_cell = None
4155 
4156     is_uuid = False
4157     if service_id is not None:
4158         is_uuid = uuidutils.is_uuid_like(service_id)
4159         if is_uuid:
4160             lookup_fn = lambda c: objects.Service.get_by_uuid(c, service_id)
4161         else:
4162             lookup_fn = lambda c: objects.Service.get_by_id(c, service_id)
4163     elif service_host is not None:
4164         lookup_fn = lambda c: (
4165             objects.Service.get_by_compute_host(c, service_host))
4166     else:
4167         LOG.exception('_find_service_in_cell called with no search parameters')
4168         # This is intentionally cryptic so we don't leak implementation details
4169         # out of the API.
4170         raise exception.NovaException()
4171 
4172     for cell in CELLS:
4173         # NOTE(danms): Services can be in cell0, so don't skip it here
4174         try:
4175             with nova_context.target_cell(context, cell) as cctxt:
4176                 cell_service = lookup_fn(cctxt)
4177         except exception.NotFound:
4178             # NOTE(danms): Keep looking in other cells
4179             continue
4180         if service and cell_service:
4181             raise exception.ServiceNotUnique()
4182         service = cell_service
4183         found_in_cell = cell
4184         if service and is_uuid:
4185             break
4186 
4187     if service:
4188         # NOTE(danms): Set the cell on the context so it remains
4189         # when we return to our caller
4190         nova_context.set_target_cell(context, found_in_cell)
4191         return service
4192     else:
4193         raise exception.NotFound()
4194 
4195 
4196 class HostAPI(base.Base):
4197     """Sub-set of the Compute Manager API for managing host operations."""
4198 
4199     def __init__(self, rpcapi=None):
4200         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
4201         self.servicegroup_api = servicegroup.API()
4202         super(HostAPI, self).__init__()
4203 
4204     def _assert_host_exists(self, context, host_name, must_be_up=False):
4205         """Raise HostNotFound if compute host doesn't exist."""
4206         service = objects.Service.get_by_compute_host(context, host_name)
4207         if not service:
4208             raise exception.HostNotFound(host=host_name)
4209         if must_be_up and not self.servicegroup_api.service_is_up(service):
4210             raise exception.ComputeServiceUnavailable(host=host_name)
4211         return service['host']
4212 
4213     @wrap_exception()
4214     @target_host_cell
4215     def set_host_enabled(self, context, host_name, enabled):
4216         """Sets the specified host's ability to accept new instances."""
4217         host_name = self._assert_host_exists(context, host_name)
4218         payload = {'host_name': host_name, 'enabled': enabled}
4219         compute_utils.notify_about_host_update(context,
4220                                                'set_enabled.start',
4221                                                payload)
4222         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
4223                 host=host_name)
4224         compute_utils.notify_about_host_update(context,
4225                                                'set_enabled.end',
4226                                                payload)
4227         return result
4228 
4229     @target_host_cell
4230     def get_host_uptime(self, context, host_name):
4231         """Returns the result of calling "uptime" on the target host."""
4232         host_name = self._assert_host_exists(context, host_name,
4233                          must_be_up=True)
4234         return self.rpcapi.get_host_uptime(context, host=host_name)
4235 
4236     @wrap_exception()
4237     @target_host_cell
4238     def host_power_action(self, context, host_name, action):
4239         """Reboots, shuts down or powers up the host."""
4240         host_name = self._assert_host_exists(context, host_name)
4241         payload = {'host_name': host_name, 'action': action}
4242         compute_utils.notify_about_host_update(context,
4243                                                'power_action.start',
4244                                                payload)
4245         result = self.rpcapi.host_power_action(context, action=action,
4246                 host=host_name)
4247         compute_utils.notify_about_host_update(context,
4248                                                'power_action.end',
4249                                                payload)
4250         return result
4251 
4252     @wrap_exception()
4253     @target_host_cell
4254     def set_host_maintenance(self, context, host_name, mode):
4255         """Start/Stop host maintenance window. On start, it triggers
4256         guest VMs evacuation.
4257         """
4258         host_name = self._assert_host_exists(context, host_name)
4259         payload = {'host_name': host_name, 'mode': mode}
4260         compute_utils.notify_about_host_update(context,
4261                                                'set_maintenance.start',
4262                                                payload)
4263         result = self.rpcapi.host_maintenance_mode(context,
4264                 host_param=host_name, mode=mode, host=host_name)
4265         compute_utils.notify_about_host_update(context,
4266                                                'set_maintenance.end',
4267                                                payload)
4268         return result
4269 
4270     def service_get_all(self, context, filters=None, set_zones=False,
4271                         all_cells=False):
4272         """Returns a list of services, optionally filtering the results.
4273 
4274         If specified, 'filters' should be a dictionary containing services
4275         attributes and matching values.  Ie, to get a list of services for
4276         the 'compute' topic, use filters={'topic': 'compute'}.
4277 
4278         If all_cells=True, then scan all cells and merge the results.
4279         """
4280         if filters is None:
4281             filters = {}
4282         disabled = filters.pop('disabled', None)
4283         if 'availability_zone' in filters:
4284             set_zones = True
4285 
4286         # NOTE(danms): Eventually this all_cells nonsense should go away
4287         # and we should always iterate over the cells. However, certain
4288         # callers need the legacy behavior for now.
4289         if all_cells:
4290             load_cells()
4291             services = []
4292             for cell in CELLS:
4293                 with nova_context.target_cell(context, cell) as cctxt:
4294                     cell_services = objects.ServiceList.get_all(
4295                         cctxt, disabled, set_zones=set_zones)
4296                 services.extend(cell_services)
4297         else:
4298             services = objects.ServiceList.get_all(context, disabled,
4299                                                    set_zones=set_zones)
4300         ret_services = []
4301         for service in services:
4302             for key, val in filters.items():
4303                 if service[key] != val:
4304                     break
4305             else:
4306                 # All filters matched.
4307                 ret_services.append(service)
4308         return ret_services
4309 
4310     def service_get_by_id(self, context, service_id):
4311         """Get service entry for the given service id or uuid."""
4312         try:
4313             return _find_service_in_cell(context, service_id=service_id)
4314         except exception.NotFound:
4315             raise exception.ServiceNotFound(service_id=service_id)
4316 
4317     @target_host_cell
4318     def service_get_by_compute_host(self, context, host_name):
4319         """Get service entry for the given compute hostname."""
4320         return objects.Service.get_by_compute_host(context, host_name)
4321 
4322     def _service_update(self, context, host_name, binary, params_to_update):
4323         """Performs the actual service update operation."""
4324         service = objects.Service.get_by_args(context, host_name, binary)
4325         service.update(params_to_update)
4326         service.save()
4327         return service
4328 
4329     @target_host_cell
4330     def service_update(self, context, host_name, binary, params_to_update):
4331         """Enable / Disable a service.
4332 
4333         For compute services, this stops new builds and migrations going to
4334         the host.
4335         """
4336         return self._service_update(context, host_name, binary,
4337                                     params_to_update)
4338 
4339     def _service_delete(self, context, service_id):
4340         """Performs the actual Service deletion operation."""
4341         try:
4342             service = _find_service_in_cell(context, service_id=service_id)
4343         except exception.NotFound:
4344             raise exception.ServiceNotFound(service_id=service_id)
4345         service.destroy()
4346 
4347     def service_delete(self, context, service_id):
4348         """Deletes the specified service found via id or uuid."""
4349         self._service_delete(context, service_id)
4350 
4351     @target_host_cell
4352     def instance_get_all_by_host(self, context, host_name):
4353         """Return all instances on the given host."""
4354         return objects.InstanceList.get_by_host(context, host_name)
4355 
4356     def task_log_get_all(self, context, task_name, period_beginning,
4357                          period_ending, host=None, state=None):
4358         """Return the task logs within a given range, optionally
4359         filtering by host and/or state.
4360         """
4361         return self.db.task_log_get_all(context, task_name,
4362                                         period_beginning,
4363                                         period_ending,
4364                                         host=host,
4365                                         state=state)
4366 
4367     def compute_node_get(self, context, compute_id):
4368         """Return compute node entry for particular integer ID or UUID."""
4369         load_cells()
4370 
4371         # NOTE(danms): Unfortunately this API exposes database identifiers
4372         # which means we really can't do something efficient here
4373         is_uuid = uuidutils.is_uuid_like(compute_id)
4374         for cell in CELLS:
4375             if cell.uuid == objects.CellMapping.CELL0_UUID:
4376                 continue
4377             with nova_context.target_cell(context, cell) as cctxt:
4378                 try:
4379                     if is_uuid:
4380                         # NOTE(mriedem): We wouldn't have to loop over cells if
4381                         # we stored the ComputeNode.uuid in the HostMapping but
4382                         # we don't have that. It could be added but would
4383                         # require an online data migration to update existing
4384                         # host mappings.
4385                         return objects.ComputeNode.get_by_uuid(cctxt,
4386                                                                compute_id)
4387                     return objects.ComputeNode.get_by_id(cctxt,
4388                                                          int(compute_id))
4389                 except exception.ComputeHostNotFound:
4390                     # NOTE(danms): Keep looking in other cells
4391                     continue
4392 
4393         raise exception.ComputeHostNotFound(host=compute_id)
4394 
4395     def compute_node_get_all(self, context, limit=None, marker=None):
4396         load_cells()
4397 
4398         computes = []
4399         uuid_marker = marker and uuidutils.is_uuid_like(marker)
4400         for cell in CELLS:
4401             if cell.uuid == objects.CellMapping.CELL0_UUID:
4402                 continue
4403             with nova_context.target_cell(context, cell) as cctxt:
4404 
4405                 # If we have a marker and it's a uuid, see if the compute node
4406                 # is in this cell.
4407                 if marker and uuid_marker:
4408                     try:
4409                         compute_marker = objects.ComputeNode.get_by_uuid(
4410                             cctxt, marker)
4411                         # we found the marker compute node, so use it's id
4412                         # for the actual marker for paging in this cell's db
4413                         marker = compute_marker.id
4414                     except exception.ComputeHostNotFound:
4415                         # The marker node isn't in this cell so keep looking.
4416                         continue
4417 
4418                 try:
4419                     cell_computes = objects.ComputeNodeList.get_by_pagination(
4420                         cctxt, limit=limit, marker=marker)
4421                 except exception.MarkerNotFound:
4422                     # NOTE(danms): Keep looking through cells
4423                     continue
4424                 computes.extend(cell_computes)
4425                 # NOTE(danms): We must have found the marker, so continue on
4426                 # without one
4427                 marker = None
4428                 if limit:
4429                     limit -= len(cell_computes)
4430                     if limit <= 0:
4431                         break
4432 
4433         if marker is not None and len(computes) == 0:
4434             # NOTE(danms): If we did not find the marker in any cell,
4435             # mimic the db_api behavior here.
4436             raise exception.MarkerNotFound(marker=marker)
4437 
4438         return objects.ComputeNodeList(objects=computes)
4439 
4440     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
4441         load_cells()
4442 
4443         computes = []
4444         for cell in CELLS:
4445             if cell.uuid == objects.CellMapping.CELL0_UUID:
4446                 continue
4447             with nova_context.target_cell(context, cell) as cctxt:
4448                 cell_computes = objects.ComputeNodeList.get_by_hypervisor(
4449                     cctxt, hypervisor_match)
4450             computes.extend(cell_computes)
4451         return objects.ComputeNodeList(objects=computes)
4452 
4453     def compute_node_statistics(self, context):
4454         load_cells()
4455 
4456         cell_stats = []
4457         for cell in CELLS:
4458             if cell.uuid == objects.CellMapping.CELL0_UUID:
4459                 continue
4460             with nova_context.target_cell(context, cell) as cctxt:
4461                 cell_stats.append(self.db.compute_node_statistics(cctxt))
4462 
4463         if cell_stats:
4464             keys = cell_stats[0].keys()
4465             return {k: sum(stats[k] for stats in cell_stats)
4466                     for k in keys}
4467         else:
4468             return {}
4469 
4470 
4471 class InstanceActionAPI(base.Base):
4472     """Sub-set of the Compute Manager API for managing instance actions."""
4473 
4474     def actions_get(self, context, instance):
4475         return objects.InstanceActionList.get_by_instance_uuid(
4476             context, instance.uuid)
4477 
4478     def action_get_by_request_id(self, context, instance, request_id):
4479         return objects.InstanceAction.get_by_request_id(
4480             context, instance.uuid, request_id)
4481 
4482     def action_events_get(self, context, instance, action_id):
4483         return objects.InstanceActionEventList.get_by_action(
4484             context, action_id)
4485 
4486 
4487 class AggregateAPI(base.Base):
4488     """Sub-set of the Compute Manager API for managing host aggregates."""
4489     def __init__(self, **kwargs):
4490         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4491         self.scheduler_client = scheduler_client.SchedulerClient()
4492         super(AggregateAPI, self).__init__(**kwargs)
4493 
4494     @wrap_exception()
4495     def create_aggregate(self, context, aggregate_name, availability_zone):
4496         """Creates the model for the aggregate."""
4497 
4498         aggregate = objects.Aggregate(context=context)
4499         aggregate.name = aggregate_name
4500         if availability_zone:
4501             aggregate.metadata = {'availability_zone': availability_zone}
4502         aggregate.create()
4503         self.scheduler_client.update_aggregates(context, [aggregate])
4504         return aggregate
4505 
4506     def get_aggregate(self, context, aggregate_id):
4507         """Get an aggregate by id."""
4508         return objects.Aggregate.get_by_id(context, aggregate_id)
4509 
4510     def get_aggregate_list(self, context):
4511         """Get all the aggregates."""
4512         return objects.AggregateList.get_all(context)
4513 
4514     def get_aggregates_by_host(self, context, compute_host):
4515         """Get all the aggregates where the given host is presented."""
4516         return objects.AggregateList.get_by_host(context, compute_host)
4517 
4518     @wrap_exception()
4519     def update_aggregate(self, context, aggregate_id, values):
4520         """Update the properties of an aggregate."""
4521         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4522         if 'name' in values:
4523             aggregate.name = values.pop('name')
4524             aggregate.save()
4525         self.is_safe_to_update_az(context, values, aggregate=aggregate,
4526                                   action_name=AGGREGATE_ACTION_UPDATE)
4527         if values:
4528             aggregate.update_metadata(values)
4529             aggregate.updated_at = timeutils.utcnow()
4530         self.scheduler_client.update_aggregates(context, [aggregate])
4531         # If updated values include availability_zones, then the cache
4532         # which stored availability_zones and host need to be reset
4533         if values.get('availability_zone'):
4534             availability_zones.reset_cache()
4535         return aggregate
4536 
4537     @wrap_exception()
4538     def update_aggregate_metadata(self, context, aggregate_id, metadata):
4539         """Updates the aggregate metadata."""
4540         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4541         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
4542                                   action_name=AGGREGATE_ACTION_UPDATE_META)
4543         aggregate.update_metadata(metadata)
4544         self.scheduler_client.update_aggregates(context, [aggregate])
4545         # If updated metadata include availability_zones, then the cache
4546         # which stored availability_zones and host need to be reset
4547         if metadata and metadata.get('availability_zone'):
4548             availability_zones.reset_cache()
4549         aggregate.updated_at = timeutils.utcnow()
4550         return aggregate
4551 
4552     @wrap_exception()
4553     def delete_aggregate(self, context, aggregate_id):
4554         """Deletes the aggregate."""
4555         aggregate_payload = {'aggregate_id': aggregate_id}
4556         compute_utils.notify_about_aggregate_update(context,
4557                                                     "delete.start",
4558                                                     aggregate_payload)
4559         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4560 
4561         compute_utils.notify_about_aggregate_action(
4562             context=context,
4563             aggregate=aggregate,
4564             action=fields_obj.NotificationAction.DELETE,
4565             phase=fields_obj.NotificationPhase.START)
4566 
4567         if len(aggregate.hosts) > 0:
4568             msg = _("Host aggregate is not empty")
4569             raise exception.InvalidAggregateActionDelete(
4570                 aggregate_id=aggregate_id, reason=msg)
4571         aggregate.destroy()
4572         self.scheduler_client.delete_aggregate(context, aggregate)
4573         compute_utils.notify_about_aggregate_update(context,
4574                                                     "delete.end",
4575                                                     aggregate_payload)
4576         compute_utils.notify_about_aggregate_action(
4577             context=context,
4578             aggregate=aggregate,
4579             action=fields_obj.NotificationAction.DELETE,
4580             phase=fields_obj.NotificationPhase.END)
4581 
4582     def is_safe_to_update_az(self, context, metadata, aggregate,
4583                              hosts=None,
4584                              action_name=AGGREGATE_ACTION_ADD):
4585         """Determine if updates alter an aggregate's availability zone.
4586 
4587             :param context: local context
4588             :param metadata: Target metadata for updating aggregate
4589             :param aggregate: Aggregate to update
4590             :param hosts: Hosts to check. If None, aggregate.hosts is used
4591             :type hosts: list
4592             :action_name: Calling method for logging purposes
4593 
4594         """
4595         if 'availability_zone' in metadata:
4596             if not metadata['availability_zone']:
4597                 msg = _("Aggregate %s does not support empty named "
4598                         "availability zone") % aggregate.name
4599                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4600                                                   msg)
4601             _hosts = hosts or aggregate.hosts
4602             host_aggregates = objects.AggregateList.get_by_metadata_key(
4603                 context, 'availability_zone', hosts=_hosts)
4604             conflicting_azs = [
4605                 agg.availability_zone for agg in host_aggregates
4606                 if agg.availability_zone != metadata['availability_zone']
4607                 and agg.id != aggregate.id]
4608             if conflicting_azs:
4609                 msg = _("One or more hosts already in availability zone(s) "
4610                         "%s") % conflicting_azs
4611                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4612                                                   msg)
4613 
4614     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
4615         if action_name == AGGREGATE_ACTION_ADD:
4616             raise exception.InvalidAggregateActionAdd(
4617                 aggregate_id=aggregate_id, reason=reason)
4618         elif action_name == AGGREGATE_ACTION_UPDATE:
4619             raise exception.InvalidAggregateActionUpdate(
4620                 aggregate_id=aggregate_id, reason=reason)
4621         elif action_name == AGGREGATE_ACTION_UPDATE_META:
4622             raise exception.InvalidAggregateActionUpdateMeta(
4623                 aggregate_id=aggregate_id, reason=reason)
4624         elif action_name == AGGREGATE_ACTION_DELETE:
4625             raise exception.InvalidAggregateActionDelete(
4626                 aggregate_id=aggregate_id, reason=reason)
4627 
4628         raise exception.NovaException(
4629             _("Unexpected aggregate action %s") % action_name)
4630 
4631     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
4632         # Update the availability_zone cache to avoid getting wrong
4633         # availability_zone in cache retention time when add/remove
4634         # host to/from aggregate.
4635         if aggregate_meta and aggregate_meta.get('availability_zone'):
4636             availability_zones.update_host_availability_zone_cache(context,
4637                                                                    host_name)
4638 
4639     @wrap_exception()
4640     def add_host_to_aggregate(self, context, aggregate_id, host_name):
4641         """Adds the host to an aggregate."""
4642         aggregate_payload = {'aggregate_id': aggregate_id,
4643                              'host_name': host_name}
4644         compute_utils.notify_about_aggregate_update(context,
4645                                                     "addhost.start",
4646                                                     aggregate_payload)
4647         # validates the host; HostMappingNotFound or ComputeHostNotFound
4648         # is raised if invalid
4649         try:
4650             mapping = objects.HostMapping.get_by_host(context, host_name)
4651             nova_context.set_target_cell(context, mapping.cell_mapping)
4652             objects.Service.get_by_compute_host(context, host_name)
4653         except exception.HostMappingNotFound:
4654             try:
4655                 # NOTE(danms): This targets our cell
4656                 _find_service_in_cell(context, service_host=host_name)
4657             except exception.NotFound:
4658                 raise exception.ComputeHostNotFound(host=host_name)
4659 
4660         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4661         self.is_safe_to_update_az(context, aggregate.metadata,
4662                                   hosts=[host_name], aggregate=aggregate)
4663 
4664         aggregate.add_host(host_name)
4665         self.scheduler_client.update_aggregates(context, [aggregate])
4666         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
4667         # NOTE(jogo): Send message to host to support resource pools
4668         self.compute_rpcapi.add_aggregate_host(context,
4669                 aggregate=aggregate, host_param=host_name, host=host_name)
4670         aggregate_payload.update({'name': aggregate.name})
4671         compute_utils.notify_about_aggregate_update(context,
4672                                                     "addhost.end",
4673                                                     aggregate_payload)
4674         return aggregate
4675 
4676     @wrap_exception()
4677     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
4678         """Removes host from the aggregate."""
4679         aggregate_payload = {'aggregate_id': aggregate_id,
4680                              'host_name': host_name}
4681         compute_utils.notify_about_aggregate_update(context,
4682                                                     "removehost.start",
4683                                                     aggregate_payload)
4684         # validates the host; HostMappingNotFound or ComputeHostNotFound
4685         # is raised if invalid
4686         mapping = objects.HostMapping.get_by_host(context, host_name)
4687         nova_context.set_target_cell(context, mapping.cell_mapping)
4688         objects.Service.get_by_compute_host(context, host_name)
4689         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4690         aggregate.delete_host(host_name)
4691         self.scheduler_client.update_aggregates(context, [aggregate])
4692         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
4693         self.compute_rpcapi.remove_aggregate_host(context,
4694                 aggregate=aggregate, host_param=host_name, host=host_name)
4695         compute_utils.notify_about_aggregate_update(context,
4696                                                     "removehost.end",
4697                                                     aggregate_payload)
4698         return aggregate
4699 
4700 
4701 class KeypairAPI(base.Base):
4702     """Subset of the Compute Manager API for managing key pairs."""
4703 
4704     get_notifier = functools.partial(rpc.get_notifier, service='api')
4705     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
4706                                        get_notifier=get_notifier,
4707                                        binary='nova-api')
4708 
4709     def _notify(self, context, event_suffix, keypair_name):
4710         payload = {
4711             'tenant_id': context.project_id,
4712             'user_id': context.user_id,
4713             'key_name': keypair_name,
4714         }
4715         notify = self.get_notifier()
4716         notify.info(context, 'keypair.%s' % event_suffix, payload)
4717 
4718     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
4719         safe_chars = "_- " + string.digits + string.ascii_letters
4720         clean_value = "".join(x for x in key_name if x in safe_chars)
4721         if clean_value != key_name:
4722             raise exception.InvalidKeypair(
4723                 reason=_("Keypair name contains unsafe characters"))
4724 
4725         try:
4726             utils.check_string_length(key_name, min_length=1, max_length=255)
4727         except exception.InvalidInput:
4728             raise exception.InvalidKeypair(
4729                 reason=_('Keypair name must be string and between '
4730                          '1 and 255 characters long'))
4731 
4732         count = objects.Quotas.count_as_dict(context, 'key_pairs', user_id)
4733         count_value = count['user']['key_pairs']
4734 
4735         try:
4736             objects.Quotas.limit_check(context, key_pairs=count_value + 1)
4737         except exception.OverQuota:
4738             raise exception.KeypairLimitExceeded()
4739 
4740     @wrap_exception()
4741     def import_key_pair(self, context, user_id, key_name, public_key,
4742                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
4743         """Import a key pair using an existing public key."""
4744         self._validate_new_key_pair(context, user_id, key_name, key_type)
4745 
4746         self._notify(context, 'import.start', key_name)
4747 
4748         fingerprint = self._generate_fingerprint(public_key, key_type)
4749 
4750         keypair = objects.KeyPair(context)
4751         keypair.user_id = user_id
4752         keypair.name = key_name
4753         keypair.type = key_type
4754         keypair.fingerprint = fingerprint
4755         keypair.public_key = public_key
4756         keypair.create()
4757 
4758         self._notify(context, 'import.end', key_name)
4759 
4760         return keypair
4761 
4762     @wrap_exception()
4763     def create_key_pair(self, context, user_id, key_name,
4764                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
4765         """Create a new key pair."""
4766         self._validate_new_key_pair(context, user_id, key_name, key_type)
4767 
4768         keypair = objects.KeyPair(context)
4769         keypair.user_id = user_id
4770         keypair.name = key_name
4771         keypair.type = key_type
4772         keypair.fingerprint = None
4773         keypair.public_key = None
4774 
4775         self._notify(context, 'create.start', key_name)
4776         compute_utils.notify_about_keypair_action(
4777             context=context,
4778             keypair=keypair,
4779             action=fields_obj.NotificationAction.CREATE,
4780             phase=fields_obj.NotificationPhase.START)
4781 
4782         private_key, public_key, fingerprint = self._generate_key_pair(
4783             user_id, key_type)
4784 
4785         keypair.fingerprint = fingerprint
4786         keypair.public_key = public_key
4787         keypair.create()
4788         compute_utils.notify_about_keypair_action(
4789             context=context,
4790             keypair=keypair,
4791             action=fields_obj.NotificationAction.CREATE,
4792             phase=fields_obj.NotificationPhase.END)
4793 
4794         self._notify(context, 'create.end', key_name)
4795 
4796         return keypair, private_key
4797 
4798     def _generate_fingerprint(self, public_key, key_type):
4799         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4800             return crypto.generate_fingerprint(public_key)
4801         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4802             return crypto.generate_x509_fingerprint(public_key)
4803 
4804     def _generate_key_pair(self, user_id, key_type):
4805         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4806             return crypto.generate_key_pair()
4807         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4808             return crypto.generate_winrm_x509_cert(user_id)
4809 
4810     @wrap_exception()
4811     def delete_key_pair(self, context, user_id, key_name):
4812         """Delete a keypair by name."""
4813         self._notify(context, 'delete.start', key_name)
4814         objects.KeyPair.destroy_by_name(context, user_id, key_name)
4815         self._notify(context, 'delete.end', key_name)
4816 
4817     def get_key_pairs(self, context, user_id, limit=None, marker=None):
4818         """List key pairs."""
4819         return objects.KeyPairList.get_by_user(
4820             context, user_id, limit=limit, marker=marker)
4821 
4822     def get_key_pair(self, context, user_id, key_name):
4823         """Get a keypair by name."""
4824         return objects.KeyPair.get_by_name(context, user_id, key_name)
4825 
4826 
4827 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
4828     """Sub-set of the Compute API related to managing security groups
4829     and security group rules
4830     """
4831 
4832     # The nova security group api does not use a uuid for the id.
4833     id_is_uuid = False
4834 
4835     def __init__(self, **kwargs):
4836         super(SecurityGroupAPI, self).__init__(**kwargs)
4837         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4838 
4839     def validate_property(self, value, property, allowed):
4840         """Validate given security group property.
4841 
4842         :param value:          the value to validate, as a string or unicode
4843         :param property:       the property, either 'name' or 'description'
4844         :param allowed:        the range of characters allowed
4845         """
4846 
4847         try:
4848             val = value.strip()
4849         except AttributeError:
4850             msg = _("Security group %s is not a string or unicode") % property
4851             self.raise_invalid_property(msg)
4852         utils.check_string_length(val, name=property, min_length=1,
4853                                   max_length=255)
4854 
4855         if allowed and not re.match(allowed, val):
4856             # Some validation to ensure that values match API spec.
4857             # - Alphanumeric characters, spaces, dashes, and underscores.
4858             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
4859             #  probably create a param validator that can be used elsewhere.
4860             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
4861                      "invalid. Content limited to '%(allowed)s'.") %
4862                    {'value': value, 'allowed': allowed,
4863                     'property': property.capitalize()})
4864             self.raise_invalid_property(msg)
4865 
4866     def ensure_default(self, context):
4867         """Ensure that a context has a security group.
4868 
4869         Creates a security group for the security context if it does not
4870         already exist.
4871 
4872         :param context: the security context
4873         """
4874         self.db.security_group_ensure_default(context)
4875 
4876     def create_security_group(self, context, name, description):
4877         try:
4878             objects.Quotas.check_deltas(context, {'security_groups': 1},
4879                                         context.project_id,
4880                                         user_id=context.user_id)
4881         except exception.OverQuota:
4882             msg = _("Quota exceeded, too many security groups.")
4883             self.raise_over_quota(msg)
4884 
4885         LOG.info("Create Security Group %s", name)
4886 
4887         self.ensure_default(context)
4888 
4889         group = {'user_id': context.user_id,
4890                  'project_id': context.project_id,
4891                  'name': name,
4892                  'description': description}
4893         try:
4894             group_ref = self.db.security_group_create(context, group)
4895         except exception.SecurityGroupExists:
4896             msg = _('Security group %s already exists') % name
4897             self.raise_group_already_exists(msg)
4898 
4899         # NOTE(melwitt): We recheck the quota after creating the object to
4900         # prevent users from allocating more resources than their allowed quota
4901         # in the event of a race. This is configurable because it can be
4902         # expensive if strict quota limits are not required in a deployment.
4903         if CONF.quota.recheck_quota:
4904             try:
4905                 objects.Quotas.check_deltas(context, {'security_groups': 0},
4906                                             context.project_id,
4907                                             user_id=context.user_id)
4908             except exception.OverQuota:
4909                 self.db.security_group_destroy(context, group_ref['id'])
4910                 msg = _("Quota exceeded, too many security groups.")
4911                 self.raise_over_quota(msg)
4912 
4913         return group_ref
4914 
4915     def update_security_group(self, context, security_group,
4916                                 name, description):
4917         if security_group['name'] in RO_SECURITY_GROUPS:
4918             msg = (_("Unable to update system group '%s'") %
4919                     security_group['name'])
4920             self.raise_invalid_group(msg)
4921 
4922         group = {'name': name,
4923                  'description': description}
4924 
4925         columns_to_join = ['rules.grantee_group']
4926         group_ref = self.db.security_group_update(context,
4927                 security_group['id'],
4928                 group,
4929                 columns_to_join=columns_to_join)
4930         return group_ref
4931 
4932     def get(self, context, name=None, id=None, map_exception=False):
4933         self.ensure_default(context)
4934         cols = ['rules']
4935         try:
4936             if name:
4937                 return self.db.security_group_get_by_name(context,
4938                                                           context.project_id,
4939                                                           name,
4940                                                           columns_to_join=cols)
4941             elif id:
4942                 return self.db.security_group_get(context, id,
4943                                                   columns_to_join=cols)
4944         except exception.NotFound as exp:
4945             if map_exception:
4946                 msg = exp.format_message()
4947                 self.raise_not_found(msg)
4948             else:
4949                 raise
4950 
4951     def list(self, context, names=None, ids=None, project=None,
4952              search_opts=None):
4953         self.ensure_default(context)
4954 
4955         groups = []
4956         if names or ids:
4957             if names:
4958                 for name in names:
4959                     groups.append(self.db.security_group_get_by_name(context,
4960                                                                      project,
4961                                                                      name))
4962             if ids:
4963                 for id in ids:
4964                     groups.append(self.db.security_group_get(context, id))
4965 
4966         elif context.is_admin:
4967             # TODO(eglynn): support a wider set of search options than just
4968             # all_tenants, at least include the standard filters defined for
4969             # the EC2 DescribeSecurityGroups API for the non-admin case also
4970             if (search_opts and 'all_tenants' in search_opts):
4971                 groups = self.db.security_group_get_all(context)
4972             else:
4973                 groups = self.db.security_group_get_by_project(context,
4974                                                                project)
4975 
4976         elif project:
4977             groups = self.db.security_group_get_by_project(context, project)
4978 
4979         return groups
4980 
4981     def destroy(self, context, security_group):
4982         if security_group['name'] in RO_SECURITY_GROUPS:
4983             msg = _("Unable to delete system group '%s'") % \
4984                     security_group['name']
4985             self.raise_invalid_group(msg)
4986 
4987         if self.db.security_group_in_use(context, security_group['id']):
4988             msg = _("Security group is still in use")
4989             self.raise_invalid_group(msg)
4990 
4991         LOG.info("Delete security group %s", security_group['name'])
4992         self.db.security_group_destroy(context, security_group['id'])
4993 
4994     def is_associated_with_server(self, security_group, instance_uuid):
4995         """Check if the security group is already associated
4996            with the instance. If Yes, return True.
4997         """
4998 
4999         if not security_group:
5000             return False
5001 
5002         instances = security_group.get('instances')
5003         if not instances:
5004             return False
5005 
5006         for inst in instances:
5007             if (instance_uuid == inst['uuid']):
5008                 return True
5009 
5010         return False
5011 
5012     def add_to_instance(self, context, instance, security_group_name):
5013         """Add security group to the instance."""
5014         security_group = self.db.security_group_get_by_name(context,
5015                 context.project_id,
5016                 security_group_name)
5017 
5018         instance_uuid = instance.uuid
5019 
5020         # check if the security group is associated with the server
5021         if self.is_associated_with_server(security_group, instance_uuid):
5022             raise exception.SecurityGroupExistsForInstance(
5023                                         security_group_id=security_group['id'],
5024                                         instance_id=instance_uuid)
5025 
5026         self.db.instance_add_security_group(context.elevated(),
5027                                             instance_uuid,
5028                                             security_group['id'])
5029         if instance.host:
5030             self.compute_rpcapi.refresh_instance_security_rules(
5031                     context, instance, instance.host)
5032 
5033     def remove_from_instance(self, context, instance, security_group_name):
5034         """Remove the security group associated with the instance."""
5035         security_group = self.db.security_group_get_by_name(context,
5036                 context.project_id,
5037                 security_group_name)
5038 
5039         instance_uuid = instance.uuid
5040 
5041         # check if the security group is associated with the server
5042         if not self.is_associated_with_server(security_group, instance_uuid):
5043             raise exception.SecurityGroupNotExistsForInstance(
5044                                     security_group_id=security_group['id'],
5045                                     instance_id=instance_uuid)
5046 
5047         self.db.instance_remove_security_group(context.elevated(),
5048                                                instance_uuid,
5049                                                security_group['id'])
5050         if instance.host:
5051             self.compute_rpcapi.refresh_instance_security_rules(
5052                     context, instance, instance.host)
5053 
5054     def get_rule(self, context, id):
5055         self.ensure_default(context)
5056         try:
5057             return self.db.security_group_rule_get(context, id)
5058         except exception.NotFound:
5059             msg = _("Rule (%s) not found") % id
5060             self.raise_not_found(msg)
5061 
5062     def add_rules(self, context, id, name, vals):
5063         """Add security group rule(s) to security group.
5064 
5065         Note: the Nova security group API doesn't support adding multiple
5066         security group rules at once but the EC2 one does. Therefore,
5067         this function is written to support both.
5068         """
5069 
5070         count = objects.Quotas.count_as_dict(context,
5071                                              'security_group_rules', id)
5072         count_value = count['user']['security_group_rules']
5073         try:
5074             projected = count_value + len(vals)
5075             objects.Quotas.limit_check(context, security_group_rules=projected)
5076         except exception.OverQuota:
5077             msg = _("Quota exceeded, too many security group rules.")
5078             self.raise_over_quota(msg)
5079 
5080         msg = ("Security group %(name)s added %(protocol)s ingress "
5081                "(%(from_port)s:%(to_port)s)")
5082         rules = []
5083         for v in vals:
5084             rule = self.db.security_group_rule_create(context, v)
5085             rules.append(rule)
5086             LOG.info(msg, {'name': name,
5087                            'protocol': rule.protocol,
5088                            'from_port': rule.from_port,
5089                            'to_port': rule.to_port})
5090 
5091         self.trigger_rules_refresh(context, id=id)
5092         return rules
5093 
5094     def remove_rules(self, context, security_group, rule_ids):
5095         msg = ("Security group %(name)s removed %(protocol)s ingress "
5096                "(%(from_port)s:%(to_port)s)")
5097         for rule_id in rule_ids:
5098             rule = self.get_rule(context, rule_id)
5099             LOG.info(msg, {'name': security_group['name'],
5100                            'protocol': rule.protocol,
5101                            'from_port': rule.from_port,
5102                            'to_port': rule.to_port})
5103 
5104             self.db.security_group_rule_destroy(context, rule_id)
5105 
5106         # NOTE(vish): we removed some rules, so refresh
5107         self.trigger_rules_refresh(context, id=security_group['id'])
5108 
5109     def remove_default_rules(self, context, rule_ids):
5110         for rule_id in rule_ids:
5111             self.db.security_group_default_rule_destroy(context, rule_id)
5112 
5113     def add_default_rules(self, context, vals):
5114         rules = [self.db.security_group_default_rule_create(context, v)
5115                  for v in vals]
5116         return rules
5117 
5118     def default_rule_exists(self, context, values):
5119         """Indicates whether the specified rule values are already
5120            defined in the default security group rules.
5121         """
5122         for rule in self.db.security_group_default_rule_list(context):
5123             keys = ('cidr', 'from_port', 'to_port', 'protocol')
5124             for key in keys:
5125                 if rule.get(key) != values.get(key):
5126                     break
5127             else:
5128                 return rule.get('id') or True
5129         return False
5130 
5131     def get_all_default_rules(self, context):
5132         try:
5133             rules = self.db.security_group_default_rule_list(context)
5134         except Exception:
5135             msg = 'cannot get default security group rules'
5136             raise exception.SecurityGroupDefaultRuleNotFound(msg)
5137 
5138         return rules
5139 
5140     def get_default_rule(self, context, id):
5141         return self.db.security_group_default_rule_get(context, id)
5142 
5143     def validate_id(self, id):
5144         try:
5145             return int(id)
5146         except ValueError:
5147             msg = _("Security group id should be integer")
5148             self.raise_invalid_property(msg)
5149 
5150     def _refresh_instance_security_rules(self, context, instances):
5151         for instance in instances:
5152             if instance.host is not None:
5153                 self.compute_rpcapi.refresh_instance_security_rules(
5154                         context, instance, instance.host)
5155 
5156     def trigger_rules_refresh(self, context, id):
5157         """Called when a rule is added to or removed from a security_group."""
5158         instances = objects.InstanceList.get_by_security_group_id(context, id)
5159         self._refresh_instance_security_rules(context, instances)
5160 
5161     def trigger_members_refresh(self, context, group_ids):
5162         """Called when a security group gains a new or loses a member.
5163 
5164         Sends an update request to each compute node for each instance for
5165         which this is relevant.
5166         """
5167         instances = objects.InstanceList.get_by_grantee_security_group_ids(
5168             context, group_ids)
5169         self._refresh_instance_security_rules(context, instances)
5170 
5171     def get_instance_security_groups(self, context, instance, detailed=False):
5172         if detailed:
5173             return self.db.security_group_get_by_instance(context,
5174                                                           instance.uuid)
5175         return [{'name': group.name} for group in instance.security_groups]
