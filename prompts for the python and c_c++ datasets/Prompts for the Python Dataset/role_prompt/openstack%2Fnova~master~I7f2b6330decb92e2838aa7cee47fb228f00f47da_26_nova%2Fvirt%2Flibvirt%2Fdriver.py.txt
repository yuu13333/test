I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, and Parallels.
25 """
26 
27 import binascii
28 import collections
29 from collections import deque
30 import contextlib
31 import copy
32 import errno
33 import functools
34 import glob
35 import grp
36 import itertools
37 import operator
38 import os
39 import pwd
40 import random
41 import shutil
42 import sys
43 import tempfile
44 import threading
45 import time
46 import typing as ty
47 import uuid
48 
49 from castellan import key_manager
50 from copy import deepcopy
51 import eventlet
52 from eventlet import greenthread
53 from eventlet import tpool
54 from lxml import etree
55 from os_brick import encryptors
56 from os_brick.encryptors import luks as luks_encryptor
57 from os_brick import exception as brick_exception
58 from os_brick.initiator import connector
59 import os_resource_classes as orc
60 import os_traits as ot
61 from oslo_concurrency import processutils
62 from oslo_log import log as logging
63 from oslo_serialization import base64
64 from oslo_serialization import jsonutils
65 from oslo_service import loopingcall
66 from oslo_utils import encodeutils
67 from oslo_utils import excutils
68 from oslo_utils import fileutils
69 from oslo_utils import importutils
70 from oslo_utils import netutils as oslo_netutils
71 from oslo_utils import strutils
72 from oslo_utils import timeutils
73 from oslo_utils import units
74 from oslo_utils import uuidutils
75 
76 from nova.api.metadata import base as instance_metadata
77 from nova.api.metadata import password
78 from nova import block_device
79 from nova.compute import power_state
80 from nova.compute import provider_tree
81 from nova.compute import task_states
82 from nova.compute import utils as compute_utils
83 from nova.compute import vm_states
84 import nova.conf
85 from nova.console import serial as serial_console
86 from nova.console import type as ctype
87 from nova import context as nova_context
88 from nova import crypto
89 from nova.db import constants as db_const
90 from nova import exception
91 from nova.i18n import _
92 from nova.image import glance
93 from nova.network import model as network_model
94 from nova import objects
95 from nova.objects import diagnostics as diagnostics_obj
96 from nova.objects import fields
97 from nova.pci import manager as pci_manager
98 from nova.pci import utils as pci_utils
99 import nova.privsep.libvirt
100 import nova.privsep.path
101 import nova.privsep.utils
102 from nova.storage import rbd_utils
103 from nova import utils
104 from nova import version
105 from nova.virt import block_device as driver_block_device
106 from nova.virt import configdrive
107 from nova.virt.disk import api as disk_api
108 from nova.virt.disk.vfs import guestfs
109 from nova.virt import driver
110 from nova.virt import event as virtevent
111 from nova.virt import hardware
112 from nova.virt.image import model as imgmodel
113 from nova.virt import images
114 from nova.virt.libvirt import blockinfo
115 from nova.virt.libvirt import config as vconfig
116 from nova.virt.libvirt import designer
117 from nova.virt.libvirt import event as libvirtevent
118 from nova.virt.libvirt import guest as libvirt_guest
119 from nova.virt.libvirt import host
120 from nova.virt.libvirt import imagebackend
121 from nova.virt.libvirt import imagecache
122 from nova.virt.libvirt import instancejobtracker
123 from nova.virt.libvirt import migration as libvirt_migrate
124 from nova.virt.libvirt.storage import dmcrypt
125 from nova.virt.libvirt.storage import lvm
126 from nova.virt.libvirt import utils as libvirt_utils
127 from nova.virt.libvirt import vif as libvirt_vif
128 from nova.virt.libvirt.volume import fs
129 from nova.virt.libvirt.volume import mount
130 from nova.virt.libvirt.volume import remotefs
131 from nova.virt.libvirt.volume import volume
132 from nova.virt import netutils
133 from nova.volume import cinder
134 
135 libvirt: ty.Any = None
136 
137 uefi_logged = False
138 
139 LOG = logging.getLogger(__name__)
140 
141 CONF = nova.conf.CONF
142 
143 MAX_CONSOLE_BYTES = 100 * units.Ki
144 VALID_DISK_CACHEMODES = [
145     "default", "none", "writethrough", "writeback", "directsync", "unsafe",
146 ]
147 
148 # The libvirt driver will prefix any disable reason codes with this string.
149 DISABLE_PREFIX = 'AUTO: '
150 # Disable reason for the service which was enabled or disabled without reason
151 DISABLE_REASON_UNDEFINED = None
152 
153 # Guest config console string
154 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
155 
156 GuestNumaConfig = collections.namedtuple(
157     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
158 
159 
160 class InjectionInfo(collections.namedtuple(
161         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
162     __slots__ = ()
163 
164     def __repr__(self):
165         return ('InjectionInfo(network_info=%r, files=%r, '
166                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
167 
168 
169 # NOTE(lyarwood): Dict of volume drivers supported by the libvirt driver, keyed
170 # by the connection_info['driver_volume_type'] returned by Cinder for each
171 # volume type it supports
172 # TODO(lyarwood): Add host configurables to allow this list to be changed.
173 # Allowing native iSCSI to be reintroduced etc.
174 VOLUME_DRIVERS = {
175     'iscsi': 'nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
176     'iser': 'nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
177     'local': 'nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
178     'fake': 'nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
179     'rbd': 'nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
180     'nfs': 'nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
181     'smbfs': 'nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
182     'fibre_channel': 'nova.virt.libvirt.volume.fibrechannel.LibvirtFibreChannelVolumeDriver',  # noqa:E501
183     'gpfs': 'nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
184     'quobyte': 'nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
185     'scaleio': 'nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
186     'vzstorage': 'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',  # noqa:E501
187     'storpool': 'nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',  # noqa:E501
188     'nvmeof': 'nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
189 }
190 
191 
192 def patch_tpool_proxy():
193     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
194     or __repr__() calls. See bug #962840 for details.
195     We perform a monkey patch to replace those two instance methods.
196     """
197     def str_method(self):
198         return str(self._obj)
199 
200     def repr_method(self):
201         return repr(self._obj)
202 
203     tpool.Proxy.__str__ = str_method
204     tpool.Proxy.__repr__ = repr_method
205 
206 
207 patch_tpool_proxy()
208 
209 # For information about when MIN_{LIBVIRT,QEMU}_VERSION and
210 # NEXT_MIN_{LIBVIRT,QEMU}_VERSION can be changed, consult the following:
211 #
212 # doc/source/reference/libvirt-distro-support-matrix.rst
213 #
214 # DO NOT FORGET to update this document when touching any versions below!
215 MIN_LIBVIRT_VERSION = (6, 0, 0)
216 MIN_QEMU_VERSION = (4, 2, 0)
217 NEXT_MIN_LIBVIRT_VERSION = (7, 0, 0)
218 NEXT_MIN_QEMU_VERSION = (5, 2, 0)
219 
220 # Virtuozzo driver support
221 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
222 
223 
224 # Names of the types that do not get compressed during migration
225 NO_COMPRESSION_TYPES = ('qcow2',)
226 
227 
228 # number of serial console limit
229 QEMU_MAX_SERIAL_PORTS = 4
230 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
231 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
232 
233 VGPU_RESOURCE_SEMAPHORE = 'vgpu_resources'
234 
235 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
236 
237 # VDPA interface support
238 MIN_LIBVIRT_VDPA = (6, 9, 0)
239 MIN_QEMU_VDPA = (5, 1, 0)
240 
241 
242 class AsyncDeviceEventsHandler:
243     """A synchornization point between libvirt events an clients waiting for
244     such events.
245 
246     It provides an interface for the clients to wait for one or more libvirt
247     event types. It implements event delivery by expecting the libvirt driver
248     to forward libvirt specific events to notify_waiters()
249 
250     It handles multiple clients for the same instance, device and event
251     type and delivers the event to each clients.
252     """
253 
254     class Waiter:
255         def __init__(
256             self,
257             instance_uuid: str,
258             device_name: str,
259             event_types: ty.Set[ty.Type[libvirtevent.DeviceEvent]]
260         ):
261             self.instance_uuid = instance_uuid
262             self.device_name = device_name
263             self.event_types = event_types
264             self.threading_event = threading.Event()
265             self.result: ty.Optional[libvirtevent.DeviceEvent] = None
266 
267         def matches(self, event: libvirtevent.DeviceEvent) -> bool:
268             """Returns true if the event is one of the expected event types
269             for the given instance and device.
270             """
271             return (
272                 self.instance_uuid == event.uuid and
273                 self.device_name == event.dev and
274                 isinstance(event, tuple(self.event_types)))
275 
276         def __repr__(self) -> str:
277             return (
278                 "AsyncDeviceEventsHandler.Waiter("
279                 f"instance_uuid={self.instance_uuid}, "
280                 f"device_name={self.device_name}, "
281                 f"event_types={self.event_types})")
282 
283     def __init__(self):
284         self._lock = threading.Lock()
285         # Ongoing device operations in libvirt where we wait for the events
286         # about success or failure.
287         self._waiters: ty.Set[AsyncDeviceEventsHandler.Waiter] = set()
288 
289     def create_waiter(
290         self,
291         instance_uuid: str,
292         device_name: str,
293         event_types: ty.Set[ty.Type[libvirtevent.DeviceEvent]]
294     ) -> 'AsyncDeviceEventsHandler.Waiter':
295         """Returns an opaque token the caller can use in wait() to
296         wait for the libvirt event
297 
298         :param instance_uuid: The UUID of the instance.
299         :param device_name: The device name alias used by libvirt for this
300             device.
301         :param event_type: A set of classes derived from DeviceEvent
302             specifying which event types the caller waits for. Specifying more
303             than one event type means waiting for either of the events to be
304             received.
305         :returns: an opaque token to be used with wait_for_event().
306         """
307         waiter = AsyncDeviceEventsHandler.Waiter(
308             instance_uuid, device_name, event_types)
309         with self._lock:
310             self._waiters.add(waiter)
311 
312         return waiter
313 
314     def delete_waiter(self, token: 'AsyncDeviceEventsHandler.Waiter'):
315         """Deletes the waiter
316 
317         :param token: the opaque token returned by create_waiter() to be
318             deleted
319         """
320         with self._lock:
321             self._waiters.remove(token)
322 
323     def wait(
324         self, token: 'AsyncDeviceEventsHandler.Waiter', timeout: float,
325     ) -> ty.Optional[libvirtevent.DeviceEvent]:
326         """Blocks waiting for the libvirt event represented by the opaque token
327 
328         :param token: A token created by calling create_waiter()
329         :param timeout: Maximum number of seconds this call blocks waiting for
330             the event to be received
331         :returns: The received libvirt event, or None in case of timeout
332         """
333         token.threading_event.wait(timeout)
334 
335         with self._lock:
336             self._waiters.remove(token)
337 
338         return token.result
339 
340     def notify_waiters(self, event: libvirtevent.DeviceEvent) -> bool:
341         """Unblocks the client waiting for this event.
342 
343         :param event: the libvirt event that is received
344         :returns: True if there was a client waiting and False otherwise.
345         """
346         dispatched = False
347         with self._lock:
348             for waiter in self._waiters:
349                 if waiter.matches(event):
350                     waiter.result = event
351                     waiter.threading_event.set()
352                     dispatched = True
353 
354         return dispatched
355 
356     def cleanup_waiters(self, instance_uuid: str) -> None:
357         """Deletes all waiters and unblock all clients related to the specific
358         instance.
359 
360         param instance_uuid: The instance UUID for which the cleanup is
361             requested
362         """
363         with self._lock:
364             instance_waiters = set()
365             for waiter in self._waiters:
366                 if waiter.instance_uuid == instance_uuid:
367                     # unblock any waiting thread
368                     waiter.threading_event.set()
369                     instance_waiters.add(waiter)
370 
371             self._waiters -= instance_waiters
372 
373         if instance_waiters:
374             LOG.debug(
375                 'Cleaned up device related libvirt event waiters: %s',
376                 instance_waiters)
377 
378 
379 class LibvirtDriver(driver.ComputeDriver):
380     def __init__(self, virtapi, read_only=False):
381         # NOTE(aspiers) Some of these are dynamic, so putting
382         # capabilities on the instance rather than on the class.
383         # This prevents the risk of one test setting a capability
384         # which bleeds over into other tests.
385 
386         # LVM and RBD require raw images. If we are not configured to
387         # force convert images into raw format, then we _require_ raw
388         # images only.
389         raw_only = ('rbd', 'lvm')
390         requires_raw_image = (CONF.libvirt.images_type in raw_only and
391                               not CONF.force_raw_images)
392         requires_ploop_image = CONF.libvirt.virt_type == 'parallels'
393 
394         self.capabilities = {
395             "has_imagecache": True,
396             "supports_evacuate": True,
397             "supports_migrate_to_same_host": False,
398             "supports_attach_interface": True,
399             "supports_device_tagging": True,
400             "supports_tagged_attach_interface": True,
401             "supports_tagged_attach_volume": True,
402             "supports_extend_volume": True,
403             "supports_multiattach": True,
404             "supports_trusted_certs": True,
405             # Supported image types
406             "supports_image_type_aki": True,
407             "supports_image_type_ari": True,
408             "supports_image_type_ami": True,
409             "supports_image_type_raw": True,
410             "supports_image_type_iso": True,
411             # NOTE(danms): Certain backends do not work with complex image
412             # formats. If we are configured for those backends, then we
413             # should not expose the corresponding support traits.
414             "supports_image_type_qcow2": not requires_raw_image,
415             "supports_image_type_ploop": requires_ploop_image,
416             "supports_pcpus": True,
417             "supports_accelerators": True,
418             "supports_bfv_rescue": True,
419             "supports_vtpm": CONF.libvirt.swtpm_enabled,
420         }
421         super(LibvirtDriver, self).__init__(virtapi)
422 
423         if not sys.platform.startswith('linux'):
424             raise exception.InternalError(
425                 _('The libvirt driver only works on Linux'))
426 
427         global libvirt
428         if libvirt is None:
429             libvirt = importutils.import_module('libvirt')
430             libvirt_migrate.libvirt = libvirt
431 
432         self._host = host.Host(self._uri(), read_only,
433                                lifecycle_event_handler=self.emit_event,
434                                conn_event_handler=self._handle_conn_event)
435         self._supported_perf_events = []
436 
437         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver(self._host)
438 
439         # NOTE(lyarwood): Volume drivers are loaded on-demand
440         self.volume_drivers: ty.Dict[str, volume.LibvirtBaseVolumeDriver] = {}
441 
442         self._disk_cachemode = None
443         self.image_cache_manager = imagecache.ImageCacheManager()
444         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
445 
446         self.disk_cachemodes = {}
447 
448         for mode_str in CONF.libvirt.disk_cachemodes:
449             disk_type, sep, cache_mode = mode_str.partition('=')
450             if cache_mode not in VALID_DISK_CACHEMODES:
451                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
452                             'for disk type %(disk_type)s.',
453                             {'cache_mode': cache_mode, 'disk_type': disk_type})
454                 continue
455             self.disk_cachemodes[disk_type] = cache_mode
456 
457         self._volume_api = cinder.API()
458         self._image_api = glance.API()
459 
460         # The default choice for the sysinfo_serial config option is "unique"
461         # which does not have a special function since the value is just the
462         # instance.uuid.
463         sysinfo_serial_funcs = {
464             'none': lambda: None,
465             'hardware': self._get_host_sysinfo_serial_hardware,
466             'os': self._get_host_sysinfo_serial_os,
467             'auto': self._get_host_sysinfo_serial_auto,
468         }
469 
470         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
471             CONF.libvirt.sysinfo_serial, lambda: None)
472 
473         self.job_tracker = instancejobtracker.InstanceJobTracker()
474         self._remotefs = remotefs.RemoteFilesystem()
475 
476         self._live_migration_flags = self._block_migration_flags = 0
477         self.active_migrations = {}
478 
479         # Compute reserved hugepages from conf file at the very
480         # beginning to ensure any syntax error will be reported and
481         # avoid any re-calculation when computing resources.
482         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
483 
484         # Copy of the compute service ProviderTree object that is updated
485         # every time update_provider_tree() is called.
486         # NOTE(sbauza): We only want a read-only cache, this attribute is not
487         # intended to be updatable directly
488         self.provider_tree: provider_tree.ProviderTree = None
489 
490         # driver traits will not change during the runtime of the agent
491         # so calcuate them once and save them
492         self._static_traits = None
493 
494         # The CPU models in the configuration are case-insensitive, but the CPU
495         # model in the libvirt is case-sensitive, therefore create a mapping to
496         # map the lower case CPU model name to normal CPU model name.
497         self.cpu_models_mapping = {}
498         self.cpu_model_flag_mapping = {}
499 
500         self._vpmems_by_name, self._vpmems_by_rc = self._discover_vpmems(
501                 vpmem_conf=CONF.libvirt.pmem_namespaces)
502 
503         # We default to not support vGPUs unless the configuration is set.
504         self.pgpu_type_mapping = collections.defaultdict(str)
505         self.supported_vgpu_types = self._get_supported_vgpu_types()
506 
507         # Handles ongoing device manipultion in libvirt where we wait for the
508         # events about success or failure.
509         self._device_event_handler = AsyncDeviceEventsHandler()
510 
511     def _discover_vpmems(self, vpmem_conf=None):
512         """Discover vpmems on host and configuration.
513 
514         :param vpmem_conf: pmem namespaces configuration from CONF
515         :returns: a dict of vpmem keyed by name, and
516                   a dict of vpmem list keyed by resource class
517         :raises: exception.InvalidConfiguration if Libvirt or QEMU version
518                  does not meet requirement.
519         """
520         if not vpmem_conf:
521             return {}, {}
522 
523         # vpmem keyed by name {name: objects.LibvirtVPMEMDevice,...}
524         vpmems_by_name: ty.Dict[str, 'objects.LibvirtVPMEMDevice'] = {}
525         # vpmem list keyed by resource class
526         # {'RC_0': [objects.LibvirtVPMEMDevice, ...], 'RC_1': [...]}
527         vpmems_by_rc: ty.Dict[str, ty.List['objects.LibvirtVPMEMDevice']] = (
528             collections.defaultdict(list)
529         )
530 
531         vpmems_host = self._get_vpmems_on_host()
532         for ns_conf in vpmem_conf:
533             try:
534                 ns_label, ns_names = ns_conf.split(":", 1)
535             except ValueError:
536                 reason = _("The configuration doesn't follow the format")
537                 raise exception.PMEMNamespaceConfigInvalid(
538                         reason=reason)
539             ns_names = ns_names.split("|")
540             for ns_name in ns_names:
541                 if ns_name not in vpmems_host:
542                     reason = _("The PMEM namespace %s isn't on host") % ns_name
543                     raise exception.PMEMNamespaceConfigInvalid(
544                             reason=reason)
545                 if ns_name in vpmems_by_name:
546                     reason = (_("Duplicated PMEM namespace %s configured") %
547                                 ns_name)
548                     raise exception.PMEMNamespaceConfigInvalid(
549                             reason=reason)
550                 pmem_ns_updated = vpmems_host[ns_name]
551                 pmem_ns_updated.label = ns_label
552                 vpmems_by_name[ns_name] = pmem_ns_updated
553                 rc = orc.normalize_name(
554                         "PMEM_NAMESPACE_%s" % ns_label)
555                 vpmems_by_rc[rc].append(pmem_ns_updated)
556 
557         return vpmems_by_name, vpmems_by_rc
558 
559     def _get_vpmems_on_host(self):
560         """Get PMEM namespaces on host using ndctl utility."""
561         try:
562             output = nova.privsep.libvirt.get_pmem_namespaces()
563         except Exception as e:
564             reason = _("Get PMEM namespaces by ndctl utility, "
565                     "please ensure ndctl is installed: %s") % e
566             raise exception.GetPMEMNamespacesFailed(reason=reason)
567 
568         if not output:
569             return {}
570         namespaces = jsonutils.loads(output)
571         vpmems_host = {}  # keyed by namespace name
572         for ns in namespaces:
573             # store namespace info parsed from ndctl utility return
574             if not ns.get('name'):
575                 # The name is used to identify namespaces, it's optional
576                 # config when creating namespace. If an namespace don't have
577                 # name, it can not be used by Nova, we will skip it.
578                 continue
579             vpmems_host[ns['name']] = objects.LibvirtVPMEMDevice(
580                 name=ns['name'],
581                 devpath= '/dev/' + ns['daxregion']['devices'][0]['chardev'],
582                 size=ns['size'],
583                 align=ns['daxregion']['align'])
584         return vpmems_host
585 
586     @property
587     def disk_cachemode(self):
588         # It can be confusing to understand the QEMU cache mode
589         # behaviour, because each cache=$MODE is a convenient shorthand
590         # to toggle _three_ cache.* booleans.  Consult the below table
591         # (quoting from the QEMU man page):
592         #
593         #              | cache.writeback | cache.direct | cache.no-flush
594         # --------------------------------------------------------------
595         # writeback    | on              | off          | off
596         # none         | on              | on           | off
597         # writethrough | off             | off          | off
598         # directsync   | off             | on           | off
599         # unsafe       | on              | off          | on
600         #
601         # Where:
602         #
603         #  - 'cache.writeback=off' means: QEMU adds an automatic fsync()
604         #    after each write request.
605         #
606         #  - 'cache.direct=on' means: Use Linux's O_DIRECT, i.e. bypass
607         #    the kernel page cache.  Caches in any other layer (disk
608         #    cache, QEMU metadata caches, etc.) can still be present.
609         #
610         #  - 'cache.no-flush=on' means: Ignore flush requests, i.e.
611         #    never call fsync(), even if the guest explicitly requested
612         #    it.
613         #
614         # Use cache mode "none" (cache.writeback=on, cache.direct=on,
615         # cache.no-flush=off) for consistent performance and
616         # migration correctness.  Some filesystems don't support
617         # O_DIRECT, though.  For those we fallback to the next
618         # reasonable option that is "writeback" (cache.writeback=on,
619         # cache.direct=off, cache.no-flush=off).
620 
621         if self._disk_cachemode is None:
622             self._disk_cachemode = "none"
623             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
624                 self._disk_cachemode = "writeback"
625         return self._disk_cachemode
626 
627     def _set_cache_mode(self, conf):
628         """Set cache mode on LibvirtConfigGuestDisk object."""
629         try:
630             source_type = conf.source_type
631             driver_cache = conf.driver_cache
632         except AttributeError:
633             return
634 
635         # Shareable disks like for a multi-attach volume need to have the
636         # driver cache disabled.
637         if getattr(conf, 'shareable', False):
638             conf.driver_cache = 'none'
639         else:
640             cache_mode = self.disk_cachemodes.get(source_type,
641                                                   driver_cache)
642             conf.driver_cache = cache_mode
643 
644         # NOTE(acewit): If the [libvirt]disk_cachemodes is set as
645         # `block=writeback` or `block=writethrough` or `block=unsafe`,
646         # whose correponding Linux's IO semantic is not O_DIRECT in
647         # file nova.conf, then it will result in an attachment failure
648         # because of the libvirt bug
649         # (https://bugzilla.redhat.com/show_bug.cgi?id=1086704)
650         if ((getattr(conf, 'driver_io', None) == "native") and
651                 conf.driver_cache not in [None, 'none', 'directsync']):
652             conf.driver_io = "threads"
653             LOG.warning("The guest disk driver io mode has fallen back "
654                         "from 'native' to 'threads' because the "
655                         "disk cache mode is set as %(cachemode)s, which does "
656                         "not use O_DIRECT. See the following bug report "
657                         "for more details: https://launchpad.net/bugs/1841363",
658                         {'cachemode': conf.driver_cache})
659 
660     def _do_quality_warnings(self):
661         """Warn about potential configuration issues.
662 
663         This will log a warning message for things such as untested driver or
664         host arch configurations in order to indicate potential issues to
665         administrators.
666         """
667         if CONF.libvirt.virt_type not in ('qemu', 'kvm'):
668             LOG.warning(
669                 "Support for the '%(type)s' libvirt backend has been "
670                 "deprecated and will be removed in a future release.",
671                 {'type': CONF.libvirt.virt_type},
672             )
673 
674         caps = self._host.get_capabilities()
675         hostarch = caps.host.cpu.arch
676         if hostarch not in (
677             fields.Architecture.I686, fields.Architecture.X86_64,
678         ):
679             LOG.warning(
680                 'The libvirt driver is not tested on %(arch)s by the '
681                 'OpenStack project and thus its quality can not be ensured. '
682                 'For more information, see: https://docs.openstack.org/'
683                 'nova/latest/user/support-matrix.html',
684                 {'arch': hostarch},
685             )
686 
687     def _handle_conn_event(self, enabled, reason):
688         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
689                  {'enabled': enabled, 'reason': reason})
690         self._set_host_enabled(enabled, reason)
691 
692     def init_host(self, host):
693         self._host.initialize()
694 
695         self._update_host_specific_capabilities()
696 
697         self._check_cpu_set_configuration()
698 
699         self._do_quality_warnings()
700 
701         self._parse_migration_flags()
702 
703         self._supported_perf_events = self._get_supported_perf_events()
704 
705         self._check_file_backed_memory_support()
706 
707         self._check_my_ip()
708 
709         if (CONF.libvirt.virt_type == 'lxc' and
710                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
711             LOG.warning("Running libvirt-lxc without user namespaces is "
712                         "dangerous. Containers spawned by Nova will be run "
713                         "as the host's root user. It is highly suggested "
714                         "that user namespaces be used in a public or "
715                         "multi-tenant environment.")
716 
717         # Stop libguestfs using KVM unless we're also configured
718         # to use this. This solves problem where people need to
719         # stop Nova use of KVM because nested-virt is broken
720         if CONF.libvirt.virt_type != "kvm":
721             guestfs.force_tcg()
722 
723         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
724             raise exception.InternalError(
725                 _('Nova requires libvirt version %s or greater.') %
726                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
727 
728         if CONF.libvirt.virt_type in ("qemu", "kvm"):
729             if not self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
730                 raise exception.InternalError(
731                     _('Nova requires QEMU version %s or greater.') %
732                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
733 
734         if CONF.libvirt.virt_type == 'parallels':
735             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
736                 raise exception.InternalError(
737                     _('Nova requires Virtuozzo version %s or greater.') %
738                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
739 
740         # Give the cloud admin a heads up if we are intending to
741         # change the MIN_LIBVIRT_VERSION in the next release.
742         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
743             LOG.warning('Running Nova with a libvirt version less than '
744                         '%(version)s is deprecated. The required minimum '
745                         'version of libvirt will be raised to %(version)s '
746                         'in the next release.',
747                         {'version': libvirt_utils.version_to_string(
748                             NEXT_MIN_LIBVIRT_VERSION)})
749         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
750             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
751             LOG.warning('Running Nova with a QEMU version less than '
752                         '%(version)s is deprecated. The required minimum '
753                         'version of QEMU will be raised to %(version)s '
754                         'in the next release.',
755                         {'version': libvirt_utils.version_to_string(
756                             NEXT_MIN_QEMU_VERSION)})
757 
758         # Allowing both "tunnelling via libvirtd" (which will be
759         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
760         # new enough) and "native TLS" options at the same time is
761         # nonsensical.
762         if (CONF.libvirt.live_migration_tunnelled and
763                 CONF.libvirt.live_migration_with_native_tls):
764             msg = _("Setting both 'live_migration_tunnelled' and "
765                     "'live_migration_with_native_tls' at the same "
766                     "time is invalid. If you have the relevant "
767                     "libvirt and QEMU versions, and TLS configured "
768                     "in your environment, pick "
769                     "'live_migration_with_native_tls'.")
770             raise exception.Invalid(msg)
771 
772         # Some imagebackends are only able to import raw disk images,
773         # and will fail if given any other format. See the bug
774         # https://bugs.launchpad.net/nova/+bug/1816686 for more details.
775         if CONF.libvirt.images_type in ('rbd',):
776             if not CONF.force_raw_images:
777                 msg = _("'[DEFAULT]/force_raw_images = False' is not "
778                         "allowed with '[libvirt]/images_type = rbd'. "
779                         "Please check the two configs and if you really "
780                         "do want to use rbd as images_type, set "
781                         "force_raw_images to True.")
782                 raise exception.InvalidConfiguration(msg)
783 
784         # TODO(sbauza): Remove this code once mediated devices are persisted
785         # across reboots.
786         self._recreate_assigned_mediated_devices()
787 
788         self._check_cpu_compatibility()
789 
790         self._check_vtpm_support()
791 
792         self._register_instance_machine_type()
793 
794     def _update_host_specific_capabilities(self) -> None:
795         """Update driver capabilities based on capabilities of the host."""
796         # TODO(stephenfin): We should also be reporting e.g. SEV functionality
797         # or UEFI bootloader support in this manner
798         self.capabilities.update({
799             'supports_secure_boot': self._host.supports_secure_boot,
800         })
801 
802     def _register_instance_machine_type(self):
803         """Register the machine type of instances on this host
804 
805         For each instance found on this host by InstanceList.get_by_host ensure
806         a machine type is registered within the system metadata of the instance
807         """
808         context = nova_context.get_admin_context()
809         hostname = self._host.get_hostname()
810 
811         for instance in objects.InstanceList.get_by_host(context, hostname):
812             # NOTE(lyarwood): Skip if hw_machine_type is set already in the
813             # image_meta of the instance. Note that this value comes from the
814             # system metadata of the instance where it is stored under the
815             # image_hw_machine_type key.
816             if instance.image_meta.properties.get('hw_machine_type'):
817                 continue
818 
819             # Fetch and record the machine type from the config
820             hw_machine_type = libvirt_utils.get_machine_type(
821                 instance.image_meta)
822             # NOTE(lyarwood): As above this updates
823             # image_meta.properties.hw_machine_type within the instance and
824             # will be returned the next time libvirt_utils.get_machine_type is
825             # called for the instance image meta.
826             instance.system_metadata['image_hw_machine_type'] = hw_machine_type
827             instance.save()
828             LOG.debug("Instance machine_type updated to %s", hw_machine_type,
829                       instance=instance)
830 
831     def _prepare_cpu_flag(self, flag):
832         # NOTE(kchamart) This helper method will be used while computing
833         # guest CPU compatibility.  It will take into account a
834         # comma-separated list of CPU flags from
835         # `[libvirt]cpu_model_extra_flags`.  If the CPU flag starts
836         # with '+', it is enabled for the guest; if it starts with '-',
837         # it is disabled.  If neither '+' nor '-' is specified, the CPU
838         # flag is enabled.
839         if flag.startswith('-'):
840             flag = flag.lstrip('-')
841             policy_value = 'disable'
842         else:
843             flag = flag.lstrip('+')
844             policy_value = 'require'
845 
846         cpu_feature = vconfig.LibvirtConfigGuestCPUFeature(
847                         flag, policy=policy_value)
848         return cpu_feature
849 
850     def _check_cpu_compatibility(self):
851         mode = CONF.libvirt.cpu_mode
852         models = CONF.libvirt.cpu_models
853 
854         if (CONF.libvirt.virt_type not in ("kvm", "qemu") and
855                 mode not in (None, 'none')):
856             msg = _("Config requested an explicit CPU model, but "
857                     "the current libvirt hypervisor '%s' does not "
858                     "support selecting CPU models") % CONF.libvirt.virt_type
859             raise exception.Invalid(msg)
860 
861         if mode != "custom":
862             if not models:
863                 return
864             msg = _("The cpu_models option is not required when "
865                     "cpu_mode!=custom")
866             raise exception.Invalid(msg)
867 
868         if not models:
869             msg = _("The cpu_models option is required when cpu_mode=custom")
870             raise exception.Invalid(msg)
871 
872         cpu = vconfig.LibvirtConfigGuestCPU()
873         for model in models:
874             cpu.model = self._get_cpu_model_mapping(model)
875             try:
876                 self._compare_cpu(cpu, self._get_cpu_info(), None)
877             except exception.InvalidCPUInfo as e:
878                 msg = (_("Configured CPU model: %(model)s is not "
879                          "compatible with host CPU. Please correct your "
880                          "config and try again. %(e)s") % {
881                             'model': model, 'e': e})
882                 raise exception.InvalidCPUInfo(msg)
883 
884         # Use guest CPU model to check the compatibility between guest CPU and
885         # configured extra_flags
886         cpu = vconfig.LibvirtConfigGuestCPU()
887         cpu.model = self._host.get_capabilities().host.cpu.model
888         for flag in set(x.lower() for x in CONF.libvirt.cpu_model_extra_flags):
889             cpu_feature = self._prepare_cpu_flag(flag)
890             cpu.add_feature(cpu_feature)
891             try:
892                 self._compare_cpu(cpu, self._get_cpu_info(), None)
893             except exception.InvalidCPUInfo as e:
894                 msg = (_("Configured extra flag: %(flag)s it not correct, or "
895                          "the host CPU does not support this flag. Please "
896                          "correct the config and try again. %(e)s") % {
897                             'flag': flag, 'e': e})
898                 raise exception.InvalidCPUInfo(msg)
899 
900     def _check_vtpm_support(self) -> None:
901         # TODO(efried): A key manager must be configured to create/retrieve
902         # secrets. Is there a way to check that one is set up correctly?
903         # CONF.key_manager.backend is optional :(
904         if not CONF.libvirt.swtpm_enabled:
905             return
906 
907         if CONF.libvirt.virt_type not in ('qemu', 'kvm'):
908             msg = _(
909                 "vTPM support requires '[libvirt] virt_type' of 'qemu' or "
910                 "'kvm'; found '%s'.")
911             raise exception.InvalidConfiguration(msg % CONF.libvirt.virt_type)
912 
913         # These executables need to be installed for libvirt to make use of
914         # emulated TPM.
915         # NOTE(stephenfin): This checks using the PATH of the user running
916         # nova-compute rather than the libvirtd service, meaning it's an
917         # imperfect check but the best we can do
918         if not any(shutil.which(cmd) for cmd in ('swtpm_setup', 'swtpm')):
919             msg = _(
920                 "vTPM support is configured but the 'swtpm' and "
921                 "'swtpm_setup' binaries could not be found on PATH.")
922             raise exception.InvalidConfiguration(msg)
923 
924         # The user and group must be valid on this host for cold migration and
925         # resize to function.
926         try:
927             pwd.getpwnam(CONF.libvirt.swtpm_user)
928         except KeyError:
929             msg = _(
930                 "The user configured in '[libvirt] swtpm_user' does not exist "
931                 "on this host; expected '%s'.")
932             raise exception.InvalidConfiguration(msg % CONF.libvirt.swtpm_user)
933 
934         try:
935             grp.getgrnam(CONF.libvirt.swtpm_group)
936         except KeyError:
937             msg = _(
938                 "The group configured in '[libvirt] swtpm_group' does not "
939                 "exist on this host; expected '%s'.")
940             raise exception.InvalidConfiguration(
941                 msg % CONF.libvirt.swtpm_group)
942 
943         LOG.debug('Enabling emulated TPM support')
944 
945     @staticmethod
946     def _is_existing_mdev(uuid):
947         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
948         # libvirt daemon won't know when a mediated device is created unless
949         # you restart that daemon. Until all kernels we support are not having
950         # that possible race, check the sysfs directly instead of asking the
951         # libvirt API.
952         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
953         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
954 
955     def _recreate_assigned_mediated_devices(self):
956         """Recreate assigned mdevs that could have disappeared if we reboot
957         the host.
958         """
959         # NOTE(sbauza): This method just calls sysfs to recreate mediated
960         # devices by looking up existing guest XMLs and doesn't use
961         # the Placement API so it works with or without a vGPU reshape.
962         mdevs = self._get_all_assigned_mediated_devices()
963         for (mdev_uuid, instance_uuid) in mdevs.items():
964             if not self._is_existing_mdev(mdev_uuid):
965                 dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
966                 dev_info = self._get_mediated_device_information(dev_name)
967                 parent = dev_info['parent']
968                 parent_type = self._get_vgpu_type_per_pgpu(parent)
969                 if dev_info['type'] != parent_type:
970                     # NOTE(sbauza): The mdev was created by using a different
971                     # vGPU type. We can't recreate the mdev until the operator
972                     # modifies the configuration.
973                     parent = "{}:{}:{}.{}".format(*parent[4:].split('_'))
974                     msg = ("The instance UUID %(inst)s uses a VGPU that "
975                            "its parent pGPU %(parent)s no longer "
976                            "supports as the instance vGPU type %(type)s "
977                            "is not accepted for the pGPU. Please correct "
978                            "the configuration accordingly." %
979                            {'inst': instance_uuid,
980                             'parent': parent,
981                             'type': dev_info['type']})
982                     raise exception.InvalidLibvirtGPUConfig(reason=msg)
983                 self._create_new_mediated_device(parent, uuid=mdev_uuid)
984 
985     def _check_file_backed_memory_support(self):
986         if not CONF.libvirt.file_backed_memory:
987             return
988 
989         # file_backed_memory is only compatible with qemu/kvm virts
990         if CONF.libvirt.virt_type not in ("qemu", "kvm"):
991             raise exception.InternalError(
992                 _('Running Nova with file_backed_memory and virt_type '
993                   '%(type)s is not supported. file_backed_memory is only '
994                   'supported with qemu and kvm types.') %
995                 {'type': CONF.libvirt.virt_type})
996 
997         # file-backed memory doesn't work with memory overcommit.
998         # Block service startup if file-backed memory is enabled and
999         # ram_allocation_ratio is not 1.0
1000         if CONF.ram_allocation_ratio != 1.0:
1001             raise exception.InternalError(
1002                 'Running Nova with file_backed_memory requires '
1003                 'ram_allocation_ratio configured to 1.0')
1004 
1005         if CONF.reserved_host_memory_mb:
1006             # this is a hard failure as placement won't allow total < reserved
1007             if CONF.reserved_host_memory_mb >= CONF.libvirt.file_backed_memory:
1008                 msg = _(
1009                     "'[libvirt] file_backed_memory', which represents total "
1010                     "memory reported to placement, must be greater than "
1011                     "reserved memory configured via '[DEFAULT] "
1012                     "reserved_host_memory_mb'"
1013                 )
1014                 raise exception.InternalError(msg)
1015 
1016             # TODO(stephenfin): Change this to an exception in W or later
1017             LOG.warning(
1018                 "Reserving memory via '[DEFAULT] reserved_host_memory_mb' "
1019                 "is not compatible with file-backed memory. Consider "
1020                 "setting '[DEFAULT] reserved_host_memory_mb' to 0. This will "
1021                 "be an error in a future release."
1022             )
1023 
1024     def _check_my_ip(self):
1025         ips = compute_utils.get_machine_ips()
1026         if CONF.my_ip not in ips:
1027             LOG.warning('my_ip address (%(my_ip)s) was not found on '
1028                         'any of the interfaces: %(ifaces)s',
1029                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
1030 
1031     def _check_cpu_set_configuration(self):
1032         # evaluate these now to force a quick fail if they're invalid
1033         vcpu_pin_set = hardware.get_vcpu_pin_set() or set()
1034         cpu_shared_set = hardware.get_cpu_shared_set() or set()
1035         cpu_dedicated_set = hardware.get_cpu_dedicated_set() or set()
1036 
1037         # TODO(stephenfin): Remove this in U once we remove the 'vcpu_pin_set'
1038         # option
1039         if not vcpu_pin_set:
1040             if not (cpu_shared_set or cpu_dedicated_set):
1041                 return
1042 
1043             if not cpu_dedicated_set.isdisjoint(cpu_shared_set):
1044                 msg = _(
1045                     "The '[compute] cpu_dedicated_set' and '[compute] "
1046                     "cpu_shared_set' configuration options must be "
1047                     "disjoint.")
1048                 raise exception.InvalidConfiguration(msg)
1049 
1050             if CONF.reserved_host_cpus:
1051                 msg = _(
1052                     "The 'reserved_host_cpus' config option cannot be defined "
1053                     "alongside the '[compute] cpu_shared_set' or '[compute] "
1054                     "cpu_dedicated_set' options. Unset 'reserved_host_cpus'.")
1055                 raise exception.InvalidConfiguration(msg)
1056 
1057             return
1058 
1059         if cpu_dedicated_set:
1060             # NOTE(stephenfin): This is a new option in Train so it can be
1061             # an error
1062             msg = _(
1063                 "The 'vcpu_pin_set' config option has been deprecated and "
1064                 "cannot be defined alongside '[compute] cpu_dedicated_set'. "
1065                 "Unset 'vcpu_pin_set'.")
1066             raise exception.InvalidConfiguration(msg)
1067 
1068         if cpu_shared_set:
1069             LOG.warning(
1070                 "The '[compute] cpu_shared_set' and 'vcpu_pin_set' config "
1071                 "options have both been defined. While 'vcpu_pin_set' is "
1072                 "defined, it will continue to be used to configure the "
1073                 "specific host CPUs used for 'VCPU' inventory, while "
1074                 "'[compute] cpu_shared_set' will only be used for guest "
1075                 "emulator threads when 'hw:emulator_threads_policy=shared' "
1076                 "is defined in the flavor. This is legacy behavior and will "
1077                 "not be supported in a future release. "
1078                 "If you wish to define specific host CPUs to be used for "
1079                 "'VCPU' or 'PCPU' inventory, you must migrate the "
1080                 "'vcpu_pin_set' config option value to '[compute] "
1081                 "cpu_shared_set' and '[compute] cpu_dedicated_set', "
1082                 "respectively, and undefine 'vcpu_pin_set'.")
1083         else:
1084             LOG.warning(
1085                 "The 'vcpu_pin_set' config option has been deprecated and "
1086                 "will be removed in a future release. When defined, "
1087                 "'vcpu_pin_set' will be used to calculate 'VCPU' inventory "
1088                 "and schedule instances that have 'VCPU' allocations. "
1089                 "If you wish to define specific host CPUs to be used for "
1090                 "'VCPU' or 'PCPU' inventory, you must migrate the "
1091                 "'vcpu_pin_set' config option value to '[compute] "
1092                 "cpu_shared_set' and '[compute] cpu_dedicated_set', "
1093                 "respectively, and undefine 'vcpu_pin_set'.")
1094 
1095     def _prepare_migration_flags(self):
1096         migration_flags = 0
1097 
1098         migration_flags |= libvirt.VIR_MIGRATE_LIVE
1099 
1100         # Enable support for p2p migrations
1101         migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
1102 
1103         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
1104         # instance will remain defined on the source host
1105         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
1106 
1107         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
1108         # destination host
1109         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
1110 
1111         live_migration_flags = block_migration_flags = migration_flags
1112 
1113         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
1114         # will be live-migrations instead
1115         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
1116 
1117         return (live_migration_flags, block_migration_flags)
1118 
1119     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
1120     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
1121     # default, deprecate and remove the support for "tunnelled live
1122     # migration" (and related config attribute), because:
1123     #
1124     #  (a) it cannot handle live migration of disks in a non-shared
1125     #      storage setup (a.k.a. "block migration");
1126     #
1127     #  (b) has a huge performance overhead and latency, because it burns
1128     #      more CPU and memory bandwidth due to increased number of data
1129     #      copies on both source and destination hosts.
1130     #
1131     # Both the above limitations are addressed by the QEMU-native TLS
1132     # support (`live_migration_with_native_tls`).
1133     def _handle_live_migration_tunnelled(self, migration_flags):
1134         if CONF.libvirt.live_migration_tunnelled:
1135             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
1136         return migration_flags
1137 
1138     def _handle_native_tls(self, migration_flags):
1139         if (CONF.libvirt.live_migration_with_native_tls):
1140             migration_flags |= libvirt.VIR_MIGRATE_TLS
1141         return migration_flags
1142 
1143     def _handle_live_migration_post_copy(self, migration_flags):
1144         if CONF.libvirt.live_migration_permit_post_copy:
1145             migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
1146         return migration_flags
1147 
1148     def _handle_live_migration_auto_converge(self, migration_flags):
1149         if self._is_post_copy_enabled(migration_flags):
1150             LOG.info('The live_migration_permit_post_copy is set to '
1151                      'True and post copy live migration is available '
1152                      'so auto-converge will not be in use.')
1153         elif CONF.libvirt.live_migration_permit_auto_converge:
1154             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
1155         return migration_flags
1156 
1157     def _parse_migration_flags(self):
1158         (live_migration_flags,
1159             block_migration_flags) = self._prepare_migration_flags()
1160 
1161         live_migration_flags = self._handle_live_migration_tunnelled(
1162             live_migration_flags)
1163         block_migration_flags = self._handle_live_migration_tunnelled(
1164             block_migration_flags)
1165 
1166         live_migration_flags = self._handle_native_tls(
1167             live_migration_flags)
1168         block_migration_flags = self._handle_native_tls(
1169             block_migration_flags)
1170 
1171         live_migration_flags = self._handle_live_migration_post_copy(
1172             live_migration_flags)
1173         block_migration_flags = self._handle_live_migration_post_copy(
1174             block_migration_flags)
1175 
1176         live_migration_flags = self._handle_live_migration_auto_converge(
1177             live_migration_flags)
1178         block_migration_flags = self._handle_live_migration_auto_converge(
1179             block_migration_flags)
1180 
1181         self._live_migration_flags = live_migration_flags
1182         self._block_migration_flags = block_migration_flags
1183 
1184     # TODO(sahid): This method is targeted for removal when the tests
1185     # have been updated to avoid its use
1186     #
1187     # All libvirt API calls on the libvirt.Connect object should be
1188     # encapsulated by methods on the nova.virt.libvirt.host.Host
1189     # object, rather than directly invoking the libvirt APIs. The goal
1190     # is to avoid a direct dependency on the libvirt API from the
1191     # driver.py file.
1192     def _get_connection(self):
1193         return self._host.get_connection()
1194 
1195     _conn = property(_get_connection)
1196 
1197     @staticmethod
1198     def _uri():
1199         if CONF.libvirt.virt_type == 'lxc':
1200             uri = CONF.libvirt.connection_uri or 'lxc:///'
1201         elif CONF.libvirt.virt_type == 'parallels':
1202             uri = CONF.libvirt.connection_uri or 'parallels:///system'
1203         else:
1204             uri = CONF.libvirt.connection_uri or 'qemu:///system'
1205         return uri
1206 
1207     @staticmethod
1208     def _live_migration_uri(dest):
1209         uris = {
1210             'kvm': 'qemu+%(scheme)s://%(dest)s/system',
1211             'qemu': 'qemu+%(scheme)s://%(dest)s/system',
1212             'parallels': 'parallels+tcp://%(dest)s/system',
1213         }
1214         dest = oslo_netutils.escape_ipv6(dest)
1215 
1216         virt_type = CONF.libvirt.virt_type
1217         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
1218         uri = CONF.libvirt.live_migration_uri
1219         if uri:
1220             return uri % dest
1221 
1222         uri = uris.get(virt_type)
1223         if uri is None:
1224             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
1225 
1226         str_format = {
1227             'dest': dest,
1228             'scheme': CONF.libvirt.live_migration_scheme or 'tcp',
1229         }
1230         return uri % str_format
1231 
1232     @staticmethod
1233     def _migrate_uri(dest):
1234         uri = None
1235         dest = oslo_netutils.escape_ipv6(dest)
1236 
1237         # Only QEMU live migrations supports migrate-uri parameter
1238         virt_type = CONF.libvirt.virt_type
1239         if virt_type in ('qemu', 'kvm'):
1240             # QEMU accept two schemes: tcp and rdma.  By default
1241             # libvirt build the URI using the remote hostname and the
1242             # tcp schema.
1243             uri = 'tcp://%s' % dest
1244         # Because dest might be of type unicode, here we might return value of
1245         # type unicode as well which is not acceptable by libvirt python
1246         # binding when Python 2.7 is in use, so let's convert it explicitly
1247         # back to string. When Python 3.x is in use, libvirt python binding
1248         # accepts unicode type so it is completely fine to do a no-op str(uri)
1249         # conversion which will return value of type unicode.
1250         return uri and str(uri)
1251 
1252     def instance_exists(self, instance):
1253         """Efficient override of base instance_exists method."""
1254         try:
1255             self._host.get_guest(instance)
1256             return True
1257         except (exception.InternalError, exception.InstanceNotFound):
1258             return False
1259 
1260     def list_instances(self):
1261         names = []
1262         for guest in self._host.list_guests(only_running=False):
1263             names.append(guest.name)
1264 
1265         return names
1266 
1267     def list_instance_uuids(self):
1268         uuids = []
1269         for guest in self._host.list_guests(only_running=False):
1270             uuids.append(guest.uuid)
1271 
1272         return uuids
1273 
1274     def plug_vifs(self, instance, network_info):
1275         """Plug VIFs into networks."""
1276         for vif in network_info:
1277             self.vif_driver.plug(instance, vif)
1278 
1279     def _unplug_vifs(self, instance, network_info, ignore_errors):
1280         """Unplug VIFs from networks."""
1281         for vif in network_info:
1282             try:
1283                 self.vif_driver.unplug(instance, vif)
1284             except exception.NovaException:
1285                 if not ignore_errors:
1286                     raise
1287 
1288     def unplug_vifs(self, instance, network_info):
1289         self._unplug_vifs(instance, network_info, False)
1290 
1291     def _teardown_container(self, instance):
1292         inst_path = libvirt_utils.get_instance_path(instance)
1293         container_dir = os.path.join(inst_path, 'rootfs')
1294         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
1295         LOG.debug('Attempting to teardown container at path %(dir)s with '
1296                   'root device: %(rootfs_dev)s',
1297                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
1298                   instance=instance)
1299         disk_api.teardown_container(container_dir, rootfs_dev)
1300 
1301     def _destroy(self, instance):
1302         try:
1303             guest = self._host.get_guest(instance)
1304             if CONF.serial_console.enabled:
1305                 # This method is called for several events: destroy,
1306                 # rebuild, hard-reboot, power-off - For all of these
1307                 # events we want to release the serial ports acquired
1308                 # for the guest before destroying it.
1309                 serials = self._get_serial_ports_from_guest(guest)
1310                 for hostname, port in serials:
1311                     serial_console.release_port(host=hostname, port=port)
1312         except exception.InstanceNotFound:
1313             guest = None
1314 
1315         # If the instance is already terminated, we're still happy
1316         # Otherwise, destroy it
1317         old_domid = -1
1318         if guest is not None:
1319             try:
1320                 old_domid = guest.id
1321                 guest.poweroff()
1322 
1323             except libvirt.libvirtError as e:
1324                 is_okay = False
1325                 errcode = e.get_error_code()
1326                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1327                     # Domain already gone. This can safely be ignored.
1328                     is_okay = True
1329                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
1330                     # If the instance is already shut off, we get this:
1331                     # Code=55 Error=Requested operation is not valid:
1332                     # domain is not running
1333 
1334                     state = guest.get_power_state(self._host)
1335                     if state == power_state.SHUTDOWN:
1336                         is_okay = True
1337                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
1338                     errmsg = e.get_error_message()
1339                     if (CONF.libvirt.virt_type == 'lxc' and
1340                         errmsg == 'internal error: '
1341                                   'Some processes refused to die'):
1342                         # Some processes in the container didn't die
1343                         # fast enough for libvirt. The container will
1344                         # eventually die. For now, move on and let
1345                         # the wait_for_destroy logic take over.
1346                         is_okay = True
1347                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
1348                     LOG.warning("Cannot destroy instance, operation time out",
1349                                 instance=instance)
1350                     reason = _("operation time out")
1351                     raise exception.InstancePowerOffFailure(reason=reason)
1352                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
1353                     with excutils.save_and_reraise_exception():
1354                         LOG.warning("Cannot destroy instance, general system "
1355                                     "call failure", instance=instance)
1356                 if not is_okay:
1357                     with excutils.save_and_reraise_exception():
1358                         LOG.error('Error from libvirt during destroy. '
1359                                   'Code=%(errcode)s Error=%(e)s',
1360                                   {'errcode': errcode, 'e': e},
1361                                   instance=instance)
1362 
1363         def _wait_for_destroy(expected_domid):
1364             """Called at an interval until the VM is gone."""
1365             # NOTE(vish): If the instance disappears during the destroy
1366             #             we ignore it so the cleanup can still be
1367             #             attempted because we would prefer destroy to
1368             #             never fail.
1369             try:
1370                 dom_info = self.get_info(instance)
1371                 state = dom_info.state
1372                 new_domid = dom_info.internal_id
1373             except exception.InstanceNotFound:
1374                 LOG.debug("During wait destroy, instance disappeared.",
1375                           instance=instance)
1376                 state = power_state.SHUTDOWN
1377 
1378             if state == power_state.SHUTDOWN:
1379                 LOG.info("Instance destroyed successfully.", instance=instance)
1380                 raise loopingcall.LoopingCallDone()
1381 
1382             # NOTE(wangpan): If the instance was booted again after destroy,
1383             #                this may be an endless loop, so check the id of
1384             #                domain here, if it changed and the instance is
1385             #                still running, we should destroy it again.
1386             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
1387             if new_domid != expected_domid:
1388                 LOG.info("Instance may be started again.", instance=instance)
1389                 kwargs['is_running'] = True
1390                 raise loopingcall.LoopingCallDone()
1391 
1392         kwargs = {'is_running': False}
1393         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1394                                                      old_domid)
1395         timer.start(interval=0.5).wait()
1396         if kwargs['is_running']:
1397             LOG.info("Going to destroy instance again.", instance=instance)
1398             self._destroy(instance)
1399         else:
1400             # NOTE(GuanQiang): teardown container to avoid resource leak
1401             if CONF.libvirt.virt_type == 'lxc':
1402                 self._teardown_container(instance)
1403 
1404     def destroy(self, context, instance, network_info, block_device_info=None,
1405                 destroy_disks=True):
1406         self._destroy(instance)
1407         # NOTE(gibi): if there was device detach in progress then we need to
1408         # unblock the waiting threads and clean up.
1409         self._device_event_handler.cleanup_waiters(instance.uuid)
1410         self.cleanup(context, instance, network_info, block_device_info,
1411                      destroy_disks)
1412 
1413     def _undefine_domain(self, instance):
1414         try:
1415             guest = self._host.get_guest(instance)
1416             try:
1417                 hw_firmware_type = instance.image_meta.properties.get(
1418                     'hw_firmware_type')
1419                 support_uefi = self._check_uefi_support(hw_firmware_type)
1420                 guest.delete_configuration(support_uefi)
1421             except libvirt.libvirtError as e:
1422                 with excutils.save_and_reraise_exception() as ctxt:
1423                     errcode = e.get_error_code()
1424                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1425                         LOG.debug("Called undefine, but domain already gone.",
1426                                   instance=instance)
1427                         ctxt.reraise = False
1428                     else:
1429                         LOG.error('Error from libvirt during undefine. '
1430                                   'Code=%(errcode)s Error=%(e)s',
1431                                   {'errcode': errcode,
1432                                    'e': encodeutils.exception_to_unicode(e)},
1433                                   instance=instance)
1434         except exception.InstanceNotFound:
1435             pass
1436 
1437     def cleanup(self, context, instance, network_info, block_device_info=None,
1438                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1439         """Cleanup the instance from the host.
1440 
1441         Identify if the instance disks and instance path should be removed
1442         from the host before calling down into the _cleanup method for the
1443         actual removal of resources from the host.
1444 
1445         :param context: security context
1446         :param instance: instance object for the instance being cleaned up
1447         :param network_info: instance network information
1448         :param block_device_info: optional instance block device information
1449         :param destroy_disks: if local ephemeral disks should be destroyed
1450         :param migrate_data: optional migrate_data object
1451         :param destroy_vifs: if plugged vifs should be unplugged
1452         """
1453         cleanup_instance_dir = False
1454         cleanup_instance_disks = False
1455         # We assume destroy_disks means destroy instance directory and disks
1456         if destroy_disks:
1457             cleanup_instance_dir = True
1458             cleanup_instance_disks = True
1459         else:
1460             # NOTE(mdbooth): I think the theory here was that if this is a
1461             # migration with shared block storage then we need to delete the
1462             # instance directory because that's not shared. I'm pretty sure
1463             # this is wrong.
1464             if migrate_data and 'is_shared_block_storage' in migrate_data:
1465                 cleanup_instance_dir = migrate_data.is_shared_block_storage
1466 
1467             # NOTE(lyarwood): The following workaround allows operators to
1468             # ensure that non-shared instance directories are removed after an
1469             # evacuation or revert resize when using the shared RBD
1470             # imagebackend. This workaround is not required when cleaning up
1471             # migrations that provide migrate_data to this method as the
1472             # existing is_shared_block_storage conditional will cause the
1473             # instance directory to be removed.
1474             if not cleanup_instance_dir:
1475                 if CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup:
1476                     cleanup_instance_dir = CONF.libvirt.images_type == 'rbd'
1477 
1478         return self._cleanup(
1479                 context, instance, network_info,
1480                 block_device_info=block_device_info,
1481                 destroy_vifs=destroy_vifs,
1482                 cleanup_instance_dir=cleanup_instance_dir,
1483                 cleanup_instance_disks=cleanup_instance_disks)
1484 
1485     def _cleanup(self, context, instance, network_info, block_device_info=None,
1486                  destroy_vifs=True, cleanup_instance_dir=False,
1487                  cleanup_instance_disks=False):
1488         """Cleanup the domain and any attached resources from the host.
1489 
1490         This method cleans up any pmem devices, unplugs VIFs, disconnects
1491         attached volumes and undefines the instance domain within libvirt.
1492         It also optionally removes the ephemeral disks and the instance
1493         directory from the host depending on the cleanup_instance_dir|disks
1494         kwargs provided.
1495 
1496         :param context: security context
1497         :param instance: instance object for the instance being cleaned up
1498         :param network_info: instance network information
1499         :param block_device_info: optional instance block device information
1500         :param destroy_vifs: if plugged vifs should be unplugged
1501         :param cleanup_instance_dir: If the instance dir should be removed
1502         :param cleanup_instance_disks: If the instance disks should be removed
1503         """
1504         # zero the data on backend pmem device
1505         vpmems = self._get_vpmems(instance)
1506         if vpmems:
1507             self._cleanup_vpmems(vpmems)
1508 
1509         if destroy_vifs:
1510             self._unplug_vifs(instance, network_info, True)
1511 
1512         # FIXME(wangpan): if the instance is booted again here, such as the
1513         #                 soft reboot operation boot it here, it will become
1514         #                 "running deleted", should we check and destroy it
1515         #                 at the end of this method?
1516 
1517         # NOTE(vish): we disconnect from volumes regardless
1518         block_device_mapping = driver.block_device_info_get_mapping(
1519             block_device_info)
1520         for vol in block_device_mapping:
1521             connection_info = vol['connection_info']
1522             if not connection_info:
1523                 # if booting from a volume, creation could have failed meaning
1524                 # this would be unset
1525                 continue
1526 
1527             try:
1528                 self._disconnect_volume(context, connection_info, instance)
1529             except Exception as exc:
1530                 with excutils.save_and_reraise_exception() as ctxt:
1531                     if cleanup_instance_disks:
1532                         # Don't block on Volume errors if we're trying to
1533                         # delete the instance as we may be partially created
1534                         # or deleted
1535                         ctxt.reraise = False
1536                         LOG.warning(
1537                             "Ignoring Volume Error on vol %(vol_id)s "
1538                             "during delete %(exc)s",
1539                             {'vol_id': vol.get('volume_id'),
1540                              'exc': encodeutils.exception_to_unicode(exc)},
1541                             instance=instance)
1542 
1543         if cleanup_instance_disks:
1544             # NOTE(haomai): destroy volumes if needed
1545             if CONF.libvirt.images_type == 'lvm':
1546                 self._cleanup_lvm(instance, block_device_info)
1547             if CONF.libvirt.images_type == 'rbd':
1548                 self._cleanup_rbd(instance)
1549 
1550         if cleanup_instance_dir:
1551             attempts = int(instance.system_metadata.get('clean_attempts',
1552                                                         '0'))
1553             success = self.delete_instance_files(instance)
1554             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1555             # task in the compute manager. The tight coupling is not great...
1556             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1557             if success:
1558                 instance.cleaned = True
1559             try:
1560                 instance.save()
1561             except exception.InstanceNotFound:
1562                 pass
1563 
1564         if cleanup_instance_disks:
1565             crypto.delete_vtpm_secret(context, instance)
1566 
1567         self._undefine_domain(instance)
1568 
1569     def cleanup_lingering_instance_resources(self, instance):
1570         # zero the data on backend pmem device, if fails
1571         # it will raise an exception
1572         vpmems = self._get_vpmems(instance)
1573         if vpmems:
1574             self._cleanup_vpmems(vpmems)
1575 
1576     def _cleanup_vpmems(self, vpmems):
1577         for vpmem in vpmems:
1578             try:
1579                 nova.privsep.libvirt.cleanup_vpmem(vpmem.devpath)
1580             except Exception as e:
1581                 raise exception.VPMEMCleanupFailed(dev=vpmem.devpath,
1582                                                    error=e)
1583 
1584     def _get_serial_ports_from_guest(self, guest, mode=None):
1585         """Returns an iterator over serial port(s) configured on guest.
1586 
1587         :param mode: Should be a value in (None, bind, connect)
1588         """
1589         xml = guest.get_xml_desc()
1590         tree = etree.fromstring(xml)
1591 
1592         # The 'serial' device is the base for x86 platforms. Other platforms
1593         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1594         xpath_mode = "[@mode='%s']" % mode if mode else ""
1595         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1596         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1597 
1598         tcp_devices = tree.findall(serial_tcp)
1599         if len(tcp_devices) == 0:
1600             tcp_devices = tree.findall(console_tcp)
1601         for source in tcp_devices:
1602             yield (source.get("host"), int(source.get("service")))
1603 
1604     def _get_scsi_controller_next_unit(self, guest):
1605         """Returns the max disk unit used by scsi controller"""
1606         xml = guest.get_xml_desc()
1607         tree = etree.fromstring(xml)
1608         addrs = "./devices/disk[target/@bus='scsi']/address[@type='drive']"
1609 
1610         ret = []
1611         for obj in tree.xpath(addrs):
1612             ret.append(int(obj.get('unit', 0)))
1613         return max(ret) + 1 if ret else 0
1614 
1615     def _cleanup_rbd(self, instance):
1616         # NOTE(nic): On revert_resize, the cleanup steps for the root
1617         # volume are handled with an "rbd snap rollback" command,
1618         # and none of this is needed (and is, in fact, harmful) so
1619         # filter out non-ephemerals from the list
1620         if instance.task_state == task_states.RESIZE_REVERTING:
1621             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1622                                       disk.endswith('disk.local'))
1623         else:
1624             filter_fn = lambda disk: disk.startswith(instance.uuid)
1625         rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
1626 
1627     def _cleanup_lvm(self, instance, block_device_info):
1628         """Delete all LVM disks for given instance object."""
1629         if instance.get('ephemeral_key_uuid') is not None:
1630             # detach encrypted volumes
1631             disks = self._get_instance_disk_info(instance, block_device_info)
1632             for disk in disks:
1633                 if dmcrypt.is_encrypted(disk['path']):
1634                     dmcrypt.delete_volume(disk['path'])
1635 
1636         disks = self._lvm_disks(instance)
1637         if disks:
1638             lvm.remove_volumes(disks)
1639 
1640     def _lvm_disks(self, instance):
1641         """Returns all LVM disks for given instance object."""
1642         if CONF.libvirt.images_volume_group:
1643             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1644             if not os.path.exists(vg):
1645                 return []
1646             pattern = '%s_' % instance.uuid
1647 
1648             def belongs_to_instance(disk):
1649                 return disk.startswith(pattern)
1650 
1651             def fullpath(name):
1652                 return os.path.join(vg, name)
1653 
1654             logical_volumes = lvm.list_volumes(vg)
1655 
1656             disks = [fullpath(disk) for disk in logical_volumes
1657                      if belongs_to_instance(disk)]
1658             return disks
1659         return []
1660 
1661     def get_volume_connector(self, instance):
1662         root_helper = utils.get_root_helper()
1663         return connector.get_connector_properties(
1664             root_helper, CONF.my_block_storage_ip,
1665             CONF.libvirt.volume_use_multipath,
1666             enforce_multipath=True,
1667             host=CONF.host)
1668 
1669     def _cleanup_resize_vtpm(
1670         self,
1671         context: nova_context.RequestContext,
1672         instance: 'objects.Instance',
1673     ) -> None:
1674         """Handle vTPM when confirming a migration or resize.
1675 
1676         If the old flavor have vTPM and the new one doesn't, there are keys to
1677         be deleted.
1678         """
1679         old_vtpm_config = hardware.get_vtpm_constraint(
1680             instance.old_flavor, instance.image_meta)
1681         new_vtpm_config = hardware.get_vtpm_constraint(
1682             instance.new_flavor, instance.image_meta)
1683 
1684         if old_vtpm_config and not new_vtpm_config:
1685             # the instance no longer cares for its vTPM so delete the related
1686             # secret; the deletion of the instance directory and undefining of
1687             # the domain will take care of the TPM files themselves
1688             LOG.info('New flavor no longer requests vTPM; deleting secret.')
1689             crypto.delete_vtpm_secret(context, instance)
1690 
1691     # TODO(stephenfin): Fold this back into its only caller, cleanup_resize
1692     def _cleanup_resize(self, context, instance, network_info):
1693         inst_base = libvirt_utils.get_instance_path(instance)
1694         target = inst_base + '_resize'
1695 
1696         # zero the data on backend old pmem device
1697         vpmems = self._get_vpmems(instance, prefix='old')
1698         if vpmems:
1699             self._cleanup_vpmems(vpmems)
1700 
1701         # Remove any old vTPM data, if necessary
1702         self._cleanup_resize_vtpm(context, instance)
1703 
1704         # Deletion can fail over NFS, so retry the deletion as required.
1705         # Set maximum attempt as 5, most test can remove the directory
1706         # for the second time.
1707         attempts = 0
1708         while(os.path.exists(target) and attempts < 5):
1709             shutil.rmtree(target, ignore_errors=True)
1710             if os.path.exists(target):
1711                 time.sleep(random.randint(20, 200) / 100.0)
1712             attempts += 1
1713 
1714         # NOTE(mriedem): Some image backends will recreate the instance path
1715         # and disk.info during init, and all we need the root disk for
1716         # here is removing cloned snapshots which is backend-specific, so
1717         # check that first before initializing the image backend object. If
1718         # there is ever an image type that supports clone *and* re-creates
1719         # the instance directory and disk.info on init, this condition will
1720         # need to be re-visited to make sure that backend doesn't re-create
1721         # the disk. Refer to bugs: 1666831 1728603 1769131
1722         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1723             root_disk = self.image_backend.by_name(instance, 'disk')
1724             if root_disk.exists():
1725                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1726 
1727         if instance.host != CONF.host:
1728             self._undefine_domain(instance)
1729             # TODO(sean-k-mooney): remove this call to unplug_vifs after
1730             # Wallaby is released. VIFs are now unplugged in resize_instance.
1731             try:
1732                 self.unplug_vifs(instance, network_info)
1733             except exception.InternalError as e:
1734                 LOG.debug(e, instance=instance)
1735 
1736     def _get_volume_driver(
1737         self, connection_info: ty.Dict[str, ty.Any]
1738     ) -> 'volume.LibvirtBaseVolumeDriver':
1739         """Fetch the nova.virt.libvirt.volume driver
1740 
1741         Based on the provided connection_info return a nova.virt.libvirt.volume
1742         driver. This will call out to os-brick to construct an connector and
1743         check if the connector is valid on the underlying host.
1744 
1745         :param connection_info: The connection_info associated with the volume
1746         :raises: VolumeDriverNotFound if no driver is found or if the host
1747             doesn't support the requested driver. This retains legacy behaviour
1748             when only supported drivers were loaded on startup leading to a
1749             VolumeDriverNotFound being raised later if an invalid driver was
1750             requested.
1751         """
1752         driver_type = connection_info.get('driver_volume_type')
1753 
1754         # If the driver_type isn't listed in the supported type list fail
1755         if driver_type not in VOLUME_DRIVERS:
1756             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1757 
1758         # Return the cached driver
1759         if driver_type in self.volume_drivers:
1760             return self.volume_drivers.get(driver_type)
1761 
1762         @utils.synchronized('cache_volume_driver')
1763         def _cache_volume_driver(driver_type):
1764             # Check if another request cached the driver while we waited
1765             if driver_type in self.volume_drivers:
1766                 return self.volume_drivers.get(driver_type)
1767 
1768             try:
1769                 driver_class = importutils.import_class(
1770                     VOLUME_DRIVERS.get(driver_type))
1771                 self.volume_drivers[driver_type] = driver_class(self._host)
1772                 return self.volume_drivers.get(driver_type)
1773             except brick_exception.InvalidConnectorProtocol:
1774                 LOG.debug('Unable to load volume driver %s. It is not '
1775                           'supported on this host.', driver_type)
1776                 # NOTE(lyarwood): This exception is a subclass of
1777                 # VolumeDriverNotFound to ensure no callers have to change
1778                 # their error handling code after the move to on-demand loading
1779                 # of the volume drivers and associated os-brick connectors.
1780                 raise exception.VolumeDriverNotSupported(
1781                     volume_driver=VOLUME_DRIVERS.get(driver_type))
1782 
1783         # Cache the volume driver if it hasn't already been
1784         return _cache_volume_driver(driver_type)
1785 
1786     def _connect_volume(self, context, connection_info, instance,
1787                         encryption=None):
1788         vol_driver = self._get_volume_driver(connection_info)
1789         vol_driver.connect_volume(connection_info, instance)
1790         try:
1791             self._attach_encryptor(context, connection_info, encryption)
1792         except Exception:
1793             # Encryption failed so rollback the volume connection.
1794             with excutils.save_and_reraise_exception(logger=LOG):
1795                 LOG.exception("Failure attaching encryptor; rolling back "
1796                               "volume connection", instance=instance)
1797                 vol_driver.disconnect_volume(connection_info, instance)
1798 
1799     def _should_disconnect_target(self, context, instance, multiattach,
1800                                   vol_driver, volume_id):
1801         # NOTE(jdg): Multiattach is a special case (not to be confused
1802         # with shared_targets). With multiattach we may have a single volume
1803         # attached multiple times to *this* compute node (ie Server-1 and
1804         # Server-2).  So, if we receive a call to delete the attachment for
1805         # Server-1 we need to take special care to make sure that the Volume
1806         # isn't also attached to another Server on this Node.  Otherwise we
1807         # will indiscriminantly delete the connection for all Server and that's
1808         # no good.  So check if it's attached multiple times on this node
1809         # if it is we skip the call to brick to delete the connection.
1810         if not multiattach:
1811             return True
1812 
1813         # NOTE(deiter): Volume drivers using _HostMountStateManager are another
1814         # special case. _HostMountStateManager ensures that the compute node
1815         # only attempts to mount a single mountpoint in use by multiple
1816         # attachments once, and that it is not unmounted until it is no longer
1817         # in use by any attachments. So we can skip the multiattach check for
1818         # volume drivers that based on LibvirtMountedFileSystemVolumeDriver.
1819         if isinstance(vol_driver, fs.LibvirtMountedFileSystemVolumeDriver):
1820             return True
1821 
1822         connection_count = 0
1823         volume = self._volume_api.get(context, volume_id)
1824         attachments = volume.get('attachments', {})
1825         if len(attachments) > 1:
1826             # First we get a list of all Server UUID's associated with
1827             # this Host (Compute Node).  We're going to use this to
1828             # determine if the Volume being detached is also in-use by
1829             # another Server on this Host, ie just check to see if more
1830             # than one attachment.server_id for this volume is in our
1831             # list of Server UUID's for this Host
1832             servers_this_host = objects.InstanceList.get_uuids_by_host(
1833                 context, instance.host)
1834 
1835             # NOTE(jdg): nova.volume.cinder translates the
1836             # volume['attachments'] response into a dict which includes
1837             # the Server UUID as the key, so we're using that
1838             # here to check against our server_this_host list
1839             for server_id, data in attachments.items():
1840                 if server_id in servers_this_host:
1841                     connection_count += 1
1842         return (False if connection_count > 1 else True)
1843 
1844     def _disconnect_volume(self, context, connection_info, instance,
1845                            encryption=None):
1846         self._detach_encryptor(context, connection_info, encryption=encryption)
1847         vol_driver = self._get_volume_driver(connection_info)
1848         volume_id = driver_block_device.get_volume_id(connection_info)
1849         multiattach = connection_info.get('multiattach', False)
1850         if self._should_disconnect_target(
1851                 context, instance, multiattach, vol_driver, volume_id):
1852             vol_driver.disconnect_volume(connection_info, instance)
1853         else:
1854             LOG.info('Detected multiple connections on this host for '
1855                      'volume: %(volume)s, skipping target disconnect.',
1856                      {'volume': volume_id})
1857 
1858     def _extend_volume(self, connection_info, instance, requested_size):
1859         vol_driver = self._get_volume_driver(connection_info)
1860         return vol_driver.extend_volume(connection_info, instance,
1861                                         requested_size)
1862 
1863     def _allow_native_luksv1(self, encryption=None):
1864         """Check if QEMU's native LUKSv1 decryption should be used.
1865         """
1866         # NOTE(lyarwood): Native LUKSv1 decryption can be disabled via a
1867         # workarounds configurable in order to aviod known performance issues
1868         # with the libgcrypt lib.
1869         if CONF.workarounds.disable_native_luksv1:
1870             return False
1871 
1872         # NOTE(lyarwood): Ensure the LUKSv1 provider is used.
1873         provider = None
1874         if encryption:
1875             provider = encryption.get('provider', None)
1876         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1877             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1878         return provider == encryptors.LUKS
1879 
1880     def _get_volume_config(self, connection_info, disk_info):
1881         vol_driver = self._get_volume_driver(connection_info)
1882         conf = vol_driver.get_config(connection_info, disk_info)
1883         self._set_cache_mode(conf)
1884         return conf
1885 
1886     def _get_volume_encryptor(self, connection_info, encryption):
1887         root_helper = utils.get_root_helper()
1888         return encryptors.get_volume_encryptor(root_helper=root_helper,
1889                                                keymgr=key_manager.API(CONF),
1890                                                connection_info=connection_info,
1891                                                **encryption)
1892 
1893     def _get_volume_encryption(self, context, connection_info):
1894         """Get the encryption metadata dict if it is not provided
1895         """
1896         encryption = {}
1897         volume_id = driver_block_device.get_volume_id(connection_info)
1898         if volume_id:
1899             encryption = encryptors.get_encryption_metadata(context,
1900                             self._volume_api, volume_id, connection_info)
1901         return encryption
1902 
1903     def _attach_encryptor(self, context, connection_info, encryption):
1904         """Attach the frontend encryptor if one is required by the volume.
1905 
1906         The request context is only used when an encryption metadata dict is
1907         not provided. The encryption metadata dict being populated is then used
1908         to determine if an attempt to attach the encryptor should be made.
1909 
1910         """
1911         # NOTE(lyarwood): Skip any attempt to fetch encryption metadata or the
1912         # actual passphrase from the key manager if a libvirt secert already
1913         # exists locally for the volume. This suggests that the instance was
1914         # only powered off or the underlying host rebooted.
1915         volume_id = driver_block_device.get_volume_id(connection_info)
1916         if self._host.find_secret('volume', volume_id):
1917             LOG.debug("A libvirt secret for volume %s has been found on the "
1918                       "host, skipping any attempt to create another or attach "
1919                       "an os-brick encryptor.", volume_id)
1920             return
1921 
1922         if encryption is None:
1923             encryption = self._get_volume_encryption(context, connection_info)
1924 
1925         if encryption and self._allow_native_luksv1(encryption=encryption):
1926             # NOTE(lyarwood): Fetch the associated key for the volume and
1927             # decode the passphrase from the key.
1928             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1929             # with volumes, leading to the binary to hex to string conversion
1930             # below.
1931             keymgr = key_manager.API(CONF)
1932             key = keymgr.get(context, encryption['encryption_key_id'])
1933             key_encoded = key.get_encoded()
1934             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1935 
1936             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1937             # encryptors and format any volume that does not identify as
1938             # encrypted with LUKS.
1939             # FIXME(lyarwood): Remove this once c-vol correctly formats
1940             # encrypted volumes during their initial creation:
1941             # https://bugs.launchpad.net/cinder/+bug/1739442
1942             device_path = connection_info.get('data').get('device_path')
1943             if device_path:
1944                 root_helper = utils.get_root_helper()
1945                 if not luks_encryptor.is_luks(root_helper, device_path):
1946                     encryptor = self._get_volume_encryptor(connection_info,
1947                                                            encryption)
1948                     encryptor._format_volume(passphrase, **encryption)
1949 
1950             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1951             # on the compute node. This secret is used later when generating
1952             # the volume config.
1953             self._host.create_secret('volume', volume_id, password=passphrase)
1954         elif encryption:
1955             encryptor = self._get_volume_encryptor(connection_info,
1956                                                    encryption)
1957             encryptor.attach_volume(context, **encryption)
1958 
1959     def _detach_encryptor(self, context, connection_info, encryption):
1960         """Detach the frontend encryptor if one is required by the volume.
1961 
1962         The request context is only used when an encryption metadata dict is
1963         not provided. The encryption metadata dict being populated is then used
1964         to determine if an attempt to detach the encryptor should be made.
1965 
1966         If native LUKS decryption is enabled then delete previously created
1967         Libvirt volume secret from the host.
1968         """
1969         volume_id = driver_block_device.get_volume_id(connection_info)
1970         if volume_id and self._host.find_secret('volume', volume_id):
1971             return self._host.delete_secret('volume', volume_id)
1972         if encryption is None:
1973             encryption = self._get_volume_encryption(context, connection_info)
1974 
1975         # NOTE(lyarwood): Handle bugs #1821696 and #1917619 by avoiding the use
1976         # of the os-brick encryptors if we don't have a device_path. The lack
1977         # of a device_path here suggests the volume was natively attached to
1978         # QEMU anyway as volumes without a device_path are not supported by
1979         # os-brick encryptors. For volumes with a device_path the calls to
1980         # the os-brick encryptors are safe as they are actually idempotent,
1981         # ignoring any failures caused by the volumes actually being natively
1982         # attached previously.
1983         if (encryption and connection_info['data'].get('device_path') is None):
1984             return
1985 
1986         if encryption:
1987             encryptor = self._get_volume_encryptor(connection_info,
1988                                                    encryption)
1989             encryptor.detach_volume(**encryption)
1990 
1991     def _check_discard_for_attach_volume(self, conf, instance):
1992         """Perform some checks for volumes configured for discard support.
1993 
1994         If discard is configured for the volume, and the guest is using a
1995         configuration known to not work, we will log a message explaining
1996         the reason why.
1997         """
1998         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1999             LOG.debug('Attempting to attach volume %(id)s with discard '
2000                       'support enabled to an instance using an '
2001                       'unsupported configuration. target_bus = '
2002                       '%(bus)s. Trim commands will not be issued to '
2003                       'the storage device.',
2004                       {'bus': conf.target_bus,
2005                        'id': conf.serial},
2006                       instance=instance)
2007 
2008     def attach_volume(self, context, connection_info, instance, mountpoint,
2009                       disk_bus=None, device_type=None, encryption=None):
2010         guest = self._host.get_guest(instance)
2011 
2012         disk_dev = mountpoint.rpartition("/")[2]
2013         bdm = {
2014             'device_name': disk_dev,
2015             'disk_bus': disk_bus,
2016             'device_type': device_type}
2017 
2018         # Note(cfb): If the volume has a custom block size, check that
2019         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
2020         #            presence of a block size is considered mandatory by
2021         #            cinder so we fail if we can't honor the request.
2022         data = {}
2023         if ('data' in connection_info):
2024             data = connection_info['data']
2025         if ('logical_block_size' in data or 'physical_block_size' in data):
2026             if ((CONF.libvirt.virt_type != "kvm" and
2027                  CONF.libvirt.virt_type != "qemu")):
2028                 msg = _("Volume sets block size, but the current "
2029                         "libvirt hypervisor '%s' does not support custom "
2030                         "block size") % CONF.libvirt.virt_type
2031                 raise exception.InvalidHypervisorType(msg)
2032 
2033         self._connect_volume(context, connection_info, instance,
2034                              encryption=encryption)
2035         disk_info = blockinfo.get_info_from_bdm(
2036             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
2037         if disk_info['bus'] == 'scsi':
2038             disk_info['unit'] = self._get_scsi_controller_next_unit(guest)
2039 
2040         conf = self._get_volume_config(connection_info, disk_info)
2041 
2042         self._check_discard_for_attach_volume(conf, instance)
2043 
2044         try:
2045             state = guest.get_power_state(self._host)
2046             live = state in (power_state.RUNNING, power_state.PAUSED)
2047 
2048             guest.attach_device(conf, persistent=True, live=live)
2049             # NOTE(artom) If we're attaching with a device role tag, we need to
2050             # rebuild device_metadata. If we're attaching without a role
2051             # tag, we're rebuilding it here needlessly anyways. This isn't a
2052             # massive deal, and it helps reduce code complexity by not having
2053             # to indicate to the virt driver that the attach is tagged. The
2054             # really important optimization of not calling the database unless
2055             # device_metadata has actually changed is done for us by
2056             # instance.save().
2057             instance.device_metadata = self._build_device_metadata(
2058                 context, instance)
2059             instance.save()
2060         except Exception:
2061             LOG.exception('Failed to attach volume at mountpoint: %s',
2062                           mountpoint, instance=instance)
2063             with excutils.save_and_reraise_exception():
2064                 self._disconnect_volume(context, connection_info, instance,
2065                                         encryption=encryption)
2066 
2067     def _swap_volume(self, guest, disk_dev, conf, resize_to, hw_firmware_type):
2068         """Swap existing disk with a new block device.
2069 
2070         Call virDomainBlockRebase or virDomainBlockCopy with Libvirt >= 6.0.0
2071         to copy and then pivot to a new volume.
2072 
2073         :param: guest: Guest object representing the guest domain
2074         :param: disk_dev: Device within the domain that is being swapped
2075         :param: conf: LibvirtConfigGuestDisk object representing the new volume
2076         :param: resize_to: Size of the dst volume, 0 if the same as the src
2077         :param: hw_firmware_type: fields.FirmwareType if set in the imagemeta
2078         """
2079         dev = guest.get_block_device(disk_dev)
2080 
2081         # Save a copy of the domain's persistent XML file. We'll use this
2082         # to redefine the domain if anything fails during the volume swap.
2083         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2084 
2085         # Abort is an idempotent operation, so make sure any block
2086         # jobs which may have failed are ended.
2087         try:
2088             dev.abort_job()
2089         except Exception:
2090             pass
2091 
2092         try:
2093             # NOTE (rmk): virDomainBlockRebase and virDomainBlockCopy cannot be
2094             # executed on persistent domains, so we need to temporarily
2095             # undefine it. If any part of this block fails, the domain is
2096             # re-defined regardless.
2097             if guest.has_persistent_configuration():
2098                 support_uefi = self._check_uefi_support(hw_firmware_type)
2099                 guest.delete_configuration(support_uefi)
2100 
2101             try:
2102                 dev.copy(conf.to_xml(), reuse_ext=True)
2103 
2104                 while not dev.is_job_complete():
2105                     time.sleep(0.5)
2106 
2107                 dev.abort_job(pivot=True)
2108 
2109             except Exception as exc:
2110                 # NOTE(lyarwood): conf.source_path is not set for RBD disks so
2111                 # fallback to conf.target_dev when None.
2112                 new_path = conf.source_path or conf.target_dev
2113                 old_path = disk_dev
2114                 LOG.exception("Failure rebasing volume %(new_path)s on "
2115                     "%(old_path)s.", {'new_path': new_path,
2116                                       'old_path': old_path})
2117                 raise exception.VolumeRebaseFailed(reason=str(exc))
2118 
2119             if resize_to:
2120                 dev.resize(resize_to * units.Gi)
2121 
2122             # Make sure we will redefine the domain using the updated
2123             # configuration after the volume was swapped. The dump_inactive
2124             # keyword arg controls whether we pull the inactive (persistent)
2125             # or active (live) config from the domain. We want to pull the
2126             # live config after the volume was updated to use when we redefine
2127             # the domain.
2128             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
2129         finally:
2130             self._host.write_instance_config(xml)
2131 
2132     def swap_volume(self, context, old_connection_info,
2133                     new_connection_info, instance, mountpoint, resize_to):
2134 
2135         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
2136         old_encrypt = self._get_volume_encryption(context, old_connection_info)
2137         new_encrypt = self._get_volume_encryption(context, new_connection_info)
2138         if ((old_encrypt and self._allow_native_luksv1(old_encrypt)) or
2139             (new_encrypt and self._allow_native_luksv1(new_encrypt))):
2140             raise NotImplementedError(_("Swap volume is not supported for "
2141                 "encrypted volumes when native LUKS decryption is enabled."))
2142 
2143         guest = self._host.get_guest(instance)
2144 
2145         disk_dev = mountpoint.rpartition("/")[2]
2146         if not guest.get_disk(disk_dev):
2147             raise exception.DiskNotFound(location=disk_dev)
2148         disk_info = {
2149             'dev': disk_dev,
2150             'bus': blockinfo.get_disk_bus_for_disk_dev(
2151                 CONF.libvirt.virt_type, disk_dev),
2152             'type': 'disk',
2153             }
2154         # NOTE (lyarwood): new_connection_info will be modified by the
2155         # following _connect_volume call down into the volume drivers. The
2156         # majority of the volume drivers will add a device_path that is in turn
2157         # used by _get_volume_config to set the source_path of the
2158         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
2159         # this to the BDM here as the upper compute swap_volume method will
2160         # eventually do this for us.
2161         self._connect_volume(context, new_connection_info, instance)
2162         conf = self._get_volume_config(new_connection_info, disk_info)
2163         hw_firmware_type = instance.image_meta.properties.get(
2164             'hw_firmware_type')
2165 
2166         try:
2167             self._swap_volume(guest, disk_dev, conf,
2168                               resize_to, hw_firmware_type)
2169         except exception.VolumeRebaseFailed:
2170             with excutils.save_and_reraise_exception():
2171                 self._disconnect_volume(context, new_connection_info, instance)
2172 
2173         self._disconnect_volume(context, old_connection_info, instance)
2174 
2175     def _get_existing_domain_xml(self, instance, network_info,
2176                                  block_device_info=None):
2177         try:
2178             guest = self._host.get_guest(instance)
2179             xml = guest.get_xml_desc()
2180         except exception.InstanceNotFound:
2181             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2182                                                 instance,
2183                                                 instance.image_meta,
2184                                                 block_device_info)
2185             xml = self._get_guest_xml(nova_context.get_admin_context(),
2186                                       instance, network_info, disk_info,
2187                                       instance.image_meta,
2188                                       block_device_info=block_device_info)
2189         return xml
2190 
2191     def emit_event(self, event: virtevent.InstanceEvent) -> None:
2192         """Handles libvirt specific events locally and dispatches the rest to
2193         the compute manager.
2194         """
2195         if isinstance(event, libvirtevent.LibvirtEvent):
2196             # These are libvirt specific events handled here on the driver
2197             # level instead of propagating them to the compute manager level
2198             if isinstance(event, libvirtevent.DeviceEvent):
2199                 had_clients = self._device_event_handler.notify_waiters(event)
2200 
2201                 if had_clients:
2202                     LOG.debug(
2203                         "Received event %s from libvirt while the driver is "
2204                         "waiting for it; dispatched.",
2205                         event,
2206                     )
2207                 else:
2208                     LOG.warning(
2209                         "Received event %s from libvirt but the driver is not "
2210                         "waiting for it; ignored.",
2211                         event,
2212                     )
2213             else:
2214                 LOG.debug(
2215                     "Received event %s from libvirt but no handler is "
2216                     "implemented for it in the libvirt driver so it is "
2217                     "ignored", event)
2218         else:
2219             # Let the generic driver code dispatch the event to the compute
2220             # manager
2221             super().emit_event(event)
2222 
2223     def _detach_with_retry(
2224         self,
2225         guest: libvirt_guest.Guest,
2226         instance_uuid: str,
2227         # to properly typehint this param we would need typing.Protocol but
2228         # that is only available since python 3.8
2229         get_device_conf_func: ty.Callable,
2230         device_name: str,
2231         live: bool,
2232     ) -> None:
2233         """Detaches a device from the guest
2234 
2235         If live detach is requested then this call will wait for the libvirt
2236         event signalling the end of the detach process.
2237 
2238         If the live detach times out then it will retry the detach. Detach from
2239         the persistent config is not retried as it is:
2240 
2241         * synchronous and no event is sent from libvirt
2242         * it is always expected to succeed if the device is in the domain
2243           config
2244 
2245         :param guest: the guest we are detach the device from
2246         :param instance_uuid: the UUID of the instance we are detaching the
2247             device from
2248         :param get_device_conf_func: function which returns the configuration
2249             for device from the domain, having one optional boolean parameter
2250             `from_persistent_config` to select which domain config to query
2251         :param device_name: This is the name of the device used solely for
2252             error messages. Note that it is not the same as the device alias
2253             used by libvirt to identify the device.
2254         :param live: bool to indicate whether it affects the guest in running
2255             state. If live is True then the device is detached from both the
2256             persistent and the live config. If live is False the device only
2257             detached from the persistent config.
2258         :raises exception.DeviceNotFound: if the device does not exist in the
2259             domain even before we try to detach or if libvirt reported that the
2260             device is missing from the domain synchronously.
2261         :raises exception.DeviceDetachFailed: if libvirt reported error during
2262             detaching from the live domain or we timed out waiting for libvirt
2263             events and run out of retries
2264         :raises libvirt.libvirtError: for any other errors reported by libvirt
2265             synchronously.
2266         """
2267         persistent = guest.has_persistent_configuration()
2268 
2269         if not persistent and not live:
2270             # nothing to do
2271             return
2272 
2273         persistent_dev = None
2274         if persistent:
2275             persistent_dev = get_device_conf_func(from_persistent_config=True)
2276 
2277         live_dev = None
2278         if live:
2279             live_dev = get_device_conf_func()
2280 
2281         if live and live_dev is None:
2282             # caller requested a live detach but device is not present
2283             raise exception.DeviceNotFound(device=device_name)
2284 
2285         if not live and persistent_dev is None:
2286             # caller requested detach from the persistent domain but device is
2287             # not present
2288             raise exception.DeviceNotFound(device=device_name)
2289 
2290         if persistent_dev:
2291             try:
2292                 self._detach_from_persistent(
2293                     guest, instance_uuid, persistent_dev, get_device_conf_func,
2294                     device_name)
2295             except exception.DeviceNotFound:
2296                 if live:
2297                     # ignore the error so that we can do the live detach
2298                     LOG.warning(
2299                         'Libvirt reported sync error while detaching '
2300                         'device %s from instance %s from the persistent '
2301                         'domain config. Ignoring the error to proceed with '
2302                         'live detach as that was also requested.',
2303                         device_name, instance_uuid)
2304                 else:
2305                     # if only persistent detach was requested then give up
2306                     raise
2307 
2308         if live and live_dev:
2309             self._detach_from_live_with_retry(
2310                 guest, instance_uuid, live_dev, get_device_conf_func,
2311                 device_name)
2312 
2313     def _detach_from_persistent(
2314         self,
2315         guest: libvirt_guest.Guest,
2316         instance_uuid: str,
2317         persistent_dev: ty.Union[
2318             vconfig.LibvirtConfigGuestDisk,
2319             vconfig.LibvirtConfigGuestInterface],
2320         get_device_conf_func,
2321         device_name: str,
2322     ):
2323         LOG.debug(
2324             'Attempting to detach device %s from instance %s from '
2325             'the persistent domain config.', device_name, instance_uuid)
2326 
2327         self._detach_sync(
2328             persistent_dev, guest, instance_uuid, device_name,
2329             persistent=True, live=False)
2330 
2331         # make sure the dev is really gone
2332         persistent_dev = get_device_conf_func(
2333             from_persistent_config=True)
2334         if not persistent_dev:
2335             LOG.info(
2336                 'Successfully detached device %s from instance %s '
2337                 'from the persistent domain config.',
2338                 device_name, instance_uuid)
2339         else:
2340             # Based on the libvirt devs this should never happen
2341             LOG.warning(
2342                 'Failed to detach device %s from instance %s '
2343                 'from the persistent domain config. Libvirt did not '
2344                 'report any error but the device is still in the '
2345                 'config.', device_name, instance_uuid)
2346 
2347     def _detach_from_live_with_retry(
2348         self,
2349         guest: libvirt_guest.Guest,
2350         instance_uuid: str,
2351         live_dev: ty.Union[
2352             vconfig.LibvirtConfigGuestDisk,
2353             vconfig.LibvirtConfigGuestInterface],
2354         get_device_conf_func,
2355         device_name: str,
2356     ):
2357         max_attempts = CONF.libvirt.device_detach_attempts
2358         for attempt in range(max_attempts):
2359             LOG.debug(
2360                 '(%s/%s): Attempting to detach device %s with device '
2361                 'alias %s from instance %s from the live domain config.',
2362                 attempt + 1, max_attempts, device_name, live_dev.alias,
2363                 instance_uuid)
2364 
2365             self._detach_from_live_and_wait_for_event(
2366                 live_dev, guest, instance_uuid, device_name)
2367 
2368             # make sure the dev is really gone
2369             live_dev = get_device_conf_func()
2370             if not live_dev:
2371                 LOG.info(
2372                     'Successfully detached device %s from instance %s '
2373                     'from the live domain config.', device_name, instance_uuid)
2374                 # we are done
2375                 return
2376 
2377             LOG.debug(
2378                 'Failed to detach device %s with device alias %s from '
2379                 'instance %s from the live domain config. Libvirt did not '
2380                 'report any error but the device is still in the config.',
2381                 device_name, live_dev.alias, instance_uuid)
2382 
2383         msg = (
2384             'Run out of retry while detaching device %s with device '
2385             'alias %s from instance %s from the live domain config. '
2386             'Device is still attached to the guest.')
2387         LOG.error(msg, device_name, live_dev.alias, instance_uuid)
2388         raise exception.DeviceDetachFailed(
2389             device=device_name,
2390             reason=msg % (device_name, live_dev.alias, instance_uuid))
2391 
2392     def _detach_from_live_and_wait_for_event(
2393         self,
2394         dev: ty.Union[
2395             vconfig.LibvirtConfigGuestDisk,
2396             vconfig.LibvirtConfigGuestInterface],
2397         guest: libvirt_guest.Guest,
2398         instance_uuid: str,
2399         device_name: str,
2400     ) -> None:
2401         """Detaches a device from the live config of the guest and waits for
2402         the libvirt event singling the finish of the detach.
2403 
2404         :param dev: the device configuration to be detached
2405         :param guest: the guest we are detach the device from
2406         :param instance_uuid: the UUID of the instance we are detaching the
2407             device from
2408         :param device_name: This is the name of the device used solely for
2409             error messages.
2410         :raises exception.DeviceNotFound: if libvirt reported that the device
2411             is missing from the domain synchronously.
2412         :raises libvirt.libvirtError: for any other errors reported by libvirt
2413             synchronously.
2414         :raises DeviceDetachFailed: if libvirt sent DeviceRemovalFailedEvent
2415         """
2416         # So we will issue an detach to libvirt and we will wait for an
2417         # event from libvirt about the result. We need to set up the event
2418         # handling before the detach to avoid missing the event if libvirt
2419         # is really fast
2420         # NOTE(gibi): we need to use the alias name of the device as that
2421         # is what libvirt will send back to us in the event
2422         waiter = self._device_event_handler.create_waiter(
2423             instance_uuid, dev.alias,
2424             {libvirtevent.DeviceRemovedEvent,
2425              libvirtevent.DeviceRemovalFailedEvent})
2426         try:
2427             self._detach_sync(
2428                 dev, guest, instance_uuid, device_name, persistent=False,
2429                 live=True)
2430         except Exception:
2431             # clean up the libvirt event handler as we failed synchronously
2432             self._device_event_handler.delete_waiter(waiter)
2433             raise
2434 
2435         LOG.debug(
2436             'Start waiting for the detach event from libvirt for '
2437             'device %s with device alias %s for instance %s',
2438             device_name, dev.alias, instance_uuid)
2439         # We issued the detach without any exception so we can wait for
2440         # a libvirt event to arrive to notify us about the result
2441         # NOTE(gibi): we expect that this call will be unblocked by an
2442         # incoming libvirt DeviceRemovedEvent or DeviceRemovalFailedEvent
2443         event = self._device_event_handler.wait(
2444             waiter, timeout=CONF.libvirt.device_detach_timeout)
2445 
2446         if not event:
2447             # This should not happen based on information from the libvirt
2448             # developers. But it does at least during the cleanup of the
2449             # tempest test case
2450             # ServerRescueNegativeTestJSON.test_rescued_vm_detach_volume
2451             # Log a warning and let the upper layer detect that the device is
2452             # still attached and retry
2453             LOG.error(
2454                 'Waiting for libvirt event about the detach of '
2455                 'device %s with device alias %s from instance %s is timed '
2456                 'out.', device_name, dev.alias, instance_uuid)
2457 
2458         if isinstance(event, libvirtevent.DeviceRemovalFailedEvent):
2459             # Based on the libvirt developers this signals a permanent failure
2460             LOG.error(
2461                 'Received DeviceRemovalFailedEvent from libvirt for the '
2462                 'detach of device %s with device alias %s from instance %s ',
2463                 device_name, dev.alias, instance_uuid)
2464             raise exception.DeviceDetachFailed(
2465                 device=device_name,
2466                 reason="DeviceRemovalFailedEvent received from libvirt")
2467 
2468     @staticmethod
2469     def _detach_sync(
2470         dev: ty.Union[
2471             vconfig.LibvirtConfigGuestDisk,
2472             vconfig.LibvirtConfigGuestInterface],
2473         guest: libvirt_guest.Guest,
2474         instance_uuid: str,
2475         device_name: str,
2476         persistent: bool,
2477         live: bool,
2478     ):
2479         """Detaches a device from the guest without waiting for libvirt events
2480 
2481         It only handles synchronous errors (i.e. exceptions) but do not wait
2482         for any event from libvirt.
2483 
2484         :param dev: the device configuration to be detached
2485         :param guest: the guest we are detach the device from
2486         :param instance_uuid: the UUID of the instance we are detaching the
2487             device from
2488         :param device_name: This is the name of the device used solely for
2489             error messages.
2490         :param live: detach the device from the live domain config only
2491         :param persistent: detach the device from the persistent domain config
2492             only
2493         :raises exception.DeviceNotFound: if libvirt reported that the device
2494             is missing from the domain synchronously.
2495         :raises libvirt.libvirtError: for any other errors reported by libvirt
2496             synchronously.
2497         """
2498         try:
2499             guest.detach_device(dev, persistent=persistent, live=live)
2500         except libvirt.libvirtError as ex:
2501             if ex.get_error_code() == libvirt.VIR_ERR_DEVICE_MISSING:
2502                 LOG.debug(
2503                     'Libvirt failed to detach device %s from instance %s '
2504                     'synchronously (persistent=%s, live=%s) with error: %s.',
2505                     device_name, instance_uuid, persistent, live, str(ex))
2506                 raise exception.DeviceNotFound(device=device_name) from ex
2507 
2508             LOG.warning(
2509                 'Unexpected libvirt error while detaching device %s from '
2510                 'instance %s: %s', device_name, instance_uuid, str(ex))
2511             raise
2512 
2513     def detach_volume(self, context, connection_info, instance, mountpoint,
2514                       encryption=None):
2515         disk_dev = mountpoint.rpartition("/")[2]
2516         try:
2517             guest = self._host.get_guest(instance)
2518 
2519             state = guest.get_power_state(self._host)
2520             live = state in (power_state.RUNNING, power_state.PAUSED)
2521             # NOTE(lyarwood): The volume must be detached from the VM before
2522             # detaching any attached encryptors or disconnecting the underlying
2523             # volume in _disconnect_volume. Otherwise, the encryptor or volume
2524             # driver may report that the volume is still in use.
2525             get_dev = functools.partial(guest.get_disk, disk_dev)
2526             self._detach_with_retry(
2527                 guest,
2528                 instance.uuid,
2529                 get_dev,
2530                 device_name=disk_dev,
2531                 live=live
2532             )
2533         except exception.InstanceNotFound:
2534             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
2535             #                will throw InstanceNotFound exception. Need to
2536             #                disconnect volume under this circumstance.
2537             LOG.warning("During detach_volume, instance disappeared.",
2538                         instance=instance)
2539         except exception.DeviceNotFound:
2540             # We should still try to disconnect logical device from
2541             # host, an error might have happened during a previous
2542             # call.
2543             LOG.info("Device %s not found in instance.",
2544                      disk_dev, instance=instance)
2545         except libvirt.libvirtError as ex:
2546             # NOTE(vish): This is called to cleanup volumes after live
2547             #             migration, so we should still disconnect even if
2548             #             the instance doesn't exist here anymore.
2549             error_code = ex.get_error_code()
2550             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2551                 # NOTE(vish):
2552                 LOG.warning("During detach_volume, instance disappeared.",
2553                             instance=instance)
2554             else:
2555                 raise
2556 
2557         self._disconnect_volume(context, connection_info, instance,
2558                                 encryption=encryption)
2559 
2560     def _resize_attached_volume(self, new_size, block_device, instance):
2561         LOG.debug('Resizing target device %(dev)s to %(size)u',
2562                   {'dev': block_device._disk, 'size': new_size},
2563                   instance=instance)
2564         block_device.resize(new_size)
2565 
2566     def _resize_attached_encrypted_volume(self, original_new_size,
2567                                           block_device, instance,
2568                                           connection_info, encryption):
2569         # TODO(lyarwood): Also handle the dm-crpyt encryption providers of
2570         # plain and LUKSv2, for now just use the original_new_size.
2571         decrypted_device_new_size = original_new_size
2572 
2573         # NOTE(lyarwood): original_new_size currently refers to the total size
2574         # of the extended volume in bytes. With natively decrypted LUKSv1
2575         # volumes we need to ensure this now takes the LUKSv1 header and key
2576         # material into account. Otherwise QEMU will attempt and fail to grow
2577         # host block devices and remote RBD volumes.
2578         if self._allow_native_luksv1(encryption):
2579             try:
2580                 # NOTE(lyarwood): Find the path to provide to qemu-img
2581                 if 'device_path' in connection_info['data']:
2582                     path = connection_info['data']['device_path']
2583                 elif connection_info['driver_volume_type'] == 'rbd':
2584                     volume_name = connection_info['data']['name']
2585                     path = f"rbd:{volume_name}"
2586                     if connection_info['data'].get('auth_enabled'):
2587                         username = connection_info['data']['auth_username']
2588                         path = f"rbd:{volume_name}:id={username}"
2589                 else:
2590                     path = 'unknown'
2591                     raise exception.DiskNotFound(location='unknown')
2592 
2593                 info = images.privileged_qemu_img_info(path)
2594                 format_specific_data = info.format_specific['data']
2595                 payload_offset = format_specific_data['payload-offset']
2596 
2597                 # NOTE(lyarwood): Ensure the underlying device is not resized
2598                 # by subtracting the LUKSv1 payload_offset (where the users
2599                 # encrypted data starts) from the original_new_size (the total
2600                 # size of the underlying volume). Both are reported in bytes.
2601                 decrypted_device_new_size = original_new_size - payload_offset
2602 
2603             except exception.DiskNotFound:
2604                 with excutils.save_and_reraise_exception():
2605                     LOG.exception('Unable to access the encrypted disk %s.',
2606                                   path, instance=instance)
2607             except Exception:
2608                 with excutils.save_and_reraise_exception():
2609                     LOG.exception('Unknown error when attempting to find the '
2610                                   'payload_offset for LUKSv1 encrypted disk '
2611                                   '%s.', path, instance=instance)
2612         # NOTE(lyarwood): Resize the decrypted device within the instance to
2613         # the calculated size as with normal volumes.
2614         self._resize_attached_volume(
2615             decrypted_device_new_size, block_device, instance)
2616 
2617     def extend_volume(self, context, connection_info, instance,
2618                       requested_size):
2619         try:
2620             new_size = self._extend_volume(connection_info, instance,
2621                                            requested_size)
2622         except NotImplementedError:
2623             raise exception.ExtendVolumeNotSupported()
2624 
2625         # Resize the device in QEMU so its size is updated and
2626         # detected by the instance without rebooting.
2627         try:
2628             guest = self._host.get_guest(instance)
2629             state = guest.get_power_state(self._host)
2630             volume_id = driver_block_device.get_volume_id(connection_info)
2631             active_state = state in (power_state.RUNNING, power_state.PAUSED)
2632             if active_state:
2633                 if 'device_path' in connection_info['data']:
2634                     disk_path = connection_info['data']['device_path']
2635                 else:
2636                     # Some drivers (eg. net) don't put the device_path
2637                     # into the connection_info. Match disks by their serial
2638                     # number instead
2639                     disk = next(iter([
2640                         d for d in guest.get_all_disks()
2641                         if d.serial == volume_id
2642                     ]), None)
2643                     if not disk:
2644                         raise exception.VolumeNotFound(volume_id=volume_id)
2645                     disk_path = disk.target_dev
2646                 dev = guest.get_block_device(disk_path)
2647                 encryption = encryptors.get_encryption_metadata(
2648                     context, self._volume_api, volume_id, connection_info)
2649                 if encryption:
2650                     self._resize_attached_encrypted_volume(
2651                         new_size, dev, instance,
2652                         connection_info, encryption)
2653                 else:
2654                     self._resize_attached_volume(
2655                         new_size, dev, instance)
2656             else:
2657                 LOG.debug('Skipping block device resize, guest is not running',
2658                           instance=instance)
2659         except exception.InstanceNotFound:
2660             with excutils.save_and_reraise_exception():
2661                 LOG.warning('During extend_volume, instance disappeared.',
2662                             instance=instance)
2663         except libvirt.libvirtError:
2664             with excutils.save_and_reraise_exception():
2665                 LOG.exception('resizing block device failed.',
2666                               instance=instance)
2667 
2668     def attach_interface(self, context, instance, image_meta, vif):
2669         guest = self._host.get_guest(instance)
2670 
2671         self.vif_driver.plug(instance, vif)
2672         cfg = self.vif_driver.get_config(instance, vif, image_meta,
2673                                          instance.flavor,
2674                                          CONF.libvirt.virt_type)
2675         try:
2676             state = guest.get_power_state(self._host)
2677             live = state in (power_state.RUNNING, power_state.PAUSED)
2678             guest.attach_device(cfg, persistent=True, live=live)
2679         except libvirt.libvirtError:
2680             LOG.error('attaching network adapter failed.',
2681                       instance=instance, exc_info=True)
2682             self.vif_driver.unplug(instance, vif)
2683             raise exception.InterfaceAttachFailed(
2684                     instance_uuid=instance.uuid)
2685         try:
2686             # NOTE(artom) If we're attaching with a device role tag, we need to
2687             # rebuild device_metadata. If we're attaching without a role
2688             # tag, we're rebuilding it here needlessly anyways. This isn't a
2689             # massive deal, and it helps reduce code complexity by not having
2690             # to indicate to the virt driver that the attach is tagged. The
2691             # really important optimization of not calling the database unless
2692             # device_metadata has actually changed is done for us by
2693             # instance.save().
2694             instance.device_metadata = self._build_device_metadata(
2695                 context, instance)
2696             instance.save()
2697         except Exception:
2698             # NOTE(artom) If we fail here it means the interface attached
2699             # successfully but building and/or saving the device metadata
2700             # failed. Just unplugging the vif is therefore not enough cleanup,
2701             # we need to detach the interface.
2702             with excutils.save_and_reraise_exception(reraise=False):
2703                 LOG.error('Interface attached successfully but building '
2704                           'and/or saving device metadata failed.',
2705                           instance=instance, exc_info=True)
2706                 self.detach_interface(context, instance, vif)
2707                 raise exception.InterfaceAttachFailed(
2708                     instance_uuid=instance.uuid)
2709         try:
2710             guest.set_metadata(
2711                 self._get_guest_config_meta(
2712                     instance, instance.get_network_info()))
2713         except libvirt.libvirtError:
2714             LOG.warning('updating libvirt metadata failed.', instance=instance)
2715 
2716     def detach_interface(self, context, instance, vif):
2717         guest = self._host.get_guest(instance)
2718         cfg = self.vif_driver.get_config(instance, vif,
2719                                          instance.image_meta,
2720                                          instance.flavor,
2721                                          CONF.libvirt.virt_type)
2722         interface = guest.get_interface_by_cfg(cfg)
2723         try:
2724             # NOTE(mriedem): When deleting an instance and using Neutron,
2725             # we can be racing against Neutron deleting the port and
2726             # sending the vif-deleted event which then triggers a call to
2727             # detach the interface, so if the interface is not found then
2728             # we can just log it as a warning.
2729             if not interface:
2730                 mac = vif.get('address')
2731                 # The interface is gone so just log it as a warning.
2732                 LOG.warning('Detaching interface %(mac)s failed because '
2733                             'the device is no longer found on the guest.',
2734                             {'mac': mac}, instance=instance)
2735                 return
2736 
2737             state = guest.get_power_state(self._host)
2738             live = state in (power_state.RUNNING, power_state.PAUSED)
2739             get_dev = functools.partial(guest.get_interface_by_cfg, cfg)
2740             self._detach_with_retry(
2741                 guest,
2742                 instance.uuid,
2743                 get_dev,
2744                 device_name=self.vif_driver.get_vif_devname(vif),
2745                 live=live,
2746             )
2747         except exception.DeviceDetachFailed:
2748             # We failed to detach the device even with the retry loop, so let's
2749             # dump some debug information to the logs before raising back up.
2750             with excutils.save_and_reraise_exception():
2751                 devname = self.vif_driver.get_vif_devname(vif)
2752                 interface = guest.get_interface_by_cfg(cfg)
2753                 if interface:
2754                     LOG.warning(
2755                         'Failed to detach interface %(devname)s after '
2756                         'repeated attempts. Final interface xml:\n'
2757                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
2758                         {'devname': devname,
2759                          'interface_xml': interface.to_xml(),
2760                          'guest_xml': guest.get_xml_desc()},
2761                         instance=instance)
2762         except exception.DeviceNotFound:
2763             # The interface is gone so just log it as a warning.
2764             LOG.warning('Detaching interface %(mac)s failed because '
2765                         'the device is no longer found on the guest.',
2766                         {'mac': vif.get('address')}, instance=instance)
2767         except libvirt.libvirtError as ex:
2768             error_code = ex.get_error_code()
2769             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2770                 LOG.warning("During detach_interface, instance disappeared.",
2771                             instance=instance)
2772             else:
2773                 # NOTE(mriedem): When deleting an instance and using Neutron,
2774                 # we can be racing against Neutron deleting the port and
2775                 # sending the vif-deleted event which then triggers a call to
2776                 # detach the interface, so we might have failed because the
2777                 # network device no longer exists. Libvirt will fail with
2778                 # "operation failed: no matching network device was found"
2779                 # which unfortunately does not have a unique error code so we
2780                 # need to look up the interface by config and if it's not found
2781                 # then we can just log it as a warning rather than tracing an
2782                 # error.
2783                 mac = vif.get('address')
2784                 # Get a fresh instance of the guest in case it is gone.
2785                 try:
2786                     guest = self._host.get_guest(instance)
2787                 except exception.InstanceNotFound:
2788                     LOG.info("Instance disappeared while detaching interface "
2789                              "%s", vif['id'], instance=instance)
2790                     return
2791                 interface = guest.get_interface_by_cfg(cfg)
2792                 if interface:
2793                     LOG.error('detaching network adapter failed.',
2794                               instance=instance, exc_info=True)
2795                     raise exception.InterfaceDetachFailed(
2796                             instance_uuid=instance.uuid)
2797 
2798                 # The interface is gone so just log it as a warning.
2799                 LOG.warning('Detaching interface %(mac)s failed because '
2800                             'the device is no longer found on the guest.',
2801                             {'mac': mac}, instance=instance)
2802         finally:
2803             # NOTE(gibi): we need to unplug the vif _after_ the detach is done
2804             # on the libvirt side as otherwise libvirt will still manage the
2805             # device that our unplug code trying to reset. This can cause a
2806             # race and leave the detached device configured. Also even if we
2807             # are failed to detach due to race conditions the unplug is
2808             # necessary for the same reason
2809             self.vif_driver.unplug(instance, vif)
2810         try:
2811             # NOTE(nmiki): In order for the interface to be removed from
2812             # network_info, the nova-compute process need to wait for
2813             # processing on the neutron side.
2814             # Here, I simply exclude the target VIF from metadata.
2815             network_info = list(filter(lambda info: info['id'] != vif['id'],
2816                                        instance.get_network_info()))
2817             guest.set_metadata(
2818                 self._get_guest_config_meta(instance, network_info))
2819         except libvirt.libvirtError:
2820             LOG.warning('updating libvirt metadata failed.', instance=instance)
2821 
2822     def _create_snapshot_metadata(self, image_meta, instance,
2823                                   img_fmt, snp_name):
2824         metadata = {'status': 'active',
2825                     'name': snp_name,
2826                     'properties': {
2827                                    'kernel_id': instance.kernel_id,
2828                                    'image_location': 'snapshot',
2829                                    'image_state': 'available',
2830                                    'owner_id': instance.project_id,
2831                                    'ramdisk_id': instance.ramdisk_id,
2832                                    }
2833                     }
2834         if instance.os_type:
2835             metadata['properties']['os_type'] = instance.os_type
2836 
2837         # NOTE(vish): glance forces ami disk format to be ami
2838         if image_meta.disk_format == 'ami':
2839             metadata['disk_format'] = 'ami'
2840         else:
2841             metadata['disk_format'] = img_fmt
2842 
2843         if image_meta.obj_attr_is_set("container_format"):
2844             metadata['container_format'] = image_meta.container_format
2845         else:
2846             metadata['container_format'] = "bare"
2847 
2848         return metadata
2849 
2850     def snapshot(self, context, instance, image_id, update_task_state):
2851         """Create snapshot from a running VM instance.
2852 
2853         This command only works with qemu 0.14+
2854         """
2855         try:
2856             guest = self._host.get_guest(instance)
2857         except exception.InstanceNotFound:
2858             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2859 
2860         snapshot = self._image_api.get(context, image_id)
2861 
2862         # source_format is an on-disk format
2863         # source_type is a backend type
2864         disk_path, source_format = libvirt_utils.find_disk(guest)
2865         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
2866 
2867         # We won't have source_type for raw or qcow2 disks, because we can't
2868         # determine that from the path. We should have it from the libvirt
2869         # xml, though.
2870         if source_type is None:
2871             source_type = source_format
2872         # For lxc instances we won't have it either from libvirt xml
2873         # (because we just gave libvirt the mounted filesystem), or the path,
2874         # so source_type is still going to be None. In this case,
2875         # root_disk is going to default to CONF.libvirt.images_type
2876         # below, which is still safe.
2877 
2878         image_format = CONF.libvirt.snapshot_image_format or source_type
2879 
2880         # NOTE(bfilippov): save lvm and rbd as raw
2881         if image_format == 'lvm' or image_format == 'rbd':
2882             image_format = 'raw'
2883 
2884         metadata = self._create_snapshot_metadata(instance.image_meta,
2885                                                   instance,
2886                                                   image_format,
2887                                                   snapshot['name'])
2888 
2889         snapshot_name = uuidutils.generate_uuid(dashed=False)
2890 
2891         # store current state so we know what to resume back to if we suspend
2892         original_power_state = guest.get_power_state(self._host)
2893 
2894         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
2895         #               cold snapshots. Currently, checking for encryption is
2896         #               redundant because LVM supports only cold snapshots.
2897         #               It is necessary in case this situation changes in the
2898         #               future.
2899         if (
2900             self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU) and
2901             source_type != 'lvm' and
2902             not CONF.ephemeral_storage_encryption.enabled and
2903             not CONF.workarounds.disable_libvirt_livesnapshot and
2904             # NOTE(stephenfin): Live snapshotting doesn't make sense for
2905             # shutdown instances
2906             original_power_state != power_state.SHUTDOWN
2907         ):
2908             live_snapshot = True
2909         else:
2910             live_snapshot = False
2911 
2912         self._suspend_guest_for_snapshot(
2913             context, live_snapshot, original_power_state, instance)
2914 
2915         root_disk = self.image_backend.by_libvirt_path(
2916             instance, disk_path, image_type=source_type)
2917 
2918         if live_snapshot:
2919             LOG.info("Beginning live snapshot process", instance=instance)
2920         else:
2921             LOG.info("Beginning cold snapshot process", instance=instance)
2922 
2923         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
2924 
2925         update_task_state(task_state=task_states.IMAGE_UPLOADING,
2926                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
2927 
2928         try:
2929             metadata['location'] = root_disk.direct_snapshot(
2930                 context, snapshot_name, image_format, image_id,
2931                 instance.image_ref)
2932             self._resume_guest_after_snapshot(
2933                 context, live_snapshot, original_power_state, instance, guest)
2934             self._image_api.update(context, image_id, metadata,
2935                                    purge_props=False)
2936         except (NotImplementedError, exception.ImageUnacceptable,
2937                 exception.Forbidden) as e:
2938             if type(e) != NotImplementedError:
2939                 LOG.warning('Performing standard snapshot because direct '
2940                             'snapshot failed: %(error)s',
2941                             {'error': encodeutils.exception_to_unicode(e)})
2942             failed_snap = metadata.pop('location', None)
2943             if failed_snap:
2944                 failed_snap = {'url': str(failed_snap)}
2945             root_disk.cleanup_direct_snapshot(failed_snap,
2946                                                   also_destroy_volume=True,
2947                                                   ignore_errors=True)
2948             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
2949                               expected_state=task_states.IMAGE_UPLOADING)
2950 
2951             # TODO(nic): possibly abstract this out to the root_disk
2952             if source_type == 'rbd' and live_snapshot:
2953                 # Standard snapshot uses qemu-img convert from RBD which is
2954                 # not safe to run with live_snapshot.
2955                 live_snapshot = False
2956                 # Suspend the guest, so this is no longer a live snapshot
2957                 self._suspend_guest_for_snapshot(
2958                     context, live_snapshot, original_power_state, instance)
2959 
2960             snapshot_directory = CONF.libvirt.snapshots_directory
2961             fileutils.ensure_tree(snapshot_directory)
2962             with utils.tempdir(dir=snapshot_directory) as tmpdir:
2963                 try:
2964                     out_path = os.path.join(tmpdir, snapshot_name)
2965                     if live_snapshot:
2966                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2967                         os.chmod(tmpdir, 0o701)
2968                         self._live_snapshot(context, instance, guest,
2969                                             disk_path, out_path, source_format,
2970                                             image_format, instance.image_meta)
2971                     else:
2972                         root_disk.snapshot_extract(out_path, image_format)
2973                     LOG.info("Snapshot extracted, beginning image upload",
2974                              instance=instance)
2975                 except libvirt.libvirtError as ex:
2976                     error_code = ex.get_error_code()
2977                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2978                         LOG.info('Instance %(instance_name)s disappeared '
2979                                  'while taking snapshot of it: [Error Code '
2980                                  '%(error_code)s] %(ex)s',
2981                                  {'instance_name': instance.name,
2982                                   'error_code': error_code,
2983                                   'ex': ex},
2984                                  instance=instance)
2985                         raise exception.InstanceNotFound(
2986                             instance_id=instance.uuid)
2987                     else:
2988                         raise
2989                 finally:
2990                     self._resume_guest_after_snapshot(
2991                         context, live_snapshot, original_power_state, instance,
2992                         guest)
2993 
2994                 # Upload that image to the image service
2995                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2996                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2997                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2998                     # execute operation with disk concurrency semaphore
2999                     with compute_utils.disk_ops_semaphore:
3000                         self._image_api.update(context,
3001                                                image_id,
3002                                                metadata,
3003                                                image_file)
3004         except Exception:
3005             with excutils.save_and_reraise_exception():
3006                 LOG.exception("Failed to snapshot image")
3007                 failed_snap = metadata.pop('location', None)
3008                 if failed_snap:
3009                     failed_snap = {'url': str(failed_snap)}
3010                 root_disk.cleanup_direct_snapshot(
3011                         failed_snap, also_destroy_volume=True,
3012                         ignore_errors=True)
3013 
3014         LOG.info("Snapshot image upload complete", instance=instance)
3015 
3016     def _needs_suspend_resume_for_snapshot(
3017         self,
3018         live_snapshot: bool,
3019         current_power_state: int,
3020     ):
3021         # NOTE(dkang): managedSave does not work for LXC
3022         if CONF.libvirt.virt_type == 'lxc':
3023             return False
3024 
3025         # Live snapshots do not necessitate suspending the domain
3026         if live_snapshot:
3027             return False
3028 
3029         # ...and neither does a non-running domain
3030         return current_power_state in (power_state.RUNNING, power_state.PAUSED)
3031 
3032     def _suspend_guest_for_snapshot(
3033         self,
3034         context: nova_context.RequestContext,
3035         live_snapshot: bool,
3036         current_power_state: int,
3037         instance: 'objects.Instance',
3038     ):
3039         if self._needs_suspend_resume_for_snapshot(
3040             live_snapshot, current_power_state,
3041         ):
3042             self.suspend(context, instance)
3043 
3044     def _resume_guest_after_snapshot(
3045         self,
3046         context: nova_context.RequestContext,
3047         live_snapshot: bool,
3048         original_power_state: int,
3049         instance: 'objects.Instance',
3050         guest: libvirt_guest.Guest,
3051     ):
3052         if not self._needs_suspend_resume_for_snapshot(
3053             live_snapshot, original_power_state,
3054         ):
3055             return
3056 
3057         current_power_state = guest.get_power_state(self._host)
3058 
3059         # TODO(stephenfin): Any reason we couldn't use 'self.resume' here?
3060         guest.launch(pause=current_power_state == power_state.PAUSED)
3061 
3062         self._attach_pci_devices(
3063             guest, pci_manager.get_instance_pci_devs(instance))
3064         self._attach_direct_passthrough_ports(context, instance, guest)
3065 
3066     def _can_set_admin_password(self, image_meta):
3067 
3068         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3069             if not image_meta.properties.get('hw_qemu_guest_agent', False):
3070                 raise exception.QemuGuestAgentNotEnabled()
3071         elif not CONF.libvirt.virt_type == 'parallels':
3072             raise exception.SetAdminPasswdNotSupported()
3073 
3074     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
3075         sshkey = instance.key_data if 'key_data' in instance else None
3076         if sshkey and sshkey.startswith("ssh-rsa"):
3077             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
3078             # NOTE(melwitt): The convert_password method doesn't actually do
3079             # anything with the context argument, so we can pass None.
3080             instance.system_metadata.update(
3081                 password.convert_password(None, base64.encode_as_text(enc)))
3082             instance.save()
3083 
3084     def set_admin_password(self, instance, new_pass):
3085         self._can_set_admin_password(instance.image_meta)
3086 
3087         guest = self._host.get_guest(instance)
3088         user = instance.image_meta.properties.get("os_admin_user")
3089         if not user:
3090             if instance.os_type == "windows":
3091                 user = "Administrator"
3092             else:
3093                 user = "root"
3094         try:
3095             guest.set_user_password(user, new_pass)
3096         except libvirt.libvirtError as ex:
3097             error_code = ex.get_error_code()
3098             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
3099                 LOG.debug('Failed to set password: QEMU agent unresponsive',
3100                           instance_uuid=instance.uuid)
3101                 raise NotImplementedError()
3102 
3103             err_msg = encodeutils.exception_to_unicode(ex)
3104             msg = (_('Error from libvirt while set password for username '
3105                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
3106                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
3107             raise exception.InternalError(msg)
3108         else:
3109             # Save the password in sysmeta so it may be retrieved from the
3110             # metadata service.
3111             self._save_instance_password_if_sshkey_present(instance, new_pass)
3112 
3113     def _can_quiesce(self, instance, image_meta):
3114         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
3115             raise exception.InstanceQuiesceNotSupported(
3116                 instance_id=instance.uuid)
3117 
3118         if not image_meta.properties.get('hw_qemu_guest_agent', False):
3119             raise exception.QemuGuestAgentNotEnabled()
3120 
3121     def _requires_quiesce(self, image_meta):
3122         return image_meta.properties.get('os_require_quiesce', False)
3123 
3124     def _set_quiesced(self, context, instance, image_meta, quiesced):
3125         self._can_quiesce(instance, image_meta)
3126         try:
3127             guest = self._host.get_guest(instance)
3128             if quiesced:
3129                 guest.freeze_filesystems()
3130             else:
3131                 guest.thaw_filesystems()
3132         except libvirt.libvirtError as ex:
3133             error_code = ex.get_error_code()
3134             err_msg = encodeutils.exception_to_unicode(ex)
3135             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
3136                      '[Error Code %(error_code)s] %(ex)s')
3137                    % {'instance_name': instance.name,
3138                       'error_code': error_code, 'ex': err_msg})
3139             raise exception.InternalError(msg)
3140 
3141     def quiesce(self, context, instance, image_meta):
3142         """Freeze the guest filesystems to prepare for snapshot.
3143 
3144         The qemu-guest-agent must be setup to execute fsfreeze.
3145         """
3146         self._set_quiesced(context, instance, image_meta, True)
3147 
3148     def unquiesce(self, context, instance, image_meta):
3149         """Thaw the guest filesystems after snapshot."""
3150         self._set_quiesced(context, instance, image_meta, False)
3151 
3152     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
3153                        source_format, image_format, image_meta):
3154         """Snapshot an instance without downtime."""
3155         dev = guest.get_block_device(disk_path)
3156 
3157         # Save a copy of the domain's persistent XML file
3158         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
3159 
3160         # Abort is an idempotent operation, so make sure any block
3161         # jobs which may have failed are ended.
3162         try:
3163             dev.abort_job()
3164         except Exception:
3165             pass
3166 
3167         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
3168         #             in QEMU 1.3. In order to do this, we need to create
3169         #             a destination image with the original backing file
3170         #             and matching size of the instance root disk.
3171         src_disk_size = libvirt_utils.get_disk_size(disk_path,
3172                                                     format=source_format)
3173         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
3174                                                         format=source_format,
3175                                                         basename=False)
3176         disk_delta = out_path + '.delta'
3177         libvirt_utils.create_cow_image(src_back_path, disk_delta,
3178                                        src_disk_size)
3179 
3180         quiesced = False
3181         try:
3182             self._set_quiesced(context, instance, image_meta, True)
3183             quiesced = True
3184         except exception.NovaException as err:
3185             if self._requires_quiesce(image_meta):
3186                 raise
3187             LOG.info('Skipping quiescing instance: %(reason)s.',
3188                      {'reason': err}, instance=instance)
3189 
3190         try:
3191             # NOTE (rmk): blockRebase cannot be executed on persistent
3192             #             domains, so we need to temporarily undefine it.
3193             #             If any part of this block fails, the domain is
3194             #             re-defined regardless.
3195             if guest.has_persistent_configuration():
3196                 hw_firmware_type = image_meta.properties.get(
3197                     'hw_firmware_type')
3198                 support_uefi = self._check_uefi_support(hw_firmware_type)
3199                 guest.delete_configuration(support_uefi)
3200 
3201             # NOTE (rmk): Establish a temporary mirror of our root disk and
3202             #             issue an abort once we have a complete copy.
3203             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
3204 
3205             while not dev.is_job_complete():
3206                 time.sleep(0.5)
3207 
3208             dev.abort_job()
3209             nova.privsep.path.chown(disk_delta, uid=os.getuid())
3210         finally:
3211             self._host.write_instance_config(xml)
3212             if quiesced:
3213                 self._set_quiesced(context, instance, image_meta, False)
3214 
3215         # Convert the delta (CoW) image with a backing file to a flat
3216         # image with no backing file.
3217         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
3218                                        out_path, image_format)
3219 
3220         # Remove the disk_delta file once the snapshot extracted, so that
3221         # it doesn't hang around till the snapshot gets uploaded
3222         fileutils.delete_if_exists(disk_delta)
3223 
3224     def _volume_snapshot_update_status(self, context, snapshot_id, status):
3225         """Send a snapshot status update to Cinder.
3226 
3227         This method captures and logs exceptions that occur
3228         since callers cannot do anything useful with these exceptions.
3229 
3230         Operations on the Cinder side waiting for this will time out if
3231         a failure occurs sending the update.
3232 
3233         :param context: security context
3234         :param snapshot_id: id of snapshot being updated
3235         :param status: new status value
3236 
3237         """
3238 
3239         try:
3240             self._volume_api.update_snapshot_status(context,
3241                                                     snapshot_id,
3242                                                     status)
3243         except Exception:
3244             LOG.exception('Failed to send updated snapshot status '
3245                           'to volume service.')
3246 
3247     def _volume_snapshot_create(self, context, instance, guest,
3248                                 volume_id, new_file):
3249         """Perform volume snapshot.
3250 
3251            :param guest: VM that volume is attached to
3252            :param volume_id: volume UUID to snapshot
3253            :param new_file: relative path to new qcow2 file present on share
3254 
3255         """
3256         xml = guest.get_xml_desc()
3257         xml_doc = etree.fromstring(xml)
3258 
3259         device_info = vconfig.LibvirtConfigGuest()
3260         device_info.parse_dom(xml_doc)
3261 
3262         disks_to_snap = []          # to be snapshotted by libvirt
3263         network_disks_to_snap = []  # network disks (netfs, etc.)
3264         disks_to_skip = []          # local disks not snapshotted
3265 
3266         for guest_disk in device_info.devices:
3267             if (guest_disk.root_name != 'disk'):
3268                 continue
3269 
3270             if (guest_disk.target_dev is None):
3271                 continue
3272 
3273             if (guest_disk.serial is None or guest_disk.serial != volume_id):
3274                 disks_to_skip.append(guest_disk.target_dev)
3275                 continue
3276 
3277             # disk is a Cinder volume with the correct volume_id
3278 
3279             disk_info = {
3280                 'dev': guest_disk.target_dev,
3281                 'serial': guest_disk.serial,
3282                 'current_file': guest_disk.source_path,
3283                 'source_protocol': guest_disk.source_protocol,
3284                 'source_name': guest_disk.source_name,
3285                 'source_hosts': guest_disk.source_hosts,
3286                 'source_ports': guest_disk.source_ports
3287             }
3288 
3289             # Determine path for new_file based on current path
3290             if disk_info['current_file'] is not None:
3291                 current_file = disk_info['current_file']
3292                 new_file_path = os.path.join(os.path.dirname(current_file),
3293                                              new_file)
3294                 disks_to_snap.append((current_file, new_file_path))
3295             # NOTE(mriedem): This used to include a check for gluster in
3296             # addition to netfs since they were added together. Support for
3297             # gluster was removed in the 16.0.0 Pike release. It is unclear,
3298             # however, if other volume drivers rely on the netfs disk source
3299             # protocol.
3300             elif disk_info['source_protocol'] == 'netfs':
3301                 network_disks_to_snap.append((disk_info, new_file))
3302 
3303         if not disks_to_snap and not network_disks_to_snap:
3304             msg = _('Found no disk to snapshot.')
3305             raise exception.InternalError(msg)
3306 
3307         snapshot = vconfig.LibvirtConfigGuestSnapshot()
3308 
3309         for current_name, new_filename in disks_to_snap:
3310             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
3311             snap_disk.name = current_name
3312             snap_disk.source_path = new_filename
3313             snap_disk.source_type = 'file'
3314             snap_disk.snapshot = 'external'
3315             snap_disk.driver_name = 'qcow2'
3316 
3317             snapshot.add_disk(snap_disk)
3318 
3319         for disk_info, new_filename in network_disks_to_snap:
3320             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
3321             snap_disk.name = disk_info['dev']
3322             snap_disk.source_type = 'network'
3323             snap_disk.source_protocol = disk_info['source_protocol']
3324             snap_disk.snapshot = 'external'
3325             snap_disk.source_path = new_filename
3326             old_dir = disk_info['source_name'].split('/')[0]
3327             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
3328             snap_disk.source_hosts = disk_info['source_hosts']
3329             snap_disk.source_ports = disk_info['source_ports']
3330 
3331             snapshot.add_disk(snap_disk)
3332 
3333         for dev in disks_to_skip:
3334             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
3335             snap_disk.name = dev
3336             snap_disk.snapshot = 'no'
3337 
3338             snapshot.add_disk(snap_disk)
3339 
3340         snapshot_xml = snapshot.to_xml()
3341         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
3342 
3343         image_meta = instance.image_meta
3344         try:
3345             # Check to see if we can quiesce the guest before taking the
3346             # snapshot.
3347             self._can_quiesce(instance, image_meta)
3348             try:
3349                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
3350                                reuse_ext=True, quiesce=True)
3351                 return
3352             except libvirt.libvirtError:
3353                 # If the image says that quiesce is required then we fail.
3354                 if self._requires_quiesce(image_meta):
3355                     raise
3356                 LOG.exception('Unable to create quiesced VM snapshot, '
3357                               'attempting again with quiescing disabled.',
3358                               instance=instance)
3359         except (exception.InstanceQuiesceNotSupported,
3360                 exception.QemuGuestAgentNotEnabled) as err:
3361             # If the image says that quiesce is required then we need to fail.
3362             if self._requires_quiesce(image_meta):
3363                 raise
3364             LOG.info('Skipping quiescing instance: %(reason)s.',
3365                      {'reason': err}, instance=instance)
3366 
3367         try:
3368             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
3369                            reuse_ext=True, quiesce=False)
3370         except libvirt.libvirtError:
3371             LOG.exception('Unable to create VM snapshot, '
3372                           'failing volume_snapshot operation.',
3373                           instance=instance)
3374 
3375             raise
3376 
3377     def _volume_refresh_connection_info(self, context, instance, volume_id):
3378         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
3379                   context, volume_id, instance.uuid)
3380 
3381         driver_bdm = driver_block_device.convert_volume(bdm)
3382         if driver_bdm:
3383             driver_bdm.refresh_connection_info(context, instance,
3384                                                self._volume_api, self)
3385 
3386     def volume_snapshot_create(self, context, instance, volume_id,
3387                                create_info):
3388         """Create snapshots of a Cinder volume via libvirt.
3389 
3390         :param instance: VM instance object reference
3391         :param volume_id: id of volume being snapshotted
3392         :param create_info: dict of information used to create snapshots
3393                      - snapshot_id : ID of snapshot
3394                      - type : qcow2 / <other>
3395                      - new_file : qcow2 file created by Cinder which
3396                      becomes the VM's active image after
3397                      the snapshot is complete
3398         """
3399 
3400         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
3401                   {'c_info': create_info}, instance=instance)
3402 
3403         try:
3404             guest = self._host.get_guest(instance)
3405         except exception.InstanceNotFound:
3406             raise exception.InstanceNotRunning(instance_id=instance.uuid)
3407 
3408         if create_info['type'] != 'qcow2':
3409             msg = _('Unknown type: %s') % create_info['type']
3410             raise exception.InternalError(msg)
3411 
3412         snapshot_id = create_info.get('snapshot_id', None)
3413         if snapshot_id is None:
3414             msg = _('snapshot_id required in create_info')
3415             raise exception.InternalError(msg)
3416 
3417         try:
3418             self._volume_snapshot_create(context, instance, guest,
3419                                          volume_id, create_info['new_file'])
3420         except Exception:
3421             with excutils.save_and_reraise_exception():
3422                 LOG.exception('Error occurred during volume_snapshot_create, '
3423                               'sending error status to Cinder.',
3424                               instance=instance)
3425                 self._volume_snapshot_update_status(
3426                     context, snapshot_id, 'error')
3427 
3428         self._volume_snapshot_update_status(
3429             context, snapshot_id, 'creating')
3430 
3431         def _wait_for_snapshot():
3432             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
3433 
3434             if snapshot.get('status') != 'creating':
3435                 self._volume_refresh_connection_info(context, instance,
3436                                                      volume_id)
3437                 raise loopingcall.LoopingCallDone()
3438 
3439         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
3440         timer.start(interval=0.5).wait()
3441 
3442     @staticmethod
3443     def _rebase_with_qemu_img(source_path, rebase_base):
3444         """Rebase a disk using qemu-img.
3445 
3446         :param source_path: the disk source path to rebase
3447         :type source_path: string
3448         :param rebase_base: the new parent in the backing chain
3449         :type rebase_base: None or string
3450         """
3451 
3452         if rebase_base is None:
3453             # If backing_file is specified as "" (the empty string), then
3454             # the image is rebased onto no backing file (i.e. it will exist
3455             # independently of any backing file).
3456             backing_file = ""
3457             qemu_img_extra_arg = []
3458         else:
3459             # If the rebased image is going to have a backing file then
3460             # explicitly set the backing file format to avoid any security
3461             # concerns related to file format auto detection.
3462             if os.path.isabs(rebase_base):
3463                 backing_file = rebase_base
3464             else:
3465                 # this is a probably a volume snapshot case where the
3466                 # rebase_base is relative. See bug
3467                 # https://bugs.launchpad.net/nova/+bug/1885528
3468                 backing_file_name = os.path.basename(rebase_base)
3469                 volume_path = os.path.dirname(source_path)
3470                 backing_file = os.path.join(volume_path, backing_file_name)
3471 
3472             b_file_fmt = images.qemu_img_info(backing_file).file_format
3473             qemu_img_extra_arg = ['-F', b_file_fmt]
3474 
3475         qemu_img_extra_arg.append(source_path)
3476         # execute operation with disk concurrency semaphore
3477         with compute_utils.disk_ops_semaphore:
3478             processutils.execute("qemu-img", "rebase", "-b", backing_file,
3479                                  *qemu_img_extra_arg)
3480 
3481     def _volume_snapshot_delete(self, context, instance, volume_id,
3482                                 snapshot_id, delete_info=None):
3483         """Note:
3484             if file being merged into == active image:
3485                 do a blockRebase (pull) operation
3486             else:
3487                 do a blockCommit operation
3488             Files must be adjacent in snap chain.
3489 
3490         :param instance: instance object reference
3491         :param volume_id: volume UUID
3492         :param snapshot_id: snapshot UUID (unused currently)
3493         :param delete_info: {
3494             'type':              'qcow2',
3495             'file_to_merge':     'a.img',
3496             'merge_target_file': 'b.img' or None (if merging file_to_merge into
3497                                                   active image)
3498           }
3499         """
3500 
3501         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
3502                   instance=instance)
3503 
3504         if delete_info['type'] != 'qcow2':
3505             msg = _('Unknown delete_info type %s') % delete_info['type']
3506             raise exception.InternalError(msg)
3507 
3508         try:
3509             guest = self._host.get_guest(instance)
3510         except exception.InstanceNotFound:
3511             raise exception.InstanceNotRunning(instance_id=instance.uuid)
3512 
3513         # Find dev name
3514         xml = guest.get_xml_desc()
3515         xml_doc = etree.fromstring(xml)
3516 
3517         device_info = vconfig.LibvirtConfigGuest()
3518         device_info.parse_dom(xml_doc)
3519 
3520         for guest_disk in device_info.devices:
3521             if (guest_disk.root_name != 'disk'):
3522                 continue
3523 
3524             if (guest_disk.target_dev is None or guest_disk.serial is None):
3525                 continue
3526 
3527             if (
3528                 guest_disk.source_path is None and
3529                 guest_disk.source_protocol is None
3530             ):
3531                 continue
3532 
3533             if guest_disk.serial == volume_id:
3534                 my_dev = guest_disk.target_dev
3535 
3536                 active_protocol = guest_disk.source_protocol
3537                 active_disk_object = guest_disk
3538                 break
3539         else:
3540             LOG.debug('Domain XML: %s', xml, instance=instance)
3541             msg = (_("Disk with id '%s' not found attached to instance.")
3542                    % volume_id)
3543             raise exception.InternalError(msg)
3544 
3545         LOG.debug("found device at %s", my_dev, instance=instance)
3546 
3547         def _get_snap_dev(filename, backing_store):
3548             if filename is None:
3549                 msg = _('filename cannot be None')
3550                 raise exception.InternalError(msg)
3551 
3552             # libgfapi delete
3553             LOG.debug("XML: %s", xml)
3554 
3555             LOG.debug("active disk object: %s", active_disk_object)
3556 
3557             # determine reference within backing store for desired image
3558             filename_to_merge = filename
3559             matched_name = None
3560             b = backing_store
3561             index = None
3562 
3563             current_filename = active_disk_object.source_name.split('/')[1]
3564             if current_filename == filename_to_merge:
3565                 return my_dev + '[0]'
3566 
3567             while b is not None:
3568                 source_filename = b.source_name.split('/')[1]
3569                 if source_filename == filename_to_merge:
3570                     LOG.debug('found match: %s', b.source_name)
3571                     matched_name = b.source_name
3572                     index = b.index
3573                     break
3574 
3575                 b = b.backing_store
3576 
3577             if matched_name is None:
3578                 msg = _('no match found for %s') % (filename_to_merge)
3579                 raise exception.InternalError(msg)
3580 
3581             LOG.debug('index of match (%s) is %s', b.source_name, index)
3582 
3583             my_snap_dev = '%s[%s]' % (my_dev, index)
3584             return my_snap_dev
3585 
3586         if delete_info['merge_target_file'] is None:
3587             # pull via blockRebase()
3588 
3589             # Merge the most recent snapshot into the active image
3590 
3591             rebase_disk = my_dev
3592             rebase_base = delete_info['file_to_merge']  # often None
3593             if (active_protocol is not None) and (rebase_base is not None):
3594                 rebase_base = _get_snap_dev(rebase_base,
3595                                             active_disk_object.backing_store)
3596 
3597             relative = rebase_base is not None
3598             LOG.debug(
3599                 'disk: %(disk)s, base: %(base)s, '
3600                 'bw: %(bw)s, relative: %(relative)s',
3601                 {'disk': rebase_disk,
3602                  'base': rebase_base,
3603                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
3604                  'relative': str(relative)}, instance=instance)
3605 
3606             dev = guest.get_block_device(rebase_disk)
3607             if guest.is_active():
3608                 result = dev.rebase(rebase_base, relative=relative)
3609                 if result == 0:
3610                     LOG.debug('blockRebase started successfully',
3611                               instance=instance)
3612 
3613                 while not dev.is_job_complete():
3614                     LOG.debug('waiting for blockRebase job completion',
3615                               instance=instance)
3616                     time.sleep(0.5)
3617 
3618             # If the guest is not running libvirt won't do a blockRebase.
3619             # In that case, let's ask qemu-img to rebase the disk.
3620             else:
3621                 LOG.debug('Guest is not running so doing a block rebase '
3622                           'using "qemu-img rebase"', instance=instance)
3623 
3624                 # It's unsure how well qemu-img handles network disks for
3625                 # every protocol. So let's be safe.
3626                 active_protocol = active_disk_object.source_protocol
3627                 if active_protocol is not None:
3628                     msg = _("Something went wrong when deleting a volume "
3629                             "snapshot: rebasing a %(protocol)s network disk "
3630                             "using qemu-img has not been fully tested"
3631                            ) % {'protocol': active_protocol}
3632                     LOG.error(msg)
3633                     raise exception.InternalError(msg)
3634                 self._rebase_with_qemu_img(active_disk_object.source_path,
3635                                            rebase_base)
3636 
3637         else:
3638             # commit with blockCommit()
3639             my_snap_base = None
3640             my_snap_top = None
3641             commit_disk = my_dev
3642 
3643             if active_protocol is not None:
3644                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
3645                                              active_disk_object.backing_store)
3646                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
3647                                             active_disk_object.backing_store)
3648 
3649             commit_base = my_snap_base or delete_info['merge_target_file']
3650             commit_top = my_snap_top or delete_info['file_to_merge']
3651 
3652             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
3653                       'commit_base=%(commit_base)s '
3654                       'commit_top=%(commit_top)s ',
3655                       {'commit_disk': commit_disk,
3656                        'commit_base': commit_base,
3657                        'commit_top': commit_top}, instance=instance)
3658 
3659             dev = guest.get_block_device(commit_disk)
3660             result = dev.commit(commit_base, commit_top, relative=True)
3661 
3662             if result == 0:
3663                 LOG.debug('blockCommit started successfully',
3664                           instance=instance)
3665 
3666             while not dev.is_job_complete():
3667                 LOG.debug('waiting for blockCommit job completion',
3668                           instance=instance)
3669                 time.sleep(0.5)
3670 
3671     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
3672                                delete_info):
3673         try:
3674             self._volume_snapshot_delete(context, instance, volume_id,
3675                                          snapshot_id, delete_info=delete_info)
3676         except Exception:
3677             with excutils.save_and_reraise_exception():
3678                 LOG.exception('Error occurred during volume_snapshot_delete, '
3679                               'sending error status to Cinder.',
3680                               instance=instance)
3681                 self._volume_snapshot_update_status(
3682                     context, snapshot_id, 'error_deleting')
3683 
3684         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
3685         self._volume_refresh_connection_info(context, instance, volume_id)
3686 
3687     def reboot(self, context, instance, network_info, reboot_type,
3688                block_device_info=None, bad_volumes_callback=None,
3689                accel_info=None):
3690         """Reboot a virtual machine, given an instance reference."""
3691         if reboot_type == 'SOFT':
3692             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
3693             try:
3694                 soft_reboot_success = self._soft_reboot(instance)
3695             except libvirt.libvirtError as e:
3696                 LOG.debug("Instance soft reboot failed: %s",
3697                           encodeutils.exception_to_unicode(e),
3698                           instance=instance)
3699                 soft_reboot_success = False
3700 
3701             if soft_reboot_success:
3702                 LOG.info("Instance soft rebooted successfully.",
3703                          instance=instance)
3704                 return
3705             else:
3706                 LOG.warning("Failed to soft reboot instance. "
3707                             "Trying hard reboot.",
3708                             instance=instance)
3709         return self._hard_reboot(context, instance, network_info,
3710                                  block_device_info, accel_info)
3711 
3712     def _soft_reboot(self, instance):
3713         """Attempt to shutdown and restart the instance gracefully.
3714 
3715         We use shutdown and create here so we can return if the guest
3716         responded and actually rebooted. Note that this method only
3717         succeeds if the guest responds to acpi. Therefore we return
3718         success or failure so we can fall back to a hard reboot if
3719         necessary.
3720 
3721         :returns: True if the reboot succeeded
3722         """
3723         guest = self._host.get_guest(instance)
3724 
3725         state = guest.get_power_state(self._host)
3726         old_domid = guest.id
3727         # NOTE(vish): This check allows us to reboot an instance that
3728         #             is already shutdown.
3729         if state == power_state.RUNNING:
3730             guest.shutdown()
3731         # NOTE(vish): This actually could take slightly longer than the
3732         #             FLAG defines depending on how long the get_info
3733         #             call takes to return.
3734         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
3735             guest = self._host.get_guest(instance)
3736 
3737             state = guest.get_power_state(self._host)
3738             new_domid = guest.id
3739 
3740             # NOTE(ivoks): By checking domain IDs, we make sure we are
3741             #              not recreating domain that's already running.
3742             if old_domid != new_domid:
3743                 if state in (power_state.SHUTDOWN, power_state.CRASHED):
3744                     LOG.info("Instance shutdown successfully.",
3745                              instance=instance)
3746                     guest.launch()
3747                     timer = loopingcall.FixedIntervalLoopingCall(
3748                         self._wait_for_running, instance)
3749                     timer.start(interval=0.5).wait()
3750                     return True
3751                 else:
3752                     LOG.info("Instance may have been rebooted during soft "
3753                              "reboot, so return now.", instance=instance)
3754                     return True
3755             greenthread.sleep(1)
3756         return False
3757 
3758     def _hard_reboot(self, context, instance, network_info,
3759                      block_device_info=None, accel_info=None):
3760         """Reboot a virtual machine, given an instance reference.
3761 
3762         Performs a Libvirt reset (if supported) on the domain.
3763 
3764         If Libvirt reset is unavailable this method actually destroys and
3765         re-creates the domain to ensure the reboot happens, as the guest
3766         OS cannot ignore this action.
3767         """
3768         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
3769         # need to remember the existing mdevs for reusing them.
3770         mdevs = self._get_all_assigned_mediated_devices(instance)
3771         mdevs = list(mdevs.keys())
3772         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
3773         # the hard reboot operation is relied upon by operators to be an
3774         # automated attempt to fix as many things as possible about a
3775         # non-functioning instance before resorting to manual intervention.
3776         # With this goal in mind, we tear down all the aspects of an instance
3777         # we can here without losing data. This allows us to re-initialise from
3778         # scratch, and hopefully fix, most aspects of a non-functioning guest.
3779         self.destroy(context, instance, network_info, destroy_disks=False,
3780                      block_device_info=block_device_info)
3781 
3782         # Convert the system metadata to image metadata
3783         # NOTE(mdbooth): This is a workaround for stateless Nova compute
3784         #                https://bugs.launchpad.net/nova/+bug/1349978
3785         instance_dir = libvirt_utils.get_instance_path(instance)
3786         fileutils.ensure_tree(instance_dir)
3787 
3788         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3789                                             instance,
3790                                             instance.image_meta,
3791                                             block_device_info)
3792         # NOTE(vish): This could generate the wrong device_format if we are
3793         #             using the raw backend and the images don't exist yet.
3794         #             The create_images_and_backing below doesn't properly
3795         #             regenerate raw backend images, however, so when it
3796         #             does we need to (re)generate the xml after the images
3797         #             are in place.
3798         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3799                                   instance.image_meta,
3800                                   block_device_info=block_device_info,
3801                                   mdevs=mdevs, accel_info=accel_info)
3802 
3803         # NOTE(mdbooth): context.auth_token will not be set when we call
3804         #                _hard_reboot from resume_state_on_host_boot()
3805         if context.auth_token is not None:
3806             # NOTE (rmk): Re-populate any missing backing files.
3807             config = vconfig.LibvirtConfigGuest()
3808             config.parse_str(xml)
3809             backing_disk_info = self._get_instance_disk_info_from_config(
3810                 config, block_device_info)
3811             self._create_images_and_backing(context, instance, instance_dir,
3812                                             backing_disk_info)
3813 
3814         # Initialize all the necessary networking, block devices and
3815         # start the instance.
3816         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
3817         # unplugged vifs earlier. The behavior of neutron plug events depends
3818         # on which vif type we're using and we are working with a stale network
3819         # info cache here, so won't rely on waiting for neutron plug events.
3820         # vifs_already_plugged=True means "do not wait for neutron plug events"
3821         # NOTE(efried): The instance should already have a vtpm_secret_uuid
3822         # registered if appropriate.
3823         self._create_guest_with_network(
3824             context, xml, instance, network_info, block_device_info,
3825             vifs_already_plugged=True)
3826 
3827         def _wait_for_reboot():
3828             """Called at an interval until the VM is running again."""
3829             state = self.get_info(instance).state
3830 
3831             if state == power_state.RUNNING:
3832                 LOG.info("Instance rebooted successfully.",
3833                          instance=instance)
3834                 raise loopingcall.LoopingCallDone()
3835 
3836         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
3837         timer.start(interval=0.5).wait()
3838 
3839     def pause(self, instance):
3840         """Pause VM instance."""
3841         self._host.get_guest(instance).pause()
3842 
3843     def unpause(self, instance):
3844         """Unpause paused VM instance."""
3845         guest = self._host.get_guest(instance)
3846         guest.resume()
3847         guest.sync_guest_time()
3848 
3849     def _clean_shutdown(self, instance, timeout, retry_interval):
3850         """Attempt to shutdown the instance gracefully.
3851 
3852         :param instance: The instance to be shutdown
3853         :param timeout: How long to wait in seconds for the instance to
3854                         shutdown
3855         :param retry_interval: How often in seconds to signal the instance
3856                                to shutdown while waiting
3857 
3858         :returns: True if the shutdown succeeded
3859         """
3860 
3861         # List of states that represent a shutdown instance
3862         SHUTDOWN_STATES = [power_state.SHUTDOWN,
3863                            power_state.CRASHED]
3864 
3865         try:
3866             guest = self._host.get_guest(instance)
3867         except exception.InstanceNotFound:
3868             # If the instance has gone then we don't need to
3869             # wait for it to shutdown
3870             return True
3871 
3872         state = guest.get_power_state(self._host)
3873         if state in SHUTDOWN_STATES:
3874             LOG.info("Instance already shutdown.", instance=instance)
3875             return True
3876 
3877         LOG.debug("Shutting down instance from state %s", state,
3878                   instance=instance)
3879         guest.shutdown()
3880         retry_countdown = retry_interval
3881 
3882         for sec in range(timeout):
3883 
3884             guest = self._host.get_guest(instance)
3885             state = guest.get_power_state(self._host)
3886 
3887             if state in SHUTDOWN_STATES:
3888                 LOG.info("Instance shutdown successfully after %d seconds.",
3889                          sec, instance=instance)
3890                 return True
3891 
3892             # Note(PhilD): We can't assume that the Guest was able to process
3893             #              any previous shutdown signal (for example it may
3894             #              have still been startingup, so within the overall
3895             #              timeout we re-trigger the shutdown every
3896             #              retry_interval
3897             if retry_countdown == 0:
3898                 retry_countdown = retry_interval
3899                 # Instance could shutdown at any time, in which case we
3900                 # will get an exception when we call shutdown
3901                 try:
3902                     LOG.debug("Instance in state %s after %d seconds - "
3903                               "resending shutdown", state, sec,
3904                               instance=instance)
3905                     guest.shutdown()
3906                 except libvirt.libvirtError:
3907                     # Assume this is because its now shutdown, so loop
3908                     # one more time to clean up.
3909                     LOG.debug("Ignoring libvirt exception from shutdown "
3910                               "request.", instance=instance)
3911                     continue
3912             else:
3913                 retry_countdown -= 1
3914 
3915             time.sleep(1)
3916 
3917         LOG.info("Instance failed to shutdown in %d seconds.",
3918                  timeout, instance=instance)
3919         return False
3920 
3921     def power_off(self, instance, timeout=0, retry_interval=0):
3922         """Power off the specified instance."""
3923         if timeout:
3924             self._clean_shutdown(instance, timeout, retry_interval)
3925         self._destroy(instance)
3926 
3927     def power_on(self, context, instance, network_info,
3928                  block_device_info=None, accel_info=None):
3929         """Power on the specified instance."""
3930         # We use _hard_reboot here to ensure that all backing files,
3931         # network, and block device connections, etc. are established
3932         # and available before we attempt to start the instance.
3933         self._hard_reboot(context, instance, network_info, block_device_info,
3934                           accel_info)
3935 
3936     def trigger_crash_dump(self, instance):
3937 
3938         """Trigger crash dump by injecting an NMI to the specified instance."""
3939         try:
3940             self._host.get_guest(instance).inject_nmi()
3941         except libvirt.libvirtError as ex:
3942             error_code = ex.get_error_code()
3943 
3944             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
3945                 raise exception.TriggerCrashDumpNotSupported()
3946             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
3947                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
3948 
3949             LOG.exception(
3950                 'Error from libvirt while injecting an NMI to '
3951                 '%(instance_uuid)s: [Error Code %(error_code)s] %(ex)s',
3952                 {'instance_uuid': instance.uuid,
3953                  'error_code': error_code, 'ex': ex})
3954             raise
3955 
3956     def suspend(self, context, instance):
3957         """Suspend the specified instance."""
3958         guest = self._host.get_guest(instance)
3959 
3960         self._detach_pci_devices(guest,
3961             pci_manager.get_instance_pci_devs(instance))
3962         self._detach_direct_passthrough_ports(context, instance, guest)
3963         self._detach_mediated_devices(guest)
3964         guest.save_memory_state()
3965 
3966     def resume(self, context, instance, network_info, block_device_info=None):
3967         """resume the specified instance."""
3968         xml = self._get_existing_domain_xml(instance, network_info,
3969                                             block_device_info)
3970         # NOTE(efried): The instance should already have a vtpm_secret_uuid
3971         # registered if appropriate.
3972         guest = self._create_guest_with_network(
3973             context, xml, instance, network_info, block_device_info,
3974             vifs_already_plugged=True)
3975         self._attach_pci_devices(guest,
3976             pci_manager.get_instance_pci_devs(instance))
3977         self._attach_direct_passthrough_ports(
3978             context, instance, guest, network_info)
3979         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3980                                                      instance)
3981         timer.start(interval=0.5).wait()
3982         guest.sync_guest_time()
3983 
3984     def resume_state_on_host_boot(self, context, instance, network_info,
3985                                   block_device_info=None):
3986         """resume guest state when a host is booted."""
3987         # Check if the instance is running already and avoid doing
3988         # anything if it is.
3989         try:
3990             guest = self._host.get_guest(instance)
3991             state = guest.get_power_state(self._host)
3992 
3993             ignored_states = (power_state.RUNNING,
3994                               power_state.SUSPENDED,
3995                               power_state.NOSTATE,
3996                               power_state.PAUSED)
3997 
3998             if state in ignored_states:
3999                 return
4000         except (exception.InternalError, exception.InstanceNotFound):
4001             pass
4002 
4003         # Instance is not up and could be in an unknown state.
4004         # Be as absolute as possible about getting it back into
4005         # a known and running state.
4006         self._hard_reboot(context, instance, network_info, block_device_info)
4007 
4008     def rescue(self, context, instance, network_info, image_meta,
4009                rescue_password, block_device_info):
4010         """Loads a VM using rescue images.
4011 
4012         A rescue is normally performed when something goes wrong with the
4013         primary images and data needs to be corrected/recovered. Rescuing
4014         should not edit or over-ride the original image, only allow for
4015         data recovery.
4016 
4017         Two modes are provided when rescuing an instance with this driver.
4018 
4019         The original and default rescue mode, where the rescue boot disk,
4020         original root disk and optional regenerated config drive are attached
4021         to the instance.
4022 
4023         A second stable device rescue mode is also provided where all of the
4024         original devices are attached to the instance during the rescue attempt
4025         with the addition of the rescue boot disk. This second mode is
4026         controlled by the hw_rescue_device and hw_rescue_bus image properties
4027         on the rescue image provided to this method via image_meta.
4028 
4029         :param nova.context.RequestContext context:
4030             The context for the rescue.
4031         :param nova.objects.instance.Instance instance:
4032             The instance being rescued.
4033         :param nova.network.model.NetworkInfo network_info:
4034             Necessary network information for the resume.
4035         :param nova.objects.ImageMeta image_meta:
4036             The metadata of the image of the instance.
4037         :param rescue_password: new root password to set for rescue.
4038         :param dict block_device_info:
4039             The block device mapping of the instance.
4040         """
4041         instance_dir = libvirt_utils.get_instance_path(instance)
4042         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
4043         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
4044         with open(unrescue_xml_path, 'w') as f:
4045             f.write(unrescue_xml)
4046 
4047         rescue_image_id = None
4048         rescue_image_meta = None
4049         if image_meta.obj_attr_is_set("id"):
4050             rescue_image_id = image_meta.id
4051 
4052         rescue_images = {
4053             'image_id': (rescue_image_id or
4054                         CONF.libvirt.rescue_image_id or instance.image_ref),
4055             'kernel_id': (CONF.libvirt.rescue_kernel_id or
4056                           instance.kernel_id),
4057             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
4058                            instance.ramdisk_id),
4059         }
4060 
4061         virt_type = CONF.libvirt.virt_type
4062         if hardware.check_hw_rescue_props(image_meta):
4063             LOG.info("Attempting a stable device rescue", instance=instance)
4064             # NOTE(lyarwood): Stable device rescue is not supported when using
4065             # the LXC virt_type as it does not support the required
4066             # <boot order=''> definitions allowing an instance to boot from the
4067             # rescue device added as a final device to the domain.
4068             if virt_type == 'lxc':
4069                 reason = _(
4070                     "Stable device rescue is not supported by virt_type '%s'"
4071                 )
4072                 raise exception.InstanceNotRescuable(
4073                     instance_id=instance.uuid, reason=reason % virt_type)
4074             # NOTE(lyarwood): Stable device rescue provides the original disk
4075             # mapping of the instance with the rescue device appened to the
4076             # end. As a result we need to provide the original image_meta, the
4077             # new rescue_image_meta and block_device_info when calling
4078             # get_disk_info.
4079             rescue_image_meta = image_meta
4080             if instance.image_ref:
4081                 image_meta = objects.ImageMeta.from_image_ref(
4082                     context, self._image_api, instance.image_ref)
4083             else:
4084                 # NOTE(lyarwood): If instance.image_ref isn't set attempt to
4085                 # lookup the original image_meta from the bdms. This will
4086                 # return an empty dict if no valid image_meta is found.
4087                 image_meta_dict = block_device.get_bdm_image_metadata(
4088                     context, self._image_api, self._volume_api,
4089                     block_device_info['block_device_mapping'],
4090                     legacy_bdm=False)
4091                 image_meta = objects.ImageMeta.from_dict(image_meta_dict)
4092 
4093         else:
4094             LOG.info("Attempting rescue", instance=instance)
4095             # NOTE(lyarwood): A legacy rescue only provides the rescue device
4096             # and the original root device so we don't need to provide
4097             # block_device_info to the get_disk_info call.
4098             block_device_info = None
4099 
4100         disk_info = blockinfo.get_disk_info(virt_type, instance, image_meta,
4101             rescue=True, block_device_info=block_device_info,
4102             rescue_image_meta=rescue_image_meta)
4103         LOG.debug("rescue generated disk_info: %s", disk_info)
4104 
4105         injection_info = InjectionInfo(network_info=network_info,
4106                                        admin_pass=rescue_password,
4107                                        files=None)
4108         gen_confdrive = functools.partial(self._create_configdrive,
4109                                           context, instance, injection_info,
4110                                           rescue=True)
4111         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
4112         # remember the existing mdevs for reusing them.
4113         mdevs = self._get_all_assigned_mediated_devices(instance)
4114         mdevs = list(mdevs.keys())
4115         self._create_image(context, instance, disk_info['mapping'],
4116                            injection_info=injection_info, suffix='.rescue',
4117                            disk_images=rescue_images)
4118         # NOTE(efried): The instance should already have a vtpm_secret_uuid
4119         # registered if appropriate.
4120         xml = self._get_guest_xml(context, instance, network_info, disk_info,
4121                                   image_meta, rescue=rescue_images,
4122                                   mdevs=mdevs,
4123                                   block_device_info=block_device_info)
4124         self._destroy(instance)
4125         self._create_guest(
4126             context, xml, instance, post_xml_callback=gen_confdrive,
4127         )
4128 
4129     def unrescue(
4130         self,
4131         context: nova_context.RequestContext,
4132         instance: 'objects.Instance',
4133     ):
4134         """Reboot the VM which is being rescued back into primary images."""
4135         instance_dir = libvirt_utils.get_instance_path(instance)
4136         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
4137         # The xml should already contain the secret_uuid if relevant.
4138         xml = libvirt_utils.load_file(unrescue_xml_path)
4139 
4140         self._destroy(instance)
4141         self._create_guest(context, xml, instance)
4142         os.unlink(unrescue_xml_path)
4143         rescue_files = os.path.join(instance_dir, "*.rescue")
4144         for rescue_file in glob.iglob(rescue_files):
4145             if os.path.isdir(rescue_file):
4146                 shutil.rmtree(rescue_file)
4147             else:
4148                 os.unlink(rescue_file)
4149         # cleanup rescue volume
4150         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
4151                                 if lvmdisk.endswith('.rescue')])
4152         if CONF.libvirt.images_type == 'rbd':
4153             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
4154                                       disk.endswith('.rescue'))
4155             rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
4156 
4157     def poll_rebooting_instances(self, timeout, instances):
4158         pass
4159 
4160     def spawn(self, context, instance, image_meta, injected_files,
4161               admin_password, allocations, network_info=None,
4162               block_device_info=None, power_on=True, accel_info=None):
4163         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
4164                                             instance,
4165                                             image_meta,
4166                                             block_device_info)
4167         injection_info = InjectionInfo(network_info=network_info,
4168                                        files=injected_files,
4169                                        admin_pass=admin_password)
4170         gen_confdrive = functools.partial(self._create_configdrive,
4171                                           context, instance,
4172                                           injection_info)
4173         created_instance_dir, created_disks = self._create_image(
4174                 context, instance, disk_info['mapping'],
4175                 injection_info=injection_info,
4176                 block_device_info=block_device_info)
4177 
4178         # Required by Quobyte CI
4179         self._ensure_console_log_for_instance(instance)
4180 
4181         # Does the guest need to be assigned some vGPU mediated devices ?
4182         mdevs = self._allocate_mdevs(allocations)
4183 
4184         # If the guest needs a vTPM, _get_guest_xml needs its secret to exist
4185         # and its uuid to be registered in the instance prior to _get_guest_xml
4186         if CONF.libvirt.swtpm_enabled and hardware.get_vtpm_constraint(
4187             instance.flavor, image_meta
4188         ):
4189             if not instance.system_metadata.get('vtpm_secret_uuid'):
4190                 # Create the secret via the key manager service so that we have
4191                 # it to hand when generating the XML. This is slightly wasteful
4192                 # as we'll perform a redundant key manager API call later when
4193                 # we create the domain but the alternative is an ugly mess
4194                 crypto.ensure_vtpm_secret(context, instance)
4195 
4196         xml = self._get_guest_xml(context, instance, network_info,
4197                                   disk_info, image_meta,
4198                                   block_device_info=block_device_info,
4199                                   mdevs=mdevs, accel_info=accel_info)
4200         self._create_guest_with_network(
4201             context, xml, instance, network_info, block_device_info,
4202             post_xml_callback=gen_confdrive,
4203             power_on=power_on,
4204             cleanup_instance_dir=created_instance_dir,
4205             cleanup_instance_disks=created_disks)
4206         LOG.debug("Guest created on hypervisor", instance=instance)
4207 
4208         def _wait_for_boot():
4209             """Called at an interval until the VM is running."""
4210             state = self.get_info(instance).state
4211 
4212             if state == power_state.RUNNING:
4213                 LOG.info("Instance spawned successfully.", instance=instance)
4214                 raise loopingcall.LoopingCallDone()
4215 
4216         if power_on:
4217             timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
4218             timer.start(interval=0.5).wait()
4219         else:
4220             LOG.info("Instance spawned successfully.", instance=instance)
4221 
4222     def _get_console_output_file(self, instance, console_log):
4223         bytes_to_read = MAX_CONSOLE_BYTES
4224         log_data = b""  # The last N read bytes
4225         i = 0  # in case there is a log rotation (like "virtlogd")
4226         path = console_log
4227 
4228         while bytes_to_read > 0 and os.path.exists(path):
4229             read_log_data, remaining = nova.privsep.path.last_bytes(
4230                                         path, bytes_to_read)
4231             # We need the log file content in chronological order,
4232             # that's why we *prepend* the log data.
4233             log_data = read_log_data + log_data
4234 
4235             # Prep to read the next file in the chain
4236             bytes_to_read -= len(read_log_data)
4237             path = console_log + "." + str(i)
4238             i += 1
4239 
4240             if remaining > 0:
4241                 LOG.info('Truncated console log returned, '
4242                          '%d bytes ignored', remaining, instance=instance)
4243         return log_data
4244 
4245     def get_console_output(self, context, instance):
4246         guest = self._host.get_guest(instance)
4247 
4248         xml = guest.get_xml_desc()
4249         tree = etree.fromstring(xml)
4250 
4251         # check for different types of consoles
4252         path_sources = [
4253             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
4254             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
4255             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
4256         console_type = ""
4257         console_path = ""
4258         for c_type, epath, attrib in path_sources:
4259             node = tree.find(epath)
4260             if (node is not None) and node.get(attrib):
4261                 console_type = c_type
4262                 console_path = node.get(attrib)
4263                 break
4264 
4265         # instance has no console at all
4266         if not console_path:
4267             raise exception.ConsoleNotAvailable()
4268 
4269         # instance has a console, but file doesn't exist (yet?)
4270         if not os.path.exists(console_path):
4271             LOG.info('console logfile for instance does not exist',
4272                       instance=instance)
4273             return ""
4274 
4275         # pty consoles need special handling
4276         if console_type == 'pty':
4277             console_log = self._get_console_log_path(instance)
4278             data = nova.privsep.libvirt.readpty(console_path)
4279 
4280             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
4281             # which create a dedicated file device for the console logging.
4282             # Other virt_types like lxc and parallels depend on the flush of
4283             # that PTY device into the "console.log" file to ensure that a
4284             # series of "get_console_output" calls return the complete content
4285             # even after rebooting a guest.
4286             nova.privsep.path.writefile(console_log, 'a+', data)
4287 
4288             # set console path to logfile, not to pty device
4289             console_path = console_log
4290 
4291         # return logfile content
4292         return self._get_console_output_file(instance, console_path)
4293 
4294     def get_host_ip_addr(self):
4295         return CONF.my_ip
4296 
4297     def get_vnc_console(self, context, instance):
4298         def get_vnc_port_for_instance(instance_name):
4299             guest = self._host.get_guest(instance)
4300 
4301             xml = guest.get_xml_desc()
4302             xml_dom = etree.fromstring(xml)
4303 
4304             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
4305             if graphic is not None:
4306                 return graphic.get('port')
4307             # NOTE(rmk): We had VNC consoles enabled but the instance in
4308             # question is not actually listening for connections.
4309             raise exception.ConsoleTypeUnavailable(console_type='vnc')
4310 
4311         port = get_vnc_port_for_instance(instance.name)
4312         host = CONF.vnc.server_proxyclient_address
4313 
4314         return ctype.ConsoleVNC(host=host, port=port)
4315 
4316     def get_spice_console(self, context, instance):
4317         def get_spice_ports_for_instance(instance_name):
4318             guest = self._host.get_guest(instance)
4319 
4320             xml = guest.get_xml_desc()
4321             xml_dom = etree.fromstring(xml)
4322 
4323             graphic = xml_dom.find("./devices/graphics[@type='spice']")
4324             if graphic is not None:
4325                 return (graphic.get('port'), graphic.get('tlsPort'))
4326             # NOTE(rmk): We had Spice consoles enabled but the instance in
4327             # question is not actually listening for connections.
4328             raise exception.ConsoleTypeUnavailable(console_type='spice')
4329 
4330         ports = get_spice_ports_for_instance(instance.name)
4331         host = CONF.spice.server_proxyclient_address
4332 
4333         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
4334 
4335     def get_serial_console(self, context, instance):
4336         guest = self._host.get_guest(instance)
4337         for hostname, port in self._get_serial_ports_from_guest(
4338                 guest, mode='bind'):
4339             return ctype.ConsoleSerial(host=hostname, port=port)
4340         raise exception.ConsoleTypeUnavailable(console_type='serial')
4341 
4342     @staticmethod
4343     def _create_ephemeral(target, ephemeral_size,
4344                           fs_label, os_type, is_block_dev=False,
4345                           context=None, specified_fs=None,
4346                           vm_mode=None):
4347         if not is_block_dev:
4348             if (CONF.libvirt.virt_type == "parallels" and
4349                     vm_mode == fields.VMMode.EXE):
4350 
4351                 libvirt_utils.create_ploop_image('expanded', target,
4352                                                  '%dG' % ephemeral_size,
4353                                                  specified_fs)
4354                 return
4355             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
4356 
4357         # Run as root only for block devices.
4358         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
4359                       specified_fs=specified_fs)
4360 
4361     @staticmethod
4362     def _create_swap(target, swap_mb, context=None):
4363         """Create a swap file of specified size."""
4364         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
4365         nova.privsep.fs.unprivileged_mkfs('swap', target)
4366 
4367     @staticmethod
4368     def _get_console_log_path(instance):
4369         return os.path.join(libvirt_utils.get_instance_path(instance),
4370                             'console.log')
4371 
4372     def _ensure_console_log_for_instance(self, instance):
4373         # NOTE(mdbooth): Although libvirt will create this file for us
4374         # automatically when it starts, it will initially create it with
4375         # root ownership and then chown it depending on the configuration of
4376         # the domain it is launching. Quobyte CI explicitly disables the
4377         # chown by setting dynamic_ownership=0 in libvirt's config.
4378         # Consequently when the domain starts it is unable to write to its
4379         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
4380         #
4381         # To work around this, we create the file manually before starting
4382         # the domain so it has the same ownership as Nova. This works
4383         # for Quobyte CI because it is also configured to run qemu as the same
4384         # user as the Nova service. Installations which don't set
4385         # dynamic_ownership=0 are not affected because libvirt will always
4386         # correctly configure permissions regardless of initial ownership.
4387         #
4388         # Setting dynamic_ownership=0 is dubious and potentially broken in
4389         # more ways than console.log (see comment #22 on the above bug), so
4390         # Future Maintainer who finds this code problematic should check to see
4391         # if we still support it.
4392         console_file = self._get_console_log_path(instance)
4393         LOG.debug('Ensure instance console log exists: %s', console_file,
4394                   instance=instance)
4395         try:
4396             libvirt_utils.file_open(console_file, 'a').close()
4397         # NOTE(sfinucan): We can safely ignore permission issues here and
4398         # assume that it is libvirt that has taken ownership of this file.
4399         except IOError as ex:
4400             if ex.errno != errno.EACCES:
4401                 raise
4402             LOG.debug('Console file already exists: %s.', console_file)
4403 
4404     @staticmethod
4405     def _get_disk_config_image_type():
4406         # TODO(mikal): there is a bug here if images_type has
4407         # changed since creation of the instance, but I am pretty
4408         # sure that this bug already exists.
4409         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
4410 
4411     @staticmethod
4412     def _is_booted_from_volume(block_device_info):
4413         """Determines whether the VM is booting from volume
4414 
4415         Determines whether the block device info indicates that the VM
4416         is booting from a volume.
4417         """
4418         block_device_mapping = driver.block_device_info_get_mapping(
4419             block_device_info)
4420         return bool(block_device.get_root_bdm(block_device_mapping))
4421 
4422     def _inject_data(self, disk, instance, injection_info):
4423         """Injects data in a disk image
4424 
4425         Helper used for injecting data in a disk image file system.
4426 
4427         :param disk: The disk we're injecting into (an Image object)
4428         :param instance: The instance we're injecting into
4429         :param injection_info: Injection info
4430         """
4431         # Handles the partition need to be used.
4432         LOG.debug('Checking root disk injection %s',
4433                   str(injection_info), instance=instance)
4434         target_partition = None
4435         if not instance.kernel_id:
4436             target_partition = CONF.libvirt.inject_partition
4437             if target_partition == 0:
4438                 target_partition = None
4439         if CONF.libvirt.virt_type == 'lxc':
4440             target_partition = None
4441 
4442         # Handles the key injection.
4443         key = None
4444         if CONF.libvirt.inject_key and instance.get('key_data'):
4445             key = str(instance.key_data)
4446 
4447         # Handles the admin password injection.
4448         admin_pass = None
4449         if CONF.libvirt.inject_password:
4450             admin_pass = injection_info.admin_pass
4451 
4452         # Handles the network injection.
4453         net = netutils.get_injected_network_template(
4454             injection_info.network_info,
4455             libvirt_virt_type=CONF.libvirt.virt_type)
4456 
4457         # Handles the metadata injection
4458         metadata = instance.get('metadata')
4459 
4460         if any((key, net, metadata, admin_pass, injection_info.files)):
4461             LOG.debug('Injecting %s', str(injection_info),
4462                       instance=instance)
4463             img_id = instance.image_ref
4464             try:
4465                 disk_api.inject_data(disk.get_model(self._conn),
4466                                      key, net, metadata, admin_pass,
4467                                      injection_info.files,
4468                                      partition=target_partition,
4469                                      mandatory=('files',))
4470             except Exception as e:
4471                 with excutils.save_and_reraise_exception():
4472                     LOG.error('Error injecting data into image '
4473                               '%(img_id)s (%(e)s)',
4474                               {'img_id': img_id, 'e': e},
4475                               instance=instance)
4476 
4477     # NOTE(sileht): many callers of this method assume that this
4478     # method doesn't fail if an image already exists but instead
4479     # think that it will be reused (ie: (live)-migration/resize)
4480     def _create_image(self, context, instance,
4481                       disk_mapping, injection_info=None, suffix='',
4482                       disk_images=None, block_device_info=None,
4483                       fallback_from_host=None,
4484                       ignore_bdi_for_swap=False):
4485         booted_from_volume = self._is_booted_from_volume(block_device_info)
4486 
4487         def image(fname, image_type=CONF.libvirt.images_type):
4488             return self.image_backend.by_name(instance,
4489                                               fname + suffix, image_type)
4490 
4491         def raw(fname):
4492             return image(fname, image_type='raw')
4493 
4494         created_instance_dir = True
4495 
4496         # ensure directories exist and are writable
4497         instance_dir = libvirt_utils.get_instance_path(instance)
4498         if os.path.exists(instance_dir):
4499             LOG.debug("Instance directory exists: not creating",
4500                       instance=instance)
4501             created_instance_dir = False
4502         else:
4503             LOG.debug("Creating instance directory", instance=instance)
4504             fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
4505 
4506         LOG.info('Creating image', instance=instance)
4507 
4508         inst_type = instance.get_flavor()
4509         swap_mb = 0
4510         if 'disk.swap' in disk_mapping:
4511             mapping = disk_mapping['disk.swap']
4512 
4513             if ignore_bdi_for_swap:
4514                 # This is a workaround to support legacy swap resizing,
4515                 # which does not touch swap size specified in bdm,
4516                 # but works with flavor specified size only.
4517                 # In this case we follow the legacy logic and ignore block
4518                 # device info completely.
4519                 # NOTE(ft): This workaround must be removed when a correct
4520                 # implementation of resize operation changing sizes in bdms is
4521                 # developed. Also at that stage we probably may get rid of
4522                 # the direct usage of flavor swap size here,
4523                 # leaving the work with bdm only.
4524                 swap_mb = inst_type['swap']
4525             else:
4526                 swap = driver.block_device_info_get_swap(block_device_info)
4527                 if driver.swap_is_usable(swap):
4528                     swap_mb = swap['swap_size']
4529                 elif (inst_type['swap'] > 0 and
4530                       not block_device.volume_in_mapping(
4531                         mapping['dev'], block_device_info)):
4532                     swap_mb = inst_type['swap']
4533 
4534             if swap_mb > 0:
4535                 if (CONF.libvirt.virt_type == "parallels" and
4536                         instance.vm_mode == fields.VMMode.EXE):
4537                     msg = _("Swap disk is not supported "
4538                             "for Virtuozzo container")
4539                     raise exception.Invalid(msg)
4540 
4541         if not disk_images:
4542             disk_images = {'image_id': instance.image_ref,
4543                            'kernel_id': instance.kernel_id,
4544                            'ramdisk_id': instance.ramdisk_id}
4545 
4546         # NOTE(mdbooth): kernel and ramdisk, if they are defined, are hardcoded
4547         # to use raw, which means they will always be cleaned up with the
4548         # instance directory. We must not consider them for created_disks,
4549         # which may not be using the instance directory.
4550         if disk_images['kernel_id']:
4551             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
4552             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
4553                                 context=context,
4554                                 filename=fname,
4555                                 image_id=disk_images['kernel_id'])
4556             if disk_images['ramdisk_id']:
4557                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
4558                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
4559                                      context=context,
4560                                      filename=fname,
4561                                      image_id=disk_images['ramdisk_id'])
4562 
4563         created_disks = self._create_and_inject_local_root(
4564             context, instance, booted_from_volume, suffix, disk_images,
4565             injection_info, fallback_from_host)
4566 
4567         # Lookup the filesystem type if required
4568         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
4569             instance.os_type)
4570         # Generate a file extension based on the file system
4571         # type and the mkfs commands configured if any
4572         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
4573             os_type_with_default, CONF.default_ephemeral_format)
4574 
4575         vm_mode = fields.VMMode.get_from_instance(instance)
4576         ephemeral_gb = instance.flavor.ephemeral_gb
4577         if 'disk.local' in disk_mapping:
4578             disk_image = image('disk.local')
4579             # Short circuit the exists() tests if we already created a disk
4580             created_disks = created_disks or not disk_image.exists()
4581 
4582             fn = functools.partial(self._create_ephemeral,
4583                                    fs_label='ephemeral0',
4584                                    os_type=instance.os_type,
4585                                    is_block_dev=disk_image.is_block_dev,
4586                                    vm_mode=vm_mode)
4587             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
4588             size = ephemeral_gb * units.Gi
4589             disk_image.cache(fetch_func=fn,
4590                              context=context,
4591                              filename=fname,
4592                              size=size,
4593                              ephemeral_size=ephemeral_gb)
4594 
4595         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
4596                 block_device_info)):
4597             disk_image = image(blockinfo.get_eph_disk(idx))
4598             # Short circuit the exists() tests if we already created a disk
4599             created_disks = created_disks or not disk_image.exists()
4600 
4601             specified_fs = eph.get('guest_format')
4602             if specified_fs and not self.is_supported_fs_format(specified_fs):
4603                 msg = _("%s format is not supported") % specified_fs
4604                 raise exception.InvalidBDMFormat(details=msg)
4605 
4606             fn = functools.partial(self._create_ephemeral,
4607                                    fs_label='ephemeral%d' % idx,
4608                                    os_type=instance.os_type,
4609                                    is_block_dev=disk_image.is_block_dev,
4610                                    vm_mode=vm_mode)
4611             size = eph['size'] * units.Gi
4612             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
4613             disk_image.cache(fetch_func=fn,
4614                              context=context,
4615                              filename=fname,
4616                              size=size,
4617                              ephemeral_size=eph['size'],
4618                              specified_fs=specified_fs)
4619 
4620         if swap_mb > 0:
4621             size = swap_mb * units.Mi
4622             swap = image('disk.swap')
4623             # Short circuit the exists() tests if we already created a disk
4624             created_disks = created_disks or not swap.exists()
4625             swap.cache(fetch_func=self._create_swap, context=context,
4626                        filename="swap_%s" % swap_mb,
4627                        size=size, swap_mb=swap_mb)
4628 
4629         if created_disks:
4630             LOG.debug('Created local disks', instance=instance)
4631         else:
4632             LOG.debug('Did not create local disks', instance=instance)
4633 
4634         return (created_instance_dir, created_disks)
4635 
4636     def _create_and_inject_local_root(self, context, instance,
4637                                       booted_from_volume, suffix, disk_images,
4638                                       injection_info, fallback_from_host):
4639         created_disks = False
4640 
4641         # File injection only if needed
4642         need_inject = (not configdrive.required_by(instance) and
4643                        injection_info is not None and
4644                        CONF.libvirt.inject_partition != -2)
4645 
4646         # NOTE(ndipanov): Even if disk_mapping was passed in, which
4647         # currently happens only on rescue - we still don't want to
4648         # create a base image.
4649         if not booted_from_volume:
4650             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
4651             size = instance.flavor.root_gb * units.Gi
4652 
4653             if size == 0 or suffix == '.rescue':
4654                 size = None
4655 
4656             backend = self.image_backend.by_name(instance, 'disk' + suffix,
4657                                                  CONF.libvirt.images_type)
4658             created_disks = not backend.exists()
4659 
4660             if instance.task_state == task_states.RESIZE_FINISH:
4661                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
4662             if backend.SUPPORTS_CLONE:
4663                 def clone_fallback_to_fetch(
4664                     context, target, image_id, trusted_certs=None,
4665                 ):
4666                     refuse_fetch = (
4667                         CONF.libvirt.images_type == 'rbd' and
4668                         CONF.workarounds.never_download_image_if_on_rbd)
4669                     try:
4670                         backend.clone(context, disk_images['image_id'])
4671                     except exception.ImageUnacceptable:
4672                         if refuse_fetch:
4673                             # Re-raise the exception from the failed
4674                             # ceph clone.  The compute manager expects
4675                             # ImageUnacceptable as a possible result
4676                             # of spawn(), from which this is called.
4677                             with excutils.save_and_reraise_exception():
4678                                 LOG.warning(
4679                                     'Image %s is not on my ceph and '
4680                                     '[workarounds]/'
4681                                     'never_download_image_if_on_rbd=True;'
4682                                     ' refusing to fetch and upload.',
4683                                     disk_images['image_id'])
4684                         libvirt_utils.fetch_image(
4685                             context, target, image_id, trusted_certs,
4686                         )
4687                 fetch_func = clone_fallback_to_fetch
4688             else:
4689                 fetch_func = libvirt_utils.fetch_image
4690 
4691             self._try_fetch_image_cache(backend, fetch_func, context,
4692                                         root_fname, disk_images['image_id'],
4693                                         instance, size, fallback_from_host)
4694 
4695             # During unshelve or cross cell resize on Qcow2 backend, we spawn()
4696             # using a snapshot image. Extra work is needed in order to rebase
4697             # disk image to its original image_ref. Disk backing file will
4698             # then represent back image_ref instead of snapshot image.
4699             self._rebase_original_qcow2_image(context, instance, backend)
4700 
4701             if need_inject:
4702                 self._inject_data(backend, instance, injection_info)
4703 
4704         elif need_inject:
4705             LOG.warning('File injection into a boot from volume '
4706                         'instance is not supported', instance=instance)
4707 
4708         return created_disks
4709 
4710     def _needs_rebase_original_qcow2_image(self, instance, backend):
4711         if not isinstance(backend, imagebackend.Qcow2):
4712             return False
4713         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
4714             return True
4715         if instance.task_state == task_states.RESIZE_FINISH:
4716             # We need to distinguish between local versus cross cell resize.
4717             # Rebase is only needed in cross cell case because instance
4718             # is spawn from a snapshot.
4719             base_image_ref = instance.system_metadata.get(
4720                     'image_base_image_ref')
4721             if base_image_ref != instance.image_ref:
4722                 return True
4723         return False
4724 
4725     def _rebase_original_qcow2_image(self, context, instance, backend):
4726         # NOTE(aarents): During qcow2 instance unshelve/cross_cell_resize,
4727         # backing file represents a snapshot image, not original
4728         # instance.image_ref. We rebase here instance disk to original image.
4729         # This second fetch call does nothing except downloading original
4730         # backing file if missing, as image disk have already been
4731         # created/resized by first fetch call.
4732 
4733         if not self._needs_rebase_original_qcow2_image(instance, backend):
4734             return
4735 
4736         base_dir = self.image_cache_manager.cache_dir
4737         base_image_ref = instance.system_metadata.get('image_base_image_ref')
4738         root_fname = imagecache.get_cache_fname(base_image_ref)
4739         base_backing_fname = os.path.join(base_dir, root_fname)
4740 
4741         try:
4742             self._try_fetch_image_cache(backend, libvirt_utils.fetch_image,
4743                                         context, root_fname, base_image_ref,
4744                                         instance, None)
4745         except exception.ImageNotFound:
4746             # We must flatten here in order to remove dependency with an orphan
4747             # backing file (as snapshot image will be dropped once
4748             # unshelve/cross_cell_resize is successfull).
4749             LOG.warning('Current disk image is created on top of a snapshot '
4750                         'image and cannot be rebased to original image '
4751                         'because it is no longer available in the image '
4752                         'service, disk will be consequently flattened.',
4753                         instance=instance)
4754             base_backing_fname = None
4755 
4756         LOG.info('Rebasing disk image.', instance=instance)
4757         self._rebase_with_qemu_img(backend.path, base_backing_fname)
4758 
4759     def _create_configdrive(self, context, instance, injection_info,
4760                             rescue=False):
4761         # As this method being called right after the definition of a
4762         # domain, but before its actual launch, device metadata will be built
4763         # and saved in the instance for it to be used by the config drive and
4764         # the metadata service.
4765         instance.device_metadata = self._build_device_metadata(context,
4766                                                                instance)
4767         if configdrive.required_by(instance):
4768             LOG.info('Using config drive', instance=instance)
4769 
4770             name = 'disk.config'
4771             if rescue:
4772                 name += '.rescue'
4773 
4774             config_disk = self.image_backend.by_name(
4775                 instance, name, self._get_disk_config_image_type())
4776 
4777             # Don't overwrite an existing config drive
4778             if not config_disk.exists():
4779                 extra_md = {}
4780                 if injection_info.admin_pass:
4781                     extra_md['admin_pass'] = injection_info.admin_pass
4782 
4783                 inst_md = instance_metadata.InstanceMetadata(
4784                     instance, content=injection_info.files, extra_md=extra_md,
4785                     network_info=injection_info.network_info)
4786 
4787                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
4788                 with cdb:
4789                     # NOTE(mdbooth): We're hardcoding here the path of the
4790                     # config disk when using the flat backend. This isn't
4791                     # good, but it's required because we need a local path we
4792                     # know we can write to in case we're subsequently
4793                     # importing into rbd. This will be cleaned up when we
4794                     # replace this with a call to create_from_func, but that
4795                     # can't happen until we've updated the backends and we
4796                     # teach them not to cache config disks. This isn't
4797                     # possible while we're still using cache() under the hood.
4798                     config_disk_local_path = os.path.join(
4799                         libvirt_utils.get_instance_path(instance), name)
4800                     LOG.info('Creating config drive at %(path)s',
4801                              {'path': config_disk_local_path},
4802                              instance=instance)
4803 
4804                     try:
4805                         cdb.make_drive(config_disk_local_path)
4806                     except processutils.ProcessExecutionError as e:
4807                         with excutils.save_and_reraise_exception():
4808                             LOG.error('Creating config drive failed with '
4809                                       'error: %s', e, instance=instance)
4810 
4811                 try:
4812                     config_disk.import_file(
4813                         instance, config_disk_local_path, name)
4814                 finally:
4815                     # NOTE(mikal): if the config drive was imported into RBD,
4816                     # then we no longer need the local copy
4817                     if CONF.libvirt.images_type == 'rbd':
4818                         LOG.info('Deleting local config drive %(path)s '
4819                                  'because it was imported into RBD.',
4820                                  {'path': config_disk_local_path},
4821                                  instance=instance)
4822                         os.unlink(config_disk_local_path)
4823 
4824     def _detach_pci_devices(self, guest, pci_devs):
4825         try:
4826             for dev in pci_devs:
4827                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
4828                 # after detachDeviceFlags returned, we should check the dom to
4829                 # ensure the detaching is finished
4830                 xml = guest.get_xml_desc()
4831                 xml_doc = etree.fromstring(xml)
4832                 guest_config = vconfig.LibvirtConfigGuest()
4833                 guest_config.parse_dom(xml_doc)
4834 
4835                 for hdev in [d for d in guest_config.devices
4836                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
4837                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
4838                     dbsf = pci_utils.parse_address(dev.address)
4839                     if [int(x, 16) for x in hdbsf] ==\
4840                             [int(x, 16) for x in dbsf]:
4841                         raise exception.PciDeviceDetachFailed(reason=
4842                                                               "timeout",
4843                                                               dev=dev)
4844 
4845         except libvirt.libvirtError as ex:
4846             error_code = ex.get_error_code()
4847             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
4848                 LOG.warning("Instance disappeared while detaching "
4849                             "a PCI device from it.")
4850             else:
4851                 raise
4852 
4853     def _attach_pci_devices(self, guest, pci_devs):
4854         try:
4855             for dev in pci_devs:
4856                 guest.attach_device(self._get_guest_pci_device(dev))
4857 
4858         except libvirt.libvirtError:
4859             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
4860                       {'dev': pci_devs, 'dom': guest.id})
4861             raise
4862 
4863     @staticmethod
4864     def _has_direct_passthrough_port(network_info):
4865         for vif in network_info:
4866             if (vif['vnic_type'] in
4867                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
4868                 return True
4869         return False
4870 
4871     def _attach_direct_passthrough_ports(
4872         self, context, instance, guest, network_info=None):
4873         if network_info is None:
4874             network_info = instance.info_cache.network_info
4875         if network_info is None:
4876             return
4877 
4878         if self._has_direct_passthrough_port(network_info):
4879             for vif in network_info:
4880                 if (vif['vnic_type'] in
4881                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
4882                     cfg = self.vif_driver.get_config(instance,
4883                                                      vif,
4884                                                      instance.image_meta,
4885                                                      instance.flavor,
4886                                                      CONF.libvirt.virt_type)
4887                     LOG.debug('Attaching direct passthrough port %(port)s '
4888                               'to %(dom)s', {'port': vif, 'dom': guest.id},
4889                               instance=instance)
4890                     guest.attach_device(cfg)
4891 
4892     def _detach_direct_passthrough_ports(self, context, instance, guest):
4893         network_info = instance.info_cache.network_info
4894         if network_info is None:
4895             return
4896 
4897         if self._has_direct_passthrough_port(network_info):
4898             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
4899             # pci request per direct passthrough port. Therefore we can trust
4900             # that pci_slot value in the vif is correct.
4901             direct_passthrough_pci_addresses = [
4902                 vif['profile']['pci_slot']
4903                 for vif in network_info
4904                 if (vif['vnic_type'] in
4905                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
4906                     vif['profile'].get('pci_slot') is not None)
4907             ]
4908 
4909             # use detach_pci_devices to avoid failure in case of
4910             # multiple guest direct passthrough ports with the same MAC
4911             # (protection use-case, ports are on different physical
4912             # interfaces)
4913             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
4914             direct_passthrough_pci_addresses = (
4915                 [pci_dev for pci_dev in pci_devs
4916                  if pci_dev.address in direct_passthrough_pci_addresses])
4917             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
4918 
4919     def _update_compute_provider_status(self, context, service):
4920         """Calls the ComputeVirtAPI.update_compute_provider_status method
4921 
4922         :param context: nova auth RequestContext
4923         :param service: nova.objects.Service record for this host which is
4924             expected to only manage a single ComputeNode
4925         """
4926         rp_uuid = None
4927         try:
4928             rp_uuid = service.compute_node.uuid
4929             self.virtapi.update_compute_provider_status(
4930                 context, rp_uuid, enabled=not service.disabled)
4931         except Exception:
4932             # This is best effort so just log the exception but don't fail.
4933             # The update_available_resource periodic task will sync the trait.
4934             LOG.warning(
4935                 'An error occurred while updating compute node '
4936                 'resource provider status to "%s" for provider: %s',
4937                 'disabled' if service.disabled else 'enabled',
4938                 rp_uuid or service.host, exc_info=True)
4939 
4940     def _set_host_enabled(self, enabled,
4941                           disable_reason=DISABLE_REASON_UNDEFINED):
4942         """Enables / Disables the compute service on this host.
4943 
4944            This doesn't override non-automatic disablement with an automatic
4945            setting; thereby permitting operators to keep otherwise
4946            healthy hosts out of rotation.
4947         """
4948 
4949         status_name = {True: 'disabled',
4950                        False: 'enabled'}
4951 
4952         disable_service = not enabled
4953 
4954         ctx = nova_context.get_admin_context()
4955         try:
4956             service = objects.Service.get_by_compute_host(ctx, CONF.host)
4957 
4958             if service.disabled != disable_service:
4959                 # Note(jang): this is a quick fix to stop operator-
4960                 # disabled compute hosts from re-enabling themselves
4961                 # automatically. We prefix any automatic reason code
4962                 # with a fixed string. We only re-enable a host
4963                 # automatically if we find that string in place.
4964                 # This should probably be replaced with a separate flag.
4965                 if not service.disabled or (
4966                         service.disabled_reason and
4967                         service.disabled_reason.startswith(DISABLE_PREFIX)):
4968                     service.disabled = disable_service
4969                     service.disabled_reason = (
4970                        DISABLE_PREFIX + disable_reason
4971                        if disable_service and disable_reason else
4972                            DISABLE_REASON_UNDEFINED)
4973                     service.save()
4974                     LOG.debug('Updating compute service status to %s',
4975                               status_name[disable_service])
4976                     # Update the disabled trait status on the corresponding
4977                     # compute node resource provider in placement.
4978                     self._update_compute_provider_status(ctx, service)
4979                 else:
4980                     LOG.debug('Not overriding manual compute service '
4981                               'status with: %s',
4982                               status_name[disable_service])
4983         except exception.ComputeHostNotFound:
4984             LOG.warning('Cannot update service status on host "%s" '
4985                         'since it is not registered.', CONF.host)
4986         except Exception:
4987             LOG.warning('Cannot update service status on host "%s" '
4988                         'due to an unexpected exception.', CONF.host,
4989                         exc_info=True)
4990 
4991         if enabled:
4992             mount.get_manager().host_up(self._host)
4993         else:
4994             mount.get_manager().host_down()
4995 
4996     def _get_cpu_model_mapping(self, model):
4997         """Get the CPU model mapping
4998 
4999         The CPU models which admin configured are case-insensitive, libvirt is
5000         case-sensitive, therefore build a mapping to get the correct CPU model
5001         name.
5002 
5003         :param model: Case-insensitive CPU model name.
5004         :return: It will validate and return the case-sensitive CPU model name
5005                  if on a supported platform, otherwise it will just return
5006                  what was provided
5007         :raises: exception.InvalidCPUInfo if the CPU model is not supported.
5008         """
5009         cpu_info = self._get_cpu_info()
5010         if cpu_info['arch'] not in (fields.Architecture.I686,
5011                                     fields.Architecture.X86_64,
5012                                     fields.Architecture.PPC64,
5013                                     fields.Architecture.PPC64LE,
5014                                     fields.Architecture.PPC):
5015             return model
5016 
5017         if not self.cpu_models_mapping:
5018             cpu_models = self._host.get_cpu_model_names()
5019             for cpu_model in cpu_models:
5020                 self.cpu_models_mapping[cpu_model.lower()] = cpu_model
5021 
5022         if model.lower() not in self.cpu_models_mapping:
5023             msg = (_("Configured CPU model: %(model)s is not correct, "
5024                      "or your host CPU arch does not support this "
5025                      "model. Please correct your config and try "
5026                      "again.") % {'model': model})
5027             raise exception.InvalidCPUInfo(msg)
5028 
5029         return self.cpu_models_mapping.get(model.lower())
5030 
5031     # TODO(stephenfin): Libvirt exposes information about possible CPU models
5032     # via 'getDomainCapabilities' and we should use it
5033     def _get_guest_cpu_model_config(self, flavor=None, arch=None):
5034         mode = CONF.libvirt.cpu_mode
5035         models = [self._get_cpu_model_mapping(model)
5036                   for model in CONF.libvirt.cpu_models]
5037         extra_flags = set([flag.lower() for flag in
5038             CONF.libvirt.cpu_model_extra_flags])
5039 
5040         if not arch:
5041             caps = self._host.get_capabilities()
5042             arch = caps.host.cpu.arch
5043 
5044         if (
5045             CONF.libvirt.virt_type == "kvm" or
5046             CONF.libvirt.virt_type == "qemu"
5047         ):
5048             if mode is None:
5049                 # AArch64 lacks 'host-model' support because neither libvirt
5050                 # nor QEMU are able to tell what the host CPU model exactly is.
5051                 # And there is no CPU description code for ARM(64) at this
5052                 # point.
5053 
5054                 # Also worth noting: 'host-passthrough' mode will completely
5055                 # break live migration, *unless* all the Compute nodes (running
5056                 # libvirtd) have *identical* CPUs.
5057                 if arch == fields.Architecture.AARCH64:
5058                     mode = "host-passthrough"
5059                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
5060                              'migration can break unless all compute nodes '
5061                              'have identical cpus. AArch64 does not support '
5062                              'other modes.')
5063                 else:
5064                     mode = "host-model"
5065             if mode == "none":
5066                 return vconfig.LibvirtConfigGuestCPU()
5067             # On AArch64 platform the return of _get_cpu_model_mapping will not
5068             # return the default CPU model.
5069             if mode == "custom":
5070                 if arch == fields.Architecture.AARCH64:
5071                     if not models:
5072                         models = ['max']
5073 
5074         else:
5075             if mode is None or mode == "none":
5076                 return None
5077 
5078         cpu = vconfig.LibvirtConfigGuestCPU()
5079         cpu.mode = mode
5080         cpu.model = models[0] if models else None
5081 
5082         # compare flavor trait and cpu models, select the first mathched model
5083         if flavor and mode == "custom":
5084             flags = libvirt_utils.get_flags_by_flavor_specs(flavor)
5085             if flags:
5086                 cpu.model = self._match_cpu_model_by_flags(models, flags)
5087 
5088         LOG.debug("CPU mode '%(mode)s' models '%(models)s' was chosen, "
5089                   "with extra flags: '%(extra_flags)s'",
5090                   {'mode': mode,
5091                    'models': (cpu.model or ""),
5092                    'extra_flags': (extra_flags or "")})
5093 
5094         # NOTE (kchamart): Currently there's no existing way to ask if a
5095         # given CPU model + CPU flags combination is supported by KVM &
5096         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
5097         # command upfront -- before even a Nova instance (a QEMU
5098         # process) is launched -- to construct CPU models and check
5099         # their validity; so we are good there.  In the long-term,
5100         # upstream libvirt intends to add an additional new API that can
5101         # do fine-grained validation of a certain CPU model + CPU flags
5102         # against a specific QEMU binary (the libvirt RFE bug for that:
5103         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
5104         #
5105         # NOTE(kchamart) Similar to what was done in
5106         # _check_cpu_compatibility(), the below parses a comma-separated
5107         # list of CPU flags from `[libvirt]cpu_model_extra_flags` and
5108         # will selectively enable or disable a given CPU flag for the
5109         # guest, before it is launched by Nova.
5110         for flag in extra_flags:
5111             cpu_feature = self._prepare_cpu_flag(flag)
5112             cpu.add_feature(cpu_feature)
5113         return cpu
5114 
5115     def _match_cpu_model_by_flags(self, models, flags):
5116         for model in models:
5117             if flags.issubset(self.cpu_model_flag_mapping.get(model, set([]))):
5118                 return model
5119             cpu = vconfig.LibvirtConfigCPU()
5120             cpu.arch = self._host.get_capabilities().host.cpu.arch
5121             cpu.model = model
5122             features_xml = self._get_guest_baseline_cpu_features(cpu.to_xml())
5123             if features_xml:
5124                 cpu.parse_str(features_xml)
5125                 feature_names = [f.name for f in cpu.features]
5126                 self.cpu_model_flag_mapping[model] = feature_names
5127                 if flags.issubset(feature_names):
5128                     return model
5129 
5130         msg = ('No CPU model match traits, models: {models}, required '
5131                'flags: {flags}'.format(models=models, flags=flags))
5132         raise exception.InvalidCPUInfo(msg)
5133 
5134     def _get_guest_cpu_config(self, flavor, image_meta,
5135                               guest_cpu_numa_config, instance_numa_topology):
5136         arch = libvirt_utils.get_arch(image_meta)
5137         cpu = self._get_guest_cpu_model_config(flavor, arch)
5138 
5139         if cpu is None:
5140             return None
5141 
5142         topology = hardware.get_best_cpu_topology(
5143                 flavor, image_meta, numa_topology=instance_numa_topology)
5144 
5145         cpu.sockets = topology.sockets
5146         cpu.cores = topology.cores
5147         cpu.threads = topology.threads
5148         cpu.numa = guest_cpu_numa_config
5149 
5150         return cpu
5151 
5152     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
5153                                image_type=None, boot_order=None):
5154         disk_unit = None
5155         disk = self.image_backend.by_name(instance, name, image_type)
5156         if (name == 'disk.config' and image_type == 'rbd' and
5157                 not disk.exists()):
5158             # This is likely an older config drive that has not been migrated
5159             # to rbd yet. Try to fall back on 'flat' image type.
5160             # TODO(melwitt): Add online migration of some sort so we can
5161             # remove this fall back once we know all config drives are in rbd.
5162             # NOTE(vladikr): make sure that the flat image exist, otherwise
5163             # the image will be created after the domain definition.
5164             flat_disk = self.image_backend.by_name(instance, name, 'flat')
5165             if flat_disk.exists():
5166                 disk = flat_disk
5167                 LOG.debug('Config drive not found in RBD, falling back to the '
5168                           'instance directory', instance=instance)
5169         disk_info = disk_mapping[name]
5170         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
5171             disk_unit = disk_mapping['unit']
5172             disk_mapping['unit'] += 1  # Increments for the next disk added
5173         conf = disk.libvirt_info(disk_info, self.disk_cachemode,
5174                                  inst_type['extra_specs'],
5175                                  disk_unit=disk_unit,
5176                                  boot_order=boot_order)
5177         return conf
5178 
5179     def _get_guest_fs_config(self, instance, name, image_type=None):
5180         disk = self.image_backend.by_name(instance, name, image_type)
5181         return disk.libvirt_fs_info("/", "ploop")
5182 
5183     def _get_guest_storage_config(self, context, instance, image_meta,
5184                                   disk_info,
5185                                   rescue, block_device_info,
5186                                   inst_type, os_type):
5187         devices = []
5188         disk_mapping = disk_info['mapping']
5189 
5190         block_device_mapping = driver.block_device_info_get_mapping(
5191             block_device_info)
5192         mount_rootfs = CONF.libvirt.virt_type == "lxc"
5193         scsi_controller = self._get_scsi_controller(image_meta)
5194 
5195         if scsi_controller and scsi_controller.model == 'virtio-scsi':
5196             # The virtio-scsi can handle up to 256 devices but the
5197             # optional element "address" must be defined to describe
5198             # where the device is placed on the controller (see:
5199             # LibvirtConfigGuestDeviceAddressDrive).
5200             #
5201             # Note about why it's added in disk_mapping: It's not
5202             # possible to pass an 'int' by reference in Python, so we
5203             # use disk_mapping as container to keep reference of the
5204             # unit added and be able to increment it for each disk
5205             # added.
5206             #
5207             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
5208             # we need to start the disk mapping unit at 1 since we set the
5209             # bootable volume's unit to 0 for the bootable volume.
5210             disk_mapping['unit'] = 0
5211             if self._is_booted_from_volume(block_device_info):
5212                 disk_mapping['unit'] = 1
5213 
5214         def _get_ephemeral_devices():
5215             eph_devices = []
5216             for idx, eph in enumerate(
5217                 driver.block_device_info_get_ephemerals(
5218                     block_device_info)):
5219                 diskeph = self._get_guest_disk_config(
5220                     instance,
5221                     blockinfo.get_eph_disk(idx),
5222                     disk_mapping, inst_type)
5223                 eph_devices.append(diskeph)
5224             return eph_devices
5225 
5226         if mount_rootfs:
5227             fs = vconfig.LibvirtConfigGuestFilesys()
5228             fs.source_type = "mount"
5229             fs.source_dir = os.path.join(
5230                 libvirt_utils.get_instance_path(instance), 'rootfs')
5231             devices.append(fs)
5232         elif (os_type == fields.VMMode.EXE and
5233               CONF.libvirt.virt_type == "parallels"):
5234             if rescue:
5235                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
5236                 devices.append(fsrescue)
5237 
5238                 fsos = self._get_guest_fs_config(instance, "disk")
5239                 fsos.target_dir = "/mnt/rescue"
5240                 devices.append(fsos)
5241             else:
5242                 if 'disk' in disk_mapping:
5243                     fs = self._get_guest_fs_config(instance, "disk")
5244                     devices.append(fs)
5245                 devices = devices + _get_ephemeral_devices()
5246         else:
5247 
5248             if rescue and disk_mapping['disk.rescue'] == disk_mapping['root']:
5249                 diskrescue = self._get_guest_disk_config(instance,
5250                                                          'disk.rescue',
5251                                                          disk_mapping,
5252                                                          inst_type)
5253                 devices.append(diskrescue)
5254 
5255                 diskos = self._get_guest_disk_config(instance,
5256                                                      'disk',
5257                                                      disk_mapping,
5258                                                      inst_type)
5259                 devices.append(diskos)
5260             else:
5261                 if 'disk' in disk_mapping:
5262                     diskos = self._get_guest_disk_config(instance,
5263                                                          'disk',
5264                                                          disk_mapping,
5265                                                          inst_type)
5266                     devices.append(diskos)
5267 
5268                 if 'disk.local' in disk_mapping:
5269                     disklocal = self._get_guest_disk_config(instance,
5270                                                             'disk.local',
5271                                                             disk_mapping,
5272                                                             inst_type)
5273                     devices.append(disklocal)
5274                     instance.default_ephemeral_device = (
5275                         block_device.prepend_dev(disklocal.target_dev))
5276 
5277                 devices = devices + _get_ephemeral_devices()
5278 
5279                 if 'disk.swap' in disk_mapping:
5280                     diskswap = self._get_guest_disk_config(instance,
5281                                                            'disk.swap',
5282                                                            disk_mapping,
5283                                                            inst_type)
5284                     devices.append(diskswap)
5285                     instance.default_swap_device = (
5286                         block_device.prepend_dev(diskswap.target_dev))
5287 
5288             config_name = 'disk.config'
5289             if rescue and disk_mapping['disk.rescue'] == disk_mapping['root']:
5290                 config_name = 'disk.config.rescue'
5291 
5292             if config_name in disk_mapping:
5293                 diskconfig = self._get_guest_disk_config(
5294                     instance, config_name, disk_mapping, inst_type,
5295                     self._get_disk_config_image_type())
5296                 devices.append(diskconfig)
5297 
5298         for vol in block_device.get_bdms_to_connect(block_device_mapping,
5299                                                    mount_rootfs):
5300             connection_info = vol['connection_info']
5301             vol_dev = block_device.prepend_dev(vol['mount_device'])
5302             info = disk_mapping[vol_dev]
5303             self._connect_volume(context, connection_info, instance)
5304             if scsi_controller and scsi_controller.model == 'virtio-scsi':
5305                 # Check if this is the bootable volume when in a
5306                 # boot-from-volume instance, and if so, ensure the unit
5307                 # attribute is 0.
5308                 if vol.get('boot_index') == 0:
5309                     info['unit'] = 0
5310                 else:
5311                     info['unit'] = disk_mapping['unit']
5312                     disk_mapping['unit'] += 1
5313             cfg = self._get_volume_config(connection_info, info)
5314             devices.append(cfg)
5315             vol['connection_info'] = connection_info
5316             vol.save()
5317 
5318         for d in devices:
5319             self._set_cache_mode(d)
5320 
5321         if scsi_controller:
5322             devices.append(scsi_controller)
5323 
5324         if rescue and disk_mapping['disk.rescue'] != disk_mapping['root']:
5325             diskrescue = self._get_guest_disk_config(instance, 'disk.rescue',
5326                                                      disk_mapping, inst_type,
5327                                                      boot_order='1')
5328             devices.append(diskrescue)
5329 
5330         return devices
5331 
5332     @staticmethod
5333     def _get_scsi_controller(image_meta):
5334         """Return scsi controller or None based on image meta"""
5335         if image_meta.properties.get('hw_scsi_model'):
5336             hw_scsi_model = image_meta.properties.hw_scsi_model
5337             scsi_controller = vconfig.LibvirtConfigGuestController()
5338             scsi_controller.type = 'scsi'
5339             scsi_controller.model = hw_scsi_model
5340             scsi_controller.index = 0
5341             return scsi_controller
5342 
5343     def _get_host_sysinfo_serial_hardware(self):
5344         """Get a UUID from the host hardware
5345 
5346         Get a UUID for the host hardware reported by libvirt.
5347         This is typically from the SMBIOS data, unless it has
5348         been overridden in /etc/libvirt/libvirtd.conf
5349         """
5350         caps = self._host.get_capabilities()
5351         return caps.host.uuid
5352 
5353     def _get_host_sysinfo_serial_os(self):
5354         """Get a UUID from the host operating system
5355 
5356         Get a UUID for the host operating system. Modern Linux
5357         distros based on systemd provide a /etc/machine-id
5358         file containing a UUID. This is also provided inside
5359         systemd based containers and can be provided by other
5360         init systems too, since it is just a plain text file.
5361         """
5362         if not os.path.exists("/etc/machine-id"):
5363             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
5364             raise exception.InternalError(msg)
5365 
5366         with open("/etc/machine-id") as f:
5367             # We want to have '-' in the right place
5368             # so we parse & reformat the value
5369             lines = f.read().split()
5370             if not lines:
5371                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
5372                 raise exception.InternalError(msg)
5373 
5374             return str(uuid.UUID(lines[0]))
5375 
5376     def _get_host_sysinfo_serial_auto(self):
5377         if os.path.exists("/etc/machine-id"):
5378             return self._get_host_sysinfo_serial_os()
5379         else:
5380             return self._get_host_sysinfo_serial_hardware()
5381 
5382     def _get_guest_config_sysinfo(self, instance):
5383         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
5384 
5385         sysinfo.system_manufacturer = version.vendor_string()
5386         sysinfo.system_product = version.product_string()
5387         sysinfo.system_version = version.version_string_with_package()
5388 
5389         if CONF.libvirt.sysinfo_serial == 'unique':
5390             sysinfo.system_serial = instance.uuid
5391         else:
5392             sysinfo.system_serial = self._sysinfo_serial_func()
5393         sysinfo.system_uuid = instance.uuid
5394 
5395         sysinfo.system_family = "Virtual Machine"
5396 
5397         return sysinfo
5398 
5399     def _set_managed_mode(self, pcidev):
5400         # only kvm support managed mode
5401         if CONF.libvirt.virt_type in ('parallels',):
5402             pcidev.managed = 'no'
5403         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
5404             pcidev.managed = 'yes'
5405 
5406     def _get_guest_pci_device(self, pci_device):
5407 
5408         dbsf = pci_utils.parse_address(pci_device.address)
5409         dev = vconfig.LibvirtConfigGuestHostdevPCI()
5410         dev.domain, dev.bus, dev.slot, dev.function = dbsf
5411         self._set_managed_mode(dev)
5412 
5413         return dev
5414 
5415     def _get_guest_config_meta(self, instance, network_info):
5416         """Get metadata config for guest."""
5417 
5418         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
5419         meta.package = version.version_string_with_package()
5420         meta.name = instance.display_name
5421         meta.creationTime = time.time()
5422 
5423         if instance.image_ref not in ("", None):
5424             meta.roottype = "image"
5425             meta.rootid = instance.image_ref
5426 
5427         system_meta = instance.system_metadata
5428         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
5429         ometa.userid = instance.user_id
5430         ometa.username = system_meta.get('owner_user_name', 'N/A')
5431         ometa.projectid = instance.project_id
5432         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
5433         meta.owner = ometa
5434 
5435         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
5436         flavor = instance.flavor
5437         fmeta.name = flavor.name
5438         fmeta.memory = flavor.memory_mb
5439         fmeta.vcpus = flavor.vcpus
5440         fmeta.ephemeral = flavor.ephemeral_gb
5441         fmeta.disk = flavor.root_gb
5442         fmeta.swap = flavor.swap
5443 
5444         meta.flavor = fmeta
5445 
5446         ports = []
5447         for vif in network_info:
5448             ips = []
5449             for subnet in vif.get('network', {}).get('subnets', []):
5450                 for ip in subnet.get('ips', []):
5451                     ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(
5452                         ip.get('type'), ip.get('address'), ip.get('version')))
5453             ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(
5454                 vif.get('id'), ips=ips))
5455 
5456         meta.ports = vconfig.LibvirtConfigGuestMetaNovaPorts(ports)
5457 
5458         return meta
5459 
5460     @staticmethod
5461     def _create_idmaps(klass, map_strings):
5462         idmaps = []
5463         if len(map_strings) > 5:
5464             map_strings = map_strings[0:5]
5465             LOG.warning("Too many id maps, only included first five.")
5466         for map_string in map_strings:
5467             try:
5468                 idmap = klass()
5469                 values = [int(i) for i in map_string.split(":")]
5470                 idmap.start = values[0]
5471                 idmap.target = values[1]
5472                 idmap.count = values[2]
5473                 idmaps.append(idmap)
5474             except (ValueError, IndexError):
5475                 LOG.warning("Invalid value for id mapping %s", map_string)
5476         return idmaps
5477 
5478     def _get_guest_idmaps(self):
5479         id_maps: ty.List[vconfig.LibvirtConfigGuestIDMap] = []
5480         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
5481             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
5482                                            CONF.libvirt.uid_maps)
5483             id_maps.extend(uid_maps)
5484         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
5485             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
5486                                            CONF.libvirt.gid_maps)
5487             id_maps.extend(gid_maps)
5488         return id_maps
5489 
5490     def _update_guest_cputune(self, guest, flavor):
5491         is_able = self._host.is_cpu_control_policy_capable()
5492 
5493         cputuning = ['shares', 'period', 'quota']
5494         wants_cputune = any([k for k in cputuning
5495             if "quota:cpu_" + k in flavor.extra_specs.keys()])
5496 
5497         if wants_cputune and not is_able:
5498             raise exception.UnsupportedHostCPUControlPolicy()
5499 
5500         if not is_able or CONF.libvirt.virt_type not in ('lxc', 'kvm', 'qemu'):
5501             return
5502 
5503         if guest.cputune is None:
5504             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
5505             # Setting the default cpu.shares value to be a value
5506             # dependent on the number of vcpus
5507         guest.cputune.shares = 1024 * guest.vcpus
5508 
5509         for name in cputuning:
5510             key = "quota:cpu_" + name
5511             if key in flavor.extra_specs:
5512                 setattr(guest.cputune, name,
5513                         int(flavor.extra_specs[key]))
5514 
5515     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
5516                                            wants_hugepages):
5517         if instance_numa_topology:
5518             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
5519             for instance_cell in instance_numa_topology.cells:
5520                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
5521                 guest_cell.id = instance_cell.id
5522                 guest_cell.cpus = instance_cell.total_cpus
5523                 guest_cell.memory = instance_cell.memory * units.Ki
5524 
5525                 # The vhost-user network backend requires file backed
5526                 # guest memory (ie huge pages) to be marked as shared
5527                 # access, not private, so an external process can read
5528                 # and write the pages.
5529                 #
5530                 # You can't change the shared vs private flag for an
5531                 # already running guest, and since we can't predict what
5532                 # types of NIC may be hotplugged, we have no choice but
5533                 # to unconditionally turn on the shared flag. This has
5534                 # no real negative functional effect on the guest, so
5535                 # is a reasonable approach to take
5536                 if wants_hugepages:
5537                     guest_cell.memAccess = "shared"
5538                 guest_cpu_numa.cells.append(guest_cell)
5539             return guest_cpu_numa
5540 
5541     def _wants_hugepages(self, host_topology, instance_topology):
5542         """Determine if the guest / host topology implies the
5543            use of huge pages for guest RAM backing
5544         """
5545 
5546         if host_topology is None or instance_topology is None:
5547             return False
5548 
5549         avail_pagesize = [page.size_kb
5550                           for page in host_topology.cells[0].mempages]
5551         avail_pagesize.sort()
5552         # Remove smallest page size as that's not classed as a largepage
5553         avail_pagesize = avail_pagesize[1:]
5554 
5555         # See if we have page size set
5556         for cell in instance_topology.cells:
5557             if (cell.pagesize is not None and
5558                 cell.pagesize in avail_pagesize):
5559                 return True
5560 
5561         return False
5562 
5563     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
5564         """Returns the lists of pairs(tuple) of an instance cell and
5565         corresponding host cell:
5566             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
5567         """
5568         cell_pairs = []
5569         for guest_config_cell in guest_cpu_numa_config.cells:
5570             for host_cell in host_topology.cells:
5571                 if guest_config_cell.id == host_cell.id:
5572                     cell_pairs.append((guest_config_cell, host_cell))
5573         return cell_pairs
5574 
5575     def _get_pin_cpuset(self, vcpu, inst_cell, host_cell):
5576         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
5577 
5578         Prepares vcpupin config for the guest with the following caveats:
5579 
5580             a) If the specified instance vCPU is intended to be pinned, we pin
5581                it to the previously selected host CPU.
5582             b) Otherwise we float over the whole host NUMA node
5583         """
5584         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
5585         pin_cpuset.id = vcpu
5586 
5587         # 'InstanceNUMACell.cpu_pinning' tracks the CPU pinning pair for guest
5588         # CPU and host CPU. If the guest CPU is in the keys of 'cpu_pinning',
5589         # fetch the host CPU from it and pin on it, otherwise, let the guest
5590         # CPU be floating on the sharing CPU set belonging to this NUMA cell.
5591         if inst_cell.cpu_pinning and vcpu in inst_cell.cpu_pinning:
5592             pin_cpuset.cpuset = set([inst_cell.cpu_pinning[vcpu]])
5593         else:
5594             pin_cpuset.cpuset = host_cell.cpuset
5595 
5596         return pin_cpuset
5597 
5598     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
5599                                 emulator_threads_policy,
5600                                 pin_cpuset):
5601         """Returns a set of cpu_ids to add to the cpuset for emulator threads
5602            with the following caveats:
5603 
5604             a) If emulator threads policy is isolated, we pin emulator threads
5605                to one cpu we have reserved for it.
5606             b) If emulator threads policy is shared and CONF.cpu_shared_set is
5607                defined, we pin emulator threads on the set of pCPUs defined by
5608                CONF.cpu_shared_set
5609             c) Otherwise;
5610                 c1) If realtime IS NOT enabled, the emulator threads are
5611                     allowed to float cross all the pCPUs associated with
5612                     the guest vCPUs.
5613                 c2) If realtime IS enabled, at least 1 vCPU is required
5614                     to be set aside for non-realtime usage. The emulator
5615                     threads are allowed to float across the pCPUs that
5616                     are associated with the non-realtime VCPUs.
5617         """
5618         emulatorpin_cpuset = set([])
5619         shared_ids = hardware.get_cpu_shared_set()
5620 
5621         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
5622             if object_numa_cell.cpuset_reserved:
5623                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
5624         elif ((emulator_threads_policy ==
5625               fields.CPUEmulatorThreadsPolicy.SHARE) and
5626               shared_ids):
5627             online_pcpus = self._host.get_online_cpus()
5628             cpuset = shared_ids & online_pcpus
5629             if not cpuset:
5630                 msg = (_("Invalid cpu_shared_set config, one or more of the "
5631                          "specified cpuset is not online. Online cpuset(s): "
5632                          "%(online)s, requested cpuset(s): %(req)s"),
5633                        {'online': sorted(online_pcpus),
5634                         'req': sorted(shared_ids)})
5635                 raise exception.Invalid(msg)
5636             emulatorpin_cpuset = cpuset
5637         elif not vcpus_rt or vcpu not in vcpus_rt:
5638             emulatorpin_cpuset = pin_cpuset.cpuset
5639 
5640         return emulatorpin_cpuset
5641 
5642     def _get_guest_numa_config(self, instance_numa_topology, flavor,
5643                                image_meta):
5644         """Returns the config objects for the guest NUMA specs.
5645 
5646         Determines the CPUs that the guest can be pinned to if the guest
5647         specifies a cell topology and the host supports it. Constructs the
5648         libvirt XML config object representing the NUMA topology selected
5649         for the guest. Returns a tuple of:
5650 
5651             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
5652 
5653         With the following caveats:
5654 
5655             a) If there is no specified guest NUMA topology, then
5656                all tuple elements except cpu_set shall be None. cpu_set
5657                will be populated with the chosen CPUs that the guest
5658                allowed CPUs fit within.
5659 
5660             b) If there is a specified guest NUMA topology, then
5661                cpu_set will be None and guest_cpu_numa will be the
5662                LibvirtConfigGuestCPUNUMA object representing the guest's
5663                NUMA topology. If the host supports NUMA, then guest_cpu_tune
5664                will contain a LibvirtConfigGuestCPUTune object representing
5665                the optimized chosen cells that match the host capabilities
5666                with the instance's requested topology. If the host does
5667                not support NUMA, then guest_cpu_tune and guest_numa_tune
5668                will be None.
5669         """
5670 
5671         if (not self._has_numa_support() and
5672                 instance_numa_topology is not None):
5673             # We should not get here, since we should have avoided
5674             # reporting NUMA topology from _get_host_numa_topology
5675             # in the first place. Just in case of a scheduler
5676             # mess up though, raise an exception
5677             raise exception.NUMATopologyUnsupported()
5678 
5679         # We only pin an instance to some host cores if the user has provided
5680         # configuration to suggest we should.
5681         shared_cpus = None
5682         if CONF.vcpu_pin_set or CONF.compute.cpu_shared_set:
5683             shared_cpus = self._get_vcpu_available()
5684 
5685         topology = self._get_host_numa_topology()
5686 
5687         # We have instance NUMA so translate it to the config class
5688         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
5689                 instance_numa_topology,
5690                 self._wants_hugepages(topology, instance_numa_topology))
5691 
5692         if not guest_cpu_numa_config:
5693             # No NUMA topology defined for instance - let the host kernel deal
5694             # with the NUMA effects.
5695             # TODO(ndipanov): Attempt to spread the instance
5696             # across NUMA nodes and expose the topology to the
5697             # instance as an optimisation
5698             return GuestNumaConfig(shared_cpus, None, None, None)
5699 
5700         if not topology:
5701             # No NUMA topology defined for host - This will only happen with
5702             # some libvirt versions and certain platforms.
5703             return GuestNumaConfig(shared_cpus, None,
5704                                    guest_cpu_numa_config, None)
5705 
5706         # Now get configuration from the numa_topology
5707         # Init CPUTune configuration
5708         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
5709         guest_cpu_tune.emulatorpin = (
5710             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
5711         guest_cpu_tune.emulatorpin.cpuset = set([])
5712 
5713         # Init NUMATune configuration
5714         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
5715         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
5716         guest_numa_tune.memnodes = []
5717 
5718         emulator_threads_policy = None
5719         if 'emulator_threads_policy' in instance_numa_topology:
5720             emulator_threads_policy = (
5721                 instance_numa_topology.emulator_threads_policy)
5722 
5723         # Set realtime scheduler for CPUTune
5724         vcpus_rt = hardware.get_realtime_cpu_constraint(flavor, image_meta)
5725         if vcpus_rt:
5726             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
5727             designer.set_vcpu_realtime_scheduler(
5728                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
5729             guest_cpu_tune.vcpusched.append(vcpusched)
5730 
5731         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
5732         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
5733                 cell_pairs):
5734             # set NUMATune for the cell
5735             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
5736             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
5737             guest_numa_tune.memnodes.append(tnode)
5738             guest_numa_tune.memory.nodeset.append(host_cell.id)
5739 
5740             # set CPUTune for the cell
5741             object_numa_cell = instance_numa_topology.cells[guest_node_id]
5742             for cpu in guest_config_cell.cpus:
5743                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
5744                                                   host_cell)
5745                 guest_cpu_tune.vcpupin.append(pin_cpuset)
5746 
5747                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
5748                     cpu, object_numa_cell, vcpus_rt,
5749                     emulator_threads_policy, pin_cpuset)
5750                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
5751 
5752         # TODO(berrange) When the guest has >1 NUMA node, it will
5753         # span multiple host NUMA nodes. By pinning emulator threads
5754         # to the union of all nodes, we guarantee there will be
5755         # cross-node memory access by the emulator threads when
5756         # responding to guest I/O operations. The only way to avoid
5757         # this would be to pin emulator threads to a single node and
5758         # tell the guest OS to only do I/O from one of its virtual
5759         # NUMA nodes. This is not even remotely practical.
5760         #
5761         # The long term solution is to make use of a new QEMU feature
5762         # called "I/O Threads" which will let us configure an explicit
5763         # I/O thread for each guest vCPU or guest NUMA node. It is
5764         # still TBD how to make use of this feature though, especially
5765         # how to associate IO threads with guest devices to eliminate
5766         # cross NUMA node traffic. This is an area of investigation
5767         # for QEMU community devs.
5768 
5769         # Sort the vcpupin list per vCPU id for human-friendlier XML
5770         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
5771 
5772         # normalize cell.id
5773         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
5774                                                 guest_numa_tune.memnodes)):
5775             cell.id = i
5776             memnode.cellid = i
5777 
5778         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
5779                                guest_numa_tune)
5780 
5781     def _get_guest_os_type(self):
5782         """Returns the guest OS type based on virt type."""
5783         if CONF.libvirt.virt_type == "lxc":
5784             ret = fields.VMMode.EXE
5785         else:
5786             ret = fields.VMMode.HVM
5787         return ret
5788 
5789     def _set_guest_for_rescue(
5790         self, rescue, guest, inst_path, root_device_name,
5791     ):
5792         if rescue.get('kernel_id'):
5793             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
5794             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
5795             if CONF.libvirt.virt_type == "qemu":
5796                 guest.os_cmdline += " no_timer_check"
5797         if rescue.get('ramdisk_id'):
5798             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
5799 
5800     def _set_guest_for_inst_kernel(
5801         self, instance, guest, inst_path, root_device_name, image_meta,
5802     ):
5803         guest.os_kernel = os.path.join(inst_path, "kernel")
5804         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
5805         if CONF.libvirt.virt_type == "qemu":
5806             guest.os_cmdline += " no_timer_check"
5807         if instance.ramdisk_id:
5808             guest.os_initrd = os.path.join(inst_path, "ramdisk")
5809         # we only support os_command_line with images with an explicit
5810         # kernel set and don't want to break nova if there's an
5811         # os_command_line property without a specified kernel_id param
5812         if image_meta.properties.get("os_command_line"):
5813             guest.os_cmdline = image_meta.properties.os_command_line
5814 
5815     def _set_clock(self, guest, os_type, image_meta):
5816         # NOTE(mikal): Microsoft Windows expects the clock to be in
5817         # "localtime". If the clock is set to UTC, then you can use a
5818         # registry key to let windows know, but Microsoft says this is
5819         # buggy in http://support.microsoft.com/kb/2687252
5820         clk = vconfig.LibvirtConfigGuestClock()
5821         if os_type == 'windows':
5822             LOG.info('Configuring timezone for windows instance to localtime')
5823             clk.offset = 'localtime'
5824         else:
5825             clk.offset = 'utc'
5826         guest.set_clock(clk)
5827 
5828         if CONF.libvirt.virt_type == "kvm":
5829             self._set_kvm_timers(clk, os_type, image_meta)
5830 
5831     def _set_kvm_timers(self, clk, os_type, image_meta):
5832         # TODO(berrange) One day this should be per-guest
5833         # OS type configurable
5834         tmpit = vconfig.LibvirtConfigGuestTimer()
5835         tmpit.name = "pit"
5836         tmpit.tickpolicy = "delay"
5837 
5838         tmrtc = vconfig.LibvirtConfigGuestTimer()
5839         tmrtc.name = "rtc"
5840         tmrtc.tickpolicy = "catchup"
5841 
5842         clk.add_timer(tmpit)
5843         clk.add_timer(tmrtc)
5844 
5845         hpet = image_meta.properties.get('hw_time_hpet', False)
5846         guestarch = libvirt_utils.get_arch(image_meta)
5847         if guestarch in (fields.Architecture.I686,
5848                          fields.Architecture.X86_64):
5849             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
5850             # qemu -no-hpet is not supported on non-x86 targets.
5851             tmhpet = vconfig.LibvirtConfigGuestTimer()
5852             tmhpet.name = "hpet"
5853             tmhpet.present = hpet
5854             clk.add_timer(tmhpet)
5855         else:
5856             if hpet:
5857                 LOG.warning('HPET is not turned on for non-x86 guests in image'
5858                             ' %s.', image_meta.id)
5859 
5860         # Provide Windows guests with the paravirtualized hyperv timer source.
5861         # This is the windows equiv of kvm-clock, allowing Windows
5862         # guests to accurately keep time.
5863         if os_type == 'windows':
5864             tmhyperv = vconfig.LibvirtConfigGuestTimer()
5865             tmhyperv.name = "hypervclock"
5866             tmhyperv.present = True
5867             clk.add_timer(tmhyperv)
5868 
5869     def _set_features(self, guest, os_type, image_meta, flavor):
5870         hide_hypervisor_id = (strutils.bool_from_string(
5871                 flavor.extra_specs.get('hide_hypervisor_id')) or
5872                 strutils.bool_from_string(
5873                     flavor.extra_specs.get('hw:hide_hypervisor_id')) or
5874                 image_meta.properties.get('img_hide_hypervisor_id'))
5875 
5876         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
5877             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
5878             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
5879 
5880         if CONF.libvirt.virt_type in ('qemu', 'kvm') and os_type == 'windows':
5881             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
5882             hv.relaxed = True
5883 
5884             hv.spinlocks = True
5885             # Increase spinlock retries - value recommended by
5886             # KVM maintainers who certify Windows guests
5887             # with Microsoft
5888             hv.spinlock_retries = 8191
5889             hv.vapic = True
5890 
5891             # NOTE(kosamara): Spoofing the vendor_id aims to allow the nvidia
5892             # driver to work on windows VMs. At the moment, the nvidia driver
5893             # checks for the hyperv vendorid, and if it doesn't find that, it
5894             # works. In the future, its behaviour could become more strict,
5895             # checking for the presence of other hyperv feature flags to
5896             # determine that it's loaded in a VM. If that happens, this
5897             # workaround will not be enough, and we'll need to drop the whole
5898             # hyperv element.
5899             # That would disable some optimizations, reducing the guest's
5900             # performance.
5901             if hide_hypervisor_id:
5902                 hv.vendorid_spoof = True
5903 
5904             guest.features.append(hv)
5905 
5906         if CONF.libvirt.virt_type in ("qemu", "kvm"):
5907             if hide_hypervisor_id:
5908                 guest.features.append(
5909                     vconfig.LibvirtConfigGuestFeatureKvmHidden())
5910 
5911             # NOTE(sean-k-mooney): we validate that the image and flavor
5912             # cannot have conflicting values in the compute API
5913             # so we just use the values directly. If it is not set in
5914             # either the flavor or image pmu will be none and we should
5915             # not generate the element to allow qemu to decide if a vPMU
5916             # should be provided for backwards compatibility.
5917             pmu = (flavor.extra_specs.get('hw:pmu') or
5918                    image_meta.properties.get('hw_pmu'))
5919             if pmu is not None:
5920                 guest.features.append(
5921                     vconfig.LibvirtConfigGuestFeaturePMU(pmu))
5922 
5923     def _check_number_of_serial_console(self, num_ports):
5924         if (
5925             CONF.libvirt.virt_type in ("kvm", "qemu") and
5926             num_ports > ALLOWED_QEMU_SERIAL_PORTS
5927         ):
5928             raise exception.SerialPortNumberLimitExceeded(
5929                 allowed=ALLOWED_QEMU_SERIAL_PORTS,
5930                 virt_type=CONF.libvirt.virt_type)
5931 
5932     def _video_model_supported(self, model):
5933         return model in fields.VideoModel.ALL
5934 
5935     def _add_video_driver(self, guest, image_meta, flavor):
5936         video = vconfig.LibvirtConfigGuestVideo()
5937         # NOTE(ldbragst): The following logic sets the video.type
5938         # depending on supported defaults given the architecture,
5939         # virtualization type, and features. The video.type attribute can
5940         # be overridden by the user with image_meta.properties, which
5941         # is carried out in the next if statement below this one.
5942         guestarch = libvirt_utils.get_arch(image_meta)
5943         if CONF.libvirt.virt_type == 'parallels':
5944             video.type = 'vga'
5945         elif guestarch in (fields.Architecture.PPC,
5946                            fields.Architecture.PPC64,
5947                            fields.Architecture.PPC64LE):
5948             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
5949             # so use 'vga' instead when running on Power hardware.
5950             video.type = 'vga'
5951         elif guestarch == fields.Architecture.AARCH64:
5952             # NOTE(kevinz): Only virtio device type is supported by AARCH64
5953             # so use 'virtio' instead when running on AArch64 hardware.
5954             video.type = 'virtio'
5955         elif CONF.spice.enabled:
5956             video.type = 'qxl'
5957         if image_meta.properties.get('hw_video_model'):
5958             video.type = image_meta.properties.hw_video_model
5959             if not self._video_model_supported(video.type):
5960                 raise exception.InvalidVideoMode(model=video.type)
5961 
5962         # Set video memory, only if the flavor's limit is set
5963         video_ram = image_meta.properties.get('hw_video_ram', 0)
5964         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
5965         if video_ram > max_vram:
5966             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
5967                                                  max_vram=max_vram)
5968         if max_vram and video_ram:
5969             video.vram = video_ram * units.Mi // units.Ki
5970         guest.add_device(video)
5971 
5972         # NOTE(sean-k-mooney): return the video device we added
5973         # for simpler testing.
5974         return video
5975 
5976     def _add_qga_device(self, guest, instance):
5977         qga = vconfig.LibvirtConfigGuestChannel()
5978         qga.type = "unix"
5979         qga.target_name = "org.qemu.guest_agent.0"
5980         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
5981                           ("org.qemu.guest_agent.0", instance.name))
5982         guest.add_device(qga)
5983 
5984     def _add_rng_device(self, guest, flavor, image_meta):
5985         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', 'True')
5986         rng_allowed = strutils.bool_from_string(rng_allowed_str)
5987 
5988         if not rng_allowed:
5989             return
5990 
5991         rng_device = vconfig.LibvirtConfigGuestRng()
5992         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
5993         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
5994         if rate_bytes:
5995             rng_device.rate_bytes = int(rate_bytes)
5996             rng_device.rate_period = int(period)
5997         rng_path = CONF.libvirt.rng_dev_path
5998         if (rng_path and not os.path.exists(rng_path)):
5999             raise exception.RngDeviceNotExist(path=rng_path)
6000         rng_device.backend = rng_path
6001         guest.add_device(rng_device)
6002 
6003     def _add_virtio_serial_controller(self, guest, instance):
6004         virtio_controller = vconfig.LibvirtConfigGuestController()
6005         virtio_controller.type = 'virtio-serial'
6006         guest.add_device(virtio_controller)
6007 
6008     def _add_vtpm_device(
6009         self,
6010         guest: libvirt_guest.Guest,
6011         flavor: 'objects.Flavor',
6012         instance: 'objects.Instance',
6013         image_meta: 'objects.ImageMeta',
6014     ) -> None:
6015         """Add a vTPM device to the guest, if requested."""
6016         # Enable virtual tpm support if required in the flavor or image.
6017         vtpm_config = hardware.get_vtpm_constraint(flavor, image_meta)
6018         if not vtpm_config:
6019             return None
6020 
6021         vtpm_secret_uuid = instance.system_metadata.get('vtpm_secret_uuid')
6022         if not vtpm_secret_uuid:
6023             raise exception.Invalid(
6024                 'Refusing to create an emulated TPM with no secret!')
6025 
6026         vtpm = vconfig.LibvirtConfigGuestVTPM(vtpm_config, vtpm_secret_uuid)
6027         guest.add_device(vtpm)
6028 
6029     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
6030         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
6031         if image_meta.properties.get('hw_qemu_guest_agent', False):
6032             # a virtio-serial controller is required for qga. If it is not
6033             # created explicitly, libvirt will do it by itself. But in case
6034             # of AMD SEV, any virtio device should use iommu driver, and
6035             # libvirt does not know about it. That is why the controller
6036             # should be created manually.
6037             if self._sev_enabled(flavor, image_meta):
6038                 self._add_virtio_serial_controller(guest, instance)
6039 
6040             LOG.debug("Qemu guest agent is enabled through image "
6041                       "metadata", instance=instance)
6042             self._add_qga_device(guest, instance)
6043 
6044     def _get_guest_memory_backing_config(
6045             self, inst_topology, numatune, flavor, image_meta):
6046         wantsrealtime = hardware.is_realtime_enabled(flavor)
6047         if (
6048             wantsrealtime and
6049             hardware.get_emulator_thread_policy_constraint(flavor) ==
6050                 fields.CPUEmulatorThreadsPolicy.SHARE and
6051             not CONF.compute.cpu_shared_set
6052         ):
6053             # NOTE(stephenfin) Yes, it's horrible that we're doing this here,
6054             # but the shared policy unfortunately has different behavior
6055             # depending on whether the '[compute] cpu_shared_set' is configured
6056             # or not and we need it to be configured. Also note that we have
6057             # already handled other conditions, such as no emulator thread
6058             # policy being configured whatsoever, at the API level.
6059             LOG.warning(
6060                 'Instance is requesting real-time CPUs with pooled '
6061                 'emulator threads, but a shared CPU pool has not been '
6062                 'configured on this host.'
6063             )
6064             raise exception.RealtimeMaskNotFoundOrInvalid()
6065 
6066         wantsmempages = False
6067         if inst_topology:
6068             for cell in inst_topology.cells:
6069                 if cell.pagesize:
6070                     wantsmempages = True
6071                     break
6072 
6073         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
6074 
6075         if wantsmempages and wantsfilebacked:
6076             # Can't use file-backed memory with hugepages
6077             LOG.warning("Instance requested huge pages, but file-backed "
6078                     "memory is enabled, and incompatible with huge pages")
6079             raise exception.MemoryPagesUnsupported()
6080 
6081         membacking = None
6082         if wantsmempages:
6083             pages = self._get_memory_backing_hugepages_support(
6084                 inst_topology, numatune)
6085             if pages:
6086                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6087                 membacking.hugepages = pages
6088         if wantsrealtime:
6089             if not membacking:
6090                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6091             membacking.locked = True
6092             membacking.sharedpages = False
6093         if wantsfilebacked:
6094             if not membacking:
6095                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6096             membacking.filesource = True
6097             membacking.sharedaccess = True
6098             membacking.allocateimmediate = True
6099             membacking.discard = True
6100         if self._sev_enabled(flavor, image_meta):
6101             if not membacking:
6102                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
6103             membacking.locked = True
6104 
6105         return membacking
6106 
6107     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
6108         if not self._has_numa_support():
6109             # We should not get here, since we should have avoided
6110             # reporting NUMA topology from _get_host_numa_topology
6111             # in the first place. Just in case of a scheduler
6112             # mess up though, raise an exception
6113             raise exception.MemoryPagesUnsupported()
6114 
6115         host_topology = self._get_host_numa_topology()
6116 
6117         if host_topology is None:
6118             # As above, we should not get here but just in case...
6119             raise exception.MemoryPagesUnsupported()
6120 
6121         # Currently libvirt does not support the smallest
6122         # pagesize set as a backend memory.
6123         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
6124         avail_pagesize = [page.size_kb
6125                           for page in host_topology.cells[0].mempages]
6126         avail_pagesize.sort()
6127         smallest = avail_pagesize[0]
6128 
6129         pages = []
6130         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
6131             if inst_cell.pagesize and inst_cell.pagesize > smallest:
6132                 for memnode in numatune.memnodes:
6133                     if guest_cellid == memnode.cellid:
6134                         page = (
6135                             vconfig.LibvirtConfigGuestMemoryBackingPage())
6136                         page.nodeset = [guest_cellid]
6137                         page.size_kb = inst_cell.pagesize
6138                         pages.append(page)
6139                         break  # Quit early...
6140         return pages
6141 
6142     def _get_flavor(self, ctxt, instance, flavor):
6143         if flavor is not None:
6144             return flavor
6145         return instance.flavor
6146 
6147     def _check_uefi_support(self, hw_firmware_type):
6148         caps = self._host.get_capabilities()
6149         return self._host.supports_uefi and (
6150             hw_firmware_type == fields.FirmwareType.UEFI or
6151             caps.host.cpu.arch == fields.Architecture.AARCH64
6152         )
6153 
6154     def _check_secure_boot_support(
6155         self,
6156         arch: str,
6157         machine_type: str,
6158         firmware_type: str,
6159     ) -> bool:
6160         if not self._host.supports_secure_boot:
6161             # secure boot requires host configuration
6162             return False
6163 
6164         if firmware_type != fields.FirmwareType.UEFI:
6165             # secure boot is only supported with UEFI
6166             return False
6167 
6168         if (
6169             arch == fields.Architecture.X86_64 and
6170             'q35' not in machine_type
6171         ):
6172             # secure boot on x86_64 requires the Q35 machine type
6173             return False
6174 
6175         return True
6176 
6177     def _get_supported_perf_events(self):
6178         if not len(CONF.libvirt.enabled_perf_events):
6179             return []
6180 
6181         supported_events = []
6182         for event in CONF.libvirt.enabled_perf_events:
6183             libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
6184 
6185             if not hasattr(libvirt, libvirt_perf_event_name):
6186                 LOG.warning("Libvirt does not support event type '%s'.", event)
6187                 continue
6188 
6189             if event in ('cmt', 'mbml', 'mbmt'):
6190                 LOG.warning(
6191                     "Monitoring of Intel CMT `perf` event(s) '%s' is not "
6192                     "supported by recent Linux kernels; ignoring.",
6193                     event,
6194                 )
6195                 continue
6196 
6197             supported_events.append(event)
6198 
6199         return supported_events
6200 
6201     def _configure_guest_by_virt_type(
6202         self,
6203         guest: libvirt_guest.Guest,
6204         instance: 'objects.Instance',
6205         image_meta: 'objects.ImageMeta',
6206         flavor: 'objects.Flavor',
6207     ) -> None:
6208         if CONF.libvirt.virt_type in ("kvm", "qemu"):
6209             arch = libvirt_utils.get_arch(image_meta)
6210             if arch in (fields.Architecture.I686, fields.Architecture.X86_64):
6211                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
6212                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
6213 
6214             mach_type = libvirt_utils.get_machine_type(image_meta)
6215             guest.os_mach_type = mach_type
6216 
6217             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
6218 
6219             if arch == fields.Architecture.AARCH64:
6220                 if not hw_firmware_type:
6221                     hw_firmware_type = fields.FirmwareType.UEFI
6222 
6223             if hw_firmware_type == fields.FirmwareType.UEFI:
6224                 global uefi_logged
6225                 if not uefi_logged:
6226                     LOG.warning("uefi support is without some kind of "
6227                                 "functional testing and therefore "
6228                                 "considered experimental.")
6229                     uefi_logged = True
6230 
6231                 if not self._host.supports_uefi:
6232                     raise exception.UEFINotSupported()
6233 
6234                 # TODO(stephenfin): Drop this when we drop support for legacy
6235                 # architectures
6236                 if not mach_type:
6237                     # loaders are specific to arch and machine type - if we
6238                     # don't have a machine type here, we're on a legacy
6239                     # architecture that we have no default machine type for
6240                     raise exception.UEFINotSupported()
6241 
6242                 os_secure_boot = hardware.get_secure_boot_constraint(
6243                     flavor, image_meta)
6244                 if os_secure_boot == 'required':
6245                     # hard fail if we don't support secure boot and it's
6246                     # required
6247                     if not self._check_secure_boot_support(
6248                         arch, mach_type, hw_firmware_type,
6249                     ):
6250                         raise exception.SecureBootNotSupported()
6251 
6252                     guest.os_loader_secure = True
6253                 elif os_secure_boot == 'optional':
6254                     # only enable it if the host is configured appropriately
6255                     guest.os_loader_secure = self._check_secure_boot_support(
6256                         arch, mach_type, hw_firmware_type,
6257                     )
6258                 else:
6259                     guest.os_loader_secure = False
6260 
6261                 try:
6262                     loader, nvram_template = self._host.get_loader(
6263                         arch, mach_type,
6264                         has_secure_boot=guest.os_loader_secure)
6265                 except exception.UEFINotSupported as exc:
6266                     if guest.os_loader_secure:
6267                         # we raise a specific exception if we requested secure
6268                         # boot and couldn't get that
6269                         raise exception.SecureBootNotSupported() from exc
6270                     raise
6271 
6272                 guest.os_loader = loader
6273                 guest.os_loader_type = 'pflash'
6274                 guest.os_nvram_template = nvram_template
6275 
6276             # NOTE(lyarwood): If the machine type isn't recorded in the stashed
6277             # image metadata then record it through the system metadata table.
6278             # This will allow the host configuration to change in the future
6279             # without impacting existing instances.
6280             # NOTE(lyarwood): The value of ``hw_machine_type`` within the
6281             # stashed image metadata of the instance actually comes from the
6282             # system metadata table under the ``image_hw_machine_type`` key via
6283             # nova.objects.ImageMeta.from_instance and the
6284             # nova.utils.get_image_from_system_metadata function.
6285             if image_meta.properties.get('hw_machine_type') is None:
6286                 instance.system_metadata['image_hw_machine_type'] = mach_type
6287 
6288             if image_meta.properties.get('hw_boot_menu') is None:
6289                 guest.os_bootmenu = strutils.bool_from_string(
6290                     flavor.extra_specs.get('hw:boot_menu', 'no'))
6291             else:
6292                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
6293         elif CONF.libvirt.virt_type == "lxc":
6294             guest.os_init_path = "/sbin/init"
6295             guest.os_cmdline = CONSOLE
6296             guest.os_init_env["product_name"] = "OpenStack Nova"
6297         elif CONF.libvirt.virt_type == "parallels":
6298             if guest.os_type == fields.VMMode.EXE:
6299                 guest.os_init_path = "/sbin/init"
6300 
6301         return None
6302 
6303     def _conf_non_lxc(
6304         self,
6305         guest: libvirt_guest.Guest,
6306         root_device_name: str,
6307         rescue: bool,
6308         instance: 'objects.Instance',
6309         inst_path: str,
6310         image_meta: 'objects.ImageMeta',
6311         disk_info: ty.Dict[str, ty.Any],
6312     ):
6313         if rescue:
6314             self._set_guest_for_rescue(
6315                 rescue, guest, inst_path, root_device_name)
6316         elif instance.kernel_id:
6317             self._set_guest_for_inst_kernel(
6318                 instance, guest, inst_path, root_device_name, image_meta)
6319         else:
6320             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
6321 
6322     def _create_consoles(self, guest_cfg, instance, flavor, image_meta):
6323         # NOTE(markus_z): Beware! Below are so many conditionals that it is
6324         # easy to lose track. Use this chart to figure out your case:
6325         #
6326         # case | is serial | is qemu | resulting
6327         #      | enabled?  | or kvm? | devices
6328         # -------------------------------------------
6329         #    1 |        no |     no  | pty*
6330         #    2 |        no |     yes | pty with logd
6331         #    3 |       yes |      no | see case 1
6332         #    4 |       yes |     yes | tcp with logd
6333         #
6334         #    * exception: `virt_type=parallels` doesn't create a device
6335         if CONF.libvirt.virt_type == 'parallels':
6336             pass
6337         elif CONF.libvirt.virt_type == 'lxc':
6338             log_path = self._get_console_log_path(instance)
6339             self._create_pty_device(
6340                 guest_cfg, vconfig.LibvirtConfigGuestConsole,
6341                 log_path=log_path)
6342         else:  # qemu, kvm
6343             if self._is_s390x_guest(image_meta):
6344                 self._create_consoles_s390x(
6345                     guest_cfg, instance, flavor, image_meta)
6346             else:
6347                 self._create_consoles_qemu_kvm(
6348                     guest_cfg, instance, flavor, image_meta)
6349 
6350     def _is_s390x_guest(self, image_meta):
6351         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
6352         return libvirt_utils.get_arch(image_meta) in s390x_archs
6353 
6354     def _is_ppc64_guest(self, image_meta):
6355         archs = (fields.Architecture.PPC64, fields.Architecture.PPC64LE)
6356         return libvirt_utils.get_arch(image_meta) in archs
6357 
6358     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
6359                                   image_meta):
6360         char_dev_cls = vconfig.LibvirtConfigGuestSerial
6361         log_path = self._get_console_log_path(instance)
6362         if CONF.serial_console.enabled:
6363             if not self._serial_ports_already_defined(instance):
6364                 num_ports = hardware.get_number_of_serial_ports(flavor,
6365                                                                 image_meta)
6366                 self._check_number_of_serial_console(num_ports)
6367                 self._create_serial_consoles(guest_cfg, num_ports,
6368                                              char_dev_cls, log_path)
6369         else:
6370             self._create_pty_device(guest_cfg, char_dev_cls,
6371                                     log_path=log_path)
6372 
6373     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
6374         char_dev_cls = vconfig.LibvirtConfigGuestConsole
6375         log_path = self._get_console_log_path(instance)
6376         if CONF.serial_console.enabled:
6377             if not self._serial_ports_already_defined(instance):
6378                 num_ports = hardware.get_number_of_serial_ports(flavor,
6379                                                                 image_meta)
6380                 self._create_serial_consoles(guest_cfg, num_ports,
6381                                              char_dev_cls, log_path)
6382         else:
6383             self._create_pty_device(guest_cfg, char_dev_cls,
6384                                     "sclp", log_path)
6385 
6386     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
6387                            log_path=None):
6388 
6389         consolepty = char_dev_cls()
6390         consolepty.target_type = target_type
6391         consolepty.type = "pty"
6392 
6393         log = vconfig.LibvirtConfigGuestCharDeviceLog()
6394         log.file = log_path
6395         consolepty.log = log
6396 
6397         guest_cfg.add_device(consolepty)
6398 
6399     def _serial_ports_already_defined(self, instance):
6400         try:
6401             guest = self._host.get_guest(instance)
6402             if list(self._get_serial_ports_from_guest(guest)):
6403                 # Serial port are already configured for instance that
6404                 # means we are in a context of migration.
6405                 return True
6406         except exception.InstanceNotFound:
6407             LOG.debug(
6408                 "Instance does not exist yet on libvirt, we can "
6409                 "safely pass on looking for already defined serial "
6410                 "ports in its domain XML", instance=instance)
6411         return False
6412 
6413     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
6414                                 log_path):
6415         for port in range(num_ports):
6416             console = char_dev_cls()
6417             console.port = port
6418             console.type = "tcp"
6419             console.listen_host = CONF.serial_console.proxyclient_address
6420             listen_port = serial_console.acquire_port(console.listen_host)
6421             console.listen_port = listen_port
6422             # NOTE: only the first serial console gets the boot messages,
6423             # that's why we attach the logd subdevice only to that.
6424             if port == 0:
6425                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
6426                 log.file = log_path
6427                 console.log = log
6428             guest_cfg.add_device(console)
6429 
6430     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
6431         """Update VirtCPUModel object according to libvirt CPU config.
6432 
6433         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
6434                            instance's virtual cpu configuration.
6435         :param:vcpu_model: VirtCPUModel object. A new object will be created
6436                            if None.
6437 
6438         :return: Updated VirtCPUModel object, or None if cpu_config is None
6439 
6440         """
6441 
6442         if not cpu_config:
6443             return
6444         if not vcpu_model:
6445             vcpu_model = objects.VirtCPUModel()
6446 
6447         vcpu_model.arch = cpu_config.arch
6448         vcpu_model.vendor = cpu_config.vendor
6449         vcpu_model.model = cpu_config.model
6450         vcpu_model.mode = cpu_config.mode
6451         vcpu_model.match = cpu_config.match
6452 
6453         if cpu_config.sockets:
6454             vcpu_model.topology = objects.VirtCPUTopology(
6455                 sockets=cpu_config.sockets,
6456                 cores=cpu_config.cores,
6457                 threads=cpu_config.threads)
6458         else:
6459             vcpu_model.topology = None
6460 
6461         features = [objects.VirtCPUFeature(
6462             name=f.name,
6463             policy=f.policy) for f in cpu_config.features]
6464         vcpu_model.features = features
6465 
6466         return vcpu_model
6467 
6468     def _vcpu_model_to_cpu_config(self, vcpu_model):
6469         """Create libvirt CPU config according to VirtCPUModel object.
6470 
6471         :param:vcpu_model: VirtCPUModel object.
6472 
6473         :return: vconfig.LibvirtConfigGuestCPU.
6474 
6475         """
6476 
6477         cpu_config = vconfig.LibvirtConfigGuestCPU()
6478         cpu_config.arch = vcpu_model.arch
6479         cpu_config.model = vcpu_model.model
6480         cpu_config.mode = vcpu_model.mode
6481         cpu_config.match = vcpu_model.match
6482         cpu_config.vendor = vcpu_model.vendor
6483         if vcpu_model.topology:
6484             cpu_config.sockets = vcpu_model.topology.sockets
6485             cpu_config.cores = vcpu_model.topology.cores
6486             cpu_config.threads = vcpu_model.topology.threads
6487         if vcpu_model.features:
6488             for f in vcpu_model.features:
6489                 xf = vconfig.LibvirtConfigGuestCPUFeature()
6490                 xf.name = f.name
6491                 xf.policy = f.policy
6492                 cpu_config.features.add(xf)
6493         return cpu_config
6494 
6495     def _guest_needs_usb(self, guest, image_meta):
6496         """Evaluate devices currently attached to the guest."""
6497         if self._is_ppc64_guest(image_meta):
6498             # PPC64 guests get a USB keyboard and mouse automatically
6499             return True
6500 
6501         for dev in guest.devices:
6502             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
6503                 if dev.target_bus == 'usb':
6504                     return True
6505 
6506             if isinstance(dev, vconfig.LibvirtConfigGuestInput):
6507                 if dev.bus == 'usb':
6508                     return True
6509 
6510         return False
6511 
6512     def _guest_add_usb_root_controller(self, guest, image_meta):
6513         """Add USB root controller, if necessary.
6514 
6515         Note that these are added by default on x86-64. We add the controller
6516         here explicitly so that we can _disable_ it (by setting the model to
6517         'none') if it's not necessary.
6518         """
6519         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
6520         usbhost.index = 0
6521         # an unset model means autodetect, while 'none' means don't add a
6522         # controller (x86 gets one by default)
6523         usbhost.model = None
6524         if not self._guest_needs_usb(guest, image_meta):
6525             usbhost.model = 'none'
6526         guest.add_device(usbhost)
6527 
6528     def _guest_add_pcie_root_ports(self, guest):
6529         """Add PCI Express root ports.
6530 
6531         PCI Express machine can have as many PCIe devices as it has
6532         pcie-root-port controllers (slots in virtual motherboard).
6533 
6534         If we want to have more PCIe slots for hotplug then we need to create
6535         whole PCIe structure (libvirt limitation).
6536         """
6537 
6538         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
6539         guest.add_device(pcieroot)
6540 
6541         for x in range(0, CONF.libvirt.num_pcie_ports):
6542             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
6543             guest.add_device(pcierootport)
6544 
6545     def _guest_needs_pcie(self, guest):
6546         """Check for prerequisites for adding PCIe root port
6547         controllers
6548         """
6549         caps = self._host.get_capabilities()
6550 
6551         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
6552         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
6553         # have "pc" machine type.  That assumption, although it is
6554         # correct for the "forseeable future", it will be invalid when
6555         # libvirt / QEMU changes the default machine types.
6556         #
6557         # From libvirt 4.7.0 onwards (September 2018), it will ensure
6558         # that *if* 'pc' is available, it will be used as the default --
6559         # to not break existing applications.  (Refer:
6560         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
6561         # --"qemu: ensure default machine types don't change if QEMU
6562         # changes").
6563         #
6564         # But even if libvirt (>=v4.7.0) handled the default case,
6565         # relying on such assumptions is not robust.  Instead we should
6566         # get the default machine type for a given architecture reliably
6567         # -- by Nova setting it explicitly (we already do it for Arm /
6568         # AArch64 & s390x).  A part of this bug is being tracked here:
6569         # https://bugs.launchpad.net/nova/+bug/1780138).
6570 
6571         # Add PCIe root port controllers for PCI Express machines
6572         # but only if their amount is configured
6573 
6574         if not CONF.libvirt.num_pcie_ports:
6575             return False
6576         if (caps.host.cpu.arch == fields.Architecture.AARCH64 and
6577                 guest.os_mach_type.startswith('virt')):
6578             return True
6579         if (caps.host.cpu.arch == fields.Architecture.X86_64 and
6580                 guest.os_mach_type is not None and
6581                 'q35' in guest.os_mach_type):
6582             return True
6583         return False
6584 
6585     def _get_guest_config(self, instance, network_info, image_meta,
6586                           disk_info, rescue=None, block_device_info=None,
6587                           context=None, mdevs=None, accel_info=None):
6588         """Get config data for parameters.
6589 
6590         :param rescue: optional dictionary that should contain the key
6591             'ramdisk_id' if a ramdisk is needed for the rescue image and
6592             'kernel_id' if a kernel is needed for the rescue image.
6593 
6594         :param mdevs: optional list of mediated devices to assign to the guest.
6595         :param accel_info: optional list of accelerator requests (ARQs)
6596         """
6597         flavor = instance.flavor
6598         inst_path = libvirt_utils.get_instance_path(instance)
6599         disk_mapping = disk_info['mapping']
6600         vpmems = self._get_ordered_vpmems(instance, flavor)
6601 
6602         guest = vconfig.LibvirtConfigGuest()
6603         guest.virt_type = CONF.libvirt.virt_type
6604         guest.name = instance.name
6605         guest.uuid = instance.uuid
6606         # We are using default unit for memory: KiB
6607         guest.memory = flavor.memory_mb * units.Ki
6608         guest.vcpus = flavor.vcpus
6609 
6610         guest_numa_config = self._get_guest_numa_config(
6611             instance.numa_topology, flavor, image_meta)
6612 
6613         guest.cpuset = guest_numa_config.cpuset
6614         guest.cputune = guest_numa_config.cputune
6615         guest.numatune = guest_numa_config.numatune
6616 
6617         guest.membacking = self._get_guest_memory_backing_config(
6618             instance.numa_topology,
6619             guest_numa_config.numatune,
6620             flavor, image_meta)
6621 
6622         guest.metadata.append(self._get_guest_config_meta(
6623             instance, network_info))
6624         guest.idmaps = self._get_guest_idmaps()
6625 
6626         for event in self._supported_perf_events:
6627             guest.add_perf_event(event)
6628 
6629         self._update_guest_cputune(guest, flavor)
6630 
6631         guest.cpu = self._get_guest_cpu_config(
6632             flavor, image_meta, guest_numa_config.numaconfig,
6633             instance.numa_topology)
6634 
6635         # Notes(yjiang5): we always sync the instance's vcpu model with
6636         # the corresponding config file.
6637         instance.vcpu_model = self._cpu_config_to_vcpu_model(
6638             guest.cpu, instance.vcpu_model)
6639 
6640         if 'root' in disk_mapping:
6641             root_device_name = block_device.prepend_dev(
6642                 disk_mapping['root']['dev'])
6643         else:
6644             root_device_name = None
6645 
6646         guest.os_type = (
6647             fields.VMMode.get_from_instance(instance) or
6648             self._get_guest_os_type()
6649         )
6650 
6651         sev_enabled = self._sev_enabled(flavor, image_meta)
6652 
6653         self._configure_guest_by_virt_type(guest, instance, image_meta, flavor)
6654         if CONF.libvirt.virt_type != 'lxc':
6655             self._conf_non_lxc(
6656                 guest, root_device_name, rescue, instance, inst_path,
6657                 image_meta, disk_info)
6658 
6659         self._set_features(guest, instance.os_type, image_meta, flavor)
6660         self._set_clock(guest, instance.os_type, image_meta)
6661 
6662         storage_configs = self._get_guest_storage_config(context,
6663                 instance, image_meta, disk_info, rescue, block_device_info,
6664                 flavor, guest.os_type)
6665         for config in storage_configs:
6666             guest.add_device(config)
6667 
6668         for vif in network_info:
6669             config = self.vif_driver.get_config(
6670                 instance, vif, image_meta, flavor, CONF.libvirt.virt_type,
6671             )
6672             guest.add_device(config)
6673 
6674         self._create_consoles(guest, instance, flavor, image_meta)
6675 
6676         self._guest_add_spice_channel(guest)
6677 
6678         if self._guest_add_video_device(guest):
6679             self._add_video_driver(guest, image_meta, flavor)
6680 
6681             self._guest_add_pointer_device(guest, image_meta)
6682             self._guest_add_keyboard_device(guest, image_meta)
6683 
6684         # Some features are only supported 'qemu' and 'kvm' hypervisor
6685         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
6686             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
6687             self._add_rng_device(guest, flavor, image_meta)
6688             self._add_vtpm_device(guest, flavor, instance, image_meta)
6689 
6690         if self._guest_needs_pcie(guest):
6691             self._guest_add_pcie_root_ports(guest)
6692 
6693         self._guest_add_usb_root_controller(guest, image_meta)
6694 
6695         self._guest_add_pci_devices(guest, instance)
6696 
6697         pci_arq_list = []
6698         if accel_info:
6699             # NOTE(Sundar): We handle only the case where all attach handles
6700             # are of type 'PCI'. The Cyborg fake driver used for testing
6701             # returns attach handles of type 'TEST_PCI' and so its ARQs will
6702             # not get composed into the VM's domain XML. For now, we do not
6703             # expect a mixture of different attach handles for the same
6704             # instance; but that case also gets ignored by this logic.
6705             ah_types_set = {arq['attach_handle_type'] for arq in accel_info}
6706             supported_types_set = {'PCI'}
6707             if ah_types_set == supported_types_set:
6708                 pci_arq_list = accel_info
6709             else:
6710                 LOG.info('Ignoring accelerator requests for instance %s. '
6711                          'Supported Attach handle types: %s. '
6712                          'But got these unsupported types: %s.',
6713                          instance.uuid, supported_types_set,
6714                          ah_types_set.difference(supported_types_set))
6715 
6716         self._guest_add_accel_pci_devices(guest, pci_arq_list)
6717 
6718         self._guest_add_watchdog_action(guest, flavor, image_meta)
6719 
6720         self._guest_add_memory_balloon(guest)
6721 
6722         if mdevs:
6723             self._guest_add_mdevs(guest, mdevs)
6724 
6725         if sev_enabled:
6726             caps = self._host.get_capabilities()
6727             self._guest_configure_sev(guest, caps.host.cpu.arch,
6728                                       guest.os_mach_type)
6729 
6730         if vpmems:
6731             self._guest_add_vpmems(guest, vpmems)
6732 
6733         return guest
6734 
6735     def _get_ordered_vpmems(self, instance, flavor):
6736         resources = self._get_resources(instance)
6737         ordered_vpmem_resources = self._get_ordered_vpmem_resources(
6738             resources, flavor)
6739         ordered_vpmems = [self._vpmems_by_name[resource.identifier]
6740             for resource in ordered_vpmem_resources]
6741         return ordered_vpmems
6742 
6743     def _get_vpmems(self, instance, prefix=None):
6744         resources = self._get_resources(instance, prefix=prefix)
6745         vpmem_resources = self._get_vpmem_resources(resources)
6746         vpmems = [self._vpmems_by_name[resource.identifier]
6747             for resource in vpmem_resources]
6748         return vpmems
6749 
6750     def _guest_add_vpmems(self, guest, vpmems):
6751         guest.max_memory_size = guest.memory
6752         guest.max_memory_slots = 0
6753         for vpmem in vpmems:
6754             size_kb = vpmem.size // units.Ki
6755             align_kb = vpmem.align // units.Ki
6756 
6757             vpmem_config = vconfig.LibvirtConfigGuestVPMEM(
6758                 devpath=vpmem.devpath, size_kb=size_kb, align_kb=align_kb)
6759 
6760             # max memory size needs contain vpmem size
6761             guest.max_memory_size += size_kb
6762             # one vpmem will occupy one memory slot
6763             guest.max_memory_slots += 1
6764             guest.add_device(vpmem_config)
6765 
6766     def _sev_enabled(self, flavor, image_meta):
6767         """To enable AMD SEV, the following should be true:
6768 
6769         a) the supports_amd_sev instance variable in the host is
6770            true,
6771         b) the instance extra specs and/or image properties request
6772            memory encryption to be enabled, and
6773         c) there are no conflicts between extra specs, image properties
6774            and machine type selection.
6775 
6776         Most potential conflicts in c) should already be caught in the
6777         API layer.  However there is still one remaining case which
6778         needs to be handled here: when the image does not contain an
6779         hw_machine_type property, the machine type will be chosen from
6780         CONF.libvirt.hw_machine_type if configured, otherwise falling
6781         back to the hardcoded value which is currently 'pc'.  If it
6782         ends up being 'pc' or another value not in the q35 family, we
6783         need to raise an exception.  So calculate the machine type and
6784         pass it to be checked alongside the other sanity checks which
6785         are run while determining whether SEV is selected.
6786         """
6787         if not self._host.supports_amd_sev:
6788             return False
6789 
6790         mach_type = libvirt_utils.get_machine_type(image_meta)
6791         return hardware.get_mem_encryption_constraint(flavor, image_meta,
6792                                                       mach_type)
6793 
6794     def _guest_configure_sev(self, guest, arch, mach_type):
6795         sev = self._find_sev_feature(arch, mach_type)
6796         if sev is None:
6797             # In theory this should never happen because it should
6798             # only get called if SEV was requested, in which case the
6799             # guest should only get scheduled on this host if it
6800             # supports SEV, and SEV support is dependent on the
6801             # presence of this <sev> feature.  That said, it's
6802             # conceivable that something could get messed up along the
6803             # way, e.g. a mismatch in the choice of machine type.  So
6804             # make sure that if it ever does happen, we at least get a
6805             # helpful error rather than something cryptic like
6806             # "AttributeError: 'NoneType' object has no attribute 'cbitpos'
6807             raise exception.MissingDomainCapabilityFeatureException(
6808                 feature='sev')
6809 
6810         designer.set_driver_iommu_for_sev(guest)
6811         self._guest_add_launch_security(guest, sev)
6812 
6813     def _guest_add_launch_security(self, guest, sev):
6814         launch_security = vconfig.LibvirtConfigGuestSEVLaunchSecurity()
6815         launch_security.cbitpos = sev.cbitpos
6816         launch_security.reduced_phys_bits = sev.reduced_phys_bits
6817         guest.launch_security = launch_security
6818 
6819     def _find_sev_feature(self, arch, mach_type):
6820         """Search domain capabilities for the given arch and machine type
6821         for the <sev> element under <features>, and return it if found.
6822         """
6823         domain_caps = self._host.get_domain_capabilities()
6824         if arch not in domain_caps:
6825             LOG.warning(
6826                 "Wanted to add SEV to config for guest with arch %(arch)s "
6827                 "but only had domain capabilities for: %(archs)s",
6828                 {'arch': arch, 'archs': ' '.join(domain_caps)})
6829             return None
6830 
6831         if mach_type not in domain_caps[arch]:
6832             LOG.warning(
6833                 "Wanted to add SEV to config for guest with machine type "
6834                 "%(mtype)s but for arch %(arch)s only had domain capabilities "
6835                 "for machine types: %(mtypes)s",
6836                 {'mtype': mach_type, 'arch': arch,
6837                  'mtypes': ' '.join(domain_caps[arch])})
6838             return None
6839 
6840         for feature in domain_caps[arch][mach_type].features:
6841             if feature.root_name == 'sev':
6842                 return feature
6843 
6844         return None
6845 
6846     def _guest_add_mdevs(self, guest, chosen_mdevs):
6847         for chosen_mdev in chosen_mdevs:
6848             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
6849             mdev.uuid = chosen_mdev
6850             guest.add_device(mdev)
6851 
6852     @staticmethod
6853     def _guest_add_spice_channel(guest):
6854         if (
6855             CONF.spice.enabled and CONF.spice.agent_enabled and
6856             CONF.libvirt.virt_type != 'lxc'
6857         ):
6858             channel = vconfig.LibvirtConfigGuestChannel()
6859             channel.type = 'spicevmc'
6860             channel.target_name = "com.redhat.spice.0"
6861             guest.add_device(channel)
6862 
6863     @staticmethod
6864     def _guest_add_memory_balloon(guest):
6865         # Memory balloon device only support 'qemu/kvm' hypervisor
6866         if (
6867             CONF.libvirt.virt_type in ('qemu', 'kvm') and
6868             CONF.libvirt.mem_stats_period_seconds > 0
6869         ):
6870             balloon = vconfig.LibvirtConfigMemoryBalloon()
6871             balloon.model = 'virtio'
6872             balloon.period = CONF.libvirt.mem_stats_period_seconds
6873             guest.add_device(balloon)
6874 
6875     @staticmethod
6876     def _guest_add_watchdog_action(guest, flavor, image_meta):
6877         # image meta takes precedence over flavor extra specs; disable the
6878         # watchdog action by default
6879         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action') or
6880                            'disabled')
6881         watchdog_action = image_meta.properties.get('hw_watchdog_action',
6882                                                     watchdog_action)
6883         # NB(sross): currently only actually supported by KVM/QEmu
6884         if watchdog_action != 'disabled':
6885             if watchdog_action in fields.WatchdogAction.ALL:
6886                 bark = vconfig.LibvirtConfigGuestWatchdog()
6887                 bark.action = watchdog_action
6888                 guest.add_device(bark)
6889             else:
6890                 raise exception.InvalidWatchdogAction(action=watchdog_action)
6891 
6892     def _guest_add_pci_devices(self, guest, instance):
6893         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
6894             # Get all generic PCI devices (non-SR-IOV).
6895             for pci_dev in pci_manager.get_instance_pci_devs(instance):
6896                 guest.add_device(self._get_guest_pci_device(pci_dev))
6897         else:
6898             # PCI devices is only supported for QEMU/KVM hypervisor
6899             if pci_manager.get_instance_pci_devs(instance, 'all'):
6900                 raise exception.PciDeviceUnsupportedHypervisor(
6901                     type=CONF.libvirt.virt_type
6902                 )
6903 
6904     def _guest_add_accel_pci_devices(self, guest, accel_info):
6905         """Add all accelerator PCI functions from ARQ list."""
6906         for arq in accel_info:
6907             dev = vconfig.LibvirtConfigGuestHostdevPCI()
6908             pci_addr = arq['attach_handle_info']
6909             dev.domain, dev.bus, dev.slot, dev.function = (
6910                 pci_addr['domain'], pci_addr['bus'],
6911                 pci_addr['device'], pci_addr['function'])
6912             self._set_managed_mode(dev)
6913 
6914             guest.add_device(dev)
6915 
6916     @staticmethod
6917     def _guest_add_video_device(guest):
6918         if CONF.libvirt.virt_type == 'lxc':
6919             return False
6920 
6921         # NB some versions of libvirt support both SPICE and VNC
6922         # at the same time. We're not trying to second guess which
6923         # those versions are. We'll just let libvirt report the
6924         # errors appropriately if the user enables both.
6925         add_video_driver = False
6926 
6927         if CONF.vnc.enabled:
6928             graphics = vconfig.LibvirtConfigGuestGraphics()
6929             graphics.type = "vnc"
6930             graphics.listen = CONF.vnc.server_listen
6931             guest.add_device(graphics)
6932             add_video_driver = True
6933 
6934         if CONF.spice.enabled:
6935             graphics = vconfig.LibvirtConfigGuestGraphics()
6936             graphics.type = "spice"
6937             graphics.listen = CONF.spice.server_listen
6938             guest.add_device(graphics)
6939             add_video_driver = True
6940 
6941         return add_video_driver
6942 
6943     def _guest_add_pointer_device(self, guest, image_meta):
6944         """Build the pointer device to add to the instance.
6945 
6946         The configuration is determined by examining the 'hw_input_bus' image
6947         metadata property, the 'hw_pointer_model' image metadata property, and
6948         the '[DEFAULT] pointer_model' config option in that order.
6949         """
6950         pointer_bus = image_meta.properties.get('hw_input_bus')
6951         pointer_model = image_meta.properties.get('hw_pointer_model')
6952 
6953         if pointer_bus:
6954             pointer_model = 'tablet'
6955             pointer_bus = pointer_bus
6956         elif pointer_model or CONF.pointer_model == 'usbtablet':
6957             # Handle the legacy 'hw_pointer_model' image metadata property
6958             pointer_model = 'tablet'
6959             pointer_bus = 'usb'
6960         else:
6961             # If the user hasn't requested anything and the host config says to
6962             # use something other than a USB tablet, there's nothing to do
6963             return
6964 
6965         # For backward compatibility, we don't want to error out if the host
6966         # configuration requests a USB tablet but the virtual machine mode is
6967         # not configured as HVM.
6968         if guest.os_type != fields.VMMode.HVM:
6969             LOG.warning(
6970                 'USB tablet requested for guests on non-HVM host; '
6971                 'in order to accept this request the machine mode should '
6972                 'be configured as HVM.')
6973             return
6974 
6975         # Ditto for using a USB tablet when the SPICE agent is enabled, since
6976         # that has a paravirt mouse builtin which drastically reduces overhead;
6977         # this only applies if VNC is not also enabled though, since that still
6978         # needs the device
6979         if (
6980             CONF.spice.enabled and CONF.spice.agent_enabled and
6981             not CONF.vnc.enabled
6982         ):
6983             LOG.warning(
6984                 'USB tablet requested for guests but the SPICE agent is '
6985                 'enabled; ignoring request in favour of default '
6986                 'configuration.')
6987             return
6988 
6989         pointer = vconfig.LibvirtConfigGuestInput()
6990         pointer.type = pointer_model
6991         pointer.bus = pointer_bus
6992         guest.add_device(pointer)
6993 
6994         # returned for unit testing purposes
6995         return pointer
6996 
6997     def _guest_add_keyboard_device(self, guest, image_meta):
6998         """Add keyboard for graphical console use."""
6999         bus = image_meta.properties.get('hw_input_bus')
7000 
7001         if not bus:
7002             # AArch64 doesn't provide a default keyboard so we explicitly add
7003             # one; for everything else we rely on default (e.g. for x86,
7004             # libvirt will automatically add a PS2 keyboard)
7005             # TODO(stephenfin): We might want to do this for other non-x86
7006             # architectures
7007             arch = libvirt_utils.get_arch(image_meta)
7008             if arch != fields.Architecture.AARCH64:
7009                 return None
7010 
7011             bus = 'usb'
7012 
7013         keyboard = vconfig.LibvirtConfigGuestInput()
7014         keyboard.type = 'keyboard'
7015         keyboard.bus = bus
7016         guest.add_device(keyboard)
7017 
7018         # returned for unit testing purposes
7019         return keyboard
7020 
7021     def _get_guest_xml(self, context, instance, network_info, disk_info,
7022                        image_meta, rescue=None,
7023                        block_device_info=None,
7024                        mdevs=None, accel_info=None):
7025         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
7026         # this ahead of time so that we don't acquire it while also
7027         # holding the logging lock.
7028         network_info_str = str(network_info)
7029         msg = ('Start _get_guest_xml '
7030                'network_info=%(network_info)s '
7031                'disk_info=%(disk_info)s '
7032                'image_meta=%(image_meta)s rescue=%(rescue)s '
7033                'block_device_info=%(block_device_info)s' %
7034                {'network_info': network_info_str, 'disk_info': disk_info,
7035                 'image_meta': image_meta, 'rescue': rescue,
7036                 'block_device_info': block_device_info})
7037         # NOTE(mriedem): block_device_info can contain auth_password so we
7038         # need to sanitize the password in the message.
7039         LOG.debug(strutils.mask_password(msg), instance=instance)
7040         conf = self._get_guest_config(instance, network_info, image_meta,
7041                                       disk_info, rescue, block_device_info,
7042                                       context, mdevs, accel_info)
7043         xml = conf.to_xml()
7044 
7045         LOG.debug('End _get_guest_xml xml=%(xml)s',
7046                   {'xml': xml}, instance=instance)
7047         return xml
7048 
7049     def get_info(self, instance, use_cache=True):
7050         """Retrieve information from libvirt for a specific instance.
7051 
7052         If a libvirt error is encountered during lookup, we might raise a
7053         NotFound exception or Error exception depending on how severe the
7054         libvirt error is.
7055 
7056         :param instance: nova.objects.instance.Instance object
7057         :param use_cache: unused in this driver
7058         :returns: An InstanceInfo object
7059         """
7060         guest = self._host.get_guest(instance)
7061         # Kind of ugly but we need to pass host to get_info as for a
7062         # workaround, see libvirt/compat.py
7063         return guest.get_info(self._host)
7064 
7065     def _create_domain_setup_lxc(self, context, instance, image_meta,
7066                                  block_device_info):
7067         inst_path = libvirt_utils.get_instance_path(instance)
7068         block_device_mapping = driver.block_device_info_get_mapping(
7069             block_device_info)
7070         root_disk = block_device.get_root_bdm(block_device_mapping)
7071         if root_disk:
7072             self._connect_volume(context, root_disk['connection_info'],
7073                                  instance)
7074             disk_path = root_disk['connection_info']['data']['device_path']
7075 
7076             # NOTE(apmelton) - Even though the instance is being booted from a
7077             # cinder volume, it is still presented as a local block device.
7078             # LocalBlockImage is used here to indicate that the instance's
7079             # disk is backed by a local block device.
7080             image_model = imgmodel.LocalBlockImage(disk_path)
7081         else:
7082             root_disk = self.image_backend.by_name(instance, 'disk')
7083             image_model = root_disk.get_model(self._conn)
7084 
7085         container_dir = os.path.join(inst_path, 'rootfs')
7086         fileutils.ensure_tree(container_dir)
7087         rootfs_dev = disk_api.setup_container(image_model,
7088                                               container_dir=container_dir)
7089 
7090         try:
7091             # Save rootfs device to disconnect it when deleting the instance
7092             if rootfs_dev:
7093                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
7094             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
7095                 id_maps = self._get_guest_idmaps()
7096                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
7097         except Exception:
7098             with excutils.save_and_reraise_exception():
7099                 self._create_domain_cleanup_lxc(instance)
7100 
7101     def _create_domain_cleanup_lxc(self, instance):
7102         inst_path = libvirt_utils.get_instance_path(instance)
7103         container_dir = os.path.join(inst_path, 'rootfs')
7104 
7105         try:
7106             state = self.get_info(instance).state
7107         except exception.InstanceNotFound:
7108             # The domain may not be present if the instance failed to start
7109             state = None
7110 
7111         if state == power_state.RUNNING:
7112             # NOTE(uni): Now the container is running with its own private
7113             # mount namespace and so there is no need to keep the container
7114             # rootfs mounted in the host namespace
7115             LOG.debug('Attempting to unmount container filesystem: %s',
7116                       container_dir, instance=instance)
7117             disk_api.clean_lxc_namespace(container_dir=container_dir)
7118         else:
7119             disk_api.teardown_container(container_dir=container_dir)
7120 
7121     @contextlib.contextmanager
7122     def _lxc_disk_handler(self, context, instance, image_meta,
7123                           block_device_info):
7124         """Context manager to handle the pre and post instance boot,
7125            LXC specific disk operations.
7126 
7127            An image or a volume path will be prepared and setup to be
7128            used by the container, prior to starting it.
7129            The disk will be disconnected and unmounted if a container has
7130            failed to start.
7131         """
7132 
7133         if CONF.libvirt.virt_type != 'lxc':
7134             yield
7135             return
7136 
7137         self._create_domain_setup_lxc(context, instance, image_meta,
7138                                       block_device_info)
7139 
7140         try:
7141             yield
7142         finally:
7143             self._create_domain_cleanup_lxc(instance)
7144 
7145     def _create_guest(
7146         self,
7147         context: nova_context.RequestContext,
7148         xml: str,
7149         instance: 'objects.Instance',
7150         power_on: bool = True,
7151         pause: bool = False,
7152         post_xml_callback: ty.Callable = None,
7153     ) -> libvirt_guest.Guest:
7154         """Create a Guest from XML.
7155 
7156         Create a Guest, which in turn creates a libvirt domain, from XML,
7157         optionally starting it after creation.
7158 
7159         :returns guest.Guest: Created guest.
7160         """
7161         libvirt_secret = None
7162         # determine whether vTPM is in use and, if so, create the secret
7163         if CONF.libvirt.swtpm_enabled and hardware.get_vtpm_constraint(
7164             instance.flavor, instance.image_meta,
7165         ):
7166             secret_uuid, passphrase = crypto.ensure_vtpm_secret(
7167                 context, instance)
7168             libvirt_secret = self._host.create_secret(
7169                 'vtpm', instance.uuid, password=passphrase,
7170                 uuid=secret_uuid)
7171 
7172         try:
7173             guest = libvirt_guest.Guest.create(xml, self._host)
7174             if post_xml_callback is not None:
7175                 post_xml_callback()
7176 
7177             if power_on or pause:
7178                 guest.launch(pause=pause)
7179 
7180             return guest
7181         finally:
7182             if libvirt_secret is not None:
7183                 libvirt_secret.undefine()
7184 
7185     def _neutron_failed_callback(self, event_name, instance):
7186         LOG.error('Neutron Reported failure on event '
7187                   '%(event)s for instance %(uuid)s',
7188                   {'event': event_name, 'uuid': instance.uuid},
7189                   instance=instance)
7190         if CONF.vif_plugging_is_fatal:
7191             raise exception.VirtualInterfaceCreateException()
7192 
7193     def _get_neutron_events(self, network_info):
7194         # NOTE(danms): We need to collect any VIFs that are currently
7195         # down that we expect a down->up event for. Anything that is
7196         # already up will not undergo that transition, and for
7197         # anything that might be stale (cache-wise) assume it's
7198         # already up so we don't block on it.
7199         return [('network-vif-plugged', vif['id'])
7200                 for vif in network_info if vif.get('active', True) is False]
7201 
7202     def _cleanup_failed_start(self, context, instance, network_info,
7203                               block_device_info, guest,
7204                               cleanup_instance_dir=False,
7205                               cleanup_instance_disks=False):
7206         try:
7207             if guest and guest.is_active():
7208                 guest.poweroff()
7209         finally:
7210             self._cleanup(context, instance, network_info,
7211                           block_device_info=block_device_info,
7212                           destroy_vifs=True,
7213                           cleanup_instance_dir=cleanup_instance_dir,
7214                           cleanup_instance_disks=cleanup_instance_disks)
7215 
7216     def _create_guest_with_network(self, context, xml, instance, network_info,
7217                                    block_device_info, power_on=True,
7218                                    vifs_already_plugged=False,
7219                                    post_xml_callback=None,
7220                                    external_events=None,
7221                                    cleanup_instance_dir=False,
7222                                    cleanup_instance_disks=False):
7223         """Do required network setup and create domain."""
7224 
7225         timeout = CONF.vif_plugging_timeout
7226         if (
7227             CONF.libvirt.virt_type in ('kvm', 'qemu') and
7228             not vifs_already_plugged and power_on and timeout
7229         ):
7230             events = (external_events if external_events
7231                       else self._get_neutron_events(network_info))
7232         else:
7233             events = []
7234 
7235         pause = bool(events)
7236         guest: libvirt_guest.Guest = None
7237         try:
7238             with self.virtapi.wait_for_instance_event(
7239                     instance, events, deadline=timeout,
7240                     error_callback=self._neutron_failed_callback):
7241                 self.plug_vifs(instance, network_info)
7242                 with self._lxc_disk_handler(context, instance,
7243                                             instance.image_meta,
7244                                             block_device_info):
7245                     guest = self._create_guest(
7246                         context, xml, instance,
7247                         pause=pause, power_on=power_on,
7248                         post_xml_callback=post_xml_callback)
7249         except exception.VirtualInterfaceCreateException:
7250             # Neutron reported failure and we didn't swallow it, so
7251             # bail here
7252             with excutils.save_and_reraise_exception():
7253                 self._cleanup_failed_start(
7254                     context, instance, network_info, block_device_info, guest,
7255                     cleanup_instance_dir=cleanup_instance_dir,
7256                     cleanup_instance_disks=cleanup_instance_disks)
7257         except eventlet.timeout.Timeout:
7258             # We never heard from Neutron
7259             LOG.warning('Timeout waiting for %(events)s for '
7260                         'instance with vm_state %(vm_state)s and '
7261                         'task_state %(task_state)s.',
7262                         {'events': events,
7263                          'vm_state': instance.vm_state,
7264                          'task_state': instance.task_state},
7265                         instance=instance)
7266             if CONF.vif_plugging_is_fatal:
7267                 self._cleanup_failed_start(
7268                     context, instance, network_info, block_device_info, guest,
7269                     cleanup_instance_dir=cleanup_instance_dir,
7270                     cleanup_instance_disks=cleanup_instance_disks)
7271                 raise exception.VirtualInterfaceCreateException()
7272         except Exception:
7273             # Any other error, be sure to clean up
7274             LOG.error('Failed to start libvirt guest', instance=instance)
7275             with excutils.save_and_reraise_exception():
7276                 self._cleanup_failed_start(
7277                     context, instance, network_info, block_device_info, guest,
7278                     cleanup_instance_dir=cleanup_instance_dir,
7279                     cleanup_instance_disks=cleanup_instance_disks)
7280         # Resume only if domain has been paused
7281         if pause:
7282             guest.resume()
7283         return guest
7284 
7285     def _get_pcpu_available(self):
7286         """Get number of host cores to be used for PCPUs.
7287 
7288         :returns: The number of host cores to be used for PCPUs.
7289         """
7290         if not CONF.compute.cpu_dedicated_set:
7291             return set()
7292 
7293         online_cpus = self._host.get_online_cpus()
7294         dedicated_cpus = hardware.get_cpu_dedicated_set()
7295 
7296         if not dedicated_cpus.issubset(online_cpus):
7297             msg = _("Invalid '[compute] cpu_dedicated_set' config: one or "
7298                     "more of the configured CPUs is not online. Online "
7299                     "cpuset(s): %(online)s, configured cpuset(s): %(req)s")
7300             raise exception.Invalid(msg % {
7301                 'online': sorted(online_cpus),
7302                 'req': sorted(dedicated_cpus)})
7303 
7304         return dedicated_cpus
7305 
7306     def _get_vcpu_available(self):
7307         """Get host cores to be used for VCPUs.
7308 
7309         :returns: A list of host CPU cores that can be used for VCPUs.
7310         """
7311         online_cpus = self._host.get_online_cpus()
7312 
7313         # NOTE(stephenfin): The use of the legacy 'vcpu_pin_set' option happens
7314         # if it's defined, regardless of whether '[compute] cpu_shared_set' is
7315         # also configured. This is legacy behavior required for upgrades that
7316         # should be removed in the future, when we can rely exclusively on
7317         # '[compute] cpu_shared_set'.
7318         if CONF.vcpu_pin_set:
7319             # TODO(stephenfin): Remove this in U
7320             shared_cpus = hardware.get_vcpu_pin_set()
7321         elif CONF.compute.cpu_shared_set:
7322             shared_cpus = hardware.get_cpu_shared_set()
7323         elif CONF.compute.cpu_dedicated_set:
7324             return set()
7325         else:
7326             return online_cpus
7327 
7328         if not shared_cpus.issubset(online_cpus):
7329             msg = _("Invalid '%(config_opt)s' config: one or "
7330                     "more of the configured CPUs is not online. Online "
7331                     "cpuset(s): %(online)s, configured cpuset(s): %(req)s")
7332 
7333             if CONF.vcpu_pin_set:
7334                 config_opt = 'vcpu_pin_set'
7335             else:  # CONF.compute.cpu_shared_set
7336                 config_opt = '[compute] cpu_shared_set'
7337 
7338             raise exception.Invalid(msg % {
7339                 'config_opt': config_opt,
7340                 'online': sorted(online_cpus),
7341                 'req': sorted(shared_cpus)})
7342 
7343         return shared_cpus
7344 
7345     @staticmethod
7346     def _get_local_gb_info():
7347         """Get local storage info of the compute node in GB.
7348 
7349         :returns: A dict containing:
7350              :total: How big the overall usable filesystem is (in gigabytes)
7351              :free: How much space is free (in gigabytes)
7352              :used: How much space is used (in gigabytes)
7353         """
7354 
7355         if CONF.libvirt.images_type == 'lvm':
7356             info = lvm.get_volume_group_info(
7357                                CONF.libvirt.images_volume_group)
7358         elif CONF.libvirt.images_type == 'rbd':
7359             info = rbd_utils.RBDDriver().get_pool_info()
7360         else:
7361             info = libvirt_utils.get_fs_info(CONF.instances_path)
7362 
7363         for (k, v) in info.items():
7364             info[k] = v / units.Gi
7365 
7366         return info
7367 
7368     def _get_vcpu_used(self):
7369         """Get vcpu usage number of physical computer.
7370 
7371         :returns: The total number of vcpu(s) that are currently being used.
7372 
7373         """
7374 
7375         total = 0
7376 
7377         # Not all libvirt drivers will support the get_vcpus_info()
7378         #
7379         # For example, LXC does not have a concept of vCPUs, while
7380         # QEMU (TCG) traditionally handles all vCPUs in a single
7381         # thread. So both will report an exception when the vcpus()
7382         # API call is made. In such a case we should report the
7383         # guest as having 1 vCPU, since that lets us still do
7384         # CPU over commit calculations that apply as the total
7385         # guest count scales.
7386         #
7387         # It is also possible that we might see an exception if
7388         # the guest is just in middle of shutting down. Technically
7389         # we should report 0 for vCPU usage in this case, but we
7390         # we can't reliably distinguish the vcpu not supported
7391         # case from the just shutting down case. Thus we don't know
7392         # whether to report 1 or 0 for vCPU count.
7393         #
7394         # Under-reporting vCPUs is bad because it could conceivably
7395         # let the scheduler place too many guests on the host. Over-
7396         # reporting vCPUs is not a problem as it'll auto-correct on
7397         # the next refresh of usage data.
7398         #
7399         # Thus when getting an exception we always report 1 as the
7400         # vCPU count, as the least worst value.
7401         for guest in self._host.list_guests():
7402             try:
7403                 vcpus = guest.get_vcpus_info()
7404                 total += len(list(vcpus))
7405             except libvirt.libvirtError:
7406                 total += 1
7407             # NOTE(gtt116): give other tasks a chance.
7408             greenthread.sleep(0)
7409         return total
7410 
7411     def _get_supported_vgpu_types(self):
7412         if not CONF.devices.enabled_vgpu_types:
7413             return []
7414 
7415         # Make sure we register all the types as the compute service could
7416         # be calling this method before init_host()
7417         if len(CONF.devices.enabled_vgpu_types) > 1:
7418             nova.conf.devices.register_dynamic_opts(CONF)
7419 
7420         for vgpu_type in CONF.devices.enabled_vgpu_types:
7421             group = getattr(CONF, 'vgpu_%s' % vgpu_type, None)
7422             if group is None or not group.device_addresses:
7423                 first_type = CONF.devices.enabled_vgpu_types[0]
7424                 if len(CONF.devices.enabled_vgpu_types) > 1:
7425                     # Only provide the warning if the operator provided more
7426                     # than one type as it's not needed to provide groups
7427                     # if you only use one vGPU type.
7428                     msg = ("The vGPU type '%(type)s' was listed in '[devices] "
7429                            "enabled_vgpu_types' but no corresponding "
7430                            "'[vgpu_%(type)s]' group or "
7431                            "'[vgpu_%(type)s] device_addresses' "
7432                            "option was defined. Only the first type "
7433                            "'%(ftype)s' will be used." % {'type': vgpu_type,
7434                                                          'ftype': first_type})
7435                     LOG.warning(msg)
7436                 # We need to reset the mapping table that we started to provide
7437                 # keys and values from previously processed vGPUs but since
7438                 # there is a problem for this vGPU type, we only want to
7439                 # support only the first type.
7440                 self.pgpu_type_mapping.clear()
7441                 return [first_type]
7442             for device_address in group.device_addresses:
7443                 if device_address in self.pgpu_type_mapping:
7444                     raise exception.InvalidLibvirtGPUConfig(
7445                         reason="duplicate types for PCI ID %s" % device_address
7446                     )
7447                 # Just checking whether the operator fat-fingered the address.
7448                 # If it's wrong, it will return an exception
7449                 try:
7450                     pci_utils.parse_address(device_address)
7451                 except exception.PciDeviceWrongAddressFormat:
7452                     raise exception.InvalidLibvirtGPUConfig(
7453                         reason="incorrect PCI address: %s" % device_address
7454                     )
7455                 self.pgpu_type_mapping[device_address] = vgpu_type
7456         return CONF.devices.enabled_vgpu_types
7457 
7458     def _get_vgpu_type_per_pgpu(self, device_address):
7459         """Provides the vGPU type the pGPU supports.
7460 
7461         :param device_address: the libvirt PCI device name,
7462                                eg.'pci_0000_84_00_0'
7463         """
7464         # Bail out quickly if we don't support vGPUs
7465         if not self.supported_vgpu_types:
7466             return
7467 
7468         if len(self.supported_vgpu_types) == 1:
7469             # The operator wanted to only support one single type so we can
7470             # blindly return it for every single pGPU
7471             return self.supported_vgpu_types[0]
7472         # The libvirt name is like 'pci_0000_84_00_0'
7473         try:
7474             device_address = "{}:{}:{}.{}".format(
7475                 *device_address[4:].split('_'))
7476             # Validates whether it's a PCI ID...
7477             pci_utils.parse_address(device_address)
7478         # .format() can return IndexError
7479         except (exception.PciDeviceWrongAddressFormat, IndexError):
7480             # this is not a valid PCI address
7481             LOG.warning("The PCI address %s was invalid for getting the "
7482                         "related vGPU type", device_address)
7483             return
7484         try:
7485             return self.pgpu_type_mapping.get(device_address)
7486         except KeyError:
7487             LOG.warning("No vGPU type was configured for PCI address: %s",
7488                         device_address)
7489             # We accept to return None instead of raising an exception
7490             # because we prefer the callers to return the existing exceptions
7491             # in case we can't find a specific pGPU
7492             return
7493 
7494     def _count_mediated_devices(self, enabled_vgpu_types):
7495         """Counts the sysfs objects (handles) that represent a mediated device
7496         and filtered by $enabled_vgpu_types.
7497 
7498         Those handles can be in use by a libvirt guest or not.
7499 
7500         :param enabled_vgpu_types: list of enabled VGPU types on this host
7501         :returns: dict, keyed by parent GPU libvirt PCI device ID, of number of
7502         mdev device handles for that GPU
7503         """
7504 
7505         counts_per_parent: ty.Dict[str, int] = collections.defaultdict(int)
7506         mediated_devices = self._get_mediated_devices(types=enabled_vgpu_types)
7507         for mdev in mediated_devices:
7508             parent_vgpu_type = self._get_vgpu_type_per_pgpu(mdev['parent'])
7509             if mdev['type'] != parent_vgpu_type:
7510                 # Even if some mdev was created for another vGPU type, just
7511                 # verify all the mdevs related to the type that their pGPU
7512                 # has
7513                 continue
7514             counts_per_parent[mdev['parent']] += 1
7515         return counts_per_parent
7516 
7517     def _count_mdev_capable_devices(self, enabled_vgpu_types):
7518         """Counts the mdev-capable devices on this host filtered by
7519         $enabled_vgpu_types.
7520 
7521         :param enabled_vgpu_types: list of enabled VGPU types on this host
7522         :returns: dict, keyed by device name, to an integer count of available
7523             instances of each type per device
7524         """
7525         mdev_capable_devices = self._get_mdev_capable_devices(
7526             types=enabled_vgpu_types)
7527         counts_per_dev: ty.Dict[str, int] = collections.defaultdict(int)
7528         for dev in mdev_capable_devices:
7529             # dev_id is the libvirt name for the PCI device,
7530             # eg. pci_0000_84_00_0 which matches a PCI address of 0000:84:00.0
7531             dev_name = dev['dev_id']
7532             dev_supported_type = self._get_vgpu_type_per_pgpu(dev_name)
7533             for _type in dev['types']:
7534                 if _type != dev_supported_type:
7535                     # This is not the type the operator wanted to support for
7536                     # this physical GPU
7537                     continue
7538                 available = dev['types'][_type]['availableInstances']
7539                 # NOTE(sbauza): Even if we support multiple types, Nova will
7540                 # only use one per physical GPU.
7541                 counts_per_dev[dev_name] += available
7542         return counts_per_dev
7543 
7544     def _get_gpu_inventories(self):
7545         """Returns the inventories for each physical GPU for a specific type
7546         supported by the enabled_vgpu_types CONF option.
7547 
7548         :returns: dict, keyed by libvirt PCI name, of dicts like:
7549                 {'pci_0000_84_00_0':
7550                     {'total': $TOTAL,
7551                      'min_unit': 1,
7552                      'max_unit': $TOTAL,
7553                      'step_size': 1,
7554                      'reserved': 0,
7555                      'allocation_ratio': 1.0,
7556                     }
7557                 }
7558         """
7559 
7560         # Bail out early if operator doesn't care about providing vGPUs
7561         enabled_vgpu_types = self.supported_vgpu_types
7562         if not enabled_vgpu_types:
7563             return {}
7564         inventories = {}
7565         count_per_parent = self._count_mediated_devices(enabled_vgpu_types)
7566         for dev_name, count in count_per_parent.items():
7567             inventories[dev_name] = {'total': count}
7568         # Filter how many available mdevs we can create for all the supported
7569         # types.
7570         count_per_dev = self._count_mdev_capable_devices(enabled_vgpu_types)
7571         # Combine the counts into the dict that we return to the caller.
7572         for dev_name, count in count_per_dev.items():
7573             inv_per_parent = inventories.setdefault(
7574                 dev_name, {'total': 0})
7575             inv_per_parent['total'] += count
7576             inv_per_parent.update({
7577                 'min_unit': 1,
7578                 'step_size': 1,
7579                 'reserved': 0,
7580                 # NOTE(sbauza): There is no sense to have a ratio but 1.0
7581                 # since we can't overallocate vGPU resources
7582                 'allocation_ratio': 1.0,
7583                 # FIXME(sbauza): Some vendors could support only one
7584                 'max_unit': inv_per_parent['total'],
7585             })
7586 
7587         return inventories
7588 
7589     def _get_instance_capabilities(self):
7590         """Get hypervisor instance capabilities
7591 
7592         Returns a list of tuples that describe instances the
7593         hypervisor is capable of hosting.  Each tuple consists
7594         of the triplet (arch, hypervisor_type, vm_mode).
7595 
7596         :returns: List of tuples describing instance capabilities
7597         """
7598         caps = self._host.get_capabilities()
7599         instance_caps = list()
7600         for g in caps.guests:
7601             for domain_type in g.domains:
7602                 try:
7603                     instance_cap = (
7604                         fields.Architecture.canonicalize(g.arch),
7605                         fields.HVType.canonicalize(domain_type),
7606                         fields.VMMode.canonicalize(g.ostype))
7607                     instance_caps.append(instance_cap)
7608                 except exception.InvalidArchitectureName:
7609                     # NOTE(danms): Libvirt is exposing a guest arch that nova
7610                     # does not even know about. Avoid aborting here and
7611                     # continue to process the rest.
7612                     pass
7613 
7614         return instance_caps
7615 
7616     def _get_cpu_info(self):
7617         """Get cpuinfo information.
7618 
7619         Obtains cpu feature from virConnect.getCapabilities.
7620 
7621         :return: see above description
7622 
7623         """
7624 
7625         caps = self._host.get_capabilities()
7626         cpu_info = dict()
7627 
7628         cpu_info['arch'] = caps.host.cpu.arch
7629         cpu_info['model'] = caps.host.cpu.model
7630         cpu_info['vendor'] = caps.host.cpu.vendor
7631 
7632         topology = dict()
7633         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
7634         topology['sockets'] = caps.host.cpu.sockets
7635         topology['cores'] = caps.host.cpu.cores
7636         topology['threads'] = caps.host.cpu.threads
7637         cpu_info['topology'] = topology
7638 
7639         features = set()
7640         for f in caps.host.cpu.features:
7641             features.add(f.name)
7642         cpu_info['features'] = features
7643         return cpu_info
7644 
7645     # TODO(stephenfin): Move this to 'host.py'
7646     def _get_pci_passthrough_devices(self):
7647         """Get host PCI devices information.
7648 
7649         Obtains pci devices information from libvirt, and returns
7650         as a JSON string.
7651 
7652         Each device information is a dictionary, with mandatory keys
7653         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
7654         'label' and other optional device specific information.
7655 
7656         Refer to the objects/pci_device.py for more idea of these keys.
7657 
7658         :returns: a JSON string containing a list of the assignable PCI
7659                   devices information
7660         """
7661         dev_flags = (
7662             libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_NET |
7663             libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_PCI_DEV
7664         )
7665         if self._host.has_min_version(
7666             lv_ver=MIN_LIBVIRT_VDPA, hv_ver=MIN_QEMU_VDPA,
7667         ):
7668             dev_flags |= libvirt.VIR_CONNECT_LIST_NODE_DEVICES_CAP_VDPA
7669 
7670         devices = {
7671             dev.name(): dev for dev in
7672             self._host.list_all_devices(flags=dev_flags)
7673         }
7674         net_devs = [dev for dev in devices.values() if "net" in dev.listCaps()]
7675         vdpa_devs = [
7676             dev for dev in devices.values() if "vdpa" in dev.listCaps()
7677         ]
7678         pci_info = [
7679             self._host._get_pcidev_info(name, dev, net_devs, vdpa_devs)
7680             for name, dev in devices.items() if "pci" in dev.listCaps()
7681         ]
7682         return jsonutils.dumps(pci_info)
7683 
7684     def _get_mdev_capabilities_for_dev(self, devname, types=None):
7685         """Returns a dict of MDEV capable device with the ID as first key
7686         and then a list of supported types, each of them being a dict.
7687 
7688         :param types: Only return those specific types.
7689         """
7690         virtdev = self._host.device_lookup_by_name(devname)
7691         xmlstr = virtdev.XMLDesc(0)
7692         cfgdev = vconfig.LibvirtConfigNodeDevice()
7693         cfgdev.parse_str(xmlstr)
7694 
7695         device = {
7696             "dev_id": cfgdev.name,
7697             "types": {},
7698             "vendor_id": cfgdev.pci_capability.vendor_id,
7699         }
7700         for mdev_cap in cfgdev.pci_capability.mdev_capability:
7701             for cap in mdev_cap.mdev_types:
7702                 if not types or cap['type'] in types:
7703                     device["types"].update({cap['type']: {
7704                         'availableInstances': cap['availableInstances'],
7705                         # This attribute is optional
7706                         'name': cap.get('name'),
7707                         'deviceAPI': cap['deviceAPI']}})
7708         return device
7709 
7710     def _get_mdev_capable_devices(self, types=None):
7711         """Get host devices supporting mdev types.
7712 
7713         Obtain devices information from libvirt and returns a list of
7714         dictionaries.
7715 
7716         :param types: Filter only devices supporting those types.
7717         """
7718         dev_names = self._host.list_mdev_capable_devices() or []
7719         mdev_capable_devices = []
7720         for name in dev_names:
7721             device = self._get_mdev_capabilities_for_dev(name, types)
7722             if not device["types"]:
7723                 continue
7724             mdev_capable_devices.append(device)
7725         return mdev_capable_devices
7726 
7727     def _get_mediated_device_information(self, devname):
7728         """Returns a dict of a mediated device."""
7729         virtdev = self._host.device_lookup_by_name(devname)
7730         xmlstr = virtdev.XMLDesc(0)
7731         cfgdev = vconfig.LibvirtConfigNodeDevice()
7732         cfgdev.parse_str(xmlstr)
7733 
7734         device = {
7735             "dev_id": cfgdev.name,
7736             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
7737             "uuid": libvirt_utils.mdev_name2uuid(cfgdev.name),
7738             # the physical GPU PCI device
7739             "parent": cfgdev.parent,
7740             "type": cfgdev.mdev_information.type,
7741             "iommu_group": cfgdev.mdev_information.iommu_group,
7742         }
7743         return device
7744 
7745     def _get_mediated_devices(self, types=None):
7746         """Get host mediated devices.
7747 
7748         Obtain devices information from libvirt and returns a list of
7749         dictionaries.
7750 
7751         :param types: Filter only devices supporting those types.
7752         """
7753         dev_names = self._host.list_mediated_devices() or []
7754         mediated_devices = []
7755         for name in dev_names:
7756             device = self._get_mediated_device_information(name)
7757             if not types or device["type"] in types:
7758                 mediated_devices.append(device)
7759         return mediated_devices
7760 
7761     def _get_all_assigned_mediated_devices(self, instance=None):
7762         """Lookup all instances from the host and return all the mediated
7763         devices that are assigned to a guest.
7764 
7765         :param instance: Only return mediated devices for that instance.
7766 
7767         :returns: A dictionary of keys being mediated device UUIDs and their
7768                   respective values the instance UUID of the guest using it.
7769                   Returns an empty dict if an instance is provided but not
7770                   found in the hypervisor.
7771         """
7772         allocated_mdevs = {}
7773         if instance:
7774             # NOTE(sbauza): In some cases (like a migration issue), the
7775             # instance can exist in the Nova database but libvirt doesn't know
7776             # about it. For such cases, the way to fix that is to hard reboot
7777             # the instance, which will recreate the libvirt guest.
7778             # For that reason, we need to support that case by making sure
7779             # we don't raise an exception if the libvirt guest doesn't exist.
7780             try:
7781                 guest = self._host.get_guest(instance)
7782             except exception.InstanceNotFound:
7783                 # Bail out early if libvirt doesn't know about it since we
7784                 # can't know the existing mediated devices
7785                 return {}
7786             guests = [guest]
7787         else:
7788             guests = self._host.list_guests(only_running=False)
7789         for guest in guests:
7790             cfg = guest.get_config()
7791             for device in cfg.devices:
7792                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
7793                     allocated_mdevs[device.uuid] = guest.uuid
7794         return allocated_mdevs
7795 
7796     @staticmethod
7797     def _vgpu_allocations(allocations):
7798         """Filtering only the VGPU allocations from a list of allocations.
7799 
7800         :param allocations: Information about resources allocated to the
7801                             instance via placement, of the form returned by
7802                             SchedulerReportClient.get_allocations_for_consumer.
7803         """
7804         if not allocations:
7805             # If no allocations, there is no vGPU request.
7806             return {}
7807         RC_VGPU = orc.VGPU
7808         vgpu_allocations = {}
7809         for rp in allocations:
7810             res = allocations[rp]['resources']
7811             if RC_VGPU in res and res[RC_VGPU] > 0:
7812                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
7813         return vgpu_allocations
7814 
7815     def _get_existing_mdevs_not_assigned(self, parent, requested_types=None):
7816         """Returns the already created mediated devices that are not assigned
7817         to a guest yet.
7818 
7819         :param parent: Filter out result for only mdevs from the parent device.
7820         :param requested_types: Filter out the result for only mediated devices
7821                                 having those types.
7822         """
7823         allocated_mdevs = self._get_all_assigned_mediated_devices()
7824         mdevs = self._get_mediated_devices(requested_types)
7825         available_mdevs = set()
7826         for mdev in mdevs:
7827             parent_vgpu_type = self._get_vgpu_type_per_pgpu(mdev['parent'])
7828             if mdev['type'] != parent_vgpu_type:
7829                 # This mdev is using a vGPU type that is not supported by the
7830                 # configuration option for its pGPU parent, so we can't use it.
7831                 continue
7832             # FIXME(sbauza): No longer accept the parent value to be nullable
7833             # once we fix the reshape functional test
7834             if parent is None or mdev['parent'] == parent:
7835                 available_mdevs.add(mdev["uuid"])
7836 
7837         available_mdevs -= set(allocated_mdevs)
7838         return available_mdevs
7839 
7840     def _create_new_mediated_device(self, parent, uuid=None):
7841         """Find a physical device that can support a new mediated device and
7842         create it.
7843 
7844         :param parent: The libvirt name of the parent GPU, eg. pci_0000_06_00_0
7845         :param uuid: The possible mdev UUID we want to create again
7846 
7847         :returns: the newly created mdev UUID or None if not possible
7848         """
7849         supported_types = self.supported_vgpu_types
7850         # Try to see if we can still create a new mediated device
7851         devices = self._get_mdev_capable_devices(supported_types)
7852         for device in devices:
7853             dev_name = device['dev_id']
7854             # FIXME(sbauza): No longer accept the parent value to be nullable
7855             # once we fix the reshape functional test
7856             if parent is not None and dev_name != parent:
7857                 # The device is not the one that was called, not creating
7858                 # the mdev
7859                 continue
7860             dev_supported_type = self._get_vgpu_type_per_pgpu(dev_name)
7861             if dev_supported_type and device['types'][
7862                     dev_supported_type]['availableInstances'] > 0:
7863                 # That physical GPU has enough room for a new mdev
7864                 # We need the PCI address, not the libvirt name
7865                 # The libvirt name is like 'pci_0000_84_00_0'
7866                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
7867                 chosen_mdev = nova.privsep.libvirt.create_mdev(
7868                     pci_addr, dev_supported_type, uuid=uuid)
7869                 return chosen_mdev
7870 
7871     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
7872     def _allocate_mdevs(self, allocations):
7873         """Returns a list of mediated device UUIDs corresponding to available
7874         resources we can assign to the guest(s) corresponding to the allocation
7875         requests passed as argument.
7876 
7877         That method can either find an existing but unassigned mediated device
7878         it can allocate, or create a new mediated device from a capable
7879         physical device if the latter has enough left capacity.
7880 
7881         :param allocations: Information about resources allocated to the
7882                             instance via placement, of the form returned by
7883                             SchedulerReportClient.get_allocations_for_consumer.
7884                             That code is supporting Placement API version 1.12
7885         """
7886         vgpu_allocations = self._vgpu_allocations(allocations)
7887         if not vgpu_allocations:
7888             return
7889         # TODO(sbauza): For the moment, we only support allocations for only
7890         # one pGPU.
7891         if len(vgpu_allocations) > 1:
7892             LOG.warning('More than one allocation was passed over to libvirt '
7893                         'while at the moment libvirt only supports one. Only '
7894                         'the first allocation will be looked up.')
7895         rp_uuid, alloc = next(iter(vgpu_allocations.items()))
7896         vgpus_asked = alloc['resources'][orc.VGPU]
7897 
7898         # Find if we allocated against a specific pGPU (and then the allocation
7899         # is made against a child RP) or any pGPU (in case the VGPU inventory
7900         # is still on the root RP)
7901         try:
7902             allocated_rp = self.provider_tree.data(rp_uuid)
7903         except ValueError:
7904             # The provider doesn't exist, return a better understandable
7905             # exception
7906             raise exception.ComputeResourcesUnavailable(
7907                 reason='vGPU resource is not available')
7908         # FIXME(sbauza): The functional reshape test assumes that we could
7909         # run _allocate_mdevs() against non-nested RPs but this is impossible
7910         # as all inventories have been reshaped *before now* since it's done
7911         # on init_host() (when the compute restarts or whatever else calls it).
7912         # That said, since fixing the functional test isn't easy yet, let's
7913         # assume we still support a non-nested RP for now.
7914         if allocated_rp.parent_uuid is None:
7915             # We are on a root RP
7916             parent_device = None
7917         else:
7918             rp_name = allocated_rp.name
7919             # There can be multiple roots, we need to find the root name
7920             # to guess the physical device name
7921             roots = list(self.provider_tree.roots)
7922             for root in roots:
7923                 if rp_name.startswith(root.name + '_'):
7924                     # The RP name convention is :
7925                     #    root_name + '_' + parent_device
7926                     parent_device = rp_name[len(root.name) + 1:]
7927                     break
7928             else:
7929                 LOG.warning(
7930                     "pGPU device name %(name)s can't be guessed from the "
7931                     "ProviderTree roots %(roots)s",
7932                     {'name': rp_name,
7933                      'roots': ', '.join([root.name for root in roots])})
7934                 # We f... have no idea what was the parent device
7935                 # If we can't find devices having available VGPUs, just raise
7936                 raise exception.ComputeResourcesUnavailable(
7937                     reason='vGPU resource is not available')
7938 
7939         supported_types = self.supported_vgpu_types
7940         # Which mediated devices are created but not assigned to a guest ?
7941         mdevs_available = self._get_existing_mdevs_not_assigned(
7942             parent_device, supported_types)
7943 
7944         chosen_mdevs = []
7945         for c in range(vgpus_asked):
7946             chosen_mdev = None
7947             if mdevs_available:
7948                 # Take the first available mdev
7949                 chosen_mdev = mdevs_available.pop()
7950             else:
7951                 chosen_mdev = self._create_new_mediated_device(parent_device)
7952             if not chosen_mdev:
7953                 # If we can't find devices having available VGPUs, just raise
7954                 raise exception.ComputeResourcesUnavailable(
7955                     reason='vGPU resource is not available')
7956             else:
7957                 chosen_mdevs.append(chosen_mdev)
7958         return chosen_mdevs
7959 
7960     def _detach_mediated_devices(self, guest):
7961         mdevs = guest.get_all_devices(
7962             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
7963         for mdev_cfg in mdevs:
7964             try:
7965                 guest.detach_device(mdev_cfg, live=True)
7966             except libvirt.libvirtError as ex:
7967                 error_code = ex.get_error_code()
7968                 # NOTE(sbauza): There is a pending issue with libvirt that
7969                 # doesn't allow to hot-unplug mediated devices. Let's
7970                 # short-circuit the suspend action and set the instance back
7971                 # to ACTIVE.
7972                 # TODO(sbauza): Once libvirt supports this, amend the resume()
7973                 # operation to support reallocating mediated devices.
7974                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
7975                     reason = _("Suspend is not supported for instances having "
7976                                "attached vGPUs.")
7977                     raise exception.InstanceFaultRollback(
7978                         exception.InstanceSuspendFailure(reason=reason))
7979                 else:
7980                     raise
7981 
7982     def _has_numa_support(self):
7983         # This means that the host can support LibvirtConfigGuestNUMATune
7984         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
7985         caps = self._host.get_capabilities()
7986 
7987         if (caps.host.cpu.arch in (fields.Architecture.I686,
7988                                    fields.Architecture.X86_64,
7989                                    fields.Architecture.AARCH64) and
7990                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
7991             return True
7992         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
7993                                      fields.Architecture.PPC64LE)):
7994             return True
7995 
7996         return False
7997 
7998     def _get_host_numa_topology(self):
7999         if not self._has_numa_support():
8000             return
8001 
8002         caps = self._host.get_capabilities()
8003         topology = caps.host.topology
8004 
8005         if topology is None or not topology.cells:
8006             return
8007 
8008         cells = []
8009 
8010         available_shared_cpus = self._get_vcpu_available()
8011         available_dedicated_cpus = self._get_pcpu_available()
8012 
8013         # NOTE(stephenfin): In an ideal world, if the operator had not
8014         # configured this host to report PCPUs using the '[compute]
8015         # cpu_dedicated_set' option, then we should not be able to used pinned
8016         # instances on this host. However, that would force operators to update
8017         # their configuration as part of the Stein -> Train upgrade or be
8018         # unable to schedule instances on the host. As a result, we need to
8019         # revert to legacy behavior and use 'vcpu_pin_set' for both VCPUs and
8020         # PCPUs.
8021         # TODO(stephenfin): Remove this in U
8022         if not available_dedicated_cpus and not (
8023                 CONF.compute.cpu_shared_set and not CONF.vcpu_pin_set):
8024             available_dedicated_cpus = available_shared_cpus
8025 
8026         def _get_reserved_memory_for_cell(self, cell_id, page_size):
8027             cell = self._reserved_hugepages.get(cell_id, {})
8028             return cell.get(page_size, 0)
8029 
8030         def _get_physnet_numa_affinity():
8031             affinities: ty.Dict[int, ty.Set[str]] = {
8032                 cell.id: set() for cell in topology.cells
8033             }
8034             for physnet in CONF.neutron.physnets:
8035                 # This will error out if the group is not registered, which is
8036                 # exactly what we want as that would be a bug
8037                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
8038 
8039                 if not group.numa_nodes:
8040                     msg = ("the physnet '%s' was listed in '[neutron] "
8041                            "physnets' but no corresponding "
8042                            "'[neutron_physnet_%s] numa_nodes' option was "
8043                            "defined." % (physnet, physnet))
8044                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
8045 
8046                 for node in group.numa_nodes:
8047                     if node not in affinities:
8048                         msg = ("node %d for physnet %s is not present in host "
8049                                "affinity set %r" % (node, physnet, affinities))
8050                         # The config option referenced an invalid node
8051                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
8052                     affinities[node].add(physnet)
8053 
8054             return affinities
8055 
8056         def _get_tunnel_numa_affinity():
8057             affinities = {cell.id: False for cell in topology.cells}
8058 
8059             for node in CONF.neutron_tunnel.numa_nodes:
8060                 if node not in affinities:
8061                     msg = ("node %d for tunneled networks is not present "
8062                            "in host affinity set %r" % (node, affinities))
8063                     # The config option referenced an invalid node
8064                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
8065                 affinities[node] = True
8066 
8067             return affinities
8068 
8069         physnet_affinities = _get_physnet_numa_affinity()
8070         tunnel_affinities = _get_tunnel_numa_affinity()
8071 
8072         for cell in topology.cells:
8073             cpus = set(cpu.id for cpu in cell.cpus)
8074 
8075             # NOTE(artom) We assume we'll never see hardware with multipe
8076             # sockets in a single NUMA node - IOW, the socket_id for all CPUs
8077             # in a single cell will be the same. To make that assumption
8078             # explicit, we leave the cell's socket_id as None if that's the
8079             # case.
8080             socket_id = None
8081             sockets = set([cpu.socket_id for cpu in cell.cpus])
8082             if len(sockets) == 1:
8083                 socket_id = sockets.pop()
8084             else:
8085                 LOG.warning('This host appears to have multiple sockets per '
8086                             'NUMA node. The `socket` PCI NUMA affinity '
8087                             'will not be supported.')
8088 
8089             cpuset = cpus & available_shared_cpus
8090             pcpuset = cpus & available_dedicated_cpus
8091 
8092             # de-duplicate and sort the list of CPU sibling sets
8093             siblings = sorted(
8094                 set(x) for x in set(
8095                     tuple(cpu.siblings) or () for cpu in cell.cpus
8096                 )
8097             )
8098 
8099             cpus &= available_shared_cpus | available_dedicated_cpus
8100             siblings = [sib & cpus for sib in siblings]
8101             # Filter out empty sibling sets that may be left
8102             siblings = [sib for sib in siblings if len(sib) > 0]
8103 
8104             mempages = [
8105                 objects.NUMAPagesTopology(
8106                     size_kb=pages.size,
8107                     total=pages.total,
8108                     used=0,
8109                     reserved=_get_reserved_memory_for_cell(
8110                         self, cell.id, pages.size))
8111                 for pages in cell.mempages]
8112 
8113             network_metadata = objects.NetworkMetadata(
8114                 physnets=physnet_affinities[cell.id],
8115                 tunneled=tunnel_affinities[cell.id])
8116 
8117             # NOTE(stephenfin): Note that we don't actually return any usage
8118             # information here. This is because this is handled by the resource
8119             # tracker via the 'update_available_resource' periodic task, which
8120             # loops through all instances and calculated usage accordingly
8121             cell = objects.NUMACell(
8122                 id=cell.id,
8123                 socket=socket_id,
8124                 cpuset=cpuset,
8125                 pcpuset=pcpuset,
8126                 memory=cell.memory / units.Ki,
8127                 cpu_usage=0,
8128                 pinned_cpus=set(),
8129                 memory_usage=0,
8130                 siblings=siblings,
8131                 mempages=mempages,
8132                 network_metadata=network_metadata)
8133             cells.append(cell)
8134 
8135         return objects.NUMATopology(cells=cells)
8136 
8137     def get_all_volume_usage(self, context, compute_host_bdms):
8138         """Return usage info for volumes attached to vms on
8139            a given host.
8140         """
8141         vol_usage = []
8142 
8143         for instance_bdms in compute_host_bdms:
8144             instance = instance_bdms['instance']
8145 
8146             for bdm in instance_bdms['instance_bdms']:
8147                 mountpoint = bdm['device_name']
8148                 if mountpoint.startswith('/dev/'):
8149                     mountpoint = mountpoint[5:]
8150                 volume_id = bdm['volume_id']
8151 
8152                 LOG.debug("Trying to get stats for the volume %s",
8153                           volume_id, instance=instance)
8154                 vol_stats = self.block_stats(instance, mountpoint)
8155 
8156                 if vol_stats:
8157                     stats = dict(volume=volume_id,
8158                                  instance=instance,
8159                                  rd_req=vol_stats[0],
8160                                  rd_bytes=vol_stats[1],
8161                                  wr_req=vol_stats[2],
8162                                  wr_bytes=vol_stats[3])
8163                     LOG.debug(
8164                         "Got volume usage stats for the volume=%(volume)s,"
8165                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
8166                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
8167                         stats, instance=instance)
8168                     vol_usage.append(stats)
8169 
8170         return vol_usage
8171 
8172     def block_stats(self, instance, disk_id):
8173         """Note that this function takes an instance name."""
8174         try:
8175             guest = self._host.get_guest(instance)
8176             dev = guest.get_block_device(disk_id)
8177             return dev.blockStats()
8178         except libvirt.libvirtError as e:
8179             errcode = e.get_error_code()
8180             LOG.info('Getting block stats failed, device might have '
8181                      'been detached. Instance=%(instance_name)s '
8182                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
8183                      {'instance_name': instance.name, 'disk': disk_id,
8184                       'errcode': errcode, 'e': e},
8185                      instance=instance)
8186         except exception.InstanceNotFound:
8187             LOG.info('Could not find domain in libvirt for instance %s. '
8188                      'Cannot get block stats for device', instance.name,
8189                      instance=instance)
8190 
8191     def update_provider_tree(self, provider_tree, nodename, allocations=None):
8192         """Update a ProviderTree object with current resource provider,
8193         inventory information and CPU traits.
8194 
8195         :param nova.compute.provider_tree.ProviderTree provider_tree:
8196             A nova.compute.provider_tree.ProviderTree object representing all
8197             the providers in the tree associated with the compute node, and any
8198             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
8199             trait) associated via aggregate with any of those providers (but
8200             not *their* tree- or aggregate-associated providers), as currently
8201             known by placement.
8202         :param nodename:
8203             String name of the compute node (i.e.
8204             ComputeNode.hypervisor_hostname) for which the caller is requesting
8205             updated provider information.
8206         :param allocations:
8207             Dict of allocation data of the form:
8208               { $CONSUMER_UUID: {
8209                     # The shape of each "allocations" dict below is identical
8210                     # to the return from GET /allocations/{consumer_uuid}
8211                     "allocations": {
8212                         $RP_UUID: {
8213                             "generation": $RP_GEN,
8214                             "resources": {
8215                                 $RESOURCE_CLASS: $AMOUNT,
8216                                 ...
8217                             },
8218                         },
8219                         ...
8220                     },
8221                     "project_id": $PROJ_ID,
8222                     "user_id": $USER_ID,
8223                     "consumer_generation": $CONSUMER_GEN,
8224                 },
8225                 ...
8226               }
8227             If None, and the method determines that any inventory needs to be
8228             moved (from one provider to another and/or to a different resource
8229             class), the ReshapeNeeded exception must be raised. Otherwise, this
8230             dict must be edited in place to indicate the desired final state of
8231             allocations.
8232         :raises ReshapeNeeded: If allocations is None and any inventory needs
8233             to be moved from one provider to another and/or to a different
8234             resource class.
8235         :raises: ReshapeFailed if the requested tree reshape fails for
8236             whatever reason.
8237         """
8238         disk_gb = int(self._get_local_gb_info()['total'])
8239         memory_mb = int(self._host.get_memory_mb_total())
8240         vcpus = len(self._get_vcpu_available())
8241         pcpus = len(self._get_pcpu_available())
8242         memory_enc_slots = self._get_memory_encrypted_slots()
8243 
8244         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
8245         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
8246         # is set, and fallback to use the initial_xxx_allocation_ratio
8247         # otherwise.
8248         inv = provider_tree.data(nodename).inventory
8249         ratios = self._get_allocation_ratios(inv)
8250         resources: ty.Dict[str, ty.Set['objects.Resource']] = (
8251             collections.defaultdict(set)
8252         )
8253 
8254         result = {}
8255         if memory_mb:
8256             result[orc.MEMORY_MB] = {
8257                 'total': memory_mb,
8258                 'min_unit': 1,
8259                 'max_unit': memory_mb,
8260                 'step_size': 1,
8261                 'allocation_ratio': ratios[orc.MEMORY_MB],
8262                 'reserved': CONF.reserved_host_memory_mb,
8263             }
8264 
8265         # NOTE(stephenfin): We have to optionally report these since placement
8266         # forbids reporting inventory with total=0
8267         if vcpus:
8268             result[orc.VCPU] = {
8269                 'total': vcpus,
8270                 'min_unit': 1,
8271                 'max_unit': vcpus,
8272                 'step_size': 1,
8273                 'allocation_ratio': ratios[orc.VCPU],
8274                 'reserved': CONF.reserved_host_cpus,
8275             }
8276 
8277         if pcpus:
8278             result[orc.PCPU] = {
8279                 'total': pcpus,
8280                 'min_unit': 1,
8281                 'max_unit': pcpus,
8282                 'step_size': 1,
8283                 'allocation_ratio': 1,
8284                 'reserved': 0,
8285             }
8286 
8287         if memory_enc_slots:
8288             result[orc.MEM_ENCRYPTION_CONTEXT] = {
8289                 'total': memory_enc_slots,
8290                 'min_unit': 1,
8291                 'max_unit': 1,
8292                 'step_size': 1,
8293                 'allocation_ratio': 1.0,
8294                 'reserved': 0,
8295             }
8296 
8297         # If a sharing DISK_GB provider exists in the provider tree, then our
8298         # storage is shared, and we should not report the DISK_GB inventory in
8299         # the compute node provider.
8300         # TODO(efried): Reinstate non-reporting of shared resource by the
8301         # compute RP once the issues from bug #1784020 have been resolved.
8302         if provider_tree.has_sharing_provider(orc.DISK_GB):
8303             LOG.debug('Ignoring sharing provider - see bug #1784020')
8304 
8305         if disk_gb:
8306             result[orc.DISK_GB] = {
8307                 'total': disk_gb,
8308                 'min_unit': 1,
8309                 'max_unit': disk_gb,
8310                 'step_size': 1,
8311                 'allocation_ratio': ratios[orc.DISK_GB],
8312                 'reserved': (self._get_reserved_host_disk_gb_from_config() +
8313                              self._get_disk_size_reserved_for_image_cache()),
8314             }
8315 
8316         # TODO(sbauza): Use traits to providing vGPU types. For the moment,
8317         # it will be only documentation support by explaining to use
8318         # osc-placement to create custom traits for each of the pGPU RPs.
8319         self._update_provider_tree_for_vgpu(
8320            provider_tree, nodename, allocations=allocations)
8321 
8322         self._update_provider_tree_for_pcpu(
8323             provider_tree, nodename, allocations=allocations)
8324 
8325         self._update_provider_tree_for_vpmems(
8326             provider_tree, nodename, result, resources)
8327 
8328         provider_tree.update_inventory(nodename, result)
8329         provider_tree.update_resources(nodename, resources)
8330 
8331         # Add supported traits i.e. those equal to True to provider tree while
8332         # removing the unsupported ones
8333         traits_to_add = [
8334             t for t in self.static_traits if self.static_traits[t]
8335         ]
8336         traits_to_remove = set(self.static_traits) - set(traits_to_add)
8337         provider_tree.add_traits(nodename, *traits_to_add)
8338         provider_tree.remove_traits(nodename, *traits_to_remove)
8339 
8340         # Now that we updated the ProviderTree, we want to store it locally
8341         # so that spawn() or other methods can access it thru a getter
8342         self.provider_tree = copy.deepcopy(provider_tree)
8343 
8344     def _update_provider_tree_for_vpmems(self, provider_tree, nodename,
8345                                          inventory, resources):
8346         """Update resources and inventory for vpmems in provider tree."""
8347         prov_data = provider_tree.data(nodename)
8348         for rc, vpmems in self._vpmems_by_rc.items():
8349             # Skip (and omit) inventories with total=0 because placement does
8350             # not allow setting total=0 for inventory.
8351             if not len(vpmems):
8352                 continue
8353             inventory[rc] = {
8354                 'total': len(vpmems),
8355                 'max_unit': len(vpmems),
8356                 'min_unit': 1,
8357                 'step_size': 1,
8358                 'allocation_ratio': 1.0,
8359                 'reserved': 0
8360             }
8361             for vpmem in vpmems:
8362                 resource_obj = objects.Resource(
8363                     provider_uuid=prov_data.uuid,
8364                     resource_class=rc,
8365                     identifier=vpmem.name,
8366                     metadata=vpmem)
8367                 resources[rc].add(resource_obj)
8368 
8369     def _get_memory_encrypted_slots(self):
8370         slots = CONF.libvirt.num_memory_encrypted_guests
8371         if not self._host.supports_amd_sev:
8372             if slots and slots > 0:
8373                 LOG.warning("Host is configured with "
8374                             "libvirt.num_memory_encrypted_guests set to "
8375                             "%d, but is not SEV-capable.", slots)
8376             return 0
8377 
8378         # NOTE(aspiers): Auto-detection of the number of available
8379         # slots for AMD SEV is not yet possible, so honor the
8380         # configured value, or impose no limit if this is not
8381         # specified.  This does incur a risk that if operators don't
8382         # read the instructions and configure the maximum correctly,
8383         # the maximum could be exceeded resulting in SEV guests
8384         # failing at launch-time.  However at least SEV guests will
8385         # launch until the maximum, and when auto-detection code is
8386         # added later, an upgrade will magically fix the issue.
8387         #
8388         # Note also that the configured value can be 0 on an
8389         # SEV-capable host, since there might conceivably be good
8390         # reasons for the operator to want to disable SEV even when
8391         # it's available (e.g. due to performance impact, or
8392         # implementation bugs which may surface later).
8393         if slots is not None:
8394             return slots
8395         else:
8396             return db_const.MAX_INT
8397 
8398     @property
8399     def static_traits(self) -> ty.Dict[str, bool]:
8400         if self._static_traits is not None:
8401             return self._static_traits
8402 
8403         traits: ty.Dict[str, bool] = {}
8404         traits.update(self._get_cpu_traits())
8405         traits.update(self._get_storage_bus_traits())
8406         traits.update(self._get_video_model_traits())
8407         traits.update(self._get_vif_model_traits())
8408         traits.update(self._get_tpm_traits())
8409         traits.update({ot.COMPUTE_SOCKET_PCI_NUMA_AFFINITY: True})
8410 
8411         _, invalid_traits = ot.check_traits(traits)
8412         for invalid_trait in invalid_traits:
8413             LOG.debug("Trait '%s' is not valid; ignoring.", invalid_trait)
8414             del traits[invalid_trait]
8415 
8416         self._static_traits = traits
8417 
8418         return self._static_traits
8419 
8420     @staticmethod
8421     def _is_reshape_needed_vgpu_on_root(provider_tree, nodename):
8422         """Determine if root RP has VGPU inventories.
8423 
8424         Check to see if the root compute node provider in the tree for
8425         this host already has VGPU inventory because if it does, we either
8426         need to signal for a reshape (if _update_provider_tree_for_vgpu()
8427         has no allocations) or move the allocations within the ProviderTree if
8428         passed.
8429 
8430         :param provider_tree: The ProviderTree object for this host.
8431         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8432             the name of the root node provider in the tree for this host.
8433         :returns: boolean, whether we have VGPU root inventory.
8434         """
8435         root_node = provider_tree.data(nodename)
8436         return orc.VGPU in root_node.inventory
8437 
8438     @staticmethod
8439     def _ensure_pgpu_providers(inventories_dict, provider_tree, nodename):
8440         """Ensures GPU inventory providers exist in the tree for $nodename.
8441 
8442         GPU providers are named $nodename_$gpu-device-id, e.g.
8443         ``somehost.foo.bar.com_pci_0000_84_00_0``.
8444 
8445         :param inventories_dict: Dictionary of inventories for VGPU class
8446             directly provided by _get_gpu_inventories() and which looks like:
8447                 {'pci_0000_84_00_0':
8448                     {'total': $TOTAL,
8449                      'min_unit': 1,
8450                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
8451                      'step_size': 1,
8452                      'reserved': 0,
8453                      'allocation_ratio': 1.0,
8454                     }
8455                 }
8456         :param provider_tree: The ProviderTree to update.
8457         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8458             the name of the root node provider in the tree for this host.
8459         :returns: dict, keyed by GPU device ID, to ProviderData object
8460             representing that resource provider in the tree
8461         """
8462         # Create the VGPU child providers if they do not already exist.
8463         # Dict of PGPU RPs keyed by their libvirt PCI name
8464         pgpu_rps = {}
8465         for pgpu_dev_id, inventory in inventories_dict.items():
8466             # Skip (and omit) inventories with total=0 because placement does
8467             # not allow setting total=0 for inventory.
8468             if not inventory['total']:
8469                 continue
8470             # For each physical GPU, we make sure to have a child provider
8471             pgpu_rp_name = '%s_%s' % (nodename, pgpu_dev_id)
8472             if not provider_tree.exists(pgpu_rp_name):
8473                 # This is the first time creating the child provider so add
8474                 # it to the tree under the root node provider.
8475                 provider_tree.new_child(pgpu_rp_name, nodename)
8476             # We want to idempotently return the resource providers with VGPUs
8477             pgpu_rp = provider_tree.data(pgpu_rp_name)
8478             pgpu_rps[pgpu_dev_id] = pgpu_rp
8479 
8480             # The VGPU inventory goes on a child provider of the given root
8481             # node, identified by $nodename.
8482             pgpu_inventory = {orc.VGPU: inventory}
8483             provider_tree.update_inventory(pgpu_rp_name, pgpu_inventory)
8484         return pgpu_rps
8485 
8486     @staticmethod
8487     def _assert_is_root_provider(
8488             rp_uuid, root_node, consumer_uuid, alloc_data):
8489         """Asserts during a reshape that rp_uuid is for the root node provider.
8490 
8491         When reshaping, inventory and allocations should be on the root node
8492         provider and then moved to child providers.
8493 
8494         :param rp_uuid: UUID of the provider that holds inventory/allocations.
8495         :param root_node: ProviderData object representing the root node in a
8496             provider tree.
8497         :param consumer_uuid: UUID of the consumer (instance) holding resource
8498             allocations against the given rp_uuid provider.
8499         :param alloc_data: dict of allocation data for the consumer.
8500         :raises: ReshapeFailed if rp_uuid is not the root node indicating a
8501             reshape was needed but the inventory/allocation structure is not
8502             expected.
8503         """
8504         if rp_uuid != root_node.uuid:
8505             # Something is wrong - VGPU inventory should
8506             # only be on the root node provider if we are
8507             # reshaping the tree.
8508             msg = (_('Unexpected VGPU resource allocation '
8509                      'on provider %(rp_uuid)s for consumer '
8510                      '%(consumer_uuid)s: %(alloc_data)s. '
8511                      'Expected VGPU allocation to be on root '
8512                      'compute node provider %(root_uuid)s.')
8513                    % {'rp_uuid': rp_uuid,
8514                       'consumer_uuid': consumer_uuid,
8515                       'alloc_data': alloc_data,
8516                       'root_uuid': root_node.uuid})
8517             raise exception.ReshapeFailed(error=msg)
8518 
8519     def _get_assigned_mdevs_for_reshape(
8520             self, instance_uuid, rp_uuid, alloc_data):
8521         """Gets the mediated devices assigned to the instance during a reshape.
8522 
8523         :param instance_uuid: UUID of the instance consuming VGPU resources
8524             on this host.
8525         :param rp_uuid: UUID of the resource provider with VGPU inventory being
8526             consumed by the instance.
8527         :param alloc_data: dict of allocation data for the instance consumer.
8528         :return: list of mediated device UUIDs assigned to the instance
8529         :raises: ReshapeFailed if the instance is not found in the hypervisor
8530             or no mediated devices were found to be assigned to the instance
8531             indicating VGPU allocations are out of sync with the hypervisor
8532         """
8533         # FIXME(sbauza): We don't really need an Instance
8534         # object, but given some libvirt.host logs needs
8535         # to have an instance name, just provide a fake one
8536         Instance = collections.namedtuple('Instance', ['uuid', 'name'])
8537         instance = Instance(uuid=instance_uuid, name=instance_uuid)
8538         mdevs = self._get_all_assigned_mediated_devices(instance)
8539         # _get_all_assigned_mediated_devices returns {} if the instance is
8540         # not found in the hypervisor
8541         if not mdevs:
8542             # If we found a VGPU allocation against a consumer
8543             # which is not an instance, the only left case for
8544             # Nova would be a migration but we don't support
8545             # this at the moment.
8546             msg = (_('Unexpected VGPU resource allocation on provider '
8547                      '%(rp_uuid)s for consumer %(consumer_uuid)s: '
8548                      '%(alloc_data)s. The allocation is made against a '
8549                      'non-existing instance or there are no devices assigned.')
8550                    % {'rp_uuid': rp_uuid, 'consumer_uuid': instance_uuid,
8551                       'alloc_data': alloc_data})
8552             raise exception.ReshapeFailed(error=msg)
8553         return mdevs
8554 
8555     def _count_vgpus_per_pgpu(self, mdev_uuids):
8556         """Count the number of VGPUs per physical GPU mediated device.
8557 
8558         :param mdev_uuids: List of physical GPU mediated device UUIDs.
8559         :return: dict, keyed by PGPU device ID, to count of VGPUs on that
8560             device
8561         """
8562         vgpu_count_per_pgpu: ty.Dict[str, int] = collections.defaultdict(int)
8563         for mdev_uuid in mdev_uuids:
8564             # libvirt name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
8565             dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
8566             # Count how many vGPUs are in use for this instance
8567             dev_info = self._get_mediated_device_information(dev_name)
8568             pgpu_dev_id = dev_info['parent']
8569             vgpu_count_per_pgpu[pgpu_dev_id] += 1
8570         return vgpu_count_per_pgpu
8571 
8572     @staticmethod
8573     def _check_vgpu_allocations_match_real_use(
8574             vgpu_count_per_pgpu, expected_usage, rp_uuid, consumer_uuid,
8575             alloc_data):
8576         """Checks that the number of GPU devices assigned to the consumer
8577         matches what is expected from the allocations in the placement service
8578         and logs a warning if there is a mismatch.
8579 
8580         :param vgpu_count_per_pgpu: dict, keyed by PGPU device ID, to count of
8581             VGPUs on that device where each device is assigned to the consumer
8582             (guest instance on this hypervisor)
8583         :param expected_usage: The expected usage from placement for the
8584             given resource provider and consumer
8585         :param rp_uuid: UUID of the resource provider with VGPU inventory being
8586             consumed by the instance
8587         :param consumer_uuid: UUID of the consumer (instance) holding resource
8588             allocations against the given rp_uuid provider
8589         :param alloc_data: dict of allocation data for the instance consumer
8590         """
8591         actual_usage = sum(vgpu_count_per_pgpu.values())
8592         if actual_usage != expected_usage:
8593             # Don't make it blocking, just make sure you actually correctly
8594             # allocate the existing resources
8595             LOG.warning(
8596                 'Unexpected VGPU resource allocation on provider %(rp_uuid)s '
8597                 'for consumer %(consumer_uuid)s: %(alloc_data)s. Allocations '
8598                 '(%(expected_usage)s) differ from actual use '
8599                 '(%(actual_usage)s).',
8600                 {'rp_uuid': rp_uuid, 'consumer_uuid': consumer_uuid,
8601                  'alloc_data': alloc_data, 'expected_usage': expected_usage,
8602                  'actual_usage': actual_usage})
8603 
8604     def _reshape_vgpu_allocations(
8605             self, rp_uuid, root_node, consumer_uuid, alloc_data, resources,
8606             pgpu_rps):
8607         """Update existing VGPU allocations by moving them from the root node
8608         provider to the child provider for the given VGPU provider.
8609 
8610         :param rp_uuid: UUID of the VGPU resource provider with allocations
8611             from consumer_uuid (should be the root node provider before
8612             reshaping occurs)
8613         :param root_node: ProviderData object for the root compute node
8614             resource provider in the provider tree
8615         :param consumer_uuid: UUID of the consumer (instance) with VGPU
8616             allocations against the resource provider represented by rp_uuid
8617         :param alloc_data: dict of allocation information for consumer_uuid
8618         :param resources: dict, keyed by resource class, of resources allocated
8619             to consumer_uuid from rp_uuid
8620         :param pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
8621             representing that resource provider in the tree
8622         :raises: ReshapeFailed if the reshape fails for whatever reason
8623         """
8624         # We've found VGPU allocations on a provider. It should be the root
8625         # node provider.
8626         self._assert_is_root_provider(
8627             rp_uuid, root_node, consumer_uuid, alloc_data)
8628 
8629         # Find which physical GPU corresponds to this allocation.
8630         mdev_uuids = self._get_assigned_mdevs_for_reshape(
8631             consumer_uuid, rp_uuid, alloc_data)
8632 
8633         vgpu_count_per_pgpu = self._count_vgpus_per_pgpu(mdev_uuids)
8634 
8635         # We need to make sure we found all the mediated devices that
8636         # correspond to an allocation.
8637         self._check_vgpu_allocations_match_real_use(
8638             vgpu_count_per_pgpu, resources[orc.VGPU],
8639             rp_uuid, consumer_uuid, alloc_data)
8640 
8641         # Add the VGPU allocation for each VGPU provider.
8642         allocs = alloc_data['allocations']
8643         for pgpu_dev_id, pgpu_rp in pgpu_rps.items():
8644             vgpu_count = vgpu_count_per_pgpu[pgpu_dev_id]
8645             if vgpu_count:
8646                 allocs[pgpu_rp.uuid] = {
8647                     'resources': {
8648                         orc.VGPU: vgpu_count
8649                     }
8650                 }
8651         # And remove the VGPU allocation from the root node provider.
8652         del resources[orc.VGPU]
8653 
8654     def _reshape_gpu_resources(
8655             self, allocations, root_node, pgpu_rps):
8656         """Reshapes the provider tree moving VGPU inventory from root to child
8657 
8658         :param allocations:
8659             Dict of allocation data of the form:
8660               { $CONSUMER_UUID: {
8661                     # The shape of each "allocations" dict below is identical
8662                     # to the return from GET /allocations/{consumer_uuid}
8663                     "allocations": {
8664                         $RP_UUID: {
8665                             "generation": $RP_GEN,
8666                             "resources": {
8667                                 $RESOURCE_CLASS: $AMOUNT,
8668                                 ...
8669                             },
8670                         },
8671                         ...
8672                     },
8673                     "project_id": $PROJ_ID,
8674                     "user_id": $USER_ID,
8675                     "consumer_generation": $CONSUMER_GEN,
8676                 },
8677                 ...
8678               }
8679         :params root_node: The root node in the provider tree
8680         :params pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
8681             representing that resource provider in the tree
8682         """
8683         LOG.info('Reshaping tree; moving VGPU allocations from root '
8684                  'provider %s to child providers %s.', root_node.uuid,
8685                  pgpu_rps.values())
8686         # For each consumer in the allocations dict, look for VGPU
8687         # allocations and move them to the VGPU provider.
8688         for consumer_uuid, alloc_data in allocations.items():
8689             # Copy and iterate over the current set of providers to avoid
8690             # modifying keys while iterating.
8691             allocs = alloc_data['allocations']
8692             for rp_uuid in list(allocs):
8693                 resources = allocs[rp_uuid]['resources']
8694                 if orc.VGPU in resources:
8695                     self._reshape_vgpu_allocations(
8696                         rp_uuid, root_node, consumer_uuid, alloc_data,
8697                         resources, pgpu_rps)
8698 
8699     def _update_provider_tree_for_vgpu(self, provider_tree, nodename,
8700                                        allocations=None):
8701         """Updates the provider tree for VGPU inventory.
8702 
8703         Before Stein, VGPU inventory and allocations were on the root compute
8704         node provider in the tree. Starting in Stein, the VGPU inventory is
8705         on a child provider in the tree. As a result, this method will
8706         "reshape" the tree if necessary on first start of this compute service
8707         in Stein.
8708 
8709         :param provider_tree: The ProviderTree to update.
8710         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8711             the name of the root node provider in the tree for this host.
8712         :param allocations: If not None, indicates a reshape was requested and
8713             should be performed.
8714         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
8715             the method determines a reshape of the tree is needed, i.e. VGPU
8716             inventory and allocations must be migrated from the root node
8717             provider to a child provider of VGPU resources in the tree.
8718         :raises: nova.exception.ReshapeFailed if the requested tree reshape
8719             fails for whatever reason.
8720         """
8721         # First, check if this host actually has vGPU to reshape
8722         inventories_dict = self._get_gpu_inventories()
8723         if not inventories_dict:
8724             return
8725 
8726         # Check to see if the root compute node provider in the tree for
8727         # this host already has VGPU inventory because if it does, and
8728         # we're not currently reshaping (allocations is None), we need
8729         # to indicate that a reshape is needed to move the VGPU inventory
8730         # onto a child provider in the tree.
8731 
8732         # Ensure GPU providers are in the ProviderTree for the given inventory.
8733         pgpu_rps = self._ensure_pgpu_providers(
8734             inventories_dict, provider_tree, nodename)
8735 
8736         if self._is_reshape_needed_vgpu_on_root(provider_tree, nodename):
8737             if allocations is None:
8738                 # We have old VGPU inventory on root RP, but we haven't yet
8739                 # allocations. That means we need to ask for a reshape.
8740                 LOG.info('Requesting provider tree reshape in order to move '
8741                          'VGPU inventory from the root compute node provider '
8742                          '%s to a child provider.', nodename)
8743                 raise exception.ReshapeNeeded()
8744             # We have allocations, that means we already asked for a reshape
8745             # and the Placement API returned us them. We now need to move
8746             # those from the root RP to the needed children RPs.
8747             root_node = provider_tree.data(nodename)
8748             # Reshape VGPU provider inventory and allocations, moving them
8749             # from the root node provider to the child providers.
8750             self._reshape_gpu_resources(allocations, root_node, pgpu_rps)
8751             # Only delete the root inventory once the reshape is done
8752             if orc.VGPU in root_node.inventory:
8753                 del root_node.inventory[orc.VGPU]
8754                 provider_tree.update_inventory(nodename, root_node.inventory)
8755 
8756     def _update_provider_tree_for_pcpu(self, provider_tree, nodename,
8757                                        allocations=None):
8758         """Updates the provider tree for PCPU inventory.
8759 
8760         Before Train, pinned instances consumed VCPU inventory just like
8761         unpinned instances. Starting in Train, these instances now consume PCPU
8762         inventory. The function can reshape the inventory, changing allocations
8763         of VCPUs to PCPUs.
8764 
8765         :param provider_tree: The ProviderTree to update.
8766         :param nodename: The ComputeNode.hypervisor_hostname, also known as
8767             the name of the root node provider in the tree for this host.
8768         :param allocations: A dict, keyed by consumer UUID, of allocation
8769             records, or None::
8770 
8771                 {
8772                     $CONSUMER_UUID: {
8773                         "allocations": {
8774                             $RP_UUID: {
8775                                 "generation": $RP_GEN,
8776                                 "resources": {
8777                                     $RESOURCE_CLASS: $AMOUNT,
8778                                     ...
8779                                 },
8780                             },
8781                             ...
8782                         },
8783                         "project_id": $PROJ_ID,
8784                         "user_id": $USER_ID,
8785                         "consumer_generation": $CONSUMER_GEN,
8786                     },
8787                     ...
8788                 }
8789 
8790             If provided, this indicates a reshape was requested and should be
8791             performed.
8792         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
8793             the method determines a reshape of the tree is needed, i.e. VCPU
8794             inventory and allocations must be migrated to PCPU resources.
8795         :raises: nova.exception.ReshapeFailed if the requested tree reshape
8796             fails for whatever reason.
8797         """
8798         # If we're not configuring PCPUs, then we've nothing to worry about
8799         # (yet)
8800         if not CONF.compute.cpu_dedicated_set:
8801             return
8802 
8803         root_node = provider_tree.data(nodename)
8804 
8805         # Similarly, if PCPU inventories are already reported then there is no
8806         # need to reshape
8807         if orc.PCPU in root_node.inventory:
8808             return
8809 
8810         ctx = nova_context.get_admin_context()
8811         compute_node = objects.ComputeNode.get_by_nodename(ctx, nodename)
8812 
8813         # Finally, if the compute node doesn't appear to support NUMA, move
8814         # swiftly on
8815         if not compute_node.numa_topology:
8816             return
8817 
8818         # The ComputeNode.numa_topology is a StringField, deserialize
8819         numa = objects.NUMATopology.obj_from_db_obj(compute_node.numa_topology)
8820 
8821         # If the host doesn't know of any pinned CPUs, we can continue
8822         if not any(cell.pinned_cpus for cell in numa.cells):
8823             return
8824 
8825         # At this point, we know there's something to be migrated here but not
8826         # how much. If the allocations are None, we're at the startup of the
8827         # compute node and a Reshape is needed. Indicate this by raising the
8828         # ReshapeNeeded exception
8829 
8830         if allocations is None:
8831             LOG.info(
8832                 'Requesting provider tree reshape in order to move '
8833                 'VCPU to PCPU allocations to the compute node '
8834                 'provider %s', nodename)
8835             raise exception.ReshapeNeeded()
8836 
8837         # Go figure out how many VCPUs to migrate to PCPUs. We've been telling
8838         # people for years *not* to mix pinned and unpinned instances, meaning
8839         # we should be able to move all VCPUs to PCPUs, but we never actually
8840         # enforced this in code and there's an all-too-high chance someone
8841         # didn't get the memo
8842 
8843         allocations_needing_reshape = []
8844 
8845         # we need to tackle the allocations against instances on this host...
8846 
8847         instances = objects.InstanceList.get_by_host(
8848             ctx, compute_node.host, expected_attrs=['numa_topology'])
8849         for instance in instances:
8850             if not instance.numa_topology:
8851                 continue
8852 
8853             if instance.numa_topology.cpu_policy != (
8854                 fields.CPUAllocationPolicy.DEDICATED
8855             ):
8856                 continue
8857 
8858             allocations_needing_reshape.append(instance.uuid)
8859 
8860         # ...and those for any migrations
8861 
8862         migrations = objects.MigrationList.get_in_progress_by_host_and_node(
8863             ctx, compute_node.host, compute_node.hypervisor_hostname)
8864         for migration in migrations:
8865             # we don't care about migrations that have landed here, since we
8866             # already have those instances above
8867             if not migration.dest_compute or (
8868                     migration.dest_compute == compute_node.host):
8869                 continue
8870 
8871             instance = objects.Instance.get_by_uuid(
8872                 ctx, migration.instance_uuid, expected_attrs=['numa_topology'])
8873 
8874             if not instance.numa_topology:
8875                 continue
8876 
8877             if instance.numa_topology.cpu_policy != (
8878                 fields.CPUAllocationPolicy.DEDICATED
8879             ):
8880                 continue
8881 
8882             allocations_needing_reshape.append(migration.uuid)
8883 
8884         for allocation_uuid in allocations_needing_reshape:
8885             consumer_allocations = allocations.get(allocation_uuid, {}).get(
8886                 'allocations', {})
8887             # TODO(stephenfin): We can probably just check the allocations for
8888             # ComputeNode.uuid since compute nodes are the only (?) provider of
8889             # VCPU and PCPU resources
8890             for rp_uuid in consumer_allocations:
8891                 resources = consumer_allocations[rp_uuid]['resources']
8892 
8893                 if orc.PCPU in resources or orc.VCPU not in resources:
8894                     # Either this has been migrated or it's not a compute node
8895                     continue
8896 
8897                 # Switch stuff around. We can do a straight swap since an
8898                 # instance is either pinned or unpinned. By doing this, we're
8899                 # modifying the provided 'allocations' dict, which will
8900                 # eventually be used by the resource tracker to update
8901                 # placement
8902                 resources['PCPU'] = resources['VCPU']
8903                 del resources[orc.VCPU]
8904 
8905     def get_available_resource(self, nodename):
8906         """Retrieve resource information.
8907 
8908         This method is called when nova-compute launches, and
8909         as part of a periodic task that records the results in the DB.
8910 
8911         :param nodename: unused in this driver
8912         :returns: dictionary containing resource info
8913         """
8914 
8915         disk_info_dict = self._get_local_gb_info()
8916         data = {}
8917 
8918         # NOTE(dprince): calling capabilities before getVersion works around
8919         # an initialization issue with some versions of Libvirt (1.0.5.5).
8920         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
8921         # See: https://bugs.launchpad.net/nova/+bug/1215593
8922         data["supported_instances"] = self._get_instance_capabilities()
8923 
8924         data["vcpus"] = len(self._get_vcpu_available())
8925         data["memory_mb"] = self._host.get_memory_mb_total()
8926         data["local_gb"] = disk_info_dict['total']
8927         data["vcpus_used"] = self._get_vcpu_used()
8928         data["memory_mb_used"] = self._host.get_memory_mb_used()
8929         data["local_gb_used"] = disk_info_dict['used']
8930         data["hypervisor_type"] = self._host.get_driver_type()
8931         data["hypervisor_version"] = self._host.get_version()
8932         data["hypervisor_hostname"] = self._host.get_hostname()
8933         # TODO(berrange): why do we bother converting the
8934         # libvirt capabilities XML into a special JSON format ?
8935         # The data format is different across all the drivers
8936         # so we could just return the raw capabilities XML
8937         # which 'compare_cpu' could use directly
8938         #
8939         # That said, arch_filter.py now seems to rely on
8940         # the libvirt drivers format which suggests this
8941         # data format needs to be standardized across drivers
8942         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
8943 
8944         disk_free_gb = disk_info_dict['free']
8945         disk_over_committed = self._get_disk_over_committed_size_total()
8946         available_least = disk_free_gb * units.Gi - disk_over_committed
8947         data['disk_available_least'] = available_least / units.Gi
8948 
8949         data['pci_passthrough_devices'] = self._get_pci_passthrough_devices()
8950 
8951         numa_topology = self._get_host_numa_topology()
8952         if numa_topology:
8953             data['numa_topology'] = numa_topology._to_json()
8954         else:
8955             data['numa_topology'] = None
8956 
8957         return data
8958 
8959     def check_instance_shared_storage_local(self, context, instance):
8960         """Check if instance files located on shared storage.
8961 
8962         This runs check on the destination host, and then calls
8963         back to the source host to check the results.
8964 
8965         :param context: security context
8966         :param instance: nova.objects.instance.Instance object
8967         :returns:
8968          - tempfile: A dict containing the tempfile info on the destination
8969                      host
8970          - None:
8971 
8972             1. If the instance path is not existing.
8973             2. If the image backend is shared block storage type.
8974         """
8975         if self.image_backend.backend().is_shared_block_storage():
8976             return None
8977 
8978         dirpath = libvirt_utils.get_instance_path(instance)
8979 
8980         if not os.path.exists(dirpath):
8981             return None
8982 
8983         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
8984         LOG.debug("Creating tmpfile %s to verify with other "
8985                   "compute node that the instance is on "
8986                   "the same shared storage.",
8987                   tmp_file, instance=instance)
8988         os.close(fd)
8989         return {"filename": tmp_file}
8990 
8991     def check_instance_shared_storage_remote(self, context, data):
8992         return os.path.exists(data['filename'])
8993 
8994     def check_instance_shared_storage_cleanup(self, context, data):
8995         fileutils.delete_if_exists(data["filename"])
8996 
8997     def check_can_live_migrate_destination(self, context, instance,
8998                                            src_compute_info, dst_compute_info,
8999                                            block_migration=False,
9000                                            disk_over_commit=False):
9001         """Check if it is possible to execute live migration.
9002 
9003         This runs checks on the destination host, and then calls
9004         back to the source host to check the results.
9005 
9006         :param context: security context
9007         :param instance: nova.db.sqlalchemy.models.Instance
9008         :param block_migration: if true, prepare for block migration
9009         :param disk_over_commit: if true, allow disk over commit
9010         :returns: a LibvirtLiveMigrateData object
9011         """
9012         if disk_over_commit:
9013             disk_available_gb = dst_compute_info['free_disk_gb']
9014         else:
9015             disk_available_gb = dst_compute_info['disk_available_least']
9016         disk_available_mb = (
9017             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
9018 
9019         # Compare CPU
9020         try:
9021             if not instance.vcpu_model or not instance.vcpu_model.model:
9022                 source_cpu_info = src_compute_info['cpu_info']
9023                 self._compare_cpu(None, source_cpu_info, instance)
9024             else:
9025                 self._compare_cpu(instance.vcpu_model, None, instance)
9026         except exception.InvalidCPUInfo as e:
9027             raise exception.MigrationPreCheckError(reason=e)
9028 
9029         # Create file on storage, to be checked on source host
9030         filename = self._create_shared_storage_test_file(instance)
9031 
9032         data = objects.LibvirtLiveMigrateData()
9033         data.filename = filename
9034         data.image_type = CONF.libvirt.images_type
9035         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
9036         data.graphics_listen_addr_spice = CONF.spice.server_listen
9037         if CONF.serial_console.enabled:
9038             data.serial_listen_addr = CONF.serial_console.proxyclient_address
9039         else:
9040             data.serial_listen_addr = None
9041         # Notes(eliqiao): block_migration and disk_over_commit are not
9042         # nullable, so just don't set them if they are None
9043         if block_migration is not None:
9044             data.block_migration = block_migration
9045         if disk_over_commit is not None:
9046             data.disk_over_commit = disk_over_commit
9047         data.disk_available_mb = disk_available_mb
9048         data.dst_wants_file_backed_memory = CONF.libvirt.file_backed_memory > 0
9049 
9050         # TODO(artom) Set to indicate that the destination (us) can perform a
9051         # NUMA-aware live migration. NUMA-aware live migration will become
9052         # unconditionally supported in RPC 6.0, so this sentinel can be removed
9053         # then.
9054         if instance.numa_topology:
9055             data.dst_supports_numa_live_migration = True
9056 
9057         return data
9058 
9059     def post_claim_migrate_data(self, context, instance, migrate_data, claim):
9060         migrate_data.dst_numa_info = self._get_live_migrate_numa_info(
9061                 claim.claimed_numa_topology, claim.instance_type,
9062                 claim.image_meta)
9063         return migrate_data
9064 
9065     def _get_resources(self, instance, prefix=None):
9066         resources: 'objects.ResourceList' = []
9067         if prefix:
9068             migr_context = instance.migration_context
9069             attr_name = prefix + 'resources'
9070             if migr_context and attr_name in migr_context:
9071                 resources = getattr(migr_context, attr_name) or []
9072         else:
9073             resources = instance.resources or []
9074         return resources
9075 
9076     def _get_vpmem_resources(self, resources):
9077         vpmem_resources = []
9078         for resource in resources:
9079             if 'metadata' in resource and \
9080                 isinstance(resource.metadata, objects.LibvirtVPMEMDevice):
9081                 vpmem_resources.append(resource)
9082         return vpmem_resources
9083 
9084     def _get_ordered_vpmem_resources(self, resources, flavor):
9085         vpmem_resources = self._get_vpmem_resources(resources)
9086         ordered_vpmem_resources = []
9087         labels = hardware.get_vpmems(flavor)
9088         for label in labels:
9089             for vpmem_resource in vpmem_resources:
9090                 if vpmem_resource.metadata.label == label:
9091                     ordered_vpmem_resources.append(vpmem_resource)
9092                     vpmem_resources.remove(vpmem_resource)
9093                     break
9094         return ordered_vpmem_resources
9095 
9096     def _sorted_migrating_resources(self, instance, flavor):
9097         """This method is used to sort instance.migration_context.new_resources
9098         claimed on dest host, then the ordered new resources will be used to
9099         update resources info (e.g. vpmems) in the new xml which is used for
9100         live migration.
9101         """
9102         resources = self._get_resources(instance, prefix='new_')
9103         if not resources:
9104             return
9105         ordered_resources = []
9106         ordered_vpmem_resources = self._get_ordered_vpmem_resources(
9107                 resources, flavor)
9108         ordered_resources.extend(ordered_vpmem_resources)
9109         ordered_resources_obj = objects.ResourceList(objects=ordered_resources)
9110         return ordered_resources_obj
9111 
9112     def _get_live_migrate_numa_info(self, instance_numa_topology, flavor,
9113                                     image_meta):
9114         """Builds a LibvirtLiveMigrateNUMAInfo object to send to the source of
9115         a live migration, containing information about how the instance is to
9116         be pinned on the destination host.
9117 
9118         :param instance_numa_topology: The InstanceNUMATopology as fitted to
9119                                        the destination by the live migration
9120                                        Claim.
9121         :param flavor: The Flavor object for the instance.
9122         :param image_meta: The ImageMeta object for the instance.
9123         :returns: A LibvirtLiveMigrateNUMAInfo object indicating how to update
9124                   the XML for the destination host.
9125         """
9126         info = objects.LibvirtLiveMigrateNUMAInfo()
9127         cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune = \
9128             self._get_guest_numa_config(instance_numa_topology, flavor,
9129                                         image_meta)
9130         # NOTE(artom) These two should always be either None together, or
9131         # truth-y together.
9132         if guest_cpu_tune and guest_numa_tune:
9133             info.cpu_pins = {}
9134             for pin in guest_cpu_tune.vcpupin:
9135                 info.cpu_pins[str(pin.id)] = pin.cpuset
9136 
9137             info.emulator_pins = guest_cpu_tune.emulatorpin.cpuset
9138 
9139             if guest_cpu_tune.vcpusched:
9140                 # NOTE(artom) vcpusched is a list, but there's only ever one
9141                 # element in it (see _get_guest_numa_config under
9142                 # wants_realtime)
9143                 info.sched_vcpus = guest_cpu_tune.vcpusched[0].vcpus
9144                 info.sched_priority = guest_cpu_tune.vcpusched[0].priority
9145 
9146             info.cell_pins = {}
9147             for node in guest_numa_tune.memnodes:
9148                 info.cell_pins[str(node.cellid)] = set(node.nodeset)
9149 
9150         LOG.debug('Built NUMA live migration info: %s', info)
9151         return info
9152 
9153     def cleanup_live_migration_destination_check(self, context,
9154                                                  dest_check_data):
9155         """Do required cleanup on dest host after check_can_live_migrate calls
9156 
9157         :param context: security context
9158         """
9159         filename = dest_check_data.filename
9160         self._cleanup_shared_storage_test_file(filename)
9161 
9162     def check_can_live_migrate_source(self, context, instance,
9163                                       dest_check_data,
9164                                       block_device_info=None):
9165         """Check if it is possible to execute live migration.
9166 
9167         This checks if the live migration can succeed, based on the
9168         results from check_can_live_migrate_destination.
9169 
9170         :param context: security context
9171         :param instance: nova.db.sqlalchemy.models.Instance
9172         :param dest_check_data: result of check_can_live_migrate_destination
9173         :param block_device_info: result of _get_instance_block_device_info
9174         :returns: a LibvirtLiveMigrateData object
9175         """
9176         # Checking shared storage connectivity
9177         # if block migration, instances_path should not be on shared storage.
9178         source = CONF.host
9179 
9180         dest_check_data.is_shared_instance_path = (
9181             self._check_shared_storage_test_file(
9182                 dest_check_data.filename, instance))
9183 
9184         dest_check_data.is_shared_block_storage = (
9185             self._is_shared_block_storage(instance, dest_check_data,
9186                                           block_device_info))
9187 
9188         if 'block_migration' not in dest_check_data:
9189             dest_check_data.block_migration = (
9190                 not dest_check_data.is_on_shared_storage())
9191 
9192         if dest_check_data.block_migration:
9193             # TODO(eliqiao): Once block_migration flag is removed from the API
9194             # we can safely remove the if condition
9195             if dest_check_data.is_on_shared_storage():
9196                 reason = _("Block migration can not be used "
9197                            "with shared storage.")
9198                 raise exception.InvalidLocalStorage(reason=reason, path=source)
9199             if 'disk_over_commit' in dest_check_data:
9200                 self._assert_dest_node_has_enough_disk(context, instance,
9201                                         dest_check_data.disk_available_mb,
9202                                         dest_check_data.disk_over_commit,
9203                                         block_device_info)
9204             if block_device_info:
9205                 bdm = block_device_info.get('block_device_mapping')
9206                 # NOTE(eliqiao): Selective disk migrations are not supported
9207                 # with tunnelled block migrations so we can block them early.
9208                 if (bdm and
9209                     (self._block_migration_flags &
9210                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
9211                     msg = (_('Cannot block migrate instance %(uuid)s with'
9212                              ' mapped volumes. Selective block device'
9213                              ' migration is not supported with tunnelled'
9214                              ' block migrations.') % {'uuid': instance.uuid})
9215                     LOG.error(msg, instance=instance)
9216                     raise exception.MigrationPreCheckError(reason=msg)
9217         elif not (dest_check_data.is_shared_block_storage or
9218                   dest_check_data.is_shared_instance_path):
9219             reason = _("Shared storage live-migration requires either shared "
9220                        "storage or boot-from-volume with no local disks.")
9221             raise exception.InvalidSharedStorage(reason=reason, path=source)
9222 
9223         # NOTE(mikal): include the instance directory name here because it
9224         # doesn't yet exist on the destination but we want to force that
9225         # same name to be used
9226         instance_path = libvirt_utils.get_instance_path(instance,
9227                                                         relative=True)
9228         dest_check_data.instance_relative_path = instance_path
9229 
9230         # TODO(artom) Set to indicate that the source (us) can perform a
9231         # NUMA-aware live migration. NUMA-aware live migration will become
9232         # unconditionally supported in RPC 6.0, so this sentinel can be removed
9233         # then.
9234         if instance.numa_topology:
9235             dest_check_data.src_supports_numa_live_migration = True
9236 
9237         return dest_check_data
9238 
9239     def _is_shared_block_storage(self, instance, dest_check_data,
9240                                  block_device_info=None):
9241         """Check if all block storage of an instance can be shared
9242         between source and destination of a live migration.
9243 
9244         Returns true if the instance is volume backed and has no local disks,
9245         or if the image backend is the same on source and destination and the
9246         backend shares block storage between compute nodes.
9247 
9248         :param instance: nova.objects.instance.Instance object
9249         :param dest_check_data: dict with boolean fields image_type,
9250                                 is_shared_instance_path, and is_volume_backed
9251         """
9252         if (dest_check_data.obj_attr_is_set('image_type') and
9253                 CONF.libvirt.images_type == dest_check_data.image_type and
9254                 self.image_backend.backend().is_shared_block_storage()):
9255             # NOTE(dgenin): currently true only for RBD image backend
9256             return True
9257 
9258         if (dest_check_data.is_shared_instance_path and
9259                 self.image_backend.backend().is_file_in_instance_path()):
9260             # NOTE(angdraug): file based image backends (Flat, Qcow2)
9261             # place block device files under the instance path
9262             return True
9263 
9264         if (dest_check_data.is_volume_backed and
9265                 not bool(self._get_instance_disk_info(instance,
9266                                                       block_device_info))):
9267             return True
9268 
9269         return False
9270 
9271     def _assert_dest_node_has_enough_disk(self, context, instance,
9272                                              available_mb, disk_over_commit,
9273                                              block_device_info):
9274         """Checks if destination has enough disk for block migration."""
9275         # Libvirt supports qcow2 disk format,which is usually compressed
9276         # on compute nodes.
9277         # Real disk image (compressed) may enlarged to "virtual disk size",
9278         # that is specified as the maximum disk size.
9279         # (See qemu-img -f path-to-disk)
9280         # Scheduler recognizes destination host still has enough disk space
9281         # if real disk size < available disk size
9282         # if disk_over_commit is True,
9283         #  otherwise virtual disk size < available disk size.
9284 
9285         available = 0
9286         if available_mb:
9287             available = available_mb * units.Mi
9288 
9289         disk_infos = self._get_instance_disk_info(instance, block_device_info)
9290 
9291         necessary = 0
9292         if disk_over_commit:
9293             for info in disk_infos:
9294                 necessary += int(info['disk_size'])
9295         else:
9296             for info in disk_infos:
9297                 necessary += int(info['virt_disk_size'])
9298 
9299         # Check that available disk > necessary disk
9300         if (available - necessary) < 0:
9301             reason = (_('Unable to migrate %(instance_uuid)s: '
9302                         'Disk of instance is too large(available'
9303                         ' on destination host:%(available)s '
9304                         '< need:%(necessary)s)') %
9305                       {'instance_uuid': instance.uuid,
9306                        'available': available,
9307                        'necessary': necessary})
9308             raise exception.MigrationPreCheckError(reason=reason)
9309 
9310     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
9311         """Check the host is compatible with the requested CPU
9312 
9313         :param guest_cpu: nova.objects.VirtCPUModel
9314             or nova.virt.libvirt.vconfig.LibvirtConfigGuestCPU or None.
9315         :param host_cpu_str: JSON from _get_cpu_info() method
9316 
9317         If the 'guest_cpu' parameter is not None, this will be
9318         validated for migration compatibility with the host.
9319         Otherwise the 'host_cpu_str' JSON string will be used for
9320         validation.
9321 
9322         :returns:
9323             None. if given cpu info is not compatible to this server,
9324             raise exception.
9325         """
9326 
9327         # NOTE(kchamart): Comparing host to guest CPU model for emulated
9328         # guests (<domain type='qemu'>) should not matter -- in this
9329         # mode (QEMU "TCG") the CPU is fully emulated in software and no
9330         # hardware acceleration, like KVM, is involved. So, skip the CPU
9331         # compatibility check for the QEMU domain type, and retain it for
9332         # KVM guests.
9333         if CONF.libvirt.virt_type not in ['kvm']:
9334             return
9335 
9336         if guest_cpu is None:
9337             info = jsonutils.loads(host_cpu_str)
9338             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
9339             cpu = vconfig.LibvirtConfigCPU()
9340             cpu.arch = info['arch']
9341             cpu.model = info['model']
9342             cpu.vendor = info['vendor']
9343             cpu.sockets = info['topology']['sockets']
9344             cpu.cores = info['topology']['cores']
9345             cpu.threads = info['topology']['threads']
9346             for f in info['features']:
9347                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
9348         elif isinstance(guest_cpu, vconfig.LibvirtConfigGuestCPU):
9349             cpu = guest_cpu
9350         else:
9351             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
9352 
9353         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
9354              "virCPUCompareResult")
9355         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
9356         # unknown character exists in xml, then libvirt complains
9357         try:
9358             cpu_xml = cpu.to_xml()
9359             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
9360             ret = self._host.compare_cpu(cpu_xml)
9361         except libvirt.libvirtError as e:
9362             if cpu.arch == fields.Architecture.AARCH64:
9363                 LOG.debug("Host CPU compatibility check does not make "
9364                           "sense on AArch64; skip CPU comparison")
9365                 return
9366             error_code = e.get_error_code()
9367             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9368                 LOG.debug("URI %(uri)s does not support cpu comparison. "
9369                           "It will be proceeded though. Error: %(error)s",
9370                           {'uri': self._uri(), 'error': e})
9371                 return
9372             else:
9373                 LOG.error(m, {'ret': e, 'u': u})
9374                 raise exception.InvalidCPUInfo(
9375                     reason=m % {'ret': e, 'u': u})
9376 
9377         if ret <= 0:
9378             LOG.error(m, {'ret': ret, 'u': u})
9379             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
9380 
9381     def _create_shared_storage_test_file(self, instance):
9382         """Makes tmpfile under CONF.instances_path."""
9383         dirpath = CONF.instances_path
9384         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
9385         LOG.debug("Creating tmpfile %s to notify to other "
9386                   "compute nodes that they should mount "
9387                   "the same storage.", tmp_file, instance=instance)
9388         os.close(fd)
9389         return os.path.basename(tmp_file)
9390 
9391     def _check_shared_storage_test_file(self, filename, instance):
9392         """Confirms existence of the tmpfile under CONF.instances_path.
9393 
9394         Cannot confirm tmpfile return False.
9395         """
9396         # NOTE(tpatzig): if instances_path is a shared volume that is
9397         # under heavy IO (many instances on many compute nodes),
9398         # then checking the existence of the testfile fails,
9399         # just because it takes longer until the client refreshes and new
9400         # content gets visible.
9401         # os.utime (like touch) on the directory forces the client to refresh.
9402         os.utime(CONF.instances_path, None)
9403 
9404         tmp_file = os.path.join(CONF.instances_path, filename)
9405         if not os.path.exists(tmp_file):
9406             exists = False
9407         else:
9408             exists = True
9409         LOG.debug('Check if temp file %s exists to indicate shared storage '
9410                   'is being used for migration. Exists? %s', tmp_file, exists,
9411                   instance=instance)
9412         return exists
9413 
9414     def _cleanup_shared_storage_test_file(self, filename):
9415         """Removes existence of the tmpfile under CONF.instances_path."""
9416         tmp_file = os.path.join(CONF.instances_path, filename)
9417         os.remove(tmp_file)
9418 
9419     def live_migration(self, context, instance, dest,
9420                        post_method, recover_method, block_migration=False,
9421                        migrate_data=None):
9422         """Spawning live_migration operation for distributing high-load.
9423 
9424         :param context: security context
9425         :param instance:
9426             nova.db.sqlalchemy.models.Instance object
9427             instance object that is migrated.
9428         :param dest: destination host
9429         :param post_method:
9430             post operation method.
9431             expected nova.compute.manager._post_live_migration.
9432         :param recover_method:
9433             recovery method when any exception occurs.
9434             expected nova.compute.manager._rollback_live_migration.
9435         :param block_migration: if true, do block migration.
9436         :param migrate_data: a LibvirtLiveMigrateData object
9437 
9438         """
9439 
9440         # 'dest' will be substituted into 'migration_uri' so ensure
9441         # it does't contain any characters that could be used to
9442         # exploit the URI accepted by libvirt
9443         if not libvirt_utils.is_valid_hostname(dest):
9444             raise exception.InvalidHostname(hostname=dest)
9445 
9446         self._live_migration(context, instance, dest,
9447                              post_method, recover_method, block_migration,
9448                              migrate_data)
9449 
9450     def live_migration_abort(self, instance):
9451         """Aborting a running live-migration.
9452 
9453         :param instance: instance object that is in migration
9454 
9455         """
9456 
9457         guest = self._host.get_guest(instance)
9458         dom = guest._domain
9459 
9460         try:
9461             dom.abortJob()
9462         except libvirt.libvirtError as e:
9463             LOG.error("Failed to cancel migration %s",
9464                     encodeutils.exception_to_unicode(e), instance=instance)
9465             raise
9466 
9467     def _verify_serial_console_is_disabled(self):
9468         if CONF.serial_console.enabled:
9469 
9470             msg = _('Your destination node does not support'
9471                     ' retrieving listen addresses. In order'
9472                     ' for live migration to work properly you'
9473                     ' must disable serial console.')
9474             raise exception.MigrationError(reason=msg)
9475 
9476     def _detach_direct_passthrough_vifs(self, context,
9477                                         migrate_data, instance):
9478         """detaches passthrough vif to enable live migration
9479 
9480         :param context: security context
9481         :param migrate_data: a LibvirtLiveMigrateData object
9482         :param instance: instance object that is migrated.
9483         """
9484         # NOTE(sean-k-mooney): if we have vif data available we
9485         # loop over each vif and detach all direct passthrough
9486         # vifs to allow sriov live migration.
9487         direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
9488         vifs = [vif.source_vif for vif in migrate_data.vifs
9489                 if "source_vif" in vif and vif.source_vif]
9490         for vif in vifs:
9491             if vif['vnic_type'] in direct_vnics:
9492                 LOG.info("Detaching vif %s from instance "
9493                          "%s for live migration", vif['id'], instance.id)
9494                 self.detach_interface(context, instance, vif)
9495 
9496     def _live_migration_operation(self, context, instance, dest,
9497                                   block_migration, migrate_data, guest,
9498                                   device_names):
9499         """Invoke the live migration operation
9500 
9501         :param context: security context
9502         :param instance:
9503             nova.db.sqlalchemy.models.Instance object
9504             instance object that is migrated.
9505         :param dest: destination host
9506         :param block_migration: if true, do block migration.
9507         :param migrate_data: a LibvirtLiveMigrateData object
9508         :param guest: the guest domain object
9509         :param device_names: list of device names that are being migrated with
9510             instance
9511 
9512         This method is intended to be run in a background thread and will
9513         block that thread until the migration is finished or failed.
9514         """
9515         try:
9516             if migrate_data.block_migration:
9517                 migration_flags = self._block_migration_flags
9518             else:
9519                 migration_flags = self._live_migration_flags
9520 
9521             if not migrate_data.serial_listen_addr:
9522                 # In this context we want to ensure that serial console is
9523                 # disabled on source node. This is because nova couldn't
9524                 # retrieve serial listen address from destination node, so we
9525                 # consider that destination node might have serial console
9526                 # disabled as well.
9527                 self._verify_serial_console_is_disabled()
9528 
9529             # NOTE(aplanas) migrate_uri will have a value only in the
9530             # case that `live_migration_inbound_addr` parameter is
9531             # set, and we propose a non tunneled migration.
9532             migrate_uri = None
9533             if ('target_connect_addr' in migrate_data and
9534                     migrate_data.target_connect_addr is not None):
9535                 dest = migrate_data.target_connect_addr
9536                 if (migration_flags &
9537                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
9538                     migrate_uri = self._migrate_uri(dest)
9539 
9540             new_xml_str = None
9541             if CONF.libvirt.virt_type != "parallels":
9542                 # If the migrate_data has port binding information for the
9543                 # destination host, we need to prepare the guest vif config
9544                 # for the destination before we start migrating the guest.
9545                 get_vif_config = None
9546                 if 'vifs' in migrate_data and migrate_data.vifs:
9547                     # NOTE(mriedem): The vif kwarg must be built on the fly
9548                     # within get_updated_guest_xml based on migrate_data.vifs.
9549                     # We could stash the virt_type from the destination host
9550                     # into LibvirtLiveMigrateData but the host kwarg is a
9551                     # nova.virt.libvirt.host.Host object and is used to check
9552                     # information like libvirt version on the destination.
9553                     # If this becomes a problem, what we could do is get the
9554                     # VIF configs while on the destination host during
9555                     # pre_live_migration() and store those in the
9556                     # LibvirtLiveMigrateData object. For now we just use the
9557                     # source host information for virt_type and
9558                     # host (version) since the conductor live_migrate method
9559                     # _check_compatible_with_source_hypervisor() ensures that
9560                     # the hypervisor types and versions are compatible.
9561                     get_vif_config = functools.partial(
9562                         self.vif_driver.get_config,
9563                         instance=instance,
9564                         image_meta=instance.image_meta,
9565                         inst_type=instance.flavor,
9566                         virt_type=CONF.libvirt.virt_type,
9567                     )
9568                     self._detach_direct_passthrough_vifs(context,
9569                         migrate_data, instance)
9570                 new_resources = None
9571                 if isinstance(instance, objects.Instance):
9572                     new_resources = self._sorted_migrating_resources(
9573                         instance, instance.flavor)
9574                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
9575                     # TODO(sahid): It's not a really good idea to pass
9576                     # the method _get_volume_config and we should to find
9577                     # a way to avoid this in future.
9578                     guest, migrate_data, self._get_volume_config,
9579                     get_vif_config=get_vif_config, new_resources=new_resources)
9580 
9581             # NOTE(pkoniszewski): Because of precheck which blocks
9582             # tunnelled block live migration with mapped volumes we
9583             # can safely remove migrate_disks when tunnelling is on.
9584             # Otherwise we will block all tunnelled block migrations,
9585             # even when an instance does not have volumes mapped.
9586             # This is because selective disk migration is not
9587             # supported in tunnelled block live migration. Also we
9588             # cannot fallback to migrateToURI2 in this case because of
9589             # bug #1398999
9590             #
9591             # TODO(kchamart) Move the following bit to guest.migrate()
9592             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
9593                 device_names = []
9594 
9595             # TODO(sahid): This should be in
9596             # post_live_migration_at_source but no way to retrieve
9597             # ports acquired on the host for the guest at this
9598             # step. Since the domain is going to be removed from
9599             # libvird on source host after migration, we backup the
9600             # serial ports to release them if all went well.
9601             serial_ports = []
9602             if CONF.serial_console.enabled:
9603                 serial_ports = list(self._get_serial_ports_from_guest(guest))
9604 
9605             LOG.debug("About to invoke the migrate API", instance=instance)
9606             guest.migrate(self._live_migration_uri(dest),
9607                           migrate_uri=migrate_uri,
9608                           flags=migration_flags,
9609                           migrate_disks=device_names,
9610                           destination_xml=new_xml_str,
9611                           bandwidth=CONF.libvirt.live_migration_bandwidth)
9612             LOG.debug("Migrate API has completed", instance=instance)
9613 
9614             for hostname, port in serial_ports:
9615                 serial_console.release_port(host=hostname, port=port)
9616         except Exception as e:
9617             with excutils.save_and_reraise_exception():
9618                 LOG.error("Live Migration failure: %s", e, instance=instance)
9619 
9620         # If 'migrateToURI' fails we don't know what state the
9621         # VM instances on each host are in. Possibilities include
9622         #
9623         #  1. src==running, dst==none
9624         #
9625         #     Migration failed & rolled back, or never started
9626         #
9627         #  2. src==running, dst==paused
9628         #
9629         #     Migration started but is still ongoing
9630         #
9631         #  3. src==paused,  dst==paused
9632         #
9633         #     Migration data transfer completed, but switchover
9634         #     is still ongoing, or failed
9635         #
9636         #  4. src==paused,  dst==running
9637         #
9638         #     Migration data transfer completed, switchover
9639         #     happened but cleanup on source failed
9640         #
9641         #  5. src==none,    dst==running
9642         #
9643         #     Migration fully succeeded.
9644         #
9645         # Libvirt will aim to complete any migration operation
9646         # or roll it back. So even if the migrateToURI call has
9647         # returned an error, if the migration was not finished
9648         # libvirt should clean up.
9649         #
9650         # So we take the error raise here with a pinch of salt
9651         # and rely on the domain job info status to figure out
9652         # what really happened to the VM, which is a much more
9653         # reliable indicator.
9654         #
9655         # In particular we need to try very hard to ensure that
9656         # Nova does not "forget" about the guest. ie leaving it
9657         # running on a different host to the one recorded in
9658         # the database, as that would be a serious resource leak
9659 
9660         LOG.debug("Migration operation thread has finished",
9661                   instance=instance)
9662 
9663     def _live_migration_copy_disk_paths(self, context, instance, guest):
9664         '''Get list of disks to copy during migration
9665 
9666         :param context: security context
9667         :param instance: the instance being migrated
9668         :param guest: the Guest instance being migrated
9669 
9670         Get the list of disks to copy during migration.
9671 
9672         :returns: a list of local source paths and a list of device names to
9673             copy
9674         '''
9675 
9676         disk_paths = []
9677         device_names = []
9678         block_devices = []
9679 
9680         if (self._block_migration_flags &
9681                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
9682             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
9683                 context, instance.uuid)
9684             block_device_info = driver.get_block_device_info(instance,
9685                                                              bdm_list)
9686 
9687             block_device_mappings = driver.block_device_info_get_mapping(
9688                 block_device_info)
9689             for bdm in block_device_mappings:
9690                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
9691                 block_devices.append(device_name)
9692 
9693         for dev in guest.get_all_disks():
9694             if dev.readonly or dev.shareable:
9695                 continue
9696             if dev.source_type not in ["file", "block"]:
9697                 continue
9698             if dev.target_dev in block_devices:
9699                 continue
9700             disk_paths.append(dev.source_path)
9701             device_names.append(dev.target_dev)
9702         return (disk_paths, device_names)
9703 
9704     def _live_migration_data_gb(self, instance, disk_paths):
9705         '''Calculate total amount of data to be transferred
9706 
9707         :param instance: the nova.objects.Instance being migrated
9708         :param disk_paths: list of disk paths that are being migrated
9709         with instance
9710 
9711         Calculates the total amount of data that needs to be
9712         transferred during the live migration. The actual
9713         amount copied will be larger than this, due to the
9714         guest OS continuing to dirty RAM while the migration
9715         is taking place. So this value represents the minimal
9716         data size possible.
9717 
9718         :returns: data size to be copied in GB
9719         '''
9720 
9721         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
9722         if ram_gb < 2:
9723             ram_gb = 2
9724 
9725         disk_gb = 0
9726         for path in disk_paths:
9727             try:
9728                 size = os.stat(path).st_size
9729                 size_gb = (size / units.Gi)
9730                 if size_gb < 2:
9731                     size_gb = 2
9732                 disk_gb += size_gb
9733             except OSError as e:
9734                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
9735                             {'disk': path, 'ex': e})
9736                 # Ignore error since we don't want to break
9737                 # the migration monitoring thread operation
9738 
9739         return ram_gb + disk_gb
9740 
9741     def _get_migration_flags(self, is_block_migration):
9742         if is_block_migration:
9743             return self._block_migration_flags
9744         return self._live_migration_flags
9745 
9746     def _live_migration_monitor(self, context, instance, guest,
9747                                 dest, post_method,
9748                                 recover_method, block_migration,
9749                                 migrate_data, finish_event,
9750                                 disk_paths):
9751 
9752         on_migration_failure: ty.Deque[str] = deque()
9753         data_gb = self._live_migration_data_gb(instance, disk_paths)
9754         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
9755         migration = migrate_data.migration
9756         curdowntime = None
9757 
9758         migration_flags = self._get_migration_flags(
9759                                   migrate_data.block_migration)
9760 
9761         n = 0
9762         start = time.time()
9763         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
9764         # vpmem does not support post copy
9765         is_post_copy_enabled &= not bool(self._get_vpmems(instance))
9766         while True:
9767             info = guest.get_job_info()
9768 
9769             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
9770                 # Either still running, or failed or completed,
9771                 # lets untangle the mess
9772                 if not finish_event.ready():
9773                     LOG.debug("Operation thread is still running",
9774                               instance=instance)
9775                 else:
9776                     info.type = libvirt_migrate.find_job_type(guest, instance)
9777                     LOG.debug("Fixed incorrect job type to be %d",
9778                               info.type, instance=instance)
9779 
9780             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
9781                 # Migration is not yet started
9782                 LOG.debug("Migration not running yet",
9783                           instance=instance)
9784             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
9785                 # Migration is still running
9786                 #
9787                 # This is where we wire up calls to change live
9788                 # migration status. eg change max downtime, cancel
9789                 # the operation, change max bandwidth
9790                 libvirt_migrate.run_tasks(guest, instance,
9791                                           self.active_migrations,
9792                                           on_migration_failure,
9793                                           migration,
9794                                           is_post_copy_enabled)
9795 
9796                 now = time.time()
9797                 elapsed = now - start
9798 
9799                 completion_timeout = int(
9800                     CONF.libvirt.live_migration_completion_timeout * data_gb)
9801                 # NOTE(yikun): Check the completion timeout to determine
9802                 # should trigger the timeout action, and there are two choices
9803                 # ``abort`` (default) or ``force_complete``. If the action is
9804                 # set to ``force_complete``, the post-copy will be triggered
9805                 # if available else the VM will be suspended, otherwise the
9806                 # live migrate operation will be aborted.
9807                 if libvirt_migrate.should_trigger_timeout_action(
9808                         instance, elapsed, completion_timeout,
9809                         migration.status):
9810                     timeout_act = CONF.libvirt.live_migration_timeout_action
9811                     if timeout_act == 'force_complete':
9812                         self.live_migration_force_complete(instance)
9813                     else:
9814                         # timeout action is 'abort'
9815                         try:
9816                             guest.abort_job()
9817                         except libvirt.libvirtError as e:
9818                             LOG.warning("Failed to abort migration %s",
9819                                     encodeutils.exception_to_unicode(e),
9820                                     instance=instance)
9821                             self._clear_empty_migration(instance)
9822                             raise
9823 
9824                 curdowntime = libvirt_migrate.update_downtime(
9825                     guest, instance, curdowntime,
9826                     downtime_steps, elapsed)
9827 
9828                 # We loop every 500ms, so don't log on every
9829                 # iteration to avoid spamming logs for long
9830                 # running migrations. Just once every 5 secs
9831                 # is sufficient for developers to debug problems.
9832                 # We log once every 30 seconds at info to help
9833                 # admins see slow running migration operations
9834                 # when debug logs are off.
9835                 if (n % 10) == 0:
9836                     # Ignoring memory_processed, as due to repeated
9837                     # dirtying of data, this can be way larger than
9838                     # memory_total. Best to just look at what's
9839                     # remaining to copy and ignore what's done already
9840                     #
9841                     # TODO(berrange) perhaps we could include disk
9842                     # transfer stats in the progress too, but it
9843                     # might make memory info more obscure as large
9844                     # disk sizes might dwarf memory size
9845                     remaining = 100
9846                     if info.memory_total != 0:
9847                         remaining = round(info.memory_remaining *
9848                                           100 / info.memory_total)
9849 
9850                     libvirt_migrate.save_stats(instance, migration,
9851                                                info, remaining)
9852 
9853                     # NOTE(fanzhang): do not include disk transfer stats in
9854                     # the progress percentage calculation but log them.
9855                     disk_remaining = 100
9856                     if info.disk_total != 0:
9857                         disk_remaining = round(info.disk_remaining *
9858                                                100 / info.disk_total)
9859 
9860                     lg = LOG.debug
9861                     if (n % 60) == 0:
9862                         lg = LOG.info
9863 
9864                     lg("Migration running for %(secs)d secs, "
9865                        "memory %(remaining)d%% remaining "
9866                        "(bytes processed=%(processed_memory)d, "
9867                        "remaining=%(remaining_memory)d, "
9868                        "total=%(total_memory)d); "
9869                        "disk %(disk_remaining)d%% remaining "
9870                        "(bytes processed=%(processed_disk)d, "
9871                        "remaining=%(remaining_disk)d, "
9872                        "total=%(total_disk)d).",
9873                        {"secs": elapsed, "remaining": remaining,
9874                         "processed_memory": info.memory_processed,
9875                         "remaining_memory": info.memory_remaining,
9876                         "total_memory": info.memory_total,
9877                         "disk_remaining": disk_remaining,
9878                         "processed_disk": info.disk_processed,
9879                         "remaining_disk": info.disk_remaining,
9880                         "total_disk": info.disk_total}, instance=instance)
9881 
9882                 n = n + 1
9883             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
9884                 # Migration is all done
9885                 LOG.info("Migration operation has completed",
9886                          instance=instance)
9887                 post_method(context, instance, dest, block_migration,
9888                             migrate_data)
9889                 break
9890             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
9891                 # Migration did not succeed
9892                 LOG.error("Migration operation has aborted", instance=instance)
9893                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
9894                                                   on_migration_failure)
9895                 recover_method(context, instance, dest, migrate_data)
9896                 break
9897             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
9898                 # Migration was stopped by admin
9899                 LOG.warning("Migration operation was cancelled",
9900                             instance=instance)
9901                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
9902                                                   on_migration_failure)
9903                 recover_method(context, instance, dest, migrate_data,
9904                                migration_status='cancelled')
9905                 break
9906             else:
9907                 LOG.warning("Unexpected migration job type: %d",
9908                             info.type, instance=instance)
9909 
9910             time.sleep(0.5)
9911         self._clear_empty_migration(instance)
9912 
9913     def _clear_empty_migration(self, instance):
9914         try:
9915             del self.active_migrations[instance.uuid]
9916         except KeyError:
9917             LOG.warning("There are no records in active migrations "
9918                         "for instance", instance=instance)
9919 
9920     def _live_migration(self, context, instance, dest, post_method,
9921                         recover_method, block_migration,
9922                         migrate_data):
9923         """Do live migration.
9924 
9925         :param context: security context
9926         :param instance:
9927             nova.db.sqlalchemy.models.Instance object
9928             instance object that is migrated.
9929         :param dest: destination host
9930         :param post_method:
9931             post operation method.
9932             expected nova.compute.manager._post_live_migration.
9933         :param recover_method:
9934             recovery method when any exception occurs.
9935             expected nova.compute.manager._rollback_live_migration.
9936         :param block_migration: if true, do block migration.
9937         :param migrate_data: a LibvirtLiveMigrateData object
9938 
9939         This fires off a new thread to run the blocking migration
9940         operation, and then this thread monitors the progress of
9941         migration and controls its operation
9942         """
9943 
9944         guest = self._host.get_guest(instance)
9945 
9946         disk_paths = []
9947         device_names = []
9948         if (migrate_data.block_migration and
9949                 CONF.libvirt.virt_type != "parallels"):
9950             disk_paths, device_names = self._live_migration_copy_disk_paths(
9951                 context, instance, guest)
9952 
9953         opthread = utils.spawn(self._live_migration_operation,
9954                                      context, instance, dest,
9955                                      block_migration,
9956                                      migrate_data, guest,
9957                                      device_names)
9958 
9959         finish_event = eventlet.event.Event()
9960         self.active_migrations[instance.uuid] = deque()
9961 
9962         def thread_finished(thread, event):
9963             LOG.debug("Migration operation thread notification",
9964                       instance=instance)
9965             event.send()
9966         opthread.link(thread_finished, finish_event)
9967 
9968         # Let eventlet schedule the new thread right away
9969         time.sleep(0)
9970 
9971         try:
9972             LOG.debug("Starting monitoring of live migration",
9973                       instance=instance)
9974             self._live_migration_monitor(context, instance, guest, dest,
9975                                          post_method, recover_method,
9976                                          block_migration, migrate_data,
9977                                          finish_event, disk_paths)
9978         except Exception as ex:
9979             LOG.warning("Error monitoring migration: %(ex)s",
9980                         {"ex": ex}, instance=instance, exc_info=True)
9981             raise
9982         finally:
9983             LOG.debug("Live migration monitoring is all done",
9984                       instance=instance)
9985 
9986     def _is_post_copy_enabled(self, migration_flags):
9987         return (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0
9988 
9989     def live_migration_force_complete(self, instance):
9990         try:
9991             self.active_migrations[instance.uuid].append('force-complete')
9992         except KeyError:
9993             raise exception.NoActiveMigrationForInstance(
9994                 instance_id=instance.uuid)
9995 
9996     def _try_fetch_image(self, context, path, image_id, instance,
9997                          fallback_from_host=None):
9998         try:
9999             libvirt_utils.fetch_image(context, path, image_id,
10000                                       instance.trusted_certs)
10001         except exception.ImageNotFound:
10002             if not fallback_from_host:
10003                 raise
10004             LOG.debug("Image %(image_id)s doesn't exist anymore on "
10005                       "image service, attempting to copy image "
10006                       "from %(host)s",
10007                       {'image_id': image_id, 'host': fallback_from_host})
10008             libvirt_utils.copy_image(src=path, dest=path,
10009                                      host=fallback_from_host,
10010                                      receive=True)
10011 
10012     def _fetch_instance_kernel_ramdisk(self, context, instance,
10013                                        fallback_from_host=None):
10014         """Download kernel and ramdisk for instance in instance directory."""
10015         instance_dir = libvirt_utils.get_instance_path(instance)
10016         if instance.kernel_id:
10017             kernel_path = os.path.join(instance_dir, 'kernel')
10018             # NOTE(dsanders): only fetch image if it's not available at
10019             # kernel_path. This also avoids ImageNotFound exception if
10020             # the image has been deleted from glance
10021             if not os.path.exists(kernel_path):
10022                 self._try_fetch_image(context,
10023                                       kernel_path,
10024                                       instance.kernel_id,
10025                                       instance, fallback_from_host)
10026             if instance.ramdisk_id:
10027                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
10028                 # NOTE(dsanders): only fetch image if it's not available at
10029                 # ramdisk_path. This also avoids ImageNotFound exception if
10030                 # the image has been deleted from glance
10031                 if not os.path.exists(ramdisk_path):
10032                     self._try_fetch_image(context,
10033                                           ramdisk_path,
10034                                           instance.ramdisk_id,
10035                                           instance, fallback_from_host)
10036 
10037     def _reattach_instance_vifs(self, context, instance, network_info):
10038         guest = self._host.get_guest(instance)
10039         # validate that the guest has the expected number of interfaces
10040         # attached.
10041         guest_interfaces = guest.get_interfaces()
10042         # NOTE(sean-k-mooney): In general len(guest_interfaces) will
10043         # be equal to len(network_info) as interfaces will not be hot unplugged
10044         # unless they are SR-IOV direct mode interfaces. As such we do not
10045         # need an else block here as it would be a noop.
10046         if len(guest_interfaces) < len(network_info):
10047             # NOTE(sean-k-mooney): we are doing a post live migration
10048             # for a guest with sriov vif that were detached as part of
10049             # the migration. loop over the vifs and attach the missing
10050             # vif as part of the post live migration phase.
10051             direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
10052             for vif in network_info:
10053                 if vif['vnic_type'] in direct_vnics:
10054                     LOG.info("Attaching vif %s to instance %s",
10055                              vif['id'], instance.id)
10056                     self.attach_interface(context, instance,
10057                                           instance.image_meta, vif)
10058 
10059     def rollback_live_migration_at_source(self, context, instance,
10060                                           migrate_data):
10061         """reconnect sriov interfaces after failed live migration
10062         :param context: security context
10063         :param instance:  the instance being migrated
10064         :param migrate_date: a LibvirtLiveMigrateData object
10065         """
10066         network_info = network_model.NetworkInfo(
10067             [vif.source_vif for vif in migrate_data.vifs
10068                             if "source_vif" in vif and vif.source_vif])
10069         self._reattach_instance_vifs(context, instance, network_info)
10070 
10071     def rollback_live_migration_at_destination(self, context, instance,
10072                                                network_info,
10073                                                block_device_info,
10074                                                destroy_disks=True,
10075                                                migrate_data=None):
10076         """Clean up destination node after a failed live migration."""
10077         try:
10078             self.destroy(context, instance, network_info, block_device_info,
10079                          destroy_disks)
10080         finally:
10081             # NOTE(gcb): Failed block live migration may leave instance
10082             # directory at destination node, ensure it is always deleted.
10083             is_shared_instance_path = True
10084             if migrate_data:
10085                 is_shared_instance_path = migrate_data.is_shared_instance_path
10086                 if (migrate_data.obj_attr_is_set("serial_listen_ports") and
10087                         migrate_data.serial_listen_ports):
10088                     # Releases serial ports reserved.
10089                     for port in migrate_data.serial_listen_ports:
10090                         serial_console.release_port(
10091                             host=migrate_data.serial_listen_addr, port=port)
10092 
10093             if not is_shared_instance_path:
10094                 instance_dir = libvirt_utils.get_instance_path_at_destination(
10095                     instance, migrate_data)
10096                 if os.path.exists(instance_dir):
10097                     shutil.rmtree(instance_dir)
10098 
10099     def _pre_live_migration_plug_vifs(self, instance, network_info,
10100                                       migrate_data):
10101         if 'vifs' in migrate_data and migrate_data.vifs:
10102             LOG.debug('Plugging VIFs using destination host port bindings '
10103                       'before live migration.', instance=instance)
10104             vif_plug_nw_info = network_model.NetworkInfo([])
10105             for migrate_vif in migrate_data.vifs:
10106                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
10107         else:
10108             LOG.debug('Plugging VIFs before live migration.',
10109                       instance=instance)
10110             vif_plug_nw_info = network_info
10111         # Retry operation is necessary because continuous live migration
10112         # requests to the same host cause concurrent requests to iptables,
10113         # then it complains.
10114         max_retry = CONF.live_migration_retry_count
10115         for cnt in range(max_retry):
10116             try:
10117                 self.plug_vifs(instance, vif_plug_nw_info)
10118                 break
10119             except processutils.ProcessExecutionError:
10120                 if cnt == max_retry - 1:
10121                     raise
10122                 else:
10123                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
10124                                 '%(max_retry)d.',
10125                                 {'cnt': cnt, 'max_retry': max_retry},
10126                                 instance=instance)
10127                     greenthread.sleep(1)
10128 
10129     def pre_live_migration(self, context, instance, block_device_info,
10130                            network_info, disk_info, migrate_data):
10131         """Preparation live migration."""
10132         if disk_info is not None:
10133             disk_info = jsonutils.loads(disk_info)
10134 
10135         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
10136                   instance=instance)
10137         is_shared_block_storage = migrate_data.is_shared_block_storage
10138         is_shared_instance_path = migrate_data.is_shared_instance_path
10139         is_block_migration = migrate_data.block_migration
10140 
10141         if not is_shared_instance_path:
10142             instance_dir = libvirt_utils.get_instance_path_at_destination(
10143                             instance, migrate_data)
10144 
10145             if os.path.exists(instance_dir):
10146                 raise exception.DestinationDiskExists(path=instance_dir)
10147 
10148             LOG.debug('Creating instance directory: %s', instance_dir,
10149                       instance=instance)
10150             os.mkdir(instance_dir)
10151 
10152             # Recreate the disk.info file and in doing so stop the
10153             # imagebackend from recreating it incorrectly by inspecting the
10154             # contents of each file when using the Raw backend.
10155             if disk_info:
10156                 image_disk_info = {}
10157                 for info in disk_info:
10158                     image_file = os.path.basename(info['path'])
10159                     image_path = os.path.join(instance_dir, image_file)
10160                     image_disk_info[image_path] = info['type']
10161 
10162                 LOG.debug('Creating disk.info with the contents: %s',
10163                           image_disk_info, instance=instance)
10164 
10165                 image_disk_info_path = os.path.join(instance_dir,
10166                                                     'disk.info')
10167                 with open(image_disk_info_path, 'w') as f:
10168                     f.write(jsonutils.dumps(image_disk_info))
10169 
10170             if not is_shared_block_storage:
10171                 # Ensure images and backing files are present.
10172                 LOG.debug('Checking to make sure images and backing files are '
10173                           'present before live migration.', instance=instance)
10174                 self._create_images_and_backing(
10175                     context, instance, instance_dir, disk_info,
10176                     fallback_from_host=instance.host)
10177                 if (configdrive.required_by(instance) and
10178                         CONF.config_drive_format == 'iso9660'):
10179                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
10180                     # drive needs to be copied to destination prior to
10181                     # migration when instance path is not shared and block
10182                     # storage is not shared. Files that are already present
10183                     # on destination are excluded from a list of files that
10184                     # need to be copied to destination. If we don't do that
10185                     # live migration will fail on copying iso config drive to
10186                     # destination and writing to read-only device.
10187                     # Please see bug/1246201 for more details.
10188                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
10189                     self._remotefs.copy_file(src, instance_dir)
10190 
10191             if not is_block_migration:
10192                 # NOTE(angdraug): when block storage is shared between source
10193                 # and destination and instance path isn't (e.g. volume backed
10194                 # or rbd backed instance), instance path on destination has to
10195                 # be prepared
10196 
10197                 # Required by Quobyte CI
10198                 self._ensure_console_log_for_instance(instance)
10199 
10200                 # if image has kernel and ramdisk, just download
10201                 # following normal way.
10202                 self._fetch_instance_kernel_ramdisk(context, instance)
10203 
10204         # Establishing connection to volume server.
10205         block_device_mapping = driver.block_device_info_get_mapping(
10206             block_device_info)
10207 
10208         if len(block_device_mapping):
10209             LOG.debug('Connecting volumes before live migration.',
10210                       instance=instance)
10211 
10212         for bdm in block_device_mapping:
10213             connection_info = bdm['connection_info']
10214             self._connect_volume(context, connection_info, instance)
10215 
10216         self._pre_live_migration_plug_vifs(
10217             instance, network_info, migrate_data)
10218 
10219         # Store server_listen and latest disk device info
10220         if not migrate_data:
10221             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
10222         else:
10223             migrate_data.bdms = []
10224         # Store live_migration_inbound_addr
10225         migrate_data.target_connect_addr = \
10226             CONF.libvirt.live_migration_inbound_addr
10227         migrate_data.supported_perf_events = self._supported_perf_events
10228 
10229         migrate_data.serial_listen_ports = []
10230         if CONF.serial_console.enabled:
10231             num_ports = hardware.get_number_of_serial_ports(
10232                 instance.flavor, instance.image_meta)
10233             for port in range(num_ports):
10234                 migrate_data.serial_listen_ports.append(
10235                     serial_console.acquire_port(
10236                         migrate_data.serial_listen_addr))
10237 
10238         for vol in block_device_mapping:
10239             connection_info = vol['connection_info']
10240             if connection_info.get('serial'):
10241                 disk_info = blockinfo.get_info_from_bdm(
10242                     instance, CONF.libvirt.virt_type,
10243                     instance.image_meta, vol)
10244 
10245                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
10246                 bdmi.serial = connection_info['serial']
10247                 bdmi.connection_info = connection_info
10248                 bdmi.bus = disk_info['bus']
10249                 bdmi.dev = disk_info['dev']
10250                 bdmi.type = disk_info['type']
10251                 bdmi.format = disk_info.get('format')
10252                 bdmi.boot_index = disk_info.get('boot_index')
10253                 volume_secret = self._host.find_secret('volume', vol.volume_id)
10254                 if volume_secret:
10255                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
10256 
10257                 migrate_data.bdms.append(bdmi)
10258 
10259         return migrate_data
10260 
10261     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
10262                                image_id, instance, size,
10263                                fallback_from_host=None):
10264         try:
10265             image.cache(fetch_func=fetch_func,
10266                         context=context,
10267                         filename=filename,
10268                         image_id=image_id,
10269                         size=size,
10270                         trusted_certs=instance.trusted_certs)
10271         except exception.ImageNotFound:
10272             if not fallback_from_host:
10273                 raise
10274             LOG.debug("Image %(image_id)s doesn't exist anymore "
10275                       "on image service, attempting to copy "
10276                       "image from %(host)s",
10277                       {'image_id': image_id, 'host': fallback_from_host},
10278                       instance=instance)
10279 
10280             def copy_from_host(target):
10281                 libvirt_utils.copy_image(src=target,
10282                                          dest=target,
10283                                          host=fallback_from_host,
10284                                          receive=True)
10285             image.cache(fetch_func=copy_from_host, size=size,
10286                         filename=filename)
10287 
10288         # NOTE(lyarwood): If the instance vm_state is shelved offloaded then we
10289         # must be unshelving for _try_fetch_image_cache to be called.
10290         # NOTE(mriedem): Alternatively if we are doing a cross-cell move of a
10291         # non-volume-backed server and finishing (spawning) on the dest host,
10292         # we have to flatten the rbd image so we can delete the temporary
10293         # snapshot in the compute manager.
10294         mig_context = instance.migration_context
10295         cross_cell_move = (
10296                 mig_context and mig_context.is_cross_cell_move() or False)
10297         if instance.vm_state == vm_states.SHELVED_OFFLOADED or cross_cell_move:
10298             # NOTE(lyarwood): When using the rbd imagebackend the call to cache
10299             # above will attempt to clone from the shelved snapshot in Glance
10300             # if available from this compute. We then need to flatten the
10301             # resulting image to avoid it still referencing and ultimately
10302             # blocking the removal of the shelved snapshot at the end of the
10303             # unshelve. This is a no-op for all but the rbd imagebackend.
10304             action = (
10305                 'migrating instance across cells' if cross_cell_move
10306                 else 'unshelving instance')
10307             try:
10308                 image.flatten()
10309                 LOG.debug('Image %s flattened successfully while %s.',
10310                           image.path, action, instance=instance)
10311             except NotImplementedError:
10312                 # NOTE(lyarwood): There's an argument to be made for logging
10313                 # our inability to call flatten here, however given this isn't
10314                 # implemented for most of the backends it may do more harm than
10315                 # good, concerning operators etc so for now just pass.
10316                 pass
10317 
10318     def _create_images_and_backing(self, context, instance, instance_dir,
10319                                    disk_info, fallback_from_host=None):
10320         """:param context: security context
10321            :param instance:
10322                nova.db.sqlalchemy.models.Instance object
10323                instance object that is migrated.
10324            :param instance_dir:
10325                instance path to use, calculated externally to handle block
10326                migrating an instance with an old style instance path
10327            :param disk_info:
10328                disk info specified in _get_instance_disk_info_from_config
10329                (list of dicts)
10330            :param fallback_from_host:
10331                host where we can retrieve images if the glance images are
10332                not available.
10333 
10334         """
10335 
10336         # Virtuozzo containers don't use backing file
10337         if (CONF.libvirt.virt_type == "parallels" and
10338                 instance.vm_mode == fields.VMMode.EXE):
10339             return
10340 
10341         if not disk_info:
10342             disk_info = []
10343 
10344         for info in disk_info:
10345             base = os.path.basename(info['path'])
10346             # Get image type and create empty disk image, and
10347             # create backing file in case of qcow2.
10348             instance_disk = os.path.join(instance_dir, base)
10349             if not info['backing_file'] and not os.path.exists(instance_disk):
10350                 libvirt_utils.create_image(info['type'], instance_disk,
10351                                            info['virt_disk_size'])
10352             elif info['backing_file']:
10353                 # Creating backing file follows same way as spawning instances.
10354                 cache_name = os.path.basename(info['backing_file'])
10355 
10356                 disk = self.image_backend.by_name(instance, instance_disk,
10357                                                   CONF.libvirt.images_type)
10358                 if cache_name.startswith('ephemeral'):
10359                     # The argument 'size' is used by image.cache to
10360                     # validate disk size retrieved from cache against
10361                     # the instance disk size (should always return OK)
10362                     # and ephemeral_size is used by _create_ephemeral
10363                     # to build the image if the disk is not already
10364                     # cached.
10365                     disk.cache(
10366                         fetch_func=self._create_ephemeral,
10367                         fs_label=cache_name,
10368                         os_type=instance.os_type,
10369                         filename=cache_name,
10370                         size=info['virt_disk_size'],
10371                         ephemeral_size=info['virt_disk_size'] / units.Gi)
10372                 elif cache_name.startswith('swap'):
10373                     inst_type = instance.get_flavor()
10374                     swap_mb = inst_type.swap
10375                     disk.cache(fetch_func=self._create_swap,
10376                                 filename="swap_%s" % swap_mb,
10377                                 size=swap_mb * units.Mi,
10378                                 swap_mb=swap_mb)
10379                 else:
10380                     self._try_fetch_image_cache(disk,
10381                                                 libvirt_utils.fetch_image,
10382                                                 context, cache_name,
10383                                                 instance.image_ref,
10384                                                 instance,
10385                                                 info['virt_disk_size'],
10386                                                 fallback_from_host)
10387 
10388         # if disk has kernel and ramdisk, just download
10389         # following normal way.
10390         self._fetch_instance_kernel_ramdisk(
10391             context, instance, fallback_from_host=fallback_from_host)
10392 
10393     def post_live_migration(self, context, instance, block_device_info,
10394                             migrate_data=None):
10395         # NOTE(mdbooth): The block_device_info we were passed was initialized
10396         # with BDMs from the source host before they were updated to point to
10397         # the destination. We can safely use this to disconnect the source
10398         # without re-fetching.
10399         block_device_mapping = driver.block_device_info_get_mapping(
10400                 block_device_info)
10401 
10402         for vol in block_device_mapping:
10403             connection_info = vol['connection_info']
10404             # NOTE(lyarwood): Ignore exceptions here to avoid the instance
10405             # being left in an ERROR state and still marked on the source.
10406             try:
10407                 self._disconnect_volume(context, connection_info, instance)
10408             except Exception:
10409                 volume_id = driver_block_device.get_volume_id(connection_info)
10410                 LOG.exception("Ignoring exception while attempting to "
10411                               "disconnect volume %s from the source host "
10412                               "during post_live_migration", volume_id,
10413                               instance=instance)
10414 
10415     def post_live_migration_at_source(self, context, instance, network_info):
10416         """Unplug VIFs from networks at source.
10417 
10418         :param context: security context
10419         :param instance: instance object reference
10420         :param network_info: instance network information
10421         """
10422         self.unplug_vifs(instance, network_info)
10423 
10424     def post_live_migration_at_destination(self, context,
10425                                            instance,
10426                                            network_info,
10427                                            block_migration=False,
10428                                            block_device_info=None):
10429         """Post operation of live migration at destination host.
10430 
10431         :param context: security context
10432         :param instance:
10433             nova.db.sqlalchemy.models.Instance object
10434             instance object that is migrated.
10435         :param network_info: instance network information
10436         :param block_migration: if true, post operation of block_migration.
10437         """
10438         self._reattach_instance_vifs(context, instance, network_info)
10439 
10440     def _get_instance_disk_info_from_config(self, guest_config,
10441                                             block_device_info):
10442         """Get the non-volume disk information from the domain xml
10443 
10444         :param LibvirtConfigGuest guest_config: the libvirt domain config
10445                                                 for the instance
10446         :param dict block_device_info: block device info for BDMs
10447         :returns disk_info: list of dicts with keys:
10448 
10449           * 'type': the disk type (str)
10450           * 'path': the disk path (str)
10451           * 'virt_disk_size': the virtual disk size (int)
10452           * 'backing_file': backing file of a disk image (str)
10453           * 'disk_size': physical disk size (int)
10454           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
10455         """
10456         block_device_mapping = driver.block_device_info_get_mapping(
10457             block_device_info)
10458 
10459         volume_devices = set()
10460         for vol in block_device_mapping:
10461             disk_dev = vol['mount_device'].rpartition("/")[2]
10462             volume_devices.add(disk_dev)
10463 
10464         disk_info = []
10465 
10466         if (
10467             CONF.libvirt.virt_type == 'parallels' and
10468             guest_config.os_type == fields.VMMode.EXE
10469         ):
10470             node_type = 'filesystem'
10471         else:
10472             node_type = 'disk'
10473 
10474         for device in guest_config.devices:
10475             if device.root_name != node_type:
10476                 continue
10477             disk_type = device.source_type
10478             if device.root_name == 'filesystem':
10479                 target = device.target_dir
10480                 if device.source_type == 'file':
10481                     path = device.source_file
10482                 elif device.source_type == 'block':
10483                     path = device.source_dev
10484                 else:
10485                     path = None
10486             else:
10487                 target = device.target_dev
10488                 path = device.source_path
10489 
10490             if not path:
10491                 LOG.debug('skipping disk for %s as it does not have a path',
10492                           guest_config.name)
10493                 continue
10494 
10495             if disk_type not in ['file', 'block']:
10496                 LOG.debug('skipping disk because it looks like a volume', path)
10497                 continue
10498 
10499             if target in volume_devices:
10500                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
10501                           'volume', {'path': path, 'target': target})
10502                 continue
10503 
10504             if device.root_name == 'filesystem':
10505                 driver_type = device.driver_type
10506             else:
10507                 driver_type = device.driver_format
10508             # get the real disk size or
10509             # raise a localized error if image is unavailable
10510             if disk_type == 'file' and driver_type == 'ploop':
10511                 dk_size = 0
10512                 for dirpath, dirnames, filenames in os.walk(path):
10513                     for f in filenames:
10514                         fp = os.path.join(dirpath, f)
10515                         dk_size += os.path.getsize(fp)
10516                 qemu_img_info = disk_api.get_disk_info(path)
10517                 virt_size = qemu_img_info.virtual_size
10518                 backing_file = libvirt_utils.get_disk_backing_file(path)
10519                 over_commit_size = int(virt_size) - dk_size
10520 
10521             elif disk_type == 'file' and driver_type == 'qcow2':
10522                 qemu_img_info = disk_api.get_disk_info(path)
10523                 dk_size = qemu_img_info.disk_size
10524                 virt_size = qemu_img_info.virtual_size
10525                 backing_file = libvirt_utils.get_disk_backing_file(path)
10526                 over_commit_size = max(0, int(virt_size) - dk_size)
10527 
10528             elif disk_type == 'file':
10529                 dk_size = os.stat(path).st_blocks * 512
10530                 virt_size = os.path.getsize(path)
10531                 backing_file = ""
10532                 over_commit_size = int(virt_size) - dk_size
10533 
10534             elif disk_type == 'block' and block_device_info:
10535                 dk_size = lvm.get_volume_size(path)
10536                 virt_size = dk_size
10537                 backing_file = ""
10538                 over_commit_size = 0
10539 
10540             else:
10541                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
10542                           'determine if volume',
10543                           {'path': path, 'target': target})
10544                 continue
10545 
10546             disk_info.append({'type': driver_type,
10547                               'path': path,
10548                               'virt_disk_size': virt_size,
10549                               'backing_file': backing_file,
10550                               'disk_size': dk_size,
10551                               'over_committed_disk_size': over_commit_size})
10552         return disk_info
10553 
10554     def _get_instance_disk_info(self, instance, block_device_info):
10555         try:
10556             guest = self._host.get_guest(instance)
10557             config = guest.get_config()
10558         except libvirt.libvirtError as ex:
10559             error_code = ex.get_error_code()
10560             LOG.warning('Error from libvirt while getting description of '
10561                         '%(instance_name)s: [Error Code %(error_code)s] '
10562                         '%(ex)s',
10563                         {'instance_name': instance.name,
10564                          'error_code': error_code,
10565                          'ex': encodeutils.exception_to_unicode(ex)},
10566                         instance=instance)
10567             raise exception.InstanceNotFound(instance_id=instance.uuid)
10568 
10569         return self._get_instance_disk_info_from_config(config,
10570                                                         block_device_info)
10571 
10572     def get_instance_disk_info(self, instance,
10573                                block_device_info=None):
10574         return jsonutils.dumps(
10575             self._get_instance_disk_info(instance, block_device_info))
10576 
10577     def _get_disk_over_committed_size_total(self):
10578         """Return total over committed disk size for all instances."""
10579         # Disk size that all instance uses : virtual_size - disk_size
10580         disk_over_committed_size = 0
10581         instance_domains = self._host.list_instance_domains(only_running=False)
10582         if not instance_domains:
10583             return disk_over_committed_size
10584 
10585         # Get all instance uuids
10586         instance_uuids = [dom.UUIDString() for dom in instance_domains]
10587         ctx = nova_context.get_admin_context()
10588         # Get instance object list by uuid filter
10589         filters = {'uuid': instance_uuids}
10590         # NOTE(ankit): objects.InstanceList.get_by_filters method is
10591         # getting called twice one is here and another in the
10592         # _update_available_resource method of resource_tracker. Since
10593         # _update_available_resource method is synchronized, there is a
10594         # possibility the instances list retrieved here to calculate
10595         # disk_over_committed_size would differ to the list you would get
10596         # in _update_available_resource method for calculating usages based
10597         # on instance utilization.
10598         local_instance_list = objects.InstanceList.get_by_filters(
10599             ctx, filters, use_slave=True)
10600         # Convert instance list to dictionary with instance uuid as key.
10601         local_instances = {inst.uuid: inst for inst in local_instance_list}
10602 
10603         # Get bdms by instance uuids
10604         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
10605             ctx, instance_uuids)
10606 
10607         for dom in instance_domains:
10608             try:
10609                 guest = libvirt_guest.Guest(dom)
10610                 config = guest.get_config()
10611 
10612                 block_device_info = None
10613                 if guest.uuid in local_instances \
10614                         and (bdms and guest.uuid in bdms):
10615                     # Get block device info for instance
10616                     block_device_info = driver.get_block_device_info(
10617                         local_instances[guest.uuid], bdms[guest.uuid])
10618 
10619                 disk_infos = self._get_instance_disk_info_from_config(
10620                     config, block_device_info)
10621                 if not disk_infos:
10622                     continue
10623 
10624                 for info in disk_infos:
10625                     disk_over_committed_size += int(
10626                         info['over_committed_disk_size'])
10627             except libvirt.libvirtError as ex:
10628                 error_code = ex.get_error_code()
10629                 LOG.warning(
10630                     'Error from libvirt while getting description of '
10631                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
10632                     {'instance_name': guest.name,
10633                      'error_code': error_code,
10634                      'ex': encodeutils.exception_to_unicode(ex)})
10635             except OSError as e:
10636                 if e.errno in (errno.ENOENT, errno.ESTALE):
10637                     LOG.warning('Periodic task is updating the host stat, '
10638                                 'it is trying to get disk %(i_name)s, '
10639                                 'but disk file was removed by concurrent '
10640                                 'operations such as resize.',
10641                                 {'i_name': guest.name})
10642                 elif e.errno == errno.EACCES:
10643                     LOG.warning('Periodic task is updating the host stat, '
10644                                 'it is trying to get disk %(i_name)s, '
10645                                 'but access is denied. It is most likely '
10646                                 'due to a VM that exists on the compute '
10647                                 'node but is not managed by Nova.',
10648                                 {'i_name': guest.name})
10649                 else:
10650                     raise
10651             except (exception.VolumeBDMPathNotFound,
10652                     exception.DiskNotFound) as e:
10653                 if isinstance(e, exception.VolumeBDMPathNotFound):
10654                     thing = 'backing volume block device'
10655                 elif isinstance(e, exception.DiskNotFound):
10656                     thing = 'backing disk storage'
10657 
10658                 LOG.warning('Periodic task is updating the host stats, '
10659                             'it is trying to get disk info for %(i_name)s, '
10660                             'but the %(thing)s was removed by a concurrent '
10661                             'operation such as resize. Error: %(error)s',
10662                             {'i_name': guest.name, 'thing': thing, 'error': e})
10663 
10664             # NOTE(gtt116): give other tasks a chance.
10665             greenthread.sleep(0)
10666         return disk_over_committed_size
10667 
10668     def get_available_nodes(self, refresh=False):
10669         return [self._host.get_hostname()]
10670 
10671     def get_host_cpu_stats(self):
10672         """Return the current CPU state of the host."""
10673         return self._host.get_cpu_stats()
10674 
10675     def get_host_uptime(self):
10676         """Returns the result of calling "uptime"."""
10677         out, err = processutils.execute('env', 'LANG=C', 'uptime')
10678         return out
10679 
10680     def manage_image_cache(self, context, all_instances):
10681         """Manage the local cache of images."""
10682         self.image_cache_manager.update(context, all_instances)
10683 
10684     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
10685                                   shared_storage=False):
10686         """Used only for cleanup in case migrate_disk_and_power_off fails."""
10687         try:
10688             if os.path.exists(inst_base_resize):
10689                 shutil.rmtree(inst_base, ignore_errors=True)
10690                 os.rename(inst_base_resize, inst_base)
10691                 if not shared_storage:
10692                     self._remotefs.remove_dir(dest, inst_base)
10693         except Exception:
10694             pass
10695 
10696     def cache_image(self, context, image_id):
10697         cache_dir = os.path.join(CONF.instances_path,
10698                                  CONF.image_cache.subdirectory_name)
10699         path = os.path.join(cache_dir,
10700                             imagecache.get_cache_fname(image_id))
10701         if os.path.exists(path):
10702             LOG.info('Image %(image_id)s already cached; updating timestamp',
10703                      {'image_id': image_id})
10704             # NOTE(danms): The regular image cache routines use a wrapper
10705             # (_update_utime_ignore_eacces()) around this to avoid failing
10706             # on permissions (which may or may not be legit due to an NFS
10707             # race). However, since this is best-effort, errors are swallowed
10708             # by compute manager per-image, and we are compelled to report
10709             # errors up our stack, we use the raw method here to avoid the
10710             # silent ignore of the EACCESS.
10711             nova.privsep.path.utime(path)
10712             return False
10713         else:
10714             # NOTE(danms): In case we are running before the first boot, make
10715             # sure the cache directory is created
10716             if not os.path.isdir(cache_dir):
10717                 fileutils.ensure_tree(cache_dir)
10718             LOG.info('Caching image %(image_id)s by request',
10719                      {'image_id': image_id})
10720             # NOTE(danms): The imagebackend code, as called via spawn() where
10721             # images are normally cached, uses a lock on the root disk it is
10722             # creating at the time, but relies on the
10723             # compute_utils.disk_ops_semaphore for cache fetch mutual
10724             # exclusion, which is grabbed in images.fetch() (which is called
10725             # by images.fetch_to_raw() below). So, by calling fetch_to_raw(),
10726             # we are sharing the same locking for the cache fetch as the
10727             # rest of the code currently called only from spawn().
10728             images.fetch_to_raw(context, image_id, path)
10729             return True
10730 
10731     def _get_disk_size_reserved_for_image_cache(self):
10732         """Return the amount of DISK_GB resource need to be reserved for the
10733         image cache.
10734 
10735         :returns: The disk space in GB
10736         """
10737         if not CONF.workarounds.reserve_disk_resource_for_image_cache:
10738             return 0
10739 
10740         return compute_utils.convert_mb_to_ceil_gb(
10741             self.image_cache_manager.get_disk_usage() / 1024.0 / 1024.0)
10742 
10743     def _is_path_shared_with(self, dest, path):
10744         # NOTE (rmk): There are two methods of determining whether we are
10745         #             on the same filesystem: the source and dest IP are the
10746         #             same, or we create a file on the dest system via SSH
10747         #             and check whether the source system can also see it.
10748         shared_path = (dest == self.get_host_ip_addr())
10749         if not shared_path:
10750             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
10751             tmp_path = os.path.join(path, tmp_file)
10752 
10753             try:
10754                 self._remotefs.create_file(dest, tmp_path)
10755                 if os.path.exists(tmp_path):
10756                     shared_path = True
10757                     os.unlink(tmp_path)
10758                 else:
10759                     self._remotefs.remove_file(dest, tmp_path)
10760             except Exception:
10761                 pass
10762         return shared_path
10763 
10764     def migrate_disk_and_power_off(self, context, instance, dest,
10765                                    flavor, network_info,
10766                                    block_device_info=None,
10767                                    timeout=0, retry_interval=0):
10768         LOG.debug("Starting migrate_disk_and_power_off",
10769                    instance=instance)
10770 
10771         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
10772 
10773         # get_bdm_ephemeral_disk_size() will return 0 if the new
10774         # instance's requested block device mapping contain no
10775         # ephemeral devices. However, we still want to check if
10776         # the original instance's ephemeral_gb property was set and
10777         # ensure that the new requested flavor ephemeral size is greater
10778         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
10779                     instance.flavor.ephemeral_gb)
10780 
10781         # Checks if the migration needs a disk resize down.
10782         root_down = flavor.root_gb < instance.flavor.root_gb
10783         ephemeral_down = flavor.ephemeral_gb < eph_size
10784         booted_from_volume = self._is_booted_from_volume(block_device_info)
10785 
10786         if (root_down and not booted_from_volume) or ephemeral_down:
10787             reason = _("Unable to resize disk down.")
10788             raise exception.InstanceFaultRollback(
10789                 exception.ResizeError(reason=reason))
10790 
10791         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
10792         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
10793             reason = _("Migration is not supported for LVM backed instances")
10794             raise exception.InstanceFaultRollback(
10795                 exception.MigrationPreCheckError(reason=reason))
10796 
10797         # copy disks to destination
10798         # rename instance dir to +_resize at first for using
10799         # shared storage for instance dir (eg. NFS).
10800         inst_base = libvirt_utils.get_instance_path(instance)
10801         inst_base_resize = inst_base + "_resize"
10802         shared_instance_path = self._is_path_shared_with(dest, inst_base)
10803 
10804         # try to create the directory on the remote compute node
10805         # if this fails we pass the exception up the stack so we can catch
10806         # failures here earlier
10807         if not shared_instance_path:
10808             try:
10809                 self._remotefs.create_dir(dest, inst_base)
10810             except processutils.ProcessExecutionError as e:
10811                 reason = _("not able to execute ssh command: %s") % e
10812                 raise exception.InstanceFaultRollback(
10813                     exception.ResizeError(reason=reason))
10814 
10815         self.power_off(instance, timeout, retry_interval)
10816         self.unplug_vifs(instance, network_info)
10817         block_device_mapping = driver.block_device_info_get_mapping(
10818             block_device_info)
10819         for vol in block_device_mapping:
10820             connection_info = vol['connection_info']
10821             self._disconnect_volume(context, connection_info, instance)
10822 
10823         disk_info = self._get_instance_disk_info(instance, block_device_info)
10824 
10825         try:
10826             os.rename(inst_base, inst_base_resize)
10827             # if we are migrating the instance with shared instance path then
10828             # create the directory.  If it is a remote node the directory
10829             # has already been created
10830             if shared_instance_path:
10831                 dest = None
10832                 fileutils.ensure_tree(inst_base)
10833 
10834             on_execute = lambda process: \
10835                 self.job_tracker.add_job(instance, process.pid)
10836             on_completion = lambda process: \
10837                 self.job_tracker.remove_job(instance, process.pid)
10838 
10839             for info in disk_info:
10840                 # assume inst_base == dirname(info['path'])
10841                 img_path = info['path']
10842                 fname = os.path.basename(img_path)
10843                 from_path = os.path.join(inst_base_resize, fname)
10844 
10845                 # We will not copy over the swap disk here, and rely on
10846                 # finish_migration to re-create it for us. This is ok because
10847                 # the OS is shut down, and as recreating a swap disk is very
10848                 # cheap it is more efficient than copying either locally or
10849                 # over the network. This also means we don't have to resize it.
10850                 if fname == 'disk.swap':
10851                     continue
10852 
10853                 compression = info['type'] not in NO_COMPRESSION_TYPES
10854                 libvirt_utils.copy_image(from_path, img_path, host=dest,
10855                                          on_execute=on_execute,
10856                                          on_completion=on_completion,
10857                                          compression=compression)
10858 
10859             # Ensure disk.info is written to the new path to avoid disks being
10860             # reinspected and potentially changing format.
10861             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
10862             if os.path.exists(src_disk_info_path):
10863                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
10864                 libvirt_utils.copy_image(src_disk_info_path,
10865                                          dst_disk_info_path,
10866                                          host=dest, on_execute=on_execute,
10867                                          on_completion=on_completion)
10868 
10869             # Handle migration of vTPM data if needed
10870             libvirt_utils.save_and_migrate_vtpm_dir(
10871                 instance.uuid, inst_base_resize, inst_base, dest,
10872                 on_execute, on_completion)
10873 
10874         except Exception:
10875             with excutils.save_and_reraise_exception():
10876                 self._cleanup_remote_migration(dest, inst_base,
10877                                                inst_base_resize,
10878                                                shared_instance_path)
10879 
10880         return jsonutils.dumps(disk_info)
10881 
10882     def _wait_for_running(self, instance):
10883         state = self.get_info(instance).state
10884 
10885         if state == power_state.RUNNING:
10886             LOG.info("Instance running successfully.", instance=instance)
10887             raise loopingcall.LoopingCallDone()
10888 
10889     @staticmethod
10890     def _disk_raw_to_qcow2(path):
10891         """Converts a raw disk to qcow2."""
10892         path_qcow = path + '_qcow'
10893         images.convert_image(path, path_qcow, 'raw', 'qcow2')
10894         os.rename(path_qcow, path)
10895 
10896     def _finish_migration_vtpm(
10897         self,
10898         context: nova_context.RequestContext,
10899         instance: 'objects.Instance',
10900     ) -> None:
10901         """Handle vTPM when migrating or resizing an instance.
10902 
10903         Handle the case where we're resizing between different versions of TPM,
10904         or enabling/disabling TPM.
10905         """
10906         old_vtpm_config = hardware.get_vtpm_constraint(
10907             instance.old_flavor, instance.image_meta)
10908         new_vtpm_config = hardware.get_vtpm_constraint(
10909             instance.new_flavor, instance.image_meta)
10910 
10911         if old_vtpm_config:
10912             # we had a vTPM in the old flavor; figure out if we need to do
10913             # anything with it
10914             inst_base = libvirt_utils.get_instance_path(instance)
10915             swtpm_dir = os.path.join(inst_base, 'swtpm', instance.uuid)
10916             copy_swtpm_dir = True
10917 
10918             if old_vtpm_config != new_vtpm_config:
10919                 # we had vTPM in the old flavor but the new flavor either
10920                 # doesn't or has different config; delete old TPM data and let
10921                 # libvirt create new data
10922                 if os.path.exists(swtpm_dir):
10923                     LOG.info(
10924                         'Old flavor and new flavor have different vTPM '
10925                         'configuration; removing existing vTPM data.')
10926                     copy_swtpm_dir = False
10927                     shutil.rmtree(swtpm_dir)
10928 
10929             # apparently shutil.rmtree() isn't reliable on NFS so don't rely
10930             # only on path existance here.
10931             if copy_swtpm_dir and os.path.exists(swtpm_dir):
10932                 libvirt_utils.restore_vtpm_dir(swtpm_dir)
10933         elif new_vtpm_config:
10934             # we've requested vTPM in the new flavor and didn't have one
10935             # previously so we need to create a new secret
10936             crypto.ensure_vtpm_secret(context, instance)
10937 
10938     def finish_migration(
10939         self,
10940         context: nova_context.RequestContext,
10941         migration: 'objects.Migration',
10942         instance: 'objects.Instance',
10943         disk_info: str,
10944         network_info: network_model.NetworkInfo,
10945         image_meta: 'objects.ImageMeta',
10946         resize_instance: bool,
10947         allocations: ty.Dict[str, ty.Any],
10948         block_device_info: ty.Optional[ty.Dict[str, ty.Any]] = None,
10949         power_on: bool = True,
10950     ) -> None:
10951         """Complete the migration process on the destination host."""
10952         LOG.debug("Starting finish_migration", instance=instance)
10953 
10954         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
10955                                                   instance,
10956                                                   image_meta,
10957                                                   block_device_info)
10958         # assume _create_image does nothing if a target file exists.
10959         # NOTE: This has the intended side-effect of fetching a missing
10960         # backing file.
10961         self._create_image(context, instance, block_disk_info['mapping'],
10962                            block_device_info=block_device_info,
10963                            ignore_bdi_for_swap=True,
10964                            fallback_from_host=migration.source_compute)
10965 
10966         # Required by Quobyte CI
10967         self._ensure_console_log_for_instance(instance)
10968 
10969         gen_confdrive = functools.partial(
10970             self._create_configdrive, context, instance,
10971             InjectionInfo(admin_pass=None, network_info=network_info,
10972                           files=None))
10973 
10974         # Convert raw disks to qcow2 if migrating to host which uses
10975         # qcow2 from host which uses raw.
10976         for info in jsonutils.loads(disk_info):
10977             path = info['path']
10978             disk_name = os.path.basename(path)
10979 
10980             # NOTE(mdbooth): The code below looks wrong, but is actually
10981             # required to prevent a security hole when migrating from a host
10982             # with use_cow_images=False to one with use_cow_images=True.
10983             # Imagebackend uses use_cow_images to select between the
10984             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
10985             # writes to disk.info, but does not read it as it assumes qcow2.
10986             # Therefore if we don't convert raw to qcow2 here, a raw disk will
10987             # be incorrectly assumed to be qcow2, which is a severe security
10988             # flaw. The reverse is not true, because the atrociously-named-Raw
10989             # backend supports both qcow2 and raw disks, and will choose
10990             # appropriately between them as long as disk.info exists and is
10991             # correctly populated, which it is because Qcow2 writes to
10992             # disk.info.
10993             #
10994             # In general, we do not yet support format conversion during
10995             # migration. For example:
10996             #   * Converting from use_cow_images=True to use_cow_images=False
10997             #     isn't handled. This isn't a security bug, but is almost
10998             #     certainly buggy in other cases, as the 'Raw' backend doesn't
10999             #     expect a backing file.
11000             #   * Converting to/from lvm and rbd backends is not supported.
11001             #
11002             # This behaviour is inconsistent, and therefore undesirable for
11003             # users. It is tightly-coupled to implementation quirks of 2
11004             # out of 5 backends in imagebackend and defends against a severe
11005             # security flaw which is not at all obvious without deep analysis,
11006             # and is therefore undesirable to developers. We should aim to
11007             # remove it. This will not be possible, though, until we can
11008             # represent the storage layout of a specific instance
11009             # independent of the default configuration of the local compute
11010             # host.
11011 
11012             # Config disks are hard-coded to be raw even when
11013             # use_cow_images=True (see _get_disk_config_image_type),so don't
11014             # need to be converted.
11015             if (disk_name != 'disk.config' and
11016                         info['type'] == 'raw' and CONF.use_cow_images):
11017                 self._disk_raw_to_qcow2(info['path'])
11018 
11019         # Does the guest need to be assigned some vGPU mediated devices ?
11020         mdevs = self._allocate_mdevs(allocations)
11021 
11022         # Handle the case where the guest has emulated TPM
11023         self._finish_migration_vtpm(context, instance)
11024 
11025         xml = self._get_guest_xml(context, instance, network_info,
11026                                   block_disk_info, image_meta,
11027                                   block_device_info=block_device_info,
11028                                   mdevs=mdevs)
11029         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
11030         # or not we've migrated to another host, because we unplug VIFs locally
11031         # and the status change in the port might go undetected by the neutron
11032         # L2 agent (or neutron server) so neutron may not know that the VIF was
11033         # unplugged in the first place and never send an event.
11034         guest = self._create_guest_with_network(
11035             context, xml, instance, network_info, block_device_info,
11036             power_on=power_on, vifs_already_plugged=True,
11037             post_xml_callback=gen_confdrive)
11038         if power_on:
11039             timer = loopingcall.FixedIntervalLoopingCall(
11040                                                     self._wait_for_running,
11041                                                     instance)
11042             timer.start(interval=0.5).wait()
11043 
11044             # Sync guest time after migration.
11045             guest.sync_guest_time()
11046 
11047         LOG.debug("finish_migration finished successfully.", instance=instance)
11048 
11049     def _cleanup_failed_migration(self, inst_base):
11050         """Make sure that a failed migrate doesn't prevent us from rolling
11051         back in a revert.
11052         """
11053         try:
11054             shutil.rmtree(inst_base)
11055         except OSError as e:
11056             if e.errno != errno.ENOENT:
11057                 raise
11058 
11059     def _finish_revert_migration_vtpm(
11060         self,
11061         context: nova_context.RequestContext,
11062         instance: 'objects.Instance',
11063     ) -> None:
11064         """Handle vTPM differences when reverting a migration or resize.
11065 
11066         We should either restore any emulated vTPM persistent storage files or
11067         create new ones.
11068         """
11069         old_vtpm_config = hardware.get_vtpm_constraint(
11070             instance.old_flavor, instance.image_meta)
11071         new_vtpm_config = hardware.get_vtpm_constraint(
11072             instance.new_flavor, instance.image_meta)
11073 
11074         if old_vtpm_config:
11075             # the instance had a vTPM before resize and should have one again;
11076             # move the previously-saved vTPM data back to its proper location
11077             inst_base = libvirt_utils.get_instance_path(instance)
11078             swtpm_dir = os.path.join(inst_base, 'swtpm', instance.uuid)
11079             if os.path.exists(swtpm_dir):
11080                 libvirt_utils.restore_vtpm_dir(swtpm_dir)
11081         elif new_vtpm_config:
11082             # the instance gained a vTPM and must now lose it; delete the vTPM
11083             # secret, knowing that libvirt will take care of everything else on
11084             # the destination side
11085             crypto.delete_vtpm_secret(context, instance)
11086 
11087     def finish_revert_migration(
11088         self,
11089         context: nova.context.RequestContext,
11090         instance: 'objects.Instance',
11091         network_info: network_model.NetworkInfo,
11092         migration: 'objects.Migration',
11093         block_device_info: ty.Optional[ty.Dict[str, ty.Any]] = None,
11094         power_on: bool = True,
11095     ) -> None:
11096         """Finish the second half of reverting a resize on the source host."""
11097         LOG.debug('Starting finish_revert_migration', instance=instance)
11098 
11099         inst_base = libvirt_utils.get_instance_path(instance)
11100         inst_base_resize = inst_base + "_resize"
11101 
11102         # NOTE(danms): if we're recovering from a failed migration,
11103         # make sure we don't have a left-over same-host base directory
11104         # that would conflict. Also, don't fail on the rename if the
11105         # failure happened early.
11106         if os.path.exists(inst_base_resize):
11107             self._cleanup_failed_migration(inst_base)
11108             os.rename(inst_base_resize, inst_base)
11109 
11110         root_disk = self.image_backend.by_name(instance, 'disk')
11111         # Once we rollback, the snapshot is no longer needed, so remove it
11112         if root_disk.exists():
11113             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
11114             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
11115 
11116         self._finish_revert_migration_vtpm(context, instance)
11117 
11118         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
11119                                             instance,
11120                                             instance.image_meta,
11121                                             block_device_info)
11122 
11123         # The guest could already have mediated devices, using them for
11124         # the new XML
11125         mdevs = list(self._get_all_assigned_mediated_devices(instance))
11126 
11127         xml = self._get_guest_xml(context, instance, network_info, disk_info,
11128                                   instance.image_meta,
11129                                   block_device_info=block_device_info,
11130                                   mdevs=mdevs)
11131         # NOTE(artom) In some Neutron or port configurations we've already
11132         # waited for vif-plugged events in the compute manager's
11133         # _finish_revert_resize_network_migrate_finish(), right after updating
11134         # the port binding. For any ports not covered by those "bind-time"
11135         # events, we wait for "plug-time" events here.
11136         events = network_info.get_plug_time_events(migration)
11137         if events:
11138             LOG.debug('Instance is using plug-time events: %s', events,
11139                       instance=instance)
11140         self._create_guest_with_network(
11141             context, xml, instance, network_info, block_device_info,
11142             power_on=power_on, external_events=events)
11143 
11144         if power_on:
11145             timer = loopingcall.FixedIntervalLoopingCall(
11146                                                     self._wait_for_running,
11147                                                     instance)
11148             timer.start(interval=0.5).wait()
11149 
11150         LOG.debug("finish_revert_migration finished successfully.",
11151                   instance=instance)
11152 
11153     def confirm_migration(self, context, migration, instance, network_info):
11154         """Confirms a resize, destroying the source VM."""
11155         self._cleanup_resize(context, instance, network_info)
11156 
11157     @staticmethod
11158     def _get_io_devices(xml_doc):
11159         """get the list of io devices from the xml document."""
11160         result: ty.Dict[str, ty.List[str]] = {"volumes": [], "ifaces": []}
11161         try:
11162             doc = etree.fromstring(xml_doc)
11163         except Exception:
11164             return result
11165         blocks = [('./devices/disk', 'volumes'),
11166             ('./devices/interface', 'ifaces')]
11167         for block, key in blocks:
11168             section = doc.findall(block)
11169             for node in section:
11170                 for child in node:
11171                     if child.tag == 'target' and child.get('dev'):
11172                         result[key].append(child.get('dev'))
11173         return result
11174 
11175     def get_diagnostics(self, instance):
11176         guest = self._host.get_guest(instance)
11177 
11178         # TODO(sahid): We are converting all calls from a
11179         # virDomain object to use nova.virt.libvirt.Guest.
11180         # We should be able to remove domain at the end.
11181         domain = guest._domain
11182         output = {}
11183         # get cpu time, might launch an exception if the method
11184         # is not supported by the underlying hypervisor being
11185         # used by libvirt
11186         try:
11187             for vcpu in guest.get_vcpus_info():
11188                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
11189         except libvirt.libvirtError:
11190             pass
11191         # get io status
11192         xml = guest.get_xml_desc()
11193         dom_io = LibvirtDriver._get_io_devices(xml)
11194         for guest_disk in dom_io["volumes"]:
11195             try:
11196                 # blockStats might launch an exception if the method
11197                 # is not supported by the underlying hypervisor being
11198                 # used by libvirt
11199                 stats = domain.blockStats(guest_disk)
11200                 output[guest_disk + "_read_req"] = stats[0]
11201                 output[guest_disk + "_read"] = stats[1]
11202                 output[guest_disk + "_write_req"] = stats[2]
11203                 output[guest_disk + "_write"] = stats[3]
11204                 output[guest_disk + "_errors"] = stats[4]
11205             except libvirt.libvirtError:
11206                 pass
11207         for interface in dom_io["ifaces"]:
11208             try:
11209                 # interfaceStats might launch an exception if the method
11210                 # is not supported by the underlying hypervisor being
11211                 # used by libvirt
11212                 stats = domain.interfaceStats(interface)
11213                 output[interface + "_rx"] = stats[0]
11214                 output[interface + "_rx_packets"] = stats[1]
11215                 output[interface + "_rx_errors"] = stats[2]
11216                 output[interface + "_rx_drop"] = stats[3]
11217                 output[interface + "_tx"] = stats[4]
11218                 output[interface + "_tx_packets"] = stats[5]
11219                 output[interface + "_tx_errors"] = stats[6]
11220                 output[interface + "_tx_drop"] = stats[7]
11221             except libvirt.libvirtError:
11222                 pass
11223         output["memory"] = domain.maxMemory()
11224         # memoryStats might launch an exception if the method
11225         # is not supported by the underlying hypervisor being
11226         # used by libvirt
11227         try:
11228             mem = domain.memoryStats()
11229             for key in mem.keys():
11230                 output["memory-" + key] = mem[key]
11231         except (libvirt.libvirtError, AttributeError):
11232             pass
11233         return output
11234 
11235     def get_instance_diagnostics(self, instance):
11236         guest = self._host.get_guest(instance)
11237 
11238         # TODO(sahid): We are converting all calls from a
11239         # virDomain object to use nova.virt.libvirt.Guest.
11240         # We should be able to remove domain at the end.
11241         domain = guest._domain
11242 
11243         xml = guest.get_xml_desc()
11244         xml_doc = etree.fromstring(xml)
11245 
11246         # TODO(sahid): Needs to use get_info but more changes have to
11247         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
11248         # needed.
11249         state, max_mem, mem, num_cpu, cpu_time = guest._get_domain_info()
11250         config_drive = configdrive.required_by(instance)
11251         launched_at = timeutils.normalize_time(instance.launched_at)
11252         uptime = timeutils.delta_seconds(launched_at,
11253                                          timeutils.utcnow())
11254         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
11255                                         driver='libvirt',
11256                                         config_drive=config_drive,
11257                                         hypervisor=CONF.libvirt.virt_type,
11258                                         hypervisor_os='linux',
11259                                         uptime=uptime)
11260         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
11261             maximum=max_mem / units.Mi,
11262             used=mem / units.Mi)
11263 
11264         # get cpu time, might launch an exception if the method
11265         # is not supported by the underlying hypervisor being
11266         # used by libvirt
11267         try:
11268             for vcpu in guest.get_vcpus_info():
11269                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
11270         except libvirt.libvirtError:
11271             pass
11272         # get io status
11273         dom_io = LibvirtDriver._get_io_devices(xml)
11274         for guest_disk in dom_io["volumes"]:
11275             try:
11276                 # blockStats might launch an exception if the method
11277                 # is not supported by the underlying hypervisor being
11278                 # used by libvirt
11279                 stats = domain.blockStats(guest_disk)
11280                 diags.add_disk(read_bytes=stats[1],
11281                                read_requests=stats[0],
11282                                write_bytes=stats[3],
11283                                write_requests=stats[2],
11284                                errors_count=stats[4])
11285             except libvirt.libvirtError:
11286                 pass
11287 
11288         for interface in xml_doc.findall('./devices/interface'):
11289             mac_address = interface.find('mac').get('address')
11290             target = interface.find('./target')
11291 
11292             # add nic that has no target (therefore no stats)
11293             if target is None:
11294                 diags.add_nic(mac_address=mac_address)
11295                 continue
11296 
11297             # add nic with stats
11298             dev = target.get('dev')
11299             try:
11300                 if dev:
11301                     # interfaceStats might launch an exception if the
11302                     # method is not supported by the underlying hypervisor
11303                     # being used by libvirt
11304                     stats = domain.interfaceStats(dev)
11305                     diags.add_nic(mac_address=mac_address,
11306                                   rx_octets=stats[0],
11307                                   rx_errors=stats[2],
11308                                   rx_drop=stats[3],
11309                                   rx_packets=stats[1],
11310                                   tx_octets=stats[4],
11311                                   tx_errors=stats[6],
11312                                   tx_drop=stats[7],
11313                                   tx_packets=stats[5])
11314 
11315             except libvirt.libvirtError:
11316                 pass
11317 
11318         return diags
11319 
11320     @staticmethod
11321     def _prepare_device_bus(dev):
11322         """Determines the device bus and its hypervisor assigned address
11323         """
11324         bus = None
11325         address = (dev.device_addr.format_address() if
11326                    dev.device_addr else None)
11327         if isinstance(dev.device_addr,
11328                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
11329             bus = objects.PCIDeviceBus()
11330         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
11331             if dev.target_bus == 'scsi':
11332                 bus = objects.SCSIDeviceBus()
11333             elif dev.target_bus == 'ide':
11334                 bus = objects.IDEDeviceBus()
11335             elif dev.target_bus == 'usb':
11336                 bus = objects.USBDeviceBus()
11337         if address is not None and bus is not None:
11338             bus.address = address
11339         return bus
11340 
11341     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
11342                                   trusted_by_mac):
11343         """Builds a metadata object for a network interface
11344 
11345         :param dev: The LibvirtConfigGuestInterface to build metadata for.
11346         :param vifs_to_expose: The list of tagged and/or vlan'ed
11347                                VirtualInterface objects.
11348         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
11349         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
11350                                associations.
11351         :return: A NetworkInterfaceMetadata object, or None.
11352         """
11353         vif = vifs_to_expose.get(dev.mac_addr)
11354         if not vif:
11355             LOG.debug('No VIF found with MAC %s, not building metadata',
11356                       dev.mac_addr)
11357             return None
11358         bus = self._prepare_device_bus(dev)
11359         device = objects.NetworkInterfaceMetadata(mac=vif.address)
11360         if 'tag' in vif and vif.tag:
11361             device.tags = [vif.tag]
11362         if bus:
11363             device.bus = bus
11364         vlan = vlans_by_mac.get(vif.address)
11365         if vlan:
11366             device.vlan = int(vlan)
11367         device.vf_trusted = trusted_by_mac.get(vif.address, False)
11368         return device
11369 
11370     def _build_disk_metadata(self, dev, tagged_bdms):
11371         """Builds a metadata object for a disk
11372 
11373         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
11374         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
11375         :return: A DiskMetadata object, or None.
11376         """
11377         bdm = tagged_bdms.get(dev.target_dev)
11378         if not bdm:
11379             LOG.debug('No BDM found with device name %s, not building '
11380                       'metadata.', dev.target_dev)
11381             return None
11382         bus = self._prepare_device_bus(dev)
11383         device = objects.DiskMetadata(tags=[bdm.tag])
11384         # NOTE(artom) Setting the serial (which corresponds to
11385         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
11386         # find the disks's BlockDeviceMapping object when we detach the
11387         # volume and want to clean up its metadata.
11388         device.serial = bdm.volume_id
11389         if bus:
11390             device.bus = bus
11391         return device
11392 
11393     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
11394         """Builds a metadata object for a hostdev. This can only be a PF, so we
11395         don't need trusted_by_mac like in _build_interface_metadata because
11396         only VFs can be trusted.
11397 
11398         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
11399         :param vifs_to_expose: The list of tagged and/or vlan'ed
11400                                VirtualInterface objects.
11401         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
11402         :return: A NetworkInterfaceMetadata object, or None.
11403         """
11404         # Strip out the leading '0x'
11405         pci_address = pci_utils.get_pci_address(
11406             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
11407         try:
11408             mac = pci_utils.get_mac_by_pci_address(pci_address,
11409                                                    pf_interface=True)
11410         except exception.PciDeviceNotFoundById:
11411             LOG.debug('Not exposing metadata for not found PCI device %s',
11412                       pci_address)
11413             return None
11414 
11415         vif = vifs_to_expose.get(mac)
11416         if not vif:
11417             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
11418             return None
11419 
11420         device = objects.NetworkInterfaceMetadata(mac=mac)
11421         device.bus = objects.PCIDeviceBus(address=pci_address)
11422         if 'tag' in vif and vif.tag:
11423             device.tags = [vif.tag]
11424         vlan = vlans_by_mac.get(mac)
11425         if vlan:
11426             device.vlan = int(vlan)
11427         return device
11428 
11429     def _build_device_metadata(self, context, instance):
11430         """Builds a metadata object for instance devices, that maps the user
11431            provided tag to the hypervisor assigned device address.
11432         """
11433         def _get_device_name(bdm):
11434             return block_device.strip_dev(bdm.device_name)
11435 
11436         network_info = instance.info_cache.network_info
11437         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
11438         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
11439         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
11440                                                                  instance.uuid)
11441         vifs_to_expose = {vif.address: vif for vif in vifs
11442                           if ('tag' in vif and vif.tag) or
11443                              vlans_by_mac.get(vif.address)}
11444         # TODO(mriedem): We should be able to avoid the DB query here by using
11445         # block_device_info['block_device_mapping'] which is passed into most
11446         # methods that call this function.
11447         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
11448             context, instance.uuid)
11449         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
11450 
11451         devices = []
11452         guest = self._host.get_guest(instance)
11453         xml = guest.get_xml_desc()
11454         xml_dom = etree.fromstring(xml)
11455         guest_config = vconfig.LibvirtConfigGuest()
11456         guest_config.parse_dom(xml_dom)
11457 
11458         for dev in guest_config.devices:
11459             device = None
11460             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
11461                 device = self._build_interface_metadata(dev, vifs_to_expose,
11462                                                         vlans_by_mac,
11463                                                         trusted_by_mac)
11464             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
11465                 device = self._build_disk_metadata(dev, tagged_bdms)
11466             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
11467                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
11468                                                       vlans_by_mac)
11469             if device:
11470                 devices.append(device)
11471         if devices:
11472             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
11473             return dev_meta
11474 
11475     def instance_on_disk(self, instance):
11476         # ensure directories exist and are writable
11477         instance_path = libvirt_utils.get_instance_path(instance)
11478         LOG.debug('Checking instance files accessibility %s', instance_path,
11479                   instance=instance)
11480         shared_instance_path = os.access(instance_path, os.W_OK)
11481         # NOTE(flwang): For shared block storage scenario, the file system is
11482         # not really shared by the two hosts, but the volume of evacuated
11483         # instance is reachable.
11484         shared_block_storage = (self.image_backend.backend().
11485                                 is_shared_block_storage())
11486         return shared_instance_path or shared_block_storage
11487 
11488     def inject_network_info(self, instance, nw_info):
11489         pass
11490 
11491     def delete_instance_files(self, instance):
11492         target = libvirt_utils.get_instance_path(instance)
11493         # A resize may be in progress
11494         target_resize = target + '_resize'
11495         # Other threads may attempt to rename the path, so renaming the path
11496         # to target + '_del' (because it is atomic) and iterating through
11497         # twice in the unlikely event that a concurrent rename occurs between
11498         # the two rename attempts in this method. In general this method
11499         # should be fairly thread-safe without these additional checks, since
11500         # other operations involving renames are not permitted when the task
11501         # state is not None and the task state should be set to something
11502         # other than None by the time this method is invoked.
11503         target_del = target + '_del'
11504         for i in range(2):
11505             try:
11506                 os.rename(target, target_del)
11507                 break
11508             except Exception:
11509                 pass
11510             try:
11511                 os.rename(target_resize, target_del)
11512                 break
11513             except Exception:
11514                 pass
11515         # Either the target or target_resize path may still exist if all
11516         # rename attempts failed.
11517         remaining_path = None
11518         for p in (target, target_resize):
11519             if os.path.exists(p):
11520                 remaining_path = p
11521                 break
11522 
11523         # A previous delete attempt may have been interrupted, so target_del
11524         # may exist even if all rename attempts during the present method
11525         # invocation failed due to the absence of both target and
11526         # target_resize.
11527         if not remaining_path and os.path.exists(target_del):
11528             self.job_tracker.terminate_jobs(instance)
11529 
11530             LOG.info('Deleting instance files %s', target_del,
11531                      instance=instance)
11532             remaining_path = target_del
11533             try:
11534                 shutil.rmtree(target_del)
11535             except OSError as e:
11536                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
11537                           {'target': target_del, 'e': e}, instance=instance)
11538 
11539         # It is possible that the delete failed, if so don't mark the instance
11540         # as cleaned.
11541         if remaining_path and os.path.exists(remaining_path):
11542             LOG.info('Deletion of %s failed', remaining_path,
11543                      instance=instance)
11544             return False
11545 
11546         LOG.info('Deletion of %s complete', target_del, instance=instance)
11547         return True
11548 
11549     @property
11550     def need_legacy_block_device_info(self):
11551         return False
11552 
11553     def default_root_device_name(self, instance, image_meta, root_bdm):
11554         disk_bus = blockinfo.get_disk_bus_for_device_type(
11555             instance, CONF.libvirt.virt_type, image_meta, "disk")
11556         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
11557             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
11558         root_info = blockinfo.get_root_info(
11559             instance, CONF.libvirt.virt_type, image_meta,
11560             root_bdm, disk_bus, cdrom_bus)
11561         return block_device.prepend_dev(root_info['dev'])
11562 
11563     def default_device_names_for_instance(self, instance, root_device_name,
11564                                           *block_device_lists):
11565         block_device_mapping = list(itertools.chain(*block_device_lists))
11566         # NOTE(ndipanov): Null out the device names so that blockinfo code
11567         #                 will assign them
11568         for bdm in block_device_mapping:
11569             if bdm.device_name is not None:
11570                 LOG.info(
11571                     "Ignoring supplied device name: %(device_name)s. "
11572                     "Libvirt can't honour user-supplied dev names",
11573                     {'device_name': bdm.device_name}, instance=instance)
11574                 bdm.device_name = None
11575         block_device_info = driver.get_block_device_info(instance,
11576                                                          block_device_mapping)
11577 
11578         blockinfo.default_device_names(CONF.libvirt.virt_type,
11579                                        nova_context.get_admin_context(),
11580                                        instance,
11581                                        block_device_info,
11582                                        instance.image_meta)
11583 
11584     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
11585         block_device_info = driver.get_block_device_info(instance, bdms)
11586         instance_info = blockinfo.get_disk_info(
11587                 CONF.libvirt.virt_type, instance,
11588                 instance.image_meta, block_device_info=block_device_info)
11589 
11590         suggested_dev_name = block_device_obj.device_name
11591         if suggested_dev_name is not None:
11592             LOG.info(
11593                 'Ignoring supplied device name: %(suggested_dev)s',
11594                 {'suggested_dev': suggested_dev_name}, instance=instance)
11595 
11596         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
11597         #                 only when it's actually not set on the bd object
11598         block_device_obj.device_name = None
11599         disk_info = blockinfo.get_info_from_bdm(
11600             instance, CONF.libvirt.virt_type, instance.image_meta,
11601             block_device_obj, mapping=instance_info['mapping'])
11602         return block_device.prepend_dev(disk_info['dev'])
11603 
11604     def is_supported_fs_format(self, fs_type):
11605         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
11606                            nova.privsep.fs.FS_FORMAT_EXT3,
11607                            nova.privsep.fs.FS_FORMAT_EXT4,
11608                            nova.privsep.fs.FS_FORMAT_XFS]
11609 
11610     def _get_tpm_traits(self) -> ty.Dict[str, bool]:
11611         # Assert or deassert TPM support traits
11612         return {
11613             ot.COMPUTE_SECURITY_TPM_2_0: CONF.libvirt.swtpm_enabled,
11614             ot.COMPUTE_SECURITY_TPM_1_2: CONF.libvirt.swtpm_enabled,
11615         }
11616 
11617     def _get_vif_model_traits(self) -> ty.Dict[str, bool]:
11618         """Get vif model traits based on the currently enabled virt_type.
11619 
11620         Not all traits generated by this function may be valid and the result
11621         should be validated.
11622 
11623         :return: A dict of trait names mapped to boolean values.
11624         """
11625         all_models = set(itertools.chain(
11626             *libvirt_vif.SUPPORTED_VIF_MODELS.values()
11627         ))
11628         supported_models = libvirt_vif.SUPPORTED_VIF_MODELS.get(
11629             CONF.libvirt.virt_type, []
11630         )
11631         # construct the corresponding standard trait from the VIF model name
11632         return {
11633             f'COMPUTE_NET_VIF_MODEL_{model.replace("-", "_").upper()}': model
11634             in supported_models for model in all_models
11635         }
11636 
11637     def _get_storage_bus_traits(self) -> ty.Dict[str, bool]:
11638         """Get storage bus traits based on the currently enabled virt_type.
11639 
11640         For QEMU and KVM this function uses the information returned by the
11641         libvirt domain capabilities API. For other virt types we generate the
11642         traits based on the static information in the blockinfo module.
11643 
11644         Not all traits generated by this function may be valid and the result
11645         should be validated.
11646 
11647         :return: A dict of trait names mapped to boolean values.
11648         """
11649         all_buses = set(itertools.chain(
11650             *blockinfo.SUPPORTED_DEVICE_BUSES.values()
11651         ))
11652 
11653         if CONF.libvirt.virt_type in ('qemu', 'kvm'):
11654             dom_caps = self._host.get_domain_capabilities()
11655             supported_buses: ty.Set[str] = set()
11656             for arch_type in dom_caps:
11657                 for machine_type in dom_caps[arch_type]:
11658                     supported_buses.update(
11659                         dom_caps[arch_type][machine_type].devices.disk.buses
11660                     )
11661         else:
11662             supported_buses = blockinfo.SUPPORTED_DEVICE_BUSES.get(
11663                 CONF.libvirt.virt_type, []
11664             )
11665 
11666         # construct the corresponding standard trait from the storage bus name
11667         return {
11668             f'COMPUTE_STORAGE_BUS_{bus.replace("-", "_").upper()}': bus in
11669             supported_buses for bus in all_buses
11670         }
11671 
11672     def _get_video_model_traits(self) -> ty.Dict[str, bool]:
11673         """Get video model traits from libvirt.
11674 
11675         Not all traits generated by this function may be valid and the result
11676         should be validated.
11677 
11678         :return: A dict of trait names mapped to boolean values.
11679         """
11680         all_models = fields.VideoModel.ALL
11681 
11682         dom_caps = self._host.get_domain_capabilities()
11683         supported_models: ty.Set[str] = set()
11684         for arch_type in dom_caps:
11685             for machine_type in dom_caps[arch_type]:
11686                 supported_models.update(
11687                     dom_caps[arch_type][machine_type].devices.video.models
11688                 )
11689 
11690         # construct the corresponding standard trait from the video model name
11691         return {
11692             f'COMPUTE_GRAPHICS_MODEL_{model.replace("-", "_").upper()}': model
11693             in supported_models for model in all_models
11694         }
11695 
11696     def _get_cpu_traits(self) -> ty.Dict[str, bool]:
11697         """Get CPU-related traits to be set and unset on the host's resource
11698         provider.
11699 
11700         :return: A dict of trait names mapped to boolean values.
11701         """
11702         traits = self._get_cpu_feature_traits()
11703         traits[ot.HW_CPU_X86_AMD_SEV] = self._host.supports_amd_sev
11704         traits[ot.HW_CPU_HYPERTHREADING] = self._host.has_hyperthreading
11705 
11706         return traits
11707 
11708     def _get_cpu_feature_traits(self) -> ty.Dict[str, bool]:
11709         """Get CPU traits of VMs based on guest CPU model config.
11710 
11711         1. If mode is 'host-model' or 'host-passthrough', use host's
11712            CPU features.
11713         2. If mode is None, choose a default CPU model based on CPU
11714            architecture.
11715         3. If mode is 'custom', use cpu_models to generate CPU features.
11716 
11717         The code also accounts for cpu_model_extra_flags configuration when
11718         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
11719         ensures user specified CPU feature flags to be included.
11720 
11721         :return: A dict of trait names mapped to boolean values.
11722         """
11723         cpu = self._get_guest_cpu_model_config()
11724         if not cpu:
11725             LOG.info('The current libvirt hypervisor %(virt_type)s '
11726                      'does not support reporting CPU traits.',
11727                      {'virt_type': CONF.libvirt.virt_type})
11728             return {}
11729 
11730         caps = deepcopy(self._host.get_capabilities())
11731         if cpu.mode in ('host-model', 'host-passthrough'):
11732             # Account for features in cpu_model_extra_flags conf
11733             host_features: ty.Set[str] = {
11734                 f.name for f in caps.host.cpu.features | cpu.features
11735             }
11736             return libvirt_utils.cpu_features_to_traits(host_features)
11737 
11738         def _resolve_features(cpu):
11739             xml_str = cpu.to_xml()
11740             features_xml = self._get_guest_baseline_cpu_features(xml_str)
11741             feature_names = []
11742             if features_xml:
11743                 cpu = vconfig.LibvirtConfigCPU()
11744                 cpu.parse_str(features_xml)
11745                 feature_names = [f.name for f in cpu.features]
11746             return feature_names
11747 
11748         features: ty.Set[str] = set()
11749         # Choose a default CPU model when cpu_mode is not specified
11750         if cpu.mode is None:
11751             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
11752                 caps.host.cpu.arch)
11753             caps.host.cpu.features = set()
11754             features = features.union(_resolve_features(caps.host.cpu))
11755         else:
11756             models = [self._get_cpu_model_mapping(model)
11757                       for model in CONF.libvirt.cpu_models]
11758 
11759             # Aarch64 platform doesn't return the default CPU models
11760             if caps.host.cpu.arch == fields.Architecture.AARCH64:
11761                 if not models:
11762                     models = ['max']
11763             # For custom mode, iterate through cpu models
11764             for model in models:
11765                 caps.host.cpu.model = model
11766                 caps.host.cpu.features = set()
11767                 features = features.union(_resolve_features(caps.host.cpu))
11768             # Account for features in cpu_model_extra_flags conf
11769             features = features.union([f.name for f in cpu.features])
11770 
11771         return libvirt_utils.cpu_features_to_traits(features)
11772 
11773     def _get_guest_baseline_cpu_features(self, xml_str):
11774         """Calls libvirt's baselineCPU API to compute the biggest set of
11775         CPU features which is compatible with the given host CPU.
11776 
11777         :param xml_str: XML description of host CPU
11778         :return: An XML string of the computed CPU, or None on error
11779         """
11780         LOG.debug("Libvirt baseline CPU %s", xml_str)
11781         # TODO(lei-zh): baselineCPU is not supported on all platforms.
11782         # There is some work going on in the libvirt community to replace the
11783         # baseline call. Consider using the new apis when they are ready. See
11784         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
11785         try:
11786             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
11787                 return self._host.get_connection().baselineCPU(
11788                     [xml_str],
11789                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
11790             else:
11791                 return self._host.get_connection().baselineCPU([xml_str])
11792         except libvirt.libvirtError as ex:
11793             with excutils.save_and_reraise_exception() as ctxt:
11794                 error_code = ex.get_error_code()
11795                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
11796                     ctxt.reraise = False
11797                     LOG.debug('URI %(uri)s does not support full set'
11798                               ' of host capabilities: %(error)s',
11799                               {'uri': self._host._uri, 'error': ex})
11800                     return None
