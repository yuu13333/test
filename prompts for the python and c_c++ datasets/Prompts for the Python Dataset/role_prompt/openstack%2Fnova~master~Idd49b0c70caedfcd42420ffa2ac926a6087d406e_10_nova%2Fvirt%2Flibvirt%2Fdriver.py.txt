I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import copy
33 import errno
34 import functools
35 import glob
36 import itertools
37 import operator
38 import os
39 import pwd
40 import random
41 import shutil
42 import tempfile
43 import time
44 import uuid
45 
46 from castellan import key_manager
47 from copy import deepcopy
48 import eventlet
49 from eventlet import greenthread
50 from eventlet import tpool
51 from lxml import etree
52 from os_brick import encryptors
53 from os_brick.encryptors import luks as luks_encryptor
54 from os_brick import exception as brick_exception
55 from os_brick.initiator import connector
56 import os_resource_classes as orc
57 from oslo_concurrency import processutils
58 from oslo_log import log as logging
59 from oslo_serialization import base64
60 from oslo_serialization import jsonutils
61 from oslo_service import loopingcall
62 from oslo_utils import encodeutils
63 from oslo_utils import excutils
64 from oslo_utils import fileutils
65 from oslo_utils import importutils
66 from oslo_utils import netutils as oslo_netutils
67 from oslo_utils import strutils
68 from oslo_utils import timeutils
69 from oslo_utils import units
70 from oslo_utils import uuidutils
71 import six
72 from six.moves import range
73 
74 from nova.api.metadata import base as instance_metadata
75 from nova.api.metadata import password
76 from nova import block_device
77 from nova.compute import power_state
78 from nova.compute import task_states
79 from nova.compute import utils as compute_utils
80 from nova.compute import vm_states
81 import nova.conf
82 from nova.console import serial as serial_console
83 from nova.console import type as ctype
84 from nova import context as nova_context
85 from nova import crypto
86 from nova import exception
87 from nova.i18n import _
88 from nova import image
89 from nova.network import model as network_model
90 from nova import objects
91 from nova.objects import diagnostics as diagnostics_obj
92 from nova.objects import fields
93 from nova.pci import manager as pci_manager
94 from nova.pci import utils as pci_utils
95 import nova.privsep.libvirt
96 import nova.privsep.path
97 import nova.privsep.utils
98 from nova import utils
99 from nova import version
100 from nova.virt import block_device as driver_block_device
101 from nova.virt import configdrive
102 from nova.virt.disk import api as disk_api
103 from nova.virt.disk.vfs import guestfs
104 from nova.virt import driver
105 from nova.virt import firewall
106 from nova.virt import hardware
107 from nova.virt.image import model as imgmodel
108 from nova.virt import images
109 from nova.virt.libvirt import blockinfo
110 from nova.virt.libvirt import config as vconfig
111 from nova.virt.libvirt import designer
112 from nova.virt.libvirt import firewall as libvirt_firewall
113 from nova.virt.libvirt import guest as libvirt_guest
114 from nova.virt.libvirt import host
115 from nova.virt.libvirt import imagebackend
116 from nova.virt.libvirt import imagecache
117 from nova.virt.libvirt import instancejobtracker
118 from nova.virt.libvirt import migration as libvirt_migrate
119 from nova.virt.libvirt.storage import dmcrypt
120 from nova.virt.libvirt.storage import lvm
121 from nova.virt.libvirt.storage import rbd_utils
122 from nova.virt.libvirt import utils as libvirt_utils
123 from nova.virt.libvirt import vif as libvirt_vif
124 from nova.virt.libvirt.volume import mount
125 from nova.virt.libvirt.volume import remotefs
126 from nova.virt import netutils
127 from nova.volume import cinder
128 
129 libvirt = None
130 
131 uefi_logged = False
132 
133 LOG = logging.getLogger(__name__)
134 
135 CONF = nova.conf.CONF
136 
137 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
138     libvirt_firewall.__name__,
139     libvirt_firewall.IptablesFirewallDriver.__name__)
140 
141 DEFAULT_UEFI_LOADER_PATH = {
142     "x86_64": ['/usr/share/OVMF/OVMF_CODE.fd',
143                '/usr/share/OVMF/OVMF_CODE.secboot.fd',
144                '/usr/share/qemu/ovmf-x86_64-code.bin'],
145     "aarch64": ['/usr/share/AAVMF/AAVMF_CODE.fd',
146                 '/usr/share/qemu/aavmf-aarch64-code.bin']
147 }
148 
149 MAX_CONSOLE_BYTES = 100 * units.Ki
150 
151 # The libvirt driver will prefix any disable reason codes with this string.
152 DISABLE_PREFIX = 'AUTO: '
153 # Disable reason for the service which was enabled or disabled without reason
154 DISABLE_REASON_UNDEFINED = None
155 
156 # Guest config console string
157 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
158 
159 GuestNumaConfig = collections.namedtuple(
160     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
161 
162 
163 class InjectionInfo(collections.namedtuple(
164         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
165     __slots__ = ()
166 
167     def __repr__(self):
168         return ('InjectionInfo(network_info=%r, files=%r, '
169                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
170 
171 
172 libvirt_volume_drivers = [
173     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
174     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
175     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
176     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
177     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
178     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
179     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
180     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
181     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
182     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
183     'fibre_channel='
184         'nova.virt.libvirt.volume.fibrechannel.'
185         'LibvirtFibreChannelVolumeDriver',
186     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
187     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
188     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
189     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
190     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
191     'vzstorage='
192         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
193     'veritas_hyperscale='
194         'nova.virt.libvirt.volume.vrtshyperscale.'
195         'LibvirtHyperScaleVolumeDriver',
196     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
197     'nvmeof=nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
198 ]
199 
200 
201 def patch_tpool_proxy():
202     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
203     or __repr__() calls. See bug #962840 for details.
204     We perform a monkey patch to replace those two instance methods.
205     """
206     def str_method(self):
207         return str(self._obj)
208 
209     def repr_method(self):
210         return repr(self._obj)
211 
212     tpool.Proxy.__str__ = str_method
213     tpool.Proxy.__repr__ = repr_method
214 
215 
216 patch_tpool_proxy()
217 
218 # For information about when MIN_LIBVIRT_VERSION and
219 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
220 #
221 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
222 #
223 # Currently this is effectively the min version for i686/x86_64
224 # + KVM/QEMU, as other architectures/hypervisors require newer
225 # versions. Over time, this will become a common min version
226 # for all architectures/hypervisors, as this value rises to
227 # meet them.
228 MIN_LIBVIRT_VERSION = (3, 0, 0)
229 MIN_QEMU_VERSION = (2, 8, 0)
230 # TODO(berrange): Re-evaluate this at start of each release cycle
231 # to decide if we want to plan a future min version bump.
232 # MIN_LIBVIRT_VERSION can be updated to match this after
233 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
234 # one cycle
235 NEXT_MIN_LIBVIRT_VERSION = (4, 0, 0)
236 NEXT_MIN_QEMU_VERSION = (2, 11, 0)
237 
238 
239 # Virtuozzo driver support
240 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
241 
242 # aarch64 architecture with KVM
243 # 'chardev' support got sorted out in 3.6.0
244 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
245 
246 # Names of the types that do not get compressed during migration
247 NO_COMPRESSION_TYPES = ('qcow2',)
248 
249 
250 # number of serial console limit
251 QEMU_MAX_SERIAL_PORTS = 4
252 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
253 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
254 
255 MIN_LIBVIRT_OTHER_ARCH = {
256     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
257 }
258 
259 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
260 
261 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
262                                 'mbml': 'mbm_local',
263                                 'mbmt': 'mbm_total',
264                                }
265 
266 # Mediated devices support
267 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
268 
269 # libvirt>=3.10 is required for volume multiattach unless qemu<2.10.
270 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
271 # for details.
272 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
273 
274 MIN_LIBVIRT_FILE_BACKED_VERSION = (4, 0, 0)
275 MIN_QEMU_FILE_BACKED_VERSION = (2, 6, 0)
276 
277 MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION = (4, 4, 0)
278 MIN_QEMU_FILE_BACKED_DISCARD_VERSION = (2, 10, 0)
279 
280 MIN_LIBVIRT_NATIVE_TLS_VERSION = (4, 4, 0)
281 MIN_QEMU_NATIVE_TLS_VERSION = (2, 11, 0)
282 
283 # If the host has this libvirt version, then we skip the retry loop of
284 # instance destroy() call, as libvirt itself increased the wait time
285 # before the SIGKILL signal takes effect.
286 MIN_LIBVIRT_BETTER_SIGKILL_HANDLING = (4, 7, 0)
287 
288 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
289 
290 # see https://libvirt.org/formatdomain.html#elementsVideo
291 MIN_LIBVIRT_VIDEO_MODEL_VERSIONS = {
292     fields.VideoModel.GOP: (3, 2, 0),
293     fields.VideoModel.NONE: (4, 6, 0),
294 }
295 
296 # Persistent Memory (PMEM/NVDIMM) Device Support
297 MIN_LIBVIRT_PMEM_SUPPORT = (5, 0, 0)
298 MIN_QEMU_PMEM_SUPPORT = (3, 1, 0)
299 
300 
301 class LibvirtDriver(driver.ComputeDriver):
302     def __init__(self, virtapi, read_only=False):
303         # NOTE(aspiers) Some of these are dynamic, so putting
304         # capabilities on the instance rather than on the class.
305         # This prevents the risk of one test setting a capability
306         # which bleeds over into other tests.
307 
308         # LVM and RBD require raw images. If we are not configured to
309         # force convert images into raw format, then we _require_ raw
310         # images only.
311         raw_only = ('rbd', 'lvm')
312         requires_raw_image = (CONF.libvirt.images_type in raw_only and
313                               not CONF.force_raw_images)
314 
315         self.capabilities = {
316             "has_imagecache": True,
317             "supports_evacuate": True,
318             "supports_migrate_to_same_host": False,
319             "supports_attach_interface": True,
320             "supports_device_tagging": True,
321             "supports_tagged_attach_interface": True,
322             "supports_tagged_attach_volume": True,
323             "supports_extend_volume": True,
324             # Multiattach support is conditional on qemu and libvirt versions
325             # determined in init_host.
326             "supports_multiattach": False,
327             "supports_trusted_certs": True,
328             # Supported image types
329             "supports_image_type_aki": True,
330             "supports_image_type_ari": True,
331             "supports_image_type_ami": True,
332             # FIXME(danms): I can see a future where people might want to
333             # configure certain compute nodes to not allow giant raw images
334             # to be booted (like nodes that are across a WAN). Thus, at some
335             # point we may want to be able to _not_ expose "supports raw" on
336             # some nodes by policy. Until then, raw is always supported.
337             "supports_image_type_raw": True,
338             "supports_image_type_iso": True,
339             # NOTE(danms): Certain backends do not work with complex image
340             # formats. If we are configured for those backends, then we
341             # should not expose the corresponding support traits.
342             "supports_image_type_qcow2": not requires_raw_image,
343         }
344         super(LibvirtDriver, self).__init__(virtapi)
345 
346         global libvirt
347         if libvirt is None:
348             libvirt = importutils.import_module('libvirt')
349             libvirt_migrate.libvirt = libvirt
350 
351         self._host = host.Host(self._uri(), read_only,
352                                lifecycle_event_handler=self.emit_event,
353                                conn_event_handler=self._handle_conn_event)
354         self._initiator = None
355         self._fc_wwnns = None
356         self._fc_wwpns = None
357         self._caps = None
358         self._supported_perf_events = []
359         self.firewall_driver = firewall.load_driver(
360             DEFAULT_FIREWALL_DRIVER,
361             host=self._host)
362 
363         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
364 
365         # TODO(mriedem): Long-term we should load up the volume drivers on
366         # demand as needed rather than doing this on startup, as there might
367         # be unsupported volume drivers in this list based on the underlying
368         # platform.
369         self.volume_drivers = self._get_volume_drivers()
370 
371         self._disk_cachemode = None
372         self.image_cache_manager = imagecache.ImageCacheManager()
373         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
374 
375         self.disk_cachemodes = {}
376 
377         self.valid_cachemodes = ["default",
378                                  "none",
379                                  "writethrough",
380                                  "writeback",
381                                  "directsync",
382                                  "unsafe",
383                                 ]
384         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
385                                                                       'qemu')
386 
387         for mode_str in CONF.libvirt.disk_cachemodes:
388             disk_type, sep, cache_mode = mode_str.partition('=')
389             if cache_mode not in self.valid_cachemodes:
390                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
391                             'for disk type %(disk_type)s.',
392                             {'cache_mode': cache_mode, 'disk_type': disk_type})
393                 continue
394             self.disk_cachemodes[disk_type] = cache_mode
395 
396         self._volume_api = cinder.API()
397         self._image_api = image.API()
398 
399         # The default choice for the sysinfo_serial config option is "unique"
400         # which does not have a special function since the value is just the
401         # instance.uuid.
402         sysinfo_serial_funcs = {
403             'none': lambda: None,
404             'hardware': self._get_host_sysinfo_serial_hardware,
405             'os': self._get_host_sysinfo_serial_os,
406             'auto': self._get_host_sysinfo_serial_auto,
407         }
408 
409         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
410             CONF.libvirt.sysinfo_serial)
411 
412         self.job_tracker = instancejobtracker.InstanceJobTracker()
413         self._remotefs = remotefs.RemoteFilesystem()
414 
415         self._live_migration_flags = self._block_migration_flags = 0
416         self.active_migrations = {}
417 
418         # Compute reserved hugepages from conf file at the very
419         # beginning to ensure any syntax error will be reported and
420         # avoid any re-calculation when computing resources.
421         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
422 
423         # Copy of the compute service ProviderTree object that is updated
424         # every time update_provider_tree() is called.
425         # NOTE(sbauza): We only want a read-only cache, this attribute is not
426         # intended to be updatable directly
427         self.provider_tree = None
428 
429         self._vpmems_by_name = self._get_vpmems_by_name()
430         self._vpmems_by_rc = self._get_vpmems_by_rc()
431 
432     def _get_vpmems_by_rc(self):
433         vpmems_by_rc = collections.defaultdict(list)
434         for name, vpmem in self._vpmems_by_name.items():
435             rc = "CUSTOM_PMEM_NAMESPACE_%s" % vpmem.label
436             vpmems_by_rc[rc].append(vpmem)
437         return vpmems_by_rc
438 
439     def _get_vpmems_by_name(self):
440         if not CONF.libvirt.pmem_namespaces:
441             return {}
442         if not self._host.has_min_version(lv_ver=MIN_LIBVIRT_PMEM_SUPPORT,
443                                           hv_ver=MIN_QEMU_PMEM_SUPPORT):
444             raise exception.InternalError(
445                 _('Nova requires QEMU version %(qemu)s or greater '
446                   'and Libvirt version %(libvirt)s or greater '
447                   'for NVDIMM (Persistent Memorry) support.') % {
448                 'qemu': libvirt_utils.version_to_string(
449                     MIN_QEMU_PMEM_SUPPORT),
450                 'libvirt': libvirt_utils.version_to_string(
451                     MIN_LIBVIRT_PMEM_SUPPORT)})
452 
453         # vpmem info keyed by name {name: objects.LibvirtVPMEMDevice,...}
454         vpmems_by_name = {}
455         vpmems_host = self._get_vpmems_on_host()
456         for ns_conf in CONF.libvirt.pmem_namespaces:
457             try:
458                 ns_label, ns_names = ns_conf.split(":", 1)
459             except ValueError:
460                 reason = _("The configuration doesn't follow the format")
461                 raise exception.PMEMNamespaceConfigInvalid(
462                         reason=reason)
463             ns_names = ns_names.split("|")
464             for ns_name in ns_names:
465                 if ns_name not in vpmems_host:
466                     reason = _("The PMEM namespace %s isn't on host") % ns_name
467                     raise exception.PMEMNamespaceConfigInvalid(
468                             reason=reason)
469                 if ns_name in vpmems_by_name:
470                     reason = _("Duplicated pmem namespaces configured")
471                     raise exception.PMEMNamespaceConfigInvalid(
472                             reason=reason)
473                 pmem_ns_updated = vpmems_host[ns_name]
474                 pmem_ns_updated.label = ns_label
475                 vpmems_by_name[ns_name] = pmem_ns_updated
476         return vpmems_by_name
477 
478     def _get_vpmems_on_host(self):
479         vpmems_host = {}
480         namespaces = jsonutils.loads(
481             nova.privsep.libvirt.get_pmem_namespaces())
482         for ns in namespaces:
483             if not ns.get('name'):
484                 continue
485             vpmems_host[ns['name']] = objects.LibvirtVPMEMDevice(
486                 label=None,
487                 name=ns['name'],
488                 devpath= '/dev/' + ns['daxregion']['devices'][0]['chardev'],
489                 size=ns['size'],
490                 align=ns['daxregion']['align'])
491         return vpmems_host
492 
493     def _get_volume_drivers(self):
494         driver_registry = dict()
495 
496         for driver_str in libvirt_volume_drivers:
497             driver_type, _sep, driver = driver_str.partition('=')
498             driver_class = importutils.import_class(driver)
499             try:
500                 driver_registry[driver_type] = driver_class(self._host)
501             except brick_exception.InvalidConnectorProtocol:
502                 LOG.debug('Unable to load volume driver %s. It is not '
503                           'supported on this host.', driver)
504 
505         return driver_registry
506 
507     @property
508     def disk_cachemode(self):
509         # It can be confusing to understand the QEMU cache mode
510         # behaviour, because each cache=$MODE is a convenient shorthand
511         # to toggle _three_ cache.* booleans.  Consult the below table
512         # (quoting from the QEMU man page):
513         #
514         #              | cache.writeback | cache.direct | cache.no-flush
515         # --------------------------------------------------------------
516         # writeback    | on              | off          | off
517         # none         | on              | on           | off
518         # writethrough | off             | off          | off
519         # directsync   | off             | on           | off
520         # unsafe       | on              | off          | on
521         #
522         # Where:
523         #
524         #  - 'cache.writeback=off' means: QEMU adds an automatic fsync()
525         #    after each write request.
526         #
527         #  - 'cache.direct=on' means: Use Linux's O_DIRECT, i.e. bypass
528         #    the kernel page cache.  Caches in any other layer (disk
529         #    cache, QEMU metadata caches, etc.) can still be present.
530         #
531         #  - 'cache.no-flush=on' means: Ignore flush requests, i.e.
532         #    never call fsync(), even if the guest explicitly requested
533         #    it.
534         #
535         # Use cache mode "none" (cache.writeback=on, cache.direct=on,
536         # cache.no-flush=off) for consistent performance and
537         # migration correctness.  Some filesystems don't support
538         # O_DIRECT, though.  For those we fallback to the next
539         # reasonable option that is "writeback" (cache.writeback=on,
540         # cache.direct=off, cache.no-flush=off).
541 
542         if self._disk_cachemode is None:
543             self._disk_cachemode = "none"
544             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
545                 self._disk_cachemode = "writeback"
546         return self._disk_cachemode
547 
548     def _set_cache_mode(self, conf):
549         """Set cache mode on LibvirtConfigGuestDisk object."""
550         try:
551             source_type = conf.source_type
552             driver_cache = conf.driver_cache
553         except AttributeError:
554             return
555 
556         # Shareable disks like for a multi-attach volume need to have the
557         # driver cache disabled.
558         if getattr(conf, 'shareable', False):
559             conf.driver_cache = 'none'
560         else:
561             cache_mode = self.disk_cachemodes.get(source_type,
562                                                   driver_cache)
563             conf.driver_cache = cache_mode
564 
565     def _do_quality_warnings(self):
566         """Warn about potential configuration issues.
567 
568         This will log a warning message for things such as untested driver or
569         host arch configurations in order to indicate potential issues to
570         administrators.
571         """
572         caps = self._host.get_capabilities()
573         hostarch = caps.host.cpu.arch
574         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
575             hostarch not in (fields.Architecture.I686,
576                              fields.Architecture.X86_64)):
577             LOG.warning('The libvirt driver is not tested on '
578                         '%(type)s/%(arch)s by the OpenStack project and '
579                         'thus its quality can not be ensured. For more '
580                         'information, see: https://docs.openstack.org/'
581                         'nova/latest/user/support-matrix.html',
582                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
583 
584         if CONF.vnc.keymap:
585             LOG.warning('The option "[vnc] keymap" has been deprecated '
586                         'in favor of configuration within the guest. '
587                         'Update nova.conf to address this change and '
588                         'refer to bug #1682020 for more information.')
589 
590         if CONF.spice.keymap:
591             LOG.warning('The option "[spice] keymap" has been deprecated '
592                         'in favor of configuration within the guest. '
593                         'Update nova.conf to address this change and '
594                         'refer to bug #1682020 for more information.')
595 
596     def _handle_conn_event(self, enabled, reason):
597         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
598                  {'enabled': enabled, 'reason': reason})
599         self._set_host_enabled(enabled, reason)
600 
601     def init_host(self, host):
602         self._host.initialize()
603 
604         self._do_quality_warnings()
605 
606         self._parse_migration_flags()
607 
608         self._supported_perf_events = self._get_supported_perf_events()
609 
610         self._set_multiattach_support()
611 
612         self._check_file_backed_memory_support()
613 
614         self._check_my_ip()
615 
616         if (CONF.libvirt.virt_type == 'lxc' and
617                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
618             LOG.warning("Running libvirt-lxc without user namespaces is "
619                         "dangerous. Containers spawned by Nova will be run "
620                         "as the host's root user. It is highly suggested "
621                         "that user namespaces be used in a public or "
622                         "multi-tenant environment.")
623 
624         # Stop libguestfs using KVM unless we're also configured
625         # to use this. This solves problem where people need to
626         # stop Nova use of KVM because nested-virt is broken
627         if CONF.libvirt.virt_type != "kvm":
628             guestfs.force_tcg()
629 
630         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
631             raise exception.InternalError(
632                 _('Nova requires libvirt version %s or greater.') %
633                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
634 
635         if CONF.libvirt.virt_type in ("qemu", "kvm"):
636             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
637                 # "qemu-img info" calls are version dependent, so we need to
638                 # store the version in the images module.
639                 images.QEMU_VERSION = self._host.get_connection().getVersion()
640             else:
641                 raise exception.InternalError(
642                     _('Nova requires QEMU version %s or greater.') %
643                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
644 
645         if CONF.libvirt.virt_type == 'parallels':
646             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
647                 raise exception.InternalError(
648                     _('Nova requires Virtuozzo version %s or greater.') %
649                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
650 
651         # Give the cloud admin a heads up if we are intending to
652         # change the MIN_LIBVIRT_VERSION in the next release.
653         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
654             LOG.warning('Running Nova with a libvirt version less than '
655                         '%(version)s is deprecated. The required minimum '
656                         'version of libvirt will be raised to %(version)s '
657                         'in the next release.',
658                         {'version': libvirt_utils.version_to_string(
659                             NEXT_MIN_LIBVIRT_VERSION)})
660         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
661             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
662             LOG.warning('Running Nova with a QEMU version less than '
663                         '%(version)s is deprecated. The required minimum '
664                         'version of QEMU will be raised to %(version)s '
665                         'in the next release.',
666                         {'version': libvirt_utils.version_to_string(
667                             NEXT_MIN_QEMU_VERSION)})
668 
669         kvm_arch = fields.Architecture.from_host()
670         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
671             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
672                 not self._host.has_min_version(
673                     MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))):
674             raise exception.InternalError(
675                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
676                   'requires libvirt version %(libvirt_ver)s or greater') %
677                 {'arch': kvm_arch,
678                  'libvirt_ver': libvirt_utils.version_to_string(
679                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
680 
681         # Allowing both "tunnelling via libvirtd" (which will be
682         # deprecated once the MIN_{LIBVIRT,QEMU}_VERSION is sufficiently
683         # new enough) and "native TLS" options at the same time is
684         # nonsensical.
685         if (CONF.libvirt.live_migration_tunnelled and
686                 CONF.libvirt.live_migration_with_native_tls):
687             msg = _("Setting both 'live_migration_tunnelled' and "
688                     "'live_migration_with_native_tls' at the same "
689                     "time is invalid. If you have the relevant "
690                     "libvirt and QEMU versions, and TLS configured "
691                     "in your environment, pick "
692                     "'live_migration_with_native_tls'.")
693             raise exception.Invalid(msg)
694 
695         # Some imagebackends are only able to import raw disk images,
696         # and will fail if given any other format. See the bug
697         # https://bugs.launchpad.net/nova/+bug/1816686 for more details.
698         if CONF.libvirt.images_type in ('rbd',):
699             if not CONF.force_raw_images:
700                 msg = _("'[DEFAULT]/force_raw_images = False' is not "
701                         "allowed with '[libvirt]/images_type = rbd'. "
702                         "Please check the two configs and if you really "
703                         "do want to use rbd as images_type, set "
704                         "force_raw_images to True.")
705                 raise exception.InvalidConfiguration(msg)
706 
707         # TODO(sbauza): Remove this code once mediated devices are persisted
708         # across reboots.
709         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
710             self._recreate_assigned_mediated_devices()
711 
712     @staticmethod
713     def _is_existing_mdev(uuid):
714         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
715         # libvirt daemon won't know when a mediated device is created unless
716         # you restart that daemon. Until all kernels we support are not having
717         # that possible race, check the sysfs directly instead of asking the
718         # libvirt API.
719         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
720         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
721 
722     def _recreate_assigned_mediated_devices(self):
723         """Recreate assigned mdevs that could have disappeared if we reboot
724         the host.
725         """
726         # FIXME(sbauza): We blindly recreate mediated devices without checking
727         # which ResourceProvider was allocated for the instance so it would use
728         # another pGPU.
729         # TODO(sbauza): Pass all instances' allocations here.
730         mdevs = self._get_all_assigned_mediated_devices()
731         requested_types = self._get_supported_vgpu_types()
732         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
733             if not self._is_existing_mdev(mdev_uuid):
734                 self._create_new_mediated_device(requested_types, mdev_uuid)
735 
736     def _set_multiattach_support(self):
737         # Check to see if multiattach is supported. Based on bugzilla
738         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
739         # clones, the shareable flag on a disk device will only work with
740         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
741         # the capability appropriately.
742         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
743                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
744             self.capabilities['supports_multiattach'] = True
745         else:
746             LOG.debug('Volume multiattach is not supported based on current '
747                       'versions of QEMU and libvirt. QEMU must be less than '
748                       '2.10 or libvirt must be greater than or equal to 3.10.')
749 
750     def _check_file_backed_memory_support(self):
751         if CONF.libvirt.file_backed_memory:
752             # file_backed_memory is only compatible with qemu/kvm virts
753             if CONF.libvirt.virt_type not in ("qemu", "kvm"):
754                 raise exception.InternalError(
755                     _('Running Nova with file_backed_memory and virt_type '
756                       '%(type)s is not supported. file_backed_memory is only '
757                       'supported with qemu and kvm types.') %
758                     {'type': CONF.libvirt.virt_type})
759 
760             # Check needed versions for file_backed_memory
761             if not self._host.has_min_version(
762                     MIN_LIBVIRT_FILE_BACKED_VERSION,
763                     MIN_QEMU_FILE_BACKED_VERSION):
764                 raise exception.InternalError(
765                     _('Running Nova with file_backed_memory requires libvirt '
766                       'version %(libvirt)s and qemu version %(qemu)s') %
767                     {'libvirt': libvirt_utils.version_to_string(
768                         MIN_LIBVIRT_FILE_BACKED_VERSION),
769                     'qemu': libvirt_utils.version_to_string(
770                         MIN_QEMU_FILE_BACKED_VERSION)})
771 
772             # file-backed memory doesn't work with memory overcommit.
773             # Block service startup if file-backed memory is enabled and
774             # ram_allocation_ratio is not 1.0
775             if CONF.ram_allocation_ratio != 1.0:
776                 raise exception.InternalError(
777                     'Running Nova with file_backed_memory requires '
778                     'ram_allocation_ratio configured to 1.0')
779 
780     def _check_my_ip(self):
781         ips = compute_utils.get_machine_ips()
782         if CONF.my_ip not in ips:
783             LOG.warning('my_ip address (%(my_ip)s) was not found on '
784                         'any of the interfaces: %(ifaces)s',
785                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
786 
787     def _prepare_migration_flags(self):
788         migration_flags = 0
789 
790         migration_flags |= libvirt.VIR_MIGRATE_LIVE
791 
792         # Adding p2p flag only if xen is not in use, because xen does not
793         # support p2p migrations
794         if CONF.libvirt.virt_type != 'xen':
795             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
796 
797         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
798         # instance will remain defined on the source host
799         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
800 
801         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
802         # destination host
803         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
804 
805         live_migration_flags = block_migration_flags = migration_flags
806 
807         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
808         # will be live-migrations instead
809         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
810 
811         return (live_migration_flags, block_migration_flags)
812 
813     # TODO(kchamart) Once the MIN_LIBVIRT_VERSION and MIN_QEMU_VERSION
814     # reach 4.4.0 and 2.11.0, which provide "native TLS" support by
815     # default, deprecate and remove the support for "tunnelled live
816     # migration" (and related config attribute), because:
817     #
818     #  (a) it cannot handle live migration of disks in a non-shared
819     #      storage setup (a.k.a. "block migration");
820     #
821     #  (b) has a huge performance overhead and latency, because it burns
822     #      more CPU and memory bandwidth due to increased number of data
823     #      copies on both source and destination hosts.
824     #
825     # Both the above limitations are addressed by the QEMU-native TLS
826     # support (`live_migration_with_native_tls`).
827     def _handle_live_migration_tunnelled(self, migration_flags):
828         if CONF.libvirt.live_migration_tunnelled:
829             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
830         return migration_flags
831 
832     def _is_native_tls_available(self):
833         return self._host.has_min_version(MIN_LIBVIRT_NATIVE_TLS_VERSION,
834                                           MIN_QEMU_NATIVE_TLS_VERSION)
835 
836     def _handle_native_tls(self, migration_flags):
837         if (CONF.libvirt.live_migration_with_native_tls and
838                 self._is_native_tls_available()):
839             migration_flags |= libvirt.VIR_MIGRATE_TLS
840         return migration_flags
841 
842     def _handle_live_migration_post_copy(self, migration_flags):
843         if CONF.libvirt.live_migration_permit_post_copy:
844             migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
845         return migration_flags
846 
847     def _handle_live_migration_auto_converge(self, migration_flags):
848         if self._is_post_copy_enabled(migration_flags):
849             LOG.info('The live_migration_permit_post_copy is set to '
850                      'True and post copy live migration is available '
851                      'so auto-converge will not be in use.')
852         elif CONF.libvirt.live_migration_permit_auto_converge:
853             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
854         return migration_flags
855 
856     def _parse_migration_flags(self):
857         (live_migration_flags,
858             block_migration_flags) = self._prepare_migration_flags()
859 
860         live_migration_flags = self._handle_live_migration_tunnelled(
861             live_migration_flags)
862         block_migration_flags = self._handle_live_migration_tunnelled(
863             block_migration_flags)
864 
865         live_migration_flags = self._handle_native_tls(
866             live_migration_flags)
867         block_migration_flags = self._handle_native_tls(
868             block_migration_flags)
869 
870         live_migration_flags = self._handle_live_migration_post_copy(
871             live_migration_flags)
872         block_migration_flags = self._handle_live_migration_post_copy(
873             block_migration_flags)
874 
875         live_migration_flags = self._handle_live_migration_auto_converge(
876             live_migration_flags)
877         block_migration_flags = self._handle_live_migration_auto_converge(
878             block_migration_flags)
879 
880         self._live_migration_flags = live_migration_flags
881         self._block_migration_flags = block_migration_flags
882 
883     # TODO(sahid): This method is targeted for removal when the tests
884     # have been updated to avoid its use
885     #
886     # All libvirt API calls on the libvirt.Connect object should be
887     # encapsulated by methods on the nova.virt.libvirt.host.Host
888     # object, rather than directly invoking the libvirt APIs. The goal
889     # is to avoid a direct dependency on the libvirt API from the
890     # driver.py file.
891     def _get_connection(self):
892         return self._host.get_connection()
893 
894     _conn = property(_get_connection)
895 
896     @staticmethod
897     def _uri():
898         if CONF.libvirt.virt_type == 'uml':
899             uri = CONF.libvirt.connection_uri or 'uml:///system'
900         elif CONF.libvirt.virt_type == 'xen':
901             uri = CONF.libvirt.connection_uri or 'xen:///'
902         elif CONF.libvirt.virt_type == 'lxc':
903             uri = CONF.libvirt.connection_uri or 'lxc:///'
904         elif CONF.libvirt.virt_type == 'parallels':
905             uri = CONF.libvirt.connection_uri or 'parallels:///system'
906         else:
907             uri = CONF.libvirt.connection_uri or 'qemu:///system'
908         return uri
909 
910     @staticmethod
911     def _live_migration_uri(dest):
912         uris = {
913             'kvm': 'qemu+%s://%s/system',
914             'qemu': 'qemu+%s://%s/system',
915             'xen': 'xenmigr://%s/system',
916             'parallels': 'parallels+tcp://%s/system',
917         }
918         dest = oslo_netutils.escape_ipv6(dest)
919 
920         virt_type = CONF.libvirt.virt_type
921         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
922         uri = CONF.libvirt.live_migration_uri
923         if uri:
924             return uri % dest
925 
926         uri = uris.get(virt_type)
927         if uri is None:
928             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
929 
930         str_format = (dest,)
931         if virt_type in ('kvm', 'qemu'):
932             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
933             str_format = (scheme, dest)
934         return uris.get(virt_type) % str_format
935 
936     @staticmethod
937     def _migrate_uri(dest):
938         uri = None
939         dest = oslo_netutils.escape_ipv6(dest)
940 
941         # Only QEMU live migrations supports migrate-uri parameter
942         virt_type = CONF.libvirt.virt_type
943         if virt_type in ('qemu', 'kvm'):
944             # QEMU accept two schemes: tcp and rdma.  By default
945             # libvirt build the URI using the remote hostname and the
946             # tcp schema.
947             uri = 'tcp://%s' % dest
948         # Because dest might be of type unicode, here we might return value of
949         # type unicode as well which is not acceptable by libvirt python
950         # binding when Python 2.7 is in use, so let's convert it explicitly
951         # back to string. When Python 3.x is in use, libvirt python binding
952         # accepts unicode type so it is completely fine to do a no-op str(uri)
953         # conversion which will return value of type unicode.
954         return uri and str(uri)
955 
956     def instance_exists(self, instance):
957         """Efficient override of base instance_exists method."""
958         try:
959             self._host.get_guest(instance)
960             return True
961         except (exception.InternalError, exception.InstanceNotFound):
962             return False
963 
964     def list_instances(self):
965         names = []
966         for guest in self._host.list_guests(only_running=False):
967             names.append(guest.name)
968 
969         return names
970 
971     def list_instance_uuids(self):
972         uuids = []
973         for guest in self._host.list_guests(only_running=False):
974             uuids.append(guest.uuid)
975 
976         return uuids
977 
978     def plug_vifs(self, instance, network_info):
979         """Plug VIFs into networks."""
980         for vif in network_info:
981             self.vif_driver.plug(instance, vif)
982 
983     def _unplug_vifs(self, instance, network_info, ignore_errors):
984         """Unplug VIFs from networks."""
985         for vif in network_info:
986             try:
987                 self.vif_driver.unplug(instance, vif)
988             except exception.NovaException:
989                 if not ignore_errors:
990                     raise
991 
992     def unplug_vifs(self, instance, network_info):
993         self._unplug_vifs(instance, network_info, False)
994 
995     def _teardown_container(self, instance):
996         inst_path = libvirt_utils.get_instance_path(instance)
997         container_dir = os.path.join(inst_path, 'rootfs')
998         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
999         LOG.debug('Attempting to teardown container at path %(dir)s with '
1000                   'root device: %(rootfs_dev)s',
1001                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
1002                   instance=instance)
1003         disk_api.teardown_container(container_dir, rootfs_dev)
1004 
1005     def _destroy(self, instance, attempt=1):
1006         try:
1007             guest = self._host.get_guest(instance)
1008             if CONF.serial_console.enabled:
1009                 # This method is called for several events: destroy,
1010                 # rebuild, hard-reboot, power-off - For all of these
1011                 # events we want to release the serial ports acquired
1012                 # for the guest before destroying it.
1013                 serials = self._get_serial_ports_from_guest(guest)
1014                 for hostname, port in serials:
1015                     serial_console.release_port(host=hostname, port=port)
1016         except exception.InstanceNotFound:
1017             guest = None
1018 
1019         # If the instance is already terminated, we're still happy
1020         # Otherwise, destroy it
1021         old_domid = -1
1022         if guest is not None:
1023             try:
1024                 old_domid = guest.id
1025                 guest.poweroff()
1026 
1027             except libvirt.libvirtError as e:
1028                 is_okay = False
1029                 errcode = e.get_error_code()
1030                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1031                     # Domain already gone. This can safely be ignored.
1032                     is_okay = True
1033                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
1034                     # If the instance is already shut off, we get this:
1035                     # Code=55 Error=Requested operation is not valid:
1036                     # domain is not running
1037 
1038                     state = guest.get_power_state(self._host)
1039                     if state == power_state.SHUTDOWN:
1040                         is_okay = True
1041                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
1042                     errmsg = e.get_error_message()
1043                     if (CONF.libvirt.virt_type == 'lxc' and
1044                         errmsg == 'internal error: '
1045                                   'Some processes refused to die'):
1046                         # Some processes in the container didn't die
1047                         # fast enough for libvirt. The container will
1048                         # eventually die. For now, move on and let
1049                         # the wait_for_destroy logic take over.
1050                         is_okay = True
1051                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
1052                     LOG.warning("Cannot destroy instance, operation time out",
1053                                 instance=instance)
1054                     reason = _("operation time out")
1055                     raise exception.InstancePowerOffFailure(reason=reason)
1056                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
1057                     if e.get_int1() == errno.EBUSY:
1058                         # NOTE(danpb): When libvirt kills a process it sends it
1059                         # SIGTERM first and waits 10 seconds. If it hasn't gone
1060                         # it sends SIGKILL and waits another 5 seconds. If it
1061                         # still hasn't gone then you get this EBUSY error.
1062                         # Usually when a QEMU process fails to go away upon
1063                         # SIGKILL it is because it is stuck in an
1064                         # uninterruptible kernel sleep waiting on I/O from
1065                         # some non-responsive server.
1066                         # Given the CPU load of the gate tests though, it is
1067                         # conceivable that the 15 second timeout is too short,
1068                         # particularly if the VM running tempest has a high
1069                         # steal time from the cloud host. ie 15 wallclock
1070                         # seconds may have passed, but the VM might have only
1071                         # have a few seconds of scheduled run time.
1072                         #
1073                         # TODO(kchamart): Once MIN_LIBVIRT_VERSION
1074                         # reaches v4.7.0, (a) rewrite the above note,
1075                         # and (b) remove the following code that retries
1076                         # _destroy() API call (which gives SIGKILL 30
1077                         # seconds to take effect) -- because from v4.7.0
1078                         # onwards, libvirt _automatically_ increases the
1079                         # timeout to 30 seconds.  This was added in the
1080                         # following libvirt commits:
1081                         #
1082                         #   - 9a4e4b942 (process: wait longer 5->30s on
1083                         #     hard shutdown)
1084                         #
1085                         #   - be2ca0444 (process: wait longer on kill
1086                         #     per assigned Hostdev)
1087                         with excutils.save_and_reraise_exception() as ctxt:
1088                             if not self._host.has_min_version(
1089                                     MIN_LIBVIRT_BETTER_SIGKILL_HANDLING):
1090                                 LOG.warning('Error from libvirt during '
1091                                             'destroy. Code=%(errcode)s '
1092                                             'Error=%(e)s; attempt '
1093                                             '%(attempt)d of 6 ',
1094                                             {'errcode': errcode, 'e': e,
1095                                              'attempt': attempt},
1096                                             instance=instance)
1097                                 # Try up to 6 times before giving up.
1098                                 if attempt < 6:
1099                                     ctxt.reraise = False
1100                                     self._destroy(instance, attempt + 1)
1101                                     return
1102 
1103                 if not is_okay:
1104                     with excutils.save_and_reraise_exception():
1105                         LOG.error('Error from libvirt during destroy. '
1106                                   'Code=%(errcode)s Error=%(e)s',
1107                                   {'errcode': errcode, 'e': e},
1108                                   instance=instance)
1109 
1110         def _wait_for_destroy(expected_domid):
1111             """Called at an interval until the VM is gone."""
1112             # NOTE(vish): If the instance disappears during the destroy
1113             #             we ignore it so the cleanup can still be
1114             #             attempted because we would prefer destroy to
1115             #             never fail.
1116             try:
1117                 dom_info = self.get_info(instance)
1118                 state = dom_info.state
1119                 new_domid = dom_info.internal_id
1120             except exception.InstanceNotFound:
1121                 LOG.debug("During wait destroy, instance disappeared.",
1122                           instance=instance)
1123                 state = power_state.SHUTDOWN
1124 
1125             if state == power_state.SHUTDOWN:
1126                 LOG.info("Instance destroyed successfully.", instance=instance)
1127                 raise loopingcall.LoopingCallDone()
1128 
1129             # NOTE(wangpan): If the instance was booted again after destroy,
1130             #                this may be an endless loop, so check the id of
1131             #                domain here, if it changed and the instance is
1132             #                still running, we should destroy it again.
1133             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
1134             if new_domid != expected_domid:
1135                 LOG.info("Instance may be started again.", instance=instance)
1136                 kwargs['is_running'] = True
1137                 raise loopingcall.LoopingCallDone()
1138 
1139         kwargs = {'is_running': False}
1140         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
1141                                                      old_domid)
1142         timer.start(interval=0.5).wait()
1143         if kwargs['is_running']:
1144             LOG.info("Going to destroy instance again.", instance=instance)
1145             self._destroy(instance)
1146         else:
1147             # NOTE(GuanQiang): teardown container to avoid resource leak
1148             if CONF.libvirt.virt_type == 'lxc':
1149                 self._teardown_container(instance)
1150 
1151     def destroy(self, context, instance, network_info, block_device_info=None,
1152                 destroy_disks=True):
1153         self._destroy(instance)
1154         self.cleanup(context, instance, network_info, block_device_info,
1155                      destroy_disks)
1156 
1157     def _undefine_domain(self, instance):
1158         try:
1159             guest = self._host.get_guest(instance)
1160             try:
1161                 support_uefi = self._has_uefi_support()
1162                 guest.delete_configuration(support_uefi)
1163             except libvirt.libvirtError as e:
1164                 with excutils.save_and_reraise_exception() as ctxt:
1165                     errcode = e.get_error_code()
1166                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
1167                         LOG.debug("Called undefine, but domain already gone.",
1168                                   instance=instance)
1169                         ctxt.reraise = False
1170                     else:
1171                         LOG.error('Error from libvirt during undefine. '
1172                                   'Code=%(errcode)s Error=%(e)s',
1173                                   {'errcode': errcode,
1174                                    'e': encodeutils.exception_to_unicode(e)},
1175                                   instance=instance)
1176         except exception.InstanceNotFound:
1177             pass
1178 
1179     def cleanup(self, context, instance, network_info, block_device_info=None,
1180                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1181         if destroy_vifs:
1182             self._unplug_vifs(instance, network_info, True)
1183 
1184         # Continue attempting to remove firewall filters for the instance
1185         # until it's done or there is a failure to remove the filters. If
1186         # unfilter fails because the instance is not yet shutdown, try to
1187         # destroy the guest again and then retry the unfilter.
1188         while True:
1189             try:
1190                 self.unfilter_instance(instance, network_info)
1191                 break
1192             except libvirt.libvirtError as e:
1193                 try:
1194                     state = self.get_info(instance).state
1195                 except exception.InstanceNotFound:
1196                     state = power_state.SHUTDOWN
1197 
1198                 if state != power_state.SHUTDOWN:
1199                     LOG.warning("Instance may be still running, destroy "
1200                                 "it again.", instance=instance)
1201                     self._destroy(instance)
1202                 else:
1203                     errcode = e.get_error_code()
1204                     LOG.exception(_('Error from libvirt during unfilter. '
1205                                     'Code=%(errcode)s Error=%(e)s'),
1206                                   {'errcode': errcode, 'e': e},
1207                                   instance=instance)
1208                     reason = _("Error unfiltering instance.")
1209                     raise exception.InstanceTerminationFailure(reason=reason)
1210             except Exception:
1211                 raise
1212 
1213         # FIXME(wangpan): if the instance is booted again here, such as the
1214         #                 soft reboot operation boot it here, it will become
1215         #                 "running deleted", should we check and destroy it
1216         #                 at the end of this method?
1217 
1218         # NOTE(vish): we disconnect from volumes regardless
1219         block_device_mapping = driver.block_device_info_get_mapping(
1220             block_device_info)
1221         for vol in block_device_mapping:
1222             connection_info = vol['connection_info']
1223             disk_dev = vol['mount_device']
1224             if disk_dev is not None:
1225                 disk_dev = disk_dev.rpartition("/")[2]
1226             try:
1227                 self._disconnect_volume(context, connection_info, instance)
1228             except Exception as exc:
1229                 with excutils.save_and_reraise_exception() as ctxt:
1230                     if destroy_disks:
1231                         # Don't block on Volume errors if we're trying to
1232                         # delete the instance as we may be partially created
1233                         # or deleted
1234                         ctxt.reraise = False
1235                         LOG.warning(
1236                             "Ignoring Volume Error on vol %(vol_id)s "
1237                             "during delete %(exc)s",
1238                             {'vol_id': vol.get('volume_id'),
1239                              'exc': encodeutils.exception_to_unicode(exc)},
1240                             instance=instance)
1241 
1242         if destroy_disks:
1243             # NOTE(haomai): destroy volumes if needed
1244             if CONF.libvirt.images_type == 'lvm':
1245                 self._cleanup_lvm(instance, block_device_info)
1246             if CONF.libvirt.images_type == 'rbd':
1247                 self._cleanup_rbd(instance)
1248 
1249         is_shared_block_storage = False
1250         if migrate_data and 'is_shared_block_storage' in migrate_data:
1251             is_shared_block_storage = migrate_data.is_shared_block_storage
1252         # NOTE(lyarwood): The following workaround allows operators to ensure
1253         # that non-shared instance directories are removed after an evacuation
1254         # or revert resize when using the shared RBD imagebackend. This
1255         # workaround is not required when cleaning up migrations that provide
1256         # migrate_data to this method as the existing is_shared_block_storage
1257         # conditional will cause the instance directory to be removed.
1258         if ((destroy_disks or is_shared_block_storage) or
1259             (CONF.workarounds.ensure_libvirt_rbd_instance_dir_cleanup and
1260              CONF.libvirt.images_type == 'rbd')):
1261 
1262             attempts = int(instance.system_metadata.get('clean_attempts',
1263                                                         '0'))
1264             success = self.delete_instance_files(instance)
1265             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1266             # task in the compute manager. The tight coupling is not great...
1267             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1268             if success:
1269                 instance.cleaned = True
1270             instance.save()
1271 
1272         self._undefine_domain(instance)
1273 
1274     def _detach_encrypted_volumes(self, instance, block_device_info):
1275         """Detaches encrypted volumes attached to instance."""
1276         disks = self._get_instance_disk_info(instance, block_device_info)
1277         encrypted_volumes = filter(dmcrypt.is_encrypted,
1278                                    [disk['path'] for disk in disks])
1279         for path in encrypted_volumes:
1280             dmcrypt.delete_volume(path)
1281 
1282     def _get_serial_ports_from_guest(self, guest, mode=None):
1283         """Returns an iterator over serial port(s) configured on guest.
1284 
1285         :param mode: Should be a value in (None, bind, connect)
1286         """
1287         xml = guest.get_xml_desc()
1288         tree = etree.fromstring(xml)
1289 
1290         # The 'serial' device is the base for x86 platforms. Other platforms
1291         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1292         xpath_mode = "[@mode='%s']" % mode if mode else ""
1293         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1294         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1295 
1296         tcp_devices = tree.findall(serial_tcp)
1297         if len(tcp_devices) == 0:
1298             tcp_devices = tree.findall(console_tcp)
1299         for source in tcp_devices:
1300             yield (source.get("host"), int(source.get("service")))
1301 
1302     def _get_scsi_controller_max_unit(self, guest):
1303         """Returns the max disk unit used by scsi controller"""
1304         xml = guest.get_xml_desc()
1305         tree = etree.fromstring(xml)
1306         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1307 
1308         ret = []
1309         for obj in tree.findall(addrs):
1310             ret.append(int(obj.get('unit', 0)))
1311         return max(ret)
1312 
1313     def _cleanup_rbd(self, instance):
1314         # NOTE(nic): On revert_resize, the cleanup steps for the root
1315         # volume are handled with an "rbd snap rollback" command,
1316         # and none of this is needed (and is, in fact, harmful) so
1317         # filter out non-ephemerals from the list
1318         if instance.task_state == task_states.RESIZE_REVERTING:
1319             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1320                                       disk.endswith('disk.local'))
1321         else:
1322             filter_fn = lambda disk: disk.startswith(instance.uuid)
1323         rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
1324 
1325     def _cleanup_lvm(self, instance, block_device_info):
1326         """Delete all LVM disks for given instance object."""
1327         if instance.get('ephemeral_key_uuid') is not None:
1328             self._detach_encrypted_volumes(instance, block_device_info)
1329 
1330         disks = self._lvm_disks(instance)
1331         if disks:
1332             lvm.remove_volumes(disks)
1333 
1334     def _lvm_disks(self, instance):
1335         """Returns all LVM disks for given instance object."""
1336         if CONF.libvirt.images_volume_group:
1337             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1338             if not os.path.exists(vg):
1339                 return []
1340             pattern = '%s_' % instance.uuid
1341 
1342             def belongs_to_instance(disk):
1343                 return disk.startswith(pattern)
1344 
1345             def fullpath(name):
1346                 return os.path.join(vg, name)
1347 
1348             logical_volumes = lvm.list_volumes(vg)
1349 
1350             disks = [fullpath(disk) for disk in logical_volumes
1351                      if belongs_to_instance(disk)]
1352             return disks
1353         return []
1354 
1355     def get_volume_connector(self, instance):
1356         root_helper = utils.get_root_helper()
1357         return connector.get_connector_properties(
1358             root_helper, CONF.my_block_storage_ip,
1359             CONF.libvirt.volume_use_multipath,
1360             enforce_multipath=True,
1361             host=CONF.host)
1362 
1363     def _cleanup_resize(self, context, instance, network_info):
1364         inst_base = libvirt_utils.get_instance_path(instance)
1365         target = inst_base + '_resize'
1366 
1367         # Deletion can fail over NFS, so retry the deletion as required.
1368         # Set maximum attempt as 5, most test can remove the directory
1369         # for the second time.
1370         attempts = 0
1371         while(os.path.exists(target) and attempts < 5):
1372             shutil.rmtree(target, ignore_errors=True)
1373             if os.path.exists(target):
1374                 time.sleep(random.randint(20, 200) / 100.0)
1375             attempts += 1
1376 
1377         # NOTE(mriedem): Some image backends will recreate the instance path
1378         # and disk.info during init, and all we need the root disk for
1379         # here is removing cloned snapshots which is backend-specific, so
1380         # check that first before initializing the image backend object. If
1381         # there is ever an image type that supports clone *and* re-creates
1382         # the instance directory and disk.info on init, this condition will
1383         # need to be re-visited to make sure that backend doesn't re-create
1384         # the disk. Refer to bugs: 1666831 1728603 1769131
1385         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1386             root_disk = self.image_backend.by_name(instance, 'disk')
1387             if root_disk.exists():
1388                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1389 
1390         if instance.host != CONF.host:
1391             self._undefine_domain(instance)
1392             self.unplug_vifs(instance, network_info)
1393             self.unfilter_instance(instance, network_info)
1394 
1395     def _get_volume_driver(self, connection_info):
1396         driver_type = connection_info.get('driver_volume_type')
1397         if driver_type not in self.volume_drivers:
1398             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1399         return self.volume_drivers[driver_type]
1400 
1401     def _connect_volume(self, context, connection_info, instance,
1402                         encryption=None, allow_native_luks=True):
1403         vol_driver = self._get_volume_driver(connection_info)
1404         vol_driver.connect_volume(connection_info, instance)
1405         try:
1406             self._attach_encryptor(
1407                 context, connection_info, encryption, allow_native_luks)
1408         except Exception:
1409             # Encryption failed so rollback the volume connection.
1410             with excutils.save_and_reraise_exception(logger=LOG):
1411                 LOG.exception("Failure attaching encryptor; rolling back "
1412                               "volume connection", instance=instance)
1413                 vol_driver.disconnect_volume(connection_info, instance)
1414 
1415     def _should_disconnect_target(self, context, connection_info, instance):
1416         connection_count = 0
1417 
1418         # NOTE(jdg): Multiattach is a special case (not to be confused
1419         # with shared_targets). With multiattach we may have a single volume
1420         # attached multiple times to *this* compute node (ie Server-1 and
1421         # Server-2).  So, if we receive a call to delete the attachment for
1422         # Server-1 we need to take special care to make sure that the Volume
1423         # isn't also attached to another Server on this Node.  Otherwise we
1424         # will indiscriminantly delete the connection for all Server and that's
1425         # no good.  So check if it's attached multiple times on this node
1426         # if it is we skip the call to brick to delete the connection.
1427         if connection_info.get('multiattach', False):
1428             volume = self._volume_api.get(
1429                 context,
1430                 driver_block_device.get_volume_id(connection_info))
1431             attachments = volume.get('attachments', {})
1432             if len(attachments) > 1:
1433                 # First we get a list of all Server UUID's associated with
1434                 # this Host (Compute Node).  We're going to use this to
1435                 # determine if the Volume being detached is also in-use by
1436                 # another Server on this Host, ie just check to see if more
1437                 # than one attachment.server_id for this volume is in our
1438                 # list of Server UUID's for this Host
1439                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1440                     context, instance.host)
1441 
1442                 # NOTE(jdg): nova.volume.cinder translates the
1443                 # volume['attachments'] response into a dict which includes
1444                 # the Server UUID as the key, so we're using that
1445                 # here to check against our server_this_host list
1446                 for server_id, data in attachments.items():
1447                     if server_id in servers_this_host:
1448                         connection_count += 1
1449         return (False if connection_count > 1 else True)
1450 
1451     def _disconnect_volume(self, context, connection_info, instance,
1452                            encryption=None):
1453         self._detach_encryptor(context, connection_info, encryption=encryption)
1454         if self._should_disconnect_target(context, connection_info, instance):
1455             vol_driver = self._get_volume_driver(connection_info)
1456             vol_driver.disconnect_volume(connection_info, instance)
1457         else:
1458             LOG.info("Detected multiple connections on this host for volume: "
1459                      "%s, skipping target disconnect.",
1460                      driver_block_device.get_volume_id(connection_info),
1461                      instance=instance)
1462 
1463     def _extend_volume(self, connection_info, instance, requested_size):
1464         vol_driver = self._get_volume_driver(connection_info)
1465         return vol_driver.extend_volume(connection_info, instance,
1466                                         requested_size)
1467 
1468     def _use_native_luks(self, encryption=None):
1469         """Check if LUKS is the required 'provider'
1470         """
1471         provider = None
1472         if encryption:
1473             provider = encryption.get('provider', None)
1474         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1475             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1476         return provider == encryptors.LUKS
1477 
1478     def _get_volume_config(self, connection_info, disk_info):
1479         vol_driver = self._get_volume_driver(connection_info)
1480         conf = vol_driver.get_config(connection_info, disk_info)
1481         self._set_cache_mode(conf)
1482         return conf
1483 
1484     def _get_volume_encryptor(self, connection_info, encryption):
1485         root_helper = utils.get_root_helper()
1486         return encryptors.get_volume_encryptor(root_helper=root_helper,
1487                                                keymgr=key_manager.API(CONF),
1488                                                connection_info=connection_info,
1489                                                **encryption)
1490 
1491     def _get_volume_encryption(self, context, connection_info):
1492         """Get the encryption metadata dict if it is not provided
1493         """
1494         encryption = {}
1495         volume_id = driver_block_device.get_volume_id(connection_info)
1496         if volume_id:
1497             encryption = encryptors.get_encryption_metadata(context,
1498                             self._volume_api, volume_id, connection_info)
1499         return encryption
1500 
1501     def _attach_encryptor(self, context, connection_info, encryption,
1502                           allow_native_luks):
1503         """Attach the frontend encryptor if one is required by the volume.
1504 
1505         The request context is only used when an encryption metadata dict is
1506         not provided. The encryption metadata dict being populated is then used
1507         to determine if an attempt to attach the encryptor should be made.
1508 
1509         If native LUKS decryption is enabled then create a Libvirt volume
1510         secret containing the LUKS passphrase for the volume.
1511         """
1512         if encryption is None:
1513             encryption = self._get_volume_encryption(context, connection_info)
1514 
1515         if (encryption and allow_native_luks and
1516             self._use_native_luks(encryption)):
1517             # NOTE(lyarwood): Fetch the associated key for the volume and
1518             # decode the passphrase from the key.
1519             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1520             # with volumes, leading to the binary to hex to string conversion
1521             # below.
1522             keymgr = key_manager.API(CONF)
1523             key = keymgr.get(context, encryption['encryption_key_id'])
1524             key_encoded = key.get_encoded()
1525             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1526 
1527             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1528             # encryptors and format any volume that does not identify as
1529             # encrypted with LUKS.
1530             # FIXME(lyarwood): Remove this once c-vol correctly formats
1531             # encrypted volumes during their initial creation:
1532             # https://bugs.launchpad.net/cinder/+bug/1739442
1533             device_path = connection_info.get('data').get('device_path')
1534             if device_path:
1535                 root_helper = utils.get_root_helper()
1536                 if not luks_encryptor.is_luks(root_helper, device_path):
1537                     encryptor = self._get_volume_encryptor(connection_info,
1538                                                            encryption)
1539                     encryptor._format_volume(passphrase, **encryption)
1540 
1541             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1542             # on the compute node. This secret is used later when generating
1543             # the volume config.
1544             volume_id = driver_block_device.get_volume_id(connection_info)
1545             self._host.create_secret('volume', volume_id, password=passphrase)
1546         elif encryption:
1547             encryptor = self._get_volume_encryptor(connection_info,
1548                                                    encryption)
1549             encryptor.attach_volume(context, **encryption)
1550 
1551     def _detach_encryptor(self, context, connection_info, encryption):
1552         """Detach the frontend encryptor if one is required by the volume.
1553 
1554         The request context is only used when an encryption metadata dict is
1555         not provided. The encryption metadata dict being populated is then used
1556         to determine if an attempt to detach the encryptor should be made.
1557 
1558         If native LUKS decryption is enabled then delete previously created
1559         Libvirt volume secret from the host.
1560         """
1561         volume_id = driver_block_device.get_volume_id(connection_info)
1562         if volume_id and self._host.find_secret('volume', volume_id):
1563             return self._host.delete_secret('volume', volume_id)
1564         if encryption is None:
1565             encryption = self._get_volume_encryption(context, connection_info)
1566         # NOTE(lyarwood): Handle bug #1821696 where volume secrets have been
1567         # removed manually by returning if native LUKS decryption is available
1568         # and device_path is not present in the connection_info. This avoids
1569         # VolumeEncryptionNotSupported being thrown when we incorrectly build
1570         # the encryptor below due to the secrets not being present above.
1571         if (encryption and self._use_native_luks(encryption) and
1572             not connection_info['data'].get('device_path')):
1573             return
1574         if encryption:
1575             encryptor = self._get_volume_encryptor(connection_info,
1576                                                    encryption)
1577             encryptor.detach_volume(**encryption)
1578 
1579     def _check_discard_for_attach_volume(self, conf, instance):
1580         """Perform some checks for volumes configured for discard support.
1581 
1582         If discard is configured for the volume, and the guest is using a
1583         configuration known to not work, we will log a message explaining
1584         the reason why.
1585         """
1586         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1587             LOG.debug('Attempting to attach volume %(id)s with discard '
1588                       'support enabled to an instance using an '
1589                       'unsupported configuration. target_bus = '
1590                       '%(bus)s. Trim commands will not be issued to '
1591                       'the storage device.',
1592                       {'bus': conf.target_bus,
1593                        'id': conf.serial},
1594                       instance=instance)
1595 
1596     def attach_volume(self, context, connection_info, instance, mountpoint,
1597                       disk_bus=None, device_type=None, encryption=None):
1598         guest = self._host.get_guest(instance)
1599 
1600         disk_dev = mountpoint.rpartition("/")[2]
1601         bdm = {
1602             'device_name': disk_dev,
1603             'disk_bus': disk_bus,
1604             'device_type': device_type}
1605 
1606         # Note(cfb): If the volume has a custom block size, check that
1607         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1608         #            presence of a block size is considered mandatory by
1609         #            cinder so we fail if we can't honor the request.
1610         data = {}
1611         if ('data' in connection_info):
1612             data = connection_info['data']
1613         if ('logical_block_size' in data or 'physical_block_size' in data):
1614             if ((CONF.libvirt.virt_type != "kvm" and
1615                  CONF.libvirt.virt_type != "qemu")):
1616                 msg = _("Volume sets block size, but the current "
1617                         "libvirt hypervisor '%s' does not support custom "
1618                         "block size") % CONF.libvirt.virt_type
1619                 raise exception.InvalidHypervisorType(msg)
1620 
1621         self._connect_volume(context, connection_info, instance,
1622                              encryption=encryption)
1623         disk_info = blockinfo.get_info_from_bdm(
1624             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1625         if disk_info['bus'] == 'scsi':
1626             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1627 
1628         conf = self._get_volume_config(connection_info, disk_info)
1629 
1630         self._check_discard_for_attach_volume(conf, instance)
1631 
1632         try:
1633             state = guest.get_power_state(self._host)
1634             live = state in (power_state.RUNNING, power_state.PAUSED)
1635 
1636             guest.attach_device(conf, persistent=True, live=live)
1637             # NOTE(artom) If we're attaching with a device role tag, we need to
1638             # rebuild device_metadata. If we're attaching without a role
1639             # tag, we're rebuilding it here needlessly anyways. This isn't a
1640             # massive deal, and it helps reduce code complexity by not having
1641             # to indicate to the virt driver that the attach is tagged. The
1642             # really important optimization of not calling the database unless
1643             # device_metadata has actually changed is done for us by
1644             # instance.save().
1645             instance.device_metadata = self._build_device_metadata(
1646                 context, instance)
1647             instance.save()
1648 
1649         # TODO(lyarwood) Remove the following breadcrumb once all supported
1650         # distributions provide Libvirt 3.3.0 or earlier with
1651         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=7189099 applied.
1652         except libvirt.libvirtError as ex:
1653             with excutils.save_and_reraise_exception():
1654                 if 'Incorrect number of padding bytes' in six.text_type(ex):
1655                     LOG.warning(_('Failed to attach encrypted volume due to a '
1656                                   'known Libvirt issue, see the following bug '
1657                                   'for details: '
1658                                   'https://bugzilla.redhat.com/1447297'))
1659                 else:
1660                     LOG.exception(_('Failed to attach volume at mountpoint: '
1661                                     '%s'), mountpoint, instance=instance)
1662                 self._disconnect_volume(context, connection_info, instance,
1663                                         encryption=encryption)
1664         except Exception:
1665             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1666                           mountpoint, instance=instance)
1667             with excutils.save_and_reraise_exception():
1668                 self._disconnect_volume(context, connection_info, instance,
1669                                         encryption=encryption)
1670 
1671     def _swap_volume(self, guest, disk_path, conf, resize_to):
1672         """Swap existing disk with a new block device."""
1673         dev = guest.get_block_device(disk_path)
1674 
1675         # Save a copy of the domain's persistent XML file. We'll use this
1676         # to redefine the domain if anything fails during the volume swap.
1677         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1678 
1679         # Abort is an idempotent operation, so make sure any block
1680         # jobs which may have failed are ended.
1681         try:
1682             dev.abort_job()
1683         except Exception:
1684             pass
1685 
1686         try:
1687             # NOTE (rmk): blockRebase cannot be executed on persistent
1688             #             domains, so we need to temporarily undefine it.
1689             #             If any part of this block fails, the domain is
1690             #             re-defined regardless.
1691             if guest.has_persistent_configuration():
1692                 support_uefi = self._has_uefi_support()
1693                 guest.delete_configuration(support_uefi)
1694 
1695             try:
1696                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1697                 # allow writing to existing external volume file. Use
1698                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1699                 # make sure XML is generated correctly (bug 1691195)
1700                 copy_dev = conf.source_type == 'block'
1701                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1702                            copy_dev=copy_dev)
1703                 while not dev.is_job_complete():
1704                     time.sleep(0.5)
1705 
1706                 dev.abort_job(pivot=True)
1707 
1708             except Exception as exc:
1709                 LOG.exception("Failure rebasing volume %(new_path)s on "
1710                     "%(old_path)s.", {'new_path': conf.source_path,
1711                                       'old_path': disk_path})
1712                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1713 
1714             if resize_to:
1715                 dev.resize(resize_to * units.Gi / units.Ki)
1716 
1717             # Make sure we will redefine the domain using the updated
1718             # configuration after the volume was swapped. The dump_inactive
1719             # keyword arg controls whether we pull the inactive (persistent)
1720             # or active (live) config from the domain. We want to pull the
1721             # live config after the volume was updated to use when we redefine
1722             # the domain.
1723             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1724         finally:
1725             self._host.write_instance_config(xml)
1726 
1727     def swap_volume(self, context, old_connection_info,
1728                     new_connection_info, instance, mountpoint, resize_to):
1729 
1730         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1731         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1732         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1733         if ((old_encrypt and self._use_native_luks(old_encrypt)) or
1734             (new_encrypt and self._use_native_luks(new_encrypt))):
1735             raise NotImplementedError(_("Swap volume is not supported for "
1736                 "encrypted volumes when native LUKS decryption is enabled."))
1737 
1738         guest = self._host.get_guest(instance)
1739 
1740         disk_dev = mountpoint.rpartition("/")[2]
1741         if not guest.get_disk(disk_dev):
1742             raise exception.DiskNotFound(location=disk_dev)
1743         disk_info = {
1744             'dev': disk_dev,
1745             'bus': blockinfo.get_disk_bus_for_disk_dev(
1746                 CONF.libvirt.virt_type, disk_dev),
1747             'type': 'disk',
1748             }
1749         # NOTE (lyarwood): new_connection_info will be modified by the
1750         # following _connect_volume call down into the volume drivers. The
1751         # majority of the volume drivers will add a device_path that is in turn
1752         # used by _get_volume_config to set the source_path of the
1753         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1754         # this to the BDM here as the upper compute swap_volume method will
1755         # eventually do this for us.
1756         self._connect_volume(context, new_connection_info, instance)
1757         conf = self._get_volume_config(new_connection_info, disk_info)
1758         if not conf.source_path:
1759             self._disconnect_volume(context, new_connection_info, instance)
1760             raise NotImplementedError(_("Swap only supports host devices"))
1761 
1762         try:
1763             self._swap_volume(guest, disk_dev, conf, resize_to)
1764         except exception.VolumeRebaseFailed:
1765             with excutils.save_and_reraise_exception():
1766                 self._disconnect_volume(context, new_connection_info, instance)
1767 
1768         self._disconnect_volume(context, old_connection_info, instance)
1769 
1770     def _get_existing_domain_xml(self, instance, network_info,
1771                                  block_device_info=None):
1772         try:
1773             guest = self._host.get_guest(instance)
1774             xml = guest.get_xml_desc()
1775         except exception.InstanceNotFound:
1776             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1777                                                 instance,
1778                                                 instance.image_meta,
1779                                                 block_device_info)
1780             xml = self._get_guest_xml(nova_context.get_admin_context(),
1781                                       instance, network_info, disk_info,
1782                                       instance.image_meta,
1783                                       block_device_info=block_device_info)
1784         return xml
1785 
1786     def detach_volume(self, context, connection_info, instance, mountpoint,
1787                       encryption=None):
1788         disk_dev = mountpoint.rpartition("/")[2]
1789         try:
1790             guest = self._host.get_guest(instance)
1791 
1792             state = guest.get_power_state(self._host)
1793             live = state in (power_state.RUNNING, power_state.PAUSED)
1794             # NOTE(lyarwood): The volume must be detached from the VM before
1795             # detaching any attached encryptors or disconnecting the underlying
1796             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1797             # driver may report that the volume is still in use.
1798             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1799                                                              disk_dev,
1800                                                              live=live)
1801             wait_for_detach()
1802 
1803         except exception.InstanceNotFound:
1804             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1805             #                will throw InstanceNotFound exception. Need to
1806             #                disconnect volume under this circumstance.
1807             LOG.warning("During detach_volume, instance disappeared.",
1808                         instance=instance)
1809         except exception.DeviceNotFound:
1810             # We should still try to disconnect logical device from
1811             # host, an error might have happened during a previous
1812             # call.
1813             LOG.info("Device %s not found in instance.",
1814                      disk_dev, instance=instance)
1815         except libvirt.libvirtError as ex:
1816             # NOTE(vish): This is called to cleanup volumes after live
1817             #             migration, so we should still disconnect even if
1818             #             the instance doesn't exist here anymore.
1819             error_code = ex.get_error_code()
1820             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1821                 # NOTE(vish):
1822                 LOG.warning("During detach_volume, instance disappeared.",
1823                             instance=instance)
1824             else:
1825                 raise
1826 
1827         self._disconnect_volume(context, connection_info, instance,
1828                                 encryption=encryption)
1829 
1830     def extend_volume(self, connection_info, instance, requested_size):
1831         try:
1832             new_size = self._extend_volume(connection_info, instance,
1833                                            requested_size)
1834         except NotImplementedError:
1835             raise exception.ExtendVolumeNotSupported()
1836 
1837         # Resize the device in QEMU so its size is updated and
1838         # detected by the instance without rebooting.
1839         try:
1840             guest = self._host.get_guest(instance)
1841             state = guest.get_power_state(self._host)
1842             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1843             if active_state:
1844                 if 'device_path' in connection_info['data']:
1845                     disk_path = connection_info['data']['device_path']
1846                 else:
1847                     # Some drivers (eg. net) don't put the device_path
1848                     # into the connection_info. Match disks by their serial
1849                     # number instead
1850                     volume_id = driver_block_device.get_volume_id(
1851                         connection_info)
1852                     disk = next(iter([
1853                         d for d in guest.get_all_disks()
1854                         if d.serial == volume_id
1855                     ]), None)
1856                     if not disk:
1857                         raise exception.VolumeNotFound(volume_id=volume_id)
1858                     disk_path = disk.target_dev
1859 
1860                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1861                           {'dev': disk_path, 'size': new_size})
1862                 dev = guest.get_block_device(disk_path)
1863                 dev.resize(new_size // units.Ki)
1864             else:
1865                 LOG.debug('Skipping block device resize, guest is not running',
1866                           instance=instance)
1867         except exception.InstanceNotFound:
1868             with excutils.save_and_reraise_exception():
1869                 LOG.warning('During extend_volume, instance disappeared.',
1870                             instance=instance)
1871         except libvirt.libvirtError:
1872             with excutils.save_and_reraise_exception():
1873                 LOG.exception('resizing block device failed.',
1874                               instance=instance)
1875 
1876     def attach_interface(self, context, instance, image_meta, vif):
1877         guest = self._host.get_guest(instance)
1878 
1879         self.vif_driver.plug(instance, vif)
1880         self.firewall_driver.setup_basic_filtering(instance, [vif])
1881         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1882                                          instance.flavor,
1883                                          CONF.libvirt.virt_type,
1884                                          self._host)
1885         try:
1886             state = guest.get_power_state(self._host)
1887             live = state in (power_state.RUNNING, power_state.PAUSED)
1888             guest.attach_device(cfg, persistent=True, live=live)
1889         except libvirt.libvirtError:
1890             LOG.error('attaching network adapter failed.',
1891                       instance=instance, exc_info=True)
1892             self.vif_driver.unplug(instance, vif)
1893             raise exception.InterfaceAttachFailed(
1894                     instance_uuid=instance.uuid)
1895         try:
1896             # NOTE(artom) If we're attaching with a device role tag, we need to
1897             # rebuild device_metadata. If we're attaching without a role
1898             # tag, we're rebuilding it here needlessly anyways. This isn't a
1899             # massive deal, and it helps reduce code complexity by not having
1900             # to indicate to the virt driver that the attach is tagged. The
1901             # really important optimization of not calling the database unless
1902             # device_metadata has actually changed is done for us by
1903             # instance.save().
1904             instance.device_metadata = self._build_device_metadata(
1905                 context, instance)
1906             instance.save()
1907         except Exception:
1908             # NOTE(artom) If we fail here it means the interface attached
1909             # successfully but building and/or saving the device metadata
1910             # failed. Just unplugging the vif is therefore not enough cleanup,
1911             # we need to detach the interface.
1912             with excutils.save_and_reraise_exception(reraise=False):
1913                 LOG.error('Interface attached successfully but building '
1914                           'and/or saving device metadata failed.',
1915                           instance=instance, exc_info=True)
1916                 self.detach_interface(context, instance, vif)
1917                 raise exception.InterfaceAttachFailed(
1918                     instance_uuid=instance.uuid)
1919 
1920     def detach_interface(self, context, instance, vif):
1921         guest = self._host.get_guest(instance)
1922         cfg = self.vif_driver.get_config(instance, vif,
1923                                          instance.image_meta,
1924                                          instance.flavor,
1925                                          CONF.libvirt.virt_type, self._host)
1926         interface = guest.get_interface_by_cfg(cfg)
1927         try:
1928             self.vif_driver.unplug(instance, vif)
1929             # NOTE(mriedem): When deleting an instance and using Neutron,
1930             # we can be racing against Neutron deleting the port and
1931             # sending the vif-deleted event which then triggers a call to
1932             # detach the interface, so if the interface is not found then
1933             # we can just log it as a warning.
1934             if not interface:
1935                 mac = vif.get('address')
1936                 # The interface is gone so just log it as a warning.
1937                 LOG.warning('Detaching interface %(mac)s failed because '
1938                             'the device is no longer found on the guest.',
1939                             {'mac': mac}, instance=instance)
1940                 return
1941 
1942             state = guest.get_power_state(self._host)
1943             live = state in (power_state.RUNNING, power_state.PAUSED)
1944             # Now we are going to loop until the interface is detached or we
1945             # timeout.
1946             wait_for_detach = guest.detach_device_with_retry(
1947                 guest.get_interface_by_cfg, cfg, live=live,
1948                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1949             wait_for_detach()
1950         except exception.DeviceDetachFailed:
1951             # We failed to detach the device even with the retry loop, so let's
1952             # dump some debug information to the logs before raising back up.
1953             with excutils.save_and_reraise_exception():
1954                 devname = self.vif_driver.get_vif_devname(vif)
1955                 interface = guest.get_interface_by_cfg(cfg)
1956                 if interface:
1957                     LOG.warning(
1958                         'Failed to detach interface %(devname)s after '
1959                         'repeated attempts. Final interface xml:\n'
1960                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1961                         {'devname': devname,
1962                          'interface_xml': interface.to_xml(),
1963                          'guest_xml': guest.get_xml_desc()},
1964                         instance=instance)
1965         except exception.DeviceNotFound:
1966             # The interface is gone so just log it as a warning.
1967             LOG.warning('Detaching interface %(mac)s failed because '
1968                         'the device is no longer found on the guest.',
1969                         {'mac': vif.get('address')}, instance=instance)
1970         except libvirt.libvirtError as ex:
1971             error_code = ex.get_error_code()
1972             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1973                 LOG.warning("During detach_interface, instance disappeared.",
1974                             instance=instance)
1975             else:
1976                 # NOTE(mriedem): When deleting an instance and using Neutron,
1977                 # we can be racing against Neutron deleting the port and
1978                 # sending the vif-deleted event which then triggers a call to
1979                 # detach the interface, so we might have failed because the
1980                 # network device no longer exists. Libvirt will fail with
1981                 # "operation failed: no matching network device was found"
1982                 # which unfortunately does not have a unique error code so we
1983                 # need to look up the interface by config and if it's not found
1984                 # then we can just log it as a warning rather than tracing an
1985                 # error.
1986                 mac = vif.get('address')
1987                 interface = guest.get_interface_by_cfg(cfg)
1988                 if interface:
1989                     LOG.error('detaching network adapter failed.',
1990                               instance=instance, exc_info=True)
1991                     raise exception.InterfaceDetachFailed(
1992                             instance_uuid=instance.uuid)
1993 
1994                 # The interface is gone so just log it as a warning.
1995                 LOG.warning('Detaching interface %(mac)s failed because '
1996                             'the device is no longer found on the guest.',
1997                             {'mac': mac}, instance=instance)
1998 
1999     def _create_snapshot_metadata(self, image_meta, instance,
2000                                   img_fmt, snp_name):
2001         metadata = {'status': 'active',
2002                     'name': snp_name,
2003                     'properties': {
2004                                    'kernel_id': instance.kernel_id,
2005                                    'image_location': 'snapshot',
2006                                    'image_state': 'available',
2007                                    'owner_id': instance.project_id,
2008                                    'ramdisk_id': instance.ramdisk_id,
2009                                    }
2010                     }
2011         if instance.os_type:
2012             metadata['properties']['os_type'] = instance.os_type
2013 
2014         # NOTE(vish): glance forces ami disk format to be ami
2015         if image_meta.disk_format == 'ami':
2016             metadata['disk_format'] = 'ami'
2017         else:
2018             metadata['disk_format'] = img_fmt
2019 
2020         if image_meta.obj_attr_is_set("container_format"):
2021             metadata['container_format'] = image_meta.container_format
2022         else:
2023             metadata['container_format'] = "bare"
2024 
2025         return metadata
2026 
2027     def snapshot(self, context, instance, image_id, update_task_state):
2028         """Create snapshot from a running VM instance.
2029 
2030         This command only works with qemu 0.14+
2031         """
2032         try:
2033             guest = self._host.get_guest(instance)
2034 
2035             # TODO(sahid): We are converting all calls from a
2036             # virDomain object to use nova.virt.libvirt.Guest.
2037             # We should be able to remove virt_dom at the end.
2038             virt_dom = guest._domain
2039         except exception.InstanceNotFound:
2040             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2041 
2042         snapshot = self._image_api.get(context, image_id)
2043 
2044         # source_format is an on-disk format
2045         # source_type is a backend type
2046         disk_path, source_format = libvirt_utils.find_disk(guest)
2047         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
2048 
2049         # We won't have source_type for raw or qcow2 disks, because we can't
2050         # determine that from the path. We should have it from the libvirt
2051         # xml, though.
2052         if source_type is None:
2053             source_type = source_format
2054         # For lxc instances we won't have it either from libvirt xml
2055         # (because we just gave libvirt the mounted filesystem), or the path,
2056         # so source_type is still going to be None. In this case,
2057         # root_disk is going to default to CONF.libvirt.images_type
2058         # below, which is still safe.
2059 
2060         image_format = CONF.libvirt.snapshot_image_format or source_type
2061 
2062         # NOTE(bfilippov): save lvm and rbd as raw
2063         if image_format == 'lvm' or image_format == 'rbd':
2064             image_format = 'raw'
2065 
2066         metadata = self._create_snapshot_metadata(instance.image_meta,
2067                                                   instance,
2068                                                   image_format,
2069                                                   snapshot['name'])
2070 
2071         snapshot_name = uuidutils.generate_uuid(dashed=False)
2072 
2073         state = guest.get_power_state(self._host)
2074 
2075         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
2076         #               cold snapshots. Currently, checking for encryption is
2077         #               redundant because LVM supports only cold snapshots.
2078         #               It is necessary in case this situation changes in the
2079         #               future.
2080         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU) and
2081                 source_type != 'lvm' and
2082                 not CONF.ephemeral_storage_encryption.enabled and
2083                 not CONF.workarounds.disable_libvirt_livesnapshot and
2084                 # NOTE(rmk): We cannot perform live snapshots when a
2085                 # managedSave file is present, so we will use the cold/legacy
2086                 # method for instances which are shutdown or paused.
2087                 # NOTE(mriedem): Live snapshot doesn't work with paused
2088                 # instances on older versions of libvirt/qemu. We can likely
2089                 # remove the restriction on PAUSED once we require
2090                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
2091                 # Pike Ubuntu Cloud Archive testing in Queens.
2092                 state not in (power_state.SHUTDOWN, power_state.PAUSED)):
2093             live_snapshot = True
2094             # Abort is an idempotent operation, so make sure any block
2095             # jobs which may have failed are ended. This operation also
2096             # confirms the running instance, as opposed to the system as a
2097             # whole, has a new enough version of the hypervisor (bug 1193146).
2098             try:
2099                 guest.get_block_device(disk_path).abort_job()
2100             except libvirt.libvirtError as ex:
2101                 error_code = ex.get_error_code()
2102                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
2103                     live_snapshot = False
2104                 else:
2105                     pass
2106         else:
2107             live_snapshot = False
2108 
2109         self._prepare_domain_for_snapshot(context, live_snapshot, state,
2110                                           instance)
2111 
2112         root_disk = self.image_backend.by_libvirt_path(
2113             instance, disk_path, image_type=source_type)
2114 
2115         if live_snapshot:
2116             LOG.info("Beginning live snapshot process", instance=instance)
2117         else:
2118             LOG.info("Beginning cold snapshot process", instance=instance)
2119 
2120         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
2121 
2122         update_task_state(task_state=task_states.IMAGE_UPLOADING,
2123                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
2124 
2125         try:
2126             metadata['location'] = root_disk.direct_snapshot(
2127                 context, snapshot_name, image_format, image_id,
2128                 instance.image_ref)
2129             self._snapshot_domain(context, live_snapshot, virt_dom, state,
2130                                   instance)
2131             self._image_api.update(context, image_id, metadata,
2132                                    purge_props=False)
2133         except (NotImplementedError, exception.ImageUnacceptable,
2134                 exception.Forbidden) as e:
2135             if type(e) != NotImplementedError:
2136                 LOG.warning('Performing standard snapshot because direct '
2137                             'snapshot failed: %(error)s',
2138                             {'error': encodeutils.exception_to_unicode(e)})
2139             failed_snap = metadata.pop('location', None)
2140             if failed_snap:
2141                 failed_snap = {'url': str(failed_snap)}
2142             root_disk.cleanup_direct_snapshot(failed_snap,
2143                                                   also_destroy_volume=True,
2144                                                   ignore_errors=True)
2145             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
2146                               expected_state=task_states.IMAGE_UPLOADING)
2147 
2148             # TODO(nic): possibly abstract this out to the root_disk
2149             if source_type == 'rbd' and live_snapshot:
2150                 # Standard snapshot uses qemu-img convert from RBD which is
2151                 # not safe to run with live_snapshot.
2152                 live_snapshot = False
2153                 # Suspend the guest, so this is no longer a live snapshot
2154                 self._prepare_domain_for_snapshot(context, live_snapshot,
2155                                                   state, instance)
2156 
2157             snapshot_directory = CONF.libvirt.snapshots_directory
2158             fileutils.ensure_tree(snapshot_directory)
2159             with utils.tempdir(dir=snapshot_directory) as tmpdir:
2160                 try:
2161                     out_path = os.path.join(tmpdir, snapshot_name)
2162                     if live_snapshot:
2163                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
2164                         os.chmod(tmpdir, 0o701)
2165                         self._live_snapshot(context, instance, guest,
2166                                             disk_path, out_path, source_format,
2167                                             image_format, instance.image_meta)
2168                     else:
2169                         root_disk.snapshot_extract(out_path, image_format)
2170                     LOG.info("Snapshot extracted, beginning image upload",
2171                              instance=instance)
2172                 except libvirt.libvirtError as ex:
2173                     error_code = ex.get_error_code()
2174                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
2175                         LOG.info('Instance %(instance_name)s disappeared '
2176                                  'while taking snapshot of it: [Error Code '
2177                                  '%(error_code)s] %(ex)s',
2178                                  {'instance_name': instance.name,
2179                                   'error_code': error_code,
2180                                   'ex': ex},
2181                                  instance=instance)
2182                         raise exception.InstanceNotFound(
2183                             instance_id=instance.uuid)
2184                     else:
2185                         raise
2186                 finally:
2187                     self._snapshot_domain(context, live_snapshot, virt_dom,
2188                                           state, instance)
2189 
2190                 # Upload that image to the image service
2191                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
2192                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
2193                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
2194                     # execute operation with disk concurrency semaphore
2195                     with compute_utils.disk_ops_semaphore:
2196                         self._image_api.update(context,
2197                                                image_id,
2198                                                metadata,
2199                                                image_file)
2200         except Exception:
2201             with excutils.save_and_reraise_exception():
2202                 LOG.exception(_("Failed to snapshot image"))
2203                 failed_snap = metadata.pop('location', None)
2204                 if failed_snap:
2205                     failed_snap = {'url': str(failed_snap)}
2206                 root_disk.cleanup_direct_snapshot(
2207                         failed_snap, also_destroy_volume=True,
2208                         ignore_errors=True)
2209 
2210         LOG.info("Snapshot image upload complete", instance=instance)
2211 
2212     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
2213                                      instance):
2214         # NOTE(dkang): managedSave does not work for LXC
2215         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2216             if state == power_state.RUNNING or state == power_state.PAUSED:
2217                 self.suspend(context, instance)
2218 
2219     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
2220                          instance):
2221         guest = None
2222         # NOTE(dkang): because previous managedSave is not called
2223         #              for LXC, _create_domain must not be called.
2224         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2225             if state == power_state.RUNNING:
2226                 guest = self._create_domain(domain=virt_dom)
2227             elif state == power_state.PAUSED:
2228                 guest = self._create_domain(domain=virt_dom, pause=True)
2229 
2230             if guest is not None:
2231                 self._attach_pci_devices(
2232                     guest, pci_manager.get_instance_pci_devs(instance))
2233                 self._attach_direct_passthrough_ports(
2234                     context, instance, guest)
2235 
2236     def _can_set_admin_password(self, image_meta):
2237 
2238         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
2239             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2240                 raise exception.QemuGuestAgentNotEnabled()
2241         elif not CONF.libvirt.virt_type == 'parallels':
2242             raise exception.SetAdminPasswdNotSupported()
2243 
2244     # TODO(melwitt): Combine this with the similar xenapi code at some point.
2245     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2246         sshkey = instance.key_data if 'key_data' in instance else None
2247         if sshkey and sshkey.startswith("ssh-rsa"):
2248             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2249             # NOTE(melwitt): The convert_password method doesn't actually do
2250             # anything with the context argument, so we can pass None.
2251             instance.system_metadata.update(
2252                 password.convert_password(None, base64.encode_as_text(enc)))
2253             instance.save()
2254 
2255     def set_admin_password(self, instance, new_pass):
2256         self._can_set_admin_password(instance.image_meta)
2257 
2258         guest = self._host.get_guest(instance)
2259         user = instance.image_meta.properties.get("os_admin_user")
2260         if not user:
2261             if instance.os_type == "windows":
2262                 user = "Administrator"
2263             else:
2264                 user = "root"
2265         try:
2266             guest.set_user_password(user, new_pass)
2267         except libvirt.libvirtError as ex:
2268             error_code = ex.get_error_code()
2269             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2270                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2271                           instance_uuid=instance.uuid)
2272                 raise NotImplementedError()
2273 
2274             err_msg = encodeutils.exception_to_unicode(ex)
2275             msg = (_('Error from libvirt while set password for username '
2276                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2277                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2278             raise exception.InternalError(msg)
2279         else:
2280             # Save the password in sysmeta so it may be retrieved from the
2281             # metadata service.
2282             self._save_instance_password_if_sshkey_present(instance, new_pass)
2283 
2284     def _can_quiesce(self, instance, image_meta):
2285         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2286             raise exception.InstanceQuiesceNotSupported(
2287                 instance_id=instance.uuid)
2288 
2289         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2290             raise exception.QemuGuestAgentNotEnabled()
2291 
2292     def _requires_quiesce(self, image_meta):
2293         return image_meta.properties.get('os_require_quiesce', False)
2294 
2295     def _set_quiesced(self, context, instance, image_meta, quiesced):
2296         self._can_quiesce(instance, image_meta)
2297         try:
2298             guest = self._host.get_guest(instance)
2299             if quiesced:
2300                 guest.freeze_filesystems()
2301             else:
2302                 guest.thaw_filesystems()
2303         except libvirt.libvirtError as ex:
2304             error_code = ex.get_error_code()
2305             err_msg = encodeutils.exception_to_unicode(ex)
2306             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2307                      '[Error Code %(error_code)s] %(ex)s')
2308                    % {'instance_name': instance.name,
2309                       'error_code': error_code, 'ex': err_msg})
2310             raise exception.InternalError(msg)
2311 
2312     def quiesce(self, context, instance, image_meta):
2313         """Freeze the guest filesystems to prepare for snapshot.
2314 
2315         The qemu-guest-agent must be setup to execute fsfreeze.
2316         """
2317         self._set_quiesced(context, instance, image_meta, True)
2318 
2319     def unquiesce(self, context, instance, image_meta):
2320         """Thaw the guest filesystems after snapshot."""
2321         self._set_quiesced(context, instance, image_meta, False)
2322 
2323     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2324                        source_format, image_format, image_meta):
2325         """Snapshot an instance without downtime."""
2326         dev = guest.get_block_device(disk_path)
2327 
2328         # Save a copy of the domain's persistent XML file
2329         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2330 
2331         # Abort is an idempotent operation, so make sure any block
2332         # jobs which may have failed are ended.
2333         try:
2334             dev.abort_job()
2335         except Exception:
2336             pass
2337 
2338         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2339         #             in QEMU 1.3. In order to do this, we need to create
2340         #             a destination image with the original backing file
2341         #             and matching size of the instance root disk.
2342         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2343                                                     format=source_format)
2344         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2345                                                         format=source_format,
2346                                                         basename=False)
2347         disk_delta = out_path + '.delta'
2348         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2349                                        src_disk_size)
2350 
2351         quiesced = False
2352         try:
2353             self._set_quiesced(context, instance, image_meta, True)
2354             quiesced = True
2355         except exception.NovaException as err:
2356             if self._requires_quiesce(image_meta):
2357                 raise
2358             LOG.info('Skipping quiescing instance: %(reason)s.',
2359                      {'reason': err}, instance=instance)
2360 
2361         try:
2362             # NOTE (rmk): blockRebase cannot be executed on persistent
2363             #             domains, so we need to temporarily undefine it.
2364             #             If any part of this block fails, the domain is
2365             #             re-defined regardless.
2366             if guest.has_persistent_configuration():
2367                 support_uefi = self._has_uefi_support()
2368                 guest.delete_configuration(support_uefi)
2369 
2370             # NOTE (rmk): Establish a temporary mirror of our root disk and
2371             #             issue an abort once we have a complete copy.
2372             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2373 
2374             while not dev.is_job_complete():
2375                 time.sleep(0.5)
2376 
2377             dev.abort_job()
2378             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2379         finally:
2380             self._host.write_instance_config(xml)
2381             if quiesced:
2382                 self._set_quiesced(context, instance, image_meta, False)
2383 
2384         # Convert the delta (CoW) image with a backing file to a flat
2385         # image with no backing file.
2386         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2387                                        out_path, image_format)
2388 
2389     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2390         """Send a snapshot status update to Cinder.
2391 
2392         This method captures and logs exceptions that occur
2393         since callers cannot do anything useful with these exceptions.
2394 
2395         Operations on the Cinder side waiting for this will time out if
2396         a failure occurs sending the update.
2397 
2398         :param context: security context
2399         :param snapshot_id: id of snapshot being updated
2400         :param status: new status value
2401 
2402         """
2403 
2404         try:
2405             self._volume_api.update_snapshot_status(context,
2406                                                     snapshot_id,
2407                                                     status)
2408         except Exception:
2409             LOG.exception(_('Failed to send updated snapshot status '
2410                             'to volume service.'))
2411 
2412     def _volume_snapshot_create(self, context, instance, guest,
2413                                 volume_id, new_file):
2414         """Perform volume snapshot.
2415 
2416            :param guest: VM that volume is attached to
2417            :param volume_id: volume UUID to snapshot
2418            :param new_file: relative path to new qcow2 file present on share
2419 
2420         """
2421         xml = guest.get_xml_desc()
2422         xml_doc = etree.fromstring(xml)
2423 
2424         device_info = vconfig.LibvirtConfigGuest()
2425         device_info.parse_dom(xml_doc)
2426 
2427         disks_to_snap = []          # to be snapshotted by libvirt
2428         network_disks_to_snap = []  # network disks (netfs, etc.)
2429         disks_to_skip = []          # local disks not snapshotted
2430 
2431         for guest_disk in device_info.devices:
2432             if (guest_disk.root_name != 'disk'):
2433                 continue
2434 
2435             if (guest_disk.target_dev is None):
2436                 continue
2437 
2438             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2439                 disks_to_skip.append(guest_disk.target_dev)
2440                 continue
2441 
2442             # disk is a Cinder volume with the correct volume_id
2443 
2444             disk_info = {
2445                 'dev': guest_disk.target_dev,
2446                 'serial': guest_disk.serial,
2447                 'current_file': guest_disk.source_path,
2448                 'source_protocol': guest_disk.source_protocol,
2449                 'source_name': guest_disk.source_name,
2450                 'source_hosts': guest_disk.source_hosts,
2451                 'source_ports': guest_disk.source_ports
2452             }
2453 
2454             # Determine path for new_file based on current path
2455             if disk_info['current_file'] is not None:
2456                 current_file = disk_info['current_file']
2457                 new_file_path = os.path.join(os.path.dirname(current_file),
2458                                              new_file)
2459                 disks_to_snap.append((current_file, new_file_path))
2460             # NOTE(mriedem): This used to include a check for gluster in
2461             # addition to netfs since they were added together. Support for
2462             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2463             # however, if other volume drivers rely on the netfs disk source
2464             # protocol.
2465             elif disk_info['source_protocol'] == 'netfs':
2466                 network_disks_to_snap.append((disk_info, new_file))
2467 
2468         if not disks_to_snap and not network_disks_to_snap:
2469             msg = _('Found no disk to snapshot.')
2470             raise exception.InternalError(msg)
2471 
2472         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2473 
2474         for current_name, new_filename in disks_to_snap:
2475             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2476             snap_disk.name = current_name
2477             snap_disk.source_path = new_filename
2478             snap_disk.source_type = 'file'
2479             snap_disk.snapshot = 'external'
2480             snap_disk.driver_name = 'qcow2'
2481 
2482             snapshot.add_disk(snap_disk)
2483 
2484         for disk_info, new_filename in network_disks_to_snap:
2485             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2486             snap_disk.name = disk_info['dev']
2487             snap_disk.source_type = 'network'
2488             snap_disk.source_protocol = disk_info['source_protocol']
2489             snap_disk.snapshot = 'external'
2490             snap_disk.source_path = new_filename
2491             old_dir = disk_info['source_name'].split('/')[0]
2492             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2493             snap_disk.source_hosts = disk_info['source_hosts']
2494             snap_disk.source_ports = disk_info['source_ports']
2495 
2496             snapshot.add_disk(snap_disk)
2497 
2498         for dev in disks_to_skip:
2499             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2500             snap_disk.name = dev
2501             snap_disk.snapshot = 'no'
2502 
2503             snapshot.add_disk(snap_disk)
2504 
2505         snapshot_xml = snapshot.to_xml()
2506         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2507 
2508         image_meta = instance.image_meta
2509         try:
2510             # Check to see if we can quiesce the guest before taking the
2511             # snapshot.
2512             self._can_quiesce(instance, image_meta)
2513             try:
2514                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2515                                reuse_ext=True, quiesce=True)
2516                 return
2517             except libvirt.libvirtError:
2518                 # If the image says that quiesce is required then we fail.
2519                 if self._requires_quiesce(image_meta):
2520                     raise
2521                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2522                                 'attempting again with quiescing disabled.'),
2523                               instance=instance)
2524         except (exception.InstanceQuiesceNotSupported,
2525                 exception.QemuGuestAgentNotEnabled) as err:
2526             # If the image says that quiesce is required then we need to fail.
2527             if self._requires_quiesce(image_meta):
2528                 raise
2529             LOG.info('Skipping quiescing instance: %(reason)s.',
2530                      {'reason': err}, instance=instance)
2531 
2532         try:
2533             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2534                            reuse_ext=True, quiesce=False)
2535         except libvirt.libvirtError:
2536             LOG.exception(_('Unable to create VM snapshot, '
2537                             'failing volume_snapshot operation.'),
2538                           instance=instance)
2539 
2540             raise
2541 
2542     def _volume_refresh_connection_info(self, context, instance, volume_id):
2543         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2544                   context, volume_id, instance.uuid)
2545 
2546         driver_bdm = driver_block_device.convert_volume(bdm)
2547         if driver_bdm:
2548             driver_bdm.refresh_connection_info(context, instance,
2549                                                self._volume_api, self)
2550 
2551     def volume_snapshot_create(self, context, instance, volume_id,
2552                                create_info):
2553         """Create snapshots of a Cinder volume via libvirt.
2554 
2555         :param instance: VM instance object reference
2556         :param volume_id: id of volume being snapshotted
2557         :param create_info: dict of information used to create snapshots
2558                      - snapshot_id : ID of snapshot
2559                      - type : qcow2 / <other>
2560                      - new_file : qcow2 file created by Cinder which
2561                      becomes the VM's active image after
2562                      the snapshot is complete
2563         """
2564 
2565         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2566                   {'c_info': create_info}, instance=instance)
2567 
2568         try:
2569             guest = self._host.get_guest(instance)
2570         except exception.InstanceNotFound:
2571             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2572 
2573         if create_info['type'] != 'qcow2':
2574             msg = _('Unknown type: %s') % create_info['type']
2575             raise exception.InternalError(msg)
2576 
2577         snapshot_id = create_info.get('snapshot_id', None)
2578         if snapshot_id is None:
2579             msg = _('snapshot_id required in create_info')
2580             raise exception.InternalError(msg)
2581 
2582         try:
2583             self._volume_snapshot_create(context, instance, guest,
2584                                          volume_id, create_info['new_file'])
2585         except Exception:
2586             with excutils.save_and_reraise_exception():
2587                 LOG.exception(_('Error occurred during '
2588                                 'volume_snapshot_create, '
2589                                 'sending error status to Cinder.'),
2590                               instance=instance)
2591                 self._volume_snapshot_update_status(
2592                     context, snapshot_id, 'error')
2593 
2594         self._volume_snapshot_update_status(
2595             context, snapshot_id, 'creating')
2596 
2597         def _wait_for_snapshot():
2598             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2599 
2600             if snapshot.get('status') != 'creating':
2601                 self._volume_refresh_connection_info(context, instance,
2602                                                      volume_id)
2603                 raise loopingcall.LoopingCallDone()
2604 
2605         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2606         timer.start(interval=0.5).wait()
2607 
2608     @staticmethod
2609     def _rebase_with_qemu_img(guest, device, active_disk_object,
2610                               rebase_base):
2611         """Rebase a device tied to a guest using qemu-img.
2612 
2613         :param guest:the Guest which owns the device being rebased
2614         :type guest: nova.virt.libvirt.guest.Guest
2615         :param device: the guest block device to rebase
2616         :type device: nova.virt.libvirt.guest.BlockDevice
2617         :param active_disk_object: the guest block device to rebase
2618         :type active_disk_object: nova.virt.libvirt.config.\
2619                                     LibvirtConfigGuestDisk
2620         :param rebase_base: the new parent in the backing chain
2621         :type rebase_base: None or string
2622         """
2623 
2624         # It's unsure how well qemu-img handles network disks for
2625         # every protocol. So let's be safe.
2626         active_protocol = active_disk_object.source_protocol
2627         if active_protocol is not None:
2628             msg = _("Something went wrong when deleting a volume snapshot: "
2629                     "rebasing a %(protocol)s network disk using qemu-img "
2630                     "has not been fully tested") % {'protocol':
2631                     active_protocol}
2632             LOG.error(msg)
2633             raise exception.InternalError(msg)
2634 
2635         if rebase_base is None:
2636             # If backing_file is specified as "" (the empty string), then
2637             # the image is rebased onto no backing file (i.e. it will exist
2638             # independently of any backing file).
2639             backing_file = ""
2640             qemu_img_extra_arg = []
2641         else:
2642             # If the rebased image is going to have a backing file then
2643             # explicitly set the backing file format to avoid any security
2644             # concerns related to file format auto detection.
2645             backing_file = rebase_base
2646             b_file_fmt = images.qemu_img_info(backing_file).file_format
2647             qemu_img_extra_arg = ['-F', b_file_fmt]
2648 
2649         qemu_img_extra_arg.append(active_disk_object.source_path)
2650         # execute operation with disk concurrency semaphore
2651         with compute_utils.disk_ops_semaphore:
2652             processutils.execute("qemu-img", "rebase", "-b", backing_file,
2653                                  *qemu_img_extra_arg)
2654 
2655     def _volume_snapshot_delete(self, context, instance, volume_id,
2656                                 snapshot_id, delete_info=None):
2657         """Note:
2658             if file being merged into == active image:
2659                 do a blockRebase (pull) operation
2660             else:
2661                 do a blockCommit operation
2662             Files must be adjacent in snap chain.
2663 
2664         :param instance: instance object reference
2665         :param volume_id: volume UUID
2666         :param snapshot_id: snapshot UUID (unused currently)
2667         :param delete_info: {
2668             'type':              'qcow2',
2669             'file_to_merge':     'a.img',
2670             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2671                                                   active image)
2672           }
2673         """
2674 
2675         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2676                   instance=instance)
2677 
2678         if delete_info['type'] != 'qcow2':
2679             msg = _('Unknown delete_info type %s') % delete_info['type']
2680             raise exception.InternalError(msg)
2681 
2682         try:
2683             guest = self._host.get_guest(instance)
2684         except exception.InstanceNotFound:
2685             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2686 
2687         # Find dev name
2688         my_dev = None
2689         active_disk = None
2690 
2691         xml = guest.get_xml_desc()
2692         xml_doc = etree.fromstring(xml)
2693 
2694         device_info = vconfig.LibvirtConfigGuest()
2695         device_info.parse_dom(xml_doc)
2696 
2697         active_disk_object = None
2698 
2699         for guest_disk in device_info.devices:
2700             if (guest_disk.root_name != 'disk'):
2701                 continue
2702 
2703             if (guest_disk.target_dev is None or guest_disk.serial is None):
2704                 continue
2705 
2706             if guest_disk.serial == volume_id:
2707                 my_dev = guest_disk.target_dev
2708 
2709                 active_disk = guest_disk.source_path
2710                 active_protocol = guest_disk.source_protocol
2711                 active_disk_object = guest_disk
2712                 break
2713 
2714         if my_dev is None or (active_disk is None and active_protocol is None):
2715             LOG.debug('Domain XML: %s', xml, instance=instance)
2716             msg = (_('Disk with id: %s not found attached to instance.')
2717                    % volume_id)
2718             raise exception.InternalError(msg)
2719 
2720         LOG.debug("found device at %s", my_dev, instance=instance)
2721 
2722         def _get_snap_dev(filename, backing_store):
2723             if filename is None:
2724                 msg = _('filename cannot be None')
2725                 raise exception.InternalError(msg)
2726 
2727             # libgfapi delete
2728             LOG.debug("XML: %s", xml)
2729 
2730             LOG.debug("active disk object: %s", active_disk_object)
2731 
2732             # determine reference within backing store for desired image
2733             filename_to_merge = filename
2734             matched_name = None
2735             b = backing_store
2736             index = None
2737 
2738             current_filename = active_disk_object.source_name.split('/')[1]
2739             if current_filename == filename_to_merge:
2740                 return my_dev + '[0]'
2741 
2742             while b is not None:
2743                 source_filename = b.source_name.split('/')[1]
2744                 if source_filename == filename_to_merge:
2745                     LOG.debug('found match: %s', b.source_name)
2746                     matched_name = b.source_name
2747                     index = b.index
2748                     break
2749 
2750                 b = b.backing_store
2751 
2752             if matched_name is None:
2753                 msg = _('no match found for %s') % (filename_to_merge)
2754                 raise exception.InternalError(msg)
2755 
2756             LOG.debug('index of match (%s) is %s', b.source_name, index)
2757 
2758             my_snap_dev = '%s[%s]' % (my_dev, index)
2759             return my_snap_dev
2760 
2761         if delete_info['merge_target_file'] is None:
2762             # pull via blockRebase()
2763 
2764             # Merge the most recent snapshot into the active image
2765 
2766             rebase_disk = my_dev
2767             rebase_base = delete_info['file_to_merge']  # often None
2768             if (active_protocol is not None) and (rebase_base is not None):
2769                 rebase_base = _get_snap_dev(rebase_base,
2770                                             active_disk_object.backing_store)
2771 
2772             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2773             # and when available this flag _must_ be used to ensure backing
2774             # paths are maintained relative by qemu.
2775             #
2776             # If _RELATIVE flag not found, continue with old behaviour
2777             # (relative backing path seems to work for this case)
2778             try:
2779                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2780                 relative = rebase_base is not None
2781             except AttributeError:
2782                 LOG.warning(
2783                     "Relative blockrebase support was not detected. "
2784                     "Continuing with old behaviour.")
2785                 relative = False
2786 
2787             LOG.debug(
2788                 'disk: %(disk)s, base: %(base)s, '
2789                 'bw: %(bw)s, relative: %(relative)s',
2790                 {'disk': rebase_disk,
2791                  'base': rebase_base,
2792                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2793                  'relative': str(relative)}, instance=instance)
2794 
2795             dev = guest.get_block_device(rebase_disk)
2796             if guest.is_active():
2797                 result = dev.rebase(rebase_base, relative=relative)
2798                 if result == 0:
2799                     LOG.debug('blockRebase started successfully',
2800                               instance=instance)
2801 
2802                 while not dev.is_job_complete():
2803                     LOG.debug('waiting for blockRebase job completion',
2804                               instance=instance)
2805                     time.sleep(0.5)
2806 
2807             # If the guest is not running libvirt won't do a blockRebase.
2808             # In that case, let's ask qemu-img to rebase the disk.
2809             else:
2810                 LOG.debug('Guest is not running so doing a block rebase '
2811                           'using "qemu-img rebase"', instance=instance)
2812                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2813                                            rebase_base)
2814 
2815         else:
2816             # commit with blockCommit()
2817             my_snap_base = None
2818             my_snap_top = None
2819             commit_disk = my_dev
2820 
2821             if active_protocol is not None:
2822                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2823                                              active_disk_object.backing_store)
2824                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2825                                             active_disk_object.backing_store)
2826 
2827             commit_base = my_snap_base or delete_info['merge_target_file']
2828             commit_top = my_snap_top or delete_info['file_to_merge']
2829 
2830             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2831                       'commit_base=%(commit_base)s '
2832                       'commit_top=%(commit_top)s ',
2833                       {'commit_disk': commit_disk,
2834                        'commit_base': commit_base,
2835                        'commit_top': commit_top}, instance=instance)
2836 
2837             dev = guest.get_block_device(commit_disk)
2838             result = dev.commit(commit_base, commit_top, relative=True)
2839 
2840             if result == 0:
2841                 LOG.debug('blockCommit started successfully',
2842                           instance=instance)
2843 
2844             while not dev.is_job_complete():
2845                 LOG.debug('waiting for blockCommit job completion',
2846                           instance=instance)
2847                 time.sleep(0.5)
2848 
2849     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2850                                delete_info):
2851         try:
2852             self._volume_snapshot_delete(context, instance, volume_id,
2853                                          snapshot_id, delete_info=delete_info)
2854         except Exception:
2855             with excutils.save_and_reraise_exception():
2856                 LOG.exception(_('Error occurred during '
2857                                 'volume_snapshot_delete, '
2858                                 'sending error status to Cinder.'),
2859                               instance=instance)
2860                 self._volume_snapshot_update_status(
2861                     context, snapshot_id, 'error_deleting')
2862 
2863         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2864         self._volume_refresh_connection_info(context, instance, volume_id)
2865 
2866     def reboot(self, context, instance, network_info, reboot_type,
2867                block_device_info=None, bad_volumes_callback=None):
2868         """Reboot a virtual machine, given an instance reference."""
2869         if reboot_type == 'SOFT':
2870             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2871             try:
2872                 soft_reboot_success = self._soft_reboot(instance)
2873             except libvirt.libvirtError as e:
2874                 LOG.debug("Instance soft reboot failed: %s",
2875                           encodeutils.exception_to_unicode(e),
2876                           instance=instance)
2877                 soft_reboot_success = False
2878 
2879             if soft_reboot_success:
2880                 LOG.info("Instance soft rebooted successfully.",
2881                          instance=instance)
2882                 return
2883             else:
2884                 LOG.warning("Failed to soft reboot instance. "
2885                             "Trying hard reboot.",
2886                             instance=instance)
2887         return self._hard_reboot(context, instance, network_info,
2888                                  block_device_info)
2889 
2890     def _soft_reboot(self, instance):
2891         """Attempt to shutdown and restart the instance gracefully.
2892 
2893         We use shutdown and create here so we can return if the guest
2894         responded and actually rebooted. Note that this method only
2895         succeeds if the guest responds to acpi. Therefore we return
2896         success or failure so we can fall back to a hard reboot if
2897         necessary.
2898 
2899         :returns: True if the reboot succeeded
2900         """
2901         guest = self._host.get_guest(instance)
2902 
2903         state = guest.get_power_state(self._host)
2904         old_domid = guest.id
2905         # NOTE(vish): This check allows us to reboot an instance that
2906         #             is already shutdown.
2907         if state == power_state.RUNNING:
2908             guest.shutdown()
2909         # NOTE(vish): This actually could take slightly longer than the
2910         #             FLAG defines depending on how long the get_info
2911         #             call takes to return.
2912         self._prepare_pci_devices_for_use(
2913             pci_manager.get_instance_pci_devs(instance, 'all'))
2914         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2915             guest = self._host.get_guest(instance)
2916 
2917             state = guest.get_power_state(self._host)
2918             new_domid = guest.id
2919 
2920             # NOTE(ivoks): By checking domain IDs, we make sure we are
2921             #              not recreating domain that's already running.
2922             if old_domid != new_domid:
2923                 if state in [power_state.SHUTDOWN,
2924                              power_state.CRASHED]:
2925                     LOG.info("Instance shutdown successfully.",
2926                              instance=instance)
2927                     self._create_domain(domain=guest._domain)
2928                     timer = loopingcall.FixedIntervalLoopingCall(
2929                         self._wait_for_running, instance)
2930                     timer.start(interval=0.5).wait()
2931                     return True
2932                 else:
2933                     LOG.info("Instance may have been rebooted during soft "
2934                              "reboot, so return now.", instance=instance)
2935                     return True
2936             greenthread.sleep(1)
2937         return False
2938 
2939     def _hard_reboot(self, context, instance, network_info,
2940                      block_device_info=None):
2941         """Reboot a virtual machine, given an instance reference.
2942 
2943         Performs a Libvirt reset (if supported) on the domain.
2944 
2945         If Libvirt reset is unavailable this method actually destroys and
2946         re-creates the domain to ensure the reboot happens, as the guest
2947         OS cannot ignore this action.
2948         """
2949         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
2950         # need to remember the existing mdevs for reusing them.
2951         mdevs = self._get_all_assigned_mediated_devices(instance)
2952         mdevs = list(mdevs.keys())
2953         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2954         # the hard reboot operation is relied upon by operators to be an
2955         # automated attempt to fix as many things as possible about a
2956         # non-functioning instance before resorting to manual intervention.
2957         # With this goal in mind, we tear down all the aspects of an instance
2958         # we can here without losing data. This allows us to re-initialise from
2959         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2960         self.destroy(context, instance, network_info, destroy_disks=False,
2961                      block_device_info=block_device_info)
2962 
2963         # Convert the system metadata to image metadata
2964         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2965         #                https://bugs.launchpad.net/nova/+bug/1349978
2966         instance_dir = libvirt_utils.get_instance_path(instance)
2967         fileutils.ensure_tree(instance_dir)
2968 
2969         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2970                                             instance,
2971                                             instance.image_meta,
2972                                             block_device_info)
2973         # NOTE(vish): This could generate the wrong device_format if we are
2974         #             using the raw backend and the images don't exist yet.
2975         #             The create_images_and_backing below doesn't properly
2976         #             regenerate raw backend images, however, so when it
2977         #             does we need to (re)generate the xml after the images
2978         #             are in place.
2979         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2980                                   instance.image_meta,
2981                                   block_device_info=block_device_info,
2982                                   mdevs=mdevs)
2983 
2984         # NOTE(mdbooth): context.auth_token will not be set when we call
2985         #                _hard_reboot from resume_state_on_host_boot()
2986         if context.auth_token is not None:
2987             # NOTE (rmk): Re-populate any missing backing files.
2988             config = vconfig.LibvirtConfigGuest()
2989             config.parse_str(xml)
2990             backing_disk_info = self._get_instance_disk_info_from_config(
2991                 config, block_device_info)
2992             self._create_images_and_backing(context, instance, instance_dir,
2993                                             backing_disk_info)
2994 
2995         # Initialize all the necessary networking, block devices and
2996         # start the instance.
2997         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
2998         # unplugged vifs earlier. The behavior of neutron plug events depends
2999         # on which vif type we're using and we are working with a stale network
3000         # info cache here, so won't rely on waiting for neutron plug events.
3001         # vifs_already_plugged=True means "do not wait for neutron plug events"
3002         self._create_domain_and_network(context, xml, instance, network_info,
3003                                         block_device_info=block_device_info,
3004                                         vifs_already_plugged=True)
3005         self._prepare_pci_devices_for_use(
3006             pci_manager.get_instance_pci_devs(instance, 'all'))
3007 
3008         def _wait_for_reboot():
3009             """Called at an interval until the VM is running again."""
3010             state = self.get_info(instance).state
3011 
3012             if state == power_state.RUNNING:
3013                 LOG.info("Instance rebooted successfully.",
3014                          instance=instance)
3015                 raise loopingcall.LoopingCallDone()
3016 
3017         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
3018         timer.start(interval=0.5).wait()
3019 
3020     def pause(self, instance):
3021         """Pause VM instance."""
3022         self._host.get_guest(instance).pause()
3023 
3024     def unpause(self, instance):
3025         """Unpause paused VM instance."""
3026         guest = self._host.get_guest(instance)
3027         guest.resume()
3028         guest.sync_guest_time()
3029 
3030     def _clean_shutdown(self, instance, timeout, retry_interval):
3031         """Attempt to shutdown the instance gracefully.
3032 
3033         :param instance: The instance to be shutdown
3034         :param timeout: How long to wait in seconds for the instance to
3035                         shutdown
3036         :param retry_interval: How often in seconds to signal the instance
3037                                to shutdown while waiting
3038 
3039         :returns: True if the shutdown succeeded
3040         """
3041 
3042         # List of states that represent a shutdown instance
3043         SHUTDOWN_STATES = [power_state.SHUTDOWN,
3044                            power_state.CRASHED]
3045 
3046         try:
3047             guest = self._host.get_guest(instance)
3048         except exception.InstanceNotFound:
3049             # If the instance has gone then we don't need to
3050             # wait for it to shutdown
3051             return True
3052 
3053         state = guest.get_power_state(self._host)
3054         if state in SHUTDOWN_STATES:
3055             LOG.info("Instance already shutdown.", instance=instance)
3056             return True
3057 
3058         LOG.debug("Shutting down instance from state %s", state,
3059                   instance=instance)
3060         guest.shutdown()
3061         retry_countdown = retry_interval
3062 
3063         for sec in range(timeout):
3064 
3065             guest = self._host.get_guest(instance)
3066             state = guest.get_power_state(self._host)
3067 
3068             if state in SHUTDOWN_STATES:
3069                 LOG.info("Instance shutdown successfully after %d seconds.",
3070                          sec, instance=instance)
3071                 return True
3072 
3073             # Note(PhilD): We can't assume that the Guest was able to process
3074             #              any previous shutdown signal (for example it may
3075             #              have still been startingup, so within the overall
3076             #              timeout we re-trigger the shutdown every
3077             #              retry_interval
3078             if retry_countdown == 0:
3079                 retry_countdown = retry_interval
3080                 # Instance could shutdown at any time, in which case we
3081                 # will get an exception when we call shutdown
3082                 try:
3083                     LOG.debug("Instance in state %s after %d seconds - "
3084                               "resending shutdown", state, sec,
3085                               instance=instance)
3086                     guest.shutdown()
3087                 except libvirt.libvirtError:
3088                     # Assume this is because its now shutdown, so loop
3089                     # one more time to clean up.
3090                     LOG.debug("Ignoring libvirt exception from shutdown "
3091                               "request.", instance=instance)
3092                     continue
3093             else:
3094                 retry_countdown -= 1
3095 
3096             time.sleep(1)
3097 
3098         LOG.info("Instance failed to shutdown in %d seconds.",
3099                  timeout, instance=instance)
3100         return False
3101 
3102     def power_off(self, instance, timeout=0, retry_interval=0):
3103         """Power off the specified instance."""
3104         if timeout:
3105             self._clean_shutdown(instance, timeout, retry_interval)
3106         self._destroy(instance)
3107 
3108     def power_on(self, context, instance, network_info,
3109                  block_device_info=None):
3110         """Power on the specified instance."""
3111         # We use _hard_reboot here to ensure that all backing files,
3112         # network, and block device connections, etc. are established
3113         # and available before we attempt to start the instance.
3114         self._hard_reboot(context, instance, network_info, block_device_info)
3115 
3116     def trigger_crash_dump(self, instance):
3117 
3118         """Trigger crash dump by injecting an NMI to the specified instance."""
3119         try:
3120             self._host.get_guest(instance).inject_nmi()
3121         except libvirt.libvirtError as ex:
3122             error_code = ex.get_error_code()
3123 
3124             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
3125                 raise exception.TriggerCrashDumpNotSupported()
3126             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
3127                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
3128 
3129             LOG.exception(_('Error from libvirt while injecting an NMI to '
3130                             '%(instance_uuid)s: '
3131                             '[Error Code %(error_code)s] %(ex)s'),
3132                           {'instance_uuid': instance.uuid,
3133                            'error_code': error_code, 'ex': ex})
3134             raise
3135 
3136     def suspend(self, context, instance):
3137         """Suspend the specified instance."""
3138         guest = self._host.get_guest(instance)
3139 
3140         self._detach_pci_devices(guest,
3141             pci_manager.get_instance_pci_devs(instance))
3142         self._detach_direct_passthrough_ports(context, instance, guest)
3143         self._detach_mediated_devices(guest)
3144         guest.save_memory_state()
3145 
3146     def resume(self, context, instance, network_info, block_device_info=None):
3147         """resume the specified instance."""
3148         xml = self._get_existing_domain_xml(instance, network_info,
3149                                             block_device_info)
3150         guest = self._create_domain_and_network(context, xml, instance,
3151                            network_info, block_device_info=block_device_info,
3152                            vifs_already_plugged=True)
3153         self._attach_pci_devices(guest,
3154             pci_manager.get_instance_pci_devs(instance))
3155         self._attach_direct_passthrough_ports(
3156             context, instance, guest, network_info)
3157         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
3158                                                      instance)
3159         timer.start(interval=0.5).wait()
3160         guest.sync_guest_time()
3161 
3162     def resume_state_on_host_boot(self, context, instance, network_info,
3163                                   block_device_info=None):
3164         """resume guest state when a host is booted."""
3165         # Check if the instance is running already and avoid doing
3166         # anything if it is.
3167         try:
3168             guest = self._host.get_guest(instance)
3169             state = guest.get_power_state(self._host)
3170 
3171             ignored_states = (power_state.RUNNING,
3172                               power_state.SUSPENDED,
3173                               power_state.NOSTATE,
3174                               power_state.PAUSED)
3175 
3176             if state in ignored_states:
3177                 return
3178         except (exception.InternalError, exception.InstanceNotFound):
3179             pass
3180 
3181         # Instance is not up and could be in an unknown state.
3182         # Be as absolute as possible about getting it back into
3183         # a known and running state.
3184         self._hard_reboot(context, instance, network_info, block_device_info)
3185 
3186     def rescue(self, context, instance, network_info, image_meta,
3187                rescue_password):
3188         """Loads a VM using rescue images.
3189 
3190         A rescue is normally performed when something goes wrong with the
3191         primary images and data needs to be corrected/recovered. Rescuing
3192         should not edit or over-ride the original image, only allow for
3193         data recovery.
3194 
3195         """
3196         instance_dir = libvirt_utils.get_instance_path(instance)
3197         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
3198         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3199         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
3200 
3201         rescue_image_id = None
3202         if image_meta.obj_attr_is_set("id"):
3203             rescue_image_id = image_meta.id
3204 
3205         rescue_images = {
3206             'image_id': (rescue_image_id or
3207                         CONF.libvirt.rescue_image_id or instance.image_ref),
3208             'kernel_id': (CONF.libvirt.rescue_kernel_id or
3209                           instance.kernel_id),
3210             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
3211                            instance.ramdisk_id),
3212         }
3213         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3214                                             instance,
3215                                             image_meta,
3216                                             rescue=True)
3217         injection_info = InjectionInfo(network_info=network_info,
3218                                        admin_pass=rescue_password,
3219                                        files=None)
3220         gen_confdrive = functools.partial(self._create_configdrive,
3221                                           context, instance, injection_info,
3222                                           rescue=True)
3223         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
3224         # remember the existing mdevs for reusing them.
3225         mdevs = self._get_all_assigned_mediated_devices(instance)
3226         mdevs = list(mdevs.keys())
3227         self._create_image(context, instance, disk_info['mapping'],
3228                            injection_info=injection_info, suffix='.rescue',
3229                            disk_images=rescue_images)
3230         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3231                                   image_meta, rescue=rescue_images,
3232                                   mdevs=mdevs)
3233         self._destroy(instance)
3234         self._create_domain(xml, post_xml_callback=gen_confdrive)
3235 
3236     def unrescue(self, instance, network_info):
3237         """Reboot the VM which is being rescued back into primary images.
3238         """
3239         instance_dir = libvirt_utils.get_instance_path(instance)
3240         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3241         xml = libvirt_utils.load_file(unrescue_xml_path)
3242         guest = self._host.get_guest(instance)
3243 
3244         # TODO(sahid): We are converting all calls from a
3245         # virDomain object to use nova.virt.libvirt.Guest.
3246         # We should be able to remove virt_dom at the end.
3247         virt_dom = guest._domain
3248         self._destroy(instance)
3249         self._create_domain(xml, virt_dom)
3250         os.unlink(unrescue_xml_path)
3251         rescue_files = os.path.join(instance_dir, "*.rescue")
3252         for rescue_file in glob.iglob(rescue_files):
3253             if os.path.isdir(rescue_file):
3254                 shutil.rmtree(rescue_file)
3255             else:
3256                 os.unlink(rescue_file)
3257         # cleanup rescue volume
3258         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3259                                 if lvmdisk.endswith('.rescue')])
3260         if CONF.libvirt.images_type == 'rbd':
3261             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3262                                       disk.endswith('.rescue'))
3263             rbd_utils.RBDDriver().cleanup_volumes(filter_fn)
3264 
3265     def poll_rebooting_instances(self, timeout, instances):
3266         pass
3267 
3268     def spawn(self, context, instance, image_meta, injected_files,
3269               admin_password, allocations, network_info=None,
3270               block_device_info=None):
3271         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3272                                             instance,
3273                                             image_meta,
3274                                             block_device_info)
3275         injection_info = InjectionInfo(network_info=network_info,
3276                                        files=injected_files,
3277                                        admin_pass=admin_password)
3278         gen_confdrive = functools.partial(self._create_configdrive,
3279                                           context, instance,
3280                                           injection_info)
3281         self._create_image(context, instance, disk_info['mapping'],
3282                            injection_info=injection_info,
3283                            block_device_info=block_device_info)
3284 
3285         # Required by Quobyte CI
3286         self._ensure_console_log_for_instance(instance)
3287 
3288         # Does the guest need to be assigned some vGPU mediated devices ?
3289         mdevs = self._allocate_mdevs(allocations)
3290 
3291         xml = self._get_guest_xml(context, instance, network_info,
3292                                   disk_info, image_meta,
3293                                   block_device_info=block_device_info,
3294                                   mdevs=mdevs)
3295         self._create_domain_and_network(
3296             context, xml, instance, network_info,
3297             block_device_info=block_device_info,
3298             post_xml_callback=gen_confdrive,
3299             destroy_disks_on_failure=True)
3300         LOG.debug("Guest created on hypervisor", instance=instance)
3301 
3302         def _wait_for_boot():
3303             """Called at an interval until the VM is running."""
3304             state = self.get_info(instance).state
3305 
3306             if state == power_state.RUNNING:
3307                 LOG.info("Instance spawned successfully.", instance=instance)
3308                 raise loopingcall.LoopingCallDone()
3309 
3310         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3311         timer.start(interval=0.5).wait()
3312 
3313     def _get_console_output_file(self, instance, console_log):
3314         bytes_to_read = MAX_CONSOLE_BYTES
3315         log_data = b""  # The last N read bytes
3316         i = 0  # in case there is a log rotation (like "virtlogd")
3317         path = console_log
3318 
3319         while bytes_to_read > 0 and os.path.exists(path):
3320             read_log_data, remaining = nova.privsep.path.last_bytes(
3321                                         path, bytes_to_read)
3322             # We need the log file content in chronological order,
3323             # that's why we *prepend* the log data.
3324             log_data = read_log_data + log_data
3325 
3326             # Prep to read the next file in the chain
3327             bytes_to_read -= len(read_log_data)
3328             path = console_log + "." + str(i)
3329             i += 1
3330 
3331             if remaining > 0:
3332                 LOG.info('Truncated console log returned, '
3333                          '%d bytes ignored', remaining, instance=instance)
3334         return log_data
3335 
3336     def get_console_output(self, context, instance):
3337         guest = self._host.get_guest(instance)
3338 
3339         xml = guest.get_xml_desc()
3340         tree = etree.fromstring(xml)
3341 
3342         # check for different types of consoles
3343         path_sources = [
3344             ('file', "./devices/console[@type='file']/source[@path]", 'path'),
3345             ('tcp', "./devices/console[@type='tcp']/log[@file]", 'file'),
3346             ('pty', "./devices/console[@type='pty']/source[@path]", 'path')]
3347         console_type = ""
3348         console_path = ""
3349         for c_type, epath, attrib in path_sources:
3350             node = tree.find(epath)
3351             if (node is not None) and node.get(attrib):
3352                 console_type = c_type
3353                 console_path = node.get(attrib)
3354                 break
3355 
3356         # instance has no console at all
3357         if not console_path:
3358             raise exception.ConsoleNotAvailable()
3359 
3360         # instance has a console, but file doesn't exist (yet?)
3361         if not os.path.exists(console_path):
3362             LOG.info('console logfile for instance does not exist',
3363                       instance=instance)
3364             return ""
3365 
3366         # pty consoles need special handling
3367         if console_type == 'pty':
3368             console_log = self._get_console_log_path(instance)
3369             data = nova.privsep.libvirt.readpty(console_path)
3370 
3371             # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3372             # which create a dedicated file device for the console logging.
3373             # Other virt_types like xen, lxc, uml, parallels depend on the
3374             # flush of that pty device into the "console.log" file to ensure
3375             # that a series of "get_console_output" calls return the complete
3376             # content even after rebooting a guest.
3377             nova.privsep.path.writefile(console_log, 'a+', data)
3378 
3379             # set console path to logfile, not to pty device
3380             console_path = console_log
3381 
3382         # return logfile content
3383         return self._get_console_output_file(instance, console_path)
3384 
3385     def get_host_ip_addr(self):
3386         return CONF.my_ip
3387 
3388     def get_vnc_console(self, context, instance):
3389         def get_vnc_port_for_instance(instance_name):
3390             guest = self._host.get_guest(instance)
3391 
3392             xml = guest.get_xml_desc()
3393             xml_dom = etree.fromstring(xml)
3394 
3395             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3396             if graphic is not None:
3397                 return graphic.get('port')
3398             # NOTE(rmk): We had VNC consoles enabled but the instance in
3399             # question is not actually listening for connections.
3400             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3401 
3402         port = get_vnc_port_for_instance(instance.name)
3403         host = CONF.vnc.server_proxyclient_address
3404 
3405         return ctype.ConsoleVNC(host=host, port=port)
3406 
3407     def get_spice_console(self, context, instance):
3408         def get_spice_ports_for_instance(instance_name):
3409             guest = self._host.get_guest(instance)
3410 
3411             xml = guest.get_xml_desc()
3412             xml_dom = etree.fromstring(xml)
3413 
3414             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3415             if graphic is not None:
3416                 return (graphic.get('port'), graphic.get('tlsPort'))
3417             # NOTE(rmk): We had Spice consoles enabled but the instance in
3418             # question is not actually listening for connections.
3419             raise exception.ConsoleTypeUnavailable(console_type='spice')
3420 
3421         ports = get_spice_ports_for_instance(instance.name)
3422         host = CONF.spice.server_proxyclient_address
3423 
3424         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3425 
3426     def get_serial_console(self, context, instance):
3427         guest = self._host.get_guest(instance)
3428         for hostname, port in self._get_serial_ports_from_guest(
3429                 guest, mode='bind'):
3430             return ctype.ConsoleSerial(host=hostname, port=port)
3431         raise exception.ConsoleTypeUnavailable(console_type='serial')
3432 
3433     @staticmethod
3434     def _create_ephemeral(target, ephemeral_size,
3435                           fs_label, os_type, is_block_dev=False,
3436                           context=None, specified_fs=None,
3437                           vm_mode=None):
3438         if not is_block_dev:
3439             if (CONF.libvirt.virt_type == "parallels" and
3440                     vm_mode == fields.VMMode.EXE):
3441 
3442                 libvirt_utils.create_ploop_image('expanded', target,
3443                                                  '%dG' % ephemeral_size,
3444                                                  specified_fs)
3445                 return
3446             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3447 
3448         # Run as root only for block devices.
3449         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3450                       specified_fs=specified_fs)
3451 
3452     @staticmethod
3453     def _create_swap(target, swap_mb, context=None):
3454         """Create a swap file of specified size."""
3455         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3456         nova.privsep.fs.unprivileged_mkfs('swap', target)
3457 
3458     @staticmethod
3459     def _get_console_log_path(instance):
3460         return os.path.join(libvirt_utils.get_instance_path(instance),
3461                             'console.log')
3462 
3463     def _ensure_console_log_for_instance(self, instance):
3464         # NOTE(mdbooth): Although libvirt will create this file for us
3465         # automatically when it starts, it will initially create it with
3466         # root ownership and then chown it depending on the configuration of
3467         # the domain it is launching. Quobyte CI explicitly disables the
3468         # chown by setting dynamic_ownership=0 in libvirt's config.
3469         # Consequently when the domain starts it is unable to write to its
3470         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3471         #
3472         # To work around this, we create the file manually before starting
3473         # the domain so it has the same ownership as Nova. This works
3474         # for Quobyte CI because it is also configured to run qemu as the same
3475         # user as the Nova service. Installations which don't set
3476         # dynamic_ownership=0 are not affected because libvirt will always
3477         # correctly configure permissions regardless of initial ownership.
3478         #
3479         # Setting dynamic_ownership=0 is dubious and potentially broken in
3480         # more ways than console.log (see comment #22 on the above bug), so
3481         # Future Maintainer who finds this code problematic should check to see
3482         # if we still support it.
3483         console_file = self._get_console_log_path(instance)
3484         LOG.debug('Ensure instance console log exists: %s', console_file,
3485                   instance=instance)
3486         try:
3487             libvirt_utils.file_open(console_file, 'a').close()
3488         # NOTE(sfinucan): We can safely ignore permission issues here and
3489         # assume that it is libvirt that has taken ownership of this file.
3490         except IOError as ex:
3491             if ex.errno != errno.EACCES:
3492                 raise
3493             LOG.debug('Console file already exists: %s.', console_file)
3494 
3495     @staticmethod
3496     def _get_disk_config_image_type():
3497         # TODO(mikal): there is a bug here if images_type has
3498         # changed since creation of the instance, but I am pretty
3499         # sure that this bug already exists.
3500         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3501 
3502     @staticmethod
3503     def _is_booted_from_volume(block_device_info):
3504         """Determines whether the VM is booting from volume
3505 
3506         Determines whether the block device info indicates that the VM
3507         is booting from a volume.
3508         """
3509         block_device_mapping = driver.block_device_info_get_mapping(
3510             block_device_info)
3511         return bool(block_device.get_root_bdm(block_device_mapping))
3512 
3513     def _inject_data(self, disk, instance, injection_info):
3514         """Injects data in a disk image
3515 
3516         Helper used for injecting data in a disk image file system.
3517 
3518         :param disk: The disk we're injecting into (an Image object)
3519         :param instance: The instance we're injecting into
3520         :param injection_info: Injection info
3521         """
3522         # Handles the partition need to be used.
3523         LOG.debug('Checking root disk injection %s',
3524                   str(injection_info), instance=instance)
3525         target_partition = None
3526         if not instance.kernel_id:
3527             target_partition = CONF.libvirt.inject_partition
3528             if target_partition == 0:
3529                 target_partition = None
3530         if CONF.libvirt.virt_type == 'lxc':
3531             target_partition = None
3532 
3533         # Handles the key injection.
3534         if CONF.libvirt.inject_key and instance.get('key_data'):
3535             key = str(instance.key_data)
3536         else:
3537             key = None
3538 
3539         # Handles the admin password injection.
3540         if not CONF.libvirt.inject_password:
3541             admin_pass = None
3542         else:
3543             admin_pass = injection_info.admin_pass
3544 
3545         # Handles the network injection.
3546         net = netutils.get_injected_network_template(
3547             injection_info.network_info,
3548             libvirt_virt_type=CONF.libvirt.virt_type)
3549 
3550         # Handles the metadata injection
3551         metadata = instance.get('metadata')
3552 
3553         if any((key, net, metadata, admin_pass, injection_info.files)):
3554             LOG.debug('Injecting %s', str(injection_info),
3555                       instance=instance)
3556             img_id = instance.image_ref
3557             try:
3558                 disk_api.inject_data(disk.get_model(self._conn),
3559                                      key, net, metadata, admin_pass,
3560                                      injection_info.files,
3561                                      partition=target_partition,
3562                                      mandatory=('files',))
3563             except Exception as e:
3564                 with excutils.save_and_reraise_exception():
3565                     LOG.error('Error injecting data into image '
3566                               '%(img_id)s (%(e)s)',
3567                               {'img_id': img_id, 'e': e},
3568                               instance=instance)
3569 
3570     # NOTE(sileht): many callers of this method assume that this
3571     # method doesn't fail if an image already exists but instead
3572     # think that it will be reused (ie: (live)-migration/resize)
3573     def _create_image(self, context, instance,
3574                       disk_mapping, injection_info=None, suffix='',
3575                       disk_images=None, block_device_info=None,
3576                       fallback_from_host=None,
3577                       ignore_bdi_for_swap=False):
3578         booted_from_volume = self._is_booted_from_volume(block_device_info)
3579 
3580         def image(fname, image_type=CONF.libvirt.images_type):
3581             return self.image_backend.by_name(instance,
3582                                               fname + suffix, image_type)
3583 
3584         def raw(fname):
3585             return image(fname, image_type='raw')
3586 
3587         # ensure directories exist and are writable
3588         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3589 
3590         LOG.info('Creating image', instance=instance)
3591 
3592         inst_type = instance.get_flavor()
3593         swap_mb = 0
3594         if 'disk.swap' in disk_mapping:
3595             mapping = disk_mapping['disk.swap']
3596 
3597             if ignore_bdi_for_swap:
3598                 # This is a workaround to support legacy swap resizing,
3599                 # which does not touch swap size specified in bdm,
3600                 # but works with flavor specified size only.
3601                 # In this case we follow the legacy logic and ignore block
3602                 # device info completely.
3603                 # NOTE(ft): This workaround must be removed when a correct
3604                 # implementation of resize operation changing sizes in bdms is
3605                 # developed. Also at that stage we probably may get rid of
3606                 # the direct usage of flavor swap size here,
3607                 # leaving the work with bdm only.
3608                 swap_mb = inst_type['swap']
3609             else:
3610                 swap = driver.block_device_info_get_swap(block_device_info)
3611                 if driver.swap_is_usable(swap):
3612                     swap_mb = swap['swap_size']
3613                 elif (inst_type['swap'] > 0 and
3614                       not block_device.volume_in_mapping(
3615                         mapping['dev'], block_device_info)):
3616                     swap_mb = inst_type['swap']
3617 
3618             if swap_mb > 0:
3619                 if (CONF.libvirt.virt_type == "parallels" and
3620                         instance.vm_mode == fields.VMMode.EXE):
3621                     msg = _("Swap disk is not supported "
3622                             "for Virtuozzo container")
3623                     raise exception.Invalid(msg)
3624 
3625         if not disk_images:
3626             disk_images = {'image_id': instance.image_ref,
3627                            'kernel_id': instance.kernel_id,
3628                            'ramdisk_id': instance.ramdisk_id}
3629 
3630         if disk_images['kernel_id']:
3631             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3632             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3633                                 context=context,
3634                                 filename=fname,
3635                                 image_id=disk_images['kernel_id'])
3636             if disk_images['ramdisk_id']:
3637                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3638                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3639                                      context=context,
3640                                      filename=fname,
3641                                      image_id=disk_images['ramdisk_id'])
3642 
3643         if CONF.libvirt.virt_type == 'uml':
3644             # PONDERING(mikal): can I assume that root is UID zero in every
3645             # OS? Probably not.
3646             uid = pwd.getpwnam('root').pw_uid
3647             nova.privsep.path.chown(image('disk').path, uid=uid)
3648 
3649         self._create_and_inject_local_root(context, instance,
3650                                            booted_from_volume, suffix,
3651                                            disk_images, injection_info,
3652                                            fallback_from_host)
3653 
3654         # Lookup the filesystem type if required
3655         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
3656             instance.os_type)
3657         # Generate a file extension based on the file system
3658         # type and the mkfs commands configured if any
3659         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
3660             os_type_with_default, CONF.default_ephemeral_format)
3661 
3662         vm_mode = fields.VMMode.get_from_instance(instance)
3663         ephemeral_gb = instance.flavor.ephemeral_gb
3664         if 'disk.local' in disk_mapping:
3665             disk_image = image('disk.local')
3666             fn = functools.partial(self._create_ephemeral,
3667                                    fs_label='ephemeral0',
3668                                    os_type=instance.os_type,
3669                                    is_block_dev=disk_image.is_block_dev,
3670                                    vm_mode=vm_mode)
3671             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3672             size = ephemeral_gb * units.Gi
3673             disk_image.cache(fetch_func=fn,
3674                              context=context,
3675                              filename=fname,
3676                              size=size,
3677                              ephemeral_size=ephemeral_gb)
3678 
3679         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3680                 block_device_info)):
3681             disk_image = image(blockinfo.get_eph_disk(idx))
3682 
3683             specified_fs = eph.get('guest_format')
3684             if specified_fs and not self.is_supported_fs_format(specified_fs):
3685                 msg = _("%s format is not supported") % specified_fs
3686                 raise exception.InvalidBDMFormat(details=msg)
3687 
3688             fn = functools.partial(self._create_ephemeral,
3689                                    fs_label='ephemeral%d' % idx,
3690                                    os_type=instance.os_type,
3691                                    is_block_dev=disk_image.is_block_dev,
3692                                    vm_mode=vm_mode)
3693             size = eph['size'] * units.Gi
3694             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3695             disk_image.cache(fetch_func=fn,
3696                              context=context,
3697                              filename=fname,
3698                              size=size,
3699                              ephemeral_size=eph['size'],
3700                              specified_fs=specified_fs)
3701 
3702         if swap_mb > 0:
3703             size = swap_mb * units.Mi
3704             image('disk.swap').cache(fetch_func=self._create_swap,
3705                                      context=context,
3706                                      filename="swap_%s" % swap_mb,
3707                                      size=size,
3708                                      swap_mb=swap_mb)
3709 
3710     def _create_and_inject_local_root(self, context, instance,
3711                                       booted_from_volume, suffix, disk_images,
3712                                       injection_info, fallback_from_host):
3713         # File injection only if needed
3714         need_inject = (not configdrive.required_by(instance) and
3715                        injection_info is not None and
3716                        CONF.libvirt.inject_partition != -2)
3717 
3718         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3719         # currently happens only on rescue - we still don't want to
3720         # create a base image.
3721         if not booted_from_volume:
3722             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3723             size = instance.flavor.root_gb * units.Gi
3724 
3725             if size == 0 or suffix == '.rescue':
3726                 size = None
3727 
3728             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3729                                                  CONF.libvirt.images_type)
3730             if instance.task_state == task_states.RESIZE_FINISH:
3731                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3732             if backend.SUPPORTS_CLONE:
3733                 def clone_fallback_to_fetch(*args, **kwargs):
3734                     try:
3735                         backend.clone(context, disk_images['image_id'])
3736                     except exception.ImageUnacceptable:
3737                         libvirt_utils.fetch_image(*args, **kwargs)
3738                 fetch_func = clone_fallback_to_fetch
3739             else:
3740                 fetch_func = libvirt_utils.fetch_image
3741             self._try_fetch_image_cache(backend, fetch_func, context,
3742                                         root_fname, disk_images['image_id'],
3743                                         instance, size, fallback_from_host)
3744 
3745             if need_inject:
3746                 self._inject_data(backend, instance, injection_info)
3747 
3748         elif need_inject:
3749             LOG.warning('File injection into a boot from volume '
3750                         'instance is not supported', instance=instance)
3751 
3752     def _create_configdrive(self, context, instance, injection_info,
3753                             rescue=False):
3754         # As this method being called right after the definition of a
3755         # domain, but before its actual launch, device metadata will be built
3756         # and saved in the instance for it to be used by the config drive and
3757         # the metadata service.
3758         instance.device_metadata = self._build_device_metadata(context,
3759                                                                instance)
3760         if configdrive.required_by(instance):
3761             LOG.info('Using config drive', instance=instance)
3762 
3763             name = 'disk.config'
3764             if rescue:
3765                 name += '.rescue'
3766 
3767             config_disk = self.image_backend.by_name(
3768                 instance, name, self._get_disk_config_image_type())
3769 
3770             # Don't overwrite an existing config drive
3771             if not config_disk.exists():
3772                 extra_md = {}
3773                 if injection_info.admin_pass:
3774                     extra_md['admin_pass'] = injection_info.admin_pass
3775 
3776                 inst_md = instance_metadata.InstanceMetadata(
3777                     instance, content=injection_info.files, extra_md=extra_md,
3778                     network_info=injection_info.network_info,
3779                     request_context=context)
3780 
3781                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3782                 with cdb:
3783                     # NOTE(mdbooth): We're hardcoding here the path of the
3784                     # config disk when using the flat backend. This isn't
3785                     # good, but it's required because we need a local path we
3786                     # know we can write to in case we're subsequently
3787                     # importing into rbd. This will be cleaned up when we
3788                     # replace this with a call to create_from_func, but that
3789                     # can't happen until we've updated the backends and we
3790                     # teach them not to cache config disks. This isn't
3791                     # possible while we're still using cache() under the hood.
3792                     config_disk_local_path = os.path.join(
3793                         libvirt_utils.get_instance_path(instance), name)
3794                     LOG.info('Creating config drive at %(path)s',
3795                              {'path': config_disk_local_path},
3796                              instance=instance)
3797 
3798                     try:
3799                         cdb.make_drive(config_disk_local_path)
3800                     except processutils.ProcessExecutionError as e:
3801                         with excutils.save_and_reraise_exception():
3802                             LOG.error('Creating config drive failed with '
3803                                       'error: %s', e, instance=instance)
3804 
3805                 try:
3806                     config_disk.import_file(
3807                         instance, config_disk_local_path, name)
3808                 finally:
3809                     # NOTE(mikal): if the config drive was imported into RBD,
3810                     # then we no longer need the local copy
3811                     if CONF.libvirt.images_type == 'rbd':
3812                         LOG.info('Deleting local config drive %(path)s '
3813                                  'because it was imported into RBD.',
3814                                  {'path': config_disk_local_path},
3815                                  instance=instance)
3816                         os.unlink(config_disk_local_path)
3817 
3818     def _prepare_pci_devices_for_use(self, pci_devices):
3819         # kvm , qemu support managed mode
3820         # In managed mode, the configured device will be automatically
3821         # detached from the host OS drivers when the guest is started,
3822         # and then re-attached when the guest shuts down.
3823         if CONF.libvirt.virt_type != 'xen':
3824             # we do manual detach only for xen
3825             return
3826         try:
3827             for dev in pci_devices:
3828                 libvirt_dev_addr = dev['hypervisor_name']
3829                 libvirt_dev = \
3830                         self._host.device_lookup_by_name(libvirt_dev_addr)
3831                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3832                 # http://libvirt.org/html/libvirt-libvirt.html.
3833                 libvirt_dev.dettach()
3834 
3835             # Note(yjiang5): A reset of one PCI device may impact other
3836             # devices on the same bus, thus we need two separated loops
3837             # to detach and then reset it.
3838             for dev in pci_devices:
3839                 libvirt_dev_addr = dev['hypervisor_name']
3840                 libvirt_dev = \
3841                         self._host.device_lookup_by_name(libvirt_dev_addr)
3842                 libvirt_dev.reset()
3843 
3844         except libvirt.libvirtError as exc:
3845             raise exception.PciDevicePrepareFailed(id=dev['id'],
3846                                                    instance_uuid=
3847                                                    dev['instance_uuid'],
3848                                                    reason=six.text_type(exc))
3849 
3850     def _detach_pci_devices(self, guest, pci_devs):
3851         try:
3852             for dev in pci_devs:
3853                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3854                 # after detachDeviceFlags returned, we should check the dom to
3855                 # ensure the detaching is finished
3856                 xml = guest.get_xml_desc()
3857                 xml_doc = etree.fromstring(xml)
3858                 guest_config = vconfig.LibvirtConfigGuest()
3859                 guest_config.parse_dom(xml_doc)
3860 
3861                 for hdev in [d for d in guest_config.devices
3862                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3863                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3864                     dbsf = pci_utils.parse_address(dev.address)
3865                     if [int(x, 16) for x in hdbsf] ==\
3866                             [int(x, 16) for x in dbsf]:
3867                         raise exception.PciDeviceDetachFailed(reason=
3868                                                               "timeout",
3869                                                               dev=dev)
3870 
3871         except libvirt.libvirtError as ex:
3872             error_code = ex.get_error_code()
3873             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3874                 LOG.warning("Instance disappeared while detaching "
3875                             "a PCI device from it.")
3876             else:
3877                 raise
3878 
3879     def _attach_pci_devices(self, guest, pci_devs):
3880         try:
3881             for dev in pci_devs:
3882                 guest.attach_device(self._get_guest_pci_device(dev))
3883 
3884         except libvirt.libvirtError:
3885             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3886                       {'dev': pci_devs, 'dom': guest.id})
3887             raise
3888 
3889     @staticmethod
3890     def _has_direct_passthrough_port(network_info):
3891         for vif in network_info:
3892             if (vif['vnic_type'] in
3893                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3894                 return True
3895         return False
3896 
3897     def _attach_direct_passthrough_ports(
3898         self, context, instance, guest, network_info=None):
3899         if network_info is None:
3900             network_info = instance.info_cache.network_info
3901         if network_info is None:
3902             return
3903 
3904         if self._has_direct_passthrough_port(network_info):
3905             for vif in network_info:
3906                 if (vif['vnic_type'] in
3907                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3908                     cfg = self.vif_driver.get_config(instance,
3909                                                      vif,
3910                                                      instance.image_meta,
3911                                                      instance.flavor,
3912                                                      CONF.libvirt.virt_type,
3913                                                      self._host)
3914                     LOG.debug('Attaching direct passthrough port %(port)s '
3915                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3916                               instance=instance)
3917                     guest.attach_device(cfg)
3918 
3919     def _detach_direct_passthrough_ports(self, context, instance, guest):
3920         network_info = instance.info_cache.network_info
3921         if network_info is None:
3922             return
3923 
3924         if self._has_direct_passthrough_port(network_info):
3925             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3926             # pci request per direct passthrough port. Therefore we can trust
3927             # that pci_slot value in the vif is correct.
3928             direct_passthrough_pci_addresses = [
3929                 vif['profile']['pci_slot']
3930                 for vif in network_info
3931                 if (vif['vnic_type'] in
3932                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3933                     vif['profile'].get('pci_slot') is not None)
3934             ]
3935 
3936             # use detach_pci_devices to avoid failure in case of
3937             # multiple guest direct passthrough ports with the same MAC
3938             # (protection use-case, ports are on different physical
3939             # interfaces)
3940             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3941             direct_passthrough_pci_addresses = (
3942                 [pci_dev for pci_dev in pci_devs
3943                  if pci_dev.address in direct_passthrough_pci_addresses])
3944             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3945 
3946     def _update_compute_provider_status(self, context, service):
3947         """Calls the ComputeVirtAPI.update_compute_provider_status method
3948 
3949         :param context: nova auth RequestContext
3950         :param service: nova.objects.Service record for this host which is
3951             expected to only manage a single ComputeNode
3952         """
3953         rp_uuid = None
3954         try:
3955             rp_uuid = service.compute_node.uuid
3956             self.virtapi.update_compute_provider_status(
3957                 context, rp_uuid, enabled=not service.disabled)
3958         except Exception:
3959             # This is best effort so just log the exception but don't fail.
3960             # The update_available_resource periodic task will sync the trait.
3961             LOG.warning(
3962                 'An error occurred while updating compute node '
3963                 'resource provider status to "%s" for provider: %s',
3964                 'disabled' if service.disabled else 'enabled',
3965                 rp_uuid or service.host, exc_info=True)
3966 
3967     def _set_host_enabled(self, enabled,
3968                           disable_reason=DISABLE_REASON_UNDEFINED):
3969         """Enables / Disables the compute service on this host.
3970 
3971            This doesn't override non-automatic disablement with an automatic
3972            setting; thereby permitting operators to keep otherwise
3973            healthy hosts out of rotation.
3974         """
3975 
3976         status_name = {True: 'disabled',
3977                        False: 'enabled'}
3978 
3979         disable_service = not enabled
3980 
3981         ctx = nova_context.get_admin_context()
3982         try:
3983             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3984 
3985             if service.disabled != disable_service:
3986                 # Note(jang): this is a quick fix to stop operator-
3987                 # disabled compute hosts from re-enabling themselves
3988                 # automatically. We prefix any automatic reason code
3989                 # with a fixed string. We only re-enable a host
3990                 # automatically if we find that string in place.
3991                 # This should probably be replaced with a separate flag.
3992                 if not service.disabled or (
3993                         service.disabled_reason and
3994                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3995                     service.disabled = disable_service
3996                     service.disabled_reason = (
3997                        DISABLE_PREFIX + disable_reason
3998                        if disable_service and disable_reason else
3999                            DISABLE_REASON_UNDEFINED)
4000                     service.save()
4001                     LOG.debug('Updating compute service status to %s',
4002                               status_name[disable_service])
4003                     # Update the disabled trait status on the corresponding
4004                     # compute node resource provider in placement.
4005                     self._update_compute_provider_status(ctx, service)
4006                 else:
4007                     LOG.debug('Not overriding manual compute service '
4008                               'status with: %s',
4009                               status_name[disable_service])
4010         except exception.ComputeHostNotFound:
4011             LOG.warning('Cannot update service status on host "%s" '
4012                         'since it is not registered.', CONF.host)
4013         except Exception:
4014             LOG.warning('Cannot update service status on host "%s" '
4015                         'due to an unexpected exception.', CONF.host,
4016                         exc_info=True)
4017 
4018         if enabled:
4019             mount.get_manager().host_up(self._host)
4020         else:
4021             mount.get_manager().host_down()
4022 
4023     def _get_guest_cpu_model_config(self):
4024         mode = CONF.libvirt.cpu_mode
4025         model = CONF.libvirt.cpu_model
4026         extra_flags = set([flag.lower() for flag in
4027             CONF.libvirt.cpu_model_extra_flags])
4028 
4029         if (CONF.libvirt.virt_type == "kvm" or
4030             CONF.libvirt.virt_type == "qemu"):
4031             if mode is None:
4032                 caps = self._host.get_capabilities()
4033                 # AArch64 lacks 'host-model' support because neither libvirt
4034                 # nor QEMU are able to tell what the host CPU model exactly is.
4035                 # And there is no CPU description code for ARM(64) at this
4036                 # point.
4037 
4038                 # Also worth noting: 'host-passthrough' mode will completely
4039                 # break live migration, *unless* all the Compute nodes (running
4040                 # libvirtd) have *identical* CPUs.
4041                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
4042                     mode = "host-passthrough"
4043                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
4044                              'migration can break unless all compute nodes '
4045                              'have identical cpus. AArch64 does not support '
4046                              'other modes.')
4047                 else:
4048                     mode = "host-model"
4049             if mode == "none":
4050                 return vconfig.LibvirtConfigGuestCPU()
4051         else:
4052             if mode is None or mode == "none":
4053                 return None
4054 
4055         if ((CONF.libvirt.virt_type != "kvm" and
4056              CONF.libvirt.virt_type != "qemu")):
4057             msg = _("Config requested an explicit CPU model, but "
4058                     "the current libvirt hypervisor '%s' does not "
4059                     "support selecting CPU models") % CONF.libvirt.virt_type
4060             raise exception.Invalid(msg)
4061 
4062         if mode == "custom" and model is None:
4063             msg = _("Config requested a custom CPU model, but no "
4064                     "model name was provided")
4065             raise exception.Invalid(msg)
4066         elif mode != "custom" and model is not None:
4067             msg = _("A CPU model name should not be set when a "
4068                     "host CPU model is requested")
4069             raise exception.Invalid(msg)
4070 
4071         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen, "
4072                   "with extra flags: '%(extra_flags)s'",
4073                   {'mode': mode,
4074                    'model': (model or ""),
4075                    'extra_flags': (extra_flags or "")})
4076 
4077         cpu = vconfig.LibvirtConfigGuestCPU()
4078         cpu.mode = mode
4079         cpu.model = model
4080 
4081         # NOTE (kchamart): Currently there's no existing way to ask if a
4082         # given CPU model + CPU flags combination is supported by KVM &
4083         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
4084         # command upfront -- before even a Nova instance (a QEMU
4085         # process) is launched -- to construct CPU models and check
4086         # their validity; so we are good there.  In the long-term,
4087         # upstream libvirt intends to add an additional new API that can
4088         # do fine-grained validation of a certain CPU model + CPU flags
4089         # against a specific QEMU binary (the libvirt RFE bug for that:
4090         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
4091         for flag in extra_flags:
4092             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
4093 
4094         return cpu
4095 
4096     def _get_guest_cpu_config(self, flavor, image_meta,
4097                               guest_cpu_numa_config, instance_numa_topology):
4098         cpu = self._get_guest_cpu_model_config()
4099 
4100         if cpu is None:
4101             return None
4102 
4103         topology = hardware.get_best_cpu_topology(
4104                 flavor, image_meta, numa_topology=instance_numa_topology)
4105 
4106         cpu.sockets = topology.sockets
4107         cpu.cores = topology.cores
4108         cpu.threads = topology.threads
4109         cpu.numa = guest_cpu_numa_config
4110 
4111         return cpu
4112 
4113     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
4114                                image_type=None):
4115         disk_unit = None
4116         disk = self.image_backend.by_name(instance, name, image_type)
4117         if (name == 'disk.config' and image_type == 'rbd' and
4118                 not disk.exists()):
4119             # This is likely an older config drive that has not been migrated
4120             # to rbd yet. Try to fall back on 'flat' image type.
4121             # TODO(melwitt): Add online migration of some sort so we can
4122             # remove this fall back once we know all config drives are in rbd.
4123             # NOTE(vladikr): make sure that the flat image exist, otherwise
4124             # the image will be created after the domain definition.
4125             flat_disk = self.image_backend.by_name(instance, name, 'flat')
4126             if flat_disk.exists():
4127                 disk = flat_disk
4128                 LOG.debug('Config drive not found in RBD, falling back to the '
4129                           'instance directory', instance=instance)
4130         disk_info = disk_mapping[name]
4131         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
4132             disk_unit = disk_mapping['unit']
4133             disk_mapping['unit'] += 1  # Increments for the next disk added
4134         conf = disk.libvirt_info(disk_info, self.disk_cachemode,
4135                                  inst_type['extra_specs'],
4136                                  self._host.get_version(),
4137                                  disk_unit=disk_unit)
4138         return conf
4139 
4140     def _get_guest_fs_config(self, instance, name, image_type=None):
4141         disk = self.image_backend.by_name(instance, name, image_type)
4142         return disk.libvirt_fs_info("/", "ploop")
4143 
4144     def _get_guest_storage_config(self, context, instance, image_meta,
4145                                   disk_info,
4146                                   rescue, block_device_info,
4147                                   inst_type, os_type):
4148         devices = []
4149         disk_mapping = disk_info['mapping']
4150 
4151         block_device_mapping = driver.block_device_info_get_mapping(
4152             block_device_info)
4153         mount_rootfs = CONF.libvirt.virt_type == "lxc"
4154         scsi_controller = self._get_scsi_controller(image_meta)
4155 
4156         if scsi_controller and scsi_controller.model == 'virtio-scsi':
4157             # The virtio-scsi can handle up to 256 devices but the
4158             # optional element "address" must be defined to describe
4159             # where the device is placed on the controller (see:
4160             # LibvirtConfigGuestDeviceAddressDrive).
4161             #
4162             # Note about why it's added in disk_mapping: It's not
4163             # possible to pass an 'int' by reference in Python, so we
4164             # use disk_mapping as container to keep reference of the
4165             # unit added and be able to increment it for each disk
4166             # added.
4167             #
4168             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
4169             # we need to start the disk mapping unit at 1 since we set the
4170             # bootable volume's unit to 0 for the bootable volume.
4171             disk_mapping['unit'] = 0
4172             if self._is_booted_from_volume(block_device_info):
4173                 disk_mapping['unit'] = 1
4174 
4175         def _get_ephemeral_devices():
4176             eph_devices = []
4177             for idx, eph in enumerate(
4178                 driver.block_device_info_get_ephemerals(
4179                     block_device_info)):
4180                 diskeph = self._get_guest_disk_config(
4181                     instance,
4182                     blockinfo.get_eph_disk(idx),
4183                     disk_mapping, inst_type)
4184                 eph_devices.append(diskeph)
4185             return eph_devices
4186 
4187         if mount_rootfs:
4188             fs = vconfig.LibvirtConfigGuestFilesys()
4189             fs.source_type = "mount"
4190             fs.source_dir = os.path.join(
4191                 libvirt_utils.get_instance_path(instance), 'rootfs')
4192             devices.append(fs)
4193         elif (os_type == fields.VMMode.EXE and
4194               CONF.libvirt.virt_type == "parallels"):
4195             if rescue:
4196                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
4197                 devices.append(fsrescue)
4198 
4199                 fsos = self._get_guest_fs_config(instance, "disk")
4200                 fsos.target_dir = "/mnt/rescue"
4201                 devices.append(fsos)
4202             else:
4203                 if 'disk' in disk_mapping:
4204                     fs = self._get_guest_fs_config(instance, "disk")
4205                     devices.append(fs)
4206                 devices = devices + _get_ephemeral_devices()
4207         else:
4208 
4209             if rescue:
4210                 diskrescue = self._get_guest_disk_config(instance,
4211                                                          'disk.rescue',
4212                                                          disk_mapping,
4213                                                          inst_type)
4214                 devices.append(diskrescue)
4215 
4216                 diskos = self._get_guest_disk_config(instance,
4217                                                      'disk',
4218                                                      disk_mapping,
4219                                                      inst_type)
4220                 devices.append(diskos)
4221             else:
4222                 if 'disk' in disk_mapping:
4223                     diskos = self._get_guest_disk_config(instance,
4224                                                          'disk',
4225                                                          disk_mapping,
4226                                                          inst_type)
4227                     devices.append(diskos)
4228 
4229                 if 'disk.local' in disk_mapping:
4230                     disklocal = self._get_guest_disk_config(instance,
4231                                                             'disk.local',
4232                                                             disk_mapping,
4233                                                             inst_type)
4234                     devices.append(disklocal)
4235                     instance.default_ephemeral_device = (
4236                         block_device.prepend_dev(disklocal.target_dev))
4237 
4238                 devices = devices + _get_ephemeral_devices()
4239 
4240                 if 'disk.swap' in disk_mapping:
4241                     diskswap = self._get_guest_disk_config(instance,
4242                                                            'disk.swap',
4243                                                            disk_mapping,
4244                                                            inst_type)
4245                     devices.append(diskswap)
4246                     instance.default_swap_device = (
4247                         block_device.prepend_dev(diskswap.target_dev))
4248 
4249             config_name = 'disk.config.rescue' if rescue else 'disk.config'
4250             if config_name in disk_mapping:
4251                 diskconfig = self._get_guest_disk_config(
4252                     instance, config_name, disk_mapping, inst_type,
4253                     self._get_disk_config_image_type())
4254                 devices.append(diskconfig)
4255 
4256         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4257                                                    mount_rootfs):
4258             connection_info = vol['connection_info']
4259             vol_dev = block_device.prepend_dev(vol['mount_device'])
4260             info = disk_mapping[vol_dev]
4261             self._connect_volume(context, connection_info, instance)
4262             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4263                 # Check if this is the bootable volume when in a
4264                 # boot-from-volume instance, and if so, ensure the unit
4265                 # attribute is 0.
4266                 if vol.get('boot_index') == 0:
4267                     info['unit'] = 0
4268                 else:
4269                     info['unit'] = disk_mapping['unit']
4270                     disk_mapping['unit'] += 1
4271             cfg = self._get_volume_config(connection_info, info)
4272             devices.append(cfg)
4273             vol['connection_info'] = connection_info
4274             vol.save()
4275 
4276         for d in devices:
4277             self._set_cache_mode(d)
4278 
4279         if scsi_controller:
4280             devices.append(scsi_controller)
4281 
4282         return devices
4283 
4284     @staticmethod
4285     def _get_scsi_controller(image_meta):
4286         """Return scsi controller or None based on image meta"""
4287         if image_meta.properties.get('hw_scsi_model'):
4288             hw_scsi_model = image_meta.properties.hw_scsi_model
4289             scsi_controller = vconfig.LibvirtConfigGuestController()
4290             scsi_controller.type = 'scsi'
4291             scsi_controller.model = hw_scsi_model
4292             scsi_controller.index = 0
4293             return scsi_controller
4294 
4295     def _get_host_sysinfo_serial_hardware(self):
4296         """Get a UUID from the host hardware
4297 
4298         Get a UUID for the host hardware reported by libvirt.
4299         This is typically from the SMBIOS data, unless it has
4300         been overridden in /etc/libvirt/libvirtd.conf
4301         """
4302         caps = self._host.get_capabilities()
4303         return caps.host.uuid
4304 
4305     def _get_host_sysinfo_serial_os(self):
4306         """Get a UUID from the host operating system
4307 
4308         Get a UUID for the host operating system. Modern Linux
4309         distros based on systemd provide a /etc/machine-id
4310         file containing a UUID. This is also provided inside
4311         systemd based containers and can be provided by other
4312         init systems too, since it is just a plain text file.
4313         """
4314         if not os.path.exists("/etc/machine-id"):
4315             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4316             raise exception.InternalError(msg)
4317 
4318         with open("/etc/machine-id") as f:
4319             # We want to have '-' in the right place
4320             # so we parse & reformat the value
4321             lines = f.read().split()
4322             if not lines:
4323                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4324                 raise exception.InternalError(msg)
4325 
4326             return str(uuid.UUID(lines[0]))
4327 
4328     def _get_host_sysinfo_serial_auto(self):
4329         if os.path.exists("/etc/machine-id"):
4330             return self._get_host_sysinfo_serial_os()
4331         else:
4332             return self._get_host_sysinfo_serial_hardware()
4333 
4334     def _get_guest_config_sysinfo(self, instance):
4335         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4336 
4337         sysinfo.system_manufacturer = version.vendor_string()
4338         sysinfo.system_product = version.product_string()
4339         sysinfo.system_version = version.version_string_with_package()
4340 
4341         if CONF.libvirt.sysinfo_serial == 'unique':
4342             sysinfo.system_serial = instance.uuid
4343         else:
4344             sysinfo.system_serial = self._sysinfo_serial_func()
4345         sysinfo.system_uuid = instance.uuid
4346 
4347         sysinfo.system_family = "Virtual Machine"
4348 
4349         return sysinfo
4350 
4351     def _get_guest_pci_device(self, pci_device):
4352 
4353         dbsf = pci_utils.parse_address(pci_device.address)
4354         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4355         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4356 
4357         # only kvm support managed mode
4358         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4359             dev.managed = 'no'
4360         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4361             dev.managed = 'yes'
4362 
4363         return dev
4364 
4365     def _get_guest_config_meta(self, instance):
4366         """Get metadata config for guest."""
4367 
4368         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4369         meta.package = version.version_string_with_package()
4370         meta.name = instance.display_name
4371         meta.creationTime = time.time()
4372 
4373         if instance.image_ref not in ("", None):
4374             meta.roottype = "image"
4375             meta.rootid = instance.image_ref
4376 
4377         system_meta = instance.system_metadata
4378         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4379         ometa.userid = instance.user_id
4380         ometa.username = system_meta.get('owner_user_name', 'N/A')
4381         ometa.projectid = instance.project_id
4382         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4383         meta.owner = ometa
4384 
4385         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4386         flavor = instance.flavor
4387         fmeta.name = flavor.name
4388         fmeta.memory = flavor.memory_mb
4389         fmeta.vcpus = flavor.vcpus
4390         fmeta.ephemeral = flavor.ephemeral_gb
4391         fmeta.disk = flavor.root_gb
4392         fmeta.swap = flavor.swap
4393 
4394         meta.flavor = fmeta
4395 
4396         return meta
4397 
4398     @staticmethod
4399     def _create_idmaps(klass, map_strings):
4400         idmaps = []
4401         if len(map_strings) > 5:
4402             map_strings = map_strings[0:5]
4403             LOG.warning("Too many id maps, only included first five.")
4404         for map_string in map_strings:
4405             try:
4406                 idmap = klass()
4407                 values = [int(i) for i in map_string.split(":")]
4408                 idmap.start = values[0]
4409                 idmap.target = values[1]
4410                 idmap.count = values[2]
4411                 idmaps.append(idmap)
4412             except (ValueError, IndexError):
4413                 LOG.warning("Invalid value for id mapping %s", map_string)
4414         return idmaps
4415 
4416     def _get_guest_idmaps(self):
4417         id_maps = []
4418         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4419             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4420                                            CONF.libvirt.uid_maps)
4421             id_maps.extend(uid_maps)
4422         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4423             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4424                                            CONF.libvirt.gid_maps)
4425             id_maps.extend(gid_maps)
4426         return id_maps
4427 
4428     def _update_guest_cputune(self, guest, flavor, virt_type):
4429         is_able = self._host.is_cpu_control_policy_capable()
4430 
4431         cputuning = ['shares', 'period', 'quota']
4432         wants_cputune = any([k for k in cputuning
4433             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4434 
4435         if wants_cputune and not is_able:
4436             raise exception.UnsupportedHostCPUControlPolicy()
4437 
4438         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4439             return
4440 
4441         if guest.cputune is None:
4442             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4443             # Setting the default cpu.shares value to be a value
4444             # dependent on the number of vcpus
4445         guest.cputune.shares = 1024 * guest.vcpus
4446 
4447         for name in cputuning:
4448             key = "quota:cpu_" + name
4449             if key in flavor.extra_specs:
4450                 setattr(guest.cputune, name,
4451                         int(flavor.extra_specs[key]))
4452 
4453     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4454                                            wants_hugepages):
4455         if instance_numa_topology:
4456             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4457             for instance_cell in instance_numa_topology.cells:
4458                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4459                 guest_cell.id = instance_cell.id
4460                 guest_cell.cpus = instance_cell.cpuset
4461                 guest_cell.memory = instance_cell.memory * units.Ki
4462 
4463                 # The vhost-user network backend requires file backed
4464                 # guest memory (ie huge pages) to be marked as shared
4465                 # access, not private, so an external process can read
4466                 # and write the pages.
4467                 #
4468                 # You can't change the shared vs private flag for an
4469                 # already running guest, and since we can't predict what
4470                 # types of NIC may be hotplugged, we have no choice but
4471                 # to unconditionally turn on the shared flag. This has
4472                 # no real negative functional effect on the guest, so
4473                 # is a reasonable approach to take
4474                 if wants_hugepages:
4475                     guest_cell.memAccess = "shared"
4476                 guest_cpu_numa.cells.append(guest_cell)
4477             return guest_cpu_numa
4478 
4479     def _wants_hugepages(self, host_topology, instance_topology):
4480         """Determine if the guest / host topology implies the
4481            use of huge pages for guest RAM backing
4482         """
4483 
4484         if host_topology is None or instance_topology is None:
4485             return False
4486 
4487         avail_pagesize = [page.size_kb
4488                           for page in host_topology.cells[0].mempages]
4489         avail_pagesize.sort()
4490         # Remove smallest page size as that's not classed as a largepage
4491         avail_pagesize = avail_pagesize[1:]
4492 
4493         # See if we have page size set
4494         for cell in instance_topology.cells:
4495             if (cell.pagesize is not None and
4496                 cell.pagesize in avail_pagesize):
4497                 return True
4498 
4499         return False
4500 
4501     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4502         """Returns the lists of pairs(tuple) of an instance cell and
4503         corresponding host cell:
4504             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4505         """
4506         cell_pairs = []
4507         for guest_config_cell in guest_cpu_numa_config.cells:
4508             for host_cell in host_topology.cells:
4509                 if guest_config_cell.id == host_cell.id:
4510                     cell_pairs.append((guest_config_cell, host_cell))
4511         return cell_pairs
4512 
4513     def _get_pin_cpuset(self, vcpu, object_numa_cell, host_cell):
4514         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
4515         Prepares vcpupin config for the guest with the following caveats:
4516 
4517             a) If there is pinning information in the cell, we pin vcpus to
4518                individual CPUs
4519             b) Otherwise we float over the whole host NUMA node
4520         """
4521         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
4522         pin_cpuset.id = vcpu
4523 
4524         if object_numa_cell.cpu_pinning:
4525             pin_cpuset.cpuset = set([object_numa_cell.cpu_pinning[vcpu]])
4526         else:
4527             pin_cpuset.cpuset = host_cell.cpuset
4528 
4529         return pin_cpuset
4530 
4531     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
4532                                 emulator_threads_policy, wants_realtime,
4533                                 pin_cpuset):
4534         """Returns a set of cpu_ids to add to the cpuset for emulator threads
4535            with the following caveats:
4536 
4537             a) If emulator threads policy is isolated, we pin emulator threads
4538                to one cpu we have reserved for it.
4539             b) If emulator threads policy is shared and CONF.cpu_shared_set is
4540                defined, we pin emulator threads on the set of pCPUs defined by
4541                CONF.cpu_shared_set
4542             c) Otherwise;
4543                 c1) If realtime IS NOT enabled, the emulator threads are
4544                     allowed to float cross all the pCPUs associated with
4545                     the guest vCPUs.
4546                 c2) If realtime IS enabled, at least 1 vCPU is required
4547                     to be set aside for non-realtime usage. The emulator
4548                     threads are allowed to float across the pCPUs that
4549                     are associated with the non-realtime VCPUs.
4550         """
4551         emulatorpin_cpuset = set([])
4552         shared_ids = hardware.get_cpu_shared_set()
4553 
4554         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
4555             if object_numa_cell.cpuset_reserved:
4556                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
4557         elif ((emulator_threads_policy ==
4558               fields.CPUEmulatorThreadsPolicy.SHARE) and
4559               shared_ids):
4560             online_pcpus = self._host.get_online_cpus()
4561             cpuset = shared_ids & online_pcpus
4562             if not cpuset:
4563                 msg = (_("Invalid cpu_shared_set config, one or more of the "
4564                          "specified cpuset is not online. Online cpuset(s): "
4565                          "%(online)s, requested cpuset(s): %(req)s"),
4566                        {'online': sorted(online_pcpus),
4567                         'req': sorted(shared_ids)})
4568                 raise exception.Invalid(msg)
4569             emulatorpin_cpuset = cpuset
4570         elif not wants_realtime or vcpu not in vcpus_rt:
4571             emulatorpin_cpuset = pin_cpuset.cpuset
4572 
4573         return emulatorpin_cpuset
4574 
4575     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4576                                image_meta):
4577         """Returns the config objects for the guest NUMA specs.
4578 
4579         Determines the CPUs that the guest can be pinned to if the guest
4580         specifies a cell topology and the host supports it. Constructs the
4581         libvirt XML config object representing the NUMA topology selected
4582         for the guest. Returns a tuple of:
4583 
4584             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4585 
4586         With the following caveats:
4587 
4588             a) If there is no specified guest NUMA topology, then
4589                all tuple elements except cpu_set shall be None. cpu_set
4590                will be populated with the chosen CPUs that the guest
4591                allowed CPUs fit within.
4592 
4593             b) If there is a specified guest NUMA topology, then
4594                cpu_set will be None and guest_cpu_numa will be the
4595                LibvirtConfigGuestCPUNUMA object representing the guest's
4596                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4597                will contain a LibvirtConfigGuestCPUTune object representing
4598                the optimized chosen cells that match the host capabilities
4599                with the instance's requested topology. If the host does
4600                not support NUMA, then guest_cpu_tune and guest_numa_tune
4601                will be None.
4602         """
4603 
4604         if (not self._has_numa_support() and
4605                 instance_numa_topology is not None):
4606             # We should not get here, since we should have avoided
4607             # reporting NUMA topology from _get_host_numa_topology
4608             # in the first place. Just in case of a scheduler
4609             # mess up though, raise an exception
4610             raise exception.NUMATopologyUnsupported()
4611 
4612         allowed_cpus = hardware.get_vcpu_pin_set()
4613         topology = self._get_host_numa_topology()
4614 
4615         # We have instance NUMA so translate it to the config class
4616         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4617                 instance_numa_topology,
4618                 self._wants_hugepages(topology, instance_numa_topology))
4619 
4620         if not guest_cpu_numa_config:
4621             # No NUMA topology defined for instance - let the host kernel deal
4622             # with the NUMA effects.
4623             # TODO(ndipanov): Attempt to spread the instance
4624             # across NUMA nodes and expose the topology to the
4625             # instance as an optimisation
4626             return GuestNumaConfig(allowed_cpus, None, None, None)
4627 
4628         if not topology:
4629             # No NUMA topology defined for host - This will only happen with
4630             # some libvirt versions and certain platforms.
4631             return GuestNumaConfig(allowed_cpus, None,
4632                                    guest_cpu_numa_config, None)
4633 
4634         # Now get configuration from the numa_topology
4635         # Init CPUTune configuration
4636         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4637         guest_cpu_tune.emulatorpin = (
4638             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
4639         guest_cpu_tune.emulatorpin.cpuset = set([])
4640 
4641         # Init NUMATune configuration
4642         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4643         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
4644         guest_numa_tune.memnodes = []
4645 
4646         emulator_threads_policy = None
4647         if 'emulator_threads_policy' in instance_numa_topology:
4648             emulator_threads_policy = (
4649                 instance_numa_topology.emulator_threads_policy)
4650 
4651         # Set realtime scheduler for CPUTune
4652         vcpus_rt = set([])
4653         wants_realtime = hardware.is_realtime_enabled(flavor)
4654         if wants_realtime:
4655             vcpus_rt = hardware.vcpus_realtime_topology(flavor, image_meta)
4656             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4657             designer.set_vcpu_realtime_scheduler(
4658                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
4659             guest_cpu_tune.vcpusched.append(vcpusched)
4660 
4661         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
4662         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4663                 cell_pairs):
4664             # set NUMATune for the cell
4665             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4666             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
4667             guest_numa_tune.memnodes.append(tnode)
4668             guest_numa_tune.memory.nodeset.append(host_cell.id)
4669 
4670             # set CPUTune for the cell
4671             object_numa_cell = instance_numa_topology.cells[guest_node_id]
4672             for cpu in guest_config_cell.cpus:
4673                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
4674                                                   host_cell)
4675                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4676 
4677                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
4678                     cpu, object_numa_cell, vcpus_rt,
4679                     emulator_threads_policy, wants_realtime, pin_cpuset)
4680                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
4681 
4682         # TODO(berrange) When the guest has >1 NUMA node, it will
4683         # span multiple host NUMA nodes. By pinning emulator threads
4684         # to the union of all nodes, we guarantee there will be
4685         # cross-node memory access by the emulator threads when
4686         # responding to guest I/O operations. The only way to avoid
4687         # this would be to pin emulator threads to a single node and
4688         # tell the guest OS to only do I/O from one of its virtual
4689         # NUMA nodes. This is not even remotely practical.
4690         #
4691         # The long term solution is to make use of a new QEMU feature
4692         # called "I/O Threads" which will let us configure an explicit
4693         # I/O thread for each guest vCPU or guest NUMA node. It is
4694         # still TBD how to make use of this feature though, especially
4695         # how to associate IO threads with guest devices to eliminate
4696         # cross NUMA node traffic. This is an area of investigation
4697         # for QEMU community devs.
4698 
4699         # Sort the vcpupin list per vCPU id for human-friendlier XML
4700         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4701 
4702         # normalize cell.id
4703         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
4704                                                 guest_numa_tune.memnodes)):
4705             cell.id = i
4706             memnode.cellid = i
4707 
4708         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
4709                                guest_numa_tune)
4710 
4711     def _get_guest_os_type(self, virt_type):
4712         """Returns the guest OS type based on virt type."""
4713         if virt_type == "lxc":
4714             ret = fields.VMMode.EXE
4715         elif virt_type == "uml":
4716             ret = fields.VMMode.UML
4717         elif virt_type == "xen":
4718             ret = fields.VMMode.XEN
4719         else:
4720             ret = fields.VMMode.HVM
4721         return ret
4722 
4723     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4724                               root_device_name):
4725         if rescue.get('kernel_id'):
4726             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4727             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4728             if virt_type == "qemu":
4729                 guest.os_cmdline += " no_timer_check"
4730         if rescue.get('ramdisk_id'):
4731             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4732 
4733     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4734                                 root_device_name, image_meta):
4735         guest.os_kernel = os.path.join(inst_path, "kernel")
4736         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4737         if virt_type == "qemu":
4738             guest.os_cmdline += " no_timer_check"
4739         if instance.ramdisk_id:
4740             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4741         # we only support os_command_line with images with an explicit
4742         # kernel set and don't want to break nova if there's an
4743         # os_command_line property without a specified kernel_id param
4744         if image_meta.properties.get("os_command_line"):
4745             guest.os_cmdline = image_meta.properties.os_command_line
4746 
4747     def _set_clock(self, guest, os_type, image_meta, virt_type):
4748         # NOTE(mikal): Microsoft Windows expects the clock to be in
4749         # "localtime". If the clock is set to UTC, then you can use a
4750         # registry key to let windows know, but Microsoft says this is
4751         # buggy in http://support.microsoft.com/kb/2687252
4752         clk = vconfig.LibvirtConfigGuestClock()
4753         if os_type == 'windows':
4754             LOG.info('Configuring timezone for windows instance to localtime')
4755             clk.offset = 'localtime'
4756         else:
4757             clk.offset = 'utc'
4758         guest.set_clock(clk)
4759 
4760         if virt_type == "kvm":
4761             self._set_kvm_timers(clk, os_type, image_meta)
4762 
4763     def _set_kvm_timers(self, clk, os_type, image_meta):
4764         # TODO(berrange) One day this should be per-guest
4765         # OS type configurable
4766         tmpit = vconfig.LibvirtConfigGuestTimer()
4767         tmpit.name = "pit"
4768         tmpit.tickpolicy = "delay"
4769 
4770         tmrtc = vconfig.LibvirtConfigGuestTimer()
4771         tmrtc.name = "rtc"
4772         tmrtc.tickpolicy = "catchup"
4773 
4774         clk.add_timer(tmpit)
4775         clk.add_timer(tmrtc)
4776 
4777         hpet = image_meta.properties.get('hw_time_hpet', False)
4778         guestarch = libvirt_utils.get_arch(image_meta)
4779         if guestarch in (fields.Architecture.I686,
4780                          fields.Architecture.X86_64):
4781             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4782             # qemu -no-hpet is not supported on non-x86 targets.
4783             tmhpet = vconfig.LibvirtConfigGuestTimer()
4784             tmhpet.name = "hpet"
4785             tmhpet.present = hpet
4786             clk.add_timer(tmhpet)
4787         else:
4788             if hpet:
4789                 LOG.warning('HPET is not turned on for non-x86 guests in image'
4790                             ' %s.', image_meta.id)
4791 
4792         # Provide Windows guests with the paravirtualized hyperv timer source.
4793         # This is the windows equiv of kvm-clock, allowing Windows
4794         # guests to accurately keep time.
4795         if os_type == 'windows':
4796             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4797             tmhyperv.name = "hypervclock"
4798             tmhyperv.present = True
4799             clk.add_timer(tmhyperv)
4800 
4801     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
4802             flavor):
4803         hide_hypervisor_id = (strutils.bool_from_string(
4804                 flavor.extra_specs.get('hide_hypervisor_id')) or
4805             image_meta.properties.get('img_hide_hypervisor_id'))
4806 
4807         if virt_type == "xen":
4808             # PAE only makes sense in X86
4809             if caps.host.cpu.arch in (fields.Architecture.I686,
4810                                       fields.Architecture.X86_64):
4811                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4812 
4813         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4814                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4815             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4816             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4817 
4818         if (virt_type in ("qemu", "kvm") and
4819                 os_type == 'windows'):
4820             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4821             hv.relaxed = True
4822 
4823             hv.spinlocks = True
4824             # Increase spinlock retries - value recommended by
4825             # KVM maintainers who certify Windows guests
4826             # with Microsoft
4827             hv.spinlock_retries = 8191
4828             hv.vapic = True
4829 
4830             # NOTE(kosamara): Spoofing the vendor_id aims to allow the nvidia
4831             # driver to work on windows VMs. At the moment, the nvidia driver
4832             # checks for the hyperv vendorid, and if it doesn't find that, it
4833             # works. In the future, its behaviour could become more strict,
4834             # checking for the presence of other hyperv feature flags to
4835             # determine that it's loaded in a VM. If that happens, this
4836             # workaround will not be enough, and we'll need to drop the whole
4837             # hyperv element.
4838             # That would disable some optimizations, reducing the guest's
4839             # performance.
4840             if hide_hypervisor_id:
4841                 hv.vendorid_spoof = True
4842 
4843             guest.features.append(hv)
4844 
4845         if virt_type in ("qemu", "kvm"):
4846             if hide_hypervisor_id:
4847                 guest.features.append(
4848                     vconfig.LibvirtConfigGuestFeatureKvmHidden())
4849 
4850             # NOTE(sean-k-mooney): we validate that the image and flavor
4851             # cannot have conflicting values in the compute API
4852             # so we just use the values directly. If it is not set in
4853             # either the flavor or image pmu will be none and we should
4854             # not generate the element to allow qemu to decide if a vPMU
4855             # should be provided for backwards compatibility.
4856             pmu = (flavor.extra_specs.get('hw:pmu') or
4857                    image_meta.properties.get('hw_pmu'))
4858             if pmu is not None:
4859                 guest.features.append(
4860                     vconfig.LibvirtConfigGuestFeaturePMU(pmu))
4861 
4862     def _check_number_of_serial_console(self, num_ports):
4863         virt_type = CONF.libvirt.virt_type
4864         if (virt_type in ("kvm", "qemu") and
4865             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4866             raise exception.SerialPortNumberLimitExceeded(
4867                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4868 
4869     def _video_model_supported(self, model):
4870         if model not in fields.VideoModel.ALL:
4871             return False
4872         min_ver = MIN_LIBVIRT_VIDEO_MODEL_VERSIONS.get(model)
4873         if min_ver and not self._host.has_min_version(lv_ver=min_ver):
4874             return False
4875         return True
4876 
4877     def _add_video_driver(self, guest, image_meta, flavor):
4878         video = vconfig.LibvirtConfigGuestVideo()
4879         # NOTE(ldbragst): The following logic sets the video.type
4880         # depending on supported defaults given the architecture,
4881         # virtualization type, and features. The video.type attribute can
4882         # be overridden by the user with image_meta.properties, which
4883         # is carried out in the next if statement below this one.
4884         guestarch = libvirt_utils.get_arch(image_meta)
4885         if guest.os_type == fields.VMMode.XEN:
4886             video.type = 'xen'
4887         elif CONF.libvirt.virt_type == 'parallels':
4888             video.type = 'vga'
4889         elif guestarch in (fields.Architecture.PPC,
4890                            fields.Architecture.PPC64,
4891                            fields.Architecture.PPC64LE):
4892             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4893             # so use 'vga' instead when running on Power hardware.
4894             video.type = 'vga'
4895         elif guestarch == fields.Architecture.AARCH64:
4896             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4897             # so use 'virtio' instead when running on AArch64 hardware.
4898             video.type = 'virtio'
4899         elif CONF.spice.enabled:
4900             video.type = 'qxl'
4901         if image_meta.properties.get('hw_video_model'):
4902             video.type = image_meta.properties.hw_video_model
4903             if not self._video_model_supported(video.type):
4904                 raise exception.InvalidVideoMode(model=video.type)
4905 
4906         # Set video memory, only if the flavor's limit is set
4907         video_ram = image_meta.properties.get('hw_video_ram', 0)
4908         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4909         if video_ram > max_vram:
4910             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4911                                                  max_vram=max_vram)
4912         if max_vram and video_ram:
4913             video.vram = video_ram * units.Mi / units.Ki
4914         guest.add_device(video)
4915 
4916         # NOTE(sean-k-mooney): return the video device we added
4917         # for simpler testing.
4918         return video
4919 
4920     def _add_qga_device(self, guest, instance):
4921         qga = vconfig.LibvirtConfigGuestChannel()
4922         qga.type = "unix"
4923         qga.target_name = "org.qemu.guest_agent.0"
4924         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4925                           ("org.qemu.guest_agent.0", instance.name))
4926         guest.add_device(qga)
4927 
4928     def _add_rng_device(self, guest, flavor):
4929         rng_device = vconfig.LibvirtConfigGuestRng()
4930         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4931         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4932         if rate_bytes:
4933             rng_device.rate_bytes = int(rate_bytes)
4934             rng_device.rate_period = int(period)
4935         rng_path = CONF.libvirt.rng_dev_path
4936         if (rng_path and not os.path.exists(rng_path)):
4937             raise exception.RngDeviceNotExist(path=rng_path)
4938         rng_device.backend = rng_path
4939         guest.add_device(rng_device)
4940 
4941     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4942         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4943         if image_meta.properties.get('hw_qemu_guest_agent', False):
4944             LOG.debug("Qemu guest agent is enabled through image "
4945                       "metadata", instance=instance)
4946             self._add_qga_device(guest, instance)
4947         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4948         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4949         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4950         if rng_is_virtio and rng_allowed:
4951             self._add_rng_device(guest, flavor)
4952 
4953     def _get_guest_memory_backing_config(
4954             self, inst_topology, numatune, flavor):
4955         wantsmempages = False
4956         if inst_topology:
4957             for cell in inst_topology.cells:
4958                 if cell.pagesize:
4959                     wantsmempages = True
4960                     break
4961 
4962         wantsrealtime = hardware.is_realtime_enabled(flavor)
4963 
4964         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
4965 
4966         if wantsmempages and wantsfilebacked:
4967             # Can't use file-backed memory with hugepages
4968             LOG.warning("Instance requested huge pages, but file-backed "
4969                     "memory is enabled, and incompatible with huge pages")
4970             raise exception.MemoryPagesUnsupported()
4971 
4972         membacking = None
4973         if wantsmempages:
4974             pages = self._get_memory_backing_hugepages_support(
4975                 inst_topology, numatune)
4976             if pages:
4977                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4978                 membacking.hugepages = pages
4979         if wantsrealtime:
4980             if not membacking:
4981                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4982             membacking.locked = True
4983             membacking.sharedpages = False
4984         if wantsfilebacked:
4985             if not membacking:
4986                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4987             membacking.filesource = True
4988             membacking.sharedaccess = True
4989             membacking.allocateimmediate = True
4990             if self._host.has_min_version(
4991                     MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
4992                     MIN_QEMU_FILE_BACKED_DISCARD_VERSION):
4993                 membacking.discard = True
4994 
4995         return membacking
4996 
4997     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4998         if not self._has_numa_support():
4999             # We should not get here, since we should have avoided
5000             # reporting NUMA topology from _get_host_numa_topology
5001             # in the first place. Just in case of a scheduler
5002             # mess up though, raise an exception
5003             raise exception.MemoryPagesUnsupported()
5004 
5005         host_topology = self._get_host_numa_topology()
5006 
5007         if host_topology is None:
5008             # As above, we should not get here but just in case...
5009             raise exception.MemoryPagesUnsupported()
5010 
5011         # Currently libvirt does not support the smallest
5012         # pagesize set as a backend memory.
5013         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
5014         avail_pagesize = [page.size_kb
5015                           for page in host_topology.cells[0].mempages]
5016         avail_pagesize.sort()
5017         smallest = avail_pagesize[0]
5018 
5019         pages = []
5020         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
5021             if inst_cell.pagesize and inst_cell.pagesize > smallest:
5022                 for memnode in numatune.memnodes:
5023                     if guest_cellid == memnode.cellid:
5024                         page = (
5025                             vconfig.LibvirtConfigGuestMemoryBackingPage())
5026                         page.nodeset = [guest_cellid]
5027                         page.size_kb = inst_cell.pagesize
5028                         pages.append(page)
5029                         break  # Quit early...
5030         return pages
5031 
5032     def _get_flavor(self, ctxt, instance, flavor):
5033         if flavor is not None:
5034             return flavor
5035         return instance.flavor
5036 
5037     def _has_uefi_support(self):
5038         # This means that the host can support UEFI booting for guests
5039         supported_archs = [fields.Architecture.X86_64,
5040                            fields.Architecture.AARCH64]
5041         caps = self._host.get_capabilities()
5042         # TODO(dmllr, kchamart): Get rid of probing the OVMF binary file
5043         # paths, it is not robust, because nothing but the binary's
5044         # filename is reported, which means you have to detect its
5045         # architecture and features by other means.  To solve this,
5046         # query the libvirt's getDomainCapabilities() to get the
5047         # firmware paths (as reported in the 'loader' value).  Nova now
5048         # has a wrapper method for this, get_domain_capabilities().
5049         # This is a more reliable way to detect UEFI boot support.
5050         #
5051         # Further, with libvirt 5.3 onwards, support for UEFI boot is
5052         # much more simplified by the "firmware auto-selection" feature.
5053         # When using this, Nova doesn't need to query OVMF file paths at
5054         # all; libvirt will take care of it.  This is done by taking
5055         # advantage of the so-called firmware "descriptor files" --
5056         # small JSON files (which will be shipped by Linux
5057         # distributions) that describe a UEFI firmware binary's
5058         # "characteristics", such as the binary's file path, its
5059         # features, architecture, supported machine type, NVRAM template
5060         # and so forth.
5061 
5062         return ((caps.host.cpu.arch in supported_archs) and
5063                 any((os.path.exists(p)
5064                      for p in DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch])))
5065 
5066     def _get_supported_perf_events(self):
5067 
5068         if (len(CONF.libvirt.enabled_perf_events) == 0):
5069             return []
5070 
5071         supported_events = []
5072         host_cpu_info = self._get_cpu_info()
5073         for event in CONF.libvirt.enabled_perf_events:
5074             if self._supported_perf_event(event, host_cpu_info['features']):
5075                 supported_events.append(event)
5076         return supported_events
5077 
5078     def _supported_perf_event(self, event, cpu_features):
5079 
5080         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
5081 
5082         if not hasattr(libvirt, libvirt_perf_event_name):
5083             LOG.warning("Libvirt doesn't support event type %s.", event)
5084             return False
5085 
5086         if event in PERF_EVENTS_CPU_FLAG_MAPPING:
5087             LOG.warning('Monitoring Intel CMT `perf` event(s) %s is '
5088                         'deprecated and will be removed in the "Stein" '
5089                         'release.  It was broken by design in the '
5090                         'Linux kernel, so support for Intel CMT was '
5091                         'removed from Linux 4.14 onwards. Therefore '
5092                         'it is recommended to not enable them.',
5093                         event)
5094             if PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features:
5095                 LOG.warning("Host does not support event type %s.", event)
5096                 return False
5097         return True
5098 
5099     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
5100                                       image_meta, flavor, root_device_name):
5101         if virt_type == "xen":
5102             if guest.os_type == fields.VMMode.HVM:
5103                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
5104             else:
5105                 guest.os_cmdline = CONSOLE
5106         elif virt_type in ("kvm", "qemu"):
5107             if caps.host.cpu.arch in (fields.Architecture.I686,
5108                                       fields.Architecture.X86_64):
5109                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
5110                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
5111             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
5112             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5113                 if not hw_firmware_type:
5114                     hw_firmware_type = fields.FirmwareType.UEFI
5115             if hw_firmware_type == fields.FirmwareType.UEFI:
5116                 if self._has_uefi_support():
5117                     global uefi_logged
5118                     if not uefi_logged:
5119                         LOG.warning("uefi support is without some kind of "
5120                                     "functional testing and therefore "
5121                                     "considered experimental.")
5122                         uefi_logged = True
5123                     for lpath in DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]:
5124                         if os.path.exists(lpath):
5125                             guest.os_loader = lpath
5126                     guest.os_loader_type = "pflash"
5127                 else:
5128                     raise exception.UEFINotSupported()
5129             guest.os_mach_type = libvirt_utils.get_machine_type(image_meta)
5130             if image_meta.properties.get('hw_boot_menu') is None:
5131                 guest.os_bootmenu = strutils.bool_from_string(
5132                     flavor.extra_specs.get('hw:boot_menu', 'no'))
5133             else:
5134                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
5135 
5136         elif virt_type == "lxc":
5137             guest.os_init_path = "/sbin/init"
5138             guest.os_cmdline = CONSOLE
5139         elif virt_type == "uml":
5140             guest.os_kernel = "/usr/bin/linux"
5141             guest.os_root = root_device_name
5142         elif virt_type == "parallels":
5143             if guest.os_type == fields.VMMode.EXE:
5144                 guest.os_init_path = "/sbin/init"
5145 
5146     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
5147                     instance, inst_path, image_meta, disk_info):
5148         if rescue:
5149             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
5150                                        root_device_name)
5151         elif instance.kernel_id:
5152             self._set_guest_for_inst_kernel(instance, guest, inst_path,
5153                                             virt_type, root_device_name,
5154                                             image_meta)
5155         else:
5156             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
5157 
5158     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
5159                          image_meta):
5160         # NOTE(markus_z): Beware! Below are so many conditionals that it is
5161         # easy to lose track. Use this chart to figure out your case:
5162         #
5163         # case | is serial | is qemu | resulting
5164         #      | enabled?  | or kvm? | devices
5165         # -------------------------------------------
5166         #    1 |        no |     no  | pty*
5167         #    2 |        no |     yes | pty with logd
5168         #    3 |       yes |      no | see case 1
5169         #    4 |       yes |     yes | tcp with logd
5170         #
5171         #    * exception: `virt_type=parallels` doesn't create a device
5172         if virt_type == 'parallels':
5173             pass
5174         elif virt_type not in ("qemu", "kvm"):
5175             log_path = self._get_console_log_path(instance)
5176             self._create_pty_device(guest_cfg,
5177                                     vconfig.LibvirtConfigGuestConsole,
5178                                     log_path=log_path)
5179         elif (virt_type in ("qemu", "kvm") and
5180                   self._is_s390x_guest(image_meta)):
5181             self._create_consoles_s390x(guest_cfg, instance,
5182                                         flavor, image_meta)
5183         elif virt_type in ("qemu", "kvm"):
5184             self._create_consoles_qemu_kvm(guest_cfg, instance,
5185                                         flavor, image_meta)
5186 
5187     def _is_s390x_guest(self, image_meta):
5188         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
5189         return libvirt_utils.get_arch(image_meta) in s390x_archs
5190 
5191     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
5192                                   image_meta):
5193         char_dev_cls = vconfig.LibvirtConfigGuestSerial
5194         log_path = self._get_console_log_path(instance)
5195         if CONF.serial_console.enabled:
5196             if not self._serial_ports_already_defined(instance):
5197                 num_ports = hardware.get_number_of_serial_ports(flavor,
5198                                                                 image_meta)
5199                 self._check_number_of_serial_console(num_ports)
5200                 self._create_serial_consoles(guest_cfg, num_ports,
5201                                              char_dev_cls, log_path)
5202         else:
5203             self._create_pty_device(guest_cfg, char_dev_cls,
5204                                     log_path=log_path)
5205 
5206     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
5207         char_dev_cls = vconfig.LibvirtConfigGuestConsole
5208         log_path = self._get_console_log_path(instance)
5209         if CONF.serial_console.enabled:
5210             if not self._serial_ports_already_defined(instance):
5211                 num_ports = hardware.get_number_of_serial_ports(flavor,
5212                                                                 image_meta)
5213                 self._create_serial_consoles(guest_cfg, num_ports,
5214                                              char_dev_cls, log_path)
5215         else:
5216             self._create_pty_device(guest_cfg, char_dev_cls,
5217                                     "sclp", log_path)
5218 
5219     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
5220                            log_path=None):
5221 
5222         consolepty = char_dev_cls()
5223         consolepty.target_type = target_type
5224         consolepty.type = "pty"
5225 
5226         log = vconfig.LibvirtConfigGuestCharDeviceLog()
5227         log.file = log_path
5228         consolepty.log = log
5229 
5230         guest_cfg.add_device(consolepty)
5231 
5232     def _serial_ports_already_defined(self, instance):
5233         try:
5234             guest = self._host.get_guest(instance)
5235             if list(self._get_serial_ports_from_guest(guest)):
5236                 # Serial port are already configured for instance that
5237                 # means we are in a context of migration.
5238                 return True
5239         except exception.InstanceNotFound:
5240             LOG.debug(
5241                 "Instance does not exist yet on libvirt, we can "
5242                 "safely pass on looking for already defined serial "
5243                 "ports in its domain XML", instance=instance)
5244         return False
5245 
5246     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
5247                                 log_path):
5248         for port in six.moves.range(num_ports):
5249             console = char_dev_cls()
5250             console.port = port
5251             console.type = "tcp"
5252             console.listen_host = CONF.serial_console.proxyclient_address
5253             listen_port = serial_console.acquire_port(console.listen_host)
5254             console.listen_port = listen_port
5255             # NOTE: only the first serial console gets the boot messages,
5256             # that's why we attach the logd subdevice only to that.
5257             if port == 0:
5258                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5259                 log.file = log_path
5260                 console.log = log
5261             guest_cfg.add_device(console)
5262 
5263     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5264         """Update VirtCPUModel object according to libvirt CPU config.
5265 
5266         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5267                            instance's virtual cpu configuration.
5268         :param:vcpu_model: VirtCPUModel object. A new object will be created
5269                            if None.
5270 
5271         :return: Updated VirtCPUModel object, or None if cpu_config is None
5272 
5273         """
5274 
5275         if not cpu_config:
5276             return
5277         if not vcpu_model:
5278             vcpu_model = objects.VirtCPUModel()
5279 
5280         vcpu_model.arch = cpu_config.arch
5281         vcpu_model.vendor = cpu_config.vendor
5282         vcpu_model.model = cpu_config.model
5283         vcpu_model.mode = cpu_config.mode
5284         vcpu_model.match = cpu_config.match
5285 
5286         if cpu_config.sockets:
5287             vcpu_model.topology = objects.VirtCPUTopology(
5288                 sockets=cpu_config.sockets,
5289                 cores=cpu_config.cores,
5290                 threads=cpu_config.threads)
5291         else:
5292             vcpu_model.topology = None
5293 
5294         features = [objects.VirtCPUFeature(
5295             name=f.name,
5296             policy=f.policy) for f in cpu_config.features]
5297         vcpu_model.features = features
5298 
5299         return vcpu_model
5300 
5301     def _vcpu_model_to_cpu_config(self, vcpu_model):
5302         """Create libvirt CPU config according to VirtCPUModel object.
5303 
5304         :param:vcpu_model: VirtCPUModel object.
5305 
5306         :return: vconfig.LibvirtConfigGuestCPU.
5307 
5308         """
5309 
5310         cpu_config = vconfig.LibvirtConfigGuestCPU()
5311         cpu_config.arch = vcpu_model.arch
5312         cpu_config.model = vcpu_model.model
5313         cpu_config.mode = vcpu_model.mode
5314         cpu_config.match = vcpu_model.match
5315         cpu_config.vendor = vcpu_model.vendor
5316         if vcpu_model.topology:
5317             cpu_config.sockets = vcpu_model.topology.sockets
5318             cpu_config.cores = vcpu_model.topology.cores
5319             cpu_config.threads = vcpu_model.topology.threads
5320         if vcpu_model.features:
5321             for f in vcpu_model.features:
5322                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5323                 xf.name = f.name
5324                 xf.policy = f.policy
5325                 cpu_config.features.add(xf)
5326         return cpu_config
5327 
5328     def _guest_add_pcie_root_ports(self, guest):
5329         """Add PCI Express root ports.
5330 
5331         PCI Express machine can have as many PCIe devices as it has
5332         pcie-root-port controllers (slots in virtual motherboard).
5333 
5334         If we want to have more PCIe slots for hotplug then we need to create
5335         whole PCIe structure (libvirt limitation).
5336         """
5337 
5338         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5339         guest.add_device(pcieroot)
5340 
5341         for x in range(0, CONF.libvirt.num_pcie_ports):
5342             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5343             guest.add_device(pcierootport)
5344 
5345     def _guest_needs_pcie(self, guest, caps):
5346         """Check for prerequisites for adding PCIe root port
5347         controllers
5348         """
5349 
5350         # TODO(kchamart) In the third 'if' conditional below, for 'x86'
5351         # arch, we're assuming: when 'os_mach_type' is 'None', you'll
5352         # have "pc" machine type.  That assumption, although it is
5353         # correct for the "forseeable future", it will be invalid when
5354         # libvirt / QEMU changes the default machine types.
5355         #
5356         # From libvirt 4.7.0 onwards (September 2018), it will ensure
5357         # that *if* 'pc' is available, it will be used as the default --
5358         # to not break existing applications.  (Refer:
5359         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=26cfb1a3
5360         # --"qemu: ensure default machine types don't change if QEMU
5361         # changes").
5362         #
5363         # But even if libvirt (>=v4.7.0) handled the default case,
5364         # relying on such assumptions is not robust.  Instead we should
5365         # get the default machine type for a given architecture reliably
5366         # -- by Nova setting it explicitly (we already do it for Arm /
5367         # AArch64 & s390x).  A part of this bug is being tracked here:
5368         # https://bugs.launchpad.net/nova/+bug/1780138).
5369 
5370         # Add PCIe root port controllers for PCI Express machines
5371         # but only if their amount is configured
5372 
5373         if not CONF.libvirt.num_pcie_ports:
5374             return False
5375         if (caps.host.cpu.arch == fields.Architecture.AARCH64 and
5376                 guest.os_mach_type.startswith('virt')):
5377             return True
5378         if (caps.host.cpu.arch == fields.Architecture.X86_64 and
5379                 guest.os_mach_type is not None and
5380                 'q35' in guest.os_mach_type):
5381             return True
5382         return False
5383 
5384     def _guest_add_usb_host_keyboard(self, guest):
5385         """Add USB Host controller and keyboard for graphical console use.
5386 
5387         Add USB keyboard as PS/2 support may not be present on non-x86
5388         architectures.
5389         """
5390         keyboard = vconfig.LibvirtConfigGuestInput()
5391         keyboard.type = "keyboard"
5392         keyboard.bus = "usb"
5393         guest.add_device(keyboard)
5394 
5395         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5396         usbhost.index = 0
5397         guest.add_device(usbhost)
5398 
5399     def _get_guest_config(self, instance, network_info, image_meta,
5400                           disk_info, rescue=None, block_device_info=None,
5401                           context=None, mdevs=None):
5402         """Get config data for parameters.
5403 
5404         :param rescue: optional dictionary that should contain the key
5405             'ramdisk_id' if a ramdisk is needed for the rescue image and
5406             'kernel_id' if a kernel is needed for the rescue image.
5407 
5408         :param mdevs: optional list of mediated devices to assign to the guest.
5409         """
5410         flavor = instance.flavor
5411         inst_path = libvirt_utils.get_instance_path(instance)
5412         disk_mapping = disk_info['mapping']
5413 
5414         virt_type = CONF.libvirt.virt_type
5415         guest = vconfig.LibvirtConfigGuest()
5416         guest.virt_type = virt_type
5417         guest.name = instance.name
5418         guest.uuid = instance.uuid
5419         # We are using default unit for memory: KiB
5420         guest.memory = flavor.memory_mb * units.Ki
5421         guest.vcpus = flavor.vcpus
5422 
5423         guest_numa_config = self._get_guest_numa_config(
5424             instance.numa_topology, flavor, image_meta)
5425 
5426         guest.cpuset = guest_numa_config.cpuset
5427         guest.cputune = guest_numa_config.cputune
5428         guest.numatune = guest_numa_config.numatune
5429 
5430         guest.membacking = self._get_guest_memory_backing_config(
5431             instance.numa_topology,
5432             guest_numa_config.numatune,
5433             flavor)
5434 
5435         guest.metadata.append(self._get_guest_config_meta(instance))
5436         guest.idmaps = self._get_guest_idmaps()
5437 
5438         for event in self._supported_perf_events:
5439             guest.add_perf_event(event)
5440 
5441         self._update_guest_cputune(guest, flavor, virt_type)
5442 
5443         guest.cpu = self._get_guest_cpu_config(
5444             flavor, image_meta, guest_numa_config.numaconfig,
5445             instance.numa_topology)
5446 
5447         # Notes(yjiang5): we always sync the instance's vcpu model with
5448         # the corresponding config file.
5449         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5450             guest.cpu, instance.vcpu_model)
5451 
5452         if 'root' in disk_mapping:
5453             root_device_name = block_device.prepend_dev(
5454                 disk_mapping['root']['dev'])
5455         else:
5456             root_device_name = None
5457 
5458         if root_device_name:
5459             instance.root_device_name = root_device_name
5460 
5461         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5462                 self._get_guest_os_type(virt_type))
5463         caps = self._host.get_capabilities()
5464 
5465         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5466                                            image_meta, flavor,
5467                                            root_device_name)
5468         if virt_type not in ('lxc', 'uml'):
5469             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5470                     instance, inst_path, image_meta, disk_info)
5471 
5472         self._set_features(guest, instance.os_type, caps, virt_type,
5473                            image_meta, flavor)
5474         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5475 
5476         storage_configs = self._get_guest_storage_config(context,
5477                 instance, image_meta, disk_info, rescue, block_device_info,
5478                 flavor, guest.os_type)
5479         for config in storage_configs:
5480             guest.add_device(config)
5481 
5482         for vif in network_info:
5483             config = self.vif_driver.get_config(
5484                 instance, vif, image_meta,
5485                 flavor, virt_type, self._host)
5486             guest.add_device(config)
5487 
5488         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5489 
5490         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5491         if pointer:
5492             guest.add_device(pointer)
5493 
5494         self._guest_add_spice_channel(guest)
5495 
5496         if self._guest_add_video_device(guest):
5497             self._add_video_driver(guest, image_meta, flavor)
5498 
5499             # We want video == we want graphical console. Some architectures
5500             # do not have input devices attached in default configuration.
5501             # Let then add USB Host controller and USB keyboard.
5502             # x86(-64) and ppc64 have usb host controller and keyboard
5503             # s390x does not support USB
5504             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5505                 self._guest_add_usb_host_keyboard(guest)
5506 
5507         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5508         if virt_type in ('qemu', 'kvm'):
5509             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5510 
5511         if self._guest_needs_pcie(guest, caps):
5512             self._guest_add_pcie_root_ports(guest)
5513 
5514         self._guest_add_pci_devices(guest, instance)
5515 
5516         self._guest_add_watchdog_action(guest, flavor, image_meta)
5517 
5518         self._guest_add_memory_balloon(guest)
5519 
5520         if mdevs:
5521             self._guest_add_mdevs(guest, mdevs)
5522 
5523         return guest
5524 
5525     def _guest_add_mdevs(self, guest, chosen_mdevs):
5526         for chosen_mdev in chosen_mdevs:
5527             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5528             mdev.uuid = chosen_mdev
5529             guest.add_device(mdev)
5530 
5531     @staticmethod
5532     def _guest_add_spice_channel(guest):
5533         if (CONF.spice.enabled and CONF.spice.agent_enabled and
5534                 guest.virt_type not in ('lxc', 'uml', 'xen')):
5535             channel = vconfig.LibvirtConfigGuestChannel()
5536             channel.type = 'spicevmc'
5537             channel.target_name = "com.redhat.spice.0"
5538             guest.add_device(channel)
5539 
5540     @staticmethod
5541     def _guest_add_memory_balloon(guest):
5542         virt_type = guest.virt_type
5543         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5544         if (virt_type in ('xen', 'qemu', 'kvm') and
5545                     CONF.libvirt.mem_stats_period_seconds > 0):
5546             balloon = vconfig.LibvirtConfigMemoryBalloon()
5547             if virt_type in ('qemu', 'kvm'):
5548                 balloon.model = 'virtio'
5549             else:
5550                 balloon.model = 'xen'
5551             balloon.period = CONF.libvirt.mem_stats_period_seconds
5552             guest.add_device(balloon)
5553 
5554     @staticmethod
5555     def _guest_add_watchdog_action(guest, flavor, image_meta):
5556         # image meta takes precedence over flavor extra specs; disable the
5557         # watchdog action by default
5558         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action') or
5559                            'disabled')
5560         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5561                                                     watchdog_action)
5562         # NB(sross): currently only actually supported by KVM/QEmu
5563         if watchdog_action != 'disabled':
5564             if watchdog_action in fields.WatchdogAction.ALL:
5565                 bark = vconfig.LibvirtConfigGuestWatchdog()
5566                 bark.action = watchdog_action
5567                 guest.add_device(bark)
5568             else:
5569                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5570 
5571     def _guest_add_pci_devices(self, guest, instance):
5572         virt_type = guest.virt_type
5573         if virt_type in ('xen', 'qemu', 'kvm'):
5574             # Get all generic PCI devices (non-SR-IOV).
5575             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5576                 guest.add_device(self._get_guest_pci_device(pci_dev))
5577         else:
5578             # PCI devices is only supported for hypervisors
5579             #  'xen', 'qemu' and 'kvm'.
5580             if pci_manager.get_instance_pci_devs(instance, 'all'):
5581                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5582 
5583     @staticmethod
5584     def _guest_add_video_device(guest):
5585         # NB some versions of libvirt support both SPICE and VNC
5586         # at the same time. We're not trying to second guess which
5587         # those versions are. We'll just let libvirt report the
5588         # errors appropriately if the user enables both.
5589         add_video_driver = False
5590         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5591             graphics = vconfig.LibvirtConfigGuestGraphics()
5592             graphics.type = "vnc"
5593             if CONF.vnc.keymap:
5594                 graphics.keymap = CONF.vnc.keymap
5595             graphics.listen = CONF.vnc.server_listen
5596             guest.add_device(graphics)
5597             add_video_driver = True
5598         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5599             graphics = vconfig.LibvirtConfigGuestGraphics()
5600             graphics.type = "spice"
5601             if CONF.spice.keymap:
5602                 graphics.keymap = CONF.spice.keymap
5603             graphics.listen = CONF.spice.server_listen
5604             guest.add_device(graphics)
5605             add_video_driver = True
5606         return add_video_driver
5607 
5608     def _get_guest_pointer_model(self, os_type, image_meta):
5609         pointer_model = image_meta.properties.get(
5610             'hw_pointer_model', CONF.pointer_model)
5611         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5612             # TODO(sahid): We set pointer_model to keep compatibility
5613             # until the next release O*. It means operators can continue
5614             # to use the deprecated option "use_usb_tablet" or set a
5615             # specific device to use
5616             pointer_model = "usbtablet"
5617             LOG.warning('The option "use_usb_tablet" has been '
5618                         'deprecated for Newton in favor of the more '
5619                         'generic "pointer_model". Please update '
5620                         'nova.conf to address this change.')
5621 
5622         if pointer_model == "usbtablet":
5623             # We want a tablet if VNC is enabled, or SPICE is enabled and
5624             # the SPICE agent is disabled. If the SPICE agent is enabled
5625             # it provides a paravirt mouse which drastically reduces
5626             # overhead (by eliminating USB polling).
5627             if CONF.vnc.enabled or (
5628                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5629                 return self._get_guest_usb_tablet(os_type)
5630             else:
5631                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5632                     # For backward compatibility We don't want to break
5633                     # process of booting an instance if host is configured
5634                     # to use USB tablet without VNC or SPICE and SPICE
5635                     # agent disable.
5636                     LOG.warning('USB tablet requested for guests by host '
5637                                 'configuration. In order to accept this '
5638                                 'request VNC should be enabled or SPICE '
5639                                 'and SPICE agent disabled on host.')
5640                 else:
5641                     raise exception.UnsupportedPointerModelRequested(
5642                         model="usbtablet")
5643 
5644     def _get_guest_usb_tablet(self, os_type):
5645         tablet = None
5646         if os_type == fields.VMMode.HVM:
5647             tablet = vconfig.LibvirtConfigGuestInput()
5648             tablet.type = "tablet"
5649             tablet.bus = "usb"
5650         else:
5651             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5652                 # For backward compatibility We don't want to break
5653                 # process of booting an instance if virtual machine mode
5654                 # is not configured as HVM.
5655                 LOG.warning('USB tablet requested for guests by host '
5656                             'configuration. In order to accept this '
5657                             'request the machine mode should be '
5658                             'configured as HVM.')
5659             else:
5660                 raise exception.UnsupportedPointerModelRequested(
5661                     model="usbtablet")
5662         return tablet
5663 
5664     def _get_guest_xml(self, context, instance, network_info, disk_info,
5665                        image_meta, rescue=None,
5666                        block_device_info=None,
5667                        mdevs=None):
5668         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5669         # this ahead of time so that we don't acquire it while also
5670         # holding the logging lock.
5671         network_info_str = str(network_info)
5672         msg = ('Start _get_guest_xml '
5673                'network_info=%(network_info)s '
5674                'disk_info=%(disk_info)s '
5675                'image_meta=%(image_meta)s rescue=%(rescue)s '
5676                'block_device_info=%(block_device_info)s' %
5677                {'network_info': network_info_str, 'disk_info': disk_info,
5678                 'image_meta': image_meta, 'rescue': rescue,
5679                 'block_device_info': block_device_info})
5680         # NOTE(mriedem): block_device_info can contain auth_password so we
5681         # need to sanitize the password in the message.
5682         LOG.debug(strutils.mask_password(msg), instance=instance)
5683         conf = self._get_guest_config(instance, network_info, image_meta,
5684                                       disk_info, rescue, block_device_info,
5685                                       context, mdevs)
5686         xml = conf.to_xml()
5687 
5688         LOG.debug('End _get_guest_xml xml=%(xml)s',
5689                   {'xml': xml}, instance=instance)
5690         return xml
5691 
5692     def get_info(self, instance, use_cache=True):
5693         """Retrieve information from libvirt for a specific instance.
5694 
5695         If a libvirt error is encountered during lookup, we might raise a
5696         NotFound exception or Error exception depending on how severe the
5697         libvirt error is.
5698 
5699         :param instance: nova.objects.instance.Instance object
5700         :param use_cache: unused in this driver
5701         :returns: An InstanceInfo object
5702         """
5703         guest = self._host.get_guest(instance)
5704         # Kind of ugly but we need to pass host to get_info as for a
5705         # workaround, see libvirt/compat.py
5706         return guest.get_info(self._host)
5707 
5708     def _create_domain_setup_lxc(self, context, instance, image_meta,
5709                                  block_device_info):
5710         inst_path = libvirt_utils.get_instance_path(instance)
5711         block_device_mapping = driver.block_device_info_get_mapping(
5712             block_device_info)
5713         root_disk = block_device.get_root_bdm(block_device_mapping)
5714         if root_disk:
5715             self._connect_volume(context, root_disk['connection_info'],
5716                                  instance)
5717             disk_path = root_disk['connection_info']['data']['device_path']
5718 
5719             # NOTE(apmelton) - Even though the instance is being booted from a
5720             # cinder volume, it is still presented as a local block device.
5721             # LocalBlockImage is used here to indicate that the instance's
5722             # disk is backed by a local block device.
5723             image_model = imgmodel.LocalBlockImage(disk_path)
5724         else:
5725             root_disk = self.image_backend.by_name(instance, 'disk')
5726             image_model = root_disk.get_model(self._conn)
5727 
5728         container_dir = os.path.join(inst_path, 'rootfs')
5729         fileutils.ensure_tree(container_dir)
5730         rootfs_dev = disk_api.setup_container(image_model,
5731                                               container_dir=container_dir)
5732 
5733         try:
5734             # Save rootfs device to disconnect it when deleting the instance
5735             if rootfs_dev:
5736                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5737             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5738                 id_maps = self._get_guest_idmaps()
5739                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5740         except Exception:
5741             with excutils.save_and_reraise_exception():
5742                 self._create_domain_cleanup_lxc(instance)
5743 
5744     def _create_domain_cleanup_lxc(self, instance):
5745         inst_path = libvirt_utils.get_instance_path(instance)
5746         container_dir = os.path.join(inst_path, 'rootfs')
5747 
5748         try:
5749             state = self.get_info(instance).state
5750         except exception.InstanceNotFound:
5751             # The domain may not be present if the instance failed to start
5752             state = None
5753 
5754         if state == power_state.RUNNING:
5755             # NOTE(uni): Now the container is running with its own private
5756             # mount namespace and so there is no need to keep the container
5757             # rootfs mounted in the host namespace
5758             LOG.debug('Attempting to unmount container filesystem: %s',
5759                       container_dir, instance=instance)
5760             disk_api.clean_lxc_namespace(container_dir=container_dir)
5761         else:
5762             disk_api.teardown_container(container_dir=container_dir)
5763 
5764     @contextlib.contextmanager
5765     def _lxc_disk_handler(self, context, instance, image_meta,
5766                           block_device_info):
5767         """Context manager to handle the pre and post instance boot,
5768            LXC specific disk operations.
5769 
5770            An image or a volume path will be prepared and setup to be
5771            used by the container, prior to starting it.
5772            The disk will be disconnected and unmounted if a container has
5773            failed to start.
5774         """
5775 
5776         if CONF.libvirt.virt_type != 'lxc':
5777             yield
5778             return
5779 
5780         self._create_domain_setup_lxc(context, instance, image_meta,
5781                                       block_device_info)
5782 
5783         try:
5784             yield
5785         finally:
5786             self._create_domain_cleanup_lxc(instance)
5787 
5788     # TODO(sahid): Consider renaming this to _create_guest.
5789     def _create_domain(self, xml=None, domain=None,
5790                        power_on=True, pause=False, post_xml_callback=None):
5791         """Create a domain.
5792 
5793         Either domain or xml must be passed in. If both are passed, then
5794         the domain definition is overwritten from the xml.
5795 
5796         :returns guest.Guest: Guest just created
5797         """
5798         if xml:
5799             guest = libvirt_guest.Guest.create(xml, self._host)
5800             if post_xml_callback is not None:
5801                 post_xml_callback()
5802         else:
5803             guest = libvirt_guest.Guest(domain)
5804 
5805         if power_on or pause:
5806             guest.launch(pause=pause)
5807 
5808         if not utils.is_neutron():
5809             guest.enable_hairpin()
5810 
5811         return guest
5812 
5813     def _neutron_failed_callback(self, event_name, instance):
5814         LOG.error('Neutron Reported failure on event '
5815                   '%(event)s for instance %(uuid)s',
5816                   {'event': event_name, 'uuid': instance.uuid},
5817                   instance=instance)
5818         if CONF.vif_plugging_is_fatal:
5819             raise exception.VirtualInterfaceCreateException()
5820 
5821     def _get_neutron_events(self, network_info):
5822         # NOTE(danms): We need to collect any VIFs that are currently
5823         # down that we expect a down->up event for. Anything that is
5824         # already up will not undergo that transition, and for
5825         # anything that might be stale (cache-wise) assume it's
5826         # already up so we don't block on it.
5827         return [('network-vif-plugged', vif['id'])
5828                 for vif in network_info if vif.get('active', True) is False]
5829 
5830     def _cleanup_failed_start(self, context, instance, network_info,
5831                               block_device_info, guest, destroy_disks):
5832         try:
5833             if guest and guest.is_active():
5834                 guest.poweroff()
5835         finally:
5836             self.cleanup(context, instance, network_info=network_info,
5837                          block_device_info=block_device_info,
5838                          destroy_disks=destroy_disks)
5839 
5840     def _create_domain_and_network(self, context, xml, instance, network_info,
5841                                    block_device_info=None, power_on=True,
5842                                    vifs_already_plugged=False,
5843                                    post_xml_callback=None,
5844                                    destroy_disks_on_failure=False,
5845                                    external_events=None):
5846 
5847         """Do required network setup and create domain."""
5848         timeout = CONF.vif_plugging_timeout
5849         if (self._conn_supports_start_paused and utils.is_neutron() and not
5850                 vifs_already_plugged and power_on and timeout):
5851             events = (external_events if external_events
5852                       else self._get_neutron_events(network_info))
5853         else:
5854             events = []
5855 
5856         pause = bool(events)
5857         guest = None
5858         try:
5859             with self.virtapi.wait_for_instance_event(
5860                     instance, events, deadline=timeout,
5861                     error_callback=self._neutron_failed_callback):
5862                 self.plug_vifs(instance, network_info)
5863                 self.firewall_driver.setup_basic_filtering(instance,
5864                                                            network_info)
5865                 self.firewall_driver.prepare_instance_filter(instance,
5866                                                              network_info)
5867                 with self._lxc_disk_handler(context, instance,
5868                                             instance.image_meta,
5869                                             block_device_info):
5870                     guest = self._create_domain(
5871                         xml, pause=pause, power_on=power_on,
5872                         post_xml_callback=post_xml_callback)
5873 
5874                 self.firewall_driver.apply_instance_filter(instance,
5875                                                            network_info)
5876         except exception.VirtualInterfaceCreateException:
5877             # Neutron reported failure and we didn't swallow it, so
5878             # bail here
5879             with excutils.save_and_reraise_exception():
5880                 self._cleanup_failed_start(context, instance, network_info,
5881                                            block_device_info, guest,
5882                                            destroy_disks_on_failure)
5883         except eventlet.timeout.Timeout:
5884             # We never heard from Neutron
5885             LOG.warning('Timeout waiting for %(events)s for '
5886                         'instance with vm_state %(vm_state)s and '
5887                         'task_state %(task_state)s.',
5888                         {'events': events,
5889                          'vm_state': instance.vm_state,
5890                          'task_state': instance.task_state},
5891                         instance=instance)
5892             if CONF.vif_plugging_is_fatal:
5893                 self._cleanup_failed_start(context, instance, network_info,
5894                                            block_device_info, guest,
5895                                            destroy_disks_on_failure)
5896                 raise exception.VirtualInterfaceCreateException()
5897         except Exception:
5898             # Any other error, be sure to clean up
5899             LOG.error('Failed to start libvirt guest', instance=instance)
5900             with excutils.save_and_reraise_exception():
5901                 self._cleanup_failed_start(context, instance, network_info,
5902                                            block_device_info, guest,
5903                                            destroy_disks_on_failure)
5904 
5905         # Resume only if domain has been paused
5906         if pause:
5907             guest.resume()
5908         return guest
5909 
5910     def _get_vcpu_total(self):
5911         """Get available vcpu number of physical computer.
5912 
5913         :returns: the number of cpu core instances can be used.
5914 
5915         """
5916         try:
5917             total_pcpus = self._host.get_cpu_count()
5918         except libvirt.libvirtError:
5919             LOG.warning("Cannot get the number of cpu, because this "
5920                         "function is not implemented for this platform.")
5921             return 0
5922 
5923         if not CONF.vcpu_pin_set:
5924             return total_pcpus
5925 
5926         available_ids = hardware.get_vcpu_pin_set()
5927         online_pcpus = self._host.get_online_cpus()
5928         if not (available_ids <= online_pcpus):
5929             msg = _("Invalid 'vcpu_pin_set' config: one or more of the "
5930                     "requested CPUs is not online. Online cpuset(s): "
5931                     "%(online)s, requested cpuset(s): %(req)s")
5932             raise exception.Invalid(msg % {
5933                 'online': sorted(online_pcpus),
5934                 'req': sorted(available_ids)})
5935 
5936         return len(available_ids)
5937 
5938     @staticmethod
5939     def _get_local_gb_info():
5940         """Get local storage info of the compute node in GB.
5941 
5942         :returns: A dict containing:
5943              :total: How big the overall usable filesystem is (in gigabytes)
5944              :free: How much space is free (in gigabytes)
5945              :used: How much space is used (in gigabytes)
5946         """
5947 
5948         if CONF.libvirt.images_type == 'lvm':
5949             info = lvm.get_volume_group_info(
5950                                CONF.libvirt.images_volume_group)
5951         elif CONF.libvirt.images_type == 'rbd':
5952             info = rbd_utils.RBDDriver().get_pool_info()
5953         else:
5954             info = libvirt_utils.get_fs_info(CONF.instances_path)
5955 
5956         for (k, v) in info.items():
5957             info[k] = v / units.Gi
5958 
5959         return info
5960 
5961     def _get_vcpu_used(self):
5962         """Get vcpu usage number of physical computer.
5963 
5964         :returns: The total number of vcpu(s) that are currently being used.
5965 
5966         """
5967 
5968         total = 0
5969 
5970         # Not all libvirt drivers will support the get_vcpus_info()
5971         #
5972         # For example, LXC does not have a concept of vCPUs, while
5973         # QEMU (TCG) traditionally handles all vCPUs in a single
5974         # thread. So both will report an exception when the vcpus()
5975         # API call is made. In such a case we should report the
5976         # guest as having 1 vCPU, since that lets us still do
5977         # CPU over commit calculations that apply as the total
5978         # guest count scales.
5979         #
5980         # It is also possible that we might see an exception if
5981         # the guest is just in middle of shutting down. Technically
5982         # we should report 0 for vCPU usage in this case, but we
5983         # we can't reliably distinguish the vcpu not supported
5984         # case from the just shutting down case. Thus we don't know
5985         # whether to report 1 or 0 for vCPU count.
5986         #
5987         # Under-reporting vCPUs is bad because it could conceivably
5988         # let the scheduler place too many guests on the host. Over-
5989         # reporting vCPUs is not a problem as it'll auto-correct on
5990         # the next refresh of usage data.
5991         #
5992         # Thus when getting an exception we always report 1 as the
5993         # vCPU count, as the least worst value.
5994         for guest in self._host.list_guests():
5995             try:
5996                 vcpus = guest.get_vcpus_info()
5997                 total += len(list(vcpus))
5998             except libvirt.libvirtError:
5999                 total += 1
6000             # NOTE(gtt116): give other tasks a chance.
6001             greenthread.sleep(0)
6002         return total
6003 
6004     def _get_supported_vgpu_types(self):
6005         if not CONF.devices.enabled_vgpu_types:
6006             return []
6007         # TODO(sbauza): Move this check up to compute_manager.init_host
6008         if len(CONF.devices.enabled_vgpu_types) > 1:
6009             LOG.warning('libvirt only supports one GPU type per compute node,'
6010                         ' only first type will be used.')
6011         requested_types = CONF.devices.enabled_vgpu_types[:1]
6012         return requested_types
6013 
6014     def _count_mediated_devices(self, enabled_vgpu_types):
6015         """Counts the sysfs objects (handles) that represent a mediated device
6016         and filtered by $enabled_vgpu_types.
6017 
6018         Those handles can be in use by a libvirt guest or not.
6019 
6020         :param enabled_vgpu_types: list of enabled VGPU types on this host
6021         :returns: dict, keyed by parent GPU libvirt PCI device ID, of number of
6022         mdev device handles for that GPU
6023         """
6024 
6025         counts_per_parent = collections.defaultdict(int)
6026         mediated_devices = self._get_mediated_devices(types=enabled_vgpu_types)
6027         for mdev in mediated_devices:
6028             counts_per_parent[mdev['parent']] += 1
6029         return counts_per_parent
6030 
6031     def _count_mdev_capable_devices(self, enabled_vgpu_types):
6032         """Counts the mdev-capable devices on this host filtered by
6033         $enabled_vgpu_types.
6034 
6035         :param enabled_vgpu_types: list of enabled VGPU types on this host
6036         :returns: dict, keyed by device name, to an integer count of available
6037             instances of each type per device
6038         """
6039         mdev_capable_devices = self._get_mdev_capable_devices(
6040             types=enabled_vgpu_types)
6041         counts_per_dev = collections.defaultdict(int)
6042         for dev in mdev_capable_devices:
6043             # dev_id is the libvirt name for the PCI device,
6044             # eg. pci_0000_84_00_0 which matches a PCI address of 0000:84:00.0
6045             dev_name = dev['dev_id']
6046             for _type in dev['types']:
6047                 available = dev['types'][_type]['availableInstances']
6048                 # TODO(sbauza): Once we support multiple types, check which
6049                 # PCI devices are set for this type
6050                 # NOTE(sbauza): Even if we support multiple types, Nova will
6051                 # only use one per physical GPU.
6052                 counts_per_dev[dev_name] += available
6053         return counts_per_dev
6054 
6055     def _get_gpu_inventories(self):
6056         """Returns the inventories for each physical GPU for a specific type
6057         supported by the enabled_vgpu_types CONF option.
6058 
6059         :returns: dict, keyed by libvirt PCI name, of dicts like:
6060                 {'pci_0000_84_00_0':
6061                     {'total': $TOTAL,
6062                      'min_unit': 1,
6063                      'max_unit': $TOTAL,
6064                      'step_size': 1,
6065                      'reserved': 0,
6066                      'allocation_ratio': 1.0,
6067                     }
6068                 }
6069         """
6070 
6071         # Bail out early if operator doesn't care about providing vGPUs
6072         enabled_vgpu_types = self._get_supported_vgpu_types()
6073         if not enabled_vgpu_types:
6074             return {}
6075         inventories = {}
6076         count_per_parent = self._count_mediated_devices(enabled_vgpu_types)
6077         for dev_name, count in count_per_parent.items():
6078             inventories[dev_name] = {'total': count}
6079         # Filter how many available mdevs we can create for all the supported
6080         # types.
6081         count_per_dev = self._count_mdev_capable_devices(enabled_vgpu_types)
6082         # Combine the counts into the dict that we return to the caller.
6083         for dev_name, count in count_per_dev.items():
6084             inv_per_parent = inventories.setdefault(
6085                 dev_name, {'total': 0})
6086             inv_per_parent['total'] += count
6087             inv_per_parent.update({
6088                 'min_unit': 1,
6089                 'step_size': 1,
6090                 'reserved': 0,
6091                 # NOTE(sbauza): There is no sense to have a ratio but 1.0
6092                 # since we can't overallocate vGPU resources
6093                 'allocation_ratio': 1.0,
6094                 # FIXME(sbauza): Some vendors could support only one
6095                 'max_unit': inv_per_parent['total'],
6096             })
6097 
6098         return inventories
6099 
6100     def _get_instance_capabilities(self):
6101         """Get hypervisor instance capabilities
6102 
6103         Returns a list of tuples that describe instances the
6104         hypervisor is capable of hosting.  Each tuple consists
6105         of the triplet (arch, hypervisor_type, vm_mode).
6106 
6107         :returns: List of tuples describing instance capabilities
6108         """
6109         caps = self._host.get_capabilities()
6110         instance_caps = list()
6111         for g in caps.guests:
6112             for domain_type in g.domains:
6113                 try:
6114                     instance_cap = (
6115                         fields.Architecture.canonicalize(g.arch),
6116                         fields.HVType.canonicalize(domain_type),
6117                         fields.VMMode.canonicalize(g.ostype))
6118                     instance_caps.append(instance_cap)
6119                 except exception.InvalidArchitectureName:
6120                     # NOTE(danms): Libvirt is exposing a guest arch that nova
6121                     # does not even know about. Avoid aborting here and
6122                     # continue to process the rest.
6123                     pass
6124 
6125         return instance_caps
6126 
6127     def _get_cpu_info(self):
6128         """Get cpuinfo information.
6129 
6130         Obtains cpu feature from virConnect.getCapabilities.
6131 
6132         :return: see above description
6133 
6134         """
6135 
6136         caps = self._host.get_capabilities()
6137         cpu_info = dict()
6138 
6139         cpu_info['arch'] = caps.host.cpu.arch
6140         cpu_info['model'] = caps.host.cpu.model
6141         cpu_info['vendor'] = caps.host.cpu.vendor
6142 
6143         topology = dict()
6144         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
6145         topology['sockets'] = caps.host.cpu.sockets
6146         topology['cores'] = caps.host.cpu.cores
6147         topology['threads'] = caps.host.cpu.threads
6148         cpu_info['topology'] = topology
6149 
6150         features = set()
6151         for f in caps.host.cpu.features:
6152             features.add(f.name)
6153         cpu_info['features'] = features
6154         return cpu_info
6155 
6156     def _get_pcinet_info(self, vf_address):
6157         """Returns a dict of NET device."""
6158         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
6159         if not devname:
6160             return
6161 
6162         virtdev = self._host.device_lookup_by_name(devname)
6163         xmlstr = virtdev.XMLDesc(0)
6164         cfgdev = vconfig.LibvirtConfigNodeDevice()
6165         cfgdev.parse_str(xmlstr)
6166         return {'name': cfgdev.name,
6167                 'capabilities': cfgdev.pci_capability.features}
6168 
6169     def _get_pcidev_info(self, devname):
6170         """Returns a dict of PCI device."""
6171 
6172         def _get_device_type(cfgdev, pci_address):
6173             """Get a PCI device's device type.
6174 
6175             An assignable PCI device can be a normal PCI device,
6176             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
6177             Function (VF). Only normal PCI devices or SR-IOV VFs
6178             are assignable, while SR-IOV PFs are always owned by
6179             hypervisor.
6180             """
6181             for fun_cap in cfgdev.pci_capability.fun_capability:
6182                 if fun_cap.type == 'virt_functions':
6183                     return {
6184                         'dev_type': fields.PciDeviceType.SRIOV_PF,
6185                     }
6186                 if (fun_cap.type == 'phys_function' and
6187                     len(fun_cap.device_addrs) != 0):
6188                     phys_address = "%04x:%02x:%02x.%01x" % (
6189                         fun_cap.device_addrs[0][0],
6190                         fun_cap.device_addrs[0][1],
6191                         fun_cap.device_addrs[0][2],
6192                         fun_cap.device_addrs[0][3])
6193                     result = {
6194                         'dev_type': fields.PciDeviceType.SRIOV_VF,
6195                         'parent_addr': phys_address,
6196                     }
6197                     parent_ifname = None
6198                     try:
6199                         parent_ifname = pci_utils.get_ifname_by_pci_address(
6200                             pci_address, pf_interface=True)
6201                     except exception.PciDeviceNotFoundById:
6202                         # NOTE(sean-k-mooney): we ignore this error as it
6203                         # is expected when the virtual function is not a NIC.
6204                         pass
6205                     if parent_ifname:
6206                         result['parent_ifname'] = parent_ifname
6207                     return result
6208 
6209             return {'dev_type': fields.PciDeviceType.STANDARD}
6210 
6211         def _get_device_capabilities(device, address):
6212             """Get PCI VF device's additional capabilities.
6213 
6214             If a PCI device is a virtual function, this function reads the PCI
6215             parent's network capabilities (must be always a NIC device) and
6216             appends this information to the device's dictionary.
6217             """
6218             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
6219                 pcinet_info = self._get_pcinet_info(address)
6220                 if pcinet_info:
6221                     return {'capabilities':
6222                                 {'network': pcinet_info.get('capabilities')}}
6223             return {}
6224 
6225         virtdev = self._host.device_lookup_by_name(devname)
6226         xmlstr = virtdev.XMLDesc(0)
6227         cfgdev = vconfig.LibvirtConfigNodeDevice()
6228         cfgdev.parse_str(xmlstr)
6229 
6230         address = "%04x:%02x:%02x.%1x" % (
6231             cfgdev.pci_capability.domain,
6232             cfgdev.pci_capability.bus,
6233             cfgdev.pci_capability.slot,
6234             cfgdev.pci_capability.function)
6235 
6236         device = {
6237             "dev_id": cfgdev.name,
6238             "address": address,
6239             "product_id": "%04x" % cfgdev.pci_capability.product_id,
6240             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
6241             }
6242 
6243         device["numa_node"] = cfgdev.pci_capability.numa_node
6244 
6245         # requirement by DataBase Model
6246         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
6247         device.update(_get_device_type(cfgdev, address))
6248         device.update(_get_device_capabilities(device, address))
6249         return device
6250 
6251     def _get_pci_passthrough_devices(self):
6252         """Get host PCI devices information.
6253 
6254         Obtains pci devices information from libvirt, and returns
6255         as a JSON string.
6256 
6257         Each device information is a dictionary, with mandatory keys
6258         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
6259         'label' and other optional device specific information.
6260 
6261         Refer to the objects/pci_device.py for more idea of these keys.
6262 
6263         :returns: a JSON string containing a list of the assignable PCI
6264                   devices information
6265         """
6266         # Bail early if we know we can't support `listDevices` to avoid
6267         # repeated warnings within a periodic task
6268         if not getattr(self, '_list_devices_supported', True):
6269             return jsonutils.dumps([])
6270 
6271         try:
6272             dev_names = self._host.list_pci_devices() or []
6273         except libvirt.libvirtError as ex:
6274             error_code = ex.get_error_code()
6275             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6276                 self._list_devices_supported = False
6277                 LOG.warning("URI %(uri)s does not support "
6278                             "listDevices: %(error)s",
6279                             {'uri': self._uri(),
6280                              'error': encodeutils.exception_to_unicode(ex)})
6281                 return jsonutils.dumps([])
6282             else:
6283                 raise
6284 
6285         pci_info = []
6286         for name in dev_names:
6287             pci_info.append(self._get_pcidev_info(name))
6288 
6289         return jsonutils.dumps(pci_info)
6290 
6291     def _get_mdev_capabilities_for_dev(self, devname, types=None):
6292         """Returns a dict of MDEV capable device with the ID as first key
6293         and then a list of supported types, each of them being a dict.
6294 
6295         :param types: Only return those specific types.
6296         """
6297         virtdev = self._host.device_lookup_by_name(devname)
6298         xmlstr = virtdev.XMLDesc(0)
6299         cfgdev = vconfig.LibvirtConfigNodeDevice()
6300         cfgdev.parse_str(xmlstr)
6301 
6302         device = {
6303             "dev_id": cfgdev.name,
6304             "types": {},
6305             "vendor_id": cfgdev.pci_capability.vendor_id,
6306         }
6307         for mdev_cap in cfgdev.pci_capability.mdev_capability:
6308             for cap in mdev_cap.mdev_types:
6309                 if not types or cap['type'] in types:
6310                     device["types"].update({cap['type']: {
6311                         'availableInstances': cap['availableInstances'],
6312                         'name': cap['name'],
6313                         'deviceAPI': cap['deviceAPI']}})
6314         return device
6315 
6316     def _get_mdev_capable_devices(self, types=None):
6317         """Get host devices supporting mdev types.
6318 
6319         Obtain devices information from libvirt and returns a list of
6320         dictionaries.
6321 
6322         :param types: Filter only devices supporting those types.
6323         """
6324         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6325             return []
6326         dev_names = self._host.list_mdev_capable_devices() or []
6327         mdev_capable_devices = []
6328         for name in dev_names:
6329             device = self._get_mdev_capabilities_for_dev(name, types)
6330             if not device["types"]:
6331                 continue
6332             mdev_capable_devices.append(device)
6333         return mdev_capable_devices
6334 
6335     def _get_mediated_device_information(self, devname):
6336         """Returns a dict of a mediated device."""
6337         virtdev = self._host.device_lookup_by_name(devname)
6338         xmlstr = virtdev.XMLDesc(0)
6339         cfgdev = vconfig.LibvirtConfigNodeDevice()
6340         cfgdev.parse_str(xmlstr)
6341 
6342         device = {
6343             "dev_id": cfgdev.name,
6344             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6345             "uuid": libvirt_utils.mdev_name2uuid(cfgdev.name),
6346             # the physical GPU PCI device
6347             "parent": cfgdev.parent,
6348             "type": cfgdev.mdev_information.type,
6349             "iommu_group": cfgdev.mdev_information.iommu_group,
6350         }
6351         return device
6352 
6353     def _get_mediated_devices(self, types=None):
6354         """Get host mediated devices.
6355 
6356         Obtain devices information from libvirt and returns a list of
6357         dictionaries.
6358 
6359         :param types: Filter only devices supporting those types.
6360         """
6361         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6362             return []
6363         dev_names = self._host.list_mediated_devices() or []
6364         mediated_devices = []
6365         for name in dev_names:
6366             device = self._get_mediated_device_information(name)
6367             if not types or device["type"] in types:
6368                 mediated_devices.append(device)
6369         return mediated_devices
6370 
6371     def _get_all_assigned_mediated_devices(self, instance=None):
6372         """Lookup all instances from the host and return all the mediated
6373         devices that are assigned to a guest.
6374 
6375         :param instance: Only return mediated devices for that instance.
6376 
6377         :returns: A dictionary of keys being mediated device UUIDs and their
6378                   respective values the instance UUID of the guest using it.
6379                   Returns an empty dict if an instance is provided but not
6380                   found in the hypervisor.
6381         """
6382         allocated_mdevs = {}
6383         if instance:
6384             # NOTE(sbauza): In some cases (like a migration issue), the
6385             # instance can exist in the Nova database but libvirt doesn't know
6386             # about it. For such cases, the way to fix that is to hard reboot
6387             # the instance, which will recreate the libvirt guest.
6388             # For that reason, we need to support that case by making sure
6389             # we don't raise an exception if the libvirt guest doesn't exist.
6390             try:
6391                 guest = self._host.get_guest(instance)
6392             except exception.InstanceNotFound:
6393                 # Bail out early if libvirt doesn't know about it since we
6394                 # can't know the existing mediated devices
6395                 return {}
6396             guests = [guest]
6397         else:
6398             guests = self._host.list_guests(only_running=False)
6399         for guest in guests:
6400             cfg = guest.get_config()
6401             for device in cfg.devices:
6402                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6403                     allocated_mdevs[device.uuid] = guest.uuid
6404         return allocated_mdevs
6405 
6406     @staticmethod
6407     def _vgpu_allocations(allocations):
6408         """Filtering only the VGPU allocations from a list of allocations.
6409 
6410         :param allocations: Information about resources allocated to the
6411                             instance via placement, of the form returned by
6412                             SchedulerReportClient.get_allocations_for_consumer.
6413         """
6414         if not allocations:
6415             # If no allocations, there is no vGPU request.
6416             return {}
6417         RC_VGPU = orc.VGPU
6418         vgpu_allocations = {}
6419         for rp in allocations:
6420             res = allocations[rp]['resources']
6421             if RC_VGPU in res and res[RC_VGPU] > 0:
6422                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6423         return vgpu_allocations
6424 
6425     def _get_existing_mdevs_not_assigned(self, requested_types=None,
6426                                          parent=None):
6427         """Returns the already created mediated devices that are not assigned
6428         to a guest yet.
6429 
6430         :param requested_types: Filter out the result for only mediated devices
6431                                 having those types.
6432         :param parent: Filter out result for only mdevs from the parent device.
6433         """
6434         allocated_mdevs = self._get_all_assigned_mediated_devices()
6435         mdevs = self._get_mediated_devices(requested_types)
6436         available_mdevs = set()
6437         for mdev in mdevs:
6438             if parent is None or mdev['parent'] == parent:
6439                 available_mdevs.add(mdev["uuid"])
6440 
6441         available_mdevs -= set(allocated_mdevs)
6442         return available_mdevs
6443 
6444     def _create_new_mediated_device(self, requested_types, uuid=None,
6445                                     parent=None):
6446         """Find a physical device that can support a new mediated device and
6447         create it.
6448 
6449         :param requested_types: Filter only capable devices supporting those
6450                                 types.
6451         :param uuid: The possible mdev UUID we want to create again
6452         :param parent: Only create a mdev for this device
6453 
6454         :returns: the newly created mdev UUID or None if not possible
6455         """
6456         # Try to see if we can still create a new mediated device
6457         devices = self._get_mdev_capable_devices(requested_types)
6458         for device in devices:
6459             # For the moment, the libvirt driver only supports one
6460             # type per host
6461             # TODO(sbauza): Once we support more than one type, make
6462             # sure we look at the flavor/trait for the asked type.
6463             asked_type = requested_types[0]
6464             if device['types'][asked_type]['availableInstances'] > 0:
6465                 # That physical GPU has enough room for a new mdev
6466                 dev_name = device['dev_id']
6467                 # the parent attribute can be None
6468                 if parent is not None and dev_name != parent:
6469                     # The device is not the one that was called, not creating
6470                     # the mdev
6471                     continue
6472                 # We need the PCI address, not the libvirt name
6473                 # The libvirt name is like 'pci_0000_84_00_0'
6474                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6475                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6476                                                                asked_type,
6477                                                                uuid=uuid)
6478                 return chosen_mdev
6479 
6480     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6481     def _allocate_mdevs(self, allocations):
6482         """Returns a list of mediated device UUIDs corresponding to available
6483         resources we can assign to the guest(s) corresponding to the allocation
6484         requests passed as argument.
6485 
6486         That method can either find an existing but unassigned mediated device
6487         it can allocate, or create a new mediated device from a capable
6488         physical device if the latter has enough left capacity.
6489 
6490         :param allocations: Information about resources allocated to the
6491                             instance via placement, of the form returned by
6492                             SchedulerReportClient.get_allocations_for_consumer.
6493                             That code is supporting Placement API version 1.12
6494         """
6495         vgpu_allocations = self._vgpu_allocations(allocations)
6496         if not vgpu_allocations:
6497             return
6498         # TODO(sbauza): Once we have nested resource providers, find which one
6499         # is having the related allocation for the specific VGPU type.
6500         # For the moment, we should only have one allocation for
6501         # ResourceProvider.
6502         # TODO(sbauza): Iterate over all the allocations once we have
6503         # nested Resource Providers. For the moment, just take the first.
6504         if len(vgpu_allocations) > 1:
6505             LOG.warning('More than one allocation was passed over to libvirt '
6506                         'while at the moment libvirt only supports one. Only '
6507                         'the first allocation will be looked up.')
6508         rp_uuid, alloc = six.next(six.iteritems(vgpu_allocations))
6509         vgpus_asked = alloc['resources'][orc.VGPU]
6510 
6511         # Find if we allocated against a specific pGPU (and then the allocation
6512         # is made against a child RP) or any pGPU (in case the VGPU inventory
6513         # is still on the root RP)
6514         try:
6515             allocated_rp = self.provider_tree.data(rp_uuid)
6516         except ValueError:
6517             # The provider doesn't exist, return a better understandable
6518             # exception
6519             raise exception.ComputeResourcesUnavailable(
6520                 reason='vGPU resource is not available')
6521         # TODO(sbauza): Remove this conditional in Train once all VGPU
6522         # inventories are related to a child RP
6523         if allocated_rp.parent_uuid is None:
6524             # We are on a root RP
6525             parent_device = None
6526         else:
6527             rp_name = allocated_rp.name
6528             # There can be multiple roots, we need to find the root name
6529             # to guess the physical device name
6530             roots = list(self.provider_tree.roots)
6531             for root in roots:
6532                 if rp_name.startswith(root.name + '_'):
6533                     # The RP name convention is :
6534                     #    root_name + '_' + parent_device
6535                     parent_device = rp_name[len(root.name) + 1:]
6536                     break
6537             else:
6538                 LOG.warning(
6539                     "pGPU device name %(name)s can't be guessed from the "
6540                     "ProviderTree roots %(roots)s",
6541                     {'name': rp_name,
6542                      'roots': ', '.join([root.name for root in roots])})
6543                 # We f... have no idea what was the parent device
6544                 # If we can't find devices having available VGPUs, just raise
6545                 raise exception.ComputeResourcesUnavailable(
6546                     reason='vGPU resource is not available')
6547 
6548         requested_types = self._get_supported_vgpu_types()
6549         # Which mediated devices are created but not assigned to a guest ?
6550         mdevs_available = self._get_existing_mdevs_not_assigned(
6551             requested_types, parent_device)
6552 
6553         chosen_mdevs = []
6554         for c in six.moves.range(vgpus_asked):
6555             chosen_mdev = None
6556             if mdevs_available:
6557                 # Take the first available mdev
6558                 chosen_mdev = mdevs_available.pop()
6559             else:
6560                 chosen_mdev = self._create_new_mediated_device(
6561                     requested_types, parent=parent_device)
6562             if not chosen_mdev:
6563                 # If we can't find devices having available VGPUs, just raise
6564                 raise exception.ComputeResourcesUnavailable(
6565                     reason='vGPU resource is not available')
6566             else:
6567                 chosen_mdevs.append(chosen_mdev)
6568         return chosen_mdevs
6569 
6570     def _detach_mediated_devices(self, guest):
6571         mdevs = guest.get_all_devices(
6572             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6573         for mdev_cfg in mdevs:
6574             try:
6575                 guest.detach_device(mdev_cfg, live=True)
6576             except libvirt.libvirtError as ex:
6577                 error_code = ex.get_error_code()
6578                 # NOTE(sbauza): There is a pending issue with libvirt that
6579                 # doesn't allow to hot-unplug mediated devices. Let's
6580                 # short-circuit the suspend action and set the instance back
6581                 # to ACTIVE.
6582                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6583                 # operation to support reallocating mediated devices.
6584                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6585                     reason = _("Suspend is not supported for instances having "
6586                                "attached vGPUs.")
6587                     raise exception.InstanceFaultRollback(
6588                         exception.InstanceSuspendFailure(reason=reason))
6589                 else:
6590                     raise
6591 
6592     def _has_numa_support(self):
6593         # This means that the host can support LibvirtConfigGuestNUMATune
6594         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6595         caps = self._host.get_capabilities()
6596 
6597         if (caps.host.cpu.arch in (fields.Architecture.I686,
6598                                    fields.Architecture.X86_64,
6599                                    fields.Architecture.AARCH64) and
6600                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6601             return True
6602         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6603                                      fields.Architecture.PPC64LE)):
6604             return True
6605 
6606         return False
6607 
6608     def _get_host_numa_topology(self):
6609         if not self._has_numa_support():
6610             return
6611 
6612         caps = self._host.get_capabilities()
6613         topology = caps.host.topology
6614 
6615         if topology is None or not topology.cells:
6616             return
6617 
6618         cells = []
6619         allowed_cpus = hardware.get_vcpu_pin_set()
6620         online_cpus = self._host.get_online_cpus()
6621         if allowed_cpus:
6622             allowed_cpus &= online_cpus
6623         else:
6624             allowed_cpus = online_cpus
6625 
6626         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6627             cell = self._reserved_hugepages.get(cell_id, {})
6628             return cell.get(page_size, 0)
6629 
6630         def _get_physnet_numa_affinity():
6631             affinities = {cell.id: set() for cell in topology.cells}
6632             for physnet in CONF.neutron.physnets:
6633                 # This will error out if the group is not registered, which is
6634                 # exactly what we want as that would be a bug
6635                 group = getattr(CONF, 'neutron_physnet_%s' % physnet)
6636 
6637                 if not group.numa_nodes:
6638                     msg = ("the physnet '%s' was listed in '[neutron] "
6639                            "physnets' but no corresponding "
6640                            "'[neutron_physnet_%s] numa_nodes' option was "
6641                            "defined." % (physnet, physnet))
6642                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6643 
6644                 for node in group.numa_nodes:
6645                     if node not in affinities:
6646                         msg = ("node %d for physnet %s is not present in host "
6647                                "affinity set %r" % (node, physnet, affinities))
6648                         # The config option referenced an invalid node
6649                         raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6650                     affinities[node].add(physnet)
6651 
6652             return affinities
6653 
6654         def _get_tunnel_numa_affinity():
6655             affinities = {cell.id: False for cell in topology.cells}
6656 
6657             for node in CONF.neutron_tunnel.numa_nodes:
6658                 if node not in affinities:
6659                     msg = ("node %d for tunneled networks is not present "
6660                            "in host affinity set %r" % (node, affinities))
6661                     # The config option referenced an invalid node
6662                     raise exception.InvalidNetworkNUMAAffinity(reason=msg)
6663                 affinities[node] = True
6664 
6665             return affinities
6666 
6667         physnet_affinities = _get_physnet_numa_affinity()
6668         tunnel_affinities = _get_tunnel_numa_affinity()
6669 
6670         for cell in topology.cells:
6671             cpuset = set(cpu.id for cpu in cell.cpus)
6672             siblings = sorted(map(set,
6673                                   set(tuple(cpu.siblings)
6674                                         if cpu.siblings else ()
6675                                       for cpu in cell.cpus)
6676                                   ))
6677             cpuset &= allowed_cpus
6678             siblings = [sib & allowed_cpus for sib in siblings]
6679             # Filter out empty sibling sets that may be left
6680             siblings = [sib for sib in siblings if len(sib) > 0]
6681 
6682             mempages = [
6683                 objects.NUMAPagesTopology(
6684                     size_kb=pages.size,
6685                     total=pages.total,
6686                     used=0,
6687                     reserved=_get_reserved_memory_for_cell(
6688                         self, cell.id, pages.size))
6689                 for pages in cell.mempages]
6690 
6691             network_metadata = objects.NetworkMetadata(
6692                 physnets=physnet_affinities[cell.id],
6693                 tunneled=tunnel_affinities[cell.id])
6694 
6695             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6696                                     memory=cell.memory / units.Ki,
6697                                     cpu_usage=0, memory_usage=0,
6698                                     siblings=siblings,
6699                                     pinned_cpus=set([]),
6700                                     mempages=mempages,
6701                                     network_metadata=network_metadata)
6702             cells.append(cell)
6703 
6704         return objects.NUMATopology(cells=cells)
6705 
6706     def get_all_volume_usage(self, context, compute_host_bdms):
6707         """Return usage info for volumes attached to vms on
6708            a given host.
6709         """
6710         vol_usage = []
6711 
6712         for instance_bdms in compute_host_bdms:
6713             instance = instance_bdms['instance']
6714 
6715             for bdm in instance_bdms['instance_bdms']:
6716                 mountpoint = bdm['device_name']
6717                 if mountpoint.startswith('/dev/'):
6718                     mountpoint = mountpoint[5:]
6719                 volume_id = bdm['volume_id']
6720 
6721                 LOG.debug("Trying to get stats for the volume %s",
6722                           volume_id, instance=instance)
6723                 vol_stats = self.block_stats(instance, mountpoint)
6724 
6725                 if vol_stats:
6726                     stats = dict(volume=volume_id,
6727                                  instance=instance,
6728                                  rd_req=vol_stats[0],
6729                                  rd_bytes=vol_stats[1],
6730                                  wr_req=vol_stats[2],
6731                                  wr_bytes=vol_stats[3])
6732                     LOG.debug(
6733                         "Got volume usage stats for the volume=%(volume)s,"
6734                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6735                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6736                         stats, instance=instance)
6737                     vol_usage.append(stats)
6738 
6739         return vol_usage
6740 
6741     def block_stats(self, instance, disk_id):
6742         """Note that this function takes an instance name."""
6743         try:
6744             guest = self._host.get_guest(instance)
6745             dev = guest.get_block_device(disk_id)
6746             return dev.blockStats()
6747         except libvirt.libvirtError as e:
6748             errcode = e.get_error_code()
6749             LOG.info('Getting block stats failed, device might have '
6750                      'been detached. Instance=%(instance_name)s '
6751                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6752                      {'instance_name': instance.name, 'disk': disk_id,
6753                       'errcode': errcode, 'e': e},
6754                      instance=instance)
6755         except exception.InstanceNotFound:
6756             LOG.info('Could not find domain in libvirt for instance %s. '
6757                      'Cannot get block stats for device', instance.name,
6758                      instance=instance)
6759 
6760     def get_console_pool_info(self, console_type):
6761         # TODO(mdragon): console proxy should be implemented for libvirt,
6762         #                in case someone wants to use it with kvm or
6763         #                such. For now return fake data.
6764         return {'address': '127.0.0.1',
6765                 'username': 'fakeuser',
6766                 'password': 'fakepassword'}
6767 
6768     def refresh_security_group_rules(self, security_group_id):
6769         self.firewall_driver.refresh_security_group_rules(security_group_id)
6770 
6771     def refresh_instance_security_rules(self, instance):
6772         self.firewall_driver.refresh_instance_security_rules(instance)
6773 
6774     def update_provider_tree(self, provider_tree, nodename, allocations=None):
6775         """Update a ProviderTree object with current resource provider,
6776         inventory information and CPU traits.
6777 
6778         :param nova.compute.provider_tree.ProviderTree provider_tree:
6779             A nova.compute.provider_tree.ProviderTree object representing all
6780             the providers in the tree associated with the compute node, and any
6781             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
6782             trait) associated via aggregate with any of those providers (but
6783             not *their* tree- or aggregate-associated providers), as currently
6784             known by placement.
6785         :param nodename:
6786             String name of the compute node (i.e.
6787             ComputeNode.hypervisor_hostname) for which the caller is requesting
6788             updated provider information.
6789         :param allocations:
6790             Dict of allocation data of the form:
6791               { $CONSUMER_UUID: {
6792                     # The shape of each "allocations" dict below is identical
6793                     # to the return from GET /allocations/{consumer_uuid}
6794                     "allocations": {
6795                         $RP_UUID: {
6796                             "generation": $RP_GEN,
6797                             "resources": {
6798                                 $RESOURCE_CLASS: $AMOUNT,
6799                                 ...
6800                             },
6801                         },
6802                         ...
6803                     },
6804                     "project_id": $PROJ_ID,
6805                     "user_id": $USER_ID,
6806                     "consumer_generation": $CONSUMER_GEN,
6807                 },
6808                 ...
6809               }
6810             If None, and the method determines that any inventory needs to be
6811             moved (from one provider to another and/or to a different resource
6812             class), the ReshapeNeeded exception must be raised. Otherwise, this
6813             dict must be edited in place to indicate the desired final state of
6814             allocations.
6815         :raises ReshapeNeeded: If allocations is None and any inventory needs
6816             to be moved from one provider to another and/or to a different
6817             resource class.
6818         :raises: ReshapeFailed if the requested tree reshape fails for
6819             whatever reason.
6820         """
6821         disk_gb = int(self._get_local_gb_info()['total'])
6822         memory_mb = int(self._host.get_memory_mb_total())
6823         vcpus = self._get_vcpu_total()
6824 
6825         # NOTE(yikun): If the inv record does not exists, the allocation_ratio
6826         # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
6827         # is set, and fallback to use the initial_xxx_allocation_ratio
6828         # otherwise.
6829         inv = provider_tree.data(nodename).inventory
6830         ratios = self._get_allocation_ratios(inv)
6831         result = {
6832             orc.VCPU: {
6833                 'total': vcpus,
6834                 'min_unit': 1,
6835                 'max_unit': vcpus,
6836                 'step_size': 1,
6837                 'allocation_ratio': ratios[orc.VCPU],
6838                 'reserved': CONF.reserved_host_cpus,
6839             },
6840             orc.MEMORY_MB: {
6841                 'total': memory_mb,
6842                 'min_unit': 1,
6843                 'max_unit': memory_mb,
6844                 'step_size': 1,
6845                 'allocation_ratio': ratios[orc.MEMORY_MB],
6846                 'reserved': CONF.reserved_host_memory_mb,
6847             },
6848         }
6849 
6850         # If a sharing DISK_GB provider exists in the provider tree, then our
6851         # storage is shared, and we should not report the DISK_GB inventory in
6852         # the compute node provider.
6853         # TODO(efried): Reinstate non-reporting of shared resource by the
6854         # compute RP once the issues from bug #1784020 have been resolved.
6855         if provider_tree.has_sharing_provider(orc.DISK_GB):
6856             LOG.debug('Ignoring sharing provider - see bug #1784020')
6857         result[orc.DISK_GB] = {
6858             'total': disk_gb,
6859             'min_unit': 1,
6860             'max_unit': disk_gb,
6861             'step_size': 1,
6862             'allocation_ratio': ratios[orc.DISK_GB],
6863             'reserved': self._get_reserved_host_disk_gb_from_config(),
6864         }
6865 
6866         # NOTE(sbauza): For the moment, the libvirt driver only supports
6867         # providing the total number of virtual GPUs for a single GPU type. If
6868         # you have multiple physical GPUs, each of them providing multiple GPU
6869         # types, only one type will be used for each of the physical GPUs.
6870         # If one of the pGPUs doesn't support this type, it won't be used.
6871         # TODO(sbauza): Use traits to make a better world.
6872         inventories_dict = self._get_gpu_inventories()
6873         if inventories_dict:
6874             self._update_provider_tree_for_vgpu(
6875                 inventories_dict, provider_tree, nodename,
6876                 allocations=allocations)
6877 
6878         provider_tree.update_inventory(nodename, result)
6879 
6880         traits = self._get_cpu_traits()
6881         if traits is not None:
6882             # _get_cpu_traits returns a dict of trait names mapped to boolean
6883             # values. Add traits equal to True to provider tree, remove
6884             # those False traits from provider tree.
6885             traits_to_add = [t for t in traits if traits[t]]
6886             traits_to_remove = set(traits) - set(traits_to_add)
6887             provider_tree.add_traits(nodename, *traits_to_add)
6888             provider_tree.remove_traits(nodename, *traits_to_remove)
6889 
6890         # Now that we updated the ProviderTree, we want to store it locally
6891         # so that spawn() or other methods can access it thru a getter
6892         self.provider_tree = copy.deepcopy(provider_tree)
6893 
6894     @staticmethod
6895     def _is_reshape_needed_vgpu_on_root(provider_tree, nodename):
6896         """Determine if root RP has VGPU inventories.
6897 
6898         Check to see if the root compute node provider in the tree for
6899         this host already has VGPU inventory because if it does, we either
6900         need to signal for a reshape (if _update_provider_tree_for_vgpu()
6901         has no allocations) or move the allocations within the ProviderTree if
6902         passed.
6903 
6904         :param provider_tree: The ProviderTree object for this host.
6905         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6906             the name of the root node provider in the tree for this host.
6907         :returns: boolean, whether we have VGPU root inventory.
6908         """
6909         root_node = provider_tree.data(nodename)
6910         return orc.VGPU in root_node.inventory
6911 
6912     @staticmethod
6913     def _ensure_pgpu_providers(inventories_dict, provider_tree, nodename):
6914         """Ensures GPU inventory providers exist in the tree for $nodename.
6915 
6916         GPU providers are named $nodename_$gpu-device-id, e.g.
6917         ``somehost.foo.bar.com_pci_0000_84_00_0``.
6918 
6919         :param inventories_dict: Dictionary of inventories for VGPU class
6920             directly provided by _get_gpu_inventories() and which looks like:
6921                 {'pci_0000_84_00_0':
6922                     {'total': $TOTAL,
6923                      'min_unit': 1,
6924                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
6925                      'step_size': 1,
6926                      'reserved': 0,
6927                      'allocation_ratio': 1.0,
6928                     }
6929                 }
6930         :param provider_tree: The ProviderTree to update.
6931         :param nodename: The ComputeNode.hypervisor_hostname, also known as
6932             the name of the root node provider in the tree for this host.
6933         :returns: dict, keyed by GPU device ID, to ProviderData object
6934             representing that resource provider in the tree
6935         """
6936         # Create the VGPU child providers if they do not already exist.
6937         # TODO(mriedem): For the moment, _get_supported_vgpu_types() only
6938         # returns one single type but that will be changed once we support
6939         # multiple types.
6940         # Note that we can't support multiple vgpu types until a reshape has
6941         # been performed on the vgpu resources provided by the root provider,
6942         # if any.
6943 
6944         # Dict of PGPU RPs keyed by their libvirt PCI name
6945         pgpu_rps = {}
6946         for pgpu_dev_id, inventory in inventories_dict.items():
6947             # For each physical GPU, we make sure to have a child provider
6948             pgpu_rp_name = '%s_%s' % (nodename, pgpu_dev_id)
6949             if not provider_tree.exists(pgpu_rp_name):
6950                 # This is the first time creating the child provider so add
6951                 # it to the tree under the root node provider.
6952                 provider_tree.new_child(pgpu_rp_name, nodename)
6953             # We want to idempotently return the resource providers with VGPUs
6954             pgpu_rp = provider_tree.data(pgpu_rp_name)
6955             pgpu_rps[pgpu_dev_id] = pgpu_rp
6956 
6957             # The VGPU inventory goes on a child provider of the given root
6958             # node, identified by $nodename.
6959             pgpu_inventory = {orc.VGPU: inventory}
6960             provider_tree.update_inventory(pgpu_rp_name, pgpu_inventory)
6961         return pgpu_rps
6962 
6963     @staticmethod
6964     def _assert_is_root_provider(
6965             rp_uuid, root_node, consumer_uuid, alloc_data):
6966         """Asserts during a reshape that rp_uuid is for the root node provider.
6967 
6968         When reshaping, inventory and allocations should be on the root node
6969         provider and then moved to child providers.
6970 
6971         :param rp_uuid: UUID of the provider that holds inventory/allocations.
6972         :param root_node: ProviderData object representing the root node in a
6973             provider tree.
6974         :param consumer_uuid: UUID of the consumer (instance) holding resource
6975             allocations against the given rp_uuid provider.
6976         :param alloc_data: dict of allocation data for the consumer.
6977         :raises: ReshapeFailed if rp_uuid is not the root node indicating a
6978             reshape was needed but the inventory/allocation structure is not
6979             expected.
6980         """
6981         if rp_uuid != root_node.uuid:
6982             # Something is wrong - VGPU inventory should
6983             # only be on the root node provider if we are
6984             # reshaping the tree.
6985             msg = (_('Unexpected VGPU resource allocation '
6986                      'on provider %(rp_uuid)s for consumer '
6987                      '%(consumer_uuid)s: %(alloc_data)s. '
6988                      'Expected VGPU allocation to be on root '
6989                      'compute node provider %(root_uuid)s.')
6990                    % {'rp_uuid': rp_uuid,
6991                       'consumer_uuid': consumer_uuid,
6992                       'alloc_data': alloc_data,
6993                       'root_uuid': root_node.uuid})
6994             raise exception.ReshapeFailed(error=msg)
6995 
6996     def _get_assigned_mdevs_for_reshape(
6997             self, instance_uuid, rp_uuid, alloc_data):
6998         """Gets the mediated devices assigned to the instance during a reshape.
6999 
7000         :param instance_uuid: UUID of the instance consuming VGPU resources
7001             on this host.
7002         :param rp_uuid: UUID of the resource provider with VGPU inventory being
7003             consumed by the instance.
7004         :param alloc_data: dict of allocation data for the instance consumer.
7005         :return: list of mediated device UUIDs assigned to the instance
7006         :raises: ReshapeFailed if the instance is not found in the hypervisor
7007             or no mediated devices were found to be assigned to the instance
7008             indicating VGPU allocations are out of sync with the hypervisor
7009         """
7010         # FIXME(sbauza): We don't really need an Instance
7011         # object, but given some libvirt.host logs needs
7012         # to have an instance name, just provide a fake one
7013         Instance = collections.namedtuple('Instance', ['uuid', 'name'])
7014         instance = Instance(uuid=instance_uuid, name=instance_uuid)
7015         mdevs = self._get_all_assigned_mediated_devices(instance)
7016         # _get_all_assigned_mediated_devices returns {} if the instance is
7017         # not found in the hypervisor
7018         if not mdevs:
7019             # If we found a VGPU allocation against a consumer
7020             # which is not an instance, the only left case for
7021             # Nova would be a migration but we don't support
7022             # this at the moment.
7023             msg = (_('Unexpected VGPU resource allocation on provider '
7024                      '%(rp_uuid)s for consumer %(consumer_uuid)s: '
7025                      '%(alloc_data)s. The allocation is made against a '
7026                      'non-existing instance or there are no devices assigned.')
7027                    % {'rp_uuid': rp_uuid, 'consumer_uuid': instance_uuid,
7028                       'alloc_data': alloc_data})
7029             raise exception.ReshapeFailed(error=msg)
7030         return mdevs
7031 
7032     def _count_vgpus_per_pgpu(self, mdev_uuids):
7033         """Count the number of VGPUs per physical GPU mediated device.
7034 
7035         :param mdev_uuids: List of physical GPU mediated device UUIDs.
7036         :return: dict, keyed by PGPU device ID, to count of VGPUs on that
7037             device
7038         """
7039         vgpu_count_per_pgpu = collections.defaultdict(int)
7040         for mdev_uuid in mdev_uuids:
7041             # libvirt name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
7042             dev_name = libvirt_utils.mdev_uuid2name(mdev_uuid)
7043             # Count how many vGPUs are in use for this instance
7044             dev_info = self._get_mediated_device_information(dev_name)
7045             pgpu_dev_id = dev_info['parent']
7046             vgpu_count_per_pgpu[pgpu_dev_id] += 1
7047         return vgpu_count_per_pgpu
7048 
7049     @staticmethod
7050     def _check_vgpu_allocations_match_real_use(
7051             vgpu_count_per_pgpu, expected_usage, rp_uuid, consumer_uuid,
7052             alloc_data):
7053         """Checks that the number of GPU devices assigned to the consumer
7054         matches what is expected from the allocations in the placement service
7055         and logs a warning if there is a mismatch.
7056 
7057         :param vgpu_count_per_pgpu: dict, keyed by PGPU device ID, to count of
7058             VGPUs on that device where each device is assigned to the consumer
7059             (guest instance on this hypervisor)
7060         :param expected_usage: The expected usage from placement for the
7061             given resource provider and consumer
7062         :param rp_uuid: UUID of the resource provider with VGPU inventory being
7063             consumed by the instance
7064         :param consumer_uuid: UUID of the consumer (instance) holding resource
7065             allocations against the given rp_uuid provider
7066         :param alloc_data: dict of allocation data for the instance consumer
7067         """
7068         actual_usage = sum(vgpu_count_per_pgpu.values())
7069         if actual_usage != expected_usage:
7070             # Don't make it blocking, just make sure you actually correctly
7071             # allocate the existing resources
7072             LOG.warning(
7073                 'Unexpected VGPU resource allocation on provider %(rp_uuid)s '
7074                 'for consumer %(consumer_uuid)s: %(alloc_data)s. Allocations '
7075                 '(%(expected_usage)s) differ from actual use '
7076                 '(%(actual_usage)s).',
7077                 {'rp_uuid': rp_uuid, 'consumer_uuid': consumer_uuid,
7078                  'alloc_data': alloc_data, 'expected_usage': expected_usage,
7079                  'actual_usage': actual_usage})
7080 
7081     def _reshape_vgpu_allocations(
7082             self, rp_uuid, root_node, consumer_uuid, alloc_data, resources,
7083             pgpu_rps):
7084         """Update existing VGPU allocations by moving them from the root node
7085         provider to the child provider for the given VGPU provider.
7086 
7087         :param rp_uuid: UUID of the VGPU resource provider with allocations
7088             from consumer_uuid (should be the root node provider before
7089             reshaping occurs)
7090         :param root_node: ProviderData object for the root compute node
7091             resource provider in the provider tree
7092         :param consumer_uuid: UUID of the consumer (instance) with VGPU
7093             allocations against the resource provider represented by rp_uuid
7094         :param alloc_data: dict of allocation information for consumer_uuid
7095         :param resources: dict, keyed by resource class, of resources allocated
7096             to consumer_uuid from rp_uuid
7097         :param pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7098             representing that resource provider in the tree
7099         :raises: ReshapeFailed if the reshape fails for whatever reason
7100         """
7101         # We've found VGPU allocations on a provider. It should be the root
7102         # node provider.
7103         self._assert_is_root_provider(
7104             rp_uuid, root_node, consumer_uuid, alloc_data)
7105 
7106         # Find which physical GPU corresponds to this allocation.
7107         mdev_uuids = self._get_assigned_mdevs_for_reshape(
7108             consumer_uuid, rp_uuid, alloc_data)
7109 
7110         vgpu_count_per_pgpu = self._count_vgpus_per_pgpu(mdev_uuids)
7111 
7112         # We need to make sure we found all the mediated devices that
7113         # correspond to an allocation.
7114         self._check_vgpu_allocations_match_real_use(
7115             vgpu_count_per_pgpu, resources[orc.VGPU],
7116             rp_uuid, consumer_uuid, alloc_data)
7117 
7118         # Add the VGPU allocation for each VGPU provider.
7119         allocs = alloc_data['allocations']
7120         for pgpu_dev_id, pgpu_rp in pgpu_rps.items():
7121             vgpu_count = vgpu_count_per_pgpu[pgpu_dev_id]
7122             if vgpu_count:
7123                 allocs[pgpu_rp.uuid] = {
7124                     'resources': {
7125                         orc.VGPU: vgpu_count
7126                     }
7127                 }
7128         # And remove the VGPU allocation from the root node provider.
7129         del resources[orc.VGPU]
7130 
7131     def _reshape_gpu_resources(
7132             self, allocations, root_node, pgpu_rps):
7133         """Reshapes the provider tree moving VGPU inventory from root to child
7134 
7135         :param allocations:
7136             Dict of allocation data of the form:
7137               { $CONSUMER_UUID: {
7138                     # The shape of each "allocations" dict below is identical
7139                     # to the return from GET /allocations/{consumer_uuid}
7140                     "allocations": {
7141                         $RP_UUID: {
7142                             "generation": $RP_GEN,
7143                             "resources": {
7144                                 $RESOURCE_CLASS: $AMOUNT,
7145                                 ...
7146                             },
7147                         },
7148                         ...
7149                     },
7150                     "project_id": $PROJ_ID,
7151                     "user_id": $USER_ID,
7152                     "consumer_generation": $CONSUMER_GEN,
7153                 },
7154                 ...
7155               }
7156         :params root_node: The root node in the provider tree
7157         :params pgpu_rps: dict, keyed by GPU device ID, to ProviderData object
7158             representing that resource provider in the tree
7159         """
7160         LOG.info('Reshaping tree; moving VGPU allocations from root '
7161                  'provider %s to child providers %s.', root_node.uuid,
7162                  pgpu_rps.values())
7163         # For each consumer in the allocations dict, look for VGPU
7164         # allocations and move them to the VGPU provider.
7165         for consumer_uuid, alloc_data in allocations.items():
7166             # Copy and iterate over the current set of providers to avoid
7167             # modifying keys while iterating.
7168             allocs = alloc_data['allocations']
7169             for rp_uuid in list(allocs):
7170                 resources = allocs[rp_uuid]['resources']
7171                 if orc.VGPU in resources:
7172                     self._reshape_vgpu_allocations(
7173                         rp_uuid, root_node, consumer_uuid, alloc_data,
7174                         resources, pgpu_rps)
7175 
7176     def _update_provider_tree_for_vgpu(self, inventories_dict, provider_tree,
7177                                        nodename, allocations=None):
7178         """Updates the provider tree for VGPU inventory.
7179 
7180         Before Stein, VGPU inventory and allocations were on the root compute
7181         node provider in the tree. Starting in Stein, the VGPU inventory is
7182         on a child provider in the tree. As a result, this method will
7183         "reshape" the tree if necessary on first start of this compute service
7184         in Stein.
7185 
7186         :param inventories_dict: Dictionary of inventories for VGPU class
7187             directly provided by _get_gpu_inventories() and which looks like:
7188                 {'pci_0000_84_00_0':
7189                     {'total': $TOTAL,
7190                      'min_unit': 1,
7191                      'max_unit': $MAX_UNIT, # defaults to $TOTAL
7192                      'step_size': 1,
7193                      'reserved': 0,
7194                      'allocation_ratio': 1.0,
7195                     }
7196                 }
7197         :param provider_tree: The ProviderTree to update.
7198         :param nodename: The ComputeNode.hypervisor_hostname, also known as
7199             the name of the root node provider in the tree for this host.
7200         :param allocations: If not None, indicates a reshape was requested and
7201             should be performed.
7202         :raises: nova.exception.ReshapeNeeded if ``allocations`` is None and
7203             the method determines a reshape of the tree is needed, i.e. VGPU
7204             inventory and allocations must be migrated from the root node
7205             provider to a child provider of VGPU resources in the tree.
7206         :raises: nova.exception.ReshapeFailed if the requested tree reshape
7207             fails for whatever reason.
7208         """
7209         # Check to see if the root compute node provider in the tree for
7210         # this host already has VGPU inventory because if it does, and
7211         # we're not currently reshaping (allocations is None), we need
7212         # to indicate that a reshape is needed to move the VGPU inventory
7213         # onto a child provider in the tree.
7214 
7215         # Ensure GPU providers are in the ProviderTree for the given inventory.
7216         pgpu_rps = self._ensure_pgpu_providers(
7217             inventories_dict, provider_tree, nodename)
7218 
7219         if self._is_reshape_needed_vgpu_on_root(provider_tree, nodename):
7220             if allocations is None:
7221                 # We have old VGPU inventory on root RP, but we haven't yet
7222                 # allocations. That means we need to ask for a reshape.
7223                 LOG.info('Requesting provider tree reshape in order to move '
7224                          'VGPU inventory from the root compute node provider '
7225                          '%s to a child provider.', nodename)
7226                 raise exception.ReshapeNeeded()
7227             # We have allocations, that means we already asked for a reshape
7228             # and the Placement API returned us them. We now need to move
7229             # those from the root RP to the needed children RPs.
7230             root_node = provider_tree.data(nodename)
7231             # Reshape VGPU provider inventory and allocations, moving them
7232             # from the root node provider to the child providers.
7233             self._reshape_gpu_resources(allocations, root_node, pgpu_rps)
7234             # Only delete the root inventory once the reshape is done
7235             if orc.VGPU in root_node.inventory:
7236                 del root_node.inventory[orc.VGPU]
7237                 provider_tree.update_inventory(nodename, root_node.inventory)
7238 
7239     def get_available_resource(self, nodename):
7240         """Retrieve resource information.
7241 
7242         This method is called when nova-compute launches, and
7243         as part of a periodic task that records the results in the DB.
7244 
7245         :param nodename: unused in this driver
7246         :returns: dictionary containing resource info
7247         """
7248 
7249         disk_info_dict = self._get_local_gb_info()
7250         data = {}
7251 
7252         # NOTE(dprince): calling capabilities before getVersion works around
7253         # an initialization issue with some versions of Libvirt (1.0.5.5).
7254         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
7255         # See: https://bugs.launchpad.net/nova/+bug/1215593
7256         data["supported_instances"] = self._get_instance_capabilities()
7257 
7258         data["vcpus"] = self._get_vcpu_total()
7259         data["memory_mb"] = self._host.get_memory_mb_total()
7260         data["local_gb"] = disk_info_dict['total']
7261         data["vcpus_used"] = self._get_vcpu_used()
7262         data["memory_mb_used"] = self._host.get_memory_mb_used()
7263         data["local_gb_used"] = disk_info_dict['used']
7264         data["hypervisor_type"] = self._host.get_driver_type()
7265         data["hypervisor_version"] = self._host.get_version()
7266         data["hypervisor_hostname"] = self._host.get_hostname()
7267         # TODO(berrange): why do we bother converting the
7268         # libvirt capabilities XML into a special JSON format ?
7269         # The data format is different across all the drivers
7270         # so we could just return the raw capabilities XML
7271         # which 'compare_cpu' could use directly
7272         #
7273         # That said, arch_filter.py now seems to rely on
7274         # the libvirt drivers format which suggests this
7275         # data format needs to be standardized across drivers
7276         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
7277 
7278         disk_free_gb = disk_info_dict['free']
7279         disk_over_committed = self._get_disk_over_committed_size_total()
7280         available_least = disk_free_gb * units.Gi - disk_over_committed
7281         data['disk_available_least'] = available_least / units.Gi
7282 
7283         data['pci_passthrough_devices'] = self._get_pci_passthrough_devices()
7284 
7285         numa_topology = self._get_host_numa_topology()
7286         if numa_topology:
7287             data['numa_topology'] = numa_topology._to_json()
7288         else:
7289             data['numa_topology'] = None
7290 
7291         return data
7292 
7293     def check_instance_shared_storage_local(self, context, instance):
7294         """Check if instance files located on shared storage.
7295 
7296         This runs check on the destination host, and then calls
7297         back to the source host to check the results.
7298 
7299         :param context: security context
7300         :param instance: nova.objects.instance.Instance object
7301         :returns:
7302          - tempfile: A dict containing the tempfile info on the destination
7303                      host
7304          - None:
7305 
7306             1. If the instance path is not existing.
7307             2. If the image backend is shared block storage type.
7308         """
7309         if self.image_backend.backend().is_shared_block_storage():
7310             return None
7311 
7312         dirpath = libvirt_utils.get_instance_path(instance)
7313 
7314         if not os.path.exists(dirpath):
7315             return None
7316 
7317         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7318         LOG.debug("Creating tmpfile %s to verify with other "
7319                   "compute node that the instance is on "
7320                   "the same shared storage.",
7321                   tmp_file, instance=instance)
7322         os.close(fd)
7323         return {"filename": tmp_file}
7324 
7325     def check_instance_shared_storage_remote(self, context, data):
7326         return os.path.exists(data['filename'])
7327 
7328     def check_instance_shared_storage_cleanup(self, context, data):
7329         fileutils.delete_if_exists(data["filename"])
7330 
7331     def check_can_live_migrate_destination(self, context, instance,
7332                                            src_compute_info, dst_compute_info,
7333                                            block_migration=False,
7334                                            disk_over_commit=False):
7335         """Check if it is possible to execute live migration.
7336 
7337         This runs checks on the destination host, and then calls
7338         back to the source host to check the results.
7339 
7340         :param context: security context
7341         :param instance: nova.db.sqlalchemy.models.Instance
7342         :param block_migration: if true, prepare for block migration
7343         :param disk_over_commit: if true, allow disk over commit
7344         :returns: a LibvirtLiveMigrateData object
7345         """
7346         if disk_over_commit:
7347             disk_available_gb = dst_compute_info['free_disk_gb']
7348         else:
7349             disk_available_gb = dst_compute_info['disk_available_least']
7350         disk_available_mb = (
7351             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
7352 
7353         # Compare CPU
7354         if not instance.vcpu_model or not instance.vcpu_model.model:
7355             source_cpu_info = src_compute_info['cpu_info']
7356             self._compare_cpu(None, source_cpu_info, instance)
7357         else:
7358             self._compare_cpu(instance.vcpu_model, None, instance)
7359 
7360         # Create file on storage, to be checked on source host
7361         filename = self._create_shared_storage_test_file(instance)
7362 
7363         data = objects.LibvirtLiveMigrateData()
7364         data.filename = filename
7365         data.image_type = CONF.libvirt.images_type
7366         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
7367         data.graphics_listen_addr_spice = CONF.spice.server_listen
7368         if CONF.serial_console.enabled:
7369             data.serial_listen_addr = CONF.serial_console.proxyclient_address
7370         else:
7371             data.serial_listen_addr = None
7372         # Notes(eliqiao): block_migration and disk_over_commit are not
7373         # nullable, so just don't set them if they are None
7374         if block_migration is not None:
7375             data.block_migration = block_migration
7376         if disk_over_commit is not None:
7377             data.disk_over_commit = disk_over_commit
7378         data.disk_available_mb = disk_available_mb
7379         data.dst_wants_file_backed_memory = CONF.libvirt.file_backed_memory > 0
7380         data.file_backed_memory_discard = (CONF.libvirt.file_backed_memory and
7381             self._host.has_min_version(MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
7382                                        MIN_QEMU_FILE_BACKED_DISCARD_VERSION))
7383 
7384         return data
7385 
7386     def cleanup_live_migration_destination_check(self, context,
7387                                                  dest_check_data):
7388         """Do required cleanup on dest host after check_can_live_migrate calls
7389 
7390         :param context: security context
7391         """
7392         filename = dest_check_data.filename
7393         self._cleanup_shared_storage_test_file(filename)
7394 
7395     def check_can_live_migrate_source(self, context, instance,
7396                                       dest_check_data,
7397                                       block_device_info=None):
7398         """Check if it is possible to execute live migration.
7399 
7400         This checks if the live migration can succeed, based on the
7401         results from check_can_live_migrate_destination.
7402 
7403         :param context: security context
7404         :param instance: nova.db.sqlalchemy.models.Instance
7405         :param dest_check_data: result of check_can_live_migrate_destination
7406         :param block_device_info: result of _get_instance_block_device_info
7407         :returns: a LibvirtLiveMigrateData object
7408         """
7409         # Checking shared storage connectivity
7410         # if block migration, instances_path should not be on shared storage.
7411         source = CONF.host
7412 
7413         dest_check_data.is_shared_instance_path = (
7414             self._check_shared_storage_test_file(
7415                 dest_check_data.filename, instance))
7416 
7417         dest_check_data.is_shared_block_storage = (
7418             self._is_shared_block_storage(instance, dest_check_data,
7419                                           block_device_info))
7420 
7421         if 'block_migration' not in dest_check_data:
7422             dest_check_data.block_migration = (
7423                 not dest_check_data.is_on_shared_storage())
7424 
7425         if dest_check_data.block_migration:
7426             # TODO(eliqiao): Once block_migration flag is removed from the API
7427             # we can safely remove the if condition
7428             if dest_check_data.is_on_shared_storage():
7429                 reason = _("Block migration can not be used "
7430                            "with shared storage.")
7431                 raise exception.InvalidLocalStorage(reason=reason, path=source)
7432             if 'disk_over_commit' in dest_check_data:
7433                 self._assert_dest_node_has_enough_disk(context, instance,
7434                                         dest_check_data.disk_available_mb,
7435                                         dest_check_data.disk_over_commit,
7436                                         block_device_info)
7437             if block_device_info:
7438                 bdm = block_device_info.get('block_device_mapping')
7439                 # NOTE(eliqiao): Selective disk migrations are not supported
7440                 # with tunnelled block migrations so we can block them early.
7441                 if (bdm and
7442                     (self._block_migration_flags &
7443                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
7444                     msg = (_('Cannot block migrate instance %(uuid)s with'
7445                              ' mapped volumes. Selective block device'
7446                              ' migration is not supported with tunnelled'
7447                              ' block migrations.') % {'uuid': instance.uuid})
7448                     LOG.error(msg, instance=instance)
7449                     raise exception.MigrationPreCheckError(reason=msg)
7450         elif not (dest_check_data.is_shared_block_storage or
7451                   dest_check_data.is_shared_instance_path):
7452             reason = _("Shared storage live-migration requires either shared "
7453                        "storage or boot-from-volume with no local disks.")
7454             raise exception.InvalidSharedStorage(reason=reason, path=source)
7455 
7456         # NOTE(mikal): include the instance directory name here because it
7457         # doesn't yet exist on the destination but we want to force that
7458         # same name to be used
7459         instance_path = libvirt_utils.get_instance_path(instance,
7460                                                         relative=True)
7461         dest_check_data.instance_relative_path = instance_path
7462 
7463         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
7464         # of wiring up the encrypted disk configuration for the domain.
7465         # Note that this does not require the QEMU and Libvirt versions to
7466         # decrypt LUKS to be installed on the source node. Only the Nova
7467         # utility code to generate the correct XML is required, so we can
7468         # default to True here for all computes >= Queens.
7469         dest_check_data.src_supports_native_luks = True
7470 
7471         return dest_check_data
7472 
7473     def _is_shared_block_storage(self, instance, dest_check_data,
7474                                  block_device_info=None):
7475         """Check if all block storage of an instance can be shared
7476         between source and destination of a live migration.
7477 
7478         Returns true if the instance is volume backed and has no local disks,
7479         or if the image backend is the same on source and destination and the
7480         backend shares block storage between compute nodes.
7481 
7482         :param instance: nova.objects.instance.Instance object
7483         :param dest_check_data: dict with boolean fields image_type,
7484                                 is_shared_instance_path, and is_volume_backed
7485         """
7486         if (dest_check_data.obj_attr_is_set('image_type') and
7487                 CONF.libvirt.images_type == dest_check_data.image_type and
7488                 self.image_backend.backend().is_shared_block_storage()):
7489             # NOTE(dgenin): currently true only for RBD image backend
7490             return True
7491 
7492         if (dest_check_data.is_shared_instance_path and
7493                 self.image_backend.backend().is_file_in_instance_path()):
7494             # NOTE(angdraug): file based image backends (Flat, Qcow2)
7495             # place block device files under the instance path
7496             return True
7497 
7498         if (dest_check_data.is_volume_backed and
7499                 not bool(self._get_instance_disk_info(instance,
7500                                                       block_device_info))):
7501             return True
7502 
7503         return False
7504 
7505     def _assert_dest_node_has_enough_disk(self, context, instance,
7506                                              available_mb, disk_over_commit,
7507                                              block_device_info):
7508         """Checks if destination has enough disk for block migration."""
7509         # Libvirt supports qcow2 disk format,which is usually compressed
7510         # on compute nodes.
7511         # Real disk image (compressed) may enlarged to "virtual disk size",
7512         # that is specified as the maximum disk size.
7513         # (See qemu-img -f path-to-disk)
7514         # Scheduler recognizes destination host still has enough disk space
7515         # if real disk size < available disk size
7516         # if disk_over_commit is True,
7517         #  otherwise virtual disk size < available disk size.
7518 
7519         available = 0
7520         if available_mb:
7521             available = available_mb * units.Mi
7522 
7523         disk_infos = self._get_instance_disk_info(instance, block_device_info)
7524 
7525         necessary = 0
7526         if disk_over_commit:
7527             for info in disk_infos:
7528                 necessary += int(info['disk_size'])
7529         else:
7530             for info in disk_infos:
7531                 necessary += int(info['virt_disk_size'])
7532 
7533         # Check that available disk > necessary disk
7534         if (available - necessary) < 0:
7535             reason = (_('Unable to migrate %(instance_uuid)s: '
7536                         'Disk of instance is too large(available'
7537                         ' on destination host:%(available)s '
7538                         '< need:%(necessary)s)') %
7539                       {'instance_uuid': instance.uuid,
7540                        'available': available,
7541                        'necessary': necessary})
7542             raise exception.MigrationPreCheckError(reason=reason)
7543 
7544     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
7545         """Check the host is compatible with the requested CPU
7546 
7547         :param guest_cpu: nova.objects.VirtCPUModel or None
7548         :param host_cpu_str: JSON from _get_cpu_info() method
7549 
7550         If the 'guest_cpu' parameter is not None, this will be
7551         validated for migration compatibility with the host.
7552         Otherwise the 'host_cpu_str' JSON string will be used for
7553         validation.
7554 
7555         :returns:
7556             None. if given cpu info is not compatible to this server,
7557             raise exception.
7558         """
7559 
7560         # NOTE(kchamart): Comparing host to guest CPU model for emulated
7561         # guests (<domain type='qemu'>) should not matter -- in this
7562         # mode (QEMU "TCG") the CPU is fully emulated in software and no
7563         # hardware acceleration, like KVM, is involved. So, skip the CPU
7564         # compatibility check for the QEMU domain type, and retain it for
7565         # KVM guests.
7566         if CONF.libvirt.virt_type not in ['kvm']:
7567             return
7568 
7569         if guest_cpu is None:
7570             info = jsonutils.loads(host_cpu_str)
7571             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
7572             cpu = vconfig.LibvirtConfigCPU()
7573             cpu.arch = info['arch']
7574             cpu.model = info['model']
7575             cpu.vendor = info['vendor']
7576             cpu.sockets = info['topology']['sockets']
7577             cpu.cores = info['topology']['cores']
7578             cpu.threads = info['topology']['threads']
7579             for f in info['features']:
7580                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
7581         else:
7582             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
7583 
7584         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
7585              "virCPUCompareResult")
7586         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
7587         # unknown character exists in xml, then libvirt complains
7588         try:
7589             cpu_xml = cpu.to_xml()
7590             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
7591             ret = self._host.compare_cpu(cpu_xml)
7592         except libvirt.libvirtError as e:
7593             error_code = e.get_error_code()
7594             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
7595                 LOG.debug("URI %(uri)s does not support cpu comparison. "
7596                           "It will be proceeded though. Error: %(error)s",
7597                           {'uri': self._uri(), 'error': e})
7598                 return
7599             else:
7600                 LOG.error(m, {'ret': e, 'u': u})
7601                 raise exception.MigrationPreCheckError(
7602                     reason=m % {'ret': e, 'u': u})
7603 
7604         if ret <= 0:
7605             LOG.error(m, {'ret': ret, 'u': u})
7606             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
7607 
7608     def _create_shared_storage_test_file(self, instance):
7609         """Makes tmpfile under CONF.instances_path."""
7610         dirpath = CONF.instances_path
7611         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
7612         LOG.debug("Creating tmpfile %s to notify to other "
7613                   "compute nodes that they should mount "
7614                   "the same storage.", tmp_file, instance=instance)
7615         os.close(fd)
7616         return os.path.basename(tmp_file)
7617 
7618     def _check_shared_storage_test_file(self, filename, instance):
7619         """Confirms existence of the tmpfile under CONF.instances_path.
7620 
7621         Cannot confirm tmpfile return False.
7622         """
7623         # NOTE(tpatzig): if instances_path is a shared volume that is
7624         # under heavy IO (many instances on many compute nodes),
7625         # then checking the existence of the testfile fails,
7626         # just because it takes longer until the client refreshes and new
7627         # content gets visible.
7628         # os.utime (like touch) on the directory forces the client to refresh.
7629         os.utime(CONF.instances_path, None)
7630 
7631         tmp_file = os.path.join(CONF.instances_path, filename)
7632         if not os.path.exists(tmp_file):
7633             exists = False
7634         else:
7635             exists = True
7636         LOG.debug('Check if temp file %s exists to indicate shared storage '
7637                   'is being used for migration. Exists? %s', tmp_file, exists,
7638                   instance=instance)
7639         return exists
7640 
7641     def _cleanup_shared_storage_test_file(self, filename):
7642         """Removes existence of the tmpfile under CONF.instances_path."""
7643         tmp_file = os.path.join(CONF.instances_path, filename)
7644         os.remove(tmp_file)
7645 
7646     def ensure_filtering_rules_for_instance(self, instance, network_info):
7647         """Ensure that an instance's filtering rules are enabled.
7648 
7649         When migrating an instance, we need the filtering rules to
7650         be configured on the destination host before starting the
7651         migration.
7652 
7653         Also, when restarting the compute service, we need to ensure
7654         that filtering rules exist for all running services.
7655         """
7656 
7657         self.firewall_driver.setup_basic_filtering(instance, network_info)
7658         self.firewall_driver.prepare_instance_filter(instance,
7659                 network_info)
7660 
7661         # nwfilters may be defined in a separate thread in the case
7662         # of libvirt non-blocking mode, so we wait for completion
7663         timeout_count = list(range(CONF.live_migration_retry_count))
7664         while timeout_count:
7665             if self.firewall_driver.instance_filter_exists(instance,
7666                                                            network_info):
7667                 break
7668             timeout_count.pop()
7669             if len(timeout_count) == 0:
7670                 msg = _('The firewall filter for %s does not exist')
7671                 raise exception.InternalError(msg % instance.name)
7672             greenthread.sleep(1)
7673 
7674     def filter_defer_apply_on(self):
7675         self.firewall_driver.filter_defer_apply_on()
7676 
7677     def filter_defer_apply_off(self):
7678         self.firewall_driver.filter_defer_apply_off()
7679 
7680     def live_migration(self, context, instance, dest,
7681                        post_method, recover_method, block_migration=False,
7682                        migrate_data=None):
7683         """Spawning live_migration operation for distributing high-load.
7684 
7685         :param context: security context
7686         :param instance:
7687             nova.db.sqlalchemy.models.Instance object
7688             instance object that is migrated.
7689         :param dest: destination host
7690         :param post_method:
7691             post operation method.
7692             expected nova.compute.manager._post_live_migration.
7693         :param recover_method:
7694             recovery method when any exception occurs.
7695             expected nova.compute.manager._rollback_live_migration.
7696         :param block_migration: if true, do block migration.
7697         :param migrate_data: a LibvirtLiveMigrateData object
7698 
7699         """
7700 
7701         # 'dest' will be substituted into 'migration_uri' so ensure
7702         # it does't contain any characters that could be used to
7703         # exploit the URI accepted by libvirt
7704         if not libvirt_utils.is_valid_hostname(dest):
7705             raise exception.InvalidHostname(hostname=dest)
7706 
7707         self._live_migration(context, instance, dest,
7708                              post_method, recover_method, block_migration,
7709                              migrate_data)
7710 
7711     def live_migration_abort(self, instance):
7712         """Aborting a running live-migration.
7713 
7714         :param instance: instance object that is in migration
7715 
7716         """
7717 
7718         guest = self._host.get_guest(instance)
7719         dom = guest._domain
7720 
7721         try:
7722             dom.abortJob()
7723         except libvirt.libvirtError as e:
7724             LOG.error("Failed to cancel migration %s",
7725                     encodeutils.exception_to_unicode(e), instance=instance)
7726             raise
7727 
7728     def _verify_serial_console_is_disabled(self):
7729         if CONF.serial_console.enabled:
7730 
7731             msg = _('Your destination node does not support'
7732                     ' retrieving listen addresses. In order'
7733                     ' for live migration to work properly you'
7734                     ' must disable serial console.')
7735             raise exception.MigrationError(reason=msg)
7736 
7737     def _detach_direct_passthrough_vifs(self, context,
7738                                         migrate_data, instance):
7739         """detaches passthrough vif to enable live migration
7740 
7741         :param context: security context
7742         :param migrate_data: a LibvirtLiveMigrateData object
7743         :param instance: instance object that is migrated.
7744         """
7745         # NOTE(sean-k-mooney): if we have vif data available we
7746         # loop over each vif and detach all direct passthrough
7747         # vifs to allow sriov live migration.
7748         direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
7749         vifs = [vif.source_vif for vif in migrate_data.vifs
7750                 if "source_vif" in vif and vif.source_vif]
7751         for vif in vifs:
7752             if vif['vnic_type'] in direct_vnics:
7753                 LOG.info("Detaching vif %s from instnace "
7754                          "%s for live migration", vif['id'], instance.id)
7755                 self.detach_interface(context, instance, vif)
7756 
7757     def _live_migration_operation(self, context, instance, dest,
7758                                   block_migration, migrate_data, guest,
7759                                   device_names):
7760         """Invoke the live migration operation
7761 
7762         :param context: security context
7763         :param instance:
7764             nova.db.sqlalchemy.models.Instance object
7765             instance object that is migrated.
7766         :param dest: destination host
7767         :param block_migration: if true, do block migration.
7768         :param migrate_data: a LibvirtLiveMigrateData object
7769         :param guest: the guest domain object
7770         :param device_names: list of device names that are being migrated with
7771             instance
7772 
7773         This method is intended to be run in a background thread and will
7774         block that thread until the migration is finished or failed.
7775         """
7776         try:
7777             if migrate_data.block_migration:
7778                 migration_flags = self._block_migration_flags
7779             else:
7780                 migration_flags = self._live_migration_flags
7781 
7782             serial_listen_addr = libvirt_migrate.serial_listen_addr(
7783                 migrate_data)
7784             if not serial_listen_addr:
7785                 # In this context we want to ensure that serial console is
7786                 # disabled on source node. This is because nova couldn't
7787                 # retrieve serial listen address from destination node, so we
7788                 # consider that destination node might have serial console
7789                 # disabled as well.
7790                 self._verify_serial_console_is_disabled()
7791 
7792             # NOTE(aplanas) migrate_uri will have a value only in the
7793             # case that `live_migration_inbound_addr` parameter is
7794             # set, and we propose a non tunneled migration.
7795             migrate_uri = None
7796             if ('target_connect_addr' in migrate_data and
7797                     migrate_data.target_connect_addr is not None):
7798                 dest = migrate_data.target_connect_addr
7799                 if (migration_flags &
7800                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
7801                     migrate_uri = self._migrate_uri(dest)
7802 
7803             new_xml_str = None
7804             if CONF.libvirt.virt_type != "parallels":
7805                 # If the migrate_data has port binding information for the
7806                 # destination host, we need to prepare the guest vif config
7807                 # for the destination before we start migrating the guest.
7808                 get_vif_config = None
7809                 if 'vifs' in migrate_data and migrate_data.vifs:
7810                     # NOTE(mriedem): The vif kwarg must be built on the fly
7811                     # within get_updated_guest_xml based on migrate_data.vifs.
7812                     # We could stash the virt_type from the destination host
7813                     # into LibvirtLiveMigrateData but the host kwarg is a
7814                     # nova.virt.libvirt.host.Host object and is used to check
7815                     # information like libvirt version on the destination.
7816                     # If this becomes a problem, what we could do is get the
7817                     # VIF configs while on the destination host during
7818                     # pre_live_migration() and store those in the
7819                     # LibvirtLiveMigrateData object. For now we just use the
7820                     # source host information for virt_type and
7821                     # host (version) since the conductor live_migrate method
7822                     # _check_compatible_with_source_hypervisor() ensures that
7823                     # the hypervisor types and versions are compatible.
7824                     get_vif_config = functools.partial(
7825                         self.vif_driver.get_config,
7826                         instance=instance,
7827                         image_meta=instance.image_meta,
7828                         inst_type=instance.flavor,
7829                         virt_type=CONF.libvirt.virt_type,
7830                         host=self._host)
7831                     self._detach_direct_passthrough_vifs(context,
7832                         migrate_data, instance)
7833                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
7834                     # TODO(sahid): It's not a really good idea to pass
7835                     # the method _get_volume_config and we should to find
7836                     # a way to avoid this in future.
7837                     guest, migrate_data, self._get_volume_config,
7838                     get_vif_config=get_vif_config)
7839 
7840             # NOTE(pkoniszewski): Because of precheck which blocks
7841             # tunnelled block live migration with mapped volumes we
7842             # can safely remove migrate_disks when tunnelling is on.
7843             # Otherwise we will block all tunnelled block migrations,
7844             # even when an instance does not have volumes mapped.
7845             # This is because selective disk migration is not
7846             # supported in tunnelled block live migration. Also we
7847             # cannot fallback to migrateToURI2 in this case because of
7848             # bug #1398999
7849             #
7850             # TODO(kchamart) Move the following bit to guest.migrate()
7851             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
7852                 device_names = []
7853 
7854             # TODO(sahid): This should be in
7855             # post_live_migration_at_source but no way to retrieve
7856             # ports acquired on the host for the guest at this
7857             # step. Since the domain is going to be removed from
7858             # libvird on source host after migration, we backup the
7859             # serial ports to release them if all went well.
7860             serial_ports = []
7861             if CONF.serial_console.enabled:
7862                 serial_ports = list(self._get_serial_ports_from_guest(guest))
7863 
7864             LOG.debug("About to invoke the migrate API", instance=instance)
7865             guest.migrate(self._live_migration_uri(dest),
7866                           migrate_uri=migrate_uri,
7867                           flags=migration_flags,
7868                           migrate_disks=device_names,
7869                           destination_xml=new_xml_str,
7870                           bandwidth=CONF.libvirt.live_migration_bandwidth)
7871             LOG.debug("Migrate API has completed", instance=instance)
7872 
7873             for hostname, port in serial_ports:
7874                 serial_console.release_port(host=hostname, port=port)
7875         except Exception as e:
7876             with excutils.save_and_reraise_exception():
7877                 LOG.error("Live Migration failure: %s", e, instance=instance)
7878 
7879         # If 'migrateToURI' fails we don't know what state the
7880         # VM instances on each host are in. Possibilities include
7881         #
7882         #  1. src==running, dst==none
7883         #
7884         #     Migration failed & rolled back, or never started
7885         #
7886         #  2. src==running, dst==paused
7887         #
7888         #     Migration started but is still ongoing
7889         #
7890         #  3. src==paused,  dst==paused
7891         #
7892         #     Migration data transfer completed, but switchover
7893         #     is still ongoing, or failed
7894         #
7895         #  4. src==paused,  dst==running
7896         #
7897         #     Migration data transfer completed, switchover
7898         #     happened but cleanup on source failed
7899         #
7900         #  5. src==none,    dst==running
7901         #
7902         #     Migration fully succeeded.
7903         #
7904         # Libvirt will aim to complete any migration operation
7905         # or roll it back. So even if the migrateToURI call has
7906         # returned an error, if the migration was not finished
7907         # libvirt should clean up.
7908         #
7909         # So we take the error raise here with a pinch of salt
7910         # and rely on the domain job info status to figure out
7911         # what really happened to the VM, which is a much more
7912         # reliable indicator.
7913         #
7914         # In particular we need to try very hard to ensure that
7915         # Nova does not "forget" about the guest. ie leaving it
7916         # running on a different host to the one recorded in
7917         # the database, as that would be a serious resource leak
7918 
7919         LOG.debug("Migration operation thread has finished",
7920                   instance=instance)
7921 
7922     def _live_migration_copy_disk_paths(self, context, instance, guest):
7923         '''Get list of disks to copy during migration
7924 
7925         :param context: security context
7926         :param instance: the instance being migrated
7927         :param guest: the Guest instance being migrated
7928 
7929         Get the list of disks to copy during migration.
7930 
7931         :returns: a list of local source paths and a list of device names to
7932             copy
7933         '''
7934 
7935         disk_paths = []
7936         device_names = []
7937         block_devices = []
7938 
7939         if (self._block_migration_flags &
7940                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
7941             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7942                 context, instance.uuid)
7943             block_device_info = driver.get_block_device_info(instance,
7944                                                              bdm_list)
7945 
7946             block_device_mappings = driver.block_device_info_get_mapping(
7947                 block_device_info)
7948             for bdm in block_device_mappings:
7949                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7950                 block_devices.append(device_name)
7951 
7952         for dev in guest.get_all_disks():
7953             if dev.readonly or dev.shareable:
7954                 continue
7955             if dev.source_type not in ["file", "block"]:
7956                 continue
7957             if dev.target_dev in block_devices:
7958                 continue
7959             disk_paths.append(dev.source_path)
7960             device_names.append(dev.target_dev)
7961         return (disk_paths, device_names)
7962 
7963     def _live_migration_data_gb(self, instance, disk_paths):
7964         '''Calculate total amount of data to be transferred
7965 
7966         :param instance: the nova.objects.Instance being migrated
7967         :param disk_paths: list of disk paths that are being migrated
7968         with instance
7969 
7970         Calculates the total amount of data that needs to be
7971         transferred during the live migration. The actual
7972         amount copied will be larger than this, due to the
7973         guest OS continuing to dirty RAM while the migration
7974         is taking place. So this value represents the minimal
7975         data size possible.
7976 
7977         :returns: data size to be copied in GB
7978         '''
7979 
7980         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
7981         if ram_gb < 2:
7982             ram_gb = 2
7983 
7984         disk_gb = 0
7985         for path in disk_paths:
7986             try:
7987                 size = os.stat(path).st_size
7988                 size_gb = (size / units.Gi)
7989                 if size_gb < 2:
7990                     size_gb = 2
7991                 disk_gb += size_gb
7992             except OSError as e:
7993                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
7994                             {'disk': path, 'ex': e})
7995                 # Ignore error since we don't want to break
7996                 # the migration monitoring thread operation
7997 
7998         return ram_gb + disk_gb
7999 
8000     def _get_migration_flags(self, is_block_migration):
8001         if is_block_migration:
8002             return self._block_migration_flags
8003         return self._live_migration_flags
8004 
8005     def _live_migration_monitor(self, context, instance, guest,
8006                                 dest, post_method,
8007                                 recover_method, block_migration,
8008                                 migrate_data, finish_event,
8009                                 disk_paths):
8010         on_migration_failure = deque()
8011         data_gb = self._live_migration_data_gb(instance, disk_paths)
8012         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
8013         migration = migrate_data.migration
8014         curdowntime = None
8015 
8016         migration_flags = self._get_migration_flags(
8017                                   migrate_data.block_migration)
8018 
8019         n = 0
8020         start = time.time()
8021         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
8022         while True:
8023             info = guest.get_job_info()
8024 
8025             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
8026                 # Either still running, or failed or completed,
8027                 # lets untangle the mess
8028                 if not finish_event.ready():
8029                     LOG.debug("Operation thread is still running",
8030                               instance=instance)
8031                 else:
8032                     info.type = libvirt_migrate.find_job_type(guest, instance)
8033                     LOG.debug("Fixed incorrect job type to be %d",
8034                               info.type, instance=instance)
8035 
8036             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
8037                 # Migration is not yet started
8038                 LOG.debug("Migration not running yet",
8039                           instance=instance)
8040             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
8041                 # Migration is still running
8042                 #
8043                 # This is where we wire up calls to change live
8044                 # migration status. eg change max downtime, cancel
8045                 # the operation, change max bandwidth
8046                 libvirt_migrate.run_tasks(guest, instance,
8047                                           self.active_migrations,
8048                                           on_migration_failure,
8049                                           migration,
8050                                           is_post_copy_enabled)
8051 
8052                 now = time.time()
8053                 elapsed = now - start
8054 
8055                 completion_timeout = int(
8056                     CONF.libvirt.live_migration_completion_timeout * data_gb)
8057                 # NOTE(yikun): Check the completion timeout to determine
8058                 # should trigger the timeout action, and there are two choices
8059                 # ``abort`` (default) or ``force_complete``. If the action is
8060                 # set to ``force_complete``, the post-copy will be triggered
8061                 # if available else the VM will be suspended, otherwise the
8062                 # live migrate operation will be aborted.
8063                 if libvirt_migrate.should_trigger_timeout_action(
8064                         instance, elapsed, completion_timeout,
8065                         migration.status):
8066                     timeout_act = CONF.libvirt.live_migration_timeout_action
8067                     if timeout_act == 'force_complete':
8068                         self.live_migration_force_complete(instance)
8069                     else:
8070                         # timeout action is 'abort'
8071                         try:
8072                             guest.abort_job()
8073                         except libvirt.libvirtError as e:
8074                             LOG.warning("Failed to abort migration %s",
8075                                     encodeutils.exception_to_unicode(e),
8076                                     instance=instance)
8077                             self._clear_empty_migration(instance)
8078                             raise
8079 
8080                 curdowntime = libvirt_migrate.update_downtime(
8081                     guest, instance, curdowntime,
8082                     downtime_steps, elapsed)
8083 
8084                 # We loop every 500ms, so don't log on every
8085                 # iteration to avoid spamming logs for long
8086                 # running migrations. Just once every 5 secs
8087                 # is sufficient for developers to debug problems.
8088                 # We log once every 30 seconds at info to help
8089                 # admins see slow running migration operations
8090                 # when debug logs are off.
8091                 if (n % 10) == 0:
8092                     # Ignoring memory_processed, as due to repeated
8093                     # dirtying of data, this can be way larger than
8094                     # memory_total. Best to just look at what's
8095                     # remaining to copy and ignore what's done already
8096                     #
8097                     # TODO(berrange) perhaps we could include disk
8098                     # transfer stats in the progress too, but it
8099                     # might make memory info more obscure as large
8100                     # disk sizes might dwarf memory size
8101                     remaining = 100
8102                     if info.memory_total != 0:
8103                         remaining = round(info.memory_remaining *
8104                                           100 / info.memory_total)
8105 
8106                     libvirt_migrate.save_stats(instance, migration,
8107                                                info, remaining)
8108 
8109                     # NOTE(fanzhang): do not include disk transfer stats in
8110                     # the progress percentage calculation but log them.
8111                     disk_remaining = 100
8112                     if info.disk_total != 0:
8113                         disk_remaining = round(info.disk_remaining *
8114                                                100 / info.disk_total)
8115 
8116                     lg = LOG.debug
8117                     if (n % 60) == 0:
8118                         lg = LOG.info
8119 
8120                     lg("Migration running for %(secs)d secs, "
8121                        "memory %(remaining)d%% remaining "
8122                        "(bytes processed=%(processed_memory)d, "
8123                        "remaining=%(remaining_memory)d, "
8124                        "total=%(total_memory)d); "
8125                        "disk %(disk_remaining)d%% remaining "
8126                        "(bytes processed=%(processed_disk)d, "
8127                        "remaining=%(remaining_disk)d, "
8128                        "total=%(total_disk)d).",
8129                        {"secs": n / 2, "remaining": remaining,
8130                         "processed_memory": info.memory_processed,
8131                         "remaining_memory": info.memory_remaining,
8132                         "total_memory": info.memory_total,
8133                         "disk_remaining": disk_remaining,
8134                         "processed_disk": info.disk_processed,
8135                         "remaining_disk": info.disk_remaining,
8136                         "total_disk": info.disk_total}, instance=instance)
8137 
8138                 n = n + 1
8139             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
8140                 # Migration is all done
8141                 LOG.info("Migration operation has completed",
8142                          instance=instance)
8143                 post_method(context, instance, dest, block_migration,
8144                             migrate_data)
8145                 break
8146             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
8147                 # Migration did not succeed
8148                 LOG.error("Migration operation has aborted", instance=instance)
8149                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8150                                                   on_migration_failure)
8151                 recover_method(context, instance, dest, migrate_data)
8152                 break
8153             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
8154                 # Migration was stopped by admin
8155                 LOG.warning("Migration operation was cancelled",
8156                             instance=instance)
8157                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
8158                                                   on_migration_failure)
8159                 recover_method(context, instance, dest, migrate_data,
8160                                migration_status='cancelled')
8161                 break
8162             else:
8163                 LOG.warning("Unexpected migration job type: %d",
8164                             info.type, instance=instance)
8165 
8166             time.sleep(0.5)
8167         self._clear_empty_migration(instance)
8168 
8169     def _clear_empty_migration(self, instance):
8170         try:
8171             del self.active_migrations[instance.uuid]
8172         except KeyError:
8173             LOG.warning("There are no records in active migrations "
8174                         "for instance", instance=instance)
8175 
8176     def _live_migration(self, context, instance, dest, post_method,
8177                         recover_method, block_migration,
8178                         migrate_data):
8179         """Do live migration.
8180 
8181         :param context: security context
8182         :param instance:
8183             nova.db.sqlalchemy.models.Instance object
8184             instance object that is migrated.
8185         :param dest: destination host
8186         :param post_method:
8187             post operation method.
8188             expected nova.compute.manager._post_live_migration.
8189         :param recover_method:
8190             recovery method when any exception occurs.
8191             expected nova.compute.manager._rollback_live_migration.
8192         :param block_migration: if true, do block migration.
8193         :param migrate_data: a LibvirtLiveMigrateData object
8194 
8195         This fires off a new thread to run the blocking migration
8196         operation, and then this thread monitors the progress of
8197         migration and controls its operation
8198         """
8199 
8200         guest = self._host.get_guest(instance)
8201 
8202         disk_paths = []
8203         device_names = []
8204         if (migrate_data.block_migration and
8205                 CONF.libvirt.virt_type != "parallels"):
8206             disk_paths, device_names = self._live_migration_copy_disk_paths(
8207                 context, instance, guest)
8208 
8209         opthread = utils.spawn(self._live_migration_operation,
8210                                      context, instance, dest,
8211                                      block_migration,
8212                                      migrate_data, guest,
8213                                      device_names)
8214 
8215         finish_event = eventlet.event.Event()
8216         self.active_migrations[instance.uuid] = deque()
8217 
8218         def thread_finished(thread, event):
8219             LOG.debug("Migration operation thread notification",
8220                       instance=instance)
8221             event.send()
8222         opthread.link(thread_finished, finish_event)
8223 
8224         # Let eventlet schedule the new thread right away
8225         time.sleep(0)
8226 
8227         try:
8228             LOG.debug("Starting monitoring of live migration",
8229                       instance=instance)
8230             self._live_migration_monitor(context, instance, guest, dest,
8231                                          post_method, recover_method,
8232                                          block_migration, migrate_data,
8233                                          finish_event, disk_paths)
8234         except Exception as ex:
8235             LOG.warning("Error monitoring migration: %(ex)s",
8236                         {"ex": ex}, instance=instance, exc_info=True)
8237             raise
8238         finally:
8239             LOG.debug("Live migration monitoring is all done",
8240                       instance=instance)
8241 
8242     def _is_post_copy_enabled(self, migration_flags):
8243         return (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0
8244 
8245     def live_migration_force_complete(self, instance):
8246         try:
8247             self.active_migrations[instance.uuid].append('force-complete')
8248         except KeyError:
8249             raise exception.NoActiveMigrationForInstance(
8250                 instance_id=instance.uuid)
8251 
8252     def _try_fetch_image(self, context, path, image_id, instance,
8253                          fallback_from_host=None):
8254         try:
8255             libvirt_utils.fetch_image(context, path, image_id,
8256                                       instance.trusted_certs)
8257         except exception.ImageNotFound:
8258             if not fallback_from_host:
8259                 raise
8260             LOG.debug("Image %(image_id)s doesn't exist anymore on "
8261                       "image service, attempting to copy image "
8262                       "from %(host)s",
8263                       {'image_id': image_id, 'host': fallback_from_host})
8264             libvirt_utils.copy_image(src=path, dest=path,
8265                                      host=fallback_from_host,
8266                                      receive=True)
8267 
8268     def _fetch_instance_kernel_ramdisk(self, context, instance,
8269                                        fallback_from_host=None):
8270         """Download kernel and ramdisk for instance in instance directory."""
8271         instance_dir = libvirt_utils.get_instance_path(instance)
8272         if instance.kernel_id:
8273             kernel_path = os.path.join(instance_dir, 'kernel')
8274             # NOTE(dsanders): only fetch image if it's not available at
8275             # kernel_path. This also avoids ImageNotFound exception if
8276             # the image has been deleted from glance
8277             if not os.path.exists(kernel_path):
8278                 self._try_fetch_image(context,
8279                                       kernel_path,
8280                                       instance.kernel_id,
8281                                       instance, fallback_from_host)
8282             if instance.ramdisk_id:
8283                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
8284                 # NOTE(dsanders): only fetch image if it's not available at
8285                 # ramdisk_path. This also avoids ImageNotFound exception if
8286                 # the image has been deleted from glance
8287                 if not os.path.exists(ramdisk_path):
8288                     self._try_fetch_image(context,
8289                                           ramdisk_path,
8290                                           instance.ramdisk_id,
8291                                           instance, fallback_from_host)
8292 
8293     def _reattach_instance_vifs(self, context, instance, network_info):
8294         guest = self._host.get_guest(instance)
8295         # validate that the guest has the expected number of interfaces
8296         # attached.
8297         guest_interfaces = guest.get_interfaces()
8298         # NOTE(sean-k-mooney): In general len(guest_interfaces) will
8299         # be equal to len(network_info) as interfaces will not be hot unplugged
8300         # unless they are SR-IOV direct mode interfaces. As such we do not
8301         # need an else block here as it would be a noop.
8302         if len(guest_interfaces) < len(network_info):
8303             # NOTE(sean-k-mooney): we are doing a post live migration
8304             # for a guest with sriov vif that were detached as part of
8305             # the migration. loop over the vifs and attach the missing
8306             # vif as part of the post live migration phase.
8307             direct_vnics = network_model.VNIC_TYPES_DIRECT_PASSTHROUGH
8308             for vif in network_info:
8309                 if vif['vnic_type'] in direct_vnics:
8310                     LOG.info("Attaching vif %s to instance %s",
8311                              vif['id'], instance.id)
8312                     self.attach_interface(context, instance,
8313                                           instance.image_meta, vif)
8314 
8315     def rollback_live_migration_at_source(self, context, instance,
8316                                           migrate_data):
8317         """reconnect sriov interfaces after failed live migration
8318         :param context: security context
8319         :param instance:  the instance being migrated
8320         :param migrate_date: a LibvirtLiveMigrateData object
8321         """
8322         network_info = network_model.NetworkInfo(
8323             [vif.source_vif for vif in migrate_data.vifs
8324                             if "source_vif" in vif and vif.source_vif])
8325         self._reattach_instance_vifs(context, instance, network_info)
8326 
8327     def rollback_live_migration_at_destination(self, context, instance,
8328                                                network_info,
8329                                                block_device_info,
8330                                                destroy_disks=True,
8331                                                migrate_data=None):
8332         """Clean up destination node after a failed live migration."""
8333         try:
8334             self.destroy(context, instance, network_info, block_device_info,
8335                          destroy_disks)
8336         finally:
8337             # NOTE(gcb): Failed block live migration may leave instance
8338             # directory at destination node, ensure it is always deleted.
8339             is_shared_instance_path = True
8340             if migrate_data:
8341                 is_shared_instance_path = migrate_data.is_shared_instance_path
8342                 if (migrate_data.obj_attr_is_set("serial_listen_ports") and
8343                         migrate_data.serial_listen_ports):
8344                     # Releases serial ports reserved.
8345                     for port in migrate_data.serial_listen_ports:
8346                         serial_console.release_port(
8347                             host=migrate_data.serial_listen_addr, port=port)
8348 
8349             if not is_shared_instance_path:
8350                 instance_dir = libvirt_utils.get_instance_path_at_destination(
8351                     instance, migrate_data)
8352                 if os.path.exists(instance_dir):
8353                     shutil.rmtree(instance_dir)
8354 
8355     def _pre_live_migration_plug_vifs(self, instance, network_info,
8356                                       migrate_data):
8357         if 'vifs' in migrate_data and migrate_data.vifs:
8358             LOG.debug('Plugging VIFs using destination host port bindings '
8359                       'before live migration.', instance=instance)
8360             vif_plug_nw_info = network_model.NetworkInfo([])
8361             for migrate_vif in migrate_data.vifs:
8362                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
8363         else:
8364             LOG.debug('Plugging VIFs before live migration.',
8365                       instance=instance)
8366             vif_plug_nw_info = network_info
8367         # Retry operation is necessary because continuous live migration
8368         # requests to the same host cause concurrent requests to iptables,
8369         # then it complains.
8370         max_retry = CONF.live_migration_retry_count
8371         for cnt in range(max_retry):
8372             try:
8373                 self.plug_vifs(instance, vif_plug_nw_info)
8374                 break
8375             except processutils.ProcessExecutionError:
8376                 if cnt == max_retry - 1:
8377                     raise
8378                 else:
8379                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
8380                                 '%(max_retry)d.',
8381                                 {'cnt': cnt, 'max_retry': max_retry},
8382                                 instance=instance)
8383                     greenthread.sleep(1)
8384 
8385     def pre_live_migration(self, context, instance, block_device_info,
8386                            network_info, disk_info, migrate_data):
8387         """Preparation live migration."""
8388         if disk_info is not None:
8389             disk_info = jsonutils.loads(disk_info)
8390 
8391         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
8392                   instance=instance)
8393         is_shared_block_storage = migrate_data.is_shared_block_storage
8394         is_shared_instance_path = migrate_data.is_shared_instance_path
8395         is_block_migration = migrate_data.block_migration
8396 
8397         if not is_shared_instance_path:
8398             instance_dir = libvirt_utils.get_instance_path_at_destination(
8399                             instance, migrate_data)
8400 
8401             if os.path.exists(instance_dir):
8402                 raise exception.DestinationDiskExists(path=instance_dir)
8403 
8404             LOG.debug('Creating instance directory: %s', instance_dir,
8405                       instance=instance)
8406             os.mkdir(instance_dir)
8407 
8408             # Recreate the disk.info file and in doing so stop the
8409             # imagebackend from recreating it incorrectly by inspecting the
8410             # contents of each file when using the Raw backend.
8411             if disk_info:
8412                 image_disk_info = {}
8413                 for info in disk_info:
8414                     image_file = os.path.basename(info['path'])
8415                     image_path = os.path.join(instance_dir, image_file)
8416                     image_disk_info[image_path] = info['type']
8417 
8418                 LOG.debug('Creating disk.info with the contents: %s',
8419                           image_disk_info, instance=instance)
8420 
8421                 image_disk_info_path = os.path.join(instance_dir,
8422                                                     'disk.info')
8423                 libvirt_utils.write_to_file(image_disk_info_path,
8424                                             jsonutils.dumps(image_disk_info))
8425 
8426             if not is_shared_block_storage:
8427                 # Ensure images and backing files are present.
8428                 LOG.debug('Checking to make sure images and backing files are '
8429                           'present before live migration.', instance=instance)
8430                 self._create_images_and_backing(
8431                     context, instance, instance_dir, disk_info,
8432                     fallback_from_host=instance.host)
8433                 if (configdrive.required_by(instance) and
8434                         CONF.config_drive_format == 'iso9660'):
8435                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
8436                     # drive needs to be copied to destination prior to
8437                     # migration when instance path is not shared and block
8438                     # storage is not shared. Files that are already present
8439                     # on destination are excluded from a list of files that
8440                     # need to be copied to destination. If we don't do that
8441                     # live migration will fail on copying iso config drive to
8442                     # destination and writing to read-only device.
8443                     # Please see bug/1246201 for more details.
8444                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
8445                     self._remotefs.copy_file(src, instance_dir)
8446 
8447             if not is_block_migration:
8448                 # NOTE(angdraug): when block storage is shared between source
8449                 # and destination and instance path isn't (e.g. volume backed
8450                 # or rbd backed instance), instance path on destination has to
8451                 # be prepared
8452 
8453                 # Required by Quobyte CI
8454                 self._ensure_console_log_for_instance(instance)
8455 
8456                 # if image has kernel and ramdisk, just download
8457                 # following normal way.
8458                 self._fetch_instance_kernel_ramdisk(context, instance)
8459 
8460         # Establishing connection to volume server.
8461         block_device_mapping = driver.block_device_info_get_mapping(
8462             block_device_info)
8463 
8464         if len(block_device_mapping):
8465             LOG.debug('Connecting volumes before live migration.',
8466                       instance=instance)
8467 
8468         for bdm in block_device_mapping:
8469             connection_info = bdm['connection_info']
8470             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
8471             # where an instance has encrypted volumes attached using the
8472             # os-brick encryptors. Do not attempt to attach the encrypted
8473             # volume using native LUKS decryption on the destionation.
8474             src_native_luks = False
8475             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
8476                 src_native_luks = migrate_data.src_supports_native_luks
8477             self._connect_volume(context, connection_info, instance,
8478                                  allow_native_luks=src_native_luks)
8479 
8480         self._pre_live_migration_plug_vifs(
8481             instance, network_info, migrate_data)
8482 
8483         # Store server_listen and latest disk device info
8484         if not migrate_data:
8485             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
8486         else:
8487             migrate_data.bdms = []
8488         # Store live_migration_inbound_addr
8489         migrate_data.target_connect_addr = \
8490             CONF.libvirt.live_migration_inbound_addr
8491         migrate_data.supported_perf_events = self._supported_perf_events
8492 
8493         migrate_data.serial_listen_ports = []
8494         if CONF.serial_console.enabled:
8495             num_ports = hardware.get_number_of_serial_ports(
8496                 instance.flavor, instance.image_meta)
8497             for port in six.moves.range(num_ports):
8498                 migrate_data.serial_listen_ports.append(
8499                     serial_console.acquire_port(
8500                         migrate_data.serial_listen_addr))
8501 
8502         for vol in block_device_mapping:
8503             connection_info = vol['connection_info']
8504             if connection_info.get('serial'):
8505                 disk_info = blockinfo.get_info_from_bdm(
8506                     instance, CONF.libvirt.virt_type,
8507                     instance.image_meta, vol)
8508 
8509                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
8510                 bdmi.serial = connection_info['serial']
8511                 bdmi.connection_info = connection_info
8512                 bdmi.bus = disk_info['bus']
8513                 bdmi.dev = disk_info['dev']
8514                 bdmi.type = disk_info['type']
8515                 bdmi.format = disk_info.get('format')
8516                 bdmi.boot_index = disk_info.get('boot_index')
8517                 volume_secret = self._host.find_secret('volume', vol.volume_id)
8518                 if volume_secret:
8519                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
8520 
8521                 migrate_data.bdms.append(bdmi)
8522 
8523         return migrate_data
8524 
8525     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
8526                                image_id, instance, size,
8527                                fallback_from_host=None):
8528         try:
8529             image.cache(fetch_func=fetch_func,
8530                         context=context,
8531                         filename=filename,
8532                         image_id=image_id,
8533                         size=size,
8534                         trusted_certs=instance.trusted_certs)
8535         except exception.ImageNotFound:
8536             if not fallback_from_host:
8537                 raise
8538             LOG.debug("Image %(image_id)s doesn't exist anymore "
8539                       "on image service, attempting to copy "
8540                       "image from %(host)s",
8541                       {'image_id': image_id, 'host': fallback_from_host},
8542                       instance=instance)
8543 
8544             def copy_from_host(target):
8545                 libvirt_utils.copy_image(src=target,
8546                                          dest=target,
8547                                          host=fallback_from_host,
8548                                          receive=True)
8549             image.cache(fetch_func=copy_from_host, size=size,
8550                         filename=filename)
8551 
8552         # NOTE(lyarwood): If the instance vm_state is shelved offloaded then we
8553         # must be unshelving for _try_fetch_image_cache to be called.
8554         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
8555             # NOTE(lyarwood): When using the rbd imagebackend the call to cache
8556             # above will attempt to clone from the shelved snapshot in Glance
8557             # if available from this compute. We then need to flatten the
8558             # resulting image to avoid it still referencing and ultimately
8559             # blocking the removal of the shelved snapshot at the end of the
8560             # unshelve. This is a no-op for all but the rbd imagebackend.
8561             try:
8562                 image.flatten()
8563                 LOG.debug('Image %s flattened successfully while unshelving '
8564                           'instance.', image.path, instance=instance)
8565             except NotImplementedError:
8566                 # NOTE(lyarwood): There's an argument to be made for logging
8567                 # our inability to call flatten here, however given this isn't
8568                 # implemented for most of the backends it may do more harm than
8569                 # good, concerning operators etc so for now just pass.
8570                 pass
8571 
8572     def _create_images_and_backing(self, context, instance, instance_dir,
8573                                    disk_info, fallback_from_host=None):
8574         """:param context: security context
8575            :param instance:
8576                nova.db.sqlalchemy.models.Instance object
8577                instance object that is migrated.
8578            :param instance_dir:
8579                instance path to use, calculated externally to handle block
8580                migrating an instance with an old style instance path
8581            :param disk_info:
8582                disk info specified in _get_instance_disk_info_from_config
8583                (list of dicts)
8584            :param fallback_from_host:
8585                host where we can retrieve images if the glance images are
8586                not available.
8587 
8588         """
8589 
8590         # Virtuozzo containers don't use backing file
8591         if (CONF.libvirt.virt_type == "parallels" and
8592                 instance.vm_mode == fields.VMMode.EXE):
8593             return
8594 
8595         if not disk_info:
8596             disk_info = []
8597 
8598         for info in disk_info:
8599             base = os.path.basename(info['path'])
8600             # Get image type and create empty disk image, and
8601             # create backing file in case of qcow2.
8602             instance_disk = os.path.join(instance_dir, base)
8603             if not info['backing_file'] and not os.path.exists(instance_disk):
8604                 libvirt_utils.create_image(info['type'], instance_disk,
8605                                            info['virt_disk_size'])
8606             elif info['backing_file']:
8607                 # Creating backing file follows same way as spawning instances.
8608                 cache_name = os.path.basename(info['backing_file'])
8609 
8610                 disk = self.image_backend.by_name(instance, instance_disk,
8611                                                   CONF.libvirt.images_type)
8612                 if cache_name.startswith('ephemeral'):
8613                     # The argument 'size' is used by image.cache to
8614                     # validate disk size retrieved from cache against
8615                     # the instance disk size (should always return OK)
8616                     # and ephemeral_size is used by _create_ephemeral
8617                     # to build the image if the disk is not already
8618                     # cached.
8619                     disk.cache(
8620                         fetch_func=self._create_ephemeral,
8621                         fs_label=cache_name,
8622                         os_type=instance.os_type,
8623                         filename=cache_name,
8624                         size=info['virt_disk_size'],
8625                         ephemeral_size=info['virt_disk_size'] / units.Gi)
8626                 elif cache_name.startswith('swap'):
8627                     inst_type = instance.get_flavor()
8628                     swap_mb = inst_type.swap
8629                     disk.cache(fetch_func=self._create_swap,
8630                                 filename="swap_%s" % swap_mb,
8631                                 size=swap_mb * units.Mi,
8632                                 swap_mb=swap_mb)
8633                 else:
8634                     self._try_fetch_image_cache(disk,
8635                                                 libvirt_utils.fetch_image,
8636                                                 context, cache_name,
8637                                                 instance.image_ref,
8638                                                 instance,
8639                                                 info['virt_disk_size'],
8640                                                 fallback_from_host)
8641 
8642         # if disk has kernel and ramdisk, just download
8643         # following normal way.
8644         self._fetch_instance_kernel_ramdisk(
8645             context, instance, fallback_from_host=fallback_from_host)
8646 
8647     def post_live_migration(self, context, instance, block_device_info,
8648                             migrate_data=None):
8649         # Disconnect from volume server
8650         block_device_mapping = driver.block_device_info_get_mapping(
8651                 block_device_info)
8652         for vol in block_device_mapping:
8653             # NOTE(mdbooth): The block_device_info we were passed was
8654             # initialized with BDMs from the source host before they were
8655             # updated to point to the destination. We can safely use this to
8656             # disconnect the source without re-fetching.
8657             self._disconnect_volume(context, vol['connection_info'], instance)
8658 
8659     def post_live_migration_at_source(self, context, instance, network_info):
8660         """Unplug VIFs from networks at source.
8661 
8662         :param context: security context
8663         :param instance: instance object reference
8664         :param network_info: instance network information
8665         """
8666         self.unplug_vifs(instance, network_info)
8667 
8668     def post_live_migration_at_destination(self, context,
8669                                            instance,
8670                                            network_info,
8671                                            block_migration=False,
8672                                            block_device_info=None):
8673         """Post operation of live migration at destination host.
8674 
8675         :param context: security context
8676         :param instance:
8677             nova.db.sqlalchemy.models.Instance object
8678             instance object that is migrated.
8679         :param network_info: instance network information
8680         :param block_migration: if true, post operation of block_migration.
8681         """
8682         self._reattach_instance_vifs(context, instance, network_info)
8683 
8684     def _get_instance_disk_info_from_config(self, guest_config,
8685                                             block_device_info):
8686         """Get the non-volume disk information from the domain xml
8687 
8688         :param LibvirtConfigGuest guest_config: the libvirt domain config
8689                                                 for the instance
8690         :param dict block_device_info: block device info for BDMs
8691         :returns disk_info: list of dicts with keys:
8692 
8693           * 'type': the disk type (str)
8694           * 'path': the disk path (str)
8695           * 'virt_disk_size': the virtual disk size (int)
8696           * 'backing_file': backing file of a disk image (str)
8697           * 'disk_size': physical disk size (int)
8698           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
8699         """
8700         block_device_mapping = driver.block_device_info_get_mapping(
8701             block_device_info)
8702 
8703         volume_devices = set()
8704         for vol in block_device_mapping:
8705             disk_dev = vol['mount_device'].rpartition("/")[2]
8706             volume_devices.add(disk_dev)
8707 
8708         disk_info = []
8709 
8710         if (guest_config.virt_type == 'parallels' and
8711                 guest_config.os_type == fields.VMMode.EXE):
8712             node_type = 'filesystem'
8713         else:
8714             node_type = 'disk'
8715 
8716         for device in guest_config.devices:
8717             if device.root_name != node_type:
8718                 continue
8719             disk_type = device.source_type
8720             if device.root_name == 'filesystem':
8721                 target = device.target_dir
8722                 if device.source_type == 'file':
8723                     path = device.source_file
8724                 elif device.source_type == 'block':
8725                     path = device.source_dev
8726                 else:
8727                     path = None
8728             else:
8729                 target = device.target_dev
8730                 path = device.source_path
8731 
8732             if not path:
8733                 LOG.debug('skipping disk for %s as it does not have a path',
8734                           guest_config.name)
8735                 continue
8736 
8737             if disk_type not in ['file', 'block']:
8738                 LOG.debug('skipping disk because it looks like a volume', path)
8739                 continue
8740 
8741             if target in volume_devices:
8742                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
8743                           'volume', {'path': path, 'target': target})
8744                 continue
8745 
8746             if device.root_name == 'filesystem':
8747                 driver_type = device.driver_type
8748             else:
8749                 driver_type = device.driver_format
8750             # get the real disk size or
8751             # raise a localized error if image is unavailable
8752             if disk_type == 'file' and driver_type == 'ploop':
8753                 dk_size = 0
8754                 for dirpath, dirnames, filenames in os.walk(path):
8755                     for f in filenames:
8756                         fp = os.path.join(dirpath, f)
8757                         dk_size += os.path.getsize(fp)
8758                 qemu_img_info = disk_api.get_disk_info(path)
8759                 virt_size = qemu_img_info.virtual_size
8760                 backing_file = libvirt_utils.get_disk_backing_file(path)
8761                 over_commit_size = int(virt_size) - dk_size
8762 
8763             elif disk_type == 'file' and driver_type == 'qcow2':
8764                 qemu_img_info = disk_api.get_disk_info(path)
8765                 dk_size = qemu_img_info.disk_size
8766                 virt_size = qemu_img_info.virtual_size
8767                 backing_file = libvirt_utils.get_disk_backing_file(path)
8768                 over_commit_size = int(virt_size) - dk_size
8769 
8770             elif disk_type == 'file':
8771                 dk_size = os.stat(path).st_blocks * 512
8772                 virt_size = os.path.getsize(path)
8773                 backing_file = ""
8774                 over_commit_size = 0
8775 
8776             elif disk_type == 'block' and block_device_info:
8777                 dk_size = lvm.get_volume_size(path)
8778                 virt_size = dk_size
8779                 backing_file = ""
8780                 over_commit_size = 0
8781 
8782             else:
8783                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
8784                           'determine if volume',
8785                           {'path': path, 'target': target})
8786                 continue
8787 
8788             disk_info.append({'type': driver_type,
8789                               'path': path,
8790                               'virt_disk_size': virt_size,
8791                               'backing_file': backing_file,
8792                               'disk_size': dk_size,
8793                               'over_committed_disk_size': over_commit_size})
8794         return disk_info
8795 
8796     def _get_instance_disk_info(self, instance, block_device_info):
8797         try:
8798             guest = self._host.get_guest(instance)
8799             config = guest.get_config()
8800         except libvirt.libvirtError as ex:
8801             error_code = ex.get_error_code()
8802             LOG.warning('Error from libvirt while getting description of '
8803                         '%(instance_name)s: [Error Code %(error_code)s] '
8804                         '%(ex)s',
8805                         {'instance_name': instance.name,
8806                          'error_code': error_code,
8807                          'ex': encodeutils.exception_to_unicode(ex)},
8808                         instance=instance)
8809             raise exception.InstanceNotFound(instance_id=instance.uuid)
8810 
8811         return self._get_instance_disk_info_from_config(config,
8812                                                         block_device_info)
8813 
8814     def get_instance_disk_info(self, instance,
8815                                block_device_info=None):
8816         return jsonutils.dumps(
8817             self._get_instance_disk_info(instance, block_device_info))
8818 
8819     def _get_disk_over_committed_size_total(self):
8820         """Return total over committed disk size for all instances."""
8821         # Disk size that all instance uses : virtual_size - disk_size
8822         disk_over_committed_size = 0
8823         instance_domains = self._host.list_instance_domains(only_running=False)
8824         if not instance_domains:
8825             return disk_over_committed_size
8826 
8827         # Get all instance uuids
8828         instance_uuids = [dom.UUIDString() for dom in instance_domains]
8829         ctx = nova_context.get_admin_context()
8830         # Get instance object list by uuid filter
8831         filters = {'uuid': instance_uuids}
8832         # NOTE(ankit): objects.InstanceList.get_by_filters method is
8833         # getting called twice one is here and another in the
8834         # _update_available_resource method of resource_tracker. Since
8835         # _update_available_resource method is synchronized, there is a
8836         # possibility the instances list retrieved here to calculate
8837         # disk_over_committed_size would differ to the list you would get
8838         # in _update_available_resource method for calculating usages based
8839         # on instance utilization.
8840         local_instance_list = objects.InstanceList.get_by_filters(
8841             ctx, filters, use_slave=True)
8842         # Convert instance list to dictionary with instance uuid as key.
8843         local_instances = {inst.uuid: inst for inst in local_instance_list}
8844 
8845         # Get bdms by instance uuids
8846         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
8847             ctx, instance_uuids)
8848 
8849         for dom in instance_domains:
8850             try:
8851                 guest = libvirt_guest.Guest(dom)
8852                 config = guest.get_config()
8853 
8854                 block_device_info = None
8855                 if guest.uuid in local_instances \
8856                         and (bdms and guest.uuid in bdms):
8857                     # Get block device info for instance
8858                     block_device_info = driver.get_block_device_info(
8859                         local_instances[guest.uuid], bdms[guest.uuid])
8860 
8861                 disk_infos = self._get_instance_disk_info_from_config(
8862                     config, block_device_info)
8863                 if not disk_infos:
8864                     continue
8865 
8866                 for info in disk_infos:
8867                     disk_over_committed_size += int(
8868                         info['over_committed_disk_size'])
8869             except libvirt.libvirtError as ex:
8870                 error_code = ex.get_error_code()
8871                 LOG.warning(
8872                     'Error from libvirt while getting description of '
8873                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
8874                     {'instance_name': guest.name,
8875                      'error_code': error_code,
8876                      'ex': encodeutils.exception_to_unicode(ex)})
8877             except OSError as e:
8878                 if e.errno in (errno.ENOENT, errno.ESTALE):
8879                     LOG.warning('Periodic task is updating the host stat, '
8880                                 'it is trying to get disk %(i_name)s, '
8881                                 'but disk file was removed by concurrent '
8882                                 'operations such as resize.',
8883                                 {'i_name': guest.name})
8884                 elif e.errno == errno.EACCES:
8885                     LOG.warning('Periodic task is updating the host stat, '
8886                                 'it is trying to get disk %(i_name)s, '
8887                                 'but access is denied. It is most likely '
8888                                 'due to a VM that exists on the compute '
8889                                 'node but is not managed by Nova.',
8890                                 {'i_name': guest.name})
8891                 else:
8892                     raise
8893             except exception.VolumeBDMPathNotFound as e:
8894                 LOG.warning('Periodic task is updating the host stats, '
8895                             'it is trying to get disk info for %(i_name)s, '
8896                             'but the backing volume block device was removed '
8897                             'by concurrent operations such as resize. '
8898                             'Error: %(error)s',
8899                             {'i_name': guest.name, 'error': e})
8900             except exception.DiskNotFound:
8901                 with excutils.save_and_reraise_exception() as err_ctxt:
8902                     # If the instance is undergoing a task state transition,
8903                     # like moving to another host or is being deleted, we
8904                     # should ignore this instance and move on.
8905                     if guest.uuid in local_instances:
8906                         inst = local_instances[guest.uuid]
8907                         # bug 1774249 indicated when instance is in RESIZED
8908                         # state it might also can't find back disk
8909                         if (inst.task_state is not None or
8910                             inst.vm_state == vm_states.RESIZED):
8911                             LOG.info('Periodic task is updating the host '
8912                                      'stats; it is trying to get disk info '
8913                                      'for %(i_name)s, but the backing disk '
8914                                      'was removed by a concurrent operation '
8915                                      '(task_state=%(task_state)s) and '
8916                                      '(vm_state=%(vm_state)s)',
8917                                      {'i_name': guest.name,
8918                                       'task_state': inst.task_state,
8919                                       'vm_state': inst.vm_state},
8920                                      instance=inst)
8921                             err_ctxt.reraise = False
8922 
8923             # NOTE(gtt116): give other tasks a chance.
8924             greenthread.sleep(0)
8925         return disk_over_committed_size
8926 
8927     def unfilter_instance(self, instance, network_info):
8928         """See comments of same method in firewall_driver."""
8929         self.firewall_driver.unfilter_instance(instance,
8930                                                network_info=network_info)
8931 
8932     def get_available_nodes(self, refresh=False):
8933         return [self._host.get_hostname()]
8934 
8935     def get_host_cpu_stats(self):
8936         """Return the current CPU state of the host."""
8937         return self._host.get_cpu_stats()
8938 
8939     def get_host_uptime(self):
8940         """Returns the result of calling "uptime"."""
8941         out, err = processutils.execute('env', 'LANG=C', 'uptime')
8942         return out
8943 
8944     def manage_image_cache(self, context, all_instances):
8945         """Manage the local cache of images."""
8946         self.image_cache_manager.update(context, all_instances)
8947 
8948     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8949                                   shared_storage=False):
8950         """Used only for cleanup in case migrate_disk_and_power_off fails."""
8951         try:
8952             if os.path.exists(inst_base_resize):
8953                 shutil.rmtree(inst_base, ignore_errors=True)
8954                 os.rename(inst_base_resize, inst_base)
8955                 if not shared_storage:
8956                     self._remotefs.remove_dir(dest, inst_base)
8957         except Exception:
8958             pass
8959 
8960     def _is_storage_shared_with(self, dest, inst_base):
8961         # NOTE (rmk): There are two methods of determining whether we are
8962         #             on the same filesystem: the source and dest IP are the
8963         #             same, or we create a file on the dest system via SSH
8964         #             and check whether the source system can also see it.
8965         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
8966         #                it will always be shared storage
8967         if CONF.libvirt.images_type == 'rbd':
8968             return True
8969         shared_storage = (dest == self.get_host_ip_addr())
8970         if not shared_storage:
8971             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
8972             tmp_path = os.path.join(inst_base, tmp_file)
8973 
8974             try:
8975                 self._remotefs.create_file(dest, tmp_path)
8976                 if os.path.exists(tmp_path):
8977                     shared_storage = True
8978                     os.unlink(tmp_path)
8979                 else:
8980                     self._remotefs.remove_file(dest, tmp_path)
8981             except Exception:
8982                 pass
8983         return shared_storage
8984 
8985     def migrate_disk_and_power_off(self, context, instance, dest,
8986                                    flavor, network_info,
8987                                    block_device_info=None,
8988                                    timeout=0, retry_interval=0):
8989         LOG.debug("Starting migrate_disk_and_power_off",
8990                    instance=instance)
8991 
8992         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
8993 
8994         # get_bdm_ephemeral_disk_size() will return 0 if the new
8995         # instance's requested block device mapping contain no
8996         # ephemeral devices. However, we still want to check if
8997         # the original instance's ephemeral_gb property was set and
8998         # ensure that the new requested flavor ephemeral size is greater
8999         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
9000                     instance.flavor.ephemeral_gb)
9001 
9002         # Checks if the migration needs a disk resize down.
9003         root_down = flavor.root_gb < instance.flavor.root_gb
9004         ephemeral_down = flavor.ephemeral_gb < eph_size
9005         booted_from_volume = self._is_booted_from_volume(block_device_info)
9006 
9007         if (root_down and not booted_from_volume) or ephemeral_down:
9008             reason = _("Unable to resize disk down.")
9009             raise exception.InstanceFaultRollback(
9010                 exception.ResizeError(reason=reason))
9011 
9012         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
9013         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
9014             reason = _("Migration is not supported for LVM backed instances")
9015             raise exception.InstanceFaultRollback(
9016                 exception.MigrationPreCheckError(reason=reason))
9017 
9018         # copy disks to destination
9019         # rename instance dir to +_resize at first for using
9020         # shared storage for instance dir (eg. NFS).
9021         inst_base = libvirt_utils.get_instance_path(instance)
9022         inst_base_resize = inst_base + "_resize"
9023         shared_storage = self._is_storage_shared_with(dest, inst_base)
9024 
9025         # try to create the directory on the remote compute node
9026         # if this fails we pass the exception up the stack so we can catch
9027         # failures here earlier
9028         if not shared_storage:
9029             try:
9030                 self._remotefs.create_dir(dest, inst_base)
9031             except processutils.ProcessExecutionError as e:
9032                 reason = _("not able to execute ssh command: %s") % e
9033                 raise exception.InstanceFaultRollback(
9034                     exception.ResizeError(reason=reason))
9035 
9036         self.power_off(instance, timeout, retry_interval)
9037 
9038         block_device_mapping = driver.block_device_info_get_mapping(
9039             block_device_info)
9040         for vol in block_device_mapping:
9041             connection_info = vol['connection_info']
9042             self._disconnect_volume(context, connection_info, instance)
9043 
9044         disk_info = self._get_instance_disk_info(instance, block_device_info)
9045 
9046         try:
9047             os.rename(inst_base, inst_base_resize)
9048             # if we are migrating the instance with shared storage then
9049             # create the directory.  If it is a remote node the directory
9050             # has already been created
9051             if shared_storage:
9052                 dest = None
9053                 fileutils.ensure_tree(inst_base)
9054 
9055             on_execute = lambda process: \
9056                 self.job_tracker.add_job(instance, process.pid)
9057             on_completion = lambda process: \
9058                 self.job_tracker.remove_job(instance, process.pid)
9059 
9060             for info in disk_info:
9061                 # assume inst_base == dirname(info['path'])
9062                 img_path = info['path']
9063                 fname = os.path.basename(img_path)
9064                 from_path = os.path.join(inst_base_resize, fname)
9065 
9066                 # We will not copy over the swap disk here, and rely on
9067                 # finish_migration to re-create it for us. This is ok because
9068                 # the OS is shut down, and as recreating a swap disk is very
9069                 # cheap it is more efficient than copying either locally or
9070                 # over the network. This also means we don't have to resize it.
9071                 if fname == 'disk.swap':
9072                     continue
9073 
9074                 compression = info['type'] not in NO_COMPRESSION_TYPES
9075                 libvirt_utils.copy_image(from_path, img_path, host=dest,
9076                                          on_execute=on_execute,
9077                                          on_completion=on_completion,
9078                                          compression=compression)
9079 
9080             # Ensure disk.info is written to the new path to avoid disks being
9081             # reinspected and potentially changing format.
9082             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
9083             if os.path.exists(src_disk_info_path):
9084                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
9085                 libvirt_utils.copy_image(src_disk_info_path,
9086                                          dst_disk_info_path,
9087                                          host=dest, on_execute=on_execute,
9088                                          on_completion=on_completion)
9089         except Exception:
9090             with excutils.save_and_reraise_exception():
9091                 self._cleanup_remote_migration(dest, inst_base,
9092                                                inst_base_resize,
9093                                                shared_storage)
9094 
9095         return jsonutils.dumps(disk_info)
9096 
9097     def _wait_for_running(self, instance):
9098         state = self.get_info(instance).state
9099 
9100         if state == power_state.RUNNING:
9101             LOG.info("Instance running successfully.", instance=instance)
9102             raise loopingcall.LoopingCallDone()
9103 
9104     @staticmethod
9105     def _disk_raw_to_qcow2(path):
9106         """Converts a raw disk to qcow2."""
9107         path_qcow = path + '_qcow'
9108         images.convert_image(path, path_qcow, 'raw', 'qcow2')
9109         os.rename(path_qcow, path)
9110 
9111     def finish_migration(self, context, migration, instance, disk_info,
9112                          network_info, image_meta, resize_instance,
9113                          block_device_info=None, power_on=True):
9114         LOG.debug("Starting finish_migration", instance=instance)
9115 
9116         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9117                                                   instance,
9118                                                   image_meta,
9119                                                   block_device_info)
9120         # assume _create_image does nothing if a target file exists.
9121         # NOTE: This has the intended side-effect of fetching a missing
9122         # backing file.
9123         self._create_image(context, instance, block_disk_info['mapping'],
9124                            block_device_info=block_device_info,
9125                            ignore_bdi_for_swap=True,
9126                            fallback_from_host=migration.source_compute)
9127 
9128         # Required by Quobyte CI
9129         self._ensure_console_log_for_instance(instance)
9130 
9131         gen_confdrive = functools.partial(
9132             self._create_configdrive, context, instance,
9133             InjectionInfo(admin_pass=None, network_info=network_info,
9134                           files=None))
9135 
9136         # Convert raw disks to qcow2 if migrating to host which uses
9137         # qcow2 from host which uses raw.
9138         disk_info = jsonutils.loads(disk_info)
9139         for info in disk_info:
9140             path = info['path']
9141             disk_name = os.path.basename(path)
9142 
9143             # NOTE(mdbooth): The code below looks wrong, but is actually
9144             # required to prevent a security hole when migrating from a host
9145             # with use_cow_images=False to one with use_cow_images=True.
9146             # Imagebackend uses use_cow_images to select between the
9147             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
9148             # writes to disk.info, but does not read it as it assumes qcow2.
9149             # Therefore if we don't convert raw to qcow2 here, a raw disk will
9150             # be incorrectly assumed to be qcow2, which is a severe security
9151             # flaw. The reverse is not true, because the atrociously-named-Raw
9152             # backend supports both qcow2 and raw disks, and will choose
9153             # appropriately between them as long as disk.info exists and is
9154             # correctly populated, which it is because Qcow2 writes to
9155             # disk.info.
9156             #
9157             # In general, we do not yet support format conversion during
9158             # migration. For example:
9159             #   * Converting from use_cow_images=True to use_cow_images=False
9160             #     isn't handled. This isn't a security bug, but is almost
9161             #     certainly buggy in other cases, as the 'Raw' backend doesn't
9162             #     expect a backing file.
9163             #   * Converting to/from lvm and rbd backends is not supported.
9164             #
9165             # This behaviour is inconsistent, and therefore undesirable for
9166             # users. It is tightly-coupled to implementation quirks of 2
9167             # out of 5 backends in imagebackend and defends against a severe
9168             # security flaw which is not at all obvious without deep analysis,
9169             # and is therefore undesirable to developers. We should aim to
9170             # remove it. This will not be possible, though, until we can
9171             # represent the storage layout of a specific instance
9172             # independent of the default configuration of the local compute
9173             # host.
9174 
9175             # Config disks are hard-coded to be raw even when
9176             # use_cow_images=True (see _get_disk_config_image_type),so don't
9177             # need to be converted.
9178             if (disk_name != 'disk.config' and
9179                         info['type'] == 'raw' and CONF.use_cow_images):
9180                 self._disk_raw_to_qcow2(info['path'])
9181 
9182         xml = self._get_guest_xml(context, instance, network_info,
9183                                   block_disk_info, image_meta,
9184                                   block_device_info=block_device_info)
9185         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
9186         # or not we've migrated to another host, because we unplug VIFs locally
9187         # and the status change in the port might go undetected by the neutron
9188         # L2 agent (or neutron server) so neutron may not know that the VIF was
9189         # unplugged in the first place and never send an event.
9190         guest = self._create_domain_and_network(context, xml, instance,
9191                                         network_info,
9192                                         block_device_info=block_device_info,
9193                                         power_on=power_on,
9194                                         vifs_already_plugged=True,
9195                                         post_xml_callback=gen_confdrive)
9196         if power_on:
9197             timer = loopingcall.FixedIntervalLoopingCall(
9198                                                     self._wait_for_running,
9199                                                     instance)
9200             timer.start(interval=0.5).wait()
9201 
9202             # Sync guest time after migration.
9203             guest.sync_guest_time()
9204 
9205         LOG.debug("finish_migration finished successfully.", instance=instance)
9206 
9207     def _cleanup_failed_migration(self, inst_base):
9208         """Make sure that a failed migrate doesn't prevent us from rolling
9209         back in a revert.
9210         """
9211         try:
9212             shutil.rmtree(inst_base)
9213         except OSError as e:
9214             if e.errno != errno.ENOENT:
9215                 raise
9216 
9217     def finish_revert_migration(self, context, instance, network_info,
9218                                 migration, block_device_info=None,
9219                                 power_on=True):
9220         LOG.debug("Starting finish_revert_migration",
9221                   instance=instance)
9222 
9223         inst_base = libvirt_utils.get_instance_path(instance)
9224         inst_base_resize = inst_base + "_resize"
9225 
9226         # NOTE(danms): if we're recovering from a failed migration,
9227         # make sure we don't have a left-over same-host base directory
9228         # that would conflict. Also, don't fail on the rename if the
9229         # failure happened early.
9230         if os.path.exists(inst_base_resize):
9231             self._cleanup_failed_migration(inst_base)
9232             os.rename(inst_base_resize, inst_base)
9233 
9234         root_disk = self.image_backend.by_name(instance, 'disk')
9235         # Once we rollback, the snapshot is no longer needed, so remove it
9236         if root_disk.exists():
9237             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9238             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
9239 
9240         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
9241                                             instance,
9242                                             instance.image_meta,
9243                                             block_device_info)
9244         xml = self._get_guest_xml(context, instance, network_info, disk_info,
9245                                   instance.image_meta,
9246                                   block_device_info=block_device_info)
9247         # NOTE(artom) In some Neutron or port configurations we've already
9248         # waited for vif-plugged events in the compute manager's
9249         # _finish_revert_resize_network_migrate_finish(), right after updating
9250         # the port binding. For any ports not covered by those "bind-time"
9251         # events, we wait for "plug-time" events here.
9252         events = network_info.get_plug_time_events(migration)
9253         if events:
9254             LOG.debug('Instance is using plug-time events: %s', events,
9255                       instance=instance)
9256         self._create_domain_and_network(
9257             context, xml, instance, network_info,
9258             block_device_info=block_device_info, power_on=power_on,
9259             external_events=events)
9260 
9261         if power_on:
9262             timer = loopingcall.FixedIntervalLoopingCall(
9263                                                     self._wait_for_running,
9264                                                     instance)
9265             timer.start(interval=0.5).wait()
9266 
9267         LOG.debug("finish_revert_migration finished successfully.",
9268                   instance=instance)
9269 
9270     def confirm_migration(self, context, migration, instance, network_info):
9271         """Confirms a resize, destroying the source VM."""
9272         self._cleanup_resize(context, instance, network_info)
9273 
9274     @staticmethod
9275     def _get_io_devices(xml_doc):
9276         """get the list of io devices from the xml document."""
9277         result = {"volumes": [], "ifaces": []}
9278         try:
9279             doc = etree.fromstring(xml_doc)
9280         except Exception:
9281             return result
9282         blocks = [('./devices/disk', 'volumes'),
9283             ('./devices/interface', 'ifaces')]
9284         for block, key in blocks:
9285             section = doc.findall(block)
9286             for node in section:
9287                 for child in node:
9288                     if child.tag == 'target' and child.get('dev'):
9289                         result[key].append(child.get('dev'))
9290         return result
9291 
9292     def get_diagnostics(self, instance):
9293         guest = self._host.get_guest(instance)
9294 
9295         # TODO(sahid): We are converting all calls from a
9296         # virDomain object to use nova.virt.libvirt.Guest.
9297         # We should be able to remove domain at the end.
9298         domain = guest._domain
9299         output = {}
9300         # get cpu time, might launch an exception if the method
9301         # is not supported by the underlying hypervisor being
9302         # used by libvirt
9303         try:
9304             for vcpu in guest.get_vcpus_info():
9305                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
9306         except libvirt.libvirtError:
9307             pass
9308         # get io status
9309         xml = guest.get_xml_desc()
9310         dom_io = LibvirtDriver._get_io_devices(xml)
9311         for guest_disk in dom_io["volumes"]:
9312             try:
9313                 # blockStats might launch an exception if the method
9314                 # is not supported by the underlying hypervisor being
9315                 # used by libvirt
9316                 stats = domain.blockStats(guest_disk)
9317                 output[guest_disk + "_read_req"] = stats[0]
9318                 output[guest_disk + "_read"] = stats[1]
9319                 output[guest_disk + "_write_req"] = stats[2]
9320                 output[guest_disk + "_write"] = stats[3]
9321                 output[guest_disk + "_errors"] = stats[4]
9322             except libvirt.libvirtError:
9323                 pass
9324         for interface in dom_io["ifaces"]:
9325             try:
9326                 # interfaceStats might launch an exception if the method
9327                 # is not supported by the underlying hypervisor being
9328                 # used by libvirt
9329                 stats = domain.interfaceStats(interface)
9330                 output[interface + "_rx"] = stats[0]
9331                 output[interface + "_rx_packets"] = stats[1]
9332                 output[interface + "_rx_errors"] = stats[2]
9333                 output[interface + "_rx_drop"] = stats[3]
9334                 output[interface + "_tx"] = stats[4]
9335                 output[interface + "_tx_packets"] = stats[5]
9336                 output[interface + "_tx_errors"] = stats[6]
9337                 output[interface + "_tx_drop"] = stats[7]
9338             except libvirt.libvirtError:
9339                 pass
9340         output["memory"] = domain.maxMemory()
9341         # memoryStats might launch an exception if the method
9342         # is not supported by the underlying hypervisor being
9343         # used by libvirt
9344         try:
9345             mem = domain.memoryStats()
9346             for key in mem.keys():
9347                 output["memory-" + key] = mem[key]
9348         except (libvirt.libvirtError, AttributeError):
9349             pass
9350         return output
9351 
9352     def get_instance_diagnostics(self, instance):
9353         guest = self._host.get_guest(instance)
9354 
9355         # TODO(sahid): We are converting all calls from a
9356         # virDomain object to use nova.virt.libvirt.Guest.
9357         # We should be able to remove domain at the end.
9358         domain = guest._domain
9359 
9360         xml = guest.get_xml_desc()
9361         xml_doc = etree.fromstring(xml)
9362 
9363         # TODO(sahid): Needs to use get_info but more changes have to
9364         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
9365         # needed.
9366         state, max_mem, mem, num_cpu, cpu_time = guest._get_domain_info()
9367         config_drive = configdrive.required_by(instance)
9368         launched_at = timeutils.normalize_time(instance.launched_at)
9369         uptime = timeutils.delta_seconds(launched_at,
9370                                          timeutils.utcnow())
9371         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
9372                                         driver='libvirt',
9373                                         config_drive=config_drive,
9374                                         hypervisor=CONF.libvirt.virt_type,
9375                                         hypervisor_os='linux',
9376                                         uptime=uptime)
9377         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
9378             maximum=max_mem / units.Mi,
9379             used=mem / units.Mi)
9380 
9381         # get cpu time, might launch an exception if the method
9382         # is not supported by the underlying hypervisor being
9383         # used by libvirt
9384         try:
9385             for vcpu in guest.get_vcpus_info():
9386                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
9387         except libvirt.libvirtError:
9388             pass
9389         # get io status
9390         dom_io = LibvirtDriver._get_io_devices(xml)
9391         for guest_disk in dom_io["volumes"]:
9392             try:
9393                 # blockStats might launch an exception if the method
9394                 # is not supported by the underlying hypervisor being
9395                 # used by libvirt
9396                 stats = domain.blockStats(guest_disk)
9397                 diags.add_disk(read_bytes=stats[1],
9398                                read_requests=stats[0],
9399                                write_bytes=stats[3],
9400                                write_requests=stats[2],
9401                                errors_count=stats[4])
9402             except libvirt.libvirtError:
9403                 pass
9404 
9405         for interface in xml_doc.findall('./devices/interface'):
9406             mac_address = interface.find('mac').get('address')
9407             target = interface.find('./target')
9408 
9409             # add nic that has no target (therefore no stats)
9410             if target is None:
9411                 diags.add_nic(mac_address=mac_address)
9412                 continue
9413 
9414             # add nic with stats
9415             dev = target.get('dev')
9416             try:
9417                 if dev:
9418                     # interfaceStats might launch an exception if the
9419                     # method is not supported by the underlying hypervisor
9420                     # being used by libvirt
9421                     stats = domain.interfaceStats(dev)
9422                     diags.add_nic(mac_address=mac_address,
9423                                   rx_octets=stats[0],
9424                                   rx_errors=stats[2],
9425                                   rx_drop=stats[3],
9426                                   rx_packets=stats[1],
9427                                   tx_octets=stats[4],
9428                                   tx_errors=stats[6],
9429                                   tx_drop=stats[7],
9430                                   tx_packets=stats[5])
9431 
9432             except libvirt.libvirtError:
9433                 pass
9434 
9435         return diags
9436 
9437     @staticmethod
9438     def _prepare_device_bus(dev):
9439         """Determines the device bus and its hypervisor assigned address
9440         """
9441         bus = None
9442         address = (dev.device_addr.format_address() if
9443                    dev.device_addr else None)
9444         if isinstance(dev.device_addr,
9445                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
9446             bus = objects.PCIDeviceBus()
9447         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9448             if dev.target_bus == 'scsi':
9449                 bus = objects.SCSIDeviceBus()
9450             elif dev.target_bus == 'ide':
9451                 bus = objects.IDEDeviceBus()
9452             elif dev.target_bus == 'usb':
9453                 bus = objects.USBDeviceBus()
9454         if address is not None and bus is not None:
9455             bus.address = address
9456         return bus
9457 
9458     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
9459                                   trusted_by_mac):
9460         """Builds a metadata object for a network interface
9461 
9462         :param dev: The LibvirtConfigGuestInterface to build metadata for.
9463         :param vifs_to_expose: The list of tagged and/or vlan'ed
9464                                VirtualInterface objects.
9465         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9466         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
9467                                associations.
9468         :return: A NetworkInterfaceMetadata object, or None.
9469         """
9470         vif = vifs_to_expose.get(dev.mac_addr)
9471         if not vif:
9472             LOG.debug('No VIF found with MAC %s, not building metadata',
9473                       dev.mac_addr)
9474             return None
9475         bus = self._prepare_device_bus(dev)
9476         device = objects.NetworkInterfaceMetadata(mac=vif.address)
9477         if 'tag' in vif and vif.tag:
9478             device.tags = [vif.tag]
9479         if bus:
9480             device.bus = bus
9481         vlan = vlans_by_mac.get(vif.address)
9482         if vlan:
9483             device.vlan = int(vlan)
9484         device.vf_trusted = trusted_by_mac.get(vif.address, False)
9485         return device
9486 
9487     def _build_disk_metadata(self, dev, tagged_bdms):
9488         """Builds a metadata object for a disk
9489 
9490         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
9491         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
9492         :return: A DiskMetadata object, or None.
9493         """
9494         bdm = tagged_bdms.get(dev.target_dev)
9495         if not bdm:
9496             LOG.debug('No BDM found with device name %s, not building '
9497                       'metadata.', dev.target_dev)
9498             return None
9499         bus = self._prepare_device_bus(dev)
9500         device = objects.DiskMetadata(tags=[bdm.tag])
9501         # NOTE(artom) Setting the serial (which corresponds to
9502         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
9503         # find the disks's BlockDeviceMapping object when we detach the
9504         # volume and want to clean up its metadata.
9505         device.serial = bdm.volume_id
9506         if bus:
9507             device.bus = bus
9508         return device
9509 
9510     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
9511         """Builds a metadata object for a hostdev. This can only be a PF, so we
9512         don't need trusted_by_mac like in _build_interface_metadata because
9513         only VFs can be trusted.
9514 
9515         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
9516         :param vifs_to_expose: The list of tagged and/or vlan'ed
9517                                VirtualInterface objects.
9518         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
9519         :return: A NetworkInterfaceMetadata object, or None.
9520         """
9521         # Strip out the leading '0x'
9522         pci_address = pci_utils.get_pci_address(
9523             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
9524         try:
9525             mac = pci_utils.get_mac_by_pci_address(pci_address,
9526                                                    pf_interface=True)
9527         except exception.PciDeviceNotFoundById:
9528             LOG.debug('Not exposing metadata for not found PCI device %s',
9529                       pci_address)
9530             return None
9531 
9532         vif = vifs_to_expose.get(mac)
9533         if not vif:
9534             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
9535             return None
9536 
9537         device = objects.NetworkInterfaceMetadata(mac=mac)
9538         device.bus = objects.PCIDeviceBus(address=pci_address)
9539         if 'tag' in vif and vif.tag:
9540             device.tags = [vif.tag]
9541         vlan = vlans_by_mac.get(mac)
9542         if vlan:
9543             device.vlan = int(vlan)
9544         return device
9545 
9546     def _build_device_metadata(self, context, instance):
9547         """Builds a metadata object for instance devices, that maps the user
9548            provided tag to the hypervisor assigned device address.
9549         """
9550         def _get_device_name(bdm):
9551             return block_device.strip_dev(bdm.device_name)
9552 
9553         network_info = instance.info_cache.network_info
9554         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
9555         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
9556         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
9557                                                                  instance.uuid)
9558         vifs_to_expose = {vif.address: vif for vif in vifs
9559                           if ('tag' in vif and vif.tag) or
9560                              vlans_by_mac.get(vif.address)}
9561         # TODO(mriedem): We should be able to avoid the DB query here by using
9562         # block_device_info['block_device_mapping'] which is passed into most
9563         # methods that call this function.
9564         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9565             context, instance.uuid)
9566         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
9567 
9568         devices = []
9569         guest = self._host.get_guest(instance)
9570         xml = guest.get_xml_desc()
9571         xml_dom = etree.fromstring(xml)
9572         guest_config = vconfig.LibvirtConfigGuest()
9573         guest_config.parse_dom(xml_dom)
9574 
9575         for dev in guest_config.devices:
9576             device = None
9577             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
9578                 device = self._build_interface_metadata(dev, vifs_to_expose,
9579                                                         vlans_by_mac,
9580                                                         trusted_by_mac)
9581             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
9582                 device = self._build_disk_metadata(dev, tagged_bdms)
9583             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
9584                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
9585                                                       vlans_by_mac)
9586             if device:
9587                 devices.append(device)
9588         if devices:
9589             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
9590             return dev_meta
9591 
9592     def instance_on_disk(self, instance):
9593         # ensure directories exist and are writable
9594         instance_path = libvirt_utils.get_instance_path(instance)
9595         LOG.debug('Checking instance files accessibility %s', instance_path,
9596                   instance=instance)
9597         shared_instance_path = os.access(instance_path, os.W_OK)
9598         # NOTE(flwang): For shared block storage scenario, the file system is
9599         # not really shared by the two hosts, but the volume of evacuated
9600         # instance is reachable.
9601         shared_block_storage = (self.image_backend.backend().
9602                                 is_shared_block_storage())
9603         return shared_instance_path or shared_block_storage
9604 
9605     def inject_network_info(self, instance, nw_info):
9606         self.firewall_driver.setup_basic_filtering(instance, nw_info)
9607 
9608     def delete_instance_files(self, instance):
9609         target = libvirt_utils.get_instance_path(instance)
9610         # A resize may be in progress
9611         target_resize = target + '_resize'
9612         # Other threads may attempt to rename the path, so renaming the path
9613         # to target + '_del' (because it is atomic) and iterating through
9614         # twice in the unlikely event that a concurrent rename occurs between
9615         # the two rename attempts in this method. In general this method
9616         # should be fairly thread-safe without these additional checks, since
9617         # other operations involving renames are not permitted when the task
9618         # state is not None and the task state should be set to something
9619         # other than None by the time this method is invoked.
9620         target_del = target + '_del'
9621         for i in range(2):
9622             try:
9623                 os.rename(target, target_del)
9624                 break
9625             except Exception:
9626                 pass
9627             try:
9628                 os.rename(target_resize, target_del)
9629                 break
9630             except Exception:
9631                 pass
9632         # Either the target or target_resize path may still exist if all
9633         # rename attempts failed.
9634         remaining_path = None
9635         for p in (target, target_resize):
9636             if os.path.exists(p):
9637                 remaining_path = p
9638                 break
9639 
9640         # A previous delete attempt may have been interrupted, so target_del
9641         # may exist even if all rename attempts during the present method
9642         # invocation failed due to the absence of both target and
9643         # target_resize.
9644         if not remaining_path and os.path.exists(target_del):
9645             self.job_tracker.terminate_jobs(instance)
9646 
9647             LOG.info('Deleting instance files %s', target_del,
9648                      instance=instance)
9649             remaining_path = target_del
9650             try:
9651                 shutil.rmtree(target_del)
9652             except OSError as e:
9653                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
9654                           {'target': target_del, 'e': e}, instance=instance)
9655 
9656         # It is possible that the delete failed, if so don't mark the instance
9657         # as cleaned.
9658         if remaining_path and os.path.exists(remaining_path):
9659             LOG.info('Deletion of %s failed', remaining_path,
9660                      instance=instance)
9661             return False
9662 
9663         LOG.info('Deletion of %s complete', target_del, instance=instance)
9664         return True
9665 
9666     @property
9667     def need_legacy_block_device_info(self):
9668         return False
9669 
9670     def default_root_device_name(self, instance, image_meta, root_bdm):
9671         disk_bus = blockinfo.get_disk_bus_for_device_type(
9672             instance, CONF.libvirt.virt_type, image_meta, "disk")
9673         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
9674             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
9675         root_info = blockinfo.get_root_info(
9676             instance, CONF.libvirt.virt_type, image_meta,
9677             root_bdm, disk_bus, cdrom_bus)
9678         return block_device.prepend_dev(root_info['dev'])
9679 
9680     def default_device_names_for_instance(self, instance, root_device_name,
9681                                           *block_device_lists):
9682         block_device_mapping = list(itertools.chain(*block_device_lists))
9683         # NOTE(ndipanov): Null out the device names so that blockinfo code
9684         #                 will assign them
9685         for bdm in block_device_mapping:
9686             if bdm.device_name is not None:
9687                 LOG.info(
9688                     "Ignoring supplied device name: %(device_name)s. "
9689                     "Libvirt can't honour user-supplied dev names",
9690                     {'device_name': bdm.device_name}, instance=instance)
9691                 bdm.device_name = None
9692         block_device_info = driver.get_block_device_info(instance,
9693                                                          block_device_mapping)
9694 
9695         blockinfo.default_device_names(CONF.libvirt.virt_type,
9696                                        nova_context.get_admin_context(),
9697                                        instance,
9698                                        block_device_info,
9699                                        instance.image_meta)
9700 
9701     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
9702         block_device_info = driver.get_block_device_info(instance, bdms)
9703         instance_info = blockinfo.get_disk_info(
9704                 CONF.libvirt.virt_type, instance,
9705                 instance.image_meta, block_device_info=block_device_info)
9706 
9707         suggested_dev_name = block_device_obj.device_name
9708         if suggested_dev_name is not None:
9709             LOG.info(
9710                 'Ignoring supplied device name: %(suggested_dev)s',
9711                 {'suggested_dev': suggested_dev_name}, instance=instance)
9712 
9713         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
9714         #                 only when it's actually not set on the bd object
9715         block_device_obj.device_name = None
9716         disk_info = blockinfo.get_info_from_bdm(
9717             instance, CONF.libvirt.virt_type, instance.image_meta,
9718             block_device_obj, mapping=instance_info['mapping'])
9719         return block_device.prepend_dev(disk_info['dev'])
9720 
9721     def is_supported_fs_format(self, fs_type):
9722         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
9723                            nova.privsep.fs.FS_FORMAT_EXT3,
9724                            nova.privsep.fs.FS_FORMAT_EXT4,
9725                            nova.privsep.fs.FS_FORMAT_XFS]
9726 
9727     def _get_cpu_traits(self):
9728         """Get CPU traits of VMs based on guest CPU model config:
9729         1. if mode is 'host-model' or 'host-passthrough', use host's
9730         CPU features.
9731         2. if mode is None, choose a default CPU model based on CPU
9732         architecture.
9733         3. if mode is 'custom', use cpu_model to generate CPU features.
9734         The code also accounts for cpu_model_extra_flags configuration when
9735         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
9736         ensures user specified CPU feature flags to be included.
9737         :return: A dict of trait names mapped to boolean values or None.
9738         """
9739         cpu = self._get_guest_cpu_model_config()
9740         if not cpu:
9741             LOG.info('The current libvirt hypervisor %(virt_type)s '
9742                      'does not support reporting CPU traits.',
9743                      {'virt_type': CONF.libvirt.virt_type})
9744             return
9745 
9746         caps = deepcopy(self._host.get_capabilities())
9747         if cpu.mode in ('host-model', 'host-passthrough'):
9748             # Account for features in cpu_model_extra_flags conf
9749             host_features = [f.name for f in
9750                              caps.host.cpu.features | cpu.features]
9751             return libvirt_utils.cpu_features_to_traits(host_features)
9752 
9753         # Choose a default CPU model when cpu_mode is not specified
9754         if cpu.mode is None:
9755             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
9756                 caps.host.cpu.arch)
9757             caps.host.cpu.features = set()
9758         else:
9759             # For custom mode, set model to guest CPU model
9760             caps.host.cpu.model = cpu.model
9761             caps.host.cpu.features = set()
9762             # Account for features in cpu_model_extra_flags conf
9763             for f in cpu.features:
9764                 caps.host.cpu.add_feature(
9765                     vconfig.LibvirtConfigCPUFeature(name=f.name))
9766 
9767         xml_str = caps.host.cpu.to_xml()
9768         features_xml = self._get_guest_baseline_cpu_features(xml_str)
9769         feature_names = []
9770         if features_xml:
9771             cpu.parse_str(features_xml)
9772             feature_names = [f.name for f in cpu.features]
9773         return libvirt_utils.cpu_features_to_traits(feature_names)
9774 
9775     def _get_guest_baseline_cpu_features(self, xml_str):
9776         """Calls libvirt's baselineCPU API to compute the biggest set of
9777         CPU features which is compatible with the given host CPU.
9778 
9779         :param xml_str: XML description of host CPU
9780         :return: An XML string of the computed CPU, or None on error
9781         """
9782         LOG.debug("Libvirt baseline CPU %s", xml_str)
9783         # TODO(lei-zh): baselineCPU is not supported on all platforms.
9784         # There is some work going on in the libvirt community to replace the
9785         # baseline call. Consider using the new apis when they are ready. See
9786         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
9787         try:
9788             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
9789                 return self._host.get_connection().baselineCPU(
9790                     [xml_str],
9791                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
9792             else:
9793                 return self._host.get_connection().baselineCPU([xml_str])
9794         except libvirt.libvirtError as ex:
9795             with excutils.save_and_reraise_exception() as ctxt:
9796                 error_code = ex.get_error_code()
9797                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9798                     ctxt.reraise = False
9799                     LOG.info('URI %(uri)s does not support full set'
9800                              ' of host capabilities: %(error)s',
9801                              {'uri': self._host._uri, 'error': ex})
9802                     return None
