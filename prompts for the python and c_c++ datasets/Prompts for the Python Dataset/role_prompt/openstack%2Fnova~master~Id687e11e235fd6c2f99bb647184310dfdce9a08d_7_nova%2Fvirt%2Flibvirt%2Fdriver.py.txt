I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import binascii
29 import collections
30 from collections import deque
31 import contextlib
32 import errno
33 import functools
34 import glob
35 import itertools
36 import operator
37 import os
38 import pwd
39 import random
40 import shutil
41 import tempfile
42 import time
43 import uuid
44 
45 from castellan import key_manager
46 from copy import deepcopy
47 import eventlet
48 from eventlet import greenthread
49 from eventlet import tpool
50 from lxml import etree
51 from os_brick import encryptors
52 from os_brick.encryptors import luks as luks_encryptor
53 from os_brick import exception as brick_exception
54 from os_brick.initiator import connector
55 from oslo_concurrency import processutils
56 from oslo_log import log as logging
57 from oslo_serialization import base64
58 from oslo_serialization import jsonutils
59 from oslo_service import loopingcall
60 from oslo_utils import encodeutils
61 from oslo_utils import excutils
62 from oslo_utils import fileutils
63 from oslo_utils import importutils
64 from oslo_utils import strutils
65 from oslo_utils import timeutils
66 from oslo_utils import units
67 from oslo_utils import uuidutils
68 import six
69 from six.moves import range
70 
71 from nova.api.metadata import base as instance_metadata
72 from nova.api.metadata import password
73 from nova import block_device
74 from nova.compute import power_state
75 from nova.compute import task_states
76 from nova.compute import utils as compute_utils
77 from nova.compute import vm_states
78 import nova.conf
79 from nova.console import serial as serial_console
80 from nova.console import type as ctype
81 from nova import context as nova_context
82 from nova import crypto
83 from nova import exception
84 from nova.i18n import _
85 from nova import image
86 from nova.network import model as network_model
87 from nova import objects
88 from nova.objects import diagnostics as diagnostics_obj
89 from nova.objects import fields
90 from nova.objects import migrate_data as migrate_data_obj
91 from nova.pci import manager as pci_manager
92 from nova.pci import utils as pci_utils
93 import nova.privsep.libvirt
94 import nova.privsep.path
95 import nova.privsep.utils
96 from nova import rc_fields
97 from nova import utils
98 from nova import version
99 from nova.virt import block_device as driver_block_device
100 from nova.virt import configdrive
101 from nova.virt.disk import api as disk_api
102 from nova.virt.disk.vfs import guestfs
103 from nova.virt import driver
104 from nova.virt import firewall
105 from nova.virt import hardware
106 from nova.virt.image import model as imgmodel
107 from nova.virt import images
108 from nova.virt.libvirt import blockinfo
109 from nova.virt.libvirt import config as vconfig
110 from nova.virt.libvirt import designer
111 from nova.virt.libvirt import firewall as libvirt_firewall
112 from nova.virt.libvirt import guest as libvirt_guest
113 from nova.virt.libvirt import host
114 from nova.virt.libvirt import imagebackend
115 from nova.virt.libvirt import imagecache
116 from nova.virt.libvirt import instancejobtracker
117 from nova.virt.libvirt import migration as libvirt_migrate
118 from nova.virt.libvirt.storage import dmcrypt
119 from nova.virt.libvirt.storage import lvm
120 from nova.virt.libvirt.storage import rbd_utils
121 from nova.virt.libvirt import utils as libvirt_utils
122 from nova.virt.libvirt import vif as libvirt_vif
123 from nova.virt.libvirt.volume import mount
124 from nova.virt.libvirt.volume import remotefs
125 from nova.virt import netutils
126 from nova.volume import cinder
127 
128 libvirt = None
129 
130 uefi_logged = False
131 
132 LOG = logging.getLogger(__name__)
133 
134 CONF = nova.conf.CONF
135 
136 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
137     libvirt_firewall.__name__,
138     libvirt_firewall.IptablesFirewallDriver.__name__)
139 
140 DEFAULT_UEFI_LOADER_PATH = {
141     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
142     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
143 }
144 
145 MAX_CONSOLE_BYTES = 100 * units.Ki
146 
147 # The libvirt driver will prefix any disable reason codes with this string.
148 DISABLE_PREFIX = 'AUTO: '
149 # Disable reason for the service which was enabled or disabled without reason
150 DISABLE_REASON_UNDEFINED = None
151 
152 # Guest config console string
153 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
154 
155 GuestNumaConfig = collections.namedtuple(
156     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
157 
158 
159 class InjectionInfo(collections.namedtuple(
160         'InjectionInfo', ['network_info', 'files', 'admin_pass'])):
161     __slots__ = ()
162 
163     def __repr__(self):
164         return ('InjectionInfo(network_info=%r, files=%r, '
165                 'admin_pass=<SANITIZED>)') % (self.network_info, self.files)
166 
167 libvirt_volume_drivers = [
168     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
169     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
170     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
171     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
172     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
173     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
174     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
175     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
176     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
177     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
178     'fibre_channel='
179         'nova.virt.libvirt.volume.fibrechannel.'
180         'LibvirtFibreChannelVolumeDriver',
181     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
182     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
183     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
184     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
185     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
186     'vzstorage='
187         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
188     'veritas_hyperscale='
189         'nova.virt.libvirt.volume.vrtshyperscale.'
190         'LibvirtHyperScaleVolumeDriver',
191     'storpool=nova.virt.libvirt.volume.storpool.LibvirtStorPoolVolumeDriver',
192     'nvmeof=nova.virt.libvirt.volume.nvme.LibvirtNVMEVolumeDriver',
193 ]
194 
195 
196 def patch_tpool_proxy():
197     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
198     or __repr__() calls. See bug #962840 for details.
199     We perform a monkey patch to replace those two instance methods.
200     """
201     def str_method(self):
202         return str(self._obj)
203 
204     def repr_method(self):
205         return repr(self._obj)
206 
207     tpool.Proxy.__str__ = str_method
208     tpool.Proxy.__repr__ = repr_method
209 
210 
211 patch_tpool_proxy()
212 
213 # For information about when MIN_LIBVIRT_VERSION and
214 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
215 #
216 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
217 #
218 # Currently this is effectively the min version for i686/x86_64
219 # + KVM/QEMU, as other architectures/hypervisors require newer
220 # versions. Over time, this will become a common min version
221 # for all architectures/hypervisors, as this value rises to
222 # meet them.
223 MIN_LIBVIRT_VERSION = (1, 3, 1)
224 MIN_QEMU_VERSION = (2, 5, 0)
225 # TODO(berrange): Re-evaluate this at start of each release cycle
226 # to decide if we want to plan a future min version bump.
227 # MIN_LIBVIRT_VERSION can be updated to match this after
228 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
229 # one cycle
230 NEXT_MIN_LIBVIRT_VERSION = (3, 0, 0)
231 NEXT_MIN_QEMU_VERSION = (2, 8, 0)
232 
233 
234 # Virtuozzo driver support
235 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
236 
237 # Ability to set the user guest password with parallels
238 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
239 
240 # libvirt < 1.3 reported virt_functions capability
241 # only when VFs are enabled.
242 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
243 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
244 
245 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
246 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
247 MIN_QEMU_VIRTLOGD = (2, 7, 0)
248 
249 
250 # aarch64 architecture with KVM
251 # 'chardev' support got sorted out in 3.6.0
252 MIN_LIBVIRT_KVM_AARCH64_VERSION = (3, 6, 0)
253 
254 # Names of the types that do not get compressed during migration
255 NO_COMPRESSION_TYPES = ('qcow2',)
256 
257 
258 # number of serial console limit
259 QEMU_MAX_SERIAL_PORTS = 4
260 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
261 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
262 
263 # libvirt postcopy support
264 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
265 
266 MIN_LIBVIRT_OTHER_ARCH = {
267     fields.Architecture.AARCH64: MIN_LIBVIRT_KVM_AARCH64_VERSION,
268 }
269 
270 # perf events support
271 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
272 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
273 
274 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
275                                 'mbml': 'mbm_local',
276                                 'mbmt': 'mbm_total',
277                                }
278 
279 # Mediated devices support
280 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
281 
282 # libvirt>=3.10 is required for volume multiattach if qemu<2.10.
283 # See https://bugzilla.redhat.com/show_bug.cgi?id=1378242
284 # for details.
285 MIN_LIBVIRT_MULTIATTACH = (3, 10, 0)
286 
287 MIN_LIBVIRT_LUKS_VERSION = (2, 2, 0)
288 MIN_QEMU_LUKS_VERSION = (2, 6, 0)
289 
290 MIN_LIBVIRT_FILE_BACKED_VERSION = (4, 0, 0)
291 MIN_QEMU_FILE_BACKED_VERSION = (2, 6, 0)
292 
293 MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION = (4, 4, 0)
294 MIN_QEMU_FILE_BACKED_DISCARD_VERSION = (2, 10, 0)
295 
296 VGPU_RESOURCE_SEMAPHORE = "vgpu_resources"
297 
298 MIN_MIGRATION_SPEED_BW = 1  # 1 MiB/s
299 
300 
301 class LibvirtDriver(driver.ComputeDriver):
302     capabilities = {
303         "has_imagecache": True,
304         "supports_evacuate": True,
305         "supports_migrate_to_same_host": False,
306         "supports_attach_interface": True,
307         "supports_device_tagging": True,
308         "supports_tagged_attach_interface": True,
309         "supports_tagged_attach_volume": True,
310         "supports_extend_volume": True,
311         # Multiattach support is conditional on qemu and libvirt versions
312         # determined in init_host.
313         "supports_multiattach": False,
314         "supports_trusted_certs": True,
315     }
316 
317     def __init__(self, virtapi, read_only=False):
318         super(LibvirtDriver, self).__init__(virtapi)
319 
320         global libvirt
321         if libvirt is None:
322             libvirt = importutils.import_module('libvirt')
323             libvirt_migrate.libvirt = libvirt
324 
325         self._host = host.Host(self._uri(), read_only,
326                                lifecycle_event_handler=self.emit_event,
327                                conn_event_handler=self._handle_conn_event)
328         self._initiator = None
329         self._fc_wwnns = None
330         self._fc_wwpns = None
331         self._caps = None
332         self._supported_perf_events = []
333         self.firewall_driver = firewall.load_driver(
334             DEFAULT_FIREWALL_DRIVER,
335             host=self._host)
336 
337         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
338 
339         # TODO(mriedem): Long-term we should load up the volume drivers on
340         # demand as needed rather than doing this on startup, as there might
341         # be unsupported volume drivers in this list based on the underlying
342         # platform.
343         self.volume_drivers = self._get_volume_drivers()
344 
345         self._disk_cachemode = None
346         self.image_cache_manager = imagecache.ImageCacheManager()
347         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
348 
349         self.disk_cachemodes = {}
350 
351         self.valid_cachemodes = ["default",
352                                  "none",
353                                  "writethrough",
354                                  "writeback",
355                                  "directsync",
356                                  "unsafe",
357                                 ]
358         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
359                                                                       'qemu')
360 
361         for mode_str in CONF.libvirt.disk_cachemodes:
362             disk_type, sep, cache_mode = mode_str.partition('=')
363             if cache_mode not in self.valid_cachemodes:
364                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
365                             'for disk type %(disk_type)s.',
366                             {'cache_mode': cache_mode, 'disk_type': disk_type})
367                 continue
368             self.disk_cachemodes[disk_type] = cache_mode
369 
370         self._volume_api = cinder.API()
371         self._image_api = image.API()
372 
373         sysinfo_serial_funcs = {
374             'none': lambda: None,
375             'hardware': self._get_host_sysinfo_serial_hardware,
376             'os': self._get_host_sysinfo_serial_os,
377             'auto': self._get_host_sysinfo_serial_auto,
378         }
379 
380         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
381             CONF.libvirt.sysinfo_serial)
382 
383         self.job_tracker = instancejobtracker.InstanceJobTracker()
384         self._remotefs = remotefs.RemoteFilesystem()
385 
386         self._live_migration_flags = self._block_migration_flags = 0
387         self.active_migrations = {}
388 
389         # Compute reserved hugepages from conf file at the very
390         # beginning to ensure any syntax error will be reported and
391         # avoid any re-calculation when computing resources.
392         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
393 
394     def _get_volume_drivers(self):
395         driver_registry = dict()
396 
397         for driver_str in libvirt_volume_drivers:
398             driver_type, _sep, driver = driver_str.partition('=')
399             driver_class = importutils.import_class(driver)
400             try:
401                 driver_registry[driver_type] = driver_class(self._host)
402             except brick_exception.InvalidConnectorProtocol:
403                 LOG.debug('Unable to load volume driver %s. It is not '
404                           'supported on this host.', driver)
405 
406         return driver_registry
407 
408     @property
409     def disk_cachemode(self):
410         if self._disk_cachemode is None:
411             # We prefer 'none' for consistent performance, host crash
412             # safety & migration correctness by avoiding host page cache.
413             # Some filesystems don't support O_DIRECT though. For those we
414             # fallback to 'writethrough' which gives host crash safety, and
415             # is safe for migration provided the filesystem is cache coherent
416             # (cluster filesystems typically are, but things like NFS are not).
417             self._disk_cachemode = "none"
418             if not nova.privsep.utils.supports_direct_io(CONF.instances_path):
419                 self._disk_cachemode = "writethrough"
420         return self._disk_cachemode
421 
422     def _set_cache_mode(self, conf):
423         """Set cache mode on LibvirtConfigGuestDisk object."""
424         try:
425             source_type = conf.source_type
426             driver_cache = conf.driver_cache
427         except AttributeError:
428             return
429 
430         # Shareable disks like for a multi-attach volume need to have the
431         # driver cache disabled.
432         if getattr(conf, 'shareable', False):
433             conf.driver_cache = 'none'
434         else:
435             cache_mode = self.disk_cachemodes.get(source_type,
436                                                   driver_cache)
437             conf.driver_cache = cache_mode
438 
439     def _do_quality_warnings(self):
440         """Warn about potential configuration issues.
441 
442         This will log a warning message for things such as untested driver or
443         host arch configurations in order to indicate potential issues to
444         administrators.
445         """
446         caps = self._host.get_capabilities()
447         hostarch = caps.host.cpu.arch
448         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
449             hostarch not in (fields.Architecture.I686,
450                              fields.Architecture.X86_64)):
451             LOG.warning('The libvirt driver is not tested on '
452                         '%(type)s/%(arch)s by the OpenStack project and '
453                         'thus its quality can not be ensured. For more '
454                         'information, see: https://docs.openstack.org/'
455                         'nova/latest/user/support-matrix.html',
456                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
457 
458         if CONF.vnc.keymap:
459             LOG.warning('The option "[vnc] keymap" has been deprecated '
460                         'in favor of configuration within the guest. '
461                         'Update nova.conf to address this change and '
462                         'refer to bug #1682020 for more information.')
463 
464         if CONF.spice.keymap:
465             LOG.warning('The option "[spice] keymap" has been deprecated '
466                         'in favor of configuration within the guest. '
467                         'Update nova.conf to address this change and '
468                         'refer to bug #1682020 for more information.')
469 
470     def _handle_conn_event(self, enabled, reason):
471         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
472                  {'enabled': enabled, 'reason': reason})
473         self._set_host_enabled(enabled, reason)
474 
475     def init_host(self, host):
476         self._host.initialize()
477 
478         self._do_quality_warnings()
479 
480         self._parse_migration_flags()
481 
482         self._supported_perf_events = self._get_supported_perf_events()
483 
484         self._set_multiattach_support()
485 
486         self._check_file_backed_memory_support()
487 
488         if (CONF.libvirt.virt_type == 'lxc' and
489                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
490             LOG.warning("Running libvirt-lxc without user namespaces is "
491                         "dangerous. Containers spawned by Nova will be run "
492                         "as the host's root user. It is highly suggested "
493                         "that user namespaces be used in a public or "
494                         "multi-tenant environment.")
495 
496         # Stop libguestfs using KVM unless we're also configured
497         # to use this. This solves problem where people need to
498         # stop Nova use of KVM because nested-virt is broken
499         if CONF.libvirt.virt_type != "kvm":
500             guestfs.force_tcg()
501 
502         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
503             raise exception.InternalError(
504                 _('Nova requires libvirt version %s or greater.') %
505                 libvirt_utils.version_to_string(MIN_LIBVIRT_VERSION))
506 
507         if CONF.libvirt.virt_type in ("qemu", "kvm"):
508             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
509                 # "qemu-img info" calls are version dependent, so we need to
510                 # store the version in the images module.
511                 images.QEMU_VERSION = self._host.get_connection().getVersion()
512             else:
513                 raise exception.InternalError(
514                     _('Nova requires QEMU version %s or greater.') %
515                     libvirt_utils.version_to_string(MIN_QEMU_VERSION))
516 
517         if CONF.libvirt.virt_type == 'parallels':
518             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
519                 raise exception.InternalError(
520                     _('Nova requires Virtuozzo version %s or greater.') %
521                     libvirt_utils.version_to_string(MIN_VIRTUOZZO_VERSION))
522 
523         # Give the cloud admin a heads up if we are intending to
524         # change the MIN_LIBVIRT_VERSION in the next release.
525         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
526             LOG.warning('Running Nova with a libvirt version less than '
527                         '%(version)s is deprecated. The required minimum '
528                         'version of libvirt will be raised to %(version)s '
529                         'in the next release.',
530                         {'version': libvirt_utils.version_to_string(
531                             NEXT_MIN_LIBVIRT_VERSION)})
532         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
533             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
534             LOG.warning('Running Nova with a QEMU version less than '
535                         '%(version)s is deprecated. The required minimum '
536                         'version of QEMU will be raised to %(version)s '
537                         'in the next release.',
538                         {'version': libvirt_utils.version_to_string(
539                             NEXT_MIN_QEMU_VERSION)})
540 
541         kvm_arch = fields.Architecture.from_host()
542         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
543             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
544                 not self._host.has_min_version(
545                     MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))):
546             raise exception.InternalError(
547                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
548                   'requires libvirt version %(libvirt_ver)s or greater') %
549                 {'arch': kvm_arch,
550                  'libvirt_ver': libvirt_utils.version_to_string(
551                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
552 
553         # TODO(sbauza): Remove this code once mediated devices are persisted
554         # across reboots.
555         if self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
556             self._recreate_assigned_mediated_devices()
557 
558     @staticmethod
559     def _is_existing_mdev(uuid):
560         # FIXME(sbauza): Some kernel can have a uevent race meaning that the
561         # libvirt daemon won't know when a mediated device is created unless
562         # you restart that daemon. Until all kernels we support are not having
563         # that possible race, check the sysfs directly instead of asking the
564         # libvirt API.
565         # See https://bugzilla.redhat.com/show_bug.cgi?id=1376907 for ref.
566         return os.path.exists('/sys/bus/mdev/devices/{0}'.format(uuid))
567 
568     def _recreate_assigned_mediated_devices(self):
569         """Recreate assigned mdevs that could have disappeared if we reboot
570         the host.
571         """
572         mdevs = self._get_all_assigned_mediated_devices()
573         requested_types = self._get_supported_vgpu_types()
574         for (mdev_uuid, instance_uuid) in six.iteritems(mdevs):
575             if not self._is_existing_mdev(mdev_uuid):
576                 self._create_new_mediated_device(requested_types, mdev_uuid)
577 
578     def _set_multiattach_support(self):
579         # Check to see if multiattach is supported. Based on bugzilla
580         # https://bugzilla.redhat.com/show_bug.cgi?id=1378242 and related
581         # clones, the shareable flag on a disk device will only work with
582         # qemu<2.10 or libvirt>=3.10. So check those versions here and set
583         # the capability appropriately.
584         if (self._host.has_min_version(lv_ver=MIN_LIBVIRT_MULTIATTACH) or
585                 not self._host.has_min_version(hv_ver=(2, 10, 0))):
586             self.capabilities['supports_multiattach'] = True
587         else:
588             LOG.debug('Volume multiattach is not supported based on current '
589                       'versions of QEMU and libvirt. QEMU must be less than '
590                       '2.10 or libvirt must be greater than or equal to 3.10.')
591 
592     def _check_file_backed_memory_support(self):
593         if CONF.libvirt.file_backed_memory:
594             # file_backed_memory is only compatible with qemu/kvm virts
595             if CONF.libvirt.virt_type not in ("qemu", "kvm"):
596                 raise exception.InternalError(
597                     _('Running Nova with file_backed_memory and virt_type '
598                       '%(type)s is not supported. file_backed_memory is only '
599                       'supported with qemu and kvm types.') %
600                     {'type': CONF.libvirt.virt_type})
601 
602             # Check needed versions for file_backed_memory
603             if not self._host.has_min_version(
604                     MIN_LIBVIRT_FILE_BACKED_VERSION,
605                     MIN_QEMU_FILE_BACKED_VERSION):
606                 raise exception.InternalError(
607                     _('Running Nova with file_backed_memory requires libvirt '
608                       'version %(libvirt)s and qemu version %(qemu)s') %
609                     {'libvirt': libvirt_utils.version_to_string(
610                         MIN_LIBVIRT_FILE_BACKED_VERSION),
611                     'qemu': libvirt_utils.version_to_string(
612                         MIN_QEMU_FILE_BACKED_VERSION)})
613 
614             # file-backed memory doesn't work with memory overcommit.
615             # Block service startup if file-backed memory is enabled and
616             # ram_allocation_ratio is not 1.0
617             if CONF.ram_allocation_ratio != 1.0:
618                 raise exception.InternalError(
619                     'Running Nova with file_backed_memory requires '
620                     'ram_allocation_ratio configured to 1.0')
621 
622     def _prepare_migration_flags(self):
623         migration_flags = 0
624 
625         migration_flags |= libvirt.VIR_MIGRATE_LIVE
626 
627         # Adding p2p flag only if xen is not in use, because xen does not
628         # support p2p migrations
629         if CONF.libvirt.virt_type != 'xen':
630             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
631 
632         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
633         # instance will remain defined on the source host
634         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
635 
636         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
637         # destination host
638         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
639 
640         live_migration_flags = block_migration_flags = migration_flags
641 
642         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
643         # will be live-migrations instead
644         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
645 
646         return (live_migration_flags, block_migration_flags)
647 
648     def _handle_live_migration_tunnelled(self, migration_flags):
649         if (CONF.libvirt.live_migration_tunnelled is None or
650                 CONF.libvirt.live_migration_tunnelled):
651             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
652         return migration_flags
653 
654     def _is_post_copy_available(self):
655         return self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION)
656 
657     def _is_virtlogd_available(self):
658         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
659                                           MIN_QEMU_VIRTLOGD)
660 
661     def _is_native_luks_available(self):
662         return self._host.has_min_version(MIN_LIBVIRT_LUKS_VERSION,
663                                           MIN_QEMU_LUKS_VERSION)
664 
665     def _handle_live_migration_post_copy(self, migration_flags):
666         if CONF.libvirt.live_migration_permit_post_copy:
667             if self._is_post_copy_available():
668                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
669             else:
670                 LOG.info('The live_migration_permit_post_copy is set '
671                          'to True, but it is not supported.')
672         return migration_flags
673 
674     def _handle_live_migration_auto_converge(self, migration_flags):
675         if (self._is_post_copy_available() and
676                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
677             LOG.info('The live_migration_permit_post_copy is set to '
678                      'True and post copy live migration is available '
679                      'so auto-converge will not be in use.')
680         elif CONF.libvirt.live_migration_permit_auto_converge:
681             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
682         return migration_flags
683 
684     def _parse_migration_flags(self):
685         (live_migration_flags,
686             block_migration_flags) = self._prepare_migration_flags()
687 
688         live_migration_flags = self._handle_live_migration_tunnelled(
689             live_migration_flags)
690         block_migration_flags = self._handle_live_migration_tunnelled(
691             block_migration_flags)
692 
693         live_migration_flags = self._handle_live_migration_post_copy(
694             live_migration_flags)
695         block_migration_flags = self._handle_live_migration_post_copy(
696             block_migration_flags)
697 
698         live_migration_flags = self._handle_live_migration_auto_converge(
699             live_migration_flags)
700         block_migration_flags = self._handle_live_migration_auto_converge(
701             block_migration_flags)
702 
703         self._live_migration_flags = live_migration_flags
704         self._block_migration_flags = block_migration_flags
705 
706     # TODO(sahid): This method is targeted for removal when the tests
707     # have been updated to avoid its use
708     #
709     # All libvirt API calls on the libvirt.Connect object should be
710     # encapsulated by methods on the nova.virt.libvirt.host.Host
711     # object, rather than directly invoking the libvirt APIs. The goal
712     # is to avoid a direct dependency on the libvirt API from the
713     # driver.py file.
714     def _get_connection(self):
715         return self._host.get_connection()
716 
717     _conn = property(_get_connection)
718 
719     @staticmethod
720     def _uri():
721         if CONF.libvirt.virt_type == 'uml':
722             uri = CONF.libvirt.connection_uri or 'uml:///system'
723         elif CONF.libvirt.virt_type == 'xen':
724             uri = CONF.libvirt.connection_uri or 'xen:///'
725         elif CONF.libvirt.virt_type == 'lxc':
726             uri = CONF.libvirt.connection_uri or 'lxc:///'
727         elif CONF.libvirt.virt_type == 'parallels':
728             uri = CONF.libvirt.connection_uri or 'parallels:///system'
729         else:
730             uri = CONF.libvirt.connection_uri or 'qemu:///system'
731         return uri
732 
733     @staticmethod
734     def _live_migration_uri(dest):
735         uris = {
736             'kvm': 'qemu+%s://%s/system',
737             'qemu': 'qemu+%s://%s/system',
738             'xen': 'xenmigr://%s/system',
739             'parallels': 'parallels+tcp://%s/system',
740         }
741         virt_type = CONF.libvirt.virt_type
742         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
743         uri = CONF.libvirt.live_migration_uri
744         if uri:
745             return uri % dest
746 
747         uri = uris.get(virt_type)
748         if uri is None:
749             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
750 
751         str_format = (dest,)
752         if virt_type in ('kvm', 'qemu'):
753             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
754             str_format = (scheme, dest)
755         return uris.get(virt_type) % str_format
756 
757     @staticmethod
758     def _migrate_uri(dest):
759         uri = None
760         # Only QEMU live migrations supports migrate-uri parameter
761         virt_type = CONF.libvirt.virt_type
762         if virt_type in ('qemu', 'kvm'):
763             # QEMU accept two schemes: tcp and rdma.  By default
764             # libvirt build the URI using the remote hostname and the
765             # tcp schema.
766             uri = 'tcp://%s' % dest
767         # Because dest might be of type unicode, here we might return value of
768         # type unicode as well which is not acceptable by libvirt python
769         # binding when Python 2.7 is in use, so let's convert it explicitly
770         # back to string. When Python 3.x is in use, libvirt python binding
771         # accepts unicode type so it is completely fine to do a no-op str(uri)
772         # conversion which will return value of type unicode.
773         return uri and str(uri)
774 
775     def instance_exists(self, instance):
776         """Efficient override of base instance_exists method."""
777         try:
778             self._host.get_guest(instance)
779             return True
780         except (exception.InternalError, exception.InstanceNotFound):
781             return False
782 
783     def estimate_instance_overhead(self, instance_info):
784         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
785             instance_info)
786         if isinstance(instance_info, objects.Flavor):
787             # A flavor object is passed during case of migrate
788             emu_policy = hardware.get_emulator_thread_policy_constraint(
789                 instance_info)
790             if emu_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
791                 overhead['vcpus'] += 1
792         else:
793             # An instance object is passed during case of spawing or a
794             # dict is passed when computing resource for an instance
795             numa_topology = hardware.instance_topology_from_instance(
796                 instance_info)
797             if numa_topology and numa_topology.emulator_threads_isolated:
798                 overhead['vcpus'] += 1
799         return overhead
800 
801     def list_instances(self):
802         names = []
803         for guest in self._host.list_guests(only_running=False):
804             names.append(guest.name)
805 
806         return names
807 
808     def list_instance_uuids(self):
809         uuids = []
810         for guest in self._host.list_guests(only_running=False):
811             uuids.append(guest.uuid)
812 
813         return uuids
814 
815     def plug_vifs(self, instance, network_info):
816         """Plug VIFs into networks."""
817         for vif in network_info:
818             self.vif_driver.plug(instance, vif)
819 
820     def _unplug_vifs(self, instance, network_info, ignore_errors):
821         """Unplug VIFs from networks."""
822         for vif in network_info:
823             try:
824                 self.vif_driver.unplug(instance, vif)
825             except exception.NovaException:
826                 if not ignore_errors:
827                     raise
828 
829     def unplug_vifs(self, instance, network_info):
830         self._unplug_vifs(instance, network_info, False)
831 
832     def _teardown_container(self, instance):
833         inst_path = libvirt_utils.get_instance_path(instance)
834         container_dir = os.path.join(inst_path, 'rootfs')
835         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
836         LOG.debug('Attempting to teardown container at path %(dir)s with '
837                   'root device: %(rootfs_dev)s',
838                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
839                   instance=instance)
840         disk_api.teardown_container(container_dir, rootfs_dev)
841 
842     def _destroy(self, instance, attempt=1):
843         try:
844             guest = self._host.get_guest(instance)
845             if CONF.serial_console.enabled:
846                 # This method is called for several events: destroy,
847                 # rebuild, hard-reboot, power-off - For all of these
848                 # events we want to release the serial ports acquired
849                 # for the guest before destroying it.
850                 serials = self._get_serial_ports_from_guest(guest)
851                 for hostname, port in serials:
852                     serial_console.release_port(host=hostname, port=port)
853         except exception.InstanceNotFound:
854             guest = None
855 
856         # If the instance is already terminated, we're still happy
857         # Otherwise, destroy it
858         old_domid = -1
859         if guest is not None:
860             try:
861                 old_domid = guest.id
862                 guest.poweroff()
863 
864             except libvirt.libvirtError as e:
865                 is_okay = False
866                 errcode = e.get_error_code()
867                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
868                     # Domain already gone. This can safely be ignored.
869                     is_okay = True
870                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
871                     # If the instance is already shut off, we get this:
872                     # Code=55 Error=Requested operation is not valid:
873                     # domain is not running
874 
875                     state = guest.get_power_state(self._host)
876                     if state == power_state.SHUTDOWN:
877                         is_okay = True
878                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
879                     errmsg = e.get_error_message()
880                     if (CONF.libvirt.virt_type == 'lxc' and
881                         errmsg == 'internal error: '
882                                   'Some processes refused to die'):
883                         # Some processes in the container didn't die
884                         # fast enough for libvirt. The container will
885                         # eventually die. For now, move on and let
886                         # the wait_for_destroy logic take over.
887                         is_okay = True
888                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
889                     LOG.warning("Cannot destroy instance, operation time out",
890                                 instance=instance)
891                     reason = _("operation time out")
892                     raise exception.InstancePowerOffFailure(reason=reason)
893                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
894                     if e.get_int1() == errno.EBUSY:
895                         # NOTE(danpb): When libvirt kills a process it sends it
896                         # SIGTERM first and waits 10 seconds. If it hasn't gone
897                         # it sends SIGKILL and waits another 5 seconds. If it
898                         # still hasn't gone then you get this EBUSY error.
899                         # Usually when a QEMU process fails to go away upon
900                         # SIGKILL it is because it is stuck in an
901                         # uninterruptible kernel sleep waiting on I/O from
902                         # some non-responsive server.
903                         # Given the CPU load of the gate tests though, it is
904                         # conceivable that the 15 second timeout is too short,
905                         # particularly if the VM running tempest has a high
906                         # steal time from the cloud host. ie 15 wallclock
907                         # seconds may have passed, but the VM might have only
908                         # have a few seconds of scheduled run time.
909                         LOG.warning('Error from libvirt during destroy. '
910                                     'Code=%(errcode)s Error=%(e)s; '
911                                     'attempt %(attempt)d of 3',
912                                     {'errcode': errcode, 'e': e,
913                                      'attempt': attempt},
914                                     instance=instance)
915                         with excutils.save_and_reraise_exception() as ctxt:
916                             # Try up to 3 times before giving up.
917                             if attempt < 3:
918                                 ctxt.reraise = False
919                                 self._destroy(instance, attempt + 1)
920                                 return
921 
922                 if not is_okay:
923                     with excutils.save_and_reraise_exception():
924                         LOG.error('Error from libvirt during destroy. '
925                                   'Code=%(errcode)s Error=%(e)s',
926                                   {'errcode': errcode, 'e': e},
927                                   instance=instance)
928 
929         def _wait_for_destroy(expected_domid):
930             """Called at an interval until the VM is gone."""
931             # NOTE(vish): If the instance disappears during the destroy
932             #             we ignore it so the cleanup can still be
933             #             attempted because we would prefer destroy to
934             #             never fail.
935             try:
936                 dom_info = self.get_info(instance)
937                 state = dom_info.state
938                 new_domid = dom_info.internal_id
939             except exception.InstanceNotFound:
940                 LOG.debug("During wait destroy, instance disappeared.",
941                           instance=instance)
942                 state = power_state.SHUTDOWN
943 
944             if state == power_state.SHUTDOWN:
945                 LOG.info("Instance destroyed successfully.", instance=instance)
946                 raise loopingcall.LoopingCallDone()
947 
948             # NOTE(wangpan): If the instance was booted again after destroy,
949             #                this may be an endless loop, so check the id of
950             #                domain here, if it changed and the instance is
951             #                still running, we should destroy it again.
952             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
953             if new_domid != expected_domid:
954                 LOG.info("Instance may be started again.", instance=instance)
955                 kwargs['is_running'] = True
956                 raise loopingcall.LoopingCallDone()
957 
958         kwargs = {'is_running': False}
959         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
960                                                      old_domid)
961         timer.start(interval=0.5).wait()
962         if kwargs['is_running']:
963             LOG.info("Going to destroy instance again.", instance=instance)
964             self._destroy(instance)
965         else:
966             # NOTE(GuanQiang): teardown container to avoid resource leak
967             if CONF.libvirt.virt_type == 'lxc':
968                 self._teardown_container(instance)
969 
970     def destroy(self, context, instance, network_info, block_device_info=None,
971                 destroy_disks=True):
972         self._destroy(instance)
973         self.cleanup(context, instance, network_info, block_device_info,
974                      destroy_disks)
975 
976     def _undefine_domain(self, instance):
977         try:
978             guest = self._host.get_guest(instance)
979             try:
980                 support_uefi = self._has_uefi_support()
981                 guest.delete_configuration(support_uefi)
982             except libvirt.libvirtError as e:
983                 with excutils.save_and_reraise_exception() as ctxt:
984                     errcode = e.get_error_code()
985                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
986                         LOG.debug("Called undefine, but domain already gone.",
987                                   instance=instance)
988                         ctxt.reraise = False
989                     else:
990                         LOG.error('Error from libvirt during undefine. '
991                                   'Code=%(errcode)s Error=%(e)s',
992                                   {'errcode': errcode,
993                                    'e': encodeutils.exception_to_unicode(e)},
994                                   instance=instance)
995         except exception.InstanceNotFound:
996             pass
997 
998     def cleanup(self, context, instance, network_info, block_device_info=None,
999                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
1000         if destroy_vifs:
1001             self._unplug_vifs(instance, network_info, True)
1002 
1003         # Continue attempting to remove firewall filters for the instance
1004         # until it's done or there is a failure to remove the filters. If
1005         # unfilter fails because the instance is not yet shutdown, try to
1006         # destroy the guest again and then retry the unfilter.
1007         while True:
1008             try:
1009                 self.unfilter_instance(instance, network_info)
1010                 break
1011             except libvirt.libvirtError as e:
1012                 try:
1013                     state = self.get_info(instance).state
1014                 except exception.InstanceNotFound:
1015                     state = power_state.SHUTDOWN
1016 
1017                 if state != power_state.SHUTDOWN:
1018                     LOG.warning("Instance may be still running, destroy "
1019                                 "it again.", instance=instance)
1020                     self._destroy(instance)
1021                 else:
1022                     errcode = e.get_error_code()
1023                     LOG.exception(_('Error from libvirt during unfilter. '
1024                                     'Code=%(errcode)s Error=%(e)s'),
1025                                   {'errcode': errcode, 'e': e},
1026                                   instance=instance)
1027                     reason = _("Error unfiltering instance.")
1028                     raise exception.InstanceTerminationFailure(reason=reason)
1029             except Exception:
1030                 raise
1031 
1032         # FIXME(wangpan): if the instance is booted again here, such as the
1033         #                 soft reboot operation boot it here, it will become
1034         #                 "running deleted", should we check and destroy it
1035         #                 at the end of this method?
1036 
1037         # NOTE(vish): we disconnect from volumes regardless
1038         block_device_mapping = driver.block_device_info_get_mapping(
1039             block_device_info)
1040         for vol in block_device_mapping:
1041             connection_info = vol['connection_info']
1042             disk_dev = vol['mount_device']
1043             if disk_dev is not None:
1044                 disk_dev = disk_dev.rpartition("/")[2]
1045             try:
1046                 self._disconnect_volume(context, connection_info, instance)
1047             except Exception as exc:
1048                 with excutils.save_and_reraise_exception() as ctxt:
1049                     if destroy_disks:
1050                         # Don't block on Volume errors if we're trying to
1051                         # delete the instance as we may be partially created
1052                         # or deleted
1053                         ctxt.reraise = False
1054                         LOG.warning(
1055                             "Ignoring Volume Error on vol %(vol_id)s "
1056                             "during delete %(exc)s",
1057                             {'vol_id': vol.get('volume_id'),
1058                              'exc': encodeutils.exception_to_unicode(exc)},
1059                             instance=instance)
1060 
1061         if destroy_disks:
1062             # NOTE(haomai): destroy volumes if needed
1063             if CONF.libvirt.images_type == 'lvm':
1064                 self._cleanup_lvm(instance, block_device_info)
1065             if CONF.libvirt.images_type == 'rbd':
1066                 self._cleanup_rbd(instance)
1067 
1068         is_shared_block_storage = False
1069         if migrate_data and 'is_shared_block_storage' in migrate_data:
1070             is_shared_block_storage = migrate_data.is_shared_block_storage
1071         if destroy_disks or is_shared_block_storage:
1072             attempts = int(instance.system_metadata.get('clean_attempts',
1073                                                         '0'))
1074             success = self.delete_instance_files(instance)
1075             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1076             # task in the compute manager. The tight coupling is not great...
1077             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1078             if success:
1079                 instance.cleaned = True
1080             instance.save()
1081 
1082         self._undefine_domain(instance)
1083 
1084     def _detach_encrypted_volumes(self, instance, block_device_info):
1085         """Detaches encrypted volumes attached to instance."""
1086         disks = self._get_instance_disk_info(instance, block_device_info)
1087         encrypted_volumes = filter(dmcrypt.is_encrypted,
1088                                    [disk['path'] for disk in disks])
1089         for path in encrypted_volumes:
1090             dmcrypt.delete_volume(path)
1091 
1092     def _get_serial_ports_from_guest(self, guest, mode=None):
1093         """Returns an iterator over serial port(s) configured on guest.
1094 
1095         :param mode: Should be a value in (None, bind, connect)
1096         """
1097         xml = guest.get_xml_desc()
1098         tree = etree.fromstring(xml)
1099 
1100         # The 'serial' device is the base for x86 platforms. Other platforms
1101         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1102         xpath_mode = "[@mode='%s']" % mode if mode else ""
1103         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1104         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1105 
1106         tcp_devices = tree.findall(serial_tcp)
1107         if len(tcp_devices) == 0:
1108             tcp_devices = tree.findall(console_tcp)
1109         for source in tcp_devices:
1110             yield (source.get("host"), int(source.get("service")))
1111 
1112     def _get_scsi_controller_max_unit(self, guest):
1113         """Returns the max disk unit used by scsi controller"""
1114         xml = guest.get_xml_desc()
1115         tree = etree.fromstring(xml)
1116         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1117 
1118         ret = []
1119         for obj in tree.findall(addrs):
1120             ret.append(int(obj.get('unit', 0)))
1121         return max(ret)
1122 
1123     @staticmethod
1124     def _get_rbd_driver():
1125         return rbd_utils.RBDDriver(
1126                 pool=CONF.libvirt.images_rbd_pool,
1127                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1128                 rbd_user=CONF.libvirt.rbd_user)
1129 
1130     def _cleanup_rbd(self, instance):
1131         # NOTE(nic): On revert_resize, the cleanup steps for the root
1132         # volume are handled with an "rbd snap rollback" command,
1133         # and none of this is needed (and is, in fact, harmful) so
1134         # filter out non-ephemerals from the list
1135         if instance.task_state == task_states.RESIZE_REVERTING:
1136             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1137                                       disk.endswith('disk.local'))
1138         else:
1139             filter_fn = lambda disk: disk.startswith(instance.uuid)
1140         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1141 
1142     def _cleanup_lvm(self, instance, block_device_info):
1143         """Delete all LVM disks for given instance object."""
1144         if instance.get('ephemeral_key_uuid') is not None:
1145             self._detach_encrypted_volumes(instance, block_device_info)
1146 
1147         disks = self._lvm_disks(instance)
1148         if disks:
1149             lvm.remove_volumes(disks)
1150 
1151     def _lvm_disks(self, instance):
1152         """Returns all LVM disks for given instance object."""
1153         if CONF.libvirt.images_volume_group:
1154             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1155             if not os.path.exists(vg):
1156                 return []
1157             pattern = '%s_' % instance.uuid
1158 
1159             def belongs_to_instance(disk):
1160                 return disk.startswith(pattern)
1161 
1162             def fullpath(name):
1163                 return os.path.join(vg, name)
1164 
1165             logical_volumes = lvm.list_volumes(vg)
1166 
1167             disks = [fullpath(disk) for disk in logical_volumes
1168                      if belongs_to_instance(disk)]
1169             return disks
1170         return []
1171 
1172     def get_volume_connector(self, instance):
1173         root_helper = utils.get_root_helper()
1174         return connector.get_connector_properties(
1175             root_helper, CONF.my_block_storage_ip,
1176             CONF.libvirt.volume_use_multipath,
1177             enforce_multipath=True,
1178             host=CONF.host)
1179 
1180     def _cleanup_resize(self, context, instance, network_info):
1181         inst_base = libvirt_utils.get_instance_path(instance)
1182         target = inst_base + '_resize'
1183 
1184         # Deletion can fail over NFS, so retry the deletion as required.
1185         # Set maximum attempt as 5, most test can remove the directory
1186         # for the second time.
1187         attempts = 0
1188         while(os.path.exists(target) and attempts < 5):
1189             shutil.rmtree(target, ignore_errors=True)
1190             if os.path.exists(target):
1191                 time.sleep(random.randint(20, 200) / 100.0)
1192             attempts += 1
1193 
1194         # NOTE(mriedem): Some image backends will recreate the instance path
1195         # and disk.info during init, and all we need the root disk for
1196         # here is removing cloned snapshots which is backend-specific, so
1197         # check that first before initializing the image backend object. If
1198         # there is ever an image type that supports clone *and* re-creates
1199         # the instance directory and disk.info on init, this condition will
1200         # need to be re-visited to make sure that backend doesn't re-create
1201         # the disk. Refer to bugs: 1666831 1728603 1769131
1202         if self.image_backend.backend(CONF.libvirt.images_type).SUPPORTS_CLONE:
1203             root_disk = self.image_backend.by_name(instance, 'disk')
1204             if root_disk.exists():
1205                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
1206 
1207         if instance.host != CONF.host:
1208             self._undefine_domain(instance)
1209             self.unplug_vifs(instance, network_info)
1210             self.unfilter_instance(instance, network_info)
1211 
1212     def _get_volume_driver(self, connection_info):
1213         driver_type = connection_info.get('driver_volume_type')
1214         if driver_type not in self.volume_drivers:
1215             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1216         return self.volume_drivers[driver_type]
1217 
1218     def _connect_volume(self, context, connection_info, instance,
1219                         encryption=None, allow_native_luks=True):
1220         vol_driver = self._get_volume_driver(connection_info)
1221         vol_driver.connect_volume(connection_info, instance)
1222         self._attach_encryptor(context, connection_info, encryption,
1223                                allow_native_luks)
1224 
1225     def _should_disconnect_target(self, context, connection_info, instance):
1226         connection_count = 0
1227 
1228         # NOTE(jdg): Multiattach is a special case (not to be confused
1229         # with shared_targets). With multiattach we may have a single volume
1230         # attached multiple times to *this* compute node (ie Server-1 and
1231         # Server-2).  So, if we receive a call to delete the attachment for
1232         # Server-1 we need to take special care to make sure that the Volume
1233         # isn't also attached to another Server on this Node.  Otherwise we
1234         # will indiscriminantly delete the connection for all Server and that's
1235         # no good.  So check if it's attached multiple times on this node
1236         # if it is we skip the call to brick to delete the connection.
1237         if connection_info.get('multiattach', False):
1238             volume = self._volume_api.get(
1239                 context,
1240                 driver_block_device.get_volume_id(connection_info))
1241             attachments = volume.get('attachments', {})
1242             if len(attachments) > 1:
1243                 # First we get a list of all Server UUID's associated with
1244                 # this Host (Compute Node).  We're going to use this to
1245                 # determine if the Volume being detached is also in-use by
1246                 # another Server on this Host, ie just check to see if more
1247                 # than one attachment.server_id for this volume is in our
1248                 # list of Server UUID's for this Host
1249                 servers_this_host = objects.InstanceList.get_uuids_by_host(
1250                     context, instance.host)
1251 
1252                 # NOTE(jdg): nova.volume.cinder translates the
1253                 # volume['attachments'] response into a dict which includes
1254                 # the Server UUID as the key, so we're using that
1255                 # here to check against our server_this_host list
1256                 for server_id, data in attachments.items():
1257                     if server_id in servers_this_host:
1258                         connection_count += 1
1259         return (False if connection_count > 1 else True)
1260 
1261     def _disconnect_volume(self, context, connection_info, instance,
1262                            encryption=None):
1263         self._detach_encryptor(context, connection_info, encryption=encryption)
1264         if self._should_disconnect_target(context, connection_info, instance):
1265             vol_driver = self._get_volume_driver(connection_info)
1266             vol_driver.disconnect_volume(connection_info, instance)
1267         else:
1268             LOG.info("Detected multiple connections on this host for volume: "
1269                      "%s, skipping target disconnect.",
1270                      driver_block_device.get_volume_id(connection_info),
1271                      instance=instance)
1272 
1273     def _extend_volume(self, connection_info, instance):
1274         vol_driver = self._get_volume_driver(connection_info)
1275         return vol_driver.extend_volume(connection_info, instance)
1276 
1277     def _use_native_luks(self, encryption=None):
1278         """Is LUKS the required provider and native QEMU LUKS available
1279         """
1280         provider = None
1281         if encryption:
1282             provider = encryption.get('provider', None)
1283         if provider in encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP:
1284             provider = encryptors.LEGACY_PROVIDER_CLASS_TO_FORMAT_MAP[provider]
1285         return provider == encryptors.LUKS and self._is_native_luks_available()
1286 
1287     def _get_volume_config(self, connection_info, disk_info):
1288         vol_driver = self._get_volume_driver(connection_info)
1289         conf = vol_driver.get_config(connection_info, disk_info)
1290         self._set_cache_mode(conf)
1291         return conf
1292 
1293     def _get_volume_encryptor(self, connection_info, encryption):
1294         root_helper = utils.get_root_helper()
1295         return encryptors.get_volume_encryptor(root_helper=root_helper,
1296                                                keymgr=key_manager.API(CONF),
1297                                                connection_info=connection_info,
1298                                                **encryption)
1299 
1300     def _get_volume_encryption(self, context, connection_info):
1301         """Get the encryption metadata dict if it is not provided
1302         """
1303         encryption = {}
1304         volume_id = driver_block_device.get_volume_id(connection_info)
1305         if volume_id:
1306             encryption = encryptors.get_encryption_metadata(context,
1307                             self._volume_api, volume_id, connection_info)
1308         return encryption
1309 
1310     def _attach_encryptor(self, context, connection_info, encryption,
1311                           allow_native_luks):
1312         """Attach the frontend encryptor if one is required by the volume.
1313 
1314         The request context is only used when an encryption metadata dict is
1315         not provided. The encryption metadata dict being populated is then used
1316         to determine if an attempt to attach the encryptor should be made.
1317 
1318         If native LUKS decryption is enabled then create a Libvirt volume
1319         secret containing the LUKS passphrase for the volume.
1320         """
1321         if encryption is None:
1322             encryption = self._get_volume_encryption(context, connection_info)
1323 
1324         if (encryption and allow_native_luks and
1325             self._use_native_luks(encryption)):
1326             # NOTE(lyarwood): Fetch the associated key for the volume and
1327             # decode the passphrase from the key.
1328             # FIXME(lyarwood): c-vol currently creates symmetric keys for use
1329             # with volumes, leading to the binary to hex to string conversion
1330             # below.
1331             keymgr = key_manager.API(CONF)
1332             key = keymgr.get(context, encryption['encryption_key_id'])
1333             key_encoded = key.get_encoded()
1334             passphrase = binascii.hexlify(key_encoded).decode('utf-8')
1335 
1336             # NOTE(lyarwood): Retain the behaviour of the original os-brick
1337             # encryptors and format any volume that does not identify as
1338             # encrypted with LUKS.
1339             # FIXME(lyarwood): Remove this once c-vol correctly formats
1340             # encrypted volumes during their initial creation:
1341             # https://bugs.launchpad.net/cinder/+bug/1739442
1342             device_path = connection_info.get('data').get('device_path')
1343             if device_path:
1344                 root_helper = utils.get_root_helper()
1345                 if not luks_encryptor.is_luks(root_helper, device_path):
1346                     encryptor = self._get_volume_encryptor(connection_info,
1347                                                            encryption)
1348                     encryptor._format_volume(passphrase, **encryption)
1349 
1350             # NOTE(lyarwood): Store the passphrase as a libvirt secret locally
1351             # on the compute node. This secret is used later when generating
1352             # the volume config.
1353             volume_id = driver_block_device.get_volume_id(connection_info)
1354             self._host.create_secret('volume', volume_id, password=passphrase)
1355         elif encryption:
1356             encryptor = self._get_volume_encryptor(connection_info,
1357                                                    encryption)
1358             encryptor.attach_volume(context, **encryption)
1359 
1360     def _detach_encryptor(self, context, connection_info, encryption):
1361         """Detach the frontend encryptor if one is required by the volume.
1362 
1363         The request context is only used when an encryption metadata dict is
1364         not provided. The encryption metadata dict being populated is then used
1365         to determine if an attempt to detach the encryptor should be made.
1366 
1367         If native LUKS decryption is enabled then delete previously created
1368         Libvirt volume secret from the host.
1369         """
1370         volume_id = driver_block_device.get_volume_id(connection_info)
1371         if volume_id and self._host.find_secret('volume', volume_id):
1372             return self._host.delete_secret('volume', volume_id)
1373         if encryption is None:
1374             encryption = self._get_volume_encryption(context, connection_info)
1375         if encryption:
1376             encryptor = self._get_volume_encryptor(connection_info,
1377                                                    encryption)
1378             encryptor.detach_volume(**encryption)
1379 
1380     def _check_discard_for_attach_volume(self, conf, instance):
1381         """Perform some checks for volumes configured for discard support.
1382 
1383         If discard is configured for the volume, and the guest is using a
1384         configuration known to not work, we will log a message explaining
1385         the reason why.
1386         """
1387         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1388             LOG.debug('Attempting to attach volume %(id)s with discard '
1389                       'support enabled to an instance using an '
1390                       'unsupported configuration. target_bus = '
1391                       '%(bus)s. Trim commands will not be issued to '
1392                       'the storage device.',
1393                       {'bus': conf.target_bus,
1394                        'id': conf.serial},
1395                       instance=instance)
1396 
1397     def attach_volume(self, context, connection_info, instance, mountpoint,
1398                       disk_bus=None, device_type=None, encryption=None):
1399         guest = self._host.get_guest(instance)
1400 
1401         disk_dev = mountpoint.rpartition("/")[2]
1402         bdm = {
1403             'device_name': disk_dev,
1404             'disk_bus': disk_bus,
1405             'device_type': device_type}
1406 
1407         # Note(cfb): If the volume has a custom block size, check that
1408         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1409         #            presence of a block size is considered mandatory by
1410         #            cinder so we fail if we can't honor the request.
1411         data = {}
1412         if ('data' in connection_info):
1413             data = connection_info['data']
1414         if ('logical_block_size' in data or 'physical_block_size' in data):
1415             if ((CONF.libvirt.virt_type != "kvm" and
1416                  CONF.libvirt.virt_type != "qemu")):
1417                 msg = _("Volume sets block size, but the current "
1418                         "libvirt hypervisor '%s' does not support custom "
1419                         "block size") % CONF.libvirt.virt_type
1420                 raise exception.InvalidHypervisorType(msg)
1421 
1422         self._connect_volume(context, connection_info, instance,
1423                              encryption=encryption)
1424         disk_info = blockinfo.get_info_from_bdm(
1425             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1426         if disk_info['bus'] == 'scsi':
1427             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1428 
1429         conf = self._get_volume_config(connection_info, disk_info)
1430 
1431         self._check_discard_for_attach_volume(conf, instance)
1432 
1433         try:
1434             state = guest.get_power_state(self._host)
1435             live = state in (power_state.RUNNING, power_state.PAUSED)
1436 
1437             guest.attach_device(conf, persistent=True, live=live)
1438             # NOTE(artom) If we're attaching with a device role tag, we need to
1439             # rebuild device_metadata. If we're attaching without a role
1440             # tag, we're rebuilding it here needlessly anyways. This isn't a
1441             # massive deal, and it helps reduce code complexity by not having
1442             # to indicate to the virt driver that the attach is tagged. The
1443             # really important optimization of not calling the database unless
1444             # device_metadata has actually changed is done for us by
1445             # instance.save().
1446             instance.device_metadata = self._build_device_metadata(
1447                 context, instance)
1448             instance.save()
1449 
1450         # TODO(lyarwood) Remove the following breadcrumb once all supported
1451         # distributions provide Libvirt 3.3.0 or earlier with
1452         # https://libvirt.org/git/?p=libvirt.git;a=commit;h=7189099 applied.
1453         except libvirt.libvirtError as ex:
1454             if 'Incorrect number of padding bytes' in six.text_type(ex):
1455                 LOG.warning(_('Failed to attach encrypted volume due to a '
1456                     'known Libvirt issue, see the following bug for details: '
1457                     'https://bugzilla.redhat.com/show_bug.cgi?id=1447297'))
1458                 raise
1459         except Exception:
1460             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1461                           mountpoint, instance=instance)
1462             with excutils.save_and_reraise_exception():
1463                 self._disconnect_volume(context, connection_info, instance,
1464                                         encryption=encryption)
1465 
1466     def _swap_volume(self, guest, disk_path, conf, resize_to):
1467         """Swap existing disk with a new block device."""
1468         dev = guest.get_block_device(disk_path)
1469 
1470         # Save a copy of the domain's persistent XML file. We'll use this
1471         # to redefine the domain if anything fails during the volume swap.
1472         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1473 
1474         # Abort is an idempotent operation, so make sure any block
1475         # jobs which may have failed are ended.
1476         try:
1477             dev.abort_job()
1478         except Exception:
1479             pass
1480 
1481         try:
1482             # NOTE (rmk): blockRebase cannot be executed on persistent
1483             #             domains, so we need to temporarily undefine it.
1484             #             If any part of this block fails, the domain is
1485             #             re-defined regardless.
1486             if guest.has_persistent_configuration():
1487                 support_uefi = self._has_uefi_support()
1488                 guest.delete_configuration(support_uefi)
1489 
1490             try:
1491                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1492                 # allow writing to existing external volume file. Use
1493                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1494                 # make sure XML is generated correctly (bug 1691195)
1495                 copy_dev = conf.source_type == 'block'
1496                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1497                            copy_dev=copy_dev)
1498                 while not dev.is_job_complete():
1499                     time.sleep(0.5)
1500 
1501                 dev.abort_job(pivot=True)
1502 
1503             except Exception as exc:
1504                 LOG.exception("Failure rebasing volume %(new_path)s on "
1505                     "%(old_path)s.", {'new_path': conf.source_path,
1506                                       'old_path': disk_path})
1507                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1508 
1509             if resize_to:
1510                 dev.resize(resize_to * units.Gi / units.Ki)
1511 
1512             # Make sure we will redefine the domain using the updated
1513             # configuration after the volume was swapped. The dump_inactive
1514             # keyword arg controls whether we pull the inactive (persistent)
1515             # or active (live) config from the domain. We want to pull the
1516             # live config after the volume was updated to use when we redefine
1517             # the domain.
1518             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1519         finally:
1520             self._host.write_instance_config(xml)
1521 
1522     def swap_volume(self, context, old_connection_info,
1523                     new_connection_info, instance, mountpoint, resize_to):
1524 
1525         # NOTE(lyarwood): https://bugzilla.redhat.com/show_bug.cgi?id=760547
1526         old_encrypt = self._get_volume_encryption(context, old_connection_info)
1527         new_encrypt = self._get_volume_encryption(context, new_connection_info)
1528         if ((old_encrypt and self._use_native_luks(old_encrypt)) or
1529             (new_encrypt and self._use_native_luks(new_encrypt))):
1530             raise NotImplementedError(_("Swap volume is not supported for "
1531                 "encrypted volumes when native LUKS decryption is enabled."))
1532 
1533         guest = self._host.get_guest(instance)
1534 
1535         disk_dev = mountpoint.rpartition("/")[2]
1536         if not guest.get_disk(disk_dev):
1537             raise exception.DiskNotFound(location=disk_dev)
1538         disk_info = {
1539             'dev': disk_dev,
1540             'bus': blockinfo.get_disk_bus_for_disk_dev(
1541                 CONF.libvirt.virt_type, disk_dev),
1542             'type': 'disk',
1543             }
1544         # NOTE (lyarwood): new_connection_info will be modified by the
1545         # following _connect_volume call down into the volume drivers. The
1546         # majority of the volume drivers will add a device_path that is in turn
1547         # used by _get_volume_config to set the source_path of the
1548         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1549         # this to the BDM here as the upper compute swap_volume method will
1550         # eventually do this for us.
1551         self._connect_volume(context, new_connection_info, instance)
1552         conf = self._get_volume_config(new_connection_info, disk_info)
1553         if not conf.source_path:
1554             self._disconnect_volume(context, new_connection_info, instance)
1555             raise NotImplementedError(_("Swap only supports host devices"))
1556 
1557         try:
1558             self._swap_volume(guest, disk_dev, conf, resize_to)
1559         except exception.VolumeRebaseFailed:
1560             with excutils.save_and_reraise_exception():
1561                 self._disconnect_volume(context, new_connection_info, instance)
1562 
1563         self._disconnect_volume(context, old_connection_info, instance)
1564 
1565     def _get_existing_domain_xml(self, instance, network_info,
1566                                  block_device_info=None):
1567         try:
1568             guest = self._host.get_guest(instance)
1569             xml = guest.get_xml_desc()
1570         except exception.InstanceNotFound:
1571             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1572                                                 instance,
1573                                                 instance.image_meta,
1574                                                 block_device_info)
1575             xml = self._get_guest_xml(nova_context.get_admin_context(),
1576                                       instance, network_info, disk_info,
1577                                       instance.image_meta,
1578                                       block_device_info=block_device_info)
1579         return xml
1580 
1581     def detach_volume(self, context, connection_info, instance, mountpoint,
1582                       encryption=None):
1583         disk_dev = mountpoint.rpartition("/")[2]
1584         try:
1585             guest = self._host.get_guest(instance)
1586 
1587             state = guest.get_power_state(self._host)
1588             live = state in (power_state.RUNNING, power_state.PAUSED)
1589             # NOTE(lyarwood): The volume must be detached from the VM before
1590             # detaching any attached encryptors or disconnecting the underlying
1591             # volume in _disconnect_volume. Otherwise, the encryptor or volume
1592             # driver may report that the volume is still in use.
1593             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1594                                                              disk_dev,
1595                                                              live=live)
1596             wait_for_detach()
1597 
1598         except exception.InstanceNotFound:
1599             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1600             #                will throw InstanceNotFound exception. Need to
1601             #                disconnect volume under this circumstance.
1602             LOG.warning("During detach_volume, instance disappeared.",
1603                         instance=instance)
1604         except exception.DeviceNotFound:
1605             # We should still try to disconnect logical device from
1606             # host, an error might have happened during a previous
1607             # call.
1608             LOG.info("Device %s not found in instance.",
1609                      disk_dev, instance=instance)
1610         except libvirt.libvirtError as ex:
1611             # NOTE(vish): This is called to cleanup volumes after live
1612             #             migration, so we should still disconnect even if
1613             #             the instance doesn't exist here anymore.
1614             error_code = ex.get_error_code()
1615             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1616                 # NOTE(vish):
1617                 LOG.warning("During detach_volume, instance disappeared.",
1618                             instance=instance)
1619             else:
1620                 raise
1621 
1622         self._disconnect_volume(context, connection_info, instance,
1623                                 encryption=encryption)
1624 
1625     def extend_volume(self, connection_info, instance):
1626         try:
1627             new_size = self._extend_volume(connection_info, instance)
1628         except NotImplementedError:
1629             raise exception.ExtendVolumeNotSupported()
1630 
1631         # Resize the device in QEMU so its size is updated and
1632         # detected by the instance without rebooting.
1633         try:
1634             guest = self._host.get_guest(instance)
1635             state = guest.get_power_state(self._host)
1636             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1637             if active_state:
1638                 disk_path = connection_info['data']['device_path']
1639                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1640                           {'dev': disk_path, 'size': new_size})
1641                 dev = guest.get_block_device(disk_path)
1642                 dev.resize(new_size // units.Ki)
1643             else:
1644                 LOG.debug('Skipping block device resize, guest is not running',
1645                           instance=instance)
1646         except exception.InstanceNotFound:
1647             with excutils.save_and_reraise_exception():
1648                 LOG.warning('During extend_volume, instance disappeared.',
1649                             instance=instance)
1650         except libvirt.libvirtError:
1651             with excutils.save_and_reraise_exception():
1652                 LOG.exception('resizing block device failed.',
1653                               instance=instance)
1654 
1655     def attach_interface(self, context, instance, image_meta, vif):
1656         guest = self._host.get_guest(instance)
1657 
1658         self.vif_driver.plug(instance, vif)
1659         self.firewall_driver.setup_basic_filtering(instance, [vif])
1660         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1661                                          instance.flavor,
1662                                          CONF.libvirt.virt_type,
1663                                          self._host)
1664         try:
1665             state = guest.get_power_state(self._host)
1666             live = state in (power_state.RUNNING, power_state.PAUSED)
1667             guest.attach_device(cfg, persistent=True, live=live)
1668         except libvirt.libvirtError:
1669             LOG.error('attaching network adapter failed.',
1670                       instance=instance, exc_info=True)
1671             self.vif_driver.unplug(instance, vif)
1672             raise exception.InterfaceAttachFailed(
1673                     instance_uuid=instance.uuid)
1674         try:
1675             # NOTE(artom) If we're attaching with a device role tag, we need to
1676             # rebuild device_metadata. If we're attaching without a role
1677             # tag, we're rebuilding it here needlessly anyways. This isn't a
1678             # massive deal, and it helps reduce code complexity by not having
1679             # to indicate to the virt driver that the attach is tagged. The
1680             # really important optimization of not calling the database unless
1681             # device_metadata has actually changed is done for us by
1682             # instance.save().
1683             instance.device_metadata = self._build_device_metadata(
1684                 context, instance)
1685             instance.save()
1686         except Exception:
1687             # NOTE(artom) If we fail here it means the interface attached
1688             # successfully but building and/or saving the device metadata
1689             # failed. Just unplugging the vif is therefore not enough cleanup,
1690             # we need to detach the interface.
1691             with excutils.save_and_reraise_exception(reraise=False):
1692                 LOG.error('Interface attached successfully but building '
1693                           'and/or saving device metadata failed.',
1694                           instance=instance, exc_info=True)
1695                 self.detach_interface(context, instance, vif)
1696                 raise exception.InterfaceAttachFailed(
1697                     instance_uuid=instance.uuid)
1698 
1699     def detach_interface(self, context, instance, vif):
1700         guest = self._host.get_guest(instance)
1701         cfg = self.vif_driver.get_config(instance, vif,
1702                                          instance.image_meta,
1703                                          instance.flavor,
1704                                          CONF.libvirt.virt_type, self._host)
1705         interface = guest.get_interface_by_cfg(cfg)
1706         try:
1707             self.vif_driver.unplug(instance, vif)
1708             # NOTE(mriedem): When deleting an instance and using Neutron,
1709             # we can be racing against Neutron deleting the port and
1710             # sending the vif-deleted event which then triggers a call to
1711             # detach the interface, so if the interface is not found then
1712             # we can just log it as a warning.
1713             if not interface:
1714                 mac = vif.get('address')
1715                 # The interface is gone so just log it as a warning.
1716                 LOG.warning('Detaching interface %(mac)s failed because '
1717                             'the device is no longer found on the guest.',
1718                             {'mac': mac}, instance=instance)
1719                 return
1720 
1721             state = guest.get_power_state(self._host)
1722             live = state in (power_state.RUNNING, power_state.PAUSED)
1723             # Now we are going to loop until the interface is detached or we
1724             # timeout.
1725             wait_for_detach = guest.detach_device_with_retry(
1726                 guest.get_interface_by_cfg, cfg, live=live,
1727                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1728             wait_for_detach()
1729         except exception.DeviceDetachFailed:
1730             # We failed to detach the device even with the retry loop, so let's
1731             # dump some debug information to the logs before raising back up.
1732             with excutils.save_and_reraise_exception():
1733                 devname = self.vif_driver.get_vif_devname(vif)
1734                 interface = guest.get_interface_by_cfg(cfg)
1735                 if interface:
1736                     LOG.warning(
1737                         'Failed to detach interface %(devname)s after '
1738                         'repeated attempts. Final interface xml:\n'
1739                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1740                         {'devname': devname,
1741                          'interface_xml': interface.to_xml(),
1742                          'guest_xml': guest.get_xml_desc()},
1743                         instance=instance)
1744         except exception.DeviceNotFound:
1745             # The interface is gone so just log it as a warning.
1746             LOG.warning('Detaching interface %(mac)s failed because '
1747                         'the device is no longer found on the guest.',
1748                         {'mac': vif.get('address')}, instance=instance)
1749         except libvirt.libvirtError as ex:
1750             error_code = ex.get_error_code()
1751             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1752                 LOG.warning("During detach_interface, instance disappeared.",
1753                             instance=instance)
1754             else:
1755                 # NOTE(mriedem): When deleting an instance and using Neutron,
1756                 # we can be racing against Neutron deleting the port and
1757                 # sending the vif-deleted event which then triggers a call to
1758                 # detach the interface, so we might have failed because the
1759                 # network device no longer exists. Libvirt will fail with
1760                 # "operation failed: no matching network device was found"
1761                 # which unfortunately does not have a unique error code so we
1762                 # need to look up the interface by config and if it's not found
1763                 # then we can just log it as a warning rather than tracing an
1764                 # error.
1765                 mac = vif.get('address')
1766                 interface = guest.get_interface_by_cfg(cfg)
1767                 if interface:
1768                     LOG.error('detaching network adapter failed.',
1769                               instance=instance, exc_info=True)
1770                     raise exception.InterfaceDetachFailed(
1771                             instance_uuid=instance.uuid)
1772 
1773                 # The interface is gone so just log it as a warning.
1774                 LOG.warning('Detaching interface %(mac)s failed because '
1775                             'the device is no longer found on the guest.',
1776                             {'mac': mac}, instance=instance)
1777 
1778     def _create_snapshot_metadata(self, image_meta, instance,
1779                                   img_fmt, snp_name):
1780         metadata = {'is_public': False,
1781                     'status': 'active',
1782                     'name': snp_name,
1783                     'properties': {
1784                                    'kernel_id': instance.kernel_id,
1785                                    'image_location': 'snapshot',
1786                                    'image_state': 'available',
1787                                    'owner_id': instance.project_id,
1788                                    'ramdisk_id': instance.ramdisk_id,
1789                                    }
1790                     }
1791         if instance.os_type:
1792             metadata['properties']['os_type'] = instance.os_type
1793 
1794         # NOTE(vish): glance forces ami disk format to be ami
1795         if image_meta.disk_format == 'ami':
1796             metadata['disk_format'] = 'ami'
1797         else:
1798             metadata['disk_format'] = img_fmt
1799 
1800         if image_meta.obj_attr_is_set("container_format"):
1801             metadata['container_format'] = image_meta.container_format
1802         else:
1803             metadata['container_format'] = "bare"
1804 
1805         return metadata
1806 
1807     def snapshot(self, context, instance, image_id, update_task_state):
1808         """Create snapshot from a running VM instance.
1809 
1810         This command only works with qemu 0.14+
1811         """
1812         try:
1813             guest = self._host.get_guest(instance)
1814 
1815             # TODO(sahid): We are converting all calls from a
1816             # virDomain object to use nova.virt.libvirt.Guest.
1817             # We should be able to remove virt_dom at the end.
1818             virt_dom = guest._domain
1819         except exception.InstanceNotFound:
1820             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1821 
1822         snapshot = self._image_api.get(context, image_id)
1823 
1824         # source_format is an on-disk format
1825         # source_type is a backend type
1826         disk_path, source_format = libvirt_utils.find_disk(guest)
1827         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1828 
1829         # We won't have source_type for raw or qcow2 disks, because we can't
1830         # determine that from the path. We should have it from the libvirt
1831         # xml, though.
1832         if source_type is None:
1833             source_type = source_format
1834         # For lxc instances we won't have it either from libvirt xml
1835         # (because we just gave libvirt the mounted filesystem), or the path,
1836         # so source_type is still going to be None. In this case,
1837         # root_disk is going to default to CONF.libvirt.images_type
1838         # below, which is still safe.
1839 
1840         image_format = CONF.libvirt.snapshot_image_format or source_type
1841 
1842         # NOTE(bfilippov): save lvm and rbd as raw
1843         if image_format == 'lvm' or image_format == 'rbd':
1844             image_format = 'raw'
1845 
1846         metadata = self._create_snapshot_metadata(instance.image_meta,
1847                                                   instance,
1848                                                   image_format,
1849                                                   snapshot['name'])
1850 
1851         snapshot_name = uuidutils.generate_uuid(dashed=False)
1852 
1853         state = guest.get_power_state(self._host)
1854 
1855         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1856         #               cold snapshots. Currently, checking for encryption is
1857         #               redundant because LVM supports only cold snapshots.
1858         #               It is necessary in case this situation changes in the
1859         #               future.
1860         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1861                 and source_type not in ('lvm')
1862                 and not CONF.ephemeral_storage_encryption.enabled
1863                 and not CONF.workarounds.disable_libvirt_livesnapshot
1864                 # NOTE(rmk): We cannot perform live snapshots when a
1865                 # managedSave file is present, so we will use the cold/legacy
1866                 # method for instances which are shutdown or paused.
1867                 # NOTE(mriedem): Live snapshot doesn't work with paused
1868                 # instances on older versions of libvirt/qemu. We can likely
1869                 # remove the restriction on PAUSED once we require
1870                 # libvirt>=3.6.0 and qemu>=2.10 since that works with the
1871                 # Pike Ubuntu Cloud Archive testing in Queens.
1872                 and state not in (power_state.SHUTDOWN, power_state.PAUSED)):
1873             live_snapshot = True
1874             # Abort is an idempotent operation, so make sure any block
1875             # jobs which may have failed are ended. This operation also
1876             # confirms the running instance, as opposed to the system as a
1877             # whole, has a new enough version of the hypervisor (bug 1193146).
1878             try:
1879                 guest.get_block_device(disk_path).abort_job()
1880             except libvirt.libvirtError as ex:
1881                 error_code = ex.get_error_code()
1882                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1883                     live_snapshot = False
1884                 else:
1885                     pass
1886         else:
1887             live_snapshot = False
1888 
1889         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1890                                           instance)
1891 
1892         root_disk = self.image_backend.by_libvirt_path(
1893             instance, disk_path, image_type=source_type)
1894 
1895         if live_snapshot:
1896             LOG.info("Beginning live snapshot process", instance=instance)
1897         else:
1898             LOG.info("Beginning cold snapshot process", instance=instance)
1899 
1900         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1901 
1902         update_task_state(task_state=task_states.IMAGE_UPLOADING,
1903                           expected_state=task_states.IMAGE_PENDING_UPLOAD)
1904 
1905         try:
1906             metadata['location'] = root_disk.direct_snapshot(
1907                 context, snapshot_name, image_format, image_id,
1908                 instance.image_ref)
1909             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1910                                   instance)
1911             self._image_api.update(context, image_id, metadata,
1912                                    purge_props=False)
1913         except (NotImplementedError, exception.ImageUnacceptable,
1914                 exception.Forbidden) as e:
1915             if type(e) != NotImplementedError:
1916                 LOG.warning('Performing standard snapshot because direct '
1917                             'snapshot failed: %(error)s',
1918                             {'error': encodeutils.exception_to_unicode(e)})
1919             failed_snap = metadata.pop('location', None)
1920             if failed_snap:
1921                 failed_snap = {'url': str(failed_snap)}
1922             root_disk.cleanup_direct_snapshot(failed_snap,
1923                                                   also_destroy_volume=True,
1924                                                   ignore_errors=True)
1925             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1926                               expected_state=task_states.IMAGE_UPLOADING)
1927 
1928             # TODO(nic): possibly abstract this out to the root_disk
1929             if source_type == 'rbd' and live_snapshot:
1930                 # Standard snapshot uses qemu-img convert from RBD which is
1931                 # not safe to run with live_snapshot.
1932                 live_snapshot = False
1933                 # Suspend the guest, so this is no longer a live snapshot
1934                 self._prepare_domain_for_snapshot(context, live_snapshot,
1935                                                   state, instance)
1936 
1937             snapshot_directory = CONF.libvirt.snapshots_directory
1938             fileutils.ensure_tree(snapshot_directory)
1939             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1940                 try:
1941                     out_path = os.path.join(tmpdir, snapshot_name)
1942                     if live_snapshot:
1943                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1944                         os.chmod(tmpdir, 0o701)
1945                         self._live_snapshot(context, instance, guest,
1946                                             disk_path, out_path, source_format,
1947                                             image_format, instance.image_meta)
1948                     else:
1949                         root_disk.snapshot_extract(out_path, image_format)
1950                     LOG.info("Snapshot extracted, beginning image upload",
1951                              instance=instance)
1952                 except libvirt.libvirtError as ex:
1953                     error_code = ex.get_error_code()
1954                     if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1955                         LOG.info('Instance %(instance_name)s disappeared '
1956                                  'while taking snapshot of it: [Error Code '
1957                                  '%(error_code)s] %(ex)s',
1958                                  {'instance_name': instance.name,
1959                                   'error_code': error_code,
1960                                   'ex': ex},
1961                                  instance=instance)
1962                         raise exception.InstanceNotFound(
1963                             instance_id=instance.uuid)
1964                     else:
1965                         raise
1966                 finally:
1967                     self._snapshot_domain(context, live_snapshot, virt_dom,
1968                                           state, instance)
1969 
1970                 # Upload that image to the image service
1971                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1972                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1973                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
1974                     self._image_api.update(context,
1975                                            image_id,
1976                                            metadata,
1977                                            image_file)
1978         except Exception:
1979             with excutils.save_and_reraise_exception():
1980                 LOG.exception(_("Failed to snapshot image"))
1981                 failed_snap = metadata.pop('location', None)
1982                 if failed_snap:
1983                     failed_snap = {'url': str(failed_snap)}
1984                 root_disk.cleanup_direct_snapshot(
1985                         failed_snap, also_destroy_volume=True,
1986                         ignore_errors=True)
1987 
1988         LOG.info("Snapshot image upload complete", instance=instance)
1989 
1990     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1991                                      instance):
1992         # NOTE(dkang): managedSave does not work for LXC
1993         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1994             if state == power_state.RUNNING or state == power_state.PAUSED:
1995                 self.suspend(context, instance)
1996 
1997     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1998                          instance):
1999         guest = None
2000         # NOTE(dkang): because previous managedSave is not called
2001         #              for LXC, _create_domain must not be called.
2002         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
2003             if state == power_state.RUNNING:
2004                 guest = self._create_domain(domain=virt_dom)
2005             elif state == power_state.PAUSED:
2006                 guest = self._create_domain(domain=virt_dom, pause=True)
2007 
2008             if guest is not None:
2009                 self._attach_pci_devices(
2010                     guest, pci_manager.get_instance_pci_devs(instance))
2011                 self._attach_direct_passthrough_ports(
2012                     context, instance, guest)
2013 
2014     def _can_set_admin_password(self, image_meta):
2015 
2016         if CONF.libvirt.virt_type == 'parallels':
2017             if not self._host.has_min_version(
2018                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
2019                 raise exception.SetAdminPasswdNotSupported()
2020         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
2021             if not image_meta.properties.get('hw_qemu_guest_agent', False):
2022                 raise exception.QemuGuestAgentNotEnabled()
2023         else:
2024             raise exception.SetAdminPasswdNotSupported()
2025 
2026     # TODO(melwitt): Combine this with the similar xenapi code at some point.
2027     def _save_instance_password_if_sshkey_present(self, instance, new_pass):
2028         sshkey = instance.key_data if 'key_data' in instance else None
2029         if sshkey and sshkey.startswith("ssh-rsa"):
2030             enc = crypto.ssh_encrypt_text(sshkey, new_pass)
2031             # NOTE(melwitt): The convert_password method doesn't actually do
2032             # anything with the context argument, so we can pass None.
2033             instance.system_metadata.update(
2034                 password.convert_password(None, base64.encode_as_text(enc)))
2035             instance.save()
2036 
2037     def set_admin_password(self, instance, new_pass):
2038         self._can_set_admin_password(instance.image_meta)
2039 
2040         guest = self._host.get_guest(instance)
2041         user = instance.image_meta.properties.get("os_admin_user")
2042         if not user:
2043             if instance.os_type == "windows":
2044                 user = "Administrator"
2045             else:
2046                 user = "root"
2047         try:
2048             guest.set_user_password(user, new_pass)
2049         except libvirt.libvirtError as ex:
2050             error_code = ex.get_error_code()
2051             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
2052                 LOG.debug('Failed to set password: QEMU agent unresponsive',
2053                           instance_uuid=instance.uuid)
2054                 raise NotImplementedError()
2055 
2056             err_msg = encodeutils.exception_to_unicode(ex)
2057             msg = (_('Error from libvirt while set password for username '
2058                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
2059                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
2060             raise exception.InternalError(msg)
2061         else:
2062             # Save the password in sysmeta so it may be retrieved from the
2063             # metadata service.
2064             self._save_instance_password_if_sshkey_present(instance, new_pass)
2065 
2066     def _can_quiesce(self, instance, image_meta):
2067         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
2068             raise exception.InstanceQuiesceNotSupported(
2069                 instance_id=instance.uuid)
2070 
2071         if not image_meta.properties.get('hw_qemu_guest_agent', False):
2072             raise exception.QemuGuestAgentNotEnabled()
2073 
2074     def _requires_quiesce(self, image_meta):
2075         return image_meta.properties.get('os_require_quiesce', False)
2076 
2077     def _set_quiesced(self, context, instance, image_meta, quiesced):
2078         self._can_quiesce(instance, image_meta)
2079         try:
2080             guest = self._host.get_guest(instance)
2081             if quiesced:
2082                 guest.freeze_filesystems()
2083             else:
2084                 guest.thaw_filesystems()
2085         except libvirt.libvirtError as ex:
2086             error_code = ex.get_error_code()
2087             err_msg = encodeutils.exception_to_unicode(ex)
2088             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
2089                      '[Error Code %(error_code)s] %(ex)s')
2090                    % {'instance_name': instance.name,
2091                       'error_code': error_code, 'ex': err_msg})
2092             raise exception.InternalError(msg)
2093 
2094     def quiesce(self, context, instance, image_meta):
2095         """Freeze the guest filesystems to prepare for snapshot.
2096 
2097         The qemu-guest-agent must be setup to execute fsfreeze.
2098         """
2099         self._set_quiesced(context, instance, image_meta, True)
2100 
2101     def unquiesce(self, context, instance, image_meta):
2102         """Thaw the guest filesystems after snapshot."""
2103         self._set_quiesced(context, instance, image_meta, False)
2104 
2105     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
2106                        source_format, image_format, image_meta):
2107         """Snapshot an instance without downtime."""
2108         dev = guest.get_block_device(disk_path)
2109 
2110         # Save a copy of the domain's persistent XML file
2111         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
2112 
2113         # Abort is an idempotent operation, so make sure any block
2114         # jobs which may have failed are ended.
2115         try:
2116             dev.abort_job()
2117         except Exception:
2118             pass
2119 
2120         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
2121         #             in QEMU 1.3. In order to do this, we need to create
2122         #             a destination image with the original backing file
2123         #             and matching size of the instance root disk.
2124         src_disk_size = libvirt_utils.get_disk_size(disk_path,
2125                                                     format=source_format)
2126         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
2127                                                         format=source_format,
2128                                                         basename=False)
2129         disk_delta = out_path + '.delta'
2130         libvirt_utils.create_cow_image(src_back_path, disk_delta,
2131                                        src_disk_size)
2132 
2133         quiesced = False
2134         try:
2135             self._set_quiesced(context, instance, image_meta, True)
2136             quiesced = True
2137         except exception.NovaException as err:
2138             if self._requires_quiesce(image_meta):
2139                 raise
2140             LOG.info('Skipping quiescing instance: %(reason)s.',
2141                      {'reason': err}, instance=instance)
2142 
2143         try:
2144             # NOTE (rmk): blockRebase cannot be executed on persistent
2145             #             domains, so we need to temporarily undefine it.
2146             #             If any part of this block fails, the domain is
2147             #             re-defined regardless.
2148             if guest.has_persistent_configuration():
2149                 support_uefi = self._has_uefi_support()
2150                 guest.delete_configuration(support_uefi)
2151 
2152             # NOTE (rmk): Establish a temporary mirror of our root disk and
2153             #             issue an abort once we have a complete copy.
2154             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
2155 
2156             while not dev.is_job_complete():
2157                 time.sleep(0.5)
2158 
2159             dev.abort_job()
2160             nova.privsep.path.chown(disk_delta, uid=os.getuid())
2161         finally:
2162             self._host.write_instance_config(xml)
2163             if quiesced:
2164                 self._set_quiesced(context, instance, image_meta, False)
2165 
2166         # Convert the delta (CoW) image with a backing file to a flat
2167         # image with no backing file.
2168         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
2169                                        out_path, image_format)
2170 
2171     def _volume_snapshot_update_status(self, context, snapshot_id, status):
2172         """Send a snapshot status update to Cinder.
2173 
2174         This method captures and logs exceptions that occur
2175         since callers cannot do anything useful with these exceptions.
2176 
2177         Operations on the Cinder side waiting for this will time out if
2178         a failure occurs sending the update.
2179 
2180         :param context: security context
2181         :param snapshot_id: id of snapshot being updated
2182         :param status: new status value
2183 
2184         """
2185 
2186         try:
2187             self._volume_api.update_snapshot_status(context,
2188                                                     snapshot_id,
2189                                                     status)
2190         except Exception:
2191             LOG.exception(_('Failed to send updated snapshot status '
2192                             'to volume service.'))
2193 
2194     def _volume_snapshot_create(self, context, instance, guest,
2195                                 volume_id, new_file):
2196         """Perform volume snapshot.
2197 
2198            :param guest: VM that volume is attached to
2199            :param volume_id: volume UUID to snapshot
2200            :param new_file: relative path to new qcow2 file present on share
2201 
2202         """
2203         xml = guest.get_xml_desc()
2204         xml_doc = etree.fromstring(xml)
2205 
2206         device_info = vconfig.LibvirtConfigGuest()
2207         device_info.parse_dom(xml_doc)
2208 
2209         disks_to_snap = []          # to be snapshotted by libvirt
2210         network_disks_to_snap = []  # network disks (netfs, etc.)
2211         disks_to_skip = []          # local disks not snapshotted
2212 
2213         for guest_disk in device_info.devices:
2214             if (guest_disk.root_name != 'disk'):
2215                 continue
2216 
2217             if (guest_disk.target_dev is None):
2218                 continue
2219 
2220             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2221                 disks_to_skip.append(guest_disk.target_dev)
2222                 continue
2223 
2224             # disk is a Cinder volume with the correct volume_id
2225 
2226             disk_info = {
2227                 'dev': guest_disk.target_dev,
2228                 'serial': guest_disk.serial,
2229                 'current_file': guest_disk.source_path,
2230                 'source_protocol': guest_disk.source_protocol,
2231                 'source_name': guest_disk.source_name,
2232                 'source_hosts': guest_disk.source_hosts,
2233                 'source_ports': guest_disk.source_ports
2234             }
2235 
2236             # Determine path for new_file based on current path
2237             if disk_info['current_file'] is not None:
2238                 current_file = disk_info['current_file']
2239                 new_file_path = os.path.join(os.path.dirname(current_file),
2240                                              new_file)
2241                 disks_to_snap.append((current_file, new_file_path))
2242             # NOTE(mriedem): This used to include a check for gluster in
2243             # addition to netfs since they were added together. Support for
2244             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2245             # however, if other volume drivers rely on the netfs disk source
2246             # protocol.
2247             elif disk_info['source_protocol'] == 'netfs':
2248                 network_disks_to_snap.append((disk_info, new_file))
2249 
2250         if not disks_to_snap and not network_disks_to_snap:
2251             msg = _('Found no disk to snapshot.')
2252             raise exception.InternalError(msg)
2253 
2254         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2255 
2256         for current_name, new_filename in disks_to_snap:
2257             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2258             snap_disk.name = current_name
2259             snap_disk.source_path = new_filename
2260             snap_disk.source_type = 'file'
2261             snap_disk.snapshot = 'external'
2262             snap_disk.driver_name = 'qcow2'
2263 
2264             snapshot.add_disk(snap_disk)
2265 
2266         for disk_info, new_filename in network_disks_to_snap:
2267             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2268             snap_disk.name = disk_info['dev']
2269             snap_disk.source_type = 'network'
2270             snap_disk.source_protocol = disk_info['source_protocol']
2271             snap_disk.snapshot = 'external'
2272             snap_disk.source_path = new_filename
2273             old_dir = disk_info['source_name'].split('/')[0]
2274             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2275             snap_disk.source_hosts = disk_info['source_hosts']
2276             snap_disk.source_ports = disk_info['source_ports']
2277 
2278             snapshot.add_disk(snap_disk)
2279 
2280         for dev in disks_to_skip:
2281             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2282             snap_disk.name = dev
2283             snap_disk.snapshot = 'no'
2284 
2285             snapshot.add_disk(snap_disk)
2286 
2287         snapshot_xml = snapshot.to_xml()
2288         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2289 
2290         image_meta = instance.image_meta
2291         try:
2292             # Check to see if we can quiesce the guest before taking the
2293             # snapshot.
2294             self._can_quiesce(instance, image_meta)
2295             try:
2296                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2297                                reuse_ext=True, quiesce=True)
2298                 return
2299             except libvirt.libvirtError:
2300                 # If the image says that quiesce is required then we fail.
2301                 if self._requires_quiesce(image_meta):
2302                     raise
2303                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2304                                 'attempting again with quiescing disabled.'),
2305                               instance=instance)
2306         except (exception.InstanceQuiesceNotSupported,
2307                 exception.QemuGuestAgentNotEnabled) as err:
2308             # If the image says that quiesce is required then we need to fail.
2309             if self._requires_quiesce(image_meta):
2310                 raise
2311             LOG.info('Skipping quiescing instance: %(reason)s.',
2312                      {'reason': err}, instance=instance)
2313 
2314         try:
2315             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2316                            reuse_ext=True, quiesce=False)
2317         except libvirt.libvirtError:
2318             LOG.exception(_('Unable to create VM snapshot, '
2319                             'failing volume_snapshot operation.'),
2320                           instance=instance)
2321 
2322             raise
2323 
2324     def _volume_refresh_connection_info(self, context, instance, volume_id):
2325         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2326                   context, volume_id, instance.uuid)
2327 
2328         driver_bdm = driver_block_device.convert_volume(bdm)
2329         if driver_bdm:
2330             driver_bdm.refresh_connection_info(context, instance,
2331                                                self._volume_api, self)
2332 
2333     def volume_snapshot_create(self, context, instance, volume_id,
2334                                create_info):
2335         """Create snapshots of a Cinder volume via libvirt.
2336 
2337         :param instance: VM instance object reference
2338         :param volume_id: id of volume being snapshotted
2339         :param create_info: dict of information used to create snapshots
2340                      - snapshot_id : ID of snapshot
2341                      - type : qcow2 / <other>
2342                      - new_file : qcow2 file created by Cinder which
2343                      becomes the VM's active image after
2344                      the snapshot is complete
2345         """
2346 
2347         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2348                   {'c_info': create_info}, instance=instance)
2349 
2350         try:
2351             guest = self._host.get_guest(instance)
2352         except exception.InstanceNotFound:
2353             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2354 
2355         if create_info['type'] != 'qcow2':
2356             msg = _('Unknown type: %s') % create_info['type']
2357             raise exception.InternalError(msg)
2358 
2359         snapshot_id = create_info.get('snapshot_id', None)
2360         if snapshot_id is None:
2361             msg = _('snapshot_id required in create_info')
2362             raise exception.InternalError(msg)
2363 
2364         try:
2365             self._volume_snapshot_create(context, instance, guest,
2366                                          volume_id, create_info['new_file'])
2367         except Exception:
2368             with excutils.save_and_reraise_exception():
2369                 LOG.exception(_('Error occurred during '
2370                                 'volume_snapshot_create, '
2371                                 'sending error status to Cinder.'),
2372                               instance=instance)
2373                 self._volume_snapshot_update_status(
2374                     context, snapshot_id, 'error')
2375 
2376         self._volume_snapshot_update_status(
2377             context, snapshot_id, 'creating')
2378 
2379         def _wait_for_snapshot():
2380             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2381 
2382             if snapshot.get('status') != 'creating':
2383                 self._volume_refresh_connection_info(context, instance,
2384                                                      volume_id)
2385                 raise loopingcall.LoopingCallDone()
2386 
2387         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2388         timer.start(interval=0.5).wait()
2389 
2390     @staticmethod
2391     def _rebase_with_qemu_img(guest, device, active_disk_object,
2392                               rebase_base):
2393         """Rebase a device tied to a guest using qemu-img.
2394 
2395         :param guest:the Guest which owns the device being rebased
2396         :type guest: nova.virt.libvirt.guest.Guest
2397         :param device: the guest block device to rebase
2398         :type device: nova.virt.libvirt.guest.BlockDevice
2399         :param active_disk_object: the guest block device to rebase
2400         :type active_disk_object: nova.virt.libvirt.config.\
2401                                     LibvirtConfigGuestDisk
2402         :param rebase_base: the new parent in the backing chain
2403         :type rebase_base: None or string
2404         """
2405 
2406         # It's unsure how well qemu-img handles network disks for
2407         # every protocol. So let's be safe.
2408         active_protocol = active_disk_object.source_protocol
2409         if active_protocol is not None:
2410             msg = _("Something went wrong when deleting a volume snapshot: "
2411                     "rebasing a %(protocol)s network disk using qemu-img "
2412                     "has not been fully tested") % {'protocol':
2413                     active_protocol}
2414             LOG.error(msg)
2415             raise exception.InternalError(msg)
2416 
2417         if rebase_base is None:
2418             # If backing_file is specified as "" (the empty string), then
2419             # the image is rebased onto no backing file (i.e. it will exist
2420             # independently of any backing file).
2421             backing_file = ""
2422             qemu_img_extra_arg = []
2423         else:
2424             # If the rebased image is going to have a backing file then
2425             # explicitly set the backing file format to avoid any security
2426             # concerns related to file format auto detection.
2427             backing_file = rebase_base
2428             b_file_fmt = images.qemu_img_info(backing_file).file_format
2429             qemu_img_extra_arg = ['-F', b_file_fmt]
2430 
2431         qemu_img_extra_arg.append(active_disk_object.source_path)
2432         utils.execute("qemu-img", "rebase", "-b", backing_file,
2433                       *qemu_img_extra_arg)
2434 
2435     def _volume_snapshot_delete(self, context, instance, volume_id,
2436                                 snapshot_id, delete_info=None):
2437         """Note:
2438             if file being merged into == active image:
2439                 do a blockRebase (pull) operation
2440             else:
2441                 do a blockCommit operation
2442             Files must be adjacent in snap chain.
2443 
2444         :param instance: instance object reference
2445         :param volume_id: volume UUID
2446         :param snapshot_id: snapshot UUID (unused currently)
2447         :param delete_info: {
2448             'type':              'qcow2',
2449             'file_to_merge':     'a.img',
2450             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2451                                                   active image)
2452           }
2453         """
2454 
2455         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2456                   instance=instance)
2457 
2458         if delete_info['type'] != 'qcow2':
2459             msg = _('Unknown delete_info type %s') % delete_info['type']
2460             raise exception.InternalError(msg)
2461 
2462         try:
2463             guest = self._host.get_guest(instance)
2464         except exception.InstanceNotFound:
2465             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2466 
2467         # Find dev name
2468         my_dev = None
2469         active_disk = None
2470 
2471         xml = guest.get_xml_desc()
2472         xml_doc = etree.fromstring(xml)
2473 
2474         device_info = vconfig.LibvirtConfigGuest()
2475         device_info.parse_dom(xml_doc)
2476 
2477         active_disk_object = None
2478 
2479         for guest_disk in device_info.devices:
2480             if (guest_disk.root_name != 'disk'):
2481                 continue
2482 
2483             if (guest_disk.target_dev is None or guest_disk.serial is None):
2484                 continue
2485 
2486             if guest_disk.serial == volume_id:
2487                 my_dev = guest_disk.target_dev
2488 
2489                 active_disk = guest_disk.source_path
2490                 active_protocol = guest_disk.source_protocol
2491                 active_disk_object = guest_disk
2492                 break
2493 
2494         if my_dev is None or (active_disk is None and active_protocol is None):
2495             LOG.debug('Domain XML: %s', xml, instance=instance)
2496             msg = (_('Disk with id: %s not found attached to instance.')
2497                    % volume_id)
2498             raise exception.InternalError(msg)
2499 
2500         LOG.debug("found device at %s", my_dev, instance=instance)
2501 
2502         def _get_snap_dev(filename, backing_store):
2503             if filename is None:
2504                 msg = _('filename cannot be None')
2505                 raise exception.InternalError(msg)
2506 
2507             # libgfapi delete
2508             LOG.debug("XML: %s", xml)
2509 
2510             LOG.debug("active disk object: %s", active_disk_object)
2511 
2512             # determine reference within backing store for desired image
2513             filename_to_merge = filename
2514             matched_name = None
2515             b = backing_store
2516             index = None
2517 
2518             current_filename = active_disk_object.source_name.split('/')[1]
2519             if current_filename == filename_to_merge:
2520                 return my_dev + '[0]'
2521 
2522             while b is not None:
2523                 source_filename = b.source_name.split('/')[1]
2524                 if source_filename == filename_to_merge:
2525                     LOG.debug('found match: %s', b.source_name)
2526                     matched_name = b.source_name
2527                     index = b.index
2528                     break
2529 
2530                 b = b.backing_store
2531 
2532             if matched_name is None:
2533                 msg = _('no match found for %s') % (filename_to_merge)
2534                 raise exception.InternalError(msg)
2535 
2536             LOG.debug('index of match (%s) is %s', b.source_name, index)
2537 
2538             my_snap_dev = '%s[%s]' % (my_dev, index)
2539             return my_snap_dev
2540 
2541         if delete_info['merge_target_file'] is None:
2542             # pull via blockRebase()
2543 
2544             # Merge the most recent snapshot into the active image
2545 
2546             rebase_disk = my_dev
2547             rebase_base = delete_info['file_to_merge']  # often None
2548             if (active_protocol is not None) and (rebase_base is not None):
2549                 rebase_base = _get_snap_dev(rebase_base,
2550                                             active_disk_object.backing_store)
2551 
2552             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2553             # and when available this flag _must_ be used to ensure backing
2554             # paths are maintained relative by qemu.
2555             #
2556             # If _RELATIVE flag not found, continue with old behaviour
2557             # (relative backing path seems to work for this case)
2558             try:
2559                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2560                 relative = rebase_base is not None
2561             except AttributeError:
2562                 LOG.warning(
2563                     "Relative blockrebase support was not detected. "
2564                     "Continuing with old behaviour.")
2565                 relative = False
2566 
2567             LOG.debug(
2568                 'disk: %(disk)s, base: %(base)s, '
2569                 'bw: %(bw)s, relative: %(relative)s',
2570                 {'disk': rebase_disk,
2571                  'base': rebase_base,
2572                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2573                  'relative': str(relative)}, instance=instance)
2574 
2575             dev = guest.get_block_device(rebase_disk)
2576             if guest.is_active():
2577                 result = dev.rebase(rebase_base, relative=relative)
2578                 if result == 0:
2579                     LOG.debug('blockRebase started successfully',
2580                               instance=instance)
2581 
2582                 while not dev.is_job_complete():
2583                     LOG.debug('waiting for blockRebase job completion',
2584                               instance=instance)
2585                     time.sleep(0.5)
2586 
2587             # If the guest is not running libvirt won't do a blockRebase.
2588             # In that case, let's ask qemu-img to rebase the disk.
2589             else:
2590                 LOG.debug('Guest is not running so doing a block rebase '
2591                           'using "qemu-img rebase"', instance=instance)
2592                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2593                                            rebase_base)
2594 
2595         else:
2596             # commit with blockCommit()
2597             my_snap_base = None
2598             my_snap_top = None
2599             commit_disk = my_dev
2600 
2601             if active_protocol is not None:
2602                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2603                                              active_disk_object.backing_store)
2604                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2605                                             active_disk_object.backing_store)
2606 
2607             commit_base = my_snap_base or delete_info['merge_target_file']
2608             commit_top = my_snap_top or delete_info['file_to_merge']
2609 
2610             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2611                       'commit_base=%(commit_base)s '
2612                       'commit_top=%(commit_top)s ',
2613                       {'commit_disk': commit_disk,
2614                        'commit_base': commit_base,
2615                        'commit_top': commit_top}, instance=instance)
2616 
2617             dev = guest.get_block_device(commit_disk)
2618             result = dev.commit(commit_base, commit_top, relative=True)
2619 
2620             if result == 0:
2621                 LOG.debug('blockCommit started successfully',
2622                           instance=instance)
2623 
2624             while not dev.is_job_complete():
2625                 LOG.debug('waiting for blockCommit job completion',
2626                           instance=instance)
2627                 time.sleep(0.5)
2628 
2629     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2630                                delete_info):
2631         try:
2632             self._volume_snapshot_delete(context, instance, volume_id,
2633                                          snapshot_id, delete_info=delete_info)
2634         except Exception:
2635             with excutils.save_and_reraise_exception():
2636                 LOG.exception(_('Error occurred during '
2637                                 'volume_snapshot_delete, '
2638                                 'sending error status to Cinder.'),
2639                               instance=instance)
2640                 self._volume_snapshot_update_status(
2641                     context, snapshot_id, 'error_deleting')
2642 
2643         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2644         self._volume_refresh_connection_info(context, instance, volume_id)
2645 
2646     def reboot(self, context, instance, network_info, reboot_type,
2647                block_device_info=None, bad_volumes_callback=None):
2648         """Reboot a virtual machine, given an instance reference."""
2649         if reboot_type == 'SOFT':
2650             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2651             try:
2652                 soft_reboot_success = self._soft_reboot(instance)
2653             except libvirt.libvirtError as e:
2654                 LOG.debug("Instance soft reboot failed: %s",
2655                           encodeutils.exception_to_unicode(e),
2656                           instance=instance)
2657                 soft_reboot_success = False
2658 
2659             if soft_reboot_success:
2660                 LOG.info("Instance soft rebooted successfully.",
2661                          instance=instance)
2662                 return
2663             else:
2664                 LOG.warning("Failed to soft reboot instance. "
2665                             "Trying hard reboot.",
2666                             instance=instance)
2667         return self._hard_reboot(context, instance, network_info,
2668                                  block_device_info)
2669 
2670     def _soft_reboot(self, instance):
2671         """Attempt to shutdown and restart the instance gracefully.
2672 
2673         We use shutdown and create here so we can return if the guest
2674         responded and actually rebooted. Note that this method only
2675         succeeds if the guest responds to acpi. Therefore we return
2676         success or failure so we can fall back to a hard reboot if
2677         necessary.
2678 
2679         :returns: True if the reboot succeeded
2680         """
2681         guest = self._host.get_guest(instance)
2682 
2683         state = guest.get_power_state(self._host)
2684         old_domid = guest.id
2685         # NOTE(vish): This check allows us to reboot an instance that
2686         #             is already shutdown.
2687         if state == power_state.RUNNING:
2688             guest.shutdown()
2689         # NOTE(vish): This actually could take slightly longer than the
2690         #             FLAG defines depending on how long the get_info
2691         #             call takes to return.
2692         self._prepare_pci_devices_for_use(
2693             pci_manager.get_instance_pci_devs(instance, 'all'))
2694         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2695             guest = self._host.get_guest(instance)
2696 
2697             state = guest.get_power_state(self._host)
2698             new_domid = guest.id
2699 
2700             # NOTE(ivoks): By checking domain IDs, we make sure we are
2701             #              not recreating domain that's already running.
2702             if old_domid != new_domid:
2703                 if state in [power_state.SHUTDOWN,
2704                              power_state.CRASHED]:
2705                     LOG.info("Instance shutdown successfully.",
2706                              instance=instance)
2707                     self._create_domain(domain=guest._domain)
2708                     timer = loopingcall.FixedIntervalLoopingCall(
2709                         self._wait_for_running, instance)
2710                     timer.start(interval=0.5).wait()
2711                     return True
2712                 else:
2713                     LOG.info("Instance may have been rebooted during soft "
2714                              "reboot, so return now.", instance=instance)
2715                     return True
2716             greenthread.sleep(1)
2717         return False
2718 
2719     def _hard_reboot(self, context, instance, network_info,
2720                      block_device_info=None):
2721         """Reboot a virtual machine, given an instance reference.
2722 
2723         Performs a Libvirt reset (if supported) on the domain.
2724 
2725         If Libvirt reset is unavailable this method actually destroys and
2726         re-creates the domain to ensure the reboot happens, as the guest
2727         OS cannot ignore this action.
2728         """
2729         # NOTE(sbauza): Since we undefine the guest XML when destroying, we
2730         # need to remember the existing mdevs for reusing them.
2731         mdevs = self._get_all_assigned_mediated_devices(instance)
2732         mdevs = list(mdevs.keys())
2733         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2734         # the hard reboot operation is relied upon by operators to be an
2735         # automated attempt to fix as many things as possible about a
2736         # non-functioning instance before resorting to manual intervention.
2737         # With this goal in mind, we tear down all the aspects of an instance
2738         # we can here without losing data. This allows us to re-initialise from
2739         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2740         self.destroy(context, instance, network_info, destroy_disks=False,
2741                      block_device_info=block_device_info)
2742 
2743         # Convert the system metadata to image metadata
2744         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2745         #                https://bugs.launchpad.net/nova/+bug/1349978
2746         instance_dir = libvirt_utils.get_instance_path(instance)
2747         fileutils.ensure_tree(instance_dir)
2748 
2749         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2750                                             instance,
2751                                             instance.image_meta,
2752                                             block_device_info)
2753         # NOTE(vish): This could generate the wrong device_format if we are
2754         #             using the raw backend and the images don't exist yet.
2755         #             The create_images_and_backing below doesn't properly
2756         #             regenerate raw backend images, however, so when it
2757         #             does we need to (re)generate the xml after the images
2758         #             are in place.
2759         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2760                                   instance.image_meta,
2761                                   block_device_info=block_device_info,
2762                                   mdevs=mdevs)
2763 
2764         # NOTE(mdbooth): context.auth_token will not be set when we call
2765         #                _hard_reboot from resume_state_on_host_boot()
2766         if context.auth_token is not None:
2767             # NOTE (rmk): Re-populate any missing backing files.
2768             config = vconfig.LibvirtConfigGuest()
2769             config.parse_str(xml)
2770             backing_disk_info = self._get_instance_disk_info_from_config(
2771                 config, block_device_info)
2772             self._create_images_and_backing(context, instance, instance_dir,
2773                                             backing_disk_info)
2774 
2775         # Initialize all the necessary networking, block devices and
2776         # start the instance.
2777         # NOTE(melwitt): Pass vifs_already_plugged=True here even though we've
2778         # unplugged vifs earlier. The behavior of neutron plug events depends
2779         # on which vif type we're using and we are working with a stale network
2780         # info cache here, so won't rely on waiting for neutron plug events.
2781         # vifs_already_plugged=True means "do not wait for neutron plug events"
2782         self._create_domain_and_network(context, xml, instance, network_info,
2783                                         block_device_info=block_device_info,
2784                                         vifs_already_plugged=True)
2785         self._prepare_pci_devices_for_use(
2786             pci_manager.get_instance_pci_devs(instance, 'all'))
2787 
2788         def _wait_for_reboot():
2789             """Called at an interval until the VM is running again."""
2790             state = self.get_info(instance).state
2791 
2792             if state == power_state.RUNNING:
2793                 LOG.info("Instance rebooted successfully.",
2794                          instance=instance)
2795                 raise loopingcall.LoopingCallDone()
2796 
2797         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2798         timer.start(interval=0.5).wait()
2799 
2800     def pause(self, instance):
2801         """Pause VM instance."""
2802         self._host.get_guest(instance).pause()
2803 
2804     def unpause(self, instance):
2805         """Unpause paused VM instance."""
2806         guest = self._host.get_guest(instance)
2807         guest.resume()
2808         guest.sync_guest_time()
2809 
2810     def _clean_shutdown(self, instance, timeout, retry_interval):
2811         """Attempt to shutdown the instance gracefully.
2812 
2813         :param instance: The instance to be shutdown
2814         :param timeout: How long to wait in seconds for the instance to
2815                         shutdown
2816         :param retry_interval: How often in seconds to signal the instance
2817                                to shutdown while waiting
2818 
2819         :returns: True if the shutdown succeeded
2820         """
2821 
2822         # List of states that represent a shutdown instance
2823         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2824                            power_state.CRASHED]
2825 
2826         try:
2827             guest = self._host.get_guest(instance)
2828         except exception.InstanceNotFound:
2829             # If the instance has gone then we don't need to
2830             # wait for it to shutdown
2831             return True
2832 
2833         state = guest.get_power_state(self._host)
2834         if state in SHUTDOWN_STATES:
2835             LOG.info("Instance already shutdown.", instance=instance)
2836             return True
2837 
2838         LOG.debug("Shutting down instance from state %s", state,
2839                   instance=instance)
2840         guest.shutdown()
2841         retry_countdown = retry_interval
2842 
2843         for sec in range(timeout):
2844 
2845             guest = self._host.get_guest(instance)
2846             state = guest.get_power_state(self._host)
2847 
2848             if state in SHUTDOWN_STATES:
2849                 LOG.info("Instance shutdown successfully after %d seconds.",
2850                          sec, instance=instance)
2851                 return True
2852 
2853             # Note(PhilD): We can't assume that the Guest was able to process
2854             #              any previous shutdown signal (for example it may
2855             #              have still been startingup, so within the overall
2856             #              timeout we re-trigger the shutdown every
2857             #              retry_interval
2858             if retry_countdown == 0:
2859                 retry_countdown = retry_interval
2860                 # Instance could shutdown at any time, in which case we
2861                 # will get an exception when we call shutdown
2862                 try:
2863                     LOG.debug("Instance in state %s after %d seconds - "
2864                               "resending shutdown", state, sec,
2865                               instance=instance)
2866                     guest.shutdown()
2867                 except libvirt.libvirtError:
2868                     # Assume this is because its now shutdown, so loop
2869                     # one more time to clean up.
2870                     LOG.debug("Ignoring libvirt exception from shutdown "
2871                               "request.", instance=instance)
2872                     continue
2873             else:
2874                 retry_countdown -= 1
2875 
2876             time.sleep(1)
2877 
2878         LOG.info("Instance failed to shutdown in %d seconds.",
2879                  timeout, instance=instance)
2880         return False
2881 
2882     def power_off(self, instance, timeout=0, retry_interval=0):
2883         """Power off the specified instance."""
2884         if timeout:
2885             self._clean_shutdown(instance, timeout, retry_interval)
2886         self._destroy(instance)
2887 
2888     def power_on(self, context, instance, network_info,
2889                  block_device_info=None):
2890         """Power on the specified instance."""
2891         # We use _hard_reboot here to ensure that all backing files,
2892         # network, and block device connections, etc. are established
2893         # and available before we attempt to start the instance.
2894         self._hard_reboot(context, instance, network_info, block_device_info)
2895 
2896     def trigger_crash_dump(self, instance):
2897 
2898         """Trigger crash dump by injecting an NMI to the specified instance."""
2899         try:
2900             self._host.get_guest(instance).inject_nmi()
2901         except libvirt.libvirtError as ex:
2902             error_code = ex.get_error_code()
2903 
2904             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2905                 raise exception.TriggerCrashDumpNotSupported()
2906             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2907                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2908 
2909             LOG.exception(_('Error from libvirt while injecting an NMI to '
2910                             '%(instance_uuid)s: '
2911                             '[Error Code %(error_code)s] %(ex)s'),
2912                           {'instance_uuid': instance.uuid,
2913                            'error_code': error_code, 'ex': ex})
2914             raise
2915 
2916     def suspend(self, context, instance):
2917         """Suspend the specified instance."""
2918         guest = self._host.get_guest(instance)
2919 
2920         self._detach_pci_devices(guest,
2921             pci_manager.get_instance_pci_devs(instance))
2922         self._detach_direct_passthrough_ports(context, instance, guest)
2923         self._detach_mediated_devices(guest)
2924         guest.save_memory_state()
2925 
2926     def resume(self, context, instance, network_info, block_device_info=None):
2927         """resume the specified instance."""
2928         xml = self._get_existing_domain_xml(instance, network_info,
2929                                             block_device_info)
2930         guest = self._create_domain_and_network(context, xml, instance,
2931                            network_info, block_device_info=block_device_info,
2932                            vifs_already_plugged=True)
2933         self._attach_pci_devices(guest,
2934             pci_manager.get_instance_pci_devs(instance))
2935         self._attach_direct_passthrough_ports(
2936             context, instance, guest, network_info)
2937         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2938                                                      instance)
2939         timer.start(interval=0.5).wait()
2940         guest.sync_guest_time()
2941 
2942     def resume_state_on_host_boot(self, context, instance, network_info,
2943                                   block_device_info=None):
2944         """resume guest state when a host is booted."""
2945         # Check if the instance is running already and avoid doing
2946         # anything if it is.
2947         try:
2948             guest = self._host.get_guest(instance)
2949             state = guest.get_power_state(self._host)
2950 
2951             ignored_states = (power_state.RUNNING,
2952                               power_state.SUSPENDED,
2953                               power_state.NOSTATE,
2954                               power_state.PAUSED)
2955 
2956             if state in ignored_states:
2957                 return
2958         except (exception.InternalError, exception.InstanceNotFound):
2959             pass
2960 
2961         # Instance is not up and could be in an unknown state.
2962         # Be as absolute as possible about getting it back into
2963         # a known and running state.
2964         self._hard_reboot(context, instance, network_info, block_device_info)
2965 
2966     def rescue(self, context, instance, network_info, image_meta,
2967                rescue_password):
2968         """Loads a VM using rescue images.
2969 
2970         A rescue is normally performed when something goes wrong with the
2971         primary images and data needs to be corrected/recovered. Rescuing
2972         should not edit or over-ride the original image, only allow for
2973         data recovery.
2974 
2975         """
2976         instance_dir = libvirt_utils.get_instance_path(instance)
2977         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2978         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2979         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2980 
2981         rescue_image_id = None
2982         if image_meta.obj_attr_is_set("id"):
2983             rescue_image_id = image_meta.id
2984 
2985         rescue_images = {
2986             'image_id': (rescue_image_id or
2987                         CONF.libvirt.rescue_image_id or instance.image_ref),
2988             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2989                           instance.kernel_id),
2990             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2991                            instance.ramdisk_id),
2992         }
2993         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2994                                             instance,
2995                                             image_meta,
2996                                             rescue=True)
2997         injection_info = InjectionInfo(network_info=network_info,
2998                                        admin_pass=rescue_password,
2999                                        files=None)
3000         gen_confdrive = functools.partial(self._create_configdrive,
3001                                           context, instance, injection_info,
3002                                           rescue=True)
3003         # NOTE(sbauza): Since rescue recreates the guest XML, we need to
3004         # remember the existing mdevs for reusing them.
3005         mdevs = self._get_all_assigned_mediated_devices(instance)
3006         mdevs = list(mdevs.keys())
3007         self._create_image(context, instance, disk_info['mapping'],
3008                            injection_info=injection_info, suffix='.rescue',
3009                            disk_images=rescue_images)
3010         xml = self._get_guest_xml(context, instance, network_info, disk_info,
3011                                   image_meta, rescue=rescue_images,
3012                                   mdevs=mdevs)
3013         self._destroy(instance)
3014         self._create_domain(xml, post_xml_callback=gen_confdrive)
3015 
3016     def unrescue(self, instance, network_info):
3017         """Reboot the VM which is being rescued back into primary images.
3018         """
3019         instance_dir = libvirt_utils.get_instance_path(instance)
3020         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
3021         xml = libvirt_utils.load_file(unrescue_xml_path)
3022         guest = self._host.get_guest(instance)
3023 
3024         # TODO(sahid): We are converting all calls from a
3025         # virDomain object to use nova.virt.libvirt.Guest.
3026         # We should be able to remove virt_dom at the end.
3027         virt_dom = guest._domain
3028         self._destroy(instance)
3029         self._create_domain(xml, virt_dom)
3030         os.unlink(unrescue_xml_path)
3031         rescue_files = os.path.join(instance_dir, "*.rescue")
3032         for rescue_file in glob.iglob(rescue_files):
3033             if os.path.isdir(rescue_file):
3034                 shutil.rmtree(rescue_file)
3035             else:
3036                 os.unlink(rescue_file)
3037         # cleanup rescue volume
3038         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
3039                                 if lvmdisk.endswith('.rescue')])
3040         if CONF.libvirt.images_type == 'rbd':
3041             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
3042                                       disk.endswith('.rescue'))
3043             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
3044 
3045     def poll_rebooting_instances(self, timeout, instances):
3046         pass
3047 
3048     def spawn(self, context, instance, image_meta, injected_files,
3049               admin_password, allocations, network_info=None,
3050               block_device_info=None):
3051         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
3052                                             instance,
3053                                             image_meta,
3054                                             block_device_info)
3055         injection_info = InjectionInfo(network_info=network_info,
3056                                        files=injected_files,
3057                                        admin_pass=admin_password)
3058         gen_confdrive = functools.partial(self._create_configdrive,
3059                                           context, instance,
3060                                           injection_info)
3061         self._create_image(context, instance, disk_info['mapping'],
3062                            injection_info=injection_info,
3063                            block_device_info=block_device_info)
3064 
3065         # Required by Quobyte CI
3066         self._ensure_console_log_for_instance(instance)
3067 
3068         # Does the guest need to be assigned some vGPU mediated devices ?
3069         mdevs = self._allocate_mdevs(allocations)
3070 
3071         xml = self._get_guest_xml(context, instance, network_info,
3072                                   disk_info, image_meta,
3073                                   block_device_info=block_device_info,
3074                                   mdevs=mdevs)
3075         self._create_domain_and_network(
3076             context, xml, instance, network_info,
3077             block_device_info=block_device_info,
3078             post_xml_callback=gen_confdrive,
3079             destroy_disks_on_failure=True)
3080         LOG.debug("Guest created on hypervisor", instance=instance)
3081 
3082         def _wait_for_boot():
3083             """Called at an interval until the VM is running."""
3084             state = self.get_info(instance).state
3085 
3086             if state == power_state.RUNNING:
3087                 LOG.info("Instance spawned successfully.", instance=instance)
3088                 raise loopingcall.LoopingCallDone()
3089 
3090         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
3091         timer.start(interval=0.5).wait()
3092 
3093     def _get_console_output_file(self, instance, console_log):
3094         bytes_to_read = MAX_CONSOLE_BYTES
3095         log_data = b""  # The last N read bytes
3096         i = 0  # in case there is a log rotation (like "virtlogd")
3097         path = console_log
3098 
3099         while bytes_to_read > 0 and os.path.exists(path):
3100             read_log_data, remaining = nova.privsep.path.last_bytes(
3101                                         path, bytes_to_read)
3102             # We need the log file content in chronological order,
3103             # that's why we *prepend* the log data.
3104             log_data = read_log_data + log_data
3105 
3106             # Prep to read the next file in the chain
3107             bytes_to_read -= len(read_log_data)
3108             path = console_log + "." + str(i)
3109             i += 1
3110 
3111             if remaining > 0:
3112                 LOG.info('Truncated console log returned, '
3113                          '%d bytes ignored', remaining, instance=instance)
3114         return log_data
3115 
3116     def get_console_output(self, context, instance):
3117         guest = self._host.get_guest(instance)
3118 
3119         xml = guest.get_xml_desc()
3120         tree = etree.fromstring(xml)
3121 
3122         # If the guest has a console logging to a file prefer to use that
3123         file_consoles = tree.findall("./devices/console[@type='file']")
3124         if file_consoles:
3125             for file_console in file_consoles:
3126                 source_node = file_console.find('./source')
3127                 if source_node is None:
3128                     continue
3129                 path = source_node.get("path")
3130                 if not path:
3131                     continue
3132 
3133                 if not os.path.exists(path):
3134                     LOG.info('Instance is configured with a file console, '
3135                              'but the backing file is not (yet?) present',
3136                              instance=instance)
3137                     return ""
3138 
3139                 return self._get_console_output_file(instance, path)
3140 
3141         # Try 'pty' types
3142         pty_consoles = tree.findall("./devices/console[@type='pty']")
3143         if pty_consoles:
3144             for pty_console in pty_consoles:
3145                 source_node = pty_console.find('./source')
3146                 if source_node is None:
3147                     continue
3148                 pty = source_node.get("path")
3149                 if not pty:
3150                     continue
3151                 break
3152             else:
3153                 raise exception.ConsoleNotAvailable()
3154         else:
3155             raise exception.ConsoleNotAvailable()
3156 
3157         console_log = self._get_console_log_path(instance)
3158         data = nova.privsep.libvirt.readpty(pty)
3159 
3160         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
3161         # which create a dedicated file device for the console logging.
3162         # Other virt_types like xen, lxc, uml, parallels depend on the
3163         # flush of that pty device into the "console.log" file to ensure
3164         # that a series of "get_console_output" calls return the complete
3165         # content even after rebooting a guest.
3166         nova.privsep.path.writefile(console_log, 'a+', data)
3167         return self._get_console_output_file(instance, console_log)
3168 
3169     def get_host_ip_addr(self):
3170         ips = compute_utils.get_machine_ips()
3171         if CONF.my_ip not in ips:
3172             LOG.warning('my_ip address (%(my_ip)s) was not found on '
3173                         'any of the interfaces: %(ifaces)s',
3174                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
3175         return CONF.my_ip
3176 
3177     def get_vnc_console(self, context, instance):
3178         def get_vnc_port_for_instance(instance_name):
3179             guest = self._host.get_guest(instance)
3180 
3181             xml = guest.get_xml_desc()
3182             xml_dom = etree.fromstring(xml)
3183 
3184             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
3185             if graphic is not None:
3186                 return graphic.get('port')
3187             # NOTE(rmk): We had VNC consoles enabled but the instance in
3188             # question is not actually listening for connections.
3189             raise exception.ConsoleTypeUnavailable(console_type='vnc')
3190 
3191         port = get_vnc_port_for_instance(instance.name)
3192         host = CONF.vnc.server_proxyclient_address
3193 
3194         return ctype.ConsoleVNC(host=host, port=port)
3195 
3196     def get_spice_console(self, context, instance):
3197         def get_spice_ports_for_instance(instance_name):
3198             guest = self._host.get_guest(instance)
3199 
3200             xml = guest.get_xml_desc()
3201             xml_dom = etree.fromstring(xml)
3202 
3203             graphic = xml_dom.find("./devices/graphics[@type='spice']")
3204             if graphic is not None:
3205                 return (graphic.get('port'), graphic.get('tlsPort'))
3206             # NOTE(rmk): We had Spice consoles enabled but the instance in
3207             # question is not actually listening for connections.
3208             raise exception.ConsoleTypeUnavailable(console_type='spice')
3209 
3210         ports = get_spice_ports_for_instance(instance.name)
3211         host = CONF.spice.server_proxyclient_address
3212 
3213         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
3214 
3215     def get_serial_console(self, context, instance):
3216         guest = self._host.get_guest(instance)
3217         for hostname, port in self._get_serial_ports_from_guest(
3218                 guest, mode='bind'):
3219             return ctype.ConsoleSerial(host=hostname, port=port)
3220         raise exception.ConsoleTypeUnavailable(console_type='serial')
3221 
3222     @staticmethod
3223     def _create_ephemeral(target, ephemeral_size,
3224                           fs_label, os_type, is_block_dev=False,
3225                           context=None, specified_fs=None,
3226                           vm_mode=None):
3227         if not is_block_dev:
3228             if (CONF.libvirt.virt_type == "parallels" and
3229                     vm_mode == fields.VMMode.EXE):
3230 
3231                 libvirt_utils.create_ploop_image('expanded', target,
3232                                                  '%dG' % ephemeral_size,
3233                                                  specified_fs)
3234                 return
3235             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3236 
3237         # Run as root only for block devices.
3238         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3239                       specified_fs=specified_fs)
3240 
3241     @staticmethod
3242     def _create_swap(target, swap_mb, context=None):
3243         """Create a swap file of specified size."""
3244         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3245         nova.privsep.fs.unprivileged_mkfs('swap', target)
3246 
3247     @staticmethod
3248     def _get_console_log_path(instance):
3249         return os.path.join(libvirt_utils.get_instance_path(instance),
3250                             'console.log')
3251 
3252     def _ensure_console_log_for_instance(self, instance):
3253         # NOTE(mdbooth): Although libvirt will create this file for us
3254         # automatically when it starts, it will initially create it with
3255         # root ownership and then chown it depending on the configuration of
3256         # the domain it is launching. Quobyte CI explicitly disables the
3257         # chown by setting dynamic_ownership=0 in libvirt's config.
3258         # Consequently when the domain starts it is unable to write to its
3259         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3260         #
3261         # To work around this, we create the file manually before starting
3262         # the domain so it has the same ownership as Nova. This works
3263         # for Quobyte CI because it is also configured to run qemu as the same
3264         # user as the Nova service. Installations which don't set
3265         # dynamic_ownership=0 are not affected because libvirt will always
3266         # correctly configure permissions regardless of initial ownership.
3267         #
3268         # Setting dynamic_ownership=0 is dubious and potentially broken in
3269         # more ways than console.log (see comment #22 on the above bug), so
3270         # Future Maintainer who finds this code problematic should check to see
3271         # if we still support it.
3272         console_file = self._get_console_log_path(instance)
3273         LOG.debug('Ensure instance console log exists: %s', console_file,
3274                   instance=instance)
3275         try:
3276             libvirt_utils.file_open(console_file, 'a').close()
3277         # NOTE(sfinucan): We can safely ignore permission issues here and
3278         # assume that it is libvirt that has taken ownership of this file.
3279         except IOError as ex:
3280             if ex.errno != errno.EACCES:
3281                 raise
3282             LOG.debug('Console file already exists: %s.', console_file)
3283 
3284     @staticmethod
3285     def _get_disk_config_image_type():
3286         # TODO(mikal): there is a bug here if images_type has
3287         # changed since creation of the instance, but I am pretty
3288         # sure that this bug already exists.
3289         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3290 
3291     @staticmethod
3292     def _is_booted_from_volume(block_device_info):
3293         """Determines whether the VM is booting from volume
3294 
3295         Determines whether the block device info indicates that the VM
3296         is booting from a volume.
3297         """
3298         block_device_mapping = driver.block_device_info_get_mapping(
3299             block_device_info)
3300         return bool(block_device.get_root_bdm(block_device_mapping))
3301 
3302     def _inject_data(self, disk, instance, injection_info):
3303         """Injects data in a disk image
3304 
3305         Helper used for injecting data in a disk image file system.
3306 
3307         :param disk: The disk we're injecting into (an Image object)
3308         :param instance: The instance we're injecting into
3309         :param injection_info: Injection info
3310         """
3311         # Handles the partition need to be used.
3312         LOG.debug('Checking root disk injection %s',
3313                   str(injection_info), instance=instance)
3314         target_partition = None
3315         if not instance.kernel_id:
3316             target_partition = CONF.libvirt.inject_partition
3317             if target_partition == 0:
3318                 target_partition = None
3319         if CONF.libvirt.virt_type == 'lxc':
3320             target_partition = None
3321 
3322         # Handles the key injection.
3323         if CONF.libvirt.inject_key and instance.get('key_data'):
3324             key = str(instance.key_data)
3325         else:
3326             key = None
3327 
3328         # Handles the admin password injection.
3329         if not CONF.libvirt.inject_password:
3330             admin_pass = None
3331         else:
3332             admin_pass = injection_info.admin_pass
3333 
3334         # Handles the network injection.
3335         net = netutils.get_injected_network_template(
3336             injection_info.network_info,
3337             libvirt_virt_type=CONF.libvirt.virt_type)
3338 
3339         # Handles the metadata injection
3340         metadata = instance.get('metadata')
3341 
3342         if any((key, net, metadata, admin_pass, injection_info.files)):
3343             LOG.debug('Injecting %s', str(injection_info),
3344                       instance=instance)
3345             img_id = instance.image_ref
3346             try:
3347                 disk_api.inject_data(disk.get_model(self._conn),
3348                                      key, net, metadata, admin_pass,
3349                                      injection_info.files,
3350                                      partition=target_partition,
3351                                      mandatory=('files',))
3352             except Exception as e:
3353                 with excutils.save_and_reraise_exception():
3354                     LOG.error('Error injecting data into image '
3355                               '%(img_id)s (%(e)s)',
3356                               {'img_id': img_id, 'e': e},
3357                               instance=instance)
3358 
3359     # NOTE(sileht): many callers of this method assume that this
3360     # method doesn't fail if an image already exists but instead
3361     # think that it will be reused (ie: (live)-migration/resize)
3362     def _create_image(self, context, instance,
3363                       disk_mapping, injection_info=None, suffix='',
3364                       disk_images=None, block_device_info=None,
3365                       fallback_from_host=None,
3366                       ignore_bdi_for_swap=False):
3367         booted_from_volume = self._is_booted_from_volume(block_device_info)
3368 
3369         def image(fname, image_type=CONF.libvirt.images_type):
3370             return self.image_backend.by_name(instance,
3371                                               fname + suffix, image_type)
3372 
3373         def raw(fname):
3374             return image(fname, image_type='raw')
3375 
3376         # ensure directories exist and are writable
3377         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3378 
3379         LOG.info('Creating image', instance=instance)
3380 
3381         inst_type = instance.get_flavor()
3382         swap_mb = 0
3383         if 'disk.swap' in disk_mapping:
3384             mapping = disk_mapping['disk.swap']
3385 
3386             if ignore_bdi_for_swap:
3387                 # This is a workaround to support legacy swap resizing,
3388                 # which does not touch swap size specified in bdm,
3389                 # but works with flavor specified size only.
3390                 # In this case we follow the legacy logic and ignore block
3391                 # device info completely.
3392                 # NOTE(ft): This workaround must be removed when a correct
3393                 # implementation of resize operation changing sizes in bdms is
3394                 # developed. Also at that stage we probably may get rid of
3395                 # the direct usage of flavor swap size here,
3396                 # leaving the work with bdm only.
3397                 swap_mb = inst_type['swap']
3398             else:
3399                 swap = driver.block_device_info_get_swap(block_device_info)
3400                 if driver.swap_is_usable(swap):
3401                     swap_mb = swap['swap_size']
3402                 elif (inst_type['swap'] > 0 and
3403                       not block_device.volume_in_mapping(
3404                         mapping['dev'], block_device_info)):
3405                     swap_mb = inst_type['swap']
3406 
3407             if swap_mb > 0:
3408                 if (CONF.libvirt.virt_type == "parallels" and
3409                         instance.vm_mode == fields.VMMode.EXE):
3410                     msg = _("Swap disk is not supported "
3411                             "for Virtuozzo container")
3412                     raise exception.Invalid(msg)
3413 
3414         if not disk_images:
3415             disk_images = {'image_id': instance.image_ref,
3416                            'kernel_id': instance.kernel_id,
3417                            'ramdisk_id': instance.ramdisk_id}
3418 
3419         if disk_images['kernel_id']:
3420             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3421             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3422                                 context=context,
3423                                 filename=fname,
3424                                 image_id=disk_images['kernel_id'])
3425             if disk_images['ramdisk_id']:
3426                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3427                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3428                                      context=context,
3429                                      filename=fname,
3430                                      image_id=disk_images['ramdisk_id'])
3431 
3432         if CONF.libvirt.virt_type == 'uml':
3433             # PONDERING(mikal): can I assume that root is UID zero in every
3434             # OS? Probably not.
3435             uid = pwd.getpwnam('root').pw_uid
3436             nova.privsep.path.chown(image('disk').path, uid=uid)
3437 
3438         self._create_and_inject_local_root(context, instance,
3439                                            booted_from_volume, suffix,
3440                                            disk_images, injection_info,
3441                                            fallback_from_host)
3442 
3443         # Lookup the filesystem type if required
3444         os_type_with_default = nova.privsep.fs.get_fs_type_for_os_type(
3445             instance.os_type)
3446         # Generate a file extension based on the file system
3447         # type and the mkfs commands configured if any
3448         file_extension = nova.privsep.fs.get_file_extension_for_os_type(
3449             os_type_with_default, CONF.default_ephemeral_format)
3450 
3451         vm_mode = fields.VMMode.get_from_instance(instance)
3452         ephemeral_gb = instance.flavor.ephemeral_gb
3453         if 'disk.local' in disk_mapping:
3454             disk_image = image('disk.local')
3455             fn = functools.partial(self._create_ephemeral,
3456                                    fs_label='ephemeral0',
3457                                    os_type=instance.os_type,
3458                                    is_block_dev=disk_image.is_block_dev,
3459                                    vm_mode=vm_mode)
3460             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3461             size = ephemeral_gb * units.Gi
3462             disk_image.cache(fetch_func=fn,
3463                              context=context,
3464                              filename=fname,
3465                              size=size,
3466                              ephemeral_size=ephemeral_gb)
3467 
3468         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3469                 block_device_info)):
3470             disk_image = image(blockinfo.get_eph_disk(idx))
3471 
3472             specified_fs = eph.get('guest_format')
3473             if specified_fs and not self.is_supported_fs_format(specified_fs):
3474                 msg = _("%s format is not supported") % specified_fs
3475                 raise exception.InvalidBDMFormat(details=msg)
3476 
3477             fn = functools.partial(self._create_ephemeral,
3478                                    fs_label='ephemeral%d' % idx,
3479                                    os_type=instance.os_type,
3480                                    is_block_dev=disk_image.is_block_dev,
3481                                    vm_mode=vm_mode)
3482             size = eph['size'] * units.Gi
3483             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3484             disk_image.cache(fetch_func=fn,
3485                              context=context,
3486                              filename=fname,
3487                              size=size,
3488                              ephemeral_size=eph['size'],
3489                              specified_fs=specified_fs)
3490 
3491         if swap_mb > 0:
3492             size = swap_mb * units.Mi
3493             image('disk.swap').cache(fetch_func=self._create_swap,
3494                                      context=context,
3495                                      filename="swap_%s" % swap_mb,
3496                                      size=size,
3497                                      swap_mb=swap_mb)
3498 
3499     def _create_and_inject_local_root(self, context, instance,
3500                                       booted_from_volume, suffix, disk_images,
3501                                       injection_info, fallback_from_host):
3502         # File injection only if needed
3503         need_inject = (not configdrive.required_by(instance) and
3504                        injection_info is not None and
3505                        CONF.libvirt.inject_partition != -2)
3506 
3507         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3508         # currently happens only on rescue - we still don't want to
3509         # create a base image.
3510         if not booted_from_volume:
3511             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3512             size = instance.flavor.root_gb * units.Gi
3513 
3514             if size == 0 or suffix == '.rescue':
3515                 size = None
3516 
3517             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3518                                                  CONF.libvirt.images_type)
3519             if instance.task_state == task_states.RESIZE_FINISH:
3520                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3521             if backend.SUPPORTS_CLONE:
3522                 def clone_fallback_to_fetch(*args, **kwargs):
3523                     try:
3524                         backend.clone(context, disk_images['image_id'])
3525                     except exception.ImageUnacceptable:
3526                         libvirt_utils.fetch_image(*args, **kwargs)
3527                 fetch_func = clone_fallback_to_fetch
3528             else:
3529                 fetch_func = libvirt_utils.fetch_image
3530             self._try_fetch_image_cache(backend, fetch_func, context,
3531                                         root_fname, disk_images['image_id'],
3532                                         instance, size, fallback_from_host)
3533 
3534             if need_inject:
3535                 self._inject_data(backend, instance, injection_info)
3536 
3537         elif need_inject:
3538             LOG.warning('File injection into a boot from volume '
3539                         'instance is not supported', instance=instance)
3540 
3541     def _create_configdrive(self, context, instance, injection_info,
3542                             rescue=False):
3543         # As this method being called right after the definition of a
3544         # domain, but before its actual launch, device metadata will be built
3545         # and saved in the instance for it to be used by the config drive and
3546         # the metadata service.
3547         instance.device_metadata = self._build_device_metadata(context,
3548                                                                instance)
3549         if configdrive.required_by(instance):
3550             LOG.info('Using config drive', instance=instance)
3551 
3552             name = 'disk.config'
3553             if rescue:
3554                 name += '.rescue'
3555 
3556             config_disk = self.image_backend.by_name(
3557                 instance, name, self._get_disk_config_image_type())
3558 
3559             # Don't overwrite an existing config drive
3560             if not config_disk.exists():
3561                 extra_md = {}
3562                 if injection_info.admin_pass:
3563                     extra_md['admin_pass'] = injection_info.admin_pass
3564 
3565                 inst_md = instance_metadata.InstanceMetadata(
3566                     instance, content=injection_info.files, extra_md=extra_md,
3567                     network_info=injection_info.network_info,
3568                     request_context=context)
3569 
3570                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3571                 with cdb:
3572                     # NOTE(mdbooth): We're hardcoding here the path of the
3573                     # config disk when using the flat backend. This isn't
3574                     # good, but it's required because we need a local path we
3575                     # know we can write to in case we're subsequently
3576                     # importing into rbd. This will be cleaned up when we
3577                     # replace this with a call to create_from_func, but that
3578                     # can't happen until we've updated the backends and we
3579                     # teach them not to cache config disks. This isn't
3580                     # possible while we're still using cache() under the hood.
3581                     config_disk_local_path = os.path.join(
3582                         libvirt_utils.get_instance_path(instance), name)
3583                     LOG.info('Creating config drive at %(path)s',
3584                              {'path': config_disk_local_path},
3585                              instance=instance)
3586 
3587                     try:
3588                         cdb.make_drive(config_disk_local_path)
3589                     except processutils.ProcessExecutionError as e:
3590                         with excutils.save_and_reraise_exception():
3591                             LOG.error('Creating config drive failed with '
3592                                       'error: %s', e, instance=instance)
3593 
3594                 try:
3595                     config_disk.import_file(
3596                         instance, config_disk_local_path, name)
3597                 finally:
3598                     # NOTE(mikal): if the config drive was imported into RBD,
3599                     # then we no longer need the local copy
3600                     if CONF.libvirt.images_type == 'rbd':
3601                         LOG.info('Deleting local config drive %(path)s '
3602                                  'because it was imported into RBD.',
3603                                  {'path': config_disk_local_path},
3604                                  instance=instance)
3605                         os.unlink(config_disk_local_path)
3606 
3607     def _prepare_pci_devices_for_use(self, pci_devices):
3608         # kvm , qemu support managed mode
3609         # In managed mode, the configured device will be automatically
3610         # detached from the host OS drivers when the guest is started,
3611         # and then re-attached when the guest shuts down.
3612         if CONF.libvirt.virt_type != 'xen':
3613             # we do manual detach only for xen
3614             return
3615         try:
3616             for dev in pci_devices:
3617                 libvirt_dev_addr = dev['hypervisor_name']
3618                 libvirt_dev = \
3619                         self._host.device_lookup_by_name(libvirt_dev_addr)
3620                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3621                 # http://libvirt.org/html/libvirt-libvirt.html.
3622                 libvirt_dev.dettach()
3623 
3624             # Note(yjiang5): A reset of one PCI device may impact other
3625             # devices on the same bus, thus we need two separated loops
3626             # to detach and then reset it.
3627             for dev in pci_devices:
3628                 libvirt_dev_addr = dev['hypervisor_name']
3629                 libvirt_dev = \
3630                         self._host.device_lookup_by_name(libvirt_dev_addr)
3631                 libvirt_dev.reset()
3632 
3633         except libvirt.libvirtError as exc:
3634             raise exception.PciDevicePrepareFailed(id=dev['id'],
3635                                                    instance_uuid=
3636                                                    dev['instance_uuid'],
3637                                                    reason=six.text_type(exc))
3638 
3639     def _detach_pci_devices(self, guest, pci_devs):
3640         try:
3641             for dev in pci_devs:
3642                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3643                 # after detachDeviceFlags returned, we should check the dom to
3644                 # ensure the detaching is finished
3645                 xml = guest.get_xml_desc()
3646                 xml_doc = etree.fromstring(xml)
3647                 guest_config = vconfig.LibvirtConfigGuest()
3648                 guest_config.parse_dom(xml_doc)
3649 
3650                 for hdev in [d for d in guest_config.devices
3651                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3652                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3653                     dbsf = pci_utils.parse_address(dev.address)
3654                     if [int(x, 16) for x in hdbsf] ==\
3655                             [int(x, 16) for x in dbsf]:
3656                         raise exception.PciDeviceDetachFailed(reason=
3657                                                               "timeout",
3658                                                               dev=dev)
3659 
3660         except libvirt.libvirtError as ex:
3661             error_code = ex.get_error_code()
3662             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3663                 LOG.warning("Instance disappeared while detaching "
3664                             "a PCI device from it.")
3665             else:
3666                 raise
3667 
3668     def _attach_pci_devices(self, guest, pci_devs):
3669         try:
3670             for dev in pci_devs:
3671                 guest.attach_device(self._get_guest_pci_device(dev))
3672 
3673         except libvirt.libvirtError:
3674             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3675                       {'dev': pci_devs, 'dom': guest.id})
3676             raise
3677 
3678     @staticmethod
3679     def _has_direct_passthrough_port(network_info):
3680         for vif in network_info:
3681             if (vif['vnic_type'] in
3682                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3683                 return True
3684         return False
3685 
3686     def _attach_direct_passthrough_ports(
3687         self, context, instance, guest, network_info=None):
3688         if network_info is None:
3689             network_info = instance.info_cache.network_info
3690         if network_info is None:
3691             return
3692 
3693         if self._has_direct_passthrough_port(network_info):
3694             for vif in network_info:
3695                 if (vif['vnic_type'] in
3696                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3697                     cfg = self.vif_driver.get_config(instance,
3698                                                      vif,
3699                                                      instance.image_meta,
3700                                                      instance.flavor,
3701                                                      CONF.libvirt.virt_type,
3702                                                      self._host)
3703                     LOG.debug('Attaching direct passthrough port %(port)s '
3704                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3705                               instance=instance)
3706                     guest.attach_device(cfg)
3707 
3708     def _detach_direct_passthrough_ports(self, context, instance, guest):
3709         network_info = instance.info_cache.network_info
3710         if network_info is None:
3711             return
3712 
3713         if self._has_direct_passthrough_port(network_info):
3714             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3715             # pci request per direct passthrough port. Therefore we can trust
3716             # that pci_slot value in the vif is correct.
3717             direct_passthrough_pci_addresses = [
3718                 vif['profile']['pci_slot']
3719                 for vif in network_info
3720                 if (vif['vnic_type'] in
3721                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3722                     vif['profile'].get('pci_slot') is not None)
3723             ]
3724 
3725             # use detach_pci_devices to avoid failure in case of
3726             # multiple guest direct passthrough ports with the same MAC
3727             # (protection use-case, ports are on different physical
3728             # interfaces)
3729             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3730             direct_passthrough_pci_addresses = (
3731                 [pci_dev for pci_dev in pci_devs
3732                  if pci_dev.address in direct_passthrough_pci_addresses])
3733             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3734 
3735     def _set_host_enabled(self, enabled,
3736                           disable_reason=DISABLE_REASON_UNDEFINED):
3737         """Enables / Disables the compute service on this host.
3738 
3739            This doesn't override non-automatic disablement with an automatic
3740            setting; thereby permitting operators to keep otherwise
3741            healthy hosts out of rotation.
3742         """
3743 
3744         status_name = {True: 'disabled',
3745                        False: 'enabled'}
3746 
3747         disable_service = not enabled
3748 
3749         ctx = nova_context.get_admin_context()
3750         try:
3751             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3752 
3753             if service.disabled != disable_service:
3754                 # Note(jang): this is a quick fix to stop operator-
3755                 # disabled compute hosts from re-enabling themselves
3756                 # automatically. We prefix any automatic reason code
3757                 # with a fixed string. We only re-enable a host
3758                 # automatically if we find that string in place.
3759                 # This should probably be replaced with a separate flag.
3760                 if not service.disabled or (
3761                         service.disabled_reason and
3762                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3763                     service.disabled = disable_service
3764                     service.disabled_reason = (
3765                        DISABLE_PREFIX + disable_reason
3766                        if disable_service and disable_reason else
3767                            DISABLE_REASON_UNDEFINED)
3768                     service.save()
3769                     LOG.debug('Updating compute service status to %s',
3770                               status_name[disable_service])
3771                 else:
3772                     LOG.debug('Not overriding manual compute service '
3773                               'status with: %s',
3774                               status_name[disable_service])
3775         except exception.ComputeHostNotFound:
3776             LOG.warning('Cannot update service status on host "%s" '
3777                         'since it is not registered.', CONF.host)
3778         except Exception:
3779             LOG.warning('Cannot update service status on host "%s" '
3780                         'due to an unexpected exception.', CONF.host,
3781                         exc_info=True)
3782 
3783         if enabled:
3784             mount.get_manager().host_up(self._host)
3785         else:
3786             mount.get_manager().host_down()
3787 
3788     def _get_guest_cpu_model_config(self):
3789         mode = CONF.libvirt.cpu_mode
3790         model = CONF.libvirt.cpu_model
3791         extra_flags = set([flag.lower() for flag in
3792             CONF.libvirt.cpu_model_extra_flags])
3793 
3794         if (CONF.libvirt.virt_type == "kvm" or
3795             CONF.libvirt.virt_type == "qemu"):
3796             if mode is None:
3797                 caps = self._host.get_capabilities()
3798                 # AArch64 lacks 'host-model' support because neither libvirt
3799                 # nor QEMU are able to tell what the host CPU model exactly is.
3800                 # And there is no CPU description code for ARM(64) at this
3801                 # point.
3802 
3803                 # Also worth noting: 'host-passthrough' mode will completely
3804                 # break live migration, *unless* all the Compute nodes (running
3805                 # libvirtd) have *identical* CPUs.
3806                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
3807                     mode = "host-passthrough"
3808                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
3809                              'migration can break unless all compute nodes '
3810                              'have identical cpus. AArch64 does not support '
3811                              'other modes.')
3812                 else:
3813                     mode = "host-model"
3814             if mode == "none":
3815                 return vconfig.LibvirtConfigGuestCPU()
3816         else:
3817             if mode is None or mode == "none":
3818                 return None
3819 
3820         if ((CONF.libvirt.virt_type != "kvm" and
3821              CONF.libvirt.virt_type != "qemu")):
3822             msg = _("Config requested an explicit CPU model, but "
3823                     "the current libvirt hypervisor '%s' does not "
3824                     "support selecting CPU models") % CONF.libvirt.virt_type
3825             raise exception.Invalid(msg)
3826 
3827         if mode == "custom" and model is None:
3828             msg = _("Config requested a custom CPU model, but no "
3829                     "model name was provided")
3830             raise exception.Invalid(msg)
3831         elif mode != "custom" and model is not None:
3832             msg = _("A CPU model name should not be set when a "
3833                     "host CPU model is requested")
3834             raise exception.Invalid(msg)
3835 
3836         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen, "
3837                   "with extra flags: '%(extra_flags)s'",
3838                   {'mode': mode,
3839                    'model': (model or ""),
3840                    'extra_flags': (extra_flags or "")})
3841 
3842         cpu = vconfig.LibvirtConfigGuestCPU()
3843         cpu.mode = mode
3844         cpu.model = model
3845 
3846         # NOTE (kchamart): Currently there's no existing way to ask if a
3847         # given CPU model + CPU flags combination is supported by KVM &
3848         # a specific QEMU binary.  However, libvirt runs the 'CPUID'
3849         # command upfront -- before even a Nova instance (a QEMU
3850         # process) is launched -- to construct CPU models and check
3851         # their validity; so we are good there.  In the long-term,
3852         # upstream libvirt intends to add an additional new API that can
3853         # do fine-grained validation of a certain CPU model + CPU flags
3854         # against a specific QEMU binary (the libvirt RFE bug for that:
3855         # https://bugzilla.redhat.com/show_bug.cgi?id=1559832).
3856         for flag in extra_flags:
3857             cpu.add_feature(vconfig.LibvirtConfigGuestCPUFeature(flag))
3858 
3859         return cpu
3860 
3861     def _get_guest_cpu_config(self, flavor, image_meta,
3862                               guest_cpu_numa_config, instance_numa_topology):
3863         cpu = self._get_guest_cpu_model_config()
3864 
3865         if cpu is None:
3866             return None
3867 
3868         topology = hardware.get_best_cpu_topology(
3869                 flavor, image_meta, numa_topology=instance_numa_topology)
3870 
3871         cpu.sockets = topology.sockets
3872         cpu.cores = topology.cores
3873         cpu.threads = topology.threads
3874         cpu.numa = guest_cpu_numa_config
3875 
3876         return cpu
3877 
3878     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3879                                image_type=None):
3880         disk_unit = None
3881         disk = self.image_backend.by_name(instance, name, image_type)
3882         if (name == 'disk.config' and image_type == 'rbd' and
3883                 not disk.exists()):
3884             # This is likely an older config drive that has not been migrated
3885             # to rbd yet. Try to fall back on 'flat' image type.
3886             # TODO(melwitt): Add online migration of some sort so we can
3887             # remove this fall back once we know all config drives are in rbd.
3888             # NOTE(vladikr): make sure that the flat image exist, otherwise
3889             # the image will be created after the domain definition.
3890             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3891             if flat_disk.exists():
3892                 disk = flat_disk
3893                 LOG.debug('Config drive not found in RBD, falling back to the '
3894                           'instance directory', instance=instance)
3895         disk_info = disk_mapping[name]
3896         if 'unit' in disk_mapping and disk_info['bus'] == 'scsi':
3897             disk_unit = disk_mapping['unit']
3898             disk_mapping['unit'] += 1  # Increments for the next disk added
3899         conf = disk.libvirt_info(disk_info['bus'],
3900                                  disk_info['dev'],
3901                                  disk_info['type'],
3902                                  self.disk_cachemode,
3903                                  inst_type['extra_specs'],
3904                                  self._host.get_version(),
3905                                  disk_unit=disk_unit)
3906         return conf
3907 
3908     def _get_guest_fs_config(self, instance, name, image_type=None):
3909         disk = self.image_backend.by_name(instance, name, image_type)
3910         return disk.libvirt_fs_info("/", "ploop")
3911 
3912     def _get_guest_storage_config(self, context, instance, image_meta,
3913                                   disk_info,
3914                                   rescue, block_device_info,
3915                                   inst_type, os_type):
3916         devices = []
3917         disk_mapping = disk_info['mapping']
3918 
3919         block_device_mapping = driver.block_device_info_get_mapping(
3920             block_device_info)
3921         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3922         scsi_controller = self._get_scsi_controller(image_meta)
3923 
3924         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3925             # The virtio-scsi can handle up to 256 devices but the
3926             # optional element "address" must be defined to describe
3927             # where the device is placed on the controller (see:
3928             # LibvirtConfigGuestDeviceAddressDrive).
3929             #
3930             # Note about why it's added in disk_mapping: It's not
3931             # possible to pass an 'int' by reference in Python, so we
3932             # use disk_mapping as container to keep reference of the
3933             # unit added and be able to increment it for each disk
3934             # added.
3935             #
3936             # NOTE(jaypipes,melwitt): If this is a boot-from-volume instance,
3937             # we need to start the disk mapping unit at 1 since we set the
3938             # bootable volume's unit to 0 for the bootable volume.
3939             disk_mapping['unit'] = 0
3940             if self._is_booted_from_volume(block_device_info):
3941                 disk_mapping['unit'] = 1
3942 
3943         def _get_ephemeral_devices():
3944             eph_devices = []
3945             for idx, eph in enumerate(
3946                 driver.block_device_info_get_ephemerals(
3947                     block_device_info)):
3948                 diskeph = self._get_guest_disk_config(
3949                     instance,
3950                     blockinfo.get_eph_disk(idx),
3951                     disk_mapping, inst_type)
3952                 eph_devices.append(diskeph)
3953             return eph_devices
3954 
3955         if mount_rootfs:
3956             fs = vconfig.LibvirtConfigGuestFilesys()
3957             fs.source_type = "mount"
3958             fs.source_dir = os.path.join(
3959                 libvirt_utils.get_instance_path(instance), 'rootfs')
3960             devices.append(fs)
3961         elif (os_type == fields.VMMode.EXE and
3962               CONF.libvirt.virt_type == "parallels"):
3963             if rescue:
3964                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3965                 devices.append(fsrescue)
3966 
3967                 fsos = self._get_guest_fs_config(instance, "disk")
3968                 fsos.target_dir = "/mnt/rescue"
3969                 devices.append(fsos)
3970             else:
3971                 if 'disk' in disk_mapping:
3972                     fs = self._get_guest_fs_config(instance, "disk")
3973                     devices.append(fs)
3974                 devices = devices + _get_ephemeral_devices()
3975         else:
3976 
3977             if rescue:
3978                 diskrescue = self._get_guest_disk_config(instance,
3979                                                          'disk.rescue',
3980                                                          disk_mapping,
3981                                                          inst_type)
3982                 devices.append(diskrescue)
3983 
3984                 diskos = self._get_guest_disk_config(instance,
3985                                                      'disk',
3986                                                      disk_mapping,
3987                                                      inst_type)
3988                 devices.append(diskos)
3989             else:
3990                 if 'disk' in disk_mapping:
3991                     diskos = self._get_guest_disk_config(instance,
3992                                                          'disk',
3993                                                          disk_mapping,
3994                                                          inst_type)
3995                     devices.append(diskos)
3996 
3997                 if 'disk.local' in disk_mapping:
3998                     disklocal = self._get_guest_disk_config(instance,
3999                                                             'disk.local',
4000                                                             disk_mapping,
4001                                                             inst_type)
4002                     devices.append(disklocal)
4003                     instance.default_ephemeral_device = (
4004                         block_device.prepend_dev(disklocal.target_dev))
4005 
4006                 devices = devices + _get_ephemeral_devices()
4007 
4008                 if 'disk.swap' in disk_mapping:
4009                     diskswap = self._get_guest_disk_config(instance,
4010                                                            'disk.swap',
4011                                                            disk_mapping,
4012                                                            inst_type)
4013                     devices.append(diskswap)
4014                     instance.default_swap_device = (
4015                         block_device.prepend_dev(diskswap.target_dev))
4016 
4017             config_name = 'disk.config.rescue' if rescue else 'disk.config'
4018             if config_name in disk_mapping:
4019                 diskconfig = self._get_guest_disk_config(
4020                     instance, config_name, disk_mapping, inst_type,
4021                     self._get_disk_config_image_type())
4022                 devices.append(diskconfig)
4023 
4024         for vol in block_device.get_bdms_to_connect(block_device_mapping,
4025                                                    mount_rootfs):
4026             connection_info = vol['connection_info']
4027             vol_dev = block_device.prepend_dev(vol['mount_device'])
4028             info = disk_mapping[vol_dev]
4029             self._connect_volume(context, connection_info, instance)
4030             if scsi_controller and scsi_controller.model == 'virtio-scsi':
4031                 # Check if this is the bootable volume when in a
4032                 # boot-from-volume instance, and if so, ensure the unit
4033                 # attribute is 0.
4034                 if vol.get('boot_index') == 0:
4035                     info['unit'] = 0
4036                 else:
4037                     info['unit'] = disk_mapping['unit']
4038                     disk_mapping['unit'] += 1
4039             cfg = self._get_volume_config(connection_info, info)
4040             devices.append(cfg)
4041             vol['connection_info'] = connection_info
4042             vol.save()
4043 
4044         for d in devices:
4045             self._set_cache_mode(d)
4046 
4047         if scsi_controller:
4048             devices.append(scsi_controller)
4049 
4050         return devices
4051 
4052     @staticmethod
4053     def _get_scsi_controller(image_meta):
4054         """Return scsi controller or None based on image meta"""
4055         if image_meta.properties.get('hw_scsi_model'):
4056             hw_scsi_model = image_meta.properties.hw_scsi_model
4057             scsi_controller = vconfig.LibvirtConfigGuestController()
4058             scsi_controller.type = 'scsi'
4059             scsi_controller.model = hw_scsi_model
4060             scsi_controller.index = 0
4061             return scsi_controller
4062 
4063     def _get_host_sysinfo_serial_hardware(self):
4064         """Get a UUID from the host hardware
4065 
4066         Get a UUID for the host hardware reported by libvirt.
4067         This is typically from the SMBIOS data, unless it has
4068         been overridden in /etc/libvirt/libvirtd.conf
4069         """
4070         caps = self._host.get_capabilities()
4071         return caps.host.uuid
4072 
4073     def _get_host_sysinfo_serial_os(self):
4074         """Get a UUID from the host operating system
4075 
4076         Get a UUID for the host operating system. Modern Linux
4077         distros based on systemd provide a /etc/machine-id
4078         file containing a UUID. This is also provided inside
4079         systemd based containers and can be provided by other
4080         init systems too, since it is just a plain text file.
4081         """
4082         if not os.path.exists("/etc/machine-id"):
4083             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
4084             raise exception.InternalError(msg)
4085 
4086         with open("/etc/machine-id") as f:
4087             # We want to have '-' in the right place
4088             # so we parse & reformat the value
4089             lines = f.read().split()
4090             if not lines:
4091                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
4092                 raise exception.InternalError(msg)
4093 
4094             return str(uuid.UUID(lines[0]))
4095 
4096     def _get_host_sysinfo_serial_auto(self):
4097         if os.path.exists("/etc/machine-id"):
4098             return self._get_host_sysinfo_serial_os()
4099         else:
4100             return self._get_host_sysinfo_serial_hardware()
4101 
4102     def _get_guest_config_sysinfo(self, instance):
4103         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
4104 
4105         sysinfo.system_manufacturer = version.vendor_string()
4106         sysinfo.system_product = version.product_string()
4107         sysinfo.system_version = version.version_string_with_package()
4108 
4109         sysinfo.system_serial = self._sysinfo_serial_func()
4110         sysinfo.system_uuid = instance.uuid
4111 
4112         sysinfo.system_family = "Virtual Machine"
4113 
4114         return sysinfo
4115 
4116     def _get_guest_pci_device(self, pci_device):
4117 
4118         dbsf = pci_utils.parse_address(pci_device.address)
4119         dev = vconfig.LibvirtConfigGuestHostdevPCI()
4120         dev.domain, dev.bus, dev.slot, dev.function = dbsf
4121 
4122         # only kvm support managed mode
4123         if CONF.libvirt.virt_type in ('xen', 'parallels',):
4124             dev.managed = 'no'
4125         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
4126             dev.managed = 'yes'
4127 
4128         return dev
4129 
4130     def _get_guest_config_meta(self, instance):
4131         """Get metadata config for guest."""
4132 
4133         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
4134         meta.package = version.version_string_with_package()
4135         meta.name = instance.display_name
4136         meta.creationTime = time.time()
4137 
4138         if instance.image_ref not in ("", None):
4139             meta.roottype = "image"
4140             meta.rootid = instance.image_ref
4141 
4142         system_meta = instance.system_metadata
4143         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
4144         ometa.userid = instance.user_id
4145         ometa.username = system_meta.get('owner_user_name', 'N/A')
4146         ometa.projectid = instance.project_id
4147         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
4148         meta.owner = ometa
4149 
4150         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
4151         flavor = instance.flavor
4152         fmeta.name = flavor.name
4153         fmeta.memory = flavor.memory_mb
4154         fmeta.vcpus = flavor.vcpus
4155         fmeta.ephemeral = flavor.ephemeral_gb
4156         fmeta.disk = flavor.root_gb
4157         fmeta.swap = flavor.swap
4158 
4159         meta.flavor = fmeta
4160 
4161         return meta
4162 
4163     def _machine_type_mappings(self):
4164         mappings = {}
4165         for mapping in CONF.libvirt.hw_machine_type:
4166             host_arch, _, machine_type = mapping.partition('=')
4167             mappings[host_arch] = machine_type
4168         return mappings
4169 
4170     def _get_machine_type(self, image_meta, caps):
4171         # The underlying machine type can be set as an image attribute,
4172         # or otherwise based on some architecture specific defaults
4173 
4174         mach_type = None
4175 
4176         if image_meta.properties.get('hw_machine_type') is not None:
4177             mach_type = image_meta.properties.hw_machine_type
4178         else:
4179             # For ARM systems we will default to vexpress-a15 for armv7
4180             # and virt for aarch64
4181             if caps.host.cpu.arch == fields.Architecture.ARMV7:
4182                 mach_type = "vexpress-a15"
4183 
4184             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4185                 mach_type = "virt"
4186 
4187             if caps.host.cpu.arch in (fields.Architecture.S390,
4188                                       fields.Architecture.S390X):
4189                 mach_type = 's390-ccw-virtio'
4190 
4191             # If set in the config, use that as the default.
4192             if CONF.libvirt.hw_machine_type:
4193                 mappings = self._machine_type_mappings()
4194                 mach_type = mappings.get(caps.host.cpu.arch)
4195 
4196         return mach_type
4197 
4198     @staticmethod
4199     def _create_idmaps(klass, map_strings):
4200         idmaps = []
4201         if len(map_strings) > 5:
4202             map_strings = map_strings[0:5]
4203             LOG.warning("Too many id maps, only included first five.")
4204         for map_string in map_strings:
4205             try:
4206                 idmap = klass()
4207                 values = [int(i) for i in map_string.split(":")]
4208                 idmap.start = values[0]
4209                 idmap.target = values[1]
4210                 idmap.count = values[2]
4211                 idmaps.append(idmap)
4212             except (ValueError, IndexError):
4213                 LOG.warning("Invalid value for id mapping %s", map_string)
4214         return idmaps
4215 
4216     def _get_guest_idmaps(self):
4217         id_maps = []
4218         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
4219             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
4220                                            CONF.libvirt.uid_maps)
4221             id_maps.extend(uid_maps)
4222         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
4223             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
4224                                            CONF.libvirt.gid_maps)
4225             id_maps.extend(gid_maps)
4226         return id_maps
4227 
4228     def _update_guest_cputune(self, guest, flavor, virt_type):
4229         is_able = self._host.is_cpu_control_policy_capable()
4230 
4231         cputuning = ['shares', 'period', 'quota']
4232         wants_cputune = any([k for k in cputuning
4233             if "quota:cpu_" + k in flavor.extra_specs.keys()])
4234 
4235         if wants_cputune and not is_able:
4236             raise exception.UnsupportedHostCPUControlPolicy()
4237 
4238         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
4239             return
4240 
4241         if guest.cputune is None:
4242             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
4243             # Setting the default cpu.shares value to be a value
4244             # dependent on the number of vcpus
4245         guest.cputune.shares = 1024 * guest.vcpus
4246 
4247         for name in cputuning:
4248             key = "quota:cpu_" + name
4249             if key in flavor.extra_specs:
4250                 setattr(guest.cputune, name,
4251                         int(flavor.extra_specs[key]))
4252 
4253     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
4254                                            wants_hugepages):
4255         if instance_numa_topology:
4256             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
4257             for instance_cell in instance_numa_topology.cells:
4258                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
4259                 guest_cell.id = instance_cell.id
4260                 guest_cell.cpus = instance_cell.cpuset
4261                 guest_cell.memory = instance_cell.memory * units.Ki
4262 
4263                 # The vhost-user network backend requires file backed
4264                 # guest memory (ie huge pages) to be marked as shared
4265                 # access, not private, so an external process can read
4266                 # and write the pages.
4267                 #
4268                 # You can't change the shared vs private flag for an
4269                 # already running guest, and since we can't predict what
4270                 # types of NIC may be hotplugged, we have no choice but
4271                 # to unconditionally turn on the shared flag. This has
4272                 # no real negative functional effect on the guest, so
4273                 # is a reasonable approach to take
4274                 if wants_hugepages:
4275                     guest_cell.memAccess = "shared"
4276                 guest_cpu_numa.cells.append(guest_cell)
4277             return guest_cpu_numa
4278 
4279     def _wants_hugepages(self, host_topology, instance_topology):
4280         """Determine if the guest / host topology implies the
4281            use of huge pages for guest RAM backing
4282         """
4283 
4284         if host_topology is None or instance_topology is None:
4285             return False
4286 
4287         avail_pagesize = [page.size_kb
4288                           for page in host_topology.cells[0].mempages]
4289         avail_pagesize.sort()
4290         # Remove smallest page size as that's not classed as a largepage
4291         avail_pagesize = avail_pagesize[1:]
4292 
4293         # See if we have page size set
4294         for cell in instance_topology.cells:
4295             if (cell.pagesize is not None and
4296                 cell.pagesize in avail_pagesize):
4297                 return True
4298 
4299         return False
4300 
4301     def _get_cell_pairs(self, guest_cpu_numa_config, host_topology):
4302         """Returns the lists of pairs(tuple) of an instance cell and
4303         corresponding host cell:
4304             [(LibvirtConfigGuestCPUNUMACell, NUMACell), ...]
4305         """
4306         cell_pairs = []
4307         for guest_config_cell in guest_cpu_numa_config.cells:
4308             for host_cell in host_topology.cells:
4309                 if guest_config_cell.id == host_cell.id:
4310                     cell_pairs.append((guest_config_cell, host_cell))
4311         return cell_pairs
4312 
4313     def _get_pin_cpuset(self, vcpu, object_numa_cell, host_cell):
4314         """Returns the config object of LibvirtConfigGuestCPUTuneVCPUPin.
4315         Prepares vcpupin config for the guest with the following caveats:
4316 
4317             a) If there is pinning information in the cell, we pin vcpus to
4318                individual CPUs
4319             b) Otherwise we float over the whole host NUMA node
4320         """
4321         pin_cpuset = vconfig.LibvirtConfigGuestCPUTuneVCPUPin()
4322         pin_cpuset.id = vcpu
4323 
4324         if object_numa_cell.cpu_pinning:
4325             pin_cpuset.cpuset = set([object_numa_cell.cpu_pinning[vcpu]])
4326         else:
4327             pin_cpuset.cpuset = host_cell.cpuset
4328 
4329         return pin_cpuset
4330 
4331     def _get_emulatorpin_cpuset(self, vcpu, object_numa_cell, vcpus_rt,
4332                                 emulator_threads_policy, wants_realtime,
4333                                 pin_cpuset):
4334         """Returns a set of cpu_ids to add to the cpuset for emulator threads
4335            with the following caveats:
4336 
4337             a) If emulator threads policy is isolated, we pin emulator threads
4338                to one cpu we have reserved for it.
4339             b) If emulator threads policy is shared and CONF.cpu_shared_set is
4340                defined, we pin emulator threads on the set of pCPUs defined by
4341                CONF.cpu_shared_set
4342             c) Otherwise;
4343                 c1) If realtime IS NOT enabled, the emulator threads are
4344                     allowed to float cross all the pCPUs associated with
4345                     the guest vCPUs.
4346                 c2) If realtime IS enabled, at least 1 vCPU is required
4347                     to be set aside for non-realtime usage. The emulator
4348                     threads are allowed to float across the pCPUs that
4349                     are associated with the non-realtime VCPUs.
4350         """
4351         emulatorpin_cpuset = set([])
4352         shared_ids = hardware.get_cpu_shared_set()
4353 
4354         if emulator_threads_policy == fields.CPUEmulatorThreadsPolicy.ISOLATE:
4355             if object_numa_cell.cpuset_reserved:
4356                 emulatorpin_cpuset = object_numa_cell.cpuset_reserved
4357         elif ((emulator_threads_policy ==
4358               fields.CPUEmulatorThreadsPolicy.SHARE) and
4359               shared_ids):
4360             online_pcpus = self._host.get_online_cpus()
4361             cpuset = shared_ids & online_pcpus
4362             if not cpuset:
4363                 msg = (_("Invalid cpu_shared_set config, one or more of the "
4364                          "specified cpuset is not online. Online cpuset(s): "
4365                          "%(online)s, requested cpuset(s): %(req)s"),
4366                        {'online': sorted(online_pcpus),
4367                         'req': sorted(shared_ids)})
4368                 raise exception.Invalid(msg)
4369             emulatorpin_cpuset = cpuset
4370         elif not wants_realtime or vcpu not in vcpus_rt:
4371             emulatorpin_cpuset = pin_cpuset.cpuset
4372 
4373         return emulatorpin_cpuset
4374 
4375     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4376                                allowed_cpus=None, image_meta=None):
4377         """Returns the config objects for the guest NUMA specs.
4378 
4379         Determines the CPUs that the guest can be pinned to if the guest
4380         specifies a cell topology and the host supports it. Constructs the
4381         libvirt XML config object representing the NUMA topology selected
4382         for the guest. Returns a tuple of:
4383 
4384             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4385 
4386         With the following caveats:
4387 
4388             a) If there is no specified guest NUMA topology, then
4389                all tuple elements except cpu_set shall be None. cpu_set
4390                will be populated with the chosen CPUs that the guest
4391                allowed CPUs fit within, which could be the supplied
4392                allowed_cpus value if the host doesn't support NUMA
4393                topologies.
4394 
4395             b) If there is a specified guest NUMA topology, then
4396                cpu_set will be None and guest_cpu_numa will be the
4397                LibvirtConfigGuestCPUNUMA object representing the guest's
4398                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4399                will contain a LibvirtConfigGuestCPUTune object representing
4400                the optimized chosen cells that match the host capabilities
4401                with the instance's requested topology. If the host does
4402                not support NUMA, then guest_cpu_tune and guest_numa_tune
4403                will be None.
4404         """
4405 
4406         if (not self._has_numa_support() and
4407                 instance_numa_topology is not None):
4408             # We should not get here, since we should have avoided
4409             # reporting NUMA topology from _get_host_numa_topology
4410             # in the first place. Just in case of a scheduler
4411             # mess up though, raise an exception
4412             raise exception.NUMATopologyUnsupported()
4413 
4414         topology = self._get_host_numa_topology()
4415 
4416         # We have instance NUMA so translate it to the config class
4417         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4418                 instance_numa_topology,
4419                 self._wants_hugepages(topology, instance_numa_topology))
4420 
4421         if not guest_cpu_numa_config:
4422             # No NUMA topology defined for instance - let the host kernel deal
4423             # with the NUMA effects.
4424             # TODO(ndipanov): Attempt to spread the instance
4425             # across NUMA nodes and expose the topology to the
4426             # instance as an optimisation
4427             return GuestNumaConfig(allowed_cpus, None, None, None)
4428 
4429         if not topology:
4430             # No NUMA topology defined for host - This will only happen with
4431             # some libvirt versions and certain platforms.
4432             return GuestNumaConfig(allowed_cpus, None,
4433                                    guest_cpu_numa_config, None)
4434 
4435         # Now get configuration from the numa_topology
4436         # Init CPUTune configuration
4437         guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4438         guest_cpu_tune.emulatorpin = (
4439             vconfig.LibvirtConfigGuestCPUTuneEmulatorPin())
4440         guest_cpu_tune.emulatorpin.cpuset = set([])
4441 
4442         # Init NUMATune configuration
4443         guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4444         guest_numa_tune.memory = vconfig.LibvirtConfigGuestNUMATuneMemory()
4445         guest_numa_tune.memnodes = []
4446 
4447         emulator_threads_policy = None
4448         if 'emulator_threads_policy' in instance_numa_topology:
4449             emulator_threads_policy = (
4450                 instance_numa_topology.emulator_threads_policy)
4451 
4452         # Set realtime scheduler for CPUTune
4453         vcpus_rt = set([])
4454         wants_realtime = hardware.is_realtime_enabled(flavor)
4455         if wants_realtime:
4456             vcpus_rt = hardware.vcpus_realtime_topology(flavor, image_meta)
4457             vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4458             designer.set_vcpu_realtime_scheduler(
4459                 vcpusched, vcpus_rt, CONF.libvirt.realtime_scheduler_priority)
4460             guest_cpu_tune.vcpusched.append(vcpusched)
4461 
4462         cell_pairs = self._get_cell_pairs(guest_cpu_numa_config, topology)
4463         for guest_node_id, (guest_config_cell, host_cell) in enumerate(
4464                 cell_pairs):
4465             # set NUMATune for the cell
4466             tnode = vconfig.LibvirtConfigGuestNUMATuneMemNode()
4467             designer.set_numa_memnode(tnode, guest_node_id, host_cell.id)
4468             guest_numa_tune.memnodes.append(tnode)
4469             guest_numa_tune.memory.nodeset.append(host_cell.id)
4470 
4471             # set CPUTune for the cell
4472             object_numa_cell = instance_numa_topology.cells[guest_node_id]
4473             for cpu in guest_config_cell.cpus:
4474                 pin_cpuset = self._get_pin_cpuset(cpu, object_numa_cell,
4475                                                   host_cell)
4476                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4477 
4478                 emu_pin_cpuset = self._get_emulatorpin_cpuset(
4479                     cpu, object_numa_cell, vcpus_rt,
4480                     emulator_threads_policy, wants_realtime, pin_cpuset)
4481                 guest_cpu_tune.emulatorpin.cpuset.update(emu_pin_cpuset)
4482 
4483         # TODO(berrange) When the guest has >1 NUMA node, it will
4484         # span multiple host NUMA nodes. By pinning emulator threads
4485         # to the union of all nodes, we guarantee there will be
4486         # cross-node memory access by the emulator threads when
4487         # responding to guest I/O operations. The only way to avoid
4488         # this would be to pin emulator threads to a single node and
4489         # tell the guest OS to only do I/O from one of its virtual
4490         # NUMA nodes. This is not even remotely practical.
4491         #
4492         # The long term solution is to make use of a new QEMU feature
4493         # called "I/O Threads" which will let us configure an explicit
4494         # I/O thread for each guest vCPU or guest NUMA node. It is
4495         # still TBD how to make use of this feature though, especially
4496         # how to associate IO threads with guest devices to eliminate
4497         # cross NUMA node traffic. This is an area of investigation
4498         # for QEMU community devs.
4499 
4500         # Sort the vcpupin list per vCPU id for human-friendlier XML
4501         guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4502 
4503         # normalize cell.id
4504         for i, (cell, memnode) in enumerate(zip(guest_cpu_numa_config.cells,
4505                                                 guest_numa_tune.memnodes)):
4506             cell.id = i
4507             memnode.cellid = i
4508 
4509         return GuestNumaConfig(None, guest_cpu_tune, guest_cpu_numa_config,
4510                                guest_numa_tune)
4511 
4512     def _get_guest_os_type(self, virt_type):
4513         """Returns the guest OS type based on virt type."""
4514         if virt_type == "lxc":
4515             ret = fields.VMMode.EXE
4516         elif virt_type == "uml":
4517             ret = fields.VMMode.UML
4518         elif virt_type == "xen":
4519             ret = fields.VMMode.XEN
4520         else:
4521             ret = fields.VMMode.HVM
4522         return ret
4523 
4524     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4525                               root_device_name):
4526         if rescue.get('kernel_id'):
4527             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4528             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4529             if virt_type == "qemu":
4530                 guest.os_cmdline += " no_timer_check"
4531         if rescue.get('ramdisk_id'):
4532             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4533 
4534     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4535                                 root_device_name, image_meta):
4536         guest.os_kernel = os.path.join(inst_path, "kernel")
4537         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4538         if virt_type == "qemu":
4539             guest.os_cmdline += " no_timer_check"
4540         if instance.ramdisk_id:
4541             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4542         # we only support os_command_line with images with an explicit
4543         # kernel set and don't want to break nova if there's an
4544         # os_command_line property without a specified kernel_id param
4545         if image_meta.properties.get("os_command_line"):
4546             guest.os_cmdline = image_meta.properties.os_command_line
4547 
4548     def _set_clock(self, guest, os_type, image_meta, virt_type):
4549         # NOTE(mikal): Microsoft Windows expects the clock to be in
4550         # "localtime". If the clock is set to UTC, then you can use a
4551         # registry key to let windows know, but Microsoft says this is
4552         # buggy in http://support.microsoft.com/kb/2687252
4553         clk = vconfig.LibvirtConfigGuestClock()
4554         if os_type == 'windows':
4555             LOG.info('Configuring timezone for windows instance to localtime')
4556             clk.offset = 'localtime'
4557         else:
4558             clk.offset = 'utc'
4559         guest.set_clock(clk)
4560 
4561         if virt_type == "kvm":
4562             self._set_kvm_timers(clk, os_type, image_meta)
4563 
4564     def _set_kvm_timers(self, clk, os_type, image_meta):
4565         # TODO(berrange) One day this should be per-guest
4566         # OS type configurable
4567         tmpit = vconfig.LibvirtConfigGuestTimer()
4568         tmpit.name = "pit"
4569         tmpit.tickpolicy = "delay"
4570 
4571         tmrtc = vconfig.LibvirtConfigGuestTimer()
4572         tmrtc.name = "rtc"
4573         tmrtc.tickpolicy = "catchup"
4574 
4575         clk.add_timer(tmpit)
4576         clk.add_timer(tmrtc)
4577 
4578         guestarch = libvirt_utils.get_arch(image_meta)
4579         if guestarch in (fields.Architecture.I686,
4580                          fields.Architecture.X86_64):
4581             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4582             # qemu -no-hpet is not supported on non-x86 targets.
4583             tmhpet = vconfig.LibvirtConfigGuestTimer()
4584             tmhpet.name = "hpet"
4585             tmhpet.present = False
4586             clk.add_timer(tmhpet)
4587 
4588         # Provide Windows guests with the paravirtualized hyperv timer source.
4589         # This is the windows equiv of kvm-clock, allowing Windows
4590         # guests to accurately keep time.
4591         if os_type == 'windows':
4592             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4593             tmhyperv.name = "hypervclock"
4594             tmhyperv.present = True
4595             clk.add_timer(tmhyperv)
4596 
4597     def _set_features(self, guest, os_type, caps, virt_type, image_meta,
4598             flavor):
4599         if virt_type == "xen":
4600             # PAE only makes sense in X86
4601             if caps.host.cpu.arch in (fields.Architecture.I686,
4602                                       fields.Architecture.X86_64):
4603                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4604 
4605         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4606                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4607             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4608             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4609 
4610         if (virt_type in ("qemu", "kvm") and
4611                 os_type == 'windows'):
4612             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4613             hv.relaxed = True
4614 
4615             hv.spinlocks = True
4616             # Increase spinlock retries - value recommended by
4617             # KVM maintainers who certify Windows guests
4618             # with Microsoft
4619             hv.spinlock_retries = 8191
4620             hv.vapic = True
4621             guest.features.append(hv)
4622 
4623         flavor_hide_kvm = strutils.bool_from_string(
4624                 flavor.get('extra_specs', {}).get('hide_hypervisor_id'))
4625         if (virt_type in ("qemu", "kvm") and
4626                 (image_meta.properties.get('img_hide_hypervisor_id') or
4627                  flavor_hide_kvm)):
4628             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4629 
4630     def _check_number_of_serial_console(self, num_ports):
4631         virt_type = CONF.libvirt.virt_type
4632         if (virt_type in ("kvm", "qemu") and
4633             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4634             raise exception.SerialPortNumberLimitExceeded(
4635                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4636 
4637     def _add_video_driver(self, guest, image_meta, flavor):
4638         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4639                                "xen", "qxl", "virtio")
4640         video = vconfig.LibvirtConfigGuestVideo()
4641         # NOTE(ldbragst): The following logic sets the video.type
4642         # depending on supported defaults given the architecture,
4643         # virtualization type, and features. The video.type attribute can
4644         # be overridden by the user with image_meta.properties, which
4645         # is carried out in the next if statement below this one.
4646         guestarch = libvirt_utils.get_arch(image_meta)
4647         if guest.os_type == fields.VMMode.XEN:
4648             video.type = 'xen'
4649         elif CONF.libvirt.virt_type == 'parallels':
4650             video.type = 'vga'
4651         elif guestarch in (fields.Architecture.PPC,
4652                            fields.Architecture.PPC64,
4653                            fields.Architecture.PPC64LE):
4654             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4655             # so use 'vga' instead when running on Power hardware.
4656             video.type = 'vga'
4657         elif guestarch in (fields.Architecture.AARCH64):
4658             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4659             # so use 'virtio' instead when running on AArch64 hardware.
4660             video.type = 'virtio'
4661         elif CONF.spice.enabled:
4662             video.type = 'qxl'
4663         if image_meta.properties.get('hw_video_model'):
4664             video.type = image_meta.properties.hw_video_model
4665             if (video.type not in VALID_VIDEO_DEVICES):
4666                 raise exception.InvalidVideoMode(model=video.type)
4667 
4668         # Set video memory, only if the flavor's limit is set
4669         video_ram = image_meta.properties.get('hw_video_ram', 0)
4670         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4671         if video_ram > max_vram:
4672             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4673                                                  max_vram=max_vram)
4674         if max_vram and video_ram:
4675             video.vram = video_ram * units.Mi / units.Ki
4676         guest.add_device(video)
4677 
4678     def _add_qga_device(self, guest, instance):
4679         qga = vconfig.LibvirtConfigGuestChannel()
4680         qga.type = "unix"
4681         qga.target_name = "org.qemu.guest_agent.0"
4682         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4683                           ("org.qemu.guest_agent.0", instance.name))
4684         guest.add_device(qga)
4685 
4686     def _add_rng_device(self, guest, flavor):
4687         rng_device = vconfig.LibvirtConfigGuestRng()
4688         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4689         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4690         if rate_bytes:
4691             rng_device.rate_bytes = int(rate_bytes)
4692             rng_device.rate_period = int(period)
4693         rng_path = CONF.libvirt.rng_dev_path
4694         if (rng_path and not os.path.exists(rng_path)):
4695             raise exception.RngDeviceNotExist(path=rng_path)
4696         rng_device.backend = rng_path
4697         guest.add_device(rng_device)
4698 
4699     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4700         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4701         if image_meta.properties.get('hw_qemu_guest_agent', False):
4702             LOG.debug("Qemu guest agent is enabled through image "
4703                       "metadata", instance=instance)
4704             self._add_qga_device(guest, instance)
4705         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4706         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4707         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4708         if rng_is_virtio and rng_allowed:
4709             self._add_rng_device(guest, flavor)
4710 
4711     def _get_guest_memory_backing_config(
4712             self, inst_topology, numatune, flavor):
4713         wantsmempages = False
4714         if inst_topology:
4715             for cell in inst_topology.cells:
4716                 if cell.pagesize:
4717                     wantsmempages = True
4718                     break
4719 
4720         wantsrealtime = hardware.is_realtime_enabled(flavor)
4721 
4722         wantsfilebacked = CONF.libvirt.file_backed_memory > 0
4723 
4724         if wantsmempages and wantsfilebacked:
4725             # Can't use file-backed memory with hugepages
4726             LOG.warning("Instance requested huge pages, but file-backed "
4727                     "memory is enabled, and incompatible with huge pages")
4728             raise exception.MemoryPagesUnsupported()
4729 
4730         membacking = None
4731         if wantsmempages:
4732             pages = self._get_memory_backing_hugepages_support(
4733                 inst_topology, numatune)
4734             if pages:
4735                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4736                 membacking.hugepages = pages
4737         if wantsrealtime:
4738             if not membacking:
4739                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4740             membacking.locked = True
4741             membacking.sharedpages = False
4742         if wantsfilebacked:
4743             if not membacking:
4744                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4745             membacking.filesource = True
4746             membacking.sharedaccess = True
4747             membacking.allocateimmediate = True
4748             if self._host.has_min_version(
4749                     MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
4750                     MIN_QEMU_FILE_BACKED_DISCARD_VERSION):
4751                 membacking.discard = True
4752 
4753         return membacking
4754 
4755     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4756         if not self._has_numa_support():
4757             # We should not get here, since we should have avoided
4758             # reporting NUMA topology from _get_host_numa_topology
4759             # in the first place. Just in case of a scheduler
4760             # mess up though, raise an exception
4761             raise exception.MemoryPagesUnsupported()
4762 
4763         host_topology = self._get_host_numa_topology()
4764 
4765         if host_topology is None:
4766             # As above, we should not get here but just in case...
4767             raise exception.MemoryPagesUnsupported()
4768 
4769         # Currently libvirt does not support the smallest
4770         # pagesize set as a backend memory.
4771         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4772         avail_pagesize = [page.size_kb
4773                           for page in host_topology.cells[0].mempages]
4774         avail_pagesize.sort()
4775         smallest = avail_pagesize[0]
4776 
4777         pages = []
4778         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4779             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4780                 for memnode in numatune.memnodes:
4781                     if guest_cellid == memnode.cellid:
4782                         page = (
4783                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4784                         page.nodeset = [guest_cellid]
4785                         page.size_kb = inst_cell.pagesize
4786                         pages.append(page)
4787                         break  # Quit early...
4788         return pages
4789 
4790     def _get_flavor(self, ctxt, instance, flavor):
4791         if flavor is not None:
4792             return flavor
4793         return instance.flavor
4794 
4795     def _has_uefi_support(self):
4796         # This means that the host can support uefi booting for guests
4797         supported_archs = [fields.Architecture.X86_64,
4798                            fields.Architecture.AARCH64]
4799         caps = self._host.get_capabilities()
4800         return ((caps.host.cpu.arch in supported_archs) and
4801                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4802 
4803     def _get_supported_perf_events(self):
4804 
4805         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4806              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4807             return []
4808 
4809         supported_events = []
4810         host_cpu_info = self._get_cpu_info()
4811         for event in CONF.libvirt.enabled_perf_events:
4812             if self._supported_perf_event(event, host_cpu_info['features']):
4813                 supported_events.append(event)
4814         return supported_events
4815 
4816     def _supported_perf_event(self, event, cpu_features):
4817 
4818         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4819 
4820         if not hasattr(libvirt, libvirt_perf_event_name):
4821             LOG.warning("Libvirt doesn't support event type %s.", event)
4822             return False
4823 
4824         if event in PERF_EVENTS_CPU_FLAG_MAPPING:
4825             LOG.warning('Monitoring Intel CMT `perf` event(s) %s is '
4826                         'deprecated and will be removed in the "Stein" '
4827                         'release.  It was broken by design in the '
4828                         'Linux kernel, so support for Intel CMT was '
4829                         'removed from Linux 4.14 onwards. Therefore '
4830                         'it is recommended to not enable them.',
4831                         event)
4832             if PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features:
4833                 LOG.warning("Host does not support event type %s.", event)
4834                 return False
4835         return True
4836 
4837     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4838                                       image_meta, flavor, root_device_name):
4839         if virt_type == "xen":
4840             if guest.os_type == fields.VMMode.HVM:
4841                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4842             else:
4843                 guest.os_cmdline = CONSOLE
4844         elif virt_type in ("kvm", "qemu"):
4845             if caps.host.cpu.arch in (fields.Architecture.I686,
4846                                       fields.Architecture.X86_64):
4847                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4848                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4849             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4850             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4851                 if not hw_firmware_type:
4852                     hw_firmware_type = fields.FirmwareType.UEFI
4853             if hw_firmware_type == fields.FirmwareType.UEFI:
4854                 if self._has_uefi_support():
4855                     global uefi_logged
4856                     if not uefi_logged:
4857                         LOG.warning("uefi support is without some kind of "
4858                                     "functional testing and therefore "
4859                                     "considered experimental.")
4860                         uefi_logged = True
4861                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4862                         caps.host.cpu.arch]
4863                     guest.os_loader_type = "pflash"
4864                 else:
4865                     raise exception.UEFINotSupported()
4866             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4867             if image_meta.properties.get('hw_boot_menu') is None:
4868                 guest.os_bootmenu = strutils.bool_from_string(
4869                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4870             else:
4871                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4872 
4873         elif virt_type == "lxc":
4874             guest.os_init_path = "/sbin/init"
4875             guest.os_cmdline = CONSOLE
4876         elif virt_type == "uml":
4877             guest.os_kernel = "/usr/bin/linux"
4878             guest.os_root = root_device_name
4879         elif virt_type == "parallels":
4880             if guest.os_type == fields.VMMode.EXE:
4881                 guest.os_init_path = "/sbin/init"
4882 
4883     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4884                     instance, inst_path, image_meta, disk_info):
4885         if rescue:
4886             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4887                                        root_device_name)
4888         elif instance.kernel_id:
4889             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4890                                             virt_type, root_device_name,
4891                                             image_meta)
4892         else:
4893             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4894 
4895     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4896                          image_meta):
4897         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4898         # easy to lose track. Use this chart to figure out your case:
4899         #
4900         # case | is serial | has       | is qemu | resulting
4901         #      | enabled?  | virtlogd? | or kvm? | devices
4902         # --------------------------------------------------
4903         #    1 |        no |        no |     no  | pty*
4904         #    2 |        no |        no |     yes | file + pty
4905         #    3 |        no |       yes |      no | see case 1
4906         #    4 |        no |       yes |     yes | pty with logd
4907         #    5 |       yes |        no |      no | see case 1
4908         #    6 |       yes |        no |     yes | tcp + pty
4909         #    7 |       yes |       yes |      no | see case 1
4910         #    8 |       yes |       yes |     yes | tcp with logd
4911         #    * exception: virt_type "parallels" doesn't create a device
4912         if virt_type == 'parallels':
4913             pass
4914         elif virt_type not in ("qemu", "kvm"):
4915             log_path = self._get_console_log_path(instance)
4916             self._create_pty_device(guest_cfg,
4917                                     vconfig.LibvirtConfigGuestConsole,
4918                                     log_path=log_path)
4919         elif (virt_type in ("qemu", "kvm") and
4920                   self._is_s390x_guest(image_meta)):
4921             self._create_consoles_s390x(guest_cfg, instance,
4922                                         flavor, image_meta)
4923         elif virt_type in ("qemu", "kvm"):
4924             self._create_consoles_qemu_kvm(guest_cfg, instance,
4925                                         flavor, image_meta)
4926 
4927     def _is_s390x_guest(self, image_meta):
4928         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4929         return libvirt_utils.get_arch(image_meta) in s390x_archs
4930 
4931     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4932                                   image_meta):
4933         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4934         log_path = self._get_console_log_path(instance)
4935         if CONF.serial_console.enabled:
4936             if not self._serial_ports_already_defined(instance):
4937                 num_ports = hardware.get_number_of_serial_ports(flavor,
4938                                                                 image_meta)
4939                 self._check_number_of_serial_console(num_ports)
4940                 self._create_serial_consoles(guest_cfg, num_ports,
4941                                              char_dev_cls, log_path)
4942         else:
4943             self._create_file_device(guest_cfg, instance, char_dev_cls)
4944         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4945 
4946     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4947         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4948         log_path = self._get_console_log_path(instance)
4949         if CONF.serial_console.enabled:
4950             if not self._serial_ports_already_defined(instance):
4951                 num_ports = hardware.get_number_of_serial_ports(flavor,
4952                                                                 image_meta)
4953                 self._create_serial_consoles(guest_cfg, num_ports,
4954                                              char_dev_cls, log_path)
4955         else:
4956             self._create_file_device(guest_cfg, instance, char_dev_cls,
4957                                      "sclplm")
4958         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4959 
4960     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4961                            log_path=None):
4962         def _create_base_dev():
4963             consolepty = char_dev_cls()
4964             consolepty.target_type = target_type
4965             consolepty.type = "pty"
4966             return consolepty
4967 
4968         def _create_logd_dev():
4969             consolepty = _create_base_dev()
4970             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4971             log.file = log_path
4972             consolepty.log = log
4973             return consolepty
4974 
4975         if CONF.serial_console.enabled:
4976             if self._is_virtlogd_available():
4977                 return
4978             else:
4979                 # NOTE(markus_z): You may wonder why this is necessary and
4980                 # so do I. I'm certain that this is *not* needed in any
4981                 # real use case. It is, however, useful if you want to
4982                 # pypass the Nova API and use "virsh console <guest>" on
4983                 # an hypervisor, as this CLI command doesn't work with TCP
4984                 # devices (like the serial console is).
4985                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4986                 # Pypassing the Nova API however is a thing we don't want.
4987                 # Future changes should remove this and fix the unit tests
4988                 # which ask for the existence.
4989                 guest_cfg.add_device(_create_base_dev())
4990         else:
4991             if self._is_virtlogd_available():
4992                 guest_cfg.add_device(_create_logd_dev())
4993             else:
4994                 guest_cfg.add_device(_create_base_dev())
4995 
4996     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4997                             target_type=None):
4998         if self._is_virtlogd_available():
4999             return
5000 
5001         consolelog = char_dev_cls()
5002         consolelog.target_type = target_type
5003         consolelog.type = "file"
5004         consolelog.source_path = self._get_console_log_path(instance)
5005         guest_cfg.add_device(consolelog)
5006 
5007     def _serial_ports_already_defined(self, instance):
5008         try:
5009             guest = self._host.get_guest(instance)
5010             if list(self._get_serial_ports_from_guest(guest)):
5011                 # Serial port are already configured for instance that
5012                 # means we are in a context of migration.
5013                 return True
5014         except exception.InstanceNotFound:
5015             LOG.debug(
5016                 "Instance does not exist yet on libvirt, we can "
5017                 "safely pass on looking for already defined serial "
5018                 "ports in its domain XML", instance=instance)
5019         return False
5020 
5021     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
5022                                 log_path):
5023         for port in six.moves.range(num_ports):
5024             console = char_dev_cls()
5025             console.port = port
5026             console.type = "tcp"
5027             console.listen_host = CONF.serial_console.proxyclient_address
5028             listen_port = serial_console.acquire_port(console.listen_host)
5029             console.listen_port = listen_port
5030             # NOTE: only the first serial console gets the boot messages,
5031             # that's why we attach the logd subdevice only to that.
5032             if port == 0 and self._is_virtlogd_available():
5033                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
5034                 log.file = log_path
5035                 console.log = log
5036             guest_cfg.add_device(console)
5037 
5038     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
5039         """Update VirtCPUModel object according to libvirt CPU config.
5040 
5041         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
5042                            instance's virtual cpu configuration.
5043         :param:vcpu_model: VirtCPUModel object. A new object will be created
5044                            if None.
5045 
5046         :return: Updated VirtCPUModel object, or None if cpu_config is None
5047 
5048         """
5049 
5050         if not cpu_config:
5051             return
5052         if not vcpu_model:
5053             vcpu_model = objects.VirtCPUModel()
5054 
5055         vcpu_model.arch = cpu_config.arch
5056         vcpu_model.vendor = cpu_config.vendor
5057         vcpu_model.model = cpu_config.model
5058         vcpu_model.mode = cpu_config.mode
5059         vcpu_model.match = cpu_config.match
5060 
5061         if cpu_config.sockets:
5062             vcpu_model.topology = objects.VirtCPUTopology(
5063                 sockets=cpu_config.sockets,
5064                 cores=cpu_config.cores,
5065                 threads=cpu_config.threads)
5066         else:
5067             vcpu_model.topology = None
5068 
5069         features = [objects.VirtCPUFeature(
5070             name=f.name,
5071             policy=f.policy) for f in cpu_config.features]
5072         vcpu_model.features = features
5073 
5074         return vcpu_model
5075 
5076     def _vcpu_model_to_cpu_config(self, vcpu_model):
5077         """Create libvirt CPU config according to VirtCPUModel object.
5078 
5079         :param:vcpu_model: VirtCPUModel object.
5080 
5081         :return: vconfig.LibvirtConfigGuestCPU.
5082 
5083         """
5084 
5085         cpu_config = vconfig.LibvirtConfigGuestCPU()
5086         cpu_config.arch = vcpu_model.arch
5087         cpu_config.model = vcpu_model.model
5088         cpu_config.mode = vcpu_model.mode
5089         cpu_config.match = vcpu_model.match
5090         cpu_config.vendor = vcpu_model.vendor
5091         if vcpu_model.topology:
5092             cpu_config.sockets = vcpu_model.topology.sockets
5093             cpu_config.cores = vcpu_model.topology.cores
5094             cpu_config.threads = vcpu_model.topology.threads
5095         if vcpu_model.features:
5096             for f in vcpu_model.features:
5097                 xf = vconfig.LibvirtConfigGuestCPUFeature()
5098                 xf.name = f.name
5099                 xf.policy = f.policy
5100                 cpu_config.features.add(xf)
5101         return cpu_config
5102 
5103     def _guest_add_pcie_root_ports(self, guest):
5104         """Add PCI Express root ports.
5105 
5106         PCI Express machine can have as many PCIe devices as it has
5107         pcie-root-port controllers (slots in virtual motherboard).
5108 
5109         If we want to have more PCIe slots for hotplug then we need to create
5110         whole PCIe structure (libvirt limitation).
5111         """
5112 
5113         pcieroot = vconfig.LibvirtConfigGuestPCIeRootController()
5114         guest.add_device(pcieroot)
5115 
5116         for x in range(0, CONF.libvirt.num_pcie_ports):
5117             pcierootport = vconfig.LibvirtConfigGuestPCIeRootPortController()
5118             guest.add_device(pcierootport)
5119 
5120     def _guest_add_usb_host_keyboard(self, guest):
5121         """Add USB Host controller and keyboard for graphical console use.
5122 
5123         Add USB keyboard as PS/2 support may not be present on non-x86
5124         architectures.
5125         """
5126         keyboard = vconfig.LibvirtConfigGuestInput()
5127         keyboard.type = "keyboard"
5128         keyboard.bus = "usb"
5129         guest.add_device(keyboard)
5130 
5131         usbhost = vconfig.LibvirtConfigGuestUSBHostController()
5132         usbhost.index = 0
5133         guest.add_device(usbhost)
5134 
5135     def _get_guest_config(self, instance, network_info, image_meta,
5136                           disk_info, rescue=None, block_device_info=None,
5137                           context=None, mdevs=None):
5138         """Get config data for parameters.
5139 
5140         :param rescue: optional dictionary that should contain the key
5141             'ramdisk_id' if a ramdisk is needed for the rescue image and
5142             'kernel_id' if a kernel is needed for the rescue image.
5143 
5144         :param mdevs: optional list of mediated devices to assign to the guest.
5145         """
5146         flavor = instance.flavor
5147         inst_path = libvirt_utils.get_instance_path(instance)
5148         disk_mapping = disk_info['mapping']
5149 
5150         virt_type = CONF.libvirt.virt_type
5151         guest = vconfig.LibvirtConfigGuest()
5152         guest.virt_type = virt_type
5153         guest.name = instance.name
5154         guest.uuid = instance.uuid
5155         # We are using default unit for memory: KiB
5156         guest.memory = flavor.memory_mb * units.Ki
5157         guest.vcpus = flavor.vcpus
5158         allowed_cpus = hardware.get_vcpu_pin_set()
5159 
5160         guest_numa_config = self._get_guest_numa_config(
5161             instance.numa_topology, flavor, allowed_cpus, image_meta)
5162 
5163         guest.cpuset = guest_numa_config.cpuset
5164         guest.cputune = guest_numa_config.cputune
5165         guest.numatune = guest_numa_config.numatune
5166 
5167         guest.membacking = self._get_guest_memory_backing_config(
5168             instance.numa_topology,
5169             guest_numa_config.numatune,
5170             flavor)
5171 
5172         guest.metadata.append(self._get_guest_config_meta(instance))
5173         guest.idmaps = self._get_guest_idmaps()
5174 
5175         for event in self._supported_perf_events:
5176             guest.add_perf_event(event)
5177 
5178         self._update_guest_cputune(guest, flavor, virt_type)
5179 
5180         guest.cpu = self._get_guest_cpu_config(
5181             flavor, image_meta, guest_numa_config.numaconfig,
5182             instance.numa_topology)
5183 
5184         # Notes(yjiang5): we always sync the instance's vcpu model with
5185         # the corresponding config file.
5186         instance.vcpu_model = self._cpu_config_to_vcpu_model(
5187             guest.cpu, instance.vcpu_model)
5188 
5189         if 'root' in disk_mapping:
5190             root_device_name = block_device.prepend_dev(
5191                 disk_mapping['root']['dev'])
5192         else:
5193             root_device_name = None
5194 
5195         if root_device_name:
5196             instance.root_device_name = root_device_name
5197 
5198         guest.os_type = (fields.VMMode.get_from_instance(instance) or
5199                 self._get_guest_os_type(virt_type))
5200         caps = self._host.get_capabilities()
5201 
5202         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
5203                                            image_meta, flavor,
5204                                            root_device_name)
5205         if virt_type not in ('lxc', 'uml'):
5206             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
5207                     instance, inst_path, image_meta, disk_info)
5208 
5209         self._set_features(guest, instance.os_type, caps, virt_type,
5210                            image_meta, flavor)
5211         self._set_clock(guest, instance.os_type, image_meta, virt_type)
5212 
5213         storage_configs = self._get_guest_storage_config(context,
5214                 instance, image_meta, disk_info, rescue, block_device_info,
5215                 flavor, guest.os_type)
5216         for config in storage_configs:
5217             guest.add_device(config)
5218 
5219         for vif in network_info:
5220             config = self.vif_driver.get_config(
5221                 instance, vif, image_meta,
5222                 flavor, virt_type, self._host)
5223             guest.add_device(config)
5224 
5225         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
5226 
5227         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
5228         if pointer:
5229             guest.add_device(pointer)
5230 
5231         self._guest_add_spice_channel(guest)
5232 
5233         if self._guest_add_video_device(guest):
5234             self._add_video_driver(guest, image_meta, flavor)
5235 
5236             # We want video == we want graphical console. Some architectures
5237             # do not have input devices attached in default configuration.
5238             # Let then add USB Host controller and USB keyboard.
5239             # x86(-64) and ppc64 have usb host controller and keyboard
5240             # s390x does not support USB
5241             if caps.host.cpu.arch == fields.Architecture.AARCH64:
5242                 self._guest_add_usb_host_keyboard(guest)
5243 
5244         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
5245         if virt_type in ('qemu', 'kvm'):
5246             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
5247 
5248         # Add PCIe root port controllers for PCI Express machines
5249         # but only if their amount is configured
5250         if (CONF.libvirt.num_pcie_ports and
5251                 ((caps.host.cpu.arch == fields.Architecture.AARCH64 and
5252                 guest.os_mach_type.startswith('virt')) or
5253                 (caps.host.cpu.arch == fields.Architecture.X86_64 and
5254                 guest.os_mach_type is not None and
5255                 'q35' in guest.os_mach_type))):
5256             self._guest_add_pcie_root_ports(guest)
5257 
5258         self._guest_add_pci_devices(guest, instance)
5259 
5260         self._guest_add_watchdog_action(guest, flavor, image_meta)
5261 
5262         self._guest_add_memory_balloon(guest)
5263 
5264         if mdevs:
5265             self._guest_add_mdevs(guest, mdevs)
5266 
5267         return guest
5268 
5269     def _guest_add_mdevs(self, guest, chosen_mdevs):
5270         for chosen_mdev in chosen_mdevs:
5271             mdev = vconfig.LibvirtConfigGuestHostdevMDEV()
5272             mdev.uuid = chosen_mdev
5273             guest.add_device(mdev)
5274 
5275     @staticmethod
5276     def _guest_add_spice_channel(guest):
5277         if (CONF.spice.enabled and CONF.spice.agent_enabled
5278                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
5279             channel = vconfig.LibvirtConfigGuestChannel()
5280             channel.type = 'spicevmc'
5281             channel.target_name = "com.redhat.spice.0"
5282             guest.add_device(channel)
5283 
5284     @staticmethod
5285     def _guest_add_memory_balloon(guest):
5286         virt_type = guest.virt_type
5287         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
5288         if (virt_type in ('xen', 'qemu', 'kvm') and
5289                     CONF.libvirt.mem_stats_period_seconds > 0):
5290             balloon = vconfig.LibvirtConfigMemoryBalloon()
5291             if virt_type in ('qemu', 'kvm'):
5292                 balloon.model = 'virtio'
5293             else:
5294                 balloon.model = 'xen'
5295             balloon.period = CONF.libvirt.mem_stats_period_seconds
5296             guest.add_device(balloon)
5297 
5298     @staticmethod
5299     def _guest_add_watchdog_action(guest, flavor, image_meta):
5300         # image meta takes precedence over flavor extra specs; disable the
5301         # watchdog action by default
5302         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
5303                            or 'disabled')
5304         watchdog_action = image_meta.properties.get('hw_watchdog_action',
5305                                                     watchdog_action)
5306         # NB(sross): currently only actually supported by KVM/QEmu
5307         if watchdog_action != 'disabled':
5308             if watchdog_action in fields.WatchdogAction.ALL:
5309                 bark = vconfig.LibvirtConfigGuestWatchdog()
5310                 bark.action = watchdog_action
5311                 guest.add_device(bark)
5312             else:
5313                 raise exception.InvalidWatchdogAction(action=watchdog_action)
5314 
5315     def _guest_add_pci_devices(self, guest, instance):
5316         virt_type = guest.virt_type
5317         if virt_type in ('xen', 'qemu', 'kvm'):
5318             # Get all generic PCI devices (non-SR-IOV).
5319             for pci_dev in pci_manager.get_instance_pci_devs(instance):
5320                 guest.add_device(self._get_guest_pci_device(pci_dev))
5321         else:
5322             # PCI devices is only supported for hypervisors
5323             #  'xen', 'qemu' and 'kvm'.
5324             if pci_manager.get_instance_pci_devs(instance, 'all'):
5325                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
5326 
5327     @staticmethod
5328     def _guest_add_video_device(guest):
5329         # NB some versions of libvirt support both SPICE and VNC
5330         # at the same time. We're not trying to second guess which
5331         # those versions are. We'll just let libvirt report the
5332         # errors appropriately if the user enables both.
5333         add_video_driver = False
5334         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
5335             graphics = vconfig.LibvirtConfigGuestGraphics()
5336             graphics.type = "vnc"
5337             if CONF.vnc.keymap:
5338                 graphics.keymap = CONF.vnc.keymap
5339             graphics.listen = CONF.vnc.server_listen
5340             guest.add_device(graphics)
5341             add_video_driver = True
5342         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
5343             graphics = vconfig.LibvirtConfigGuestGraphics()
5344             graphics.type = "spice"
5345             if CONF.spice.keymap:
5346                 graphics.keymap = CONF.spice.keymap
5347             graphics.listen = CONF.spice.server_listen
5348             guest.add_device(graphics)
5349             add_video_driver = True
5350         return add_video_driver
5351 
5352     def _get_guest_pointer_model(self, os_type, image_meta):
5353         pointer_model = image_meta.properties.get(
5354             'hw_pointer_model', CONF.pointer_model)
5355         if pointer_model is None and CONF.libvirt.use_usb_tablet:
5356             # TODO(sahid): We set pointer_model to keep compatibility
5357             # until the next release O*. It means operators can continue
5358             # to use the deprecated option "use_usb_tablet" or set a
5359             # specific device to use
5360             pointer_model = "usbtablet"
5361             LOG.warning('The option "use_usb_tablet" has been '
5362                         'deprecated for Newton in favor of the more '
5363                         'generic "pointer_model". Please update '
5364                         'nova.conf to address this change.')
5365 
5366         if pointer_model == "usbtablet":
5367             # We want a tablet if VNC is enabled, or SPICE is enabled and
5368             # the SPICE agent is disabled. If the SPICE agent is enabled
5369             # it provides a paravirt mouse which drastically reduces
5370             # overhead (by eliminating USB polling).
5371             if CONF.vnc.enabled or (
5372                     CONF.spice.enabled and not CONF.spice.agent_enabled):
5373                 return self._get_guest_usb_tablet(os_type)
5374             else:
5375                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5376                     # For backward compatibility We don't want to break
5377                     # process of booting an instance if host is configured
5378                     # to use USB tablet without VNC or SPICE and SPICE
5379                     # agent disable.
5380                     LOG.warning('USB tablet requested for guests by host '
5381                                 'configuration. In order to accept this '
5382                                 'request VNC should be enabled or SPICE '
5383                                 'and SPICE agent disabled on host.')
5384                 else:
5385                     raise exception.UnsupportedPointerModelRequested(
5386                         model="usbtablet")
5387 
5388     def _get_guest_usb_tablet(self, os_type):
5389         tablet = None
5390         if os_type == fields.VMMode.HVM:
5391             tablet = vconfig.LibvirtConfigGuestInput()
5392             tablet.type = "tablet"
5393             tablet.bus = "usb"
5394         else:
5395             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5396                 # For backward compatibility We don't want to break
5397                 # process of booting an instance if virtual machine mode
5398                 # is not configured as HVM.
5399                 LOG.warning('USB tablet requested for guests by host '
5400                             'configuration. In order to accept this '
5401                             'request the machine mode should be '
5402                             'configured as HVM.')
5403             else:
5404                 raise exception.UnsupportedPointerModelRequested(
5405                     model="usbtablet")
5406         return tablet
5407 
5408     def _get_guest_xml(self, context, instance, network_info, disk_info,
5409                        image_meta, rescue=None,
5410                        block_device_info=None,
5411                        mdevs=None):
5412         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5413         # this ahead of time so that we don't acquire it while also
5414         # holding the logging lock.
5415         network_info_str = str(network_info)
5416         msg = ('Start _get_guest_xml '
5417                'network_info=%(network_info)s '
5418                'disk_info=%(disk_info)s '
5419                'image_meta=%(image_meta)s rescue=%(rescue)s '
5420                'block_device_info=%(block_device_info)s' %
5421                {'network_info': network_info_str, 'disk_info': disk_info,
5422                 'image_meta': image_meta, 'rescue': rescue,
5423                 'block_device_info': block_device_info})
5424         # NOTE(mriedem): block_device_info can contain auth_password so we
5425         # need to sanitize the password in the message.
5426         LOG.debug(strutils.mask_password(msg), instance=instance)
5427         conf = self._get_guest_config(instance, network_info, image_meta,
5428                                       disk_info, rescue, block_device_info,
5429                                       context, mdevs)
5430         xml = conf.to_xml()
5431 
5432         LOG.debug('End _get_guest_xml xml=%(xml)s',
5433                   {'xml': xml}, instance=instance)
5434         return xml
5435 
5436     def get_info(self, instance):
5437         """Retrieve information from libvirt for a specific instance.
5438 
5439         If a libvirt error is encountered during lookup, we might raise a
5440         NotFound exception or Error exception depending on how severe the
5441         libvirt error is.
5442 
5443         :param instance: nova.objects.instance.Instance object
5444         :returns: An InstanceInfo object
5445         """
5446         guest = self._host.get_guest(instance)
5447         # Kind of ugly but we need to pass host to get_info as for a
5448         # workaround, see libvirt/compat.py
5449         return guest.get_info(self._host)
5450 
5451     def _create_domain_setup_lxc(self, context, instance, image_meta,
5452                                  block_device_info):
5453         inst_path = libvirt_utils.get_instance_path(instance)
5454         block_device_mapping = driver.block_device_info_get_mapping(
5455             block_device_info)
5456         root_disk = block_device.get_root_bdm(block_device_mapping)
5457         if root_disk:
5458             self._connect_volume(context, root_disk['connection_info'],
5459                                  instance)
5460             disk_path = root_disk['connection_info']['data']['device_path']
5461 
5462             # NOTE(apmelton) - Even though the instance is being booted from a
5463             # cinder volume, it is still presented as a local block device.
5464             # LocalBlockImage is used here to indicate that the instance's
5465             # disk is backed by a local block device.
5466             image_model = imgmodel.LocalBlockImage(disk_path)
5467         else:
5468             root_disk = self.image_backend.by_name(instance, 'disk')
5469             image_model = root_disk.get_model(self._conn)
5470 
5471         container_dir = os.path.join(inst_path, 'rootfs')
5472         fileutils.ensure_tree(container_dir)
5473         rootfs_dev = disk_api.setup_container(image_model,
5474                                               container_dir=container_dir)
5475 
5476         try:
5477             # Save rootfs device to disconnect it when deleting the instance
5478             if rootfs_dev:
5479                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5480             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5481                 id_maps = self._get_guest_idmaps()
5482                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5483         except Exception:
5484             with excutils.save_and_reraise_exception():
5485                 self._create_domain_cleanup_lxc(instance)
5486 
5487     def _create_domain_cleanup_lxc(self, instance):
5488         inst_path = libvirt_utils.get_instance_path(instance)
5489         container_dir = os.path.join(inst_path, 'rootfs')
5490 
5491         try:
5492             state = self.get_info(instance).state
5493         except exception.InstanceNotFound:
5494             # The domain may not be present if the instance failed to start
5495             state = None
5496 
5497         if state == power_state.RUNNING:
5498             # NOTE(uni): Now the container is running with its own private
5499             # mount namespace and so there is no need to keep the container
5500             # rootfs mounted in the host namespace
5501             LOG.debug('Attempting to unmount container filesystem: %s',
5502                       container_dir, instance=instance)
5503             disk_api.clean_lxc_namespace(container_dir=container_dir)
5504         else:
5505             disk_api.teardown_container(container_dir=container_dir)
5506 
5507     @contextlib.contextmanager
5508     def _lxc_disk_handler(self, context, instance, image_meta,
5509                           block_device_info):
5510         """Context manager to handle the pre and post instance boot,
5511            LXC specific disk operations.
5512 
5513            An image or a volume path will be prepared and setup to be
5514            used by the container, prior to starting it.
5515            The disk will be disconnected and unmounted if a container has
5516            failed to start.
5517         """
5518 
5519         if CONF.libvirt.virt_type != 'lxc':
5520             yield
5521             return
5522 
5523         self._create_domain_setup_lxc(context, instance, image_meta,
5524                                       block_device_info)
5525 
5526         try:
5527             yield
5528         finally:
5529             self._create_domain_cleanup_lxc(instance)
5530 
5531     # TODO(sahid): Consider renaming this to _create_guest.
5532     def _create_domain(self, xml=None, domain=None,
5533                        power_on=True, pause=False, post_xml_callback=None):
5534         """Create a domain.
5535 
5536         Either domain or xml must be passed in. If both are passed, then
5537         the domain definition is overwritten from the xml.
5538 
5539         :returns guest.Guest: Guest just created
5540         """
5541         if xml:
5542             guest = libvirt_guest.Guest.create(xml, self._host)
5543             if post_xml_callback is not None:
5544                 post_xml_callback()
5545         else:
5546             guest = libvirt_guest.Guest(domain)
5547 
5548         if power_on or pause:
5549             guest.launch(pause=pause)
5550 
5551         if not utils.is_neutron():
5552             guest.enable_hairpin()
5553 
5554         return guest
5555 
5556     def _neutron_failed_callback(self, event_name, instance):
5557         LOG.error('Neutron Reported failure on event '
5558                   '%(event)s for instance %(uuid)s',
5559                   {'event': event_name, 'uuid': instance.uuid},
5560                   instance=instance)
5561         if CONF.vif_plugging_is_fatal:
5562             raise exception.VirtualInterfaceCreateException()
5563 
5564     def _neutron_failed_live_migration_callback(self, event_name, instance):
5565         msg = ('Neutron reported failure during live migration '
5566                'with %(event)s for instance %(uuid)s' %
5567                {'event': event_name, 'uuid': instance.uuid})
5568         raise exception.MigrationError(reason=msg)
5569 
5570     def _get_neutron_events(self, network_info):
5571         # NOTE(danms): We need to collect any VIFs that are currently
5572         # down that we expect a down->up event for. Anything that is
5573         # already up will not undergo that transition, and for
5574         # anything that might be stale (cache-wise) assume it's
5575         # already up so we don't block on it.
5576         return [('network-vif-plugged', vif['id'])
5577                 for vif in network_info if vif.get('active', True) is False]
5578 
5579     def _get_neutron_events_for_live_migration(self, network_info):
5580         # Neutron should send events to Nova indicating that the VIFs
5581         # are successfully plugged on destination host.
5582 
5583         # TODO(sahid): Currently we only use the mechanism of waiting
5584         # for neutron events during live-migration for linux-bridge.
5585         return [('network-vif-plugged', vif['id'])
5586                 for vif in network_info if (
5587                         vif.get('type') == network_model.VIF_TYPE_BRIDGE)]
5588 
5589     def _cleanup_failed_start(self, context, instance, network_info,
5590                               block_device_info, guest, destroy_disks):
5591         try:
5592             if guest and guest.is_active():
5593                 guest.poweroff()
5594         finally:
5595             self.cleanup(context, instance, network_info=network_info,
5596                          block_device_info=block_device_info,
5597                          destroy_disks=destroy_disks)
5598 
5599     def _create_domain_and_network(self, context, xml, instance, network_info,
5600                                    block_device_info=None, power_on=True,
5601                                    vifs_already_plugged=False,
5602                                    post_xml_callback=None,
5603                                    destroy_disks_on_failure=False):
5604 
5605         """Do required network setup and create domain."""
5606         timeout = CONF.vif_plugging_timeout
5607         if (self._conn_supports_start_paused and
5608             utils.is_neutron() and not
5609             vifs_already_plugged and power_on and timeout):
5610             events = self._get_neutron_events(network_info)
5611         else:
5612             events = []
5613 
5614         pause = bool(events)
5615         guest = None
5616         try:
5617             with self.virtapi.wait_for_instance_event(
5618                     instance, events, deadline=timeout,
5619                     error_callback=self._neutron_failed_callback):
5620                 self.plug_vifs(instance, network_info)
5621                 self.firewall_driver.setup_basic_filtering(instance,
5622                                                            network_info)
5623                 self.firewall_driver.prepare_instance_filter(instance,
5624                                                              network_info)
5625                 with self._lxc_disk_handler(context, instance,
5626                                             instance.image_meta,
5627                                             block_device_info):
5628                     guest = self._create_domain(
5629                         xml, pause=pause, power_on=power_on,
5630                         post_xml_callback=post_xml_callback)
5631 
5632                 self.firewall_driver.apply_instance_filter(instance,
5633                                                            network_info)
5634         except exception.VirtualInterfaceCreateException:
5635             # Neutron reported failure and we didn't swallow it, so
5636             # bail here
5637             with excutils.save_and_reraise_exception():
5638                 self._cleanup_failed_start(context, instance, network_info,
5639                                            block_device_info, guest,
5640                                            destroy_disks_on_failure)
5641         except eventlet.timeout.Timeout:
5642             # We never heard from Neutron
5643             LOG.warning('Timeout waiting for %(events)s for '
5644                         'instance with vm_state %(vm_state)s and '
5645                         'task_state %(task_state)s.',
5646                         {'events': events,
5647                          'vm_state': instance.vm_state,
5648                          'task_state': instance.task_state},
5649                         instance=instance)
5650             if CONF.vif_plugging_is_fatal:
5651                 self._cleanup_failed_start(context, instance, network_info,
5652                                            block_device_info, guest,
5653                                            destroy_disks_on_failure)
5654                 raise exception.VirtualInterfaceCreateException()
5655         except Exception:
5656             # Any other error, be sure to clean up
5657             LOG.error('Failed to start libvirt guest', instance=instance)
5658             with excutils.save_and_reraise_exception():
5659                 self._cleanup_failed_start(context, instance, network_info,
5660                                            block_device_info, guest,
5661                                            destroy_disks_on_failure)
5662 
5663         # Resume only if domain has been paused
5664         if pause:
5665             guest.resume()
5666         return guest
5667 
5668     def _get_vcpu_total(self):
5669         """Get available vcpu number of physical computer.
5670 
5671         :returns: the number of cpu core instances can be used.
5672 
5673         """
5674         try:
5675             total_pcpus = self._host.get_cpu_count()
5676         except libvirt.libvirtError:
5677             LOG.warning("Cannot get the number of cpu, because this "
5678                         "function is not implemented for this platform. ")
5679             return 0
5680 
5681         if not CONF.vcpu_pin_set:
5682             return total_pcpus
5683 
5684         available_ids = hardware.get_vcpu_pin_set()
5685         # We get the list of online CPUs on the host and see if the requested
5686         # set falls under these. If not, we retain the old behavior.
5687         online_pcpus = None
5688         try:
5689             online_pcpus = self._host.get_online_cpus()
5690         except libvirt.libvirtError as ex:
5691             error_code = ex.get_error_code()
5692             err_msg = encodeutils.exception_to_unicode(ex)
5693             LOG.warning(
5694                 "Couldn't retrieve the online CPUs due to a Libvirt "
5695                 "error: %(error)s with error code: %(error_code)s",
5696                 {'error': err_msg, 'error_code': error_code})
5697         if online_pcpus:
5698             if not (available_ids <= online_pcpus):
5699                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5700                          "specified cpuset is not online. Online cpuset(s): "
5701                          "%(online)s, requested cpuset(s): %(req)s"),
5702                        {'online': sorted(online_pcpus),
5703                         'req': sorted(available_ids)})
5704                 raise exception.Invalid(msg)
5705         elif sorted(available_ids)[-1] >= total_pcpus:
5706             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5707                                       "out of hypervisor cpu range."))
5708         return len(available_ids)
5709 
5710     @staticmethod
5711     def _get_local_gb_info():
5712         """Get local storage info of the compute node in GB.
5713 
5714         :returns: A dict containing:
5715              :total: How big the overall usable filesystem is (in gigabytes)
5716              :free: How much space is free (in gigabytes)
5717              :used: How much space is used (in gigabytes)
5718         """
5719 
5720         if CONF.libvirt.images_type == 'lvm':
5721             info = lvm.get_volume_group_info(
5722                                CONF.libvirt.images_volume_group)
5723         elif CONF.libvirt.images_type == 'rbd':
5724             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5725         else:
5726             info = libvirt_utils.get_fs_info(CONF.instances_path)
5727 
5728         for (k, v) in info.items():
5729             info[k] = v / units.Gi
5730 
5731         return info
5732 
5733     def _get_vcpu_used(self):
5734         """Get vcpu usage number of physical computer.
5735 
5736         :returns: The total number of vcpu(s) that are currently being used.
5737 
5738         """
5739 
5740         total = 0
5741 
5742         # Not all libvirt drivers will support the get_vcpus_info()
5743         #
5744         # For example, LXC does not have a concept of vCPUs, while
5745         # QEMU (TCG) traditionally handles all vCPUs in a single
5746         # thread. So both will report an exception when the vcpus()
5747         # API call is made. In such a case we should report the
5748         # guest as having 1 vCPU, since that lets us still do
5749         # CPU over commit calculations that apply as the total
5750         # guest count scales.
5751         #
5752         # It is also possible that we might see an exception if
5753         # the guest is just in middle of shutting down. Technically
5754         # we should report 0 for vCPU usage in this case, but we
5755         # we can't reliably distinguish the vcpu not supported
5756         # case from the just shutting down case. Thus we don't know
5757         # whether to report 1 or 0 for vCPU count.
5758         #
5759         # Under-reporting vCPUs is bad because it could conceivably
5760         # let the scheduler place too many guests on the host. Over-
5761         # reporting vCPUs is not a problem as it'll auto-correct on
5762         # the next refresh of usage data.
5763         #
5764         # Thus when getting an exception we always report 1 as the
5765         # vCPU count, as the least worst value.
5766         for guest in self._host.list_guests():
5767             try:
5768                 vcpus = guest.get_vcpus_info()
5769                 total += len(list(vcpus))
5770             except libvirt.libvirtError:
5771                 total += 1
5772             # NOTE(gtt116): give other tasks a chance.
5773             greenthread.sleep(0)
5774         return total
5775 
5776     def _get_supported_vgpu_types(self):
5777         if not CONF.devices.enabled_vgpu_types:
5778             return []
5779         # TODO(sbauza): Move this check up to compute_manager.init_host
5780         if len(CONF.devices.enabled_vgpu_types) > 1:
5781             LOG.warning('libvirt only supports one GPU type per compute node,'
5782                         ' only first type will be used.')
5783         requested_types = CONF.devices.enabled_vgpu_types[:1]
5784         return requested_types
5785 
5786     def _get_vgpu_total(self):
5787         """Returns the number of total available vGPUs for any GPU type that is
5788         enabled with the enabled_vgpu_types CONF option.
5789         """
5790         requested_types = self._get_supported_vgpu_types()
5791         # Bail out early if operator doesn't care about providing vGPUs
5792         if not requested_types:
5793             return 0
5794         # Filter how many available mdevs we can create for all the supported
5795         # types.
5796         mdev_capable_devices = self._get_mdev_capable_devices(requested_types)
5797         vgpus = 0
5798         for dev in mdev_capable_devices:
5799             for _type in dev['types']:
5800                 vgpus += dev['types'][_type]['availableInstances']
5801         # Count the already created (but possibly not assigned to a guest)
5802         # mdevs for all the supported types
5803         mediated_devices = self._get_mediated_devices(requested_types)
5804         vgpus += len(mediated_devices)
5805         return vgpus
5806 
5807     def _get_instance_capabilities(self):
5808         """Get hypervisor instance capabilities
5809 
5810         Returns a list of tuples that describe instances the
5811         hypervisor is capable of hosting.  Each tuple consists
5812         of the triplet (arch, hypervisor_type, vm_mode).
5813 
5814         Supported hypervisor_type is filtered by virt_type,
5815         a parameter set by operators via `nova.conf`.
5816 
5817         :returns: List of tuples describing instance capabilities
5818         """
5819         caps = self._host.get_capabilities()
5820         instance_caps = list()
5821         for g in caps.guests:
5822             for dt in g.domtype:
5823                 if dt != CONF.libvirt.virt_type:
5824                     continue
5825                 instance_cap = (
5826                     fields.Architecture.canonicalize(g.arch),
5827                     fields.HVType.canonicalize(dt),
5828                     fields.VMMode.canonicalize(g.ostype))
5829                 instance_caps.append(instance_cap)
5830 
5831         return instance_caps
5832 
5833     def _get_cpu_info(self):
5834         """Get cpuinfo information.
5835 
5836         Obtains cpu feature from virConnect.getCapabilities.
5837 
5838         :return: see above description
5839 
5840         """
5841 
5842         caps = self._host.get_capabilities()
5843         cpu_info = dict()
5844 
5845         cpu_info['arch'] = caps.host.cpu.arch
5846         cpu_info['model'] = caps.host.cpu.model
5847         cpu_info['vendor'] = caps.host.cpu.vendor
5848 
5849         topology = dict()
5850         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5851         topology['sockets'] = caps.host.cpu.sockets
5852         topology['cores'] = caps.host.cpu.cores
5853         topology['threads'] = caps.host.cpu.threads
5854         cpu_info['topology'] = topology
5855 
5856         features = set()
5857         for f in caps.host.cpu.features:
5858             features.add(f.name)
5859         cpu_info['features'] = features
5860         return cpu_info
5861 
5862     def _get_pcinet_info(self, vf_address):
5863         """Returns a dict of NET device."""
5864         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5865         if not devname:
5866             return
5867 
5868         virtdev = self._host.device_lookup_by_name(devname)
5869         xmlstr = virtdev.XMLDesc(0)
5870         cfgdev = vconfig.LibvirtConfigNodeDevice()
5871         cfgdev.parse_str(xmlstr)
5872         return {'name': cfgdev.name,
5873                 'capabilities': cfgdev.pci_capability.features}
5874 
5875     def _get_pcidev_info(self, devname):
5876         """Returns a dict of PCI device."""
5877 
5878         def _get_device_type(cfgdev, pci_address):
5879             """Get a PCI device's device type.
5880 
5881             An assignable PCI device can be a normal PCI device,
5882             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5883             Function (VF). Only normal PCI devices or SR-IOV VFs
5884             are assignable, while SR-IOV PFs are always owned by
5885             hypervisor.
5886             """
5887             for fun_cap in cfgdev.pci_capability.fun_capability:
5888                 if fun_cap.type == 'virt_functions':
5889                     return {
5890                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5891                     }
5892                 if (fun_cap.type == 'phys_function' and
5893                     len(fun_cap.device_addrs) != 0):
5894                     phys_address = "%04x:%02x:%02x.%01x" % (
5895                         fun_cap.device_addrs[0][0],
5896                         fun_cap.device_addrs[0][1],
5897                         fun_cap.device_addrs[0][2],
5898                         fun_cap.device_addrs[0][3])
5899                     return {
5900                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5901                         'parent_addr': phys_address,
5902                     }
5903 
5904             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5905             # only when VFs are enabled. The check below is a workaround
5906             # to get the correct report regardless of whether or not any
5907             # VFs are enabled for the device.
5908             if not self._host.has_min_version(
5909                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5910                 is_physical_function = pci_utils.is_physical_function(
5911                     *pci_utils.get_pci_address_fields(pci_address))
5912                 if is_physical_function:
5913                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5914 
5915             return {'dev_type': fields.PciDeviceType.STANDARD}
5916 
5917         def _get_device_capabilities(device, address):
5918             """Get PCI VF device's additional capabilities.
5919 
5920             If a PCI device is a virtual function, this function reads the PCI
5921             parent's network capabilities (must be always a NIC device) and
5922             appends this information to the device's dictionary.
5923             """
5924             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5925                 pcinet_info = self._get_pcinet_info(address)
5926                 if pcinet_info:
5927                     return {'capabilities':
5928                                 {'network': pcinet_info.get('capabilities')}}
5929             return {}
5930 
5931         virtdev = self._host.device_lookup_by_name(devname)
5932         xmlstr = virtdev.XMLDesc(0)
5933         cfgdev = vconfig.LibvirtConfigNodeDevice()
5934         cfgdev.parse_str(xmlstr)
5935 
5936         address = "%04x:%02x:%02x.%1x" % (
5937             cfgdev.pci_capability.domain,
5938             cfgdev.pci_capability.bus,
5939             cfgdev.pci_capability.slot,
5940             cfgdev.pci_capability.function)
5941 
5942         device = {
5943             "dev_id": cfgdev.name,
5944             "address": address,
5945             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5946             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5947             }
5948 
5949         device["numa_node"] = cfgdev.pci_capability.numa_node
5950 
5951         # requirement by DataBase Model
5952         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5953         device.update(_get_device_type(cfgdev, address))
5954         device.update(_get_device_capabilities(device, address))
5955         return device
5956 
5957     def _get_pci_passthrough_devices(self):
5958         """Get host PCI devices information.
5959 
5960         Obtains pci devices information from libvirt, and returns
5961         as a JSON string.
5962 
5963         Each device information is a dictionary, with mandatory keys
5964         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5965         'label' and other optional device specific information.
5966 
5967         Refer to the objects/pci_device.py for more idea of these keys.
5968 
5969         :returns: a JSON string containing a list of the assignable PCI
5970                   devices information
5971         """
5972         # Bail early if we know we can't support `listDevices` to avoid
5973         # repeated warnings within a periodic task
5974         if not getattr(self, '_list_devices_supported', True):
5975             return jsonutils.dumps([])
5976 
5977         try:
5978             dev_names = self._host.list_pci_devices() or []
5979         except libvirt.libvirtError as ex:
5980             error_code = ex.get_error_code()
5981             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5982                 self._list_devices_supported = False
5983                 LOG.warning("URI %(uri)s does not support "
5984                             "listDevices: %(error)s",
5985                             {'uri': self._uri(),
5986                              'error': encodeutils.exception_to_unicode(ex)})
5987                 return jsonutils.dumps([])
5988             else:
5989                 raise
5990 
5991         pci_info = []
5992         for name in dev_names:
5993             pci_info.append(self._get_pcidev_info(name))
5994 
5995         return jsonutils.dumps(pci_info)
5996 
5997     def _get_mdev_capabilities_for_dev(self, devname, types=None):
5998         """Returns a dict of MDEV capable device with the ID as first key
5999         and then a list of supported types, each of them being a dict.
6000 
6001         :param types: Only return those specific types.
6002         """
6003         virtdev = self._host.device_lookup_by_name(devname)
6004         xmlstr = virtdev.XMLDesc(0)
6005         cfgdev = vconfig.LibvirtConfigNodeDevice()
6006         cfgdev.parse_str(xmlstr)
6007 
6008         device = {
6009             "dev_id": cfgdev.name,
6010             "types": {},
6011         }
6012         for mdev_cap in cfgdev.pci_capability.mdev_capability:
6013             for cap in mdev_cap.mdev_types:
6014                 if not types or cap['type'] in types:
6015                     device["types"].update({cap['type']: {
6016                         'availableInstances': cap['availableInstances'],
6017                         'name': cap['name'],
6018                         'deviceAPI': cap['deviceAPI']}})
6019         return device
6020 
6021     def _get_mdev_capable_devices(self, types=None):
6022         """Get host devices supporting mdev types.
6023 
6024         Obtain devices information from libvirt and returns a list of
6025         dictionaries.
6026 
6027         :param types: Filter only devices supporting those types.
6028         """
6029         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6030             return []
6031         dev_names = self._host.list_mdev_capable_devices() or []
6032         mdev_capable_devices = []
6033         for name in dev_names:
6034             device = self._get_mdev_capabilities_for_dev(name, types)
6035             if not device["types"]:
6036                 continue
6037             mdev_capable_devices.append(device)
6038         return mdev_capable_devices
6039 
6040     def _get_mediated_device_information(self, devname):
6041         """Returns a dict of a mediated device."""
6042         virtdev = self._host.device_lookup_by_name(devname)
6043         xmlstr = virtdev.XMLDesc(0)
6044         cfgdev = vconfig.LibvirtConfigNodeDevice()
6045         cfgdev.parse_str(xmlstr)
6046 
6047         device = {
6048             "dev_id": cfgdev.name,
6049             # name is like mdev_00ead764_fdc0_46b6_8db9_2963f5c815b4
6050             "uuid": str(uuid.UUID(cfgdev.name[5:].replace('_', '-'))),
6051             "type": cfgdev.mdev_information.type,
6052             "iommu_group": cfgdev.mdev_information.iommu_group,
6053         }
6054         return device
6055 
6056     def _get_mediated_devices(self, types=None):
6057         """Get host mediated devices.
6058 
6059         Obtain devices information from libvirt and returns a list of
6060         dictionaries.
6061 
6062         :param types: Filter only devices supporting those types.
6063         """
6064         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
6065             return []
6066         dev_names = self._host.list_mediated_devices() or []
6067         mediated_devices = []
6068         for name in dev_names:
6069             device = self._get_mediated_device_information(name)
6070             if not types or device["type"] in types:
6071                 mediated_devices.append(device)
6072         return mediated_devices
6073 
6074     def _get_all_assigned_mediated_devices(self, instance=None):
6075         """Lookup all instances from the host and return all the mediated
6076         devices that are assigned to a guest.
6077 
6078         :param instance: Only return mediated devices for that instance.
6079 
6080         :returns: A dictionary of keys being mediated device UUIDs and their
6081                   respective values the instance UUID of the guest using it.
6082         """
6083         allocated_mdevs = {}
6084         if instance:
6085             # NOTE(sbauza): In some cases (like a migration issue), the
6086             # instance can exist in the Nova database but libvirt doesn't know
6087             # about it. For such cases, the way to fix that is to hard reboot
6088             # the instance, which will recreate the libvirt guest.
6089             # For that reason, we need to support that case by making sure
6090             # we don't raise an exception if the libvirt guest doesn't exist.
6091             try:
6092                 guest = self._host.get_guest(instance)
6093             except exception.InstanceNotFound:
6094                 # Bail out early if libvirt doesn't know about it since we
6095                 # can't know the existing mediated devices
6096                 return {}
6097             guests = [guest]
6098         else:
6099             guests = self._host.list_guests(only_running=False)
6100         for guest in guests:
6101             cfg = guest.get_config()
6102             for device in cfg.devices:
6103                 if isinstance(device, vconfig.LibvirtConfigGuestHostdevMDEV):
6104                     allocated_mdevs[device.uuid] = guest.uuid
6105         return allocated_mdevs
6106 
6107     @staticmethod
6108     def _vgpu_allocations(allocations):
6109         """Filtering only the VGPU allocations from a list of allocations.
6110 
6111         :param allocations: Information about resources allocated to the
6112                             instance via placement, of the form returned by
6113                             SchedulerReportClient.get_allocations_for_consumer.
6114         """
6115         if not allocations:
6116             # If no allocations, there is no vGPU request.
6117             return {}
6118         RC_VGPU = rc_fields.ResourceClass.VGPU
6119         vgpu_allocations = {}
6120         for rp in allocations:
6121             res = allocations[rp]['resources']
6122             if RC_VGPU in res and res[RC_VGPU] > 0:
6123                 vgpu_allocations[rp] = {'resources': {RC_VGPU: res[RC_VGPU]}}
6124         return vgpu_allocations
6125 
6126     def _get_existing_mdevs_not_assigned(self, requested_types=None):
6127         """Returns the already created mediated devices that are not assigned
6128         to a guest yet.
6129 
6130         :param requested_types: Filter out the result for only mediated devices
6131                                 having those types.
6132         """
6133         allocated_mdevs = self._get_all_assigned_mediated_devices()
6134         mdevs = self._get_mediated_devices(requested_types)
6135         available_mdevs = set([mdev["uuid"]
6136                                for mdev in mdevs]) - set(allocated_mdevs)
6137         return available_mdevs
6138 
6139     def _create_new_mediated_device(self, requested_types, uuid=None):
6140         """Find a physical device that can support a new mediated device and
6141         create it.
6142 
6143         :param requested_types: Filter only capable devices supporting those
6144                                 types.
6145         :param uuid: The possible mdev UUID we want to create again
6146 
6147         :returns: the newly created mdev UUID or None if not possible
6148         """
6149         # Try to see if we can still create a new mediated device
6150         devices = self._get_mdev_capable_devices(requested_types)
6151         for device in devices:
6152             # For the moment, the libvirt driver only supports one
6153             # type per host
6154             # TODO(sbauza): Once we support more than one type, make
6155             # sure we look at the flavor/trait for the asked type.
6156             asked_type = requested_types[0]
6157             if device['types'][asked_type]['availableInstances'] > 0:
6158                 # That physical GPU has enough room for a new mdev
6159                 dev_name = device['dev_id']
6160                 # We need the PCI address, not the libvirt name
6161                 # The libvirt name is like 'pci_0000_84_00_0'
6162                 pci_addr = "{}:{}:{}.{}".format(*dev_name[4:].split('_'))
6163                 chosen_mdev = nova.privsep.libvirt.create_mdev(pci_addr,
6164                                                                asked_type,
6165                                                                uuid=uuid)
6166                 return chosen_mdev
6167 
6168     @utils.synchronized(VGPU_RESOURCE_SEMAPHORE)
6169     def _allocate_mdevs(self, allocations):
6170         """Returns a list of mediated device UUIDs corresponding to available
6171         resources we can assign to the guest(s) corresponding to the allocation
6172         requests passed as argument.
6173 
6174         That method can either find an existing but unassigned mediated device
6175         it can allocate, or create a new mediated device from a capable
6176         physical device if the latter has enough left capacity.
6177 
6178         :param allocations: Information about resources allocated to the
6179                             instance via placement, of the form returned by
6180                             SchedulerReportClient.get_allocations_for_consumer.
6181                             That code is supporting Placement API version 1.12
6182         """
6183         vgpu_allocations = self._vgpu_allocations(allocations)
6184         if not vgpu_allocations:
6185             return
6186         # TODO(sbauza): Once we have nested resource providers, find which one
6187         # is having the related allocation for the specific VGPU type.
6188         # For the moment, we should only have one allocation for
6189         # ResourceProvider.
6190         # TODO(sbauza): Iterate over all the allocations once we have
6191         # nested Resource Providers. For the moment, just take the first.
6192         if len(vgpu_allocations) > 1:
6193             LOG.warning('More than one allocation was passed over to libvirt '
6194                         'while at the moment libvirt only supports one. Only '
6195                         'the first allocation will be looked up.')
6196         alloc = six.next(six.itervalues(vgpu_allocations))
6197         vgpus_asked = alloc['resources'][rc_fields.ResourceClass.VGPU]
6198 
6199         requested_types = self._get_supported_vgpu_types()
6200         # Which mediated devices are created but not assigned to a guest ?
6201         mdevs_available = self._get_existing_mdevs_not_assigned(
6202             requested_types)
6203 
6204         chosen_mdevs = []
6205         for c in six.moves.range(vgpus_asked):
6206             chosen_mdev = None
6207             if mdevs_available:
6208                 # Take the first available mdev
6209                 chosen_mdev = mdevs_available.pop()
6210             else:
6211                 chosen_mdev = self._create_new_mediated_device(requested_types)
6212             if not chosen_mdev:
6213                 # If we can't find devices having available VGPUs, just raise
6214                 raise exception.ComputeResourcesUnavailable(
6215                     reason='vGPU resource is not available')
6216             else:
6217                 chosen_mdevs.append(chosen_mdev)
6218         return chosen_mdevs
6219 
6220     def _detach_mediated_devices(self, guest):
6221         mdevs = guest.get_all_devices(
6222             devtype=vconfig.LibvirtConfigGuestHostdevMDEV)
6223         for mdev_cfg in mdevs:
6224             try:
6225                 guest.detach_device(mdev_cfg, live=True)
6226             except libvirt.libvirtError as ex:
6227                 error_code = ex.get_error_code()
6228                 # NOTE(sbauza): There is a pending issue with libvirt that
6229                 # doesn't allow to hot-unplug mediated devices. Let's
6230                 # short-circuit the suspend action and set the instance back
6231                 # to ACTIVE.
6232                 # TODO(sbauza): Once libvirt supports this, amend the resume()
6233                 # operation to support reallocating mediated devices.
6234                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
6235                     reason = _("Suspend is not supported for instances having "
6236                                "attached vGPUs.")
6237                     raise exception.InstanceFaultRollback(
6238                         exception.InstanceSuspendFailure(reason=reason))
6239                 else:
6240                     raise
6241 
6242     def _has_numa_support(self):
6243         # This means that the host can support LibvirtConfigGuestNUMATune
6244         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
6245         caps = self._host.get_capabilities()
6246 
6247         if (caps.host.cpu.arch in (fields.Architecture.I686,
6248                                    fields.Architecture.X86_64,
6249                                    fields.Architecture.AARCH64) and
6250                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
6251             return True
6252         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
6253                                      fields.Architecture.PPC64LE)):
6254             return True
6255 
6256         return False
6257 
6258     def _get_host_numa_topology(self):
6259         if not self._has_numa_support():
6260             return
6261 
6262         caps = self._host.get_capabilities()
6263         topology = caps.host.topology
6264 
6265         if topology is None or not topology.cells:
6266             return
6267 
6268         cells = []
6269         allowed_cpus = hardware.get_vcpu_pin_set()
6270         online_cpus = self._host.get_online_cpus()
6271         if allowed_cpus:
6272             allowed_cpus &= online_cpus
6273         else:
6274             allowed_cpus = online_cpus
6275 
6276         def _get_reserved_memory_for_cell(self, cell_id, page_size):
6277             cell = self._reserved_hugepages.get(cell_id, {})
6278             return cell.get(page_size, 0)
6279 
6280         for cell in topology.cells:
6281             cpuset = set(cpu.id for cpu in cell.cpus)
6282             siblings = sorted(map(set,
6283                                   set(tuple(cpu.siblings)
6284                                         if cpu.siblings else ()
6285                                       for cpu in cell.cpus)
6286                                   ))
6287             cpuset &= allowed_cpus
6288             siblings = [sib & allowed_cpus for sib in siblings]
6289             # Filter out empty sibling sets that may be left
6290             siblings = [sib for sib in siblings if len(sib) > 0]
6291 
6292             mempages = [
6293                 objects.NUMAPagesTopology(
6294                     size_kb=pages.size,
6295                     total=pages.total,
6296                     used=0,
6297                     reserved=_get_reserved_memory_for_cell(
6298                         self, cell.id, pages.size))
6299                 for pages in cell.mempages]
6300 
6301             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
6302                                     memory=cell.memory / units.Ki,
6303                                     cpu_usage=0, memory_usage=0,
6304                                     siblings=siblings,
6305                                     pinned_cpus=set([]),
6306                                     mempages=mempages)
6307             cells.append(cell)
6308 
6309         return objects.NUMATopology(cells=cells)
6310 
6311     def get_all_volume_usage(self, context, compute_host_bdms):
6312         """Return usage info for volumes attached to vms on
6313            a given host.
6314         """
6315         vol_usage = []
6316 
6317         for instance_bdms in compute_host_bdms:
6318             instance = instance_bdms['instance']
6319 
6320             for bdm in instance_bdms['instance_bdms']:
6321                 mountpoint = bdm['device_name']
6322                 if mountpoint.startswith('/dev/'):
6323                     mountpoint = mountpoint[5:]
6324                 volume_id = bdm['volume_id']
6325 
6326                 LOG.debug("Trying to get stats for the volume %s",
6327                           volume_id, instance=instance)
6328                 vol_stats = self.block_stats(instance, mountpoint)
6329 
6330                 if vol_stats:
6331                     stats = dict(volume=volume_id,
6332                                  instance=instance,
6333                                  rd_req=vol_stats[0],
6334                                  rd_bytes=vol_stats[1],
6335                                  wr_req=vol_stats[2],
6336                                  wr_bytes=vol_stats[3])
6337                     LOG.debug(
6338                         "Got volume usage stats for the volume=%(volume)s,"
6339                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
6340                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
6341                         stats, instance=instance)
6342                     vol_usage.append(stats)
6343 
6344         return vol_usage
6345 
6346     def block_stats(self, instance, disk_id):
6347         """Note that this function takes an instance name."""
6348         try:
6349             guest = self._host.get_guest(instance)
6350 
6351             # TODO(sahid): We are converting all calls from a
6352             # virDomain object to use nova.virt.libvirt.Guest.
6353             # We should be able to remove domain at the end.
6354             domain = guest._domain
6355             return domain.blockStats(disk_id)
6356         except libvirt.libvirtError as e:
6357             errcode = e.get_error_code()
6358             LOG.info('Getting block stats failed, device might have '
6359                      'been detached. Instance=%(instance_name)s '
6360                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
6361                      {'instance_name': instance.name, 'disk': disk_id,
6362                       'errcode': errcode, 'e': e},
6363                      instance=instance)
6364         except exception.InstanceNotFound:
6365             LOG.info('Could not find domain in libvirt for instance %s. '
6366                      'Cannot get block stats for device', instance.name,
6367                      instance=instance)
6368 
6369     def get_console_pool_info(self, console_type):
6370         # TODO(mdragon): console proxy should be implemented for libvirt,
6371         #                in case someone wants to use it with kvm or
6372         #                such. For now return fake data.
6373         return {'address': '127.0.0.1',
6374                 'username': 'fakeuser',
6375                 'password': 'fakepassword'}
6376 
6377     def refresh_security_group_rules(self, security_group_id):
6378         self.firewall_driver.refresh_security_group_rules(security_group_id)
6379 
6380     def refresh_instance_security_rules(self, instance):
6381         self.firewall_driver.refresh_instance_security_rules(instance)
6382 
6383     def update_provider_tree(self, provider_tree, nodename):
6384         """Update a ProviderTree object with current resource provider,
6385         inventory information and CPU traits.
6386 
6387         :param nova.compute.provider_tree.ProviderTree provider_tree:
6388             A nova.compute.provider_tree.ProviderTree object representing all
6389             the providers in the tree associated with the compute node, and any
6390             sharing providers (those with the ``MISC_SHARES_VIA_AGGREGATE``
6391             trait) associated via aggregate with any of those providers (but
6392             not *their* tree- or aggregate-associated providers), as currently
6393             known by placement.
6394 
6395         :param nodename:
6396             String name of the compute node (i.e.
6397             ComputeNode.hypervisor_hostname) for which the caller is requesting
6398             updated provider information.
6399         """
6400         disk_gb = int(self._get_local_gb_info()['total'])
6401         memory_mb = int(self._host.get_memory_mb_total())
6402         vcpus = self._get_vcpu_total()
6403 
6404         # NOTE(sbauza): For the moment, the libvirt driver only supports
6405         # providing the total number of virtual GPUs for a single GPU type. If
6406         # you have multiple physical GPUs, each of them providing multiple GPU
6407         # types, libvirt will return the total sum of virtual GPUs
6408         # corresponding to the single type passed in enabled_vgpu_types
6409         # configuration option. Eg. if you have 2 pGPUs supporting 'nvidia-35',
6410         # each of them having 16 available instances, the total here will be
6411         # 32.
6412         # If one of the 2 pGPUs doesn't support 'nvidia-35', it won't be used.
6413         # TODO(sbauza): Use traits to make a better world.
6414         vgpus = self._get_vgpu_total()
6415 
6416         # NOTE(jaypipes): We leave some fields like allocation_ratio and
6417         # reserved out of the returned dicts here because, for now at least,
6418         # the RT injects those values into the inventory dict based on the
6419         # compute_nodes record values.
6420         result = {
6421             rc_fields.ResourceClass.VCPU: {
6422                 'total': vcpus,
6423                 'min_unit': 1,
6424                 'max_unit': vcpus,
6425                 'step_size': 1,
6426             },
6427             rc_fields.ResourceClass.MEMORY_MB: {
6428                 'total': memory_mb,
6429                 'min_unit': 1,
6430                 'max_unit': memory_mb,
6431                 'step_size': 1,
6432             },
6433         }
6434 
6435         # If a sharing DISK_GB provider exists in the provider tree, then our
6436         # storage is shared, and we should not report the DISK_GB inventory in
6437         # the compute node provider.
6438         if not provider_tree.has_sharing_provider(
6439                 rc_fields.ResourceClass.DISK_GB):
6440             result[rc_fields.ResourceClass.DISK_GB] = {
6441                 'total': disk_gb,
6442                 'min_unit': 1,
6443                 'max_unit': disk_gb,
6444                 'step_size': 1,
6445             }
6446 
6447         if vgpus > 0:
6448             # Only provide VGPU resource classes if the driver supports it.
6449             result[rc_fields.ResourceClass.VGPU] = {
6450                 'total': vgpus,
6451                 'min_unit': 1,
6452                 'max_unit': vgpus,
6453                 'step_size': 1,
6454                 }
6455 
6456         provider_tree.update_inventory(nodename, result)
6457 
6458         traits = self._get_cpu_traits()
6459         if traits is not None:
6460             # _get_cpu_traits returns a dict of trait names mapped to boolean
6461             # values. Add traits equal to True to provider tree, remove
6462             # those False traits from provider tree.
6463             traits_to_add = [t for t in traits if traits[t]]
6464             traits_to_remove = set(traits) - set(traits_to_add)
6465             provider_tree.add_traits(nodename, *traits_to_add)
6466             provider_tree.remove_traits(nodename, *traits_to_remove)
6467 
6468     def get_available_resource(self, nodename):
6469         """Retrieve resource information.
6470 
6471         This method is called when nova-compute launches, and
6472         as part of a periodic task that records the results in the DB.
6473 
6474         :param nodename: unused in this driver
6475         :returns: dictionary containing resource info
6476         """
6477 
6478         disk_info_dict = self._get_local_gb_info()
6479         data = {}
6480 
6481         # NOTE(dprince): calling capabilities before getVersion works around
6482         # an initialization issue with some versions of Libvirt (1.0.5.5).
6483         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
6484         # See: https://bugs.launchpad.net/nova/+bug/1215593
6485         data["supported_instances"] = self._get_instance_capabilities()
6486 
6487         data["vcpus"] = self._get_vcpu_total()
6488         data["memory_mb"] = self._host.get_memory_mb_total()
6489         data["local_gb"] = disk_info_dict['total']
6490         data["vcpus_used"] = self._get_vcpu_used()
6491         data["memory_mb_used"] = self._host.get_memory_mb_used()
6492         data["local_gb_used"] = disk_info_dict['used']
6493         data["hypervisor_type"] = self._host.get_driver_type()
6494         data["hypervisor_version"] = self._host.get_version()
6495         data["hypervisor_hostname"] = self._host.get_hostname()
6496         # TODO(berrange): why do we bother converting the
6497         # libvirt capabilities XML into a special JSON format ?
6498         # The data format is different across all the drivers
6499         # so we could just return the raw capabilities XML
6500         # which 'compare_cpu' could use directly
6501         #
6502         # That said, arch_filter.py now seems to rely on
6503         # the libvirt drivers format which suggests this
6504         # data format needs to be standardized across drivers
6505         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
6506 
6507         disk_free_gb = disk_info_dict['free']
6508         disk_over_committed = self._get_disk_over_committed_size_total()
6509         available_least = disk_free_gb * units.Gi - disk_over_committed
6510         data['disk_available_least'] = available_least / units.Gi
6511 
6512         data['pci_passthrough_devices'] = \
6513             self._get_pci_passthrough_devices()
6514 
6515         numa_topology = self._get_host_numa_topology()
6516         if numa_topology:
6517             data['numa_topology'] = numa_topology._to_json()
6518         else:
6519             data['numa_topology'] = None
6520 
6521         return data
6522 
6523     def check_instance_shared_storage_local(self, context, instance):
6524         """Check if instance files located on shared storage.
6525 
6526         This runs check on the destination host, and then calls
6527         back to the source host to check the results.
6528 
6529         :param context: security context
6530         :param instance: nova.objects.instance.Instance object
6531         :returns:
6532          - tempfile: A dict containing the tempfile info on the destination
6533                      host
6534          - None:
6535 
6536             1. If the instance path is not existing.
6537             2. If the image backend is shared block storage type.
6538         """
6539         if self.image_backend.backend().is_shared_block_storage():
6540             return None
6541 
6542         dirpath = libvirt_utils.get_instance_path(instance)
6543 
6544         if not os.path.exists(dirpath):
6545             return None
6546 
6547         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6548         LOG.debug("Creating tmpfile %s to verify with other "
6549                   "compute node that the instance is on "
6550                   "the same shared storage.",
6551                   tmp_file, instance=instance)
6552         os.close(fd)
6553         return {"filename": tmp_file}
6554 
6555     def check_instance_shared_storage_remote(self, context, data):
6556         return os.path.exists(data['filename'])
6557 
6558     def check_instance_shared_storage_cleanup(self, context, data):
6559         fileutils.delete_if_exists(data["filename"])
6560 
6561     def check_can_live_migrate_destination(self, context, instance,
6562                                            src_compute_info, dst_compute_info,
6563                                            block_migration=False,
6564                                            disk_over_commit=False):
6565         """Check if it is possible to execute live migration.
6566 
6567         This runs checks on the destination host, and then calls
6568         back to the source host to check the results.
6569 
6570         :param context: security context
6571         :param instance: nova.db.sqlalchemy.models.Instance
6572         :param block_migration: if true, prepare for block migration
6573         :param disk_over_commit: if true, allow disk over commit
6574         :returns: a LibvirtLiveMigrateData object
6575         """
6576 
6577         # TODO(zcornelius): Remove this check in Stein, as we'll only support
6578         #                   Rocky and newer computes.
6579         # If file_backed_memory is enabled on this host, we have to make sure
6580         # the source is new enough to support it. Since the source generates
6581         # the XML for the destination, we depend on the source generating a
6582         # file-backed XML for us, so fail if it won't do that.
6583         if CONF.libvirt.file_backed_memory > 0:
6584             srv = objects.Service.get_by_compute_host(context, instance.host)
6585             if srv.version < 32:
6586                 msg = ("Cannot migrate instance %(uuid)s from node %(node)s. "
6587                        "Node %(node)s is not compatible with "
6588                        "file_backed_memory" % {"uuid": instance.uuid,
6589                                                "node": srv.host})
6590                 raise exception.MigrationPreCheckError(reason=msg)
6591 
6592         if disk_over_commit:
6593             disk_available_gb = dst_compute_info['local_gb']
6594         else:
6595             disk_available_gb = dst_compute_info['disk_available_least']
6596         disk_available_mb = (
6597             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
6598 
6599         # Compare CPU
6600         if not instance.vcpu_model or not instance.vcpu_model.model:
6601             source_cpu_info = src_compute_info['cpu_info']
6602             self._compare_cpu(None, source_cpu_info, instance)
6603         else:
6604             self._compare_cpu(instance.vcpu_model, None, instance)
6605 
6606         # Create file on storage, to be checked on source host
6607         filename = self._create_shared_storage_test_file(instance)
6608 
6609         data = objects.LibvirtLiveMigrateData()
6610         data.filename = filename
6611         data.image_type = CONF.libvirt.images_type
6612         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
6613         data.graphics_listen_addr_spice = CONF.spice.server_listen
6614         if CONF.serial_console.enabled:
6615             data.serial_listen_addr = CONF.serial_console.proxyclient_address
6616         else:
6617             data.serial_listen_addr = None
6618         # Notes(eliqiao): block_migration and disk_over_commit are not
6619         # nullable, so just don't set them if they are None
6620         if block_migration is not None:
6621             data.block_migration = block_migration
6622         if disk_over_commit is not None:
6623             data.disk_over_commit = disk_over_commit
6624         data.disk_available_mb = disk_available_mb
6625         data.dst_wants_file_backed_memory = \
6626                 CONF.libvirt.file_backed_memory > 0
6627         data.file_backed_memory_discard = (CONF.libvirt.file_backed_memory and
6628             self._host.has_min_version(MIN_LIBVIRT_FILE_BACKED_DISCARD_VERSION,
6629                                        MIN_QEMU_FILE_BACKED_DISCARD_VERSION))
6630 
6631         return data
6632 
6633     def cleanup_live_migration_destination_check(self, context,
6634                                                  dest_check_data):
6635         """Do required cleanup on dest host after check_can_live_migrate calls
6636 
6637         :param context: security context
6638         """
6639         filename = dest_check_data.filename
6640         self._cleanup_shared_storage_test_file(filename)
6641 
6642     def check_can_live_migrate_source(self, context, instance,
6643                                       dest_check_data,
6644                                       block_device_info=None):
6645         """Check if it is possible to execute live migration.
6646 
6647         This checks if the live migration can succeed, based on the
6648         results from check_can_live_migrate_destination.
6649 
6650         :param context: security context
6651         :param instance: nova.db.sqlalchemy.models.Instance
6652         :param dest_check_data: result of check_can_live_migrate_destination
6653         :param block_device_info: result of _get_instance_block_device_info
6654         :returns: a LibvirtLiveMigrateData object
6655         """
6656         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
6657             md_obj = objects.LibvirtLiveMigrateData()
6658             md_obj.from_legacy_dict(dest_check_data)
6659             dest_check_data = md_obj
6660 
6661         # Checking shared storage connectivity
6662         # if block migration, instances_path should not be on shared storage.
6663         source = CONF.host
6664 
6665         dest_check_data.is_shared_instance_path = (
6666             self._check_shared_storage_test_file(
6667                 dest_check_data.filename, instance))
6668 
6669         dest_check_data.is_shared_block_storage = (
6670             self._is_shared_block_storage(instance, dest_check_data,
6671                                           block_device_info))
6672 
6673         if 'block_migration' not in dest_check_data:
6674             dest_check_data.block_migration = (
6675                 not dest_check_data.is_on_shared_storage())
6676 
6677         if dest_check_data.block_migration:
6678             # TODO(eliqiao): Once block_migration flag is removed from the API
6679             # we can safely remove the if condition
6680             if dest_check_data.is_on_shared_storage():
6681                 reason = _("Block migration can not be used "
6682                            "with shared storage.")
6683                 raise exception.InvalidLocalStorage(reason=reason, path=source)
6684             if 'disk_over_commit' in dest_check_data:
6685                 self._assert_dest_node_has_enough_disk(context, instance,
6686                                         dest_check_data.disk_available_mb,
6687                                         dest_check_data.disk_over_commit,
6688                                         block_device_info)
6689             if block_device_info:
6690                 bdm = block_device_info.get('block_device_mapping')
6691                 # NOTE(eliqiao): Selective disk migrations are not supported
6692                 # with tunnelled block migrations so we can block them early.
6693                 if (bdm and
6694                     (self._block_migration_flags &
6695                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
6696                     msg = (_('Cannot block migrate instance %(uuid)s with'
6697                              ' mapped volumes. Selective block device'
6698                              ' migration is not supported with tunnelled'
6699                              ' block migrations.') % {'uuid': instance.uuid})
6700                     LOG.error(msg, instance=instance)
6701                     raise exception.MigrationPreCheckError(reason=msg)
6702         elif not (dest_check_data.is_shared_block_storage or
6703                   dest_check_data.is_shared_instance_path):
6704             reason = _("Shared storage live-migration requires either shared "
6705                        "storage or boot-from-volume with no local disks.")
6706             raise exception.InvalidSharedStorage(reason=reason, path=source)
6707 
6708         # NOTE(mikal): include the instance directory name here because it
6709         # doesn't yet exist on the destination but we want to force that
6710         # same name to be used
6711         instance_path = libvirt_utils.get_instance_path(instance,
6712                                                         relative=True)
6713         dest_check_data.instance_relative_path = instance_path
6714 
6715         # NOTE(lyarwood): Used to indicate to the dest that the src is capable
6716         # of wiring up the encrypted disk configuration for the domain.
6717         # Note that this does not require the QEMU and Libvirt versions to
6718         # decrypt LUKS to be installed on the source node. Only the Nova
6719         # utility code to generate the correct XML is required, so we can
6720         # default to True here for all computes >= Queens.
6721         dest_check_data.src_supports_native_luks = True
6722 
6723         return dest_check_data
6724 
6725     def _is_shared_block_storage(self, instance, dest_check_data,
6726                                  block_device_info=None):
6727         """Check if all block storage of an instance can be shared
6728         between source and destination of a live migration.
6729 
6730         Returns true if the instance is volume backed and has no local disks,
6731         or if the image backend is the same on source and destination and the
6732         backend shares block storage between compute nodes.
6733 
6734         :param instance: nova.objects.instance.Instance object
6735         :param dest_check_data: dict with boolean fields image_type,
6736                                 is_shared_instance_path, and is_volume_backed
6737         """
6738         if (dest_check_data.obj_attr_is_set('image_type') and
6739                 CONF.libvirt.images_type == dest_check_data.image_type and
6740                 self.image_backend.backend().is_shared_block_storage()):
6741             # NOTE(dgenin): currently true only for RBD image backend
6742             return True
6743 
6744         if (dest_check_data.is_shared_instance_path and
6745                 self.image_backend.backend().is_file_in_instance_path()):
6746             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6747             # place block device files under the instance path
6748             return True
6749 
6750         if (dest_check_data.is_volume_backed and
6751                 not bool(self._get_instance_disk_info(instance,
6752                                                       block_device_info))):
6753             return True
6754 
6755         return False
6756 
6757     def _assert_dest_node_has_enough_disk(self, context, instance,
6758                                              available_mb, disk_over_commit,
6759                                              block_device_info):
6760         """Checks if destination has enough disk for block migration."""
6761         # Libvirt supports qcow2 disk format,which is usually compressed
6762         # on compute nodes.
6763         # Real disk image (compressed) may enlarged to "virtual disk size",
6764         # that is specified as the maximum disk size.
6765         # (See qemu-img -f path-to-disk)
6766         # Scheduler recognizes destination host still has enough disk space
6767         # if real disk size < available disk size
6768         # if disk_over_commit is True,
6769         #  otherwise virtual disk size < available disk size.
6770 
6771         available = 0
6772         if available_mb:
6773             available = available_mb * units.Mi
6774 
6775         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6776 
6777         necessary = 0
6778         if disk_over_commit:
6779             for info in disk_infos:
6780                 necessary += int(info['disk_size'])
6781         else:
6782             for info in disk_infos:
6783                 necessary += int(info['virt_disk_size'])
6784 
6785         # Check that available disk > necessary disk
6786         if (available - necessary) < 0:
6787             reason = (_('Unable to migrate %(instance_uuid)s: '
6788                         'Disk of instance is too large(available'
6789                         ' on destination host:%(available)s '
6790                         '< need:%(necessary)s)') %
6791                       {'instance_uuid': instance.uuid,
6792                        'available': available,
6793                        'necessary': necessary})
6794             raise exception.MigrationPreCheckError(reason=reason)
6795 
6796     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6797         """Check the host is compatible with the requested CPU
6798 
6799         :param guest_cpu: nova.objects.VirtCPUModel or None
6800         :param host_cpu_str: JSON from _get_cpu_info() method
6801 
6802         If the 'guest_cpu' parameter is not None, this will be
6803         validated for migration compatibility with the host.
6804         Otherwise the 'host_cpu_str' JSON string will be used for
6805         validation.
6806 
6807         :returns:
6808             None. if given cpu info is not compatible to this server,
6809             raise exception.
6810         """
6811 
6812         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6813         # guests (<domain type='qemu'>) should not matter -- in this
6814         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6815         # hardware acceleration, like KVM, is involved. So, skip the CPU
6816         # compatibility check for the QEMU domain type, and retain it for
6817         # KVM guests.
6818         if CONF.libvirt.virt_type not in ['kvm']:
6819             return
6820 
6821         if guest_cpu is None:
6822             info = jsonutils.loads(host_cpu_str)
6823             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6824             cpu = vconfig.LibvirtConfigCPU()
6825             cpu.arch = info['arch']
6826             cpu.model = info['model']
6827             cpu.vendor = info['vendor']
6828             cpu.sockets = info['topology']['sockets']
6829             cpu.cores = info['topology']['cores']
6830             cpu.threads = info['topology']['threads']
6831             for f in info['features']:
6832                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6833         else:
6834             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6835 
6836         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6837              "virCPUCompareResult")
6838         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6839         # unknown character exists in xml, then libvirt complains
6840         try:
6841             cpu_xml = cpu.to_xml()
6842             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6843             ret = self._host.compare_cpu(cpu_xml)
6844         except libvirt.libvirtError as e:
6845             error_code = e.get_error_code()
6846             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6847                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6848                           "It will be proceeded though. Error: %(error)s",
6849                           {'uri': self._uri(), 'error': e})
6850                 return
6851             else:
6852                 LOG.error(m, {'ret': e, 'u': u})
6853                 raise exception.MigrationPreCheckError(
6854                     reason=m % {'ret': e, 'u': u})
6855 
6856         if ret <= 0:
6857             LOG.error(m, {'ret': ret, 'u': u})
6858             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6859 
6860     def _create_shared_storage_test_file(self, instance):
6861         """Makes tmpfile under CONF.instances_path."""
6862         dirpath = CONF.instances_path
6863         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6864         LOG.debug("Creating tmpfile %s to notify to other "
6865                   "compute nodes that they should mount "
6866                   "the same storage.", tmp_file, instance=instance)
6867         os.close(fd)
6868         return os.path.basename(tmp_file)
6869 
6870     def _check_shared_storage_test_file(self, filename, instance):
6871         """Confirms existence of the tmpfile under CONF.instances_path.
6872 
6873         Cannot confirm tmpfile return False.
6874         """
6875         # NOTE(tpatzig): if instances_path is a shared volume that is
6876         # under heavy IO (many instances on many compute nodes),
6877         # then checking the existence of the testfile fails,
6878         # just because it takes longer until the client refreshes and new
6879         # content gets visible.
6880         # os.utime (like touch) on the directory forces the client to refresh.
6881         os.utime(CONF.instances_path, None)
6882 
6883         tmp_file = os.path.join(CONF.instances_path, filename)
6884         if not os.path.exists(tmp_file):
6885             exists = False
6886         else:
6887             exists = True
6888         LOG.debug('Check if temp file %s exists to indicate shared storage '
6889                   'is being used for migration. Exists? %s', tmp_file, exists,
6890                   instance=instance)
6891         return exists
6892 
6893     def _cleanup_shared_storage_test_file(self, filename):
6894         """Removes existence of the tmpfile under CONF.instances_path."""
6895         tmp_file = os.path.join(CONF.instances_path, filename)
6896         os.remove(tmp_file)
6897 
6898     def ensure_filtering_rules_for_instance(self, instance, network_info):
6899         """Ensure that an instance's filtering rules are enabled.
6900 
6901         When migrating an instance, we need the filtering rules to
6902         be configured on the destination host before starting the
6903         migration.
6904 
6905         Also, when restarting the compute service, we need to ensure
6906         that filtering rules exist for all running services.
6907         """
6908 
6909         self.firewall_driver.setup_basic_filtering(instance, network_info)
6910         self.firewall_driver.prepare_instance_filter(instance,
6911                 network_info)
6912 
6913         # nwfilters may be defined in a separate thread in the case
6914         # of libvirt non-blocking mode, so we wait for completion
6915         timeout_count = list(range(CONF.live_migration_retry_count))
6916         while timeout_count:
6917             if self.firewall_driver.instance_filter_exists(instance,
6918                                                            network_info):
6919                 break
6920             timeout_count.pop()
6921             if len(timeout_count) == 0:
6922                 msg = _('The firewall filter for %s does not exist')
6923                 raise exception.InternalError(msg % instance.name)
6924             greenthread.sleep(1)
6925 
6926     def filter_defer_apply_on(self):
6927         self.firewall_driver.filter_defer_apply_on()
6928 
6929     def filter_defer_apply_off(self):
6930         self.firewall_driver.filter_defer_apply_off()
6931 
6932     def live_migration(self, context, instance, dest,
6933                        post_method, recover_method, block_migration=False,
6934                        migrate_data=None):
6935         """Spawning live_migration operation for distributing high-load.
6936 
6937         :param context: security context
6938         :param instance:
6939             nova.db.sqlalchemy.models.Instance object
6940             instance object that is migrated.
6941         :param dest: destination host
6942         :param post_method:
6943             post operation method.
6944             expected nova.compute.manager._post_live_migration.
6945         :param recover_method:
6946             recovery method when any exception occurs.
6947             expected nova.compute.manager._rollback_live_migration.
6948         :param block_migration: if true, do block migration.
6949         :param migrate_data: a LibvirtLiveMigrateData object
6950 
6951         """
6952 
6953         # 'dest' will be substituted into 'migration_uri' so ensure
6954         # it does't contain any characters that could be used to
6955         # exploit the URI accepted by libivrt
6956         if not libvirt_utils.is_valid_hostname(dest):
6957             raise exception.InvalidHostname(hostname=dest)
6958 
6959         self._live_migration(context, instance, dest,
6960                              post_method, recover_method, block_migration,
6961                              migrate_data)
6962 
6963     def live_migration_abort(self, instance):
6964         """Aborting a running live-migration.
6965 
6966         :param instance: instance object that is in migration
6967 
6968         """
6969 
6970         guest = self._host.get_guest(instance)
6971         dom = guest._domain
6972 
6973         try:
6974             dom.abortJob()
6975         except libvirt.libvirtError as e:
6976             LOG.error("Failed to cancel migration %s",
6977                     encodeutils.exception_to_unicode(e), instance=instance)
6978             raise
6979 
6980     def _verify_serial_console_is_disabled(self):
6981         if CONF.serial_console.enabled:
6982 
6983             msg = _('Your destination node does not support'
6984                     ' retrieving listen addresses.  In order'
6985                     ' for live migration to work properly you'
6986                     ' must disable serial console.')
6987             raise exception.MigrationError(reason=msg)
6988 
6989     def _live_migration_operation(self, context, instance, dest,
6990                                   block_migration, migrate_data, guest,
6991                                   device_names, bandwidth):
6992         """Invoke the live migration operation
6993 
6994         :param context: security context
6995         :param instance:
6996             nova.db.sqlalchemy.models.Instance object
6997             instance object that is migrated.
6998         :param dest: destination host
6999         :param block_migration: if true, do block migration.
7000         :param migrate_data: a LibvirtLiveMigrateData object
7001         :param guest: the guest domain object
7002         :param device_names: list of device names that are being migrated with
7003             instance
7004         :param bandwidth: MiB/s of bandwidth allowed for the migration at start
7005 
7006         This method is intended to be run in a background thread and will
7007         block that thread until the migration is finished or failed.
7008         """
7009         try:
7010             if migrate_data.block_migration:
7011                 migration_flags = self._block_migration_flags
7012             else:
7013                 migration_flags = self._live_migration_flags
7014 
7015             serial_listen_addr = libvirt_migrate.serial_listen_addr(
7016                 migrate_data)
7017             if not serial_listen_addr:
7018                 # In this context we want to ensure that serial console is
7019                 # disabled on source node. This is because nova couldn't
7020                 # retrieve serial listen address from destination node, so we
7021                 # consider that destination node might have serial console
7022                 # disabled as well.
7023                 self._verify_serial_console_is_disabled()
7024 
7025             # NOTE(aplanas) migrate_uri will have a value only in the
7026             # case that `live_migration_inbound_addr` parameter is
7027             # set, and we propose a non tunneled migration.
7028             migrate_uri = None
7029             if ('target_connect_addr' in migrate_data and
7030                     migrate_data.target_connect_addr is not None):
7031                 dest = migrate_data.target_connect_addr
7032                 if (migration_flags &
7033                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
7034                     migrate_uri = self._migrate_uri(dest)
7035 
7036             new_xml_str = None
7037             if CONF.libvirt.virt_type != "parallels":
7038                 # If the migrate_data has port binding information for the
7039                 # destination host, we need to prepare the guest vif config
7040                 # for the destination before we start migrating the guest.
7041                 get_vif_config = None
7042                 if 'vifs' in migrate_data and migrate_data.vifs:
7043                     # NOTE(mriedem): The vif kwarg must be built on the fly
7044                     # within get_updated_guest_xml based on migrate_data.vifs.
7045                     # We could stash the virt_type from the destination host
7046                     # into LibvirtLiveMigrateData but the host kwarg is a
7047                     # nova.virt.libvirt.host.Host object and is used to check
7048                     # information like libvirt version on the destination.
7049                     # If this becomes a problem, what we could do is get the
7050                     # VIF configs while on the destination host during
7051                     # pre_live_migration() and store those in the
7052                     # LibvirtLiveMigrateData object. For now we just use the
7053                     # source host information for virt_type and
7054                     # host (version) since the conductor live_migrate method
7055                     # _check_compatible_with_source_hypervisor() ensures that
7056                     # the hypervisor types and versions are compatible.
7057                     get_vif_config = functools.partial(
7058                         self.vif_driver.get_config,
7059                         instance=instance,
7060                         image_meta=instance.image_meta,
7061                         inst_type=instance.flavor,
7062                         virt_type=CONF.libvirt.virt_type,
7063                         host=self._host)
7064                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
7065                     # TODO(sahid): It's not a really good idea to pass
7066                     # the method _get_volume_config and we should to find
7067                     # a way to avoid this in future.
7068                     guest, migrate_data, self._get_volume_config,
7069                     get_vif_config=get_vif_config)
7070             params = {
7071                'destination_xml': new_xml_str,
7072                'migrate_disks': device_names,
7073             }
7074             # NOTE(pkoniszewski): Because of precheck which blocks
7075             # tunnelled block live migration with mapped volumes we
7076             # can safely remove migrate_disks when tunnelling is on.
7077             # Otherwise we will block all tunnelled block migrations,
7078             # even when an instance does not have volumes mapped.
7079             # This is because selective disk migration is not
7080             # supported in tunnelled block live migration. Also we
7081             # cannot fallback to migrateToURI2 in this case because of
7082             # bug #1398999
7083             if (migration_flags & libvirt.VIR_MIGRATE_TUNNELLED != 0):
7084                 params.pop('migrate_disks')
7085 
7086             # TODO(sahid): This should be in
7087             # post_live_migration_at_source but no way to retrieve
7088             # ports acquired on the host for the guest at this
7089             # step. Since the domain is going to be removed from
7090             # libvird on source host after migration, we backup the
7091             # serial ports to release them if all went well.
7092             serial_ports = []
7093             if CONF.serial_console.enabled:
7094                 serial_ports = list(self._get_serial_ports_from_guest(guest))
7095 
7096             LOG.debug("About to invoke the migrate API", instance=instance)
7097             guest.migrate(self._live_migration_uri(dest),
7098                           migrate_uri=migrate_uri,
7099                           flags=migration_flags,
7100                           params=params,
7101                           domain_xml=new_xml_str,
7102                           bandwidth=bandwidth)
7103             LOG.debug("Migrate API has completed", instance=instance)
7104 
7105             for hostname, port in serial_ports:
7106                 serial_console.release_port(host=hostname, port=port)
7107         except Exception as e:
7108             with excutils.save_and_reraise_exception():
7109                 LOG.error("Live Migration failure: %s", e, instance=instance)
7110 
7111         # If 'migrateToURI' fails we don't know what state the
7112         # VM instances on each host are in. Possibilities include
7113         #
7114         #  1. src==running, dst==none
7115         #
7116         #     Migration failed & rolled back, or never started
7117         #
7118         #  2. src==running, dst==paused
7119         #
7120         #     Migration started but is still ongoing
7121         #
7122         #  3. src==paused,  dst==paused
7123         #
7124         #     Migration data transfer completed, but switchover
7125         #     is still ongoing, or failed
7126         #
7127         #  4. src==paused,  dst==running
7128         #
7129         #     Migration data transfer completed, switchover
7130         #     happened but cleanup on source failed
7131         #
7132         #  5. src==none,    dst==running
7133         #
7134         #     Migration fully succeeded.
7135         #
7136         # Libvirt will aim to complete any migration operation
7137         # or roll it back. So even if the migrateToURI call has
7138         # returned an error, if the migration was not finished
7139         # libvirt should clean up.
7140         #
7141         # So we take the error raise here with a pinch of salt
7142         # and rely on the domain job info status to figure out
7143         # what really happened to the VM, which is a much more
7144         # reliable indicator.
7145         #
7146         # In particular we need to try very hard to ensure that
7147         # Nova does not "forget" about the guest. ie leaving it
7148         # running on a different host to the one recorded in
7149         # the database, as that would be a serious resource leak
7150 
7151         LOG.debug("Migration operation thread has finished",
7152                   instance=instance)
7153 
7154     def _live_migration_copy_disk_paths(self, context, instance, guest):
7155         '''Get list of disks to copy during migration
7156 
7157         :param context: security context
7158         :param instance: the instance being migrated
7159         :param guest: the Guest instance being migrated
7160 
7161         Get the list of disks to copy during migration.
7162 
7163         :returns: a list of local source paths and a list of device names to
7164             copy
7165         '''
7166 
7167         disk_paths = []
7168         device_names = []
7169         block_devices = []
7170 
7171         if (self._block_migration_flags &
7172                 libvirt.VIR_MIGRATE_TUNNELLED == 0):
7173             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
7174                 context, instance.uuid)
7175             block_device_info = driver.get_block_device_info(instance,
7176                                                              bdm_list)
7177 
7178             block_device_mappings = driver.block_device_info_get_mapping(
7179                 block_device_info)
7180             for bdm in block_device_mappings:
7181                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
7182                 block_devices.append(device_name)
7183 
7184         for dev in guest.get_all_disks():
7185             if dev.readonly or dev.shareable:
7186                 continue
7187             if dev.source_type not in ["file", "block"]:
7188                 continue
7189             if dev.target_dev in block_devices:
7190                 continue
7191             disk_paths.append(dev.source_path)
7192             device_names.append(dev.target_dev)
7193         return (disk_paths, device_names)
7194 
7195     def _live_migration_data_gb(self, instance, disk_paths):
7196         '''Calculate total amount of data to be transferred
7197 
7198         :param instance: the nova.objects.Instance being migrated
7199         :param disk_paths: list of disk paths that are being migrated
7200         with instance
7201 
7202         Calculates the total amount of data that needs to be
7203         transferred during the live migration. The actual
7204         amount copied will be larger than this, due to the
7205         guest OS continuing to dirty RAM while the migration
7206         is taking place. So this value represents the minimal
7207         data size possible.
7208 
7209         :returns: data size to be copied in GB
7210         '''
7211 
7212         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
7213         if ram_gb < 2:
7214             ram_gb = 2
7215 
7216         disk_gb = 0
7217         for path in disk_paths:
7218             try:
7219                 size = os.stat(path).st_size
7220                 size_gb = (size / units.Gi)
7221                 if size_gb < 2:
7222                     size_gb = 2
7223                 disk_gb += size_gb
7224             except OSError as e:
7225                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
7226                             {'disk': path, 'ex': e})
7227                 # Ignore error since we don't want to break
7228                 # the migration monitoring thread operation
7229 
7230         return ram_gb + disk_gb
7231 
7232     def _get_migration_flags(self, is_block_migration):
7233         if is_block_migration:
7234             return self._block_migration_flags
7235         return self._live_migration_flags
7236 
7237     def _live_migration_monitor(self, context, instance, guest,
7238                                 dest, post_method,
7239                                 recover_method, block_migration,
7240                                 migrate_data, finish_event,
7241                                 disk_paths):
7242         on_migration_failure = deque()
7243         data_gb = self._live_migration_data_gb(instance, disk_paths)
7244         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
7245         migration = migrate_data.migration
7246         curdowntime = None
7247 
7248         migration_flags = self._get_migration_flags(
7249                                   migrate_data.block_migration)
7250 
7251         n = 0
7252         start = time.time()
7253         progress_time = start
7254         progress_watermark = None
7255         previous_data_remaining = -1
7256         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
7257         while True:
7258             info = guest.get_job_info()
7259 
7260             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7261                 # Either still running, or failed or completed,
7262                 # lets untangle the mess
7263                 if not finish_event.ready():
7264                     LOG.debug("Operation thread is still running",
7265                               instance=instance)
7266                 else:
7267                     info.type = libvirt_migrate.find_job_type(guest, instance)
7268                     LOG.debug("Fixed incorrect job type to be %d",
7269                               info.type, instance=instance)
7270 
7271             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
7272                 # Migration is not yet started
7273                 LOG.debug("Migration not running yet",
7274                           instance=instance)
7275             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
7276                 # Migration is still running
7277                 #
7278                 # This is where we wire up calls to change live
7279                 # migration status. eg change max downtime, cancel
7280                 # the operation, change max bandwidth
7281                 libvirt_migrate.run_tasks(guest, instance,
7282                                           self.active_migrations,
7283                                           on_migration_failure,
7284                                           migration,
7285                                           is_post_copy_enabled)
7286 
7287                 now = time.time()
7288                 elapsed = now - start
7289 
7290                 if ((progress_watermark is None) or
7291                     (progress_watermark == 0) or
7292                     (progress_watermark > info.data_remaining)):
7293                     progress_watermark = info.data_remaining
7294                     progress_time = now
7295 
7296                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
7297                 completion_timeout = int(
7298                     CONF.libvirt.live_migration_completion_timeout * data_gb)
7299                 if libvirt_migrate.should_abort(instance, now, progress_time,
7300                                                 progress_timeout, elapsed,
7301                                                 completion_timeout,
7302                                                 migration.status):
7303                     try:
7304                         guest.abort_job()
7305                     except libvirt.libvirtError as e:
7306                         LOG.warning("Failed to abort migration %s",
7307                                 encodeutils.exception_to_unicode(e),
7308                                 instance=instance)
7309                         self._clear_empty_migration(instance)
7310                         raise
7311 
7312                 if (is_post_copy_enabled and
7313                     libvirt_migrate.should_switch_to_postcopy(
7314                     info.memory_iteration, info.data_remaining,
7315                     previous_data_remaining, migration.status)):
7316                     libvirt_migrate.trigger_postcopy_switch(guest,
7317                                                             instance,
7318                                                             migration)
7319                 previous_data_remaining = info.data_remaining
7320 
7321                 curdowntime = libvirt_migrate.update_downtime(
7322                     guest, instance, curdowntime,
7323                     downtime_steps, elapsed)
7324 
7325                 # We loop every 500ms, so don't log on every
7326                 # iteration to avoid spamming logs for long
7327                 # running migrations. Just once every 5 secs
7328                 # is sufficient for developers to debug problems.
7329                 # We log once every 30 seconds at info to help
7330                 # admins see slow running migration operations
7331                 # when debug logs are off.
7332                 if (n % 10) == 0:
7333                     # Ignoring memory_processed, as due to repeated
7334                     # dirtying of data, this can be way larger than
7335                     # memory_total. Best to just look at what's
7336                     # remaining to copy and ignore what's done already
7337                     #
7338                     # TODO(berrange) perhaps we could include disk
7339                     # transfer stats in the progress too, but it
7340                     # might make memory info more obscure as large
7341                     # disk sizes might dwarf memory size
7342                     remaining = 100
7343                     if info.memory_total != 0:
7344                         remaining = round(info.memory_remaining *
7345                                           100 / info.memory_total)
7346 
7347                     libvirt_migrate.save_stats(instance, migration,
7348                                                info, remaining)
7349 
7350                     lg = LOG.debug
7351                     if (n % 60) == 0:
7352                         lg = LOG.info
7353 
7354                     lg("Migration running for %(secs)d secs, "
7355                        "memory %(remaining)d%% remaining; "
7356                        "(bytes processed=%(processed_memory)d, "
7357                        "remaining=%(remaining_memory)d, "
7358                        "total=%(total_memory)d)",
7359                        {"secs": n / 2, "remaining": remaining,
7360                         "processed_memory": info.memory_processed,
7361                         "remaining_memory": info.memory_remaining,
7362                         "total_memory": info.memory_total}, instance=instance)
7363                     if info.data_remaining > progress_watermark:
7364                         lg("Data remaining %(remaining)d bytes, "
7365                            "low watermark %(watermark)d bytes "
7366                            "%(last)d seconds ago",
7367                            {"remaining": info.data_remaining,
7368                             "watermark": progress_watermark,
7369                             "last": (now - progress_time)}, instance=instance)
7370 
7371                 n = n + 1
7372             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
7373                 # Migration is all done
7374                 LOG.info("Migration operation has completed",
7375                          instance=instance)
7376                 post_method(context, instance, dest, block_migration,
7377                             migrate_data)
7378                 break
7379             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
7380                 # Migration did not succeed
7381                 LOG.error("Migration operation has aborted", instance=instance)
7382                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7383                                                   on_migration_failure)
7384                 recover_method(context, instance, dest, migrate_data)
7385                 break
7386             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
7387                 # Migration was stopped by admin
7388                 LOG.warning("Migration operation was cancelled",
7389                             instance=instance)
7390                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
7391                                                   on_migration_failure)
7392                 recover_method(context, instance, dest, migrate_data,
7393                                migration_status='cancelled')
7394                 break
7395             else:
7396                 LOG.warning("Unexpected migration job type: %d",
7397                             info.type, instance=instance)
7398 
7399             time.sleep(0.5)
7400         self._clear_empty_migration(instance)
7401 
7402     def _clear_empty_migration(self, instance):
7403         try:
7404             del self.active_migrations[instance.uuid]
7405         except KeyError:
7406             LOG.warning("There are no records in active migrations "
7407                         "for instance", instance=instance)
7408 
7409     def _live_migration(self, context, instance, dest, post_method,
7410                         recover_method, block_migration,
7411                         migrate_data):
7412         """Do live migration.
7413 
7414         :param context: security context
7415         :param instance:
7416             nova.db.sqlalchemy.models.Instance object
7417             instance object that is migrated.
7418         :param dest: destination host
7419         :param post_method:
7420             post operation method.
7421             expected nova.compute.manager._post_live_migration.
7422         :param recover_method:
7423             recovery method when any exception occurs.
7424             expected nova.compute.manager._rollback_live_migration.
7425         :param block_migration: if true, do block migration.
7426         :param migrate_data: a LibvirtLiveMigrateData object
7427 
7428         This fires off a new thread to run the blocking migration
7429         operation, and then this thread monitors the progress of
7430         migration and controls its operation
7431         """
7432 
7433         guest = self._host.get_guest(instance)
7434 
7435         disk_paths = []
7436         device_names = []
7437         if (migrate_data.block_migration and
7438                 CONF.libvirt.virt_type != "parallels"):
7439             disk_paths, device_names = self._live_migration_copy_disk_paths(
7440                 context, instance, guest)
7441 
7442         deadline = CONF.vif_plugging_timeout
7443         if utils.is_neutron() and deadline:
7444             # We don't generate events if CONF.vif_plugging_timeout=0
7445             # meaning that the operator disabled using them.
7446 
7447             # In case of Linux Bridge, the agent is waiting for new
7448             # TAP devices on destination node. They are going to be
7449             # created by libvirt at the very beginning of the
7450             # live-migration process. Then receiving the events from
7451             # Neutron will ensure that everything is configured
7452             # correctly.
7453             events = self._get_neutron_events_for_live_migration(
7454                 instance.get_network_info())
7455         else:
7456             # TODO(sahid): This 'is_neutron()' condition should be
7457             # removed when nova-network will be erased from the tree
7458             # (Rocky).
7459             events = []
7460 
7461         if events:
7462             # We start migration with the minimum bandwidth
7463             # speed. Depending on the VIF type (see:
7464             # _get_neutron_events_for_live_migration) we will wait for
7465             # Neutron to send events that confirm network is setup or
7466             # directly configure QEMU to use the maximun BW allowed.
7467             bandwidth = MIN_MIGRATION_SPEED_BW
7468         else:
7469             bandwidth = CONF.libvirt.live_migration_bandwidth
7470 
7471         try:
7472             error_cb = self._neutron_failed_live_migration_callback
7473             with self.virtapi.wait_for_instance_event(instance, events,
7474                                                       deadline=deadline,
7475                                                       error_callback=error_cb):
7476                 opthread = utils.spawn(self._live_migration_operation,
7477                                        context, instance, dest,
7478                                        block_migration,
7479                                        migrate_data, guest,
7480                                        device_names, bandwidth)
7481         except eventlet.timeout.Timeout:
7482             msg = ('Timeout waiting for VIF plugging events, '
7483                    'canceling migration')
7484             raise exception.MigrationError(reason=msg)
7485         else:
7486             if utils.is_neutron() and events:
7487                 LOG.debug('VIF events received, continuing migration '
7488                           'with max bandwidth configured: %d',
7489                           CONF.libvirt.live_migration_bandwidth,
7490                           instance=instance)
7491                 # Configure QEMU to use the maximum bandwidth allowed.
7492                 guest.migrate_configure_max_speed(
7493                     CONF.libvirt.live_migration_bandwidth)
7494 
7495         finish_event = eventlet.event.Event()
7496         self.active_migrations[instance.uuid] = deque()
7497 
7498         def thread_finished(thread, event):
7499             LOG.debug("Migration operation thread notification",
7500                       instance=instance)
7501             event.send()
7502         opthread.link(thread_finished, finish_event)
7503 
7504         # Let eventlet schedule the new thread right away
7505         time.sleep(0)
7506 
7507         try:
7508             LOG.debug("Starting monitoring of live migration",
7509                       instance=instance)
7510             self._live_migration_monitor(context, instance, guest, dest,
7511                                          post_method, recover_method,
7512                                          block_migration, migrate_data,
7513                                          finish_event, disk_paths)
7514         except Exception as ex:
7515             LOG.warning("Error monitoring migration: %(ex)s",
7516                         {"ex": ex}, instance=instance, exc_info=True)
7517             raise
7518         finally:
7519             LOG.debug("Live migration monitoring is all done",
7520                       instance=instance)
7521 
7522     def _is_post_copy_enabled(self, migration_flags):
7523         if self._is_post_copy_available():
7524             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
7525                 return True
7526         return False
7527 
7528     def live_migration_force_complete(self, instance):
7529         try:
7530             self.active_migrations[instance.uuid].append('force-complete')
7531         except KeyError:
7532             raise exception.NoActiveMigrationForInstance(
7533                 instance_id=instance.uuid)
7534 
7535     def _try_fetch_image(self, context, path, image_id, instance,
7536                          fallback_from_host=None):
7537         try:
7538             libvirt_utils.fetch_image(context, path, image_id,
7539                                       instance.trusted_certs)
7540         except exception.ImageNotFound:
7541             if not fallback_from_host:
7542                 raise
7543             LOG.debug("Image %(image_id)s doesn't exist anymore on "
7544                       "image service, attempting to copy image "
7545                       "from %(host)s",
7546                       {'image_id': image_id, 'host': fallback_from_host})
7547             libvirt_utils.copy_image(src=path, dest=path,
7548                                      host=fallback_from_host,
7549                                      receive=True)
7550 
7551     def _fetch_instance_kernel_ramdisk(self, context, instance,
7552                                        fallback_from_host=None):
7553         """Download kernel and ramdisk for instance in instance directory."""
7554         instance_dir = libvirt_utils.get_instance_path(instance)
7555         if instance.kernel_id:
7556             kernel_path = os.path.join(instance_dir, 'kernel')
7557             # NOTE(dsanders): only fetch image if it's not available at
7558             # kernel_path. This also avoids ImageNotFound exception if
7559             # the image has been deleted from glance
7560             if not os.path.exists(kernel_path):
7561                 self._try_fetch_image(context,
7562                                       kernel_path,
7563                                       instance.kernel_id,
7564                                       instance, fallback_from_host)
7565             if instance.ramdisk_id:
7566                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
7567                 # NOTE(dsanders): only fetch image if it's not available at
7568                 # ramdisk_path. This also avoids ImageNotFound exception if
7569                 # the image has been deleted from glance
7570                 if not os.path.exists(ramdisk_path):
7571                     self._try_fetch_image(context,
7572                                           ramdisk_path,
7573                                           instance.ramdisk_id,
7574                                           instance, fallback_from_host)
7575 
7576     def rollback_live_migration_at_destination(self, context, instance,
7577                                                network_info,
7578                                                block_device_info,
7579                                                destroy_disks=True,
7580                                                migrate_data=None):
7581         """Clean up destination node after a failed live migration."""
7582         try:
7583             self.destroy(context, instance, network_info, block_device_info,
7584                          destroy_disks)
7585         finally:
7586             # NOTE(gcb): Failed block live migration may leave instance
7587             # directory at destination node, ensure it is always deleted.
7588             is_shared_instance_path = True
7589             if migrate_data:
7590                 is_shared_instance_path = migrate_data.is_shared_instance_path
7591                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
7592                     and migrate_data.serial_listen_ports):
7593                     # Releases serial ports reserved.
7594                     for port in migrate_data.serial_listen_ports:
7595                         serial_console.release_port(
7596                             host=migrate_data.serial_listen_addr, port=port)
7597 
7598             if not is_shared_instance_path:
7599                 instance_dir = libvirt_utils.get_instance_path_at_destination(
7600                     instance, migrate_data)
7601                 if os.path.exists(instance_dir):
7602                     shutil.rmtree(instance_dir)
7603 
7604     def _pre_live_migration_plug_vifs(self, instance, network_info,
7605                                       migrate_data):
7606         # We call plug_vifs before the compute manager calls
7607         # ensure_filtering_rules_for_instance, to ensure bridge is set up
7608         # Retry operation is necessary because continuously request comes,
7609         # concurrent request occurs to iptables, then it complains.
7610         if 'vifs' in migrate_data and migrate_data.vifs:
7611             LOG.debug('Plugging VIFs using destination host port bindings '
7612                       'before live migration.', instance=instance)
7613             # Plug VIFs using destination host port binding information.
7614             vif_plug_nw_info = network_model.NetworkInfo([])
7615             for migrate_vif in migrate_data.vifs:
7616                 vif_plug_nw_info.append(migrate_vif.get_dest_vif())
7617         else:
7618             LOG.debug('Plugging VIFs before live migration.',
7619                       instance=instance)
7620             vif_plug_nw_info = network_info
7621         max_retry = CONF.live_migration_retry_count
7622         for cnt in range(max_retry):
7623             try:
7624                 self.plug_vifs(instance, vif_plug_nw_info)
7625                 break
7626             except processutils.ProcessExecutionError:
7627                 if cnt == max_retry - 1:
7628                     raise
7629                 else:
7630                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
7631                                 '%(max_retry)d.',
7632                                 {'cnt': cnt, 'max_retry': max_retry},
7633                                 instance=instance)
7634                     greenthread.sleep(1)
7635 
7636     def pre_live_migration(self, context, instance, block_device_info,
7637                            network_info, disk_info, migrate_data):
7638         """Preparation live migration."""
7639         if disk_info is not None:
7640             disk_info = jsonutils.loads(disk_info)
7641 
7642         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
7643                   instance=instance)
7644         is_shared_block_storage = migrate_data.is_shared_block_storage
7645         is_shared_instance_path = migrate_data.is_shared_instance_path
7646         is_block_migration = migrate_data.block_migration
7647 
7648         if not is_shared_instance_path:
7649             instance_dir = libvirt_utils.get_instance_path_at_destination(
7650                             instance, migrate_data)
7651 
7652             if os.path.exists(instance_dir):
7653                 raise exception.DestinationDiskExists(path=instance_dir)
7654 
7655             LOG.debug('Creating instance directory: %s', instance_dir,
7656                       instance=instance)
7657             os.mkdir(instance_dir)
7658 
7659             # Recreate the disk.info file and in doing so stop the
7660             # imagebackend from recreating it incorrectly by inspecting the
7661             # contents of each file when using the Raw backend.
7662             if disk_info:
7663                 image_disk_info = {}
7664                 for info in disk_info:
7665                     image_file = os.path.basename(info['path'])
7666                     image_path = os.path.join(instance_dir, image_file)
7667                     image_disk_info[image_path] = info['type']
7668 
7669                 LOG.debug('Creating disk.info with the contents: %s',
7670                           image_disk_info, instance=instance)
7671 
7672                 image_disk_info_path = os.path.join(instance_dir,
7673                                                     'disk.info')
7674                 libvirt_utils.write_to_file(image_disk_info_path,
7675                                             jsonutils.dumps(image_disk_info))
7676 
7677             if not is_shared_block_storage:
7678                 # Ensure images and backing files are present.
7679                 LOG.debug('Checking to make sure images and backing files are '
7680                           'present before live migration.', instance=instance)
7681                 self._create_images_and_backing(
7682                     context, instance, instance_dir, disk_info,
7683                     fallback_from_host=instance.host)
7684                 if (configdrive.required_by(instance) and
7685                         CONF.config_drive_format == 'iso9660'):
7686                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
7687                     # drive needs to be copied to destination prior to
7688                     # migration when instance path is not shared and block
7689                     # storage is not shared. Files that are already present
7690                     # on destination are excluded from a list of files that
7691                     # need to be copied to destination. If we don't do that
7692                     # live migration will fail on copying iso config drive to
7693                     # destination and writing to read-only device.
7694                     # Please see bug/1246201 for more details.
7695                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
7696                     self._remotefs.copy_file(src, instance_dir)
7697 
7698             if not is_block_migration:
7699                 # NOTE(angdraug): when block storage is shared between source
7700                 # and destination and instance path isn't (e.g. volume backed
7701                 # or rbd backed instance), instance path on destination has to
7702                 # be prepared
7703 
7704                 # Required by Quobyte CI
7705                 self._ensure_console_log_for_instance(instance)
7706 
7707                 # if image has kernel and ramdisk, just download
7708                 # following normal way.
7709                 self._fetch_instance_kernel_ramdisk(context, instance)
7710 
7711         # Establishing connection to volume server.
7712         block_device_mapping = driver.block_device_info_get_mapping(
7713             block_device_info)
7714 
7715         if len(block_device_mapping):
7716             LOG.debug('Connecting volumes before live migration.',
7717                       instance=instance)
7718 
7719         for bdm in block_device_mapping:
7720             connection_info = bdm['connection_info']
7721             # NOTE(lyarwood): Handle the P to Q LM during upgrade use case
7722             # where an instance has encrypted volumes attached using the
7723             # os-brick encryptors. Do not attempt to attach the encrypted
7724             # volume using native LUKS decryption on the destionation.
7725             src_native_luks = False
7726             if migrate_data.obj_attr_is_set('src_supports_native_luks'):
7727                 src_native_luks = migrate_data.src_supports_native_luks
7728             dest_native_luks = self._is_native_luks_available()
7729             allow_native_luks = src_native_luks and dest_native_luks
7730             self._connect_volume(context, connection_info, instance,
7731                                  allow_native_luks=allow_native_luks)
7732 
7733         self._pre_live_migration_plug_vifs(
7734             instance, network_info, migrate_data)
7735 
7736         # Store server_listen and latest disk device info
7737         if not migrate_data:
7738             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
7739         else:
7740             migrate_data.bdms = []
7741         # Store live_migration_inbound_addr
7742         migrate_data.target_connect_addr = \
7743             CONF.libvirt.live_migration_inbound_addr
7744         migrate_data.supported_perf_events = self._supported_perf_events
7745 
7746         migrate_data.serial_listen_ports = []
7747         if CONF.serial_console.enabled:
7748             num_ports = hardware.get_number_of_serial_ports(
7749                 instance.flavor, instance.image_meta)
7750             for port in six.moves.range(num_ports):
7751                 migrate_data.serial_listen_ports.append(
7752                     serial_console.acquire_port(
7753                         migrate_data.serial_listen_addr))
7754 
7755         for vol in block_device_mapping:
7756             connection_info = vol['connection_info']
7757             if connection_info.get('serial'):
7758                 disk_info = blockinfo.get_info_from_bdm(
7759                     instance, CONF.libvirt.virt_type,
7760                     instance.image_meta, vol)
7761 
7762                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
7763                 bdmi.serial = connection_info['serial']
7764                 bdmi.connection_info = connection_info
7765                 bdmi.bus = disk_info['bus']
7766                 bdmi.dev = disk_info['dev']
7767                 bdmi.type = disk_info['type']
7768                 bdmi.format = disk_info.get('format')
7769                 bdmi.boot_index = disk_info.get('boot_index')
7770                 volume_secret = self._host.find_secret('volume', vol.volume_id)
7771                 if volume_secret:
7772                     bdmi.encryption_secret_uuid = volume_secret.UUIDString()
7773 
7774                 migrate_data.bdms.append(bdmi)
7775 
7776         return migrate_data
7777 
7778     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
7779                                image_id, instance, size,
7780                                fallback_from_host=None):
7781         try:
7782             image.cache(fetch_func=fetch_func,
7783                         context=context,
7784                         filename=filename,
7785                         image_id=image_id,
7786                         size=size,
7787                         trusted_certs=instance.trusted_certs)
7788         except exception.ImageNotFound:
7789             if not fallback_from_host:
7790                 raise
7791             LOG.debug("Image %(image_id)s doesn't exist anymore "
7792                       "on image service, attempting to copy "
7793                       "image from %(host)s",
7794                       {'image_id': image_id, 'host': fallback_from_host},
7795                       instance=instance)
7796 
7797             def copy_from_host(target):
7798                 libvirt_utils.copy_image(src=target,
7799                                          dest=target,
7800                                          host=fallback_from_host,
7801                                          receive=True)
7802             image.cache(fetch_func=copy_from_host,
7803                         filename=filename)
7804 
7805     def _create_images_and_backing(self, context, instance, instance_dir,
7806                                    disk_info, fallback_from_host=None):
7807         """:param context: security context
7808            :param instance:
7809                nova.db.sqlalchemy.models.Instance object
7810                instance object that is migrated.
7811            :param instance_dir:
7812                instance path to use, calculated externally to handle block
7813                migrating an instance with an old style instance path
7814            :param disk_info:
7815                disk info specified in _get_instance_disk_info_from_config
7816                (list of dicts)
7817            :param fallback_from_host:
7818                host where we can retrieve images if the glance images are
7819                not available.
7820 
7821         """
7822 
7823         # Virtuozzo containers don't use backing file
7824         if (CONF.libvirt.virt_type == "parallels" and
7825                 instance.vm_mode == fields.VMMode.EXE):
7826             return
7827 
7828         if not disk_info:
7829             disk_info = []
7830 
7831         for info in disk_info:
7832             base = os.path.basename(info['path'])
7833             # Get image type and create empty disk image, and
7834             # create backing file in case of qcow2.
7835             instance_disk = os.path.join(instance_dir, base)
7836             if not info['backing_file'] and not os.path.exists(instance_disk):
7837                 libvirt_utils.create_image(info['type'], instance_disk,
7838                                            info['virt_disk_size'])
7839             elif info['backing_file']:
7840                 # Creating backing file follows same way as spawning instances.
7841                 cache_name = os.path.basename(info['backing_file'])
7842 
7843                 disk = self.image_backend.by_name(instance, instance_disk,
7844                                                   CONF.libvirt.images_type)
7845                 if cache_name.startswith('ephemeral'):
7846                     # The argument 'size' is used by image.cache to
7847                     # validate disk size retrieved from cache against
7848                     # the instance disk size (should always return OK)
7849                     # and ephemeral_size is used by _create_ephemeral
7850                     # to build the image if the disk is not already
7851                     # cached.
7852                     disk.cache(
7853                         fetch_func=self._create_ephemeral,
7854                         fs_label=cache_name,
7855                         os_type=instance.os_type,
7856                         filename=cache_name,
7857                         size=info['virt_disk_size'],
7858                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7859                 elif cache_name.startswith('swap'):
7860                     inst_type = instance.get_flavor()
7861                     swap_mb = inst_type.swap
7862                     disk.cache(fetch_func=self._create_swap,
7863                                 filename="swap_%s" % swap_mb,
7864                                 size=swap_mb * units.Mi,
7865                                 swap_mb=swap_mb)
7866                 else:
7867                     self._try_fetch_image_cache(disk,
7868                                                 libvirt_utils.fetch_image,
7869                                                 context, cache_name,
7870                                                 instance.image_ref,
7871                                                 instance,
7872                                                 info['virt_disk_size'],
7873                                                 fallback_from_host)
7874 
7875         # if disk has kernel and ramdisk, just download
7876         # following normal way.
7877         self._fetch_instance_kernel_ramdisk(
7878             context, instance, fallback_from_host=fallback_from_host)
7879 
7880     def post_live_migration(self, context, instance, block_device_info,
7881                             migrate_data=None):
7882         # Disconnect from volume server
7883         block_device_mapping = driver.block_device_info_get_mapping(
7884                 block_device_info)
7885         volume_api = self._volume_api
7886         for vol in block_device_mapping:
7887             volume_id = vol['connection_info']['serial']
7888             if vol['attachment_id'] is None:
7889                 # Cinder v2 api flow: Retrieve connection info from Cinder's
7890                 # initialize_connection API. The info returned will be
7891                 # accurate for the source server.
7892                 connector = self.get_volume_connector(instance)
7893                 connection_info = volume_api.initialize_connection(
7894                     context, volume_id, connector)
7895             else:
7896                 # cinder v3.44 api flow: Retrieve the connection_info for
7897                 # the old attachment from cinder.
7898                 old_attachment_id = \
7899                     migrate_data.old_vol_attachment_ids[volume_id]
7900                 old_attachment = volume_api.attachment_get(
7901                     context, old_attachment_id)
7902                 connection_info = old_attachment['connection_info']
7903 
7904             # TODO(leeantho) The following multipath_id logic is temporary
7905             # and will be removed in the future once os-brick is updated
7906             # to handle multipath for drivers in a more efficient way.
7907             # For now this logic is needed to ensure the connection info
7908             # data is correct.
7909 
7910             # Pull out multipath_id from the bdm information. The
7911             # multipath_id can be placed into the connection info
7912             # because it is based off of the volume and will be the
7913             # same on the source and destination hosts.
7914             if 'multipath_id' in vol['connection_info']['data']:
7915                 multipath_id = vol['connection_info']['data']['multipath_id']
7916                 connection_info['data']['multipath_id'] = multipath_id
7917 
7918             self._disconnect_volume(context, connection_info, instance)
7919 
7920     def post_live_migration_at_source(self, context, instance, network_info):
7921         """Unplug VIFs from networks at source.
7922 
7923         :param context: security context
7924         :param instance: instance object reference
7925         :param network_info: instance network information
7926         """
7927         self.unplug_vifs(instance, network_info)
7928 
7929     def post_live_migration_at_destination(self, context,
7930                                            instance,
7931                                            network_info,
7932                                            block_migration=False,
7933                                            block_device_info=None):
7934         """Post operation of live migration at destination host.
7935 
7936         :param context: security context
7937         :param instance:
7938             nova.db.sqlalchemy.models.Instance object
7939             instance object that is migrated.
7940         :param network_info: instance network information
7941         :param block_migration: if true, post operation of block_migration.
7942         """
7943         # The source node set the VIR_MIGRATE_PERSIST_DEST flag when live
7944         # migrating so the guest xml should already be persisted on the
7945         # destination host, so just perform a sanity check to make sure it
7946         # made it as expected.
7947         self._host.get_guest(instance)
7948 
7949     def _get_instance_disk_info_from_config(self, guest_config,
7950                                             block_device_info):
7951         """Get the non-volume disk information from the domain xml
7952 
7953         :param LibvirtConfigGuest guest_config: the libvirt domain config
7954                                                 for the instance
7955         :param dict block_device_info: block device info for BDMs
7956         :returns disk_info: list of dicts with keys:
7957 
7958           * 'type': the disk type (str)
7959           * 'path': the disk path (str)
7960           * 'virt_disk_size': the virtual disk size (int)
7961           * 'backing_file': backing file of a disk image (str)
7962           * 'disk_size': physical disk size (int)
7963           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7964         """
7965         block_device_mapping = driver.block_device_info_get_mapping(
7966             block_device_info)
7967 
7968         volume_devices = set()
7969         for vol in block_device_mapping:
7970             disk_dev = vol['mount_device'].rpartition("/")[2]
7971             volume_devices.add(disk_dev)
7972 
7973         disk_info = []
7974 
7975         if (guest_config.virt_type == 'parallels' and
7976                 guest_config.os_type == fields.VMMode.EXE):
7977             node_type = 'filesystem'
7978         else:
7979             node_type = 'disk'
7980 
7981         for device in guest_config.devices:
7982             if device.root_name != node_type:
7983                 continue
7984             disk_type = device.source_type
7985             if device.root_name == 'filesystem':
7986                 target = device.target_dir
7987                 if device.source_type == 'file':
7988                     path = device.source_file
7989                 elif device.source_type == 'block':
7990                     path = device.source_dev
7991                 else:
7992                     path = None
7993             else:
7994                 target = device.target_dev
7995                 path = device.source_path
7996 
7997             if not path:
7998                 LOG.debug('skipping disk for %s as it does not have a path',
7999                           guest_config.name)
8000                 continue
8001 
8002             if disk_type not in ['file', 'block']:
8003                 LOG.debug('skipping disk because it looks like a volume', path)
8004                 continue
8005 
8006             if target in volume_devices:
8007                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
8008                           'volume', {'path': path, 'target': target})
8009                 continue
8010 
8011             if device.root_name == 'filesystem':
8012                 driver_type = device.driver_type
8013             else:
8014                 driver_type = device.driver_format
8015             # get the real disk size or
8016             # raise a localized error if image is unavailable
8017             if disk_type == 'file':
8018                 if driver_type == 'ploop':
8019                     dk_size = 0
8020                     for dirpath, dirnames, filenames in os.walk(path):
8021                         for f in filenames:
8022                             fp = os.path.join(dirpath, f)
8023                             dk_size += os.path.getsize(fp)
8024                 else:
8025                     dk_size = disk_api.get_allocated_disk_size(path)
8026 
8027                 # NOTE(lyarwood): Fetch the virtual size for all file disks.
8028                 virt_size = disk_api.get_disk_size(path)
8029 
8030             elif disk_type == 'block' and block_device_info:
8031                 # FIXME(lyarwood): There's no reason to use a separate call
8032                 # here, once disk_api uses privsep this should be removed along
8033                 # with the surrounding conditionals to simplify this mess.
8034                 dk_size = lvm.get_volume_size(path)
8035                 # NOTE(lyarwood): As above, we should be using disk_api to
8036                 # fetch the virt-size but can't as it currently runs qemu-img
8037                 # as an unprivileged user, causing a failure for block devices.
8038                 virt_size = dk_size
8039             else:
8040                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
8041                           'determine if volume',
8042                           {'path': path, 'target': target})
8043                 continue
8044 
8045             if driver_type in ("qcow2", "ploop"):
8046                 backing_file = libvirt_utils.get_disk_backing_file(path)
8047                 over_commit_size = int(virt_size) - dk_size
8048             else:
8049                 backing_file = ""
8050                 over_commit_size = 0
8051 
8052             disk_info.append({'type': driver_type,
8053                               'path': path,
8054                               'virt_disk_size': virt_size,
8055                               'backing_file': backing_file,
8056                               'disk_size': dk_size,
8057                               'over_committed_disk_size': over_commit_size})
8058         return disk_info
8059 
8060     def _get_instance_disk_info(self, instance, block_device_info):
8061         try:
8062             guest = self._host.get_guest(instance)
8063             config = guest.get_config()
8064         except libvirt.libvirtError as ex:
8065             error_code = ex.get_error_code()
8066             LOG.warning('Error from libvirt while getting description of '
8067                         '%(instance_name)s: [Error Code %(error_code)s] '
8068                         '%(ex)s',
8069                         {'instance_name': instance.name,
8070                          'error_code': error_code,
8071                          'ex': encodeutils.exception_to_unicode(ex)},
8072                         instance=instance)
8073             raise exception.InstanceNotFound(instance_id=instance.uuid)
8074 
8075         return self._get_instance_disk_info_from_config(config,
8076                                                         block_device_info)
8077 
8078     def get_instance_disk_info(self, instance,
8079                                block_device_info=None):
8080         return jsonutils.dumps(
8081             self._get_instance_disk_info(instance, block_device_info))
8082 
8083     def _get_disk_over_committed_size_total(self):
8084         """Return total over committed disk size for all instances."""
8085         # Disk size that all instance uses : virtual_size - disk_size
8086         disk_over_committed_size = 0
8087         instance_domains = self._host.list_instance_domains(only_running=False)
8088         if not instance_domains:
8089             return disk_over_committed_size
8090 
8091         # Get all instance uuids
8092         instance_uuids = [dom.UUIDString() for dom in instance_domains]
8093         ctx = nova_context.get_admin_context()
8094         # Get instance object list by uuid filter
8095         filters = {'uuid': instance_uuids}
8096         # NOTE(ankit): objects.InstanceList.get_by_filters method is
8097         # getting called twice one is here and another in the
8098         # _update_available_resource method of resource_tracker. Since
8099         # _update_available_resource method is synchronized, there is a
8100         # possibility the instances list retrieved here to calculate
8101         # disk_over_committed_size would differ to the list you would get
8102         # in _update_available_resource method for calculating usages based
8103         # on instance utilization.
8104         local_instance_list = objects.InstanceList.get_by_filters(
8105             ctx, filters, use_slave=True)
8106         # Convert instance list to dictionary with instance uuid as key.
8107         local_instances = {inst.uuid: inst for inst in local_instance_list}
8108 
8109         # Get bdms by instance uuids
8110         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
8111             ctx, instance_uuids)
8112 
8113         for dom in instance_domains:
8114             try:
8115                 guest = libvirt_guest.Guest(dom)
8116                 config = guest.get_config()
8117 
8118                 block_device_info = None
8119                 if guest.uuid in local_instances \
8120                         and (bdms and guest.uuid in bdms):
8121                     # Get block device info for instance
8122                     block_device_info = driver.get_block_device_info(
8123                         local_instances[guest.uuid], bdms[guest.uuid])
8124 
8125                 disk_infos = self._get_instance_disk_info_from_config(
8126                     config, block_device_info)
8127                 if not disk_infos:
8128                     continue
8129 
8130                 for info in disk_infos:
8131                     disk_over_committed_size += int(
8132                         info['over_committed_disk_size'])
8133             except libvirt.libvirtError as ex:
8134                 error_code = ex.get_error_code()
8135                 LOG.warning(
8136                     'Error from libvirt while getting description of '
8137                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
8138                     {'instance_name': guest.name,
8139                      'error_code': error_code,
8140                      'ex': encodeutils.exception_to_unicode(ex)})
8141             except OSError as e:
8142                 if e.errno in (errno.ENOENT, errno.ESTALE):
8143                     LOG.warning('Periodic task is updating the host stat, '
8144                                 'it is trying to get disk %(i_name)s, '
8145                                 'but disk file was removed by concurrent '
8146                                 'operations such as resize.',
8147                                 {'i_name': guest.name})
8148                 elif e.errno == errno.EACCES:
8149                     LOG.warning('Periodic task is updating the host stat, '
8150                                 'it is trying to get disk %(i_name)s, '
8151                                 'but access is denied. It is most likely '
8152                                 'due to a VM that exists on the compute '
8153                                 'node but is not managed by Nova.',
8154                                 {'i_name': guest.name})
8155                 else:
8156                     raise
8157             except exception.VolumeBDMPathNotFound as e:
8158                 LOG.warning('Periodic task is updating the host stats, '
8159                             'it is trying to get disk info for %(i_name)s, '
8160                             'but the backing volume block device was removed '
8161                             'by concurrent operations such as resize. '
8162                             'Error: %(error)s',
8163                             {'i_name': guest.name, 'error': e})
8164             except exception.DiskNotFound:
8165                 with excutils.save_and_reraise_exception() as err_ctxt:
8166                     # If the instance is undergoing a task state transition,
8167                     # like moving to another host or is being deleted, we
8168                     # should ignore this instance and move on.
8169                     if guest.uuid in local_instances:
8170                         inst = local_instances[guest.uuid]
8171                         # bug 1774249 indicated when instance is in RESIZED
8172                         # state it might also can't find back disk
8173                         if (inst.task_state is not None or
8174                             inst.vm_state == vm_states.RESIZED):
8175                             LOG.info('Periodic task is updating the host '
8176                                      'stats; it is trying to get disk info '
8177                                      'for %(i_name)s, but the backing disk '
8178                                      'was removed by a concurrent operation '
8179                                      '(task_state=%(task_state)s) and '
8180                                      '(vm_state=%(vm_state)s)',
8181                                      {'i_name': guest.name,
8182                                       'task_state': inst.task_state,
8183                                       'vm_state': inst.vm_state},
8184                                      instance=inst)
8185                             err_ctxt.reraise = False
8186 
8187             # NOTE(gtt116): give other tasks a chance.
8188             greenthread.sleep(0)
8189         return disk_over_committed_size
8190 
8191     def unfilter_instance(self, instance, network_info):
8192         """See comments of same method in firewall_driver."""
8193         self.firewall_driver.unfilter_instance(instance,
8194                                                network_info=network_info)
8195 
8196     def get_available_nodes(self, refresh=False):
8197         return [self._host.get_hostname()]
8198 
8199     def get_host_cpu_stats(self):
8200         """Return the current CPU state of the host."""
8201         return self._host.get_cpu_stats()
8202 
8203     def get_host_uptime(self):
8204         """Returns the result of calling "uptime"."""
8205         out, err = utils.execute('env', 'LANG=C', 'uptime')
8206         return out
8207 
8208     def manage_image_cache(self, context, all_instances):
8209         """Manage the local cache of images."""
8210         self.image_cache_manager.update(context, all_instances)
8211 
8212     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
8213                                   shared_storage=False):
8214         """Used only for cleanup in case migrate_disk_and_power_off fails."""
8215         try:
8216             if os.path.exists(inst_base_resize):
8217                 shutil.rmtree(inst_base, ignore_errors=True)
8218                 os.rename(inst_base_resize, inst_base)
8219                 if not shared_storage:
8220                     self._remotefs.remove_dir(dest, inst_base)
8221         except Exception:
8222             pass
8223 
8224     def _is_storage_shared_with(self, dest, inst_base):
8225         # NOTE (rmk): There are two methods of determining whether we are
8226         #             on the same filesystem: the source and dest IP are the
8227         #             same, or we create a file on the dest system via SSH
8228         #             and check whether the source system can also see it.
8229         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
8230         #                it will always be shared storage
8231         if CONF.libvirt.images_type == 'rbd':
8232             return True
8233         shared_storage = (dest == self.get_host_ip_addr())
8234         if not shared_storage:
8235             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
8236             tmp_path = os.path.join(inst_base, tmp_file)
8237 
8238             try:
8239                 self._remotefs.create_file(dest, tmp_path)
8240                 if os.path.exists(tmp_path):
8241                     shared_storage = True
8242                     os.unlink(tmp_path)
8243                 else:
8244                     self._remotefs.remove_file(dest, tmp_path)
8245             except Exception:
8246                 pass
8247         return shared_storage
8248 
8249     def migrate_disk_and_power_off(self, context, instance, dest,
8250                                    flavor, network_info,
8251                                    block_device_info=None,
8252                                    timeout=0, retry_interval=0):
8253         LOG.debug("Starting migrate_disk_and_power_off",
8254                    instance=instance)
8255 
8256         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
8257 
8258         # get_bdm_ephemeral_disk_size() will return 0 if the new
8259         # instance's requested block device mapping contain no
8260         # ephemeral devices. However, we still want to check if
8261         # the original instance's ephemeral_gb property was set and
8262         # ensure that the new requested flavor ephemeral size is greater
8263         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
8264                     instance.flavor.ephemeral_gb)
8265 
8266         # Checks if the migration needs a disk resize down.
8267         root_down = flavor.root_gb < instance.flavor.root_gb
8268         ephemeral_down = flavor.ephemeral_gb < eph_size
8269         booted_from_volume = self._is_booted_from_volume(block_device_info)
8270 
8271         if (root_down and not booted_from_volume) or ephemeral_down:
8272             reason = _("Unable to resize disk down.")
8273             raise exception.InstanceFaultRollback(
8274                 exception.ResizeError(reason=reason))
8275 
8276         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
8277         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
8278             reason = _("Migration is not supported for LVM backed instances")
8279             raise exception.InstanceFaultRollback(
8280                 exception.MigrationPreCheckError(reason=reason))
8281 
8282         # copy disks to destination
8283         # rename instance dir to +_resize at first for using
8284         # shared storage for instance dir (eg. NFS).
8285         inst_base = libvirt_utils.get_instance_path(instance)
8286         inst_base_resize = inst_base + "_resize"
8287         shared_storage = self._is_storage_shared_with(dest, inst_base)
8288 
8289         # try to create the directory on the remote compute node
8290         # if this fails we pass the exception up the stack so we can catch
8291         # failures here earlier
8292         if not shared_storage:
8293             try:
8294                 self._remotefs.create_dir(dest, inst_base)
8295             except processutils.ProcessExecutionError as e:
8296                 reason = _("not able to execute ssh command: %s") % e
8297                 raise exception.InstanceFaultRollback(
8298                     exception.ResizeError(reason=reason))
8299 
8300         self.power_off(instance, timeout, retry_interval)
8301 
8302         block_device_mapping = driver.block_device_info_get_mapping(
8303             block_device_info)
8304         for vol in block_device_mapping:
8305             connection_info = vol['connection_info']
8306             self._disconnect_volume(context, connection_info, instance)
8307 
8308         disk_info = self._get_instance_disk_info(instance, block_device_info)
8309 
8310         try:
8311             os.rename(inst_base, inst_base_resize)
8312             # if we are migrating the instance with shared storage then
8313             # create the directory.  If it is a remote node the directory
8314             # has already been created
8315             if shared_storage:
8316                 dest = None
8317                 fileutils.ensure_tree(inst_base)
8318 
8319             on_execute = lambda process: \
8320                 self.job_tracker.add_job(instance, process.pid)
8321             on_completion = lambda process: \
8322                 self.job_tracker.remove_job(instance, process.pid)
8323 
8324             for info in disk_info:
8325                 # assume inst_base == dirname(info['path'])
8326                 img_path = info['path']
8327                 fname = os.path.basename(img_path)
8328                 from_path = os.path.join(inst_base_resize, fname)
8329 
8330                 # We will not copy over the swap disk here, and rely on
8331                 # finish_migration to re-create it for us. This is ok because
8332                 # the OS is shut down, and as recreating a swap disk is very
8333                 # cheap it is more efficient than copying either locally or
8334                 # over the network. This also means we don't have to resize it.
8335                 if fname == 'disk.swap':
8336                     continue
8337 
8338                 compression = info['type'] not in NO_COMPRESSION_TYPES
8339                 libvirt_utils.copy_image(from_path, img_path, host=dest,
8340                                          on_execute=on_execute,
8341                                          on_completion=on_completion,
8342                                          compression=compression)
8343 
8344             # Ensure disk.info is written to the new path to avoid disks being
8345             # reinspected and potentially changing format.
8346             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
8347             if os.path.exists(src_disk_info_path):
8348                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
8349                 libvirt_utils.copy_image(src_disk_info_path,
8350                                          dst_disk_info_path,
8351                                          host=dest, on_execute=on_execute,
8352                                          on_completion=on_completion)
8353         except Exception:
8354             with excutils.save_and_reraise_exception():
8355                 self._cleanup_remote_migration(dest, inst_base,
8356                                                inst_base_resize,
8357                                                shared_storage)
8358 
8359         return jsonutils.dumps(disk_info)
8360 
8361     def _wait_for_running(self, instance):
8362         state = self.get_info(instance).state
8363 
8364         if state == power_state.RUNNING:
8365             LOG.info("Instance running successfully.", instance=instance)
8366             raise loopingcall.LoopingCallDone()
8367 
8368     @staticmethod
8369     def _disk_raw_to_qcow2(path):
8370         """Converts a raw disk to qcow2."""
8371         path_qcow = path + '_qcow'
8372         utils.execute('qemu-img', 'convert', '-f', 'raw',
8373                       '-O', 'qcow2', path, path_qcow)
8374         os.rename(path_qcow, path)
8375 
8376     def finish_migration(self, context, migration, instance, disk_info,
8377                          network_info, image_meta, resize_instance,
8378                          block_device_info=None, power_on=True):
8379         LOG.debug("Starting finish_migration", instance=instance)
8380 
8381         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8382                                                   instance,
8383                                                   image_meta,
8384                                                   block_device_info)
8385         # assume _create_image does nothing if a target file exists.
8386         # NOTE: This has the intended side-effect of fetching a missing
8387         # backing file.
8388         self._create_image(context, instance, block_disk_info['mapping'],
8389                            block_device_info=block_device_info,
8390                            ignore_bdi_for_swap=True,
8391                            fallback_from_host=migration.source_compute)
8392 
8393         # Required by Quobyte CI
8394         self._ensure_console_log_for_instance(instance)
8395 
8396         gen_confdrive = functools.partial(
8397             self._create_configdrive, context, instance,
8398             InjectionInfo(admin_pass=None, network_info=network_info,
8399                           files=None))
8400 
8401         # Convert raw disks to qcow2 if migrating to host which uses
8402         # qcow2 from host which uses raw.
8403         disk_info = jsonutils.loads(disk_info)
8404         for info in disk_info:
8405             path = info['path']
8406             disk_name = os.path.basename(path)
8407 
8408             # NOTE(mdbooth): The code below looks wrong, but is actually
8409             # required to prevent a security hole when migrating from a host
8410             # with use_cow_images=False to one with use_cow_images=True.
8411             # Imagebackend uses use_cow_images to select between the
8412             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
8413             # writes to disk.info, but does not read it as it assumes qcow2.
8414             # Therefore if we don't convert raw to qcow2 here, a raw disk will
8415             # be incorrectly assumed to be qcow2, which is a severe security
8416             # flaw. The reverse is not true, because the atrociously-named-Raw
8417             # backend supports both qcow2 and raw disks, and will choose
8418             # appropriately between them as long as disk.info exists and is
8419             # correctly populated, which it is because Qcow2 writes to
8420             # disk.info.
8421             #
8422             # In general, we do not yet support format conversion during
8423             # migration. For example:
8424             #   * Converting from use_cow_images=True to use_cow_images=False
8425             #     isn't handled. This isn't a security bug, but is almost
8426             #     certainly buggy in other cases, as the 'Raw' backend doesn't
8427             #     expect a backing file.
8428             #   * Converting to/from lvm and rbd backends is not supported.
8429             #
8430             # This behaviour is inconsistent, and therefore undesirable for
8431             # users. It is tightly-coupled to implementation quirks of 2
8432             # out of 5 backends in imagebackend and defends against a severe
8433             # security flaw which is not at all obvious without deep analysis,
8434             # and is therefore undesirable to developers. We should aim to
8435             # remove it. This will not be possible, though, until we can
8436             # represent the storage layout of a specific instance
8437             # independent of the default configuration of the local compute
8438             # host.
8439 
8440             # Config disks are hard-coded to be raw even when
8441             # use_cow_images=True (see _get_disk_config_image_type),so don't
8442             # need to be converted.
8443             if (disk_name != 'disk.config' and
8444                         info['type'] == 'raw' and CONF.use_cow_images):
8445                 self._disk_raw_to_qcow2(info['path'])
8446 
8447         xml = self._get_guest_xml(context, instance, network_info,
8448                                   block_disk_info, image_meta,
8449                                   block_device_info=block_device_info)
8450         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
8451         # or not we've migrated to another host, because we unplug VIFs locally
8452         # and the status change in the port might go undetected by the neutron
8453         # L2 agent (or neutron server) so neutron may not know that the VIF was
8454         # unplugged in the first place and never send an event.
8455         guest = self._create_domain_and_network(context, xml, instance,
8456                                         network_info,
8457                                         block_device_info=block_device_info,
8458                                         power_on=power_on,
8459                                         vifs_already_plugged=True,
8460                                         post_xml_callback=gen_confdrive)
8461         if power_on:
8462             timer = loopingcall.FixedIntervalLoopingCall(
8463                                                     self._wait_for_running,
8464                                                     instance)
8465             timer.start(interval=0.5).wait()
8466 
8467             # Sync guest time after migration.
8468             guest.sync_guest_time()
8469 
8470         LOG.debug("finish_migration finished successfully.", instance=instance)
8471 
8472     def _cleanup_failed_migration(self, inst_base):
8473         """Make sure that a failed migrate doesn't prevent us from rolling
8474         back in a revert.
8475         """
8476         try:
8477             shutil.rmtree(inst_base)
8478         except OSError as e:
8479             if e.errno != errno.ENOENT:
8480                 raise
8481 
8482     def finish_revert_migration(self, context, instance, network_info,
8483                                 block_device_info=None, power_on=True):
8484         LOG.debug("Starting finish_revert_migration",
8485                   instance=instance)
8486 
8487         inst_base = libvirt_utils.get_instance_path(instance)
8488         inst_base_resize = inst_base + "_resize"
8489 
8490         # NOTE(danms): if we're recovering from a failed migration,
8491         # make sure we don't have a left-over same-host base directory
8492         # that would conflict. Also, don't fail on the rename if the
8493         # failure happened early.
8494         if os.path.exists(inst_base_resize):
8495             self._cleanup_failed_migration(inst_base)
8496             os.rename(inst_base_resize, inst_base)
8497 
8498         root_disk = self.image_backend.by_name(instance, 'disk')
8499         # Once we rollback, the snapshot is no longer needed, so remove it
8500         if root_disk.exists():
8501             root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
8502             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
8503 
8504         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
8505                                             instance,
8506                                             instance.image_meta,
8507                                             block_device_info)
8508         xml = self._get_guest_xml(context, instance, network_info, disk_info,
8509                                   instance.image_meta,
8510                                   block_device_info=block_device_info)
8511         self._create_domain_and_network(context, xml, instance, network_info,
8512                                         block_device_info=block_device_info,
8513                                         power_on=power_on,
8514                                         vifs_already_plugged=True)
8515 
8516         if power_on:
8517             timer = loopingcall.FixedIntervalLoopingCall(
8518                                                     self._wait_for_running,
8519                                                     instance)
8520             timer.start(interval=0.5).wait()
8521 
8522         LOG.debug("finish_revert_migration finished successfully.",
8523                   instance=instance)
8524 
8525     def confirm_migration(self, context, migration, instance, network_info):
8526         """Confirms a resize, destroying the source VM."""
8527         self._cleanup_resize(context, instance, network_info)
8528 
8529     @staticmethod
8530     def _get_io_devices(xml_doc):
8531         """get the list of io devices from the xml document."""
8532         result = {"volumes": [], "ifaces": []}
8533         try:
8534             doc = etree.fromstring(xml_doc)
8535         except Exception:
8536             return result
8537         blocks = [('./devices/disk', 'volumes'),
8538             ('./devices/interface', 'ifaces')]
8539         for block, key in blocks:
8540             section = doc.findall(block)
8541             for node in section:
8542                 for child in node.getchildren():
8543                     if child.tag == 'target' and child.get('dev'):
8544                         result[key].append(child.get('dev'))
8545         return result
8546 
8547     def get_diagnostics(self, instance):
8548         guest = self._host.get_guest(instance)
8549 
8550         # TODO(sahid): We are converting all calls from a
8551         # virDomain object to use nova.virt.libvirt.Guest.
8552         # We should be able to remove domain at the end.
8553         domain = guest._domain
8554         output = {}
8555         # get cpu time, might launch an exception if the method
8556         # is not supported by the underlying hypervisor being
8557         # used by libvirt
8558         try:
8559             for vcpu in guest.get_vcpus_info():
8560                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
8561         except libvirt.libvirtError:
8562             pass
8563         # get io status
8564         xml = guest.get_xml_desc()
8565         dom_io = LibvirtDriver._get_io_devices(xml)
8566         for guest_disk in dom_io["volumes"]:
8567             try:
8568                 # blockStats might launch an exception if the method
8569                 # is not supported by the underlying hypervisor being
8570                 # used by libvirt
8571                 stats = domain.blockStats(guest_disk)
8572                 output[guest_disk + "_read_req"] = stats[0]
8573                 output[guest_disk + "_read"] = stats[1]
8574                 output[guest_disk + "_write_req"] = stats[2]
8575                 output[guest_disk + "_write"] = stats[3]
8576                 output[guest_disk + "_errors"] = stats[4]
8577             except libvirt.libvirtError:
8578                 pass
8579         for interface in dom_io["ifaces"]:
8580             try:
8581                 # interfaceStats might launch an exception if the method
8582                 # is not supported by the underlying hypervisor being
8583                 # used by libvirt
8584                 stats = domain.interfaceStats(interface)
8585                 output[interface + "_rx"] = stats[0]
8586                 output[interface + "_rx_packets"] = stats[1]
8587                 output[interface + "_rx_errors"] = stats[2]
8588                 output[interface + "_rx_drop"] = stats[3]
8589                 output[interface + "_tx"] = stats[4]
8590                 output[interface + "_tx_packets"] = stats[5]
8591                 output[interface + "_tx_errors"] = stats[6]
8592                 output[interface + "_tx_drop"] = stats[7]
8593             except libvirt.libvirtError:
8594                 pass
8595         output["memory"] = domain.maxMemory()
8596         # memoryStats might launch an exception if the method
8597         # is not supported by the underlying hypervisor being
8598         # used by libvirt
8599         try:
8600             mem = domain.memoryStats()
8601             for key in mem.keys():
8602                 output["memory-" + key] = mem[key]
8603         except (libvirt.libvirtError, AttributeError):
8604             pass
8605         return output
8606 
8607     def get_instance_diagnostics(self, instance):
8608         guest = self._host.get_guest(instance)
8609 
8610         # TODO(sahid): We are converting all calls from a
8611         # virDomain object to use nova.virt.libvirt.Guest.
8612         # We should be able to remove domain at the end.
8613         domain = guest._domain
8614 
8615         xml = guest.get_xml_desc()
8616         xml_doc = etree.fromstring(xml)
8617 
8618         # TODO(sahid): Needs to use get_info but more changes have to
8619         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
8620         # needed.
8621         (state, max_mem, mem, num_cpu, cpu_time) = \
8622             guest._get_domain_info(self._host)
8623         config_drive = configdrive.required_by(instance)
8624         launched_at = timeutils.normalize_time(instance.launched_at)
8625         uptime = timeutils.delta_seconds(launched_at,
8626                                          timeutils.utcnow())
8627         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
8628                                         driver='libvirt',
8629                                         config_drive=config_drive,
8630                                         hypervisor=CONF.libvirt.virt_type,
8631                                         hypervisor_os='linux',
8632                                         uptime=uptime)
8633         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
8634             maximum=max_mem / units.Mi,
8635             used=mem / units.Mi)
8636 
8637         # get cpu time, might launch an exception if the method
8638         # is not supported by the underlying hypervisor being
8639         # used by libvirt
8640         try:
8641             for vcpu in guest.get_vcpus_info():
8642                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
8643         except libvirt.libvirtError:
8644             pass
8645         # get io status
8646         dom_io = LibvirtDriver._get_io_devices(xml)
8647         for guest_disk in dom_io["volumes"]:
8648             try:
8649                 # blockStats might launch an exception if the method
8650                 # is not supported by the underlying hypervisor being
8651                 # used by libvirt
8652                 stats = domain.blockStats(guest_disk)
8653                 diags.add_disk(read_bytes=stats[1],
8654                                read_requests=stats[0],
8655                                write_bytes=stats[3],
8656                                write_requests=stats[2],
8657                                errors_count=stats[4])
8658             except libvirt.libvirtError:
8659                 pass
8660         for interface in dom_io["ifaces"]:
8661             try:
8662                 # interfaceStats might launch an exception if the method
8663                 # is not supported by the underlying hypervisor being
8664                 # used by libvirt
8665                 stats = domain.interfaceStats(interface)
8666                 diags.add_nic(rx_octets=stats[0],
8667                               rx_errors=stats[2],
8668                               rx_drop=stats[3],
8669                               rx_packets=stats[1],
8670                               tx_octets=stats[4],
8671                               tx_errors=stats[6],
8672                               tx_drop=stats[7],
8673                               tx_packets=stats[5])
8674             except libvirt.libvirtError:
8675                 pass
8676 
8677         # Update mac addresses of interface if stats have been reported
8678         if diags.nic_details:
8679             nodes = xml_doc.findall('./devices/interface/mac')
8680             for index, node in enumerate(nodes):
8681                 diags.nic_details[index].mac_address = node.get('address')
8682         return diags
8683 
8684     @staticmethod
8685     def _prepare_device_bus(dev):
8686         """Determines the device bus and its hypervisor assigned address
8687         """
8688         bus = None
8689         address = (dev.device_addr.format_address() if
8690                    dev.device_addr else None)
8691         if isinstance(dev.device_addr,
8692                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
8693             bus = objects.PCIDeviceBus()
8694         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8695             if dev.target_bus == 'scsi':
8696                 bus = objects.SCSIDeviceBus()
8697             elif dev.target_bus == 'ide':
8698                 bus = objects.IDEDeviceBus()
8699             elif dev.target_bus == 'usb':
8700                 bus = objects.USBDeviceBus()
8701         if address is not None and bus is not None:
8702             bus.address = address
8703         return bus
8704 
8705     def _build_interface_metadata(self, dev, vifs_to_expose, vlans_by_mac,
8706                                   trusted_by_mac):
8707         """Builds a metadata object for a network interface
8708 
8709         :param dev: The LibvirtConfigGuestInterface to build metadata for.
8710         :param vifs_to_expose: The list of tagged and/or vlan'ed
8711                                VirtualInterface objects.
8712         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
8713         :param trusted_by_mac: A dictionary of mac address -> vf_trusted
8714                                associations.
8715         :return: A NetworkInterfaceMetadata object, or None.
8716         """
8717         vif = vifs_to_expose.get(dev.mac_addr)
8718         if not vif:
8719             LOG.debug('No VIF found with MAC %s, not building metadata',
8720                       dev.mac_addr)
8721             return None
8722         bus = self._prepare_device_bus(dev)
8723         device = objects.NetworkInterfaceMetadata(mac=vif.address)
8724         if 'tag' in vif and vif.tag:
8725             device.tags = [vif.tag]
8726         if bus:
8727             device.bus = bus
8728         vlan = vlans_by_mac.get(vif.address)
8729         if vlan:
8730             device.vlan = int(vlan)
8731         device.vf_trusted = trusted_by_mac.get(vif.address, False)
8732         return device
8733 
8734     def _build_disk_metadata(self, dev, tagged_bdms):
8735         """Builds a metadata object for a disk
8736 
8737         :param dev: The vconfig.LibvirtConfigGuestDisk to build metadata for.
8738         :param tagged_bdms: The list of tagged BlockDeviceMapping objects.
8739         :return: A DiskMetadata object, or None.
8740         """
8741         bdm = tagged_bdms.get(dev.target_dev)
8742         if not bdm:
8743             LOG.debug('No BDM found with device name %s, not building '
8744                       'metadata.', dev.target_dev)
8745             return None
8746         bus = self._prepare_device_bus(dev)
8747         device = objects.DiskMetadata(tags=[bdm.tag])
8748         # NOTE(artom) Setting the serial (which corresponds to
8749         # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
8750         # find the disks's BlockDeviceMapping object when we detach the
8751         # volume and want to clean up its metadata.
8752         device.serial = bdm.volume_id
8753         if bus:
8754             device.bus = bus
8755         return device
8756 
8757     def _build_hostdev_metadata(self, dev, vifs_to_expose, vlans_by_mac):
8758         """Builds a metadata object for a hostdev. This can only be a PF, so we
8759         don't need trusted_by_mac like in _build_interface_metadata because
8760         only VFs can be trusted.
8761 
8762         :param dev: The LibvirtConfigGuestHostdevPCI to build metadata for.
8763         :param vifs_to_expose: The list of tagged and/or vlan'ed
8764                                VirtualInterface objects.
8765         :param vlans_by_mac: A dictionary of mac address -> vlan associations.
8766         :return: A NetworkInterfaceMetadata object, or None.
8767         """
8768         # Strip out the leading '0x'
8769         pci_address = pci_utils.get_pci_address(
8770             *[x[2:] for x in (dev.domain, dev.bus, dev.slot, dev.function)])
8771         try:
8772             mac = pci_utils.get_mac_by_pci_address(pci_address,
8773                                                    pf_interface=True)
8774         except exception.PciDeviceNotFoundById:
8775             LOG.debug('Not exposing metadata for not found PCI device %s',
8776                       pci_address)
8777             return None
8778 
8779         vif = vifs_to_expose.get(mac)
8780         if not vif:
8781             LOG.debug('No VIF found with MAC %s, not building metadata', mac)
8782             return None
8783 
8784         device = objects.NetworkInterfaceMetadata(mac=mac)
8785         device.bus = objects.PCIDeviceBus(address=pci_address)
8786         if 'tag' in vif and vif.tag:
8787             device.tags = [vif.tag]
8788         vlan = vlans_by_mac.get(mac)
8789         if vlan:
8790             device.vlan = int(vlan)
8791         return device
8792 
8793     def _build_device_metadata(self, context, instance):
8794         """Builds a metadata object for instance devices, that maps the user
8795            provided tag to the hypervisor assigned device address.
8796         """
8797         def _get_device_name(bdm):
8798             return block_device.strip_dev(bdm.device_name)
8799 
8800         network_info = instance.info_cache.network_info
8801         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
8802         trusted_by_mac = netutils.get_cached_vifs_with_trusted(network_info)
8803         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
8804                                                                  instance.uuid)
8805         vifs_to_expose = {vif.address: vif for vif in vifs
8806                           if ('tag' in vif and vif.tag) or
8807                              vlans_by_mac.get(vif.address)}
8808         # TODO(mriedem): We should be able to avoid the DB query here by using
8809         # block_device_info['block_device_mapping'] which is passed into most
8810         # methods that call this function.
8811         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8812             context, instance.uuid)
8813         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
8814 
8815         devices = []
8816         guest = self._host.get_guest(instance)
8817         xml = guest.get_xml_desc()
8818         xml_dom = etree.fromstring(xml)
8819         guest_config = vconfig.LibvirtConfigGuest()
8820         guest_config.parse_dom(xml_dom)
8821 
8822         for dev in guest_config.devices:
8823             device = None
8824             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
8825                 device = self._build_interface_metadata(dev, vifs_to_expose,
8826                                                         vlans_by_mac,
8827                                                         trusted_by_mac)
8828             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8829                 device = self._build_disk_metadata(dev, tagged_bdms)
8830             if isinstance(dev, vconfig.LibvirtConfigGuestHostdevPCI):
8831                 device = self._build_hostdev_metadata(dev, vifs_to_expose,
8832                                                       vlans_by_mac)
8833             if device:
8834                 devices.append(device)
8835         if devices:
8836             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
8837             return dev_meta
8838 
8839     def instance_on_disk(self, instance):
8840         # ensure directories exist and are writable
8841         instance_path = libvirt_utils.get_instance_path(instance)
8842         LOG.debug('Checking instance files accessibility %s', instance_path,
8843                   instance=instance)
8844         shared_instance_path = os.access(instance_path, os.W_OK)
8845         # NOTE(flwang): For shared block storage scenario, the file system is
8846         # not really shared by the two hosts, but the volume of evacuated
8847         # instance is reachable.
8848         shared_block_storage = (self.image_backend.backend().
8849                                 is_shared_block_storage())
8850         return shared_instance_path or shared_block_storage
8851 
8852     def inject_network_info(self, instance, nw_info):
8853         self.firewall_driver.setup_basic_filtering(instance, nw_info)
8854 
8855     def delete_instance_files(self, instance):
8856         target = libvirt_utils.get_instance_path(instance)
8857         # A resize may be in progress
8858         target_resize = target + '_resize'
8859         # Other threads may attempt to rename the path, so renaming the path
8860         # to target + '_del' (because it is atomic) and iterating through
8861         # twice in the unlikely event that a concurrent rename occurs between
8862         # the two rename attempts in this method. In general this method
8863         # should be fairly thread-safe without these additional checks, since
8864         # other operations involving renames are not permitted when the task
8865         # state is not None and the task state should be set to something
8866         # other than None by the time this method is invoked.
8867         target_del = target + '_del'
8868         for i in range(2):
8869             try:
8870                 os.rename(target, target_del)
8871                 break
8872             except Exception:
8873                 pass
8874             try:
8875                 os.rename(target_resize, target_del)
8876                 break
8877             except Exception:
8878                 pass
8879         # Either the target or target_resize path may still exist if all
8880         # rename attempts failed.
8881         remaining_path = None
8882         for p in (target, target_resize):
8883             if os.path.exists(p):
8884                 remaining_path = p
8885                 break
8886 
8887         # A previous delete attempt may have been interrupted, so target_del
8888         # may exist even if all rename attempts during the present method
8889         # invocation failed due to the absence of both target and
8890         # target_resize.
8891         if not remaining_path and os.path.exists(target_del):
8892             self.job_tracker.terminate_jobs(instance)
8893 
8894             LOG.info('Deleting instance files %s', target_del,
8895                      instance=instance)
8896             remaining_path = target_del
8897             try:
8898                 shutil.rmtree(target_del)
8899             except OSError as e:
8900                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8901                           {'target': target_del, 'e': e}, instance=instance)
8902 
8903         # It is possible that the delete failed, if so don't mark the instance
8904         # as cleaned.
8905         if remaining_path and os.path.exists(remaining_path):
8906             LOG.info('Deletion of %s failed', remaining_path,
8907                      instance=instance)
8908             return False
8909 
8910         LOG.info('Deletion of %s complete', target_del, instance=instance)
8911         return True
8912 
8913     @property
8914     def need_legacy_block_device_info(self):
8915         return False
8916 
8917     def default_root_device_name(self, instance, image_meta, root_bdm):
8918         disk_bus = blockinfo.get_disk_bus_for_device_type(
8919             instance, CONF.libvirt.virt_type, image_meta, "disk")
8920         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8921             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8922         root_info = blockinfo.get_root_info(
8923             instance, CONF.libvirt.virt_type, image_meta,
8924             root_bdm, disk_bus, cdrom_bus)
8925         return block_device.prepend_dev(root_info['dev'])
8926 
8927     def default_device_names_for_instance(self, instance, root_device_name,
8928                                           *block_device_lists):
8929         block_device_mapping = list(itertools.chain(*block_device_lists))
8930         # NOTE(ndipanov): Null out the device names so that blockinfo code
8931         #                 will assign them
8932         for bdm in block_device_mapping:
8933             if bdm.device_name is not None:
8934                 LOG.warning(
8935                     "Ignoring supplied device name: %(device_name)s. "
8936                     "Libvirt can't honour user-supplied dev names",
8937                     {'device_name': bdm.device_name}, instance=instance)
8938                 bdm.device_name = None
8939         block_device_info = driver.get_block_device_info(instance,
8940                                                          block_device_mapping)
8941 
8942         blockinfo.default_device_names(CONF.libvirt.virt_type,
8943                                        nova_context.get_admin_context(),
8944                                        instance,
8945                                        block_device_info,
8946                                        instance.image_meta)
8947 
8948     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8949         block_device_info = driver.get_block_device_info(instance, bdms)
8950         instance_info = blockinfo.get_disk_info(
8951                 CONF.libvirt.virt_type, instance,
8952                 instance.image_meta, block_device_info=block_device_info)
8953 
8954         suggested_dev_name = block_device_obj.device_name
8955         if suggested_dev_name is not None:
8956             LOG.warning(
8957                 'Ignoring supplied device name: %(suggested_dev)s',
8958                 {'suggested_dev': suggested_dev_name}, instance=instance)
8959 
8960         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8961         #                 only when it's actually not set on the bd object
8962         block_device_obj.device_name = None
8963         disk_info = blockinfo.get_info_from_bdm(
8964             instance, CONF.libvirt.virt_type, instance.image_meta,
8965             block_device_obj, mapping=instance_info['mapping'])
8966         return block_device.prepend_dev(disk_info['dev'])
8967 
8968     def is_supported_fs_format(self, fs_type):
8969         return fs_type in [nova.privsep.fs.FS_FORMAT_EXT2,
8970                            nova.privsep.fs.FS_FORMAT_EXT3,
8971                            nova.privsep.fs.FS_FORMAT_EXT4,
8972                            nova.privsep.fs.FS_FORMAT_XFS]
8973 
8974     def _get_cpu_traits(self):
8975         """Get CPU traits of VMs based on guest CPU model config:
8976         1. if mode is 'host-model' or 'host-passthrough', use host's
8977         CPU features.
8978         2. if mode is None, choose a default CPU model based on CPU
8979         architecture.
8980         3. if mode is 'custom', use cpu_model to generate CPU features.
8981         The code also accounts for cpu_model_extra_flags configuration when
8982         cpu_mode is 'host-model', 'host-passthrough' or 'custom', this
8983         ensures user specified CPU feature flags to be included.
8984         :return: A dict of trait names mapped to boolean values or None.
8985         """
8986         cpu = self._get_guest_cpu_model_config()
8987         if not cpu:
8988             LOG.info('The current libvirt hypervisor %(virt_type)s '
8989                      'does not support reporting CPU traits.',
8990                      {'virt_type': CONF.libvirt.virt_type})
8991             return
8992 
8993         caps = deepcopy(self._host.get_capabilities())
8994         if cpu.mode in ('host-model', 'host-passthrough'):
8995             # Account for features in cpu_model_extra_flags conf
8996             host_features = [f.name for f in
8997                              caps.host.cpu.features | cpu.features]
8998             return libvirt_utils.cpu_features_to_traits(host_features)
8999 
9000         # Choose a default CPU model when cpu_mode is not specified
9001         if cpu.mode is None:
9002             caps.host.cpu.model = libvirt_utils.get_cpu_model_from_arch(
9003                 caps.host.cpu.arch)
9004             caps.host.cpu.features = set()
9005         else:
9006             # For custom mode, set model to guest CPU model
9007             caps.host.cpu.model = cpu.model
9008             caps.host.cpu.features = set()
9009             # Account for features in cpu_model_extra_flags conf
9010             for f in cpu.features:
9011                 caps.host.cpu.add_feature(
9012                     vconfig.LibvirtConfigCPUFeature(name=f.name))
9013 
9014         xml_str = caps.host.cpu.to_xml()
9015         LOG.info("Libvirt baseline CPU %s", xml_str)
9016         # TODO(lei-zh): baselineCPU is not supported on all platforms.
9017         # There is some work going on in the libvirt community to replace the
9018         # baseline call. Consider using the new apis when they are ready. See
9019         # https://www.redhat.com/archives/libvir-list/2018-May/msg01204.html.
9020         try:
9021             if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
9022                 features = self._host.get_connection().baselineCPU(
9023                     [xml_str],
9024                     libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
9025             else:
9026                 features = self._host.get_connection().baselineCPU([xml_str])
9027         except libvirt.libvirtError as ex:
9028             with excutils.save_and_reraise_exception() as ctxt:
9029                 error_code = ex.get_error_code()
9030                 if error_code == libvirt.VIR_ERR_NO_SUPPORT:
9031                     ctxt.reraise = False
9032                     LOG.info('URI %(uri)s does not support full set'
9033                              ' of host capabilities: %(error)s',
9034                              {'uri': self._host._uri, 'error': ex})
9035                     return libvirt_utils.cpu_features_to_traits([])
9036 
9037         cpu.parse_str(features)
9038         return libvirt_utils.cpu_features_to_traits(
9039             [f.name for f in cpu.features])
