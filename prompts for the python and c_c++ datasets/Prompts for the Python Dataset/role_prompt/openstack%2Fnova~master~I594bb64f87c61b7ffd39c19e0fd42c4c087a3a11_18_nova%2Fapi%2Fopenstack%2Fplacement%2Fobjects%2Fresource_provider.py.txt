I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
2 #    not use this file except in compliance with the License. You may obtain
3 #    a copy of the License at
4 #
5 #         http://www.apache.org/licenses/LICENSE-2.0
6 #
7 #    Unless required by applicable law or agreed to in writing, software
8 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
9 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
10 #    License for the specific language governing permissions and limitations
11 #    under the License.
12 
13 import collections
14 import copy
15 import itertools
16 import random
17 
18 # NOTE(cdent): The resource provider objects are designed to never be
19 # used over RPC. Remote manipulation is done with the placement HTTP
20 # API. The 'remotable' decorators should not be used, the objects should
21 # not be registered and there is no need to express VERSIONs nor handle
22 # obj_make_compatible.
23 
24 import os_traits
25 from oslo_concurrency import lockutils
26 from oslo_config import cfg
27 from oslo_db import api as oslo_db_api
28 from oslo_db import exception as db_exc
29 from oslo_log import log as logging
30 from oslo_utils import encodeutils
31 from oslo_versionedobjects import base
32 from oslo_versionedobjects import fields
33 import six
34 import sqlalchemy as sa
35 from sqlalchemy import exc as sqla_exc
36 from sqlalchemy import func
37 from sqlalchemy import sql
38 from sqlalchemy.sql import null
39 
40 from nova.api.openstack.placement import db_api
41 from nova.api.openstack.placement import exception
42 from nova.api.openstack.placement.objects import consumer as consumer_obj
43 from nova.api.openstack.placement.objects import project as project_obj
44 from nova.api.openstack.placement.objects import user as user_obj
45 from nova.api.openstack.placement import resource_class_cache as rc_cache
46 from nova.db.sqlalchemy import api_models as models
47 from nova.i18n import _
48 from nova import rc_fields
49 
50 _TRAIT_TBL = models.Trait.__table__
51 _ALLOC_TBL = models.Allocation.__table__
52 _INV_TBL = models.Inventory.__table__
53 _RP_TBL = models.ResourceProvider.__table__
54 # Not used in this file but used in tests.
55 _RC_TBL = models.ResourceClass.__table__
56 _AGG_TBL = models.PlacementAggregate.__table__
57 _RP_AGG_TBL = models.ResourceProviderAggregate.__table__
58 _RP_TRAIT_TBL = models.ResourceProviderTrait.__table__
59 _PROJECT_TBL = models.Project.__table__
60 _USER_TBL = models.User.__table__
61 _CONSUMER_TBL = models.Consumer.__table__
62 _RC_CACHE = None
63 _TRAIT_LOCK = 'trait_sync'
64 _TRAITS_SYNCED = False
65 
66 CONF = cfg.CONF
67 LOG = logging.getLogger(__name__)
68 
69 
70 @db_api.placement_context_manager.reader
71 def ensure_rc_cache(ctx):
72     """Ensures that a singleton resource class cache has been created in the
73     module's scope.
74 
75     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
76                 connection.
77     """
78     global _RC_CACHE
79     if _RC_CACHE is not None:
80         return
81     _RC_CACHE = rc_cache.ResourceClassCache(ctx)
82 
83 
84 @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
85 # Bug #1760322: If the caller raises an exception, we don't want the trait
86 # sync rolled back; so use an .independent transaction
87 @db_api.placement_context_manager.writer.independent
88 def _trait_sync(ctx):
89     """Sync the os_traits symbols to the database.
90 
91     Reads all symbols from the os_traits library, checks if any of them do
92     not exist in the database and bulk-inserts those that are not. This is
93     done once per process using this code if either Trait.get_by_name or
94     TraitList.get_all is called.
95 
96     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
97                 connection.
98     """
99     # Create a set of all traits in the os_traits library.
100     std_traits = set(os_traits.get_traits())
101     sel = sa.select([_TRAIT_TBL.c.name])
102     res = ctx.session.execute(sel).fetchall()
103     # Create a set of all traits in the db that are not custom
104     # traits.
105     db_traits = set(
106         r[0] for r in res
107         if not os_traits.is_custom(r[0])
108     )
109     # Determine those traits which are in os_traits but not
110     # currently in the database, and insert them.
111     need_sync = std_traits - db_traits
112     ins = _TRAIT_TBL.insert()
113     batch_args = [
114         {'name': six.text_type(trait)}
115         for trait in need_sync
116     ]
117     if batch_args:
118         try:
119             ctx.session.execute(ins, batch_args)
120             LOG.info("Synced traits from os_traits into API DB: %s",
121                      need_sync)
122         except db_exc.DBDuplicateEntry:
123             pass  # some other process sync'd, just ignore
124 
125 
126 def ensure_trait_sync(ctx):
127     """Ensures that the os_traits library is synchronized to the traits db.
128 
129     If _TRAITS_SYNCED is False then this process has not tried to update the
130     traits db. Do so by calling _trait_sync. Since the placement API server
131     could be multi-threaded, lock around testing _TRAITS_SYNCED to avoid
132     duplicating work.
133 
134     Different placement API server processes that talk to the same database
135     will avoid issues through the power of transactions.
136 
137     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
138                 connection.
139     """
140     global _TRAITS_SYNCED
141     # If another thread is doing this work, wait for it to complete.
142     # When that thread is done _TRAITS_SYNCED will be true in this
143     # thread and we'll simply return.
144     with lockutils.lock(_TRAIT_LOCK):
145         if not _TRAITS_SYNCED:
146             _trait_sync(ctx)
147             _TRAITS_SYNCED = True
148 
149 
150 def _get_current_inventory_resources(ctx, rp):
151     """Returns a set() containing the resource class IDs for all resources
152     currently having an inventory record for the supplied resource provider.
153 
154     :param ctx: `nova.context.RequestContext` that may be used to grab a DB
155                 connection.
156     :param rp: Resource provider to query inventory for.
157     """
158     cur_res_sel = sa.select([_INV_TBL.c.resource_class_id]).where(
159             _INV_TBL.c.resource_provider_id == rp.id)
160     existing_resources = ctx.session.execute(cur_res_sel).fetchall()
161     return set([r[0] for r in existing_resources])
162 
163 
164 def _delete_inventory_from_provider(ctx, rp, to_delete):
165     """Deletes any inventory records from the supplied provider and set() of
166     resource class identifiers.
167 
168     If there are allocations for any of the inventories to be deleted raise
169     InventoryInUse exception.
170 
171     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
172     :param rp: Resource provider from which to delete inventory.
173     :param to_delete: set() containing resource class IDs for records to
174                       delete.
175     """
176     allocation_query = sa.select(
177         [_ALLOC_TBL.c.resource_class_id.label('resource_class')]).where(
178              sa.and_(_ALLOC_TBL.c.resource_provider_id == rp.id,
179                      _ALLOC_TBL.c.resource_class_id.in_(to_delete))
180          ).group_by(_ALLOC_TBL.c.resource_class_id)
181     allocations = ctx.session.execute(allocation_query).fetchall()
182     if allocations:
183         resource_classes = ', '.join([_RC_CACHE.string_from_id(alloc[0])
184                                       for alloc in allocations])
185         raise exception.InventoryInUse(resource_classes=resource_classes,
186                                        resource_provider=rp.uuid)
187 
188     del_stmt = _INV_TBL.delete().where(sa.and_(
189             _INV_TBL.c.resource_provider_id == rp.id,
190             _INV_TBL.c.resource_class_id.in_(to_delete)))
191     res = ctx.session.execute(del_stmt)
192     return res.rowcount
193 
194 
195 def _add_inventory_to_provider(ctx, rp, inv_list, to_add):
196     """Inserts new inventory records for the supplied resource provider.
197 
198     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
199     :param rp: Resource provider to add inventory to.
200     :param inv_list: InventoryList object
201     :param to_add: set() containing resource class IDs to search inv_list for
202                    adding to resource provider.
203     """
204     for rc_id in to_add:
205         rc_str = _RC_CACHE.string_from_id(rc_id)
206         inv_record = inv_list.find(rc_str)
207         ins_stmt = _INV_TBL.insert().values(
208                 resource_provider_id=rp.id,
209                 resource_class_id=rc_id,
210                 total=inv_record.total,
211                 reserved=inv_record.reserved,
212                 min_unit=inv_record.min_unit,
213                 max_unit=inv_record.max_unit,
214                 step_size=inv_record.step_size,
215                 allocation_ratio=inv_record.allocation_ratio)
216         ctx.session.execute(ins_stmt)
217 
218 
219 def _update_inventory_for_provider(ctx, rp, inv_list, to_update):
220     """Updates existing inventory records for the supplied resource provider.
221 
222     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
223     :param rp: Resource provider on which to update inventory.
224     :param inv_list: InventoryList object
225     :param to_update: set() containing resource class IDs to search inv_list
226                       for updating in resource provider.
227     :returns: A list of (uuid, class) tuples that have exceeded their
228               capacity after this inventory update.
229     """
230     exceeded = []
231     for rc_id in to_update:
232         rc_str = _RC_CACHE.string_from_id(rc_id)
233         inv_record = inv_list.find(rc_str)
234         allocation_query = sa.select(
235             [func.sum(_ALLOC_TBL.c.used).label('usage')]).\
236             where(sa.and_(
237                 _ALLOC_TBL.c.resource_provider_id == rp.id,
238                 _ALLOC_TBL.c.resource_class_id == rc_id))
239         allocations = ctx.session.execute(allocation_query).first()
240         if (allocations
241             and allocations['usage'] is not None
242             and allocations['usage'] > inv_record.capacity):
243             exceeded.append((rp.uuid, rc_str))
244         upd_stmt = _INV_TBL.update().where(sa.and_(
245                 _INV_TBL.c.resource_provider_id == rp.id,
246                 _INV_TBL.c.resource_class_id == rc_id)).values(
247                         total=inv_record.total,
248                         reserved=inv_record.reserved,
249                         min_unit=inv_record.min_unit,
250                         max_unit=inv_record.max_unit,
251                         step_size=inv_record.step_size,
252                         allocation_ratio=inv_record.allocation_ratio)
253         res = ctx.session.execute(upd_stmt)
254         if not res.rowcount:
255             raise exception.InventoryWithResourceClassNotFound(
256                     resource_class=rc_str)
257     return exceeded
258 
259 
260 def _increment_provider_generation(ctx, rp):
261     """Increments the supplied provider's generation value, supplying the
262     currently-known generation. Returns whether the increment succeeded.
263 
264     :param ctx: `nova.context.RequestContext` that contains an oslo_db Session
265     :param rp: `ResourceProvider` whose generation should be updated.
266     :returns: The new resource provider generation value if successful.
267     :raises nova.exception.ConcurrentUpdateDetected: if another thread updated
268             the same resource provider's view of its inventory or allocations
269             in between the time when this object was originally read
270             and the call to set the inventory.
271     """
272     rp_gen = rp.generation
273     new_generation = rp_gen + 1
274     upd_stmt = _RP_TBL.update().where(sa.and_(
275             _RP_TBL.c.id == rp.id,
276             _RP_TBL.c.generation == rp_gen)).values(
277                     generation=(new_generation))
278 
279     res = ctx.session.execute(upd_stmt)
280     if res.rowcount != 1:
281         raise exception.ResourceProviderConcurrentUpdateDetected()
282     return new_generation
283 
284 
285 @db_api.placement_context_manager.writer
286 def _add_inventory(context, rp, inventory):
287     """Add one Inventory that wasn't already on the provider.
288 
289     :raises `exception.ResourceClassNotFound` if inventory.resource_class
290             cannot be found in either the standard classes or the DB.
291     """
292     rc_id = _RC_CACHE.id_from_string(inventory.resource_class)
293     inv_list = InventoryList(objects=[inventory])
294     _add_inventory_to_provider(
295         context, rp, inv_list, set([rc_id]))
296     rp.generation = _increment_provider_generation(context, rp)
297 
298 
299 @db_api.placement_context_manager.writer
300 def _update_inventory(context, rp, inventory):
301     """Update an inventory already on the provider.
302 
303     :raises `exception.ResourceClassNotFound` if inventory.resource_class
304             cannot be found in either the standard classes or the DB.
305     """
306     rc_id = _RC_CACHE.id_from_string(inventory.resource_class)
307     inv_list = InventoryList(objects=[inventory])
308     exceeded = _update_inventory_for_provider(
309         context, rp, inv_list, set([rc_id]))
310     rp.generation = _increment_provider_generation(context, rp)
311     return exceeded
312 
313 
314 @db_api.placement_context_manager.writer
315 def _delete_inventory(context, rp, resource_class):
316     """Delete up to one Inventory of the given resource_class string.
317 
318     :raises `exception.ResourceClassNotFound` if resource_class
319             cannot be found in either the standard classes or the DB.
320     """
321     rc_id = _RC_CACHE.id_from_string(resource_class)
322     if not _delete_inventory_from_provider(context, rp, [rc_id]):
323         raise exception.NotFound(
324             'No inventory of class %s found for delete'
325             % resource_class)
326     rp.generation = _increment_provider_generation(context, rp)
327 
328 
329 @db_api.placement_context_manager.writer
330 def _set_inventory(context, rp, inv_list):
331     """Given an InventoryList object, replaces the inventory of the
332     resource provider in a safe, atomic fashion using the resource
333     provider's generation as a consistent view marker.
334 
335     :param context: Nova RequestContext.
336     :param rp: `ResourceProvider` object upon which to set inventory.
337     :param inv_list: `InventoryList` object to save to backend storage.
338     :returns: A list of (uuid, class) tuples that have exceeded their
339               capacity after this inventory update.
340     :raises nova.exception.ConcurrentUpdateDetected: if another thread updated
341             the same resource provider's view of its inventory or allocations
342             in between the time when this object was originally read
343             and the call to set the inventory.
344     :raises `exception.ResourceClassNotFound` if any resource class in any
345             inventory in inv_list cannot be found in either the standard
346             classes or the DB.
347     :raises `exception.InventoryInUse` if we attempt to delete inventory
348             from a provider that has allocations for that resource class.
349     """
350     existing_resources = _get_current_inventory_resources(context, rp)
351     these_resources = set([_RC_CACHE.id_from_string(r.resource_class)
352                            for r in inv_list.objects])
353 
354     # Determine which resources we should be adding, deleting and/or
355     # updating in the resource provider's inventory by comparing sets
356     # of resource class identifiers.
357     to_add = these_resources - existing_resources
358     to_delete = existing_resources - these_resources
359     to_update = these_resources & existing_resources
360     exceeded = []
361 
362     if to_delete:
363         _delete_inventory_from_provider(context, rp, to_delete)
364     if to_add:
365         _add_inventory_to_provider(context, rp, inv_list, to_add)
366     if to_update:
367         exceeded = _update_inventory_for_provider(context, rp, inv_list,
368                                                   to_update)
369 
370     # Here is where we update the resource provider's generation value.  If
371     # this update updates zero rows, that means that another thread has updated
372     # the inventory for this resource provider between the time the caller
373     # originally read the resource provider record and inventory information
374     # and this point. We raise an exception here which will rollback the above
375     # transaction and return an error to the caller to indicate that they can
376     # attempt to retry the inventory save after reverifying any capacity
377     # conditions and re-reading the existing inventory information.
378     rp.generation = _increment_provider_generation(context, rp)
379 
380     return exceeded
381 
382 
383 @db_api.placement_context_manager.reader
384 def _get_provider_by_uuid(context, uuid):
385     """Given a UUID, return a dict of information about the resource provider
386     from the database.
387 
388     :raises: NotFound if no such provider was found
389     :param uuid: The UUID to look up
390     """
391     rpt = sa.alias(_RP_TBL, name="rp")
392     parent = sa.alias(_RP_TBL, name="parent")
393     root = sa.alias(_RP_TBL, name="root")
394     # TODO(jaypipes): Change this to an inner join when we are sure all
395     # root_provider_id values are NOT NULL
396     rp_to_root = sa.outerjoin(rpt, root, rpt.c.root_provider_id == root.c.id)
397     rp_to_parent = sa.outerjoin(rp_to_root, parent,
398         rpt.c.parent_provider_id == parent.c.id)
399     cols = [
400         rpt.c.id,
401         rpt.c.uuid,
402         rpt.c.name,
403         rpt.c.generation,
404         root.c.uuid.label("root_provider_uuid"),
405         parent.c.uuid.label("parent_provider_uuid"),
406         rpt.c.updated_at,
407         rpt.c.created_at,
408     ]
409     sel = sa.select(cols).select_from(rp_to_parent).where(rpt.c.uuid == uuid)
410     res = context.session.execute(sel).fetchone()
411     if not res:
412         raise exception.NotFound(
413             'No resource provider with uuid %s found' % uuid)
414     return dict(res)
415 
416 
417 @db_api.placement_context_manager.reader
418 def _get_aggregates_by_provider_id(context, rp_id):
419     join_statement = sa.join(
420         _AGG_TBL, _RP_AGG_TBL, sa.and_(
421             _AGG_TBL.c.id == _RP_AGG_TBL.c.aggregate_id,
422             _RP_AGG_TBL.c.resource_provider_id == rp_id))
423     sel = sa.select([_AGG_TBL.c.uuid]).select_from(join_statement)
424     return [r[0] for r in context.session.execute(sel).fetchall()]
425 
426 
427 @db_api.placement_context_manager.reader
428 def _anchors_for_sharing_providers(context, rp_ids, get_id=False):
429     """Given a list of internal IDs of sharing providers, returns a set of
430     tuples of (sharing provider UUID, anchor provider UUID), where each of
431     anchor is the unique root provider of a tree associated with the same
432     aggregate as the sharing provider. (These are the providers that can
433     "anchor" a single AllocationRequest.)
434 
435     The sharing provider may or may not itself be part of a tree; in either
436     case, an entry for this root provider is included in the result.
437 
438     If the sharing provider is not part of any aggregate, the empty list is
439     returned.
440 
441     If get_id is True, it returns a set of tuples of (sharing provider ID,
442     anchor provider ID) instead.
443     """
444     # SELECT sps.uuid, COALESCE(rps.uuid, shr_with_sps.uuid)
445     # FROM resource_providers AS sps
446     # INNER JOIN resource_provider_aggregates AS shr_aggs
447     #   ON sps.id = shr_aggs.resource_provider_id
448     # INNER JOIN resource_provider_aggregates AS shr_with_sps_aggs
449     #   ON shr_aggs.aggregate_id = shr_with_sps_aggs.aggregate_id
450     # INNER JOIN resource_providers AS shr_with_sps
451     #   ON shr_with_sps_aggs.resource_provider_id = shr_with_sps.id
452     # LEFT JOIN resource_providers AS rps
453     #   ON shr_with_sps.root_provider_id = rps.id
454     # WHERE sps.id IN $(RP_IDs)
455     rps = sa.alias(_RP_TBL, name='rps')
456     sps = sa.alias(_RP_TBL, name='sps')
457     shr_aggs = sa.alias(_RP_AGG_TBL, name='shr_aggs')
458     shr_with_sps_aggs = sa.alias(_RP_AGG_TBL, name='shr_with_sps_aggs')
459     shr_with_sps = sa.alias(_RP_TBL, name='shr_with_sps')
460     join_chain = sa.join(
461         sps, shr_aggs, sps.c.id == shr_aggs.c.resource_provider_id)
462     join_chain = sa.join(
463         join_chain, shr_with_sps_aggs,
464         shr_aggs.c.aggregate_id == shr_with_sps_aggs.c.aggregate_id)
465     join_chain = sa.join(
466         join_chain, shr_with_sps,
467         shr_with_sps_aggs.c.resource_provider_id == shr_with_sps.c.id)
468     if get_id:
469         # TODO(yikun): Change `func.coalesce(shr_with_sps.c.root_provider_id,
470         # shr_with_sps.c.id)` to `shr_with_sps.c.root_provider_id` when we are
471         # sure all root_provider_id values are NOT NULL
472         sel = sa.select([sps.c.id, func.coalesce(
473             shr_with_sps.c.root_provider_id, shr_with_sps.c.id)])
474     else:
475         # TODO(efried): Change this to an inner join and change
476         # 'func.coalesce(rps.c.uuid, shr_with_sps.c.uuid)' to `rps.c.uuid`
477         # when we are sure all root_provider_id values are NOT NULL
478         join_chain = sa.outerjoin(
479             join_chain, rps, shr_with_sps.c.root_provider_id == rps.c.id)
480         sel = sa.select([sps.c.uuid, func.coalesce(rps.c.uuid,
481                                                    shr_with_sps.c.uuid)])
482     sel = sel.select_from(join_chain)
483     sel = sel.where(sps.c.id.in_(rp_ids))
484     return set([(r[0], r[1]) for r in context.session.execute(sel).fetchall()])
485 
486 
487 @db_api.placement_context_manager.writer
488 def _set_aggregates(context, resource_provider, provided_aggregates,
489                     increment_generation=False):
490     rp_id = resource_provider.id
491     # When aggregate uuids are persisted no validation is done
492     # to ensure that they refer to something that has meaning
493     # elsewhere. It is assumed that code which makes use of the
494     # aggregates, later, will validate their fitness.
495     # TODO(cdent): At the moment we do not delete
496     # a PlacementAggregate that no longer has any associations
497     # with at least one resource provider. We may wish to do that
498     # to avoid bloat if it turns out we're creating a lot of noise.
499     # Not doing now to move things along.
500     provided_aggregates = set(provided_aggregates)
501     existing_aggregates = set(_get_aggregates_by_provider_id(context, rp_id))
502     to_add = provided_aggregates - existing_aggregates
503     target_aggregates = list(provided_aggregates)
504 
505     # Create any aggregates that do not yet exist in
506     # PlacementAggregates. This is different from
507     # the set in existing_aggregates; those are aggregates for
508     # which there are associations for the resource provider
509     # at rp_id. The following loop checks for the existence of any
510     # aggregate with the provided uuid. In this way we only
511     # create a new row in the PlacementAggregate table if the
512     # aggregate uuid has never been seen before. Code further
513     # below will update the associations.
514     for agg_uuid in to_add:
515         found_agg = context.session.query(models.PlacementAggregate.uuid).\
516             filter_by(uuid=agg_uuid).first()
517         if not found_agg:
518             new_aggregate = models.PlacementAggregate(uuid=agg_uuid)
519             try:
520                 context.session.add(new_aggregate)
521                 # Flush each aggregate to explicitly call the INSERT
522                 # statement that could result in an integrity error
523                 # if some other thread has added this agg_uuid. This
524                 # also makes sure that the new aggregates have
525                 # ids when the SELECT below happens.
526                 context.session.flush()
527             except db_exc.DBDuplicateEntry:
528                 # Something else has already added this agg_uuid
529                 pass
530 
531     # Remove all aggregate associations so we can refresh them
532     # below. This means that all associations are added, but the
533     # aggregates themselves stay around.
534     context.session.query(models.ResourceProviderAggregate).filter_by(
535         resource_provider_id=rp_id).delete()
536 
537     # Set resource_provider_id, aggregate_id pairs to
538     # ResourceProviderAggregate table.
539     if target_aggregates:
540         select_agg_id = sa.select([rp_id, models.PlacementAggregate.id]).\
541             where(models.PlacementAggregate.uuid.in_(target_aggregates))
542         insert_aggregates = models.ResourceProviderAggregate.__table__.\
543             insert().from_select(['resource_provider_id', 'aggregate_id'],
544                                  select_agg_id)
545         context.session.execute(insert_aggregates)
546 
547     if increment_generation:
548         resource_provider.generation = _increment_provider_generation(
549             context, resource_provider)
550 
551 
552 @db_api.placement_context_manager.reader
553 def _get_traits_by_provider_id(context, rp_id):
554     t = sa.alias(_TRAIT_TBL, name='t')
555     rpt = sa.alias(_RP_TRAIT_TBL, name='rpt')
556 
557     join_cond = sa.and_(t.c.id == rpt.c.trait_id,
558                         rpt.c.resource_provider_id == rp_id)
559     join = sa.join(t, rpt, join_cond)
560     sel = sa.select([t.c.id, t.c.name,
561                      t.c.created_at, t.c.updated_at]).select_from(join)
562     return [dict(r) for r in context.session.execute(sel).fetchall()]
563 
564 
565 def _add_traits_to_provider(ctx, rp_id, to_add):
566     """Adds trait associations to the provider with the supplied ID.
567 
568     :param ctx: `nova.context.RequestContext` that has an oslo_db Session
569     :param rp_id: Internal ID of the resource provider on which to add
570                   trait associations
571     :param to_add: set() containing internal trait IDs for traits to add
572     """
573     for trait_id in to_add:
574         try:
575             ins_stmt = _RP_TRAIT_TBL.insert().values(
576                 resource_provider_id=rp_id,
577                 trait_id=trait_id)
578             ctx.session.execute(ins_stmt)
579         except db_exc.DBDuplicateEntry:
580             # Another thread already set this trait for this provider. Ignore
581             # this for now (but ConcurrentUpdateDetected will end up being
582             # raised almost assuredly when we go to increment the resource
583             # provider's generation later, but that's also fine)
584             pass
585 
586 
587 def _delete_traits_from_provider(ctx, rp_id, to_delete):
588     """Deletes trait associations from the provider with the supplied ID and
589     set() of internal trait IDs.
590 
591     :param ctx: `nova.context.RequestContext` that has an oslo_db Session
592     :param rp_id: Internal ID of the resource provider from which to delete
593                   trait associations
594     :param to_delete: set() containing internal trait IDs for traits to
595                       delete
596     """
597     del_stmt = _RP_TRAIT_TBL.delete().where(
598         sa.and_(
599             _RP_TRAIT_TBL.c.resource_provider_id == rp_id,
600             _RP_TRAIT_TBL.c.trait_id.in_(to_delete)))
601     ctx.session.execute(del_stmt)
602 
603 
604 @db_api.placement_context_manager.writer
605 def _set_traits(context, rp, traits):
606     """Given a ResourceProvider object and a TraitList object, replaces the set
607     of traits associated with the resource provider.
608 
609     :raises: ConcurrentUpdateDetected if the resource provider's traits or
610              inventory was changed in between the time when we first started to
611              set traits and the end of this routine.
612 
613     :param rp: The ResourceProvider object to set traits against
614     :param traits: A TraitList object or list of Trait objects
615     """
616     # Get the internal IDs of our existing traits
617     existing_traits = _get_traits_by_provider_id(context, rp.id)
618     existing_traits = set(rec['id'] for rec in existing_traits)
619     want_traits = set(trait.id for trait in traits)
620 
621     to_add = want_traits - existing_traits
622     to_delete = existing_traits - want_traits
623 
624     if not to_add and not to_delete:
625         return
626 
627     if to_delete:
628         _delete_traits_from_provider(context, rp.id, to_delete)
629     if to_add:
630         _add_traits_to_provider(context, rp.id, to_add)
631     rp.generation = _increment_provider_generation(context, rp)
632 
633 
634 @db_api.placement_context_manager.reader
635 def _has_child_providers(context, rp_id):
636     """Returns True if the supplied resource provider has any child providers,
637     False otherwise
638     """
639     child_sel = sa.select([_RP_TBL.c.id])
640     child_sel = child_sel.where(_RP_TBL.c.parent_provider_id == rp_id)
641     child_res = context.session.execute(child_sel.limit(1)).fetchone()
642     if child_res:
643         return True
644     return False
645 
646 
647 @db_api.placement_context_manager.writer
648 def _set_root_provider_id(context, rp_id, root_id):
649     """Simply sets the root_provider_id value for a provider identified by
650     rp_id. Used in online data migration.
651 
652     :param rp_id: Internal ID of the provider to update
653     :param root_id: Value to set root provider to
654     """
655     upd = _RP_TBL.update().where(_RP_TBL.c.id == rp_id)
656     upd = upd.values(root_provider_id=root_id)
657     context.session.execute(upd)
658 
659 
660 ProviderIds = collections.namedtuple(
661     'ProviderIds', 'id uuid parent_id parent_uuid root_id root_uuid')
662 
663 
664 def _provider_ids_from_rp_ids(context, rp_ids):
665     """Given an iterable of internal resource provider IDs, returns a dict,
666     keyed by internal provider Id, of ProviderIds namedtuples describing those
667     providers.
668 
669     :returns: dict, keyed by internal provider Id, of ProviderIds namedtuples
670     :param rp_ids: iterable of internal provider IDs to look up
671     """
672     # SELECT
673     #   rp.id, rp.uuid,
674     #   parent.id AS parent_id, parent.uuid AS parent_uuid,
675     #   root.id AS root_id, root.uuid AS root_uuid
676     # FROM resource_providers AS rp
677     # LEFT JOIN resource_providers AS parent
678     #   ON rp.parent_provider_id = parent.id
679     # LEFT JOIN resource_providers AS root
680     #   ON rp.root_provider_id = root.id
681     # WHERE rp.id IN ($rp_ids)
682     me = sa.alias(_RP_TBL, name="me")
683     parent = sa.alias(_RP_TBL, name="parent")
684     root = sa.alias(_RP_TBL, name="root")
685     cols = [
686         me.c.id,
687         me.c.uuid,
688         parent.c.id.label('parent_id'),
689         parent.c.uuid.label('parent_uuid'),
690         root.c.id.label('root_id'),
691         root.c.uuid.label('root_uuid'),
692     ]
693     # TODO(jaypipes): Change this to an inner join when we are sure all
694     # root_provider_id values are NOT NULL
695     me_to_root = sa.outerjoin(me, root, me.c.root_provider_id == root.c.id)
696     me_to_parent = sa.outerjoin(me_to_root, parent,
697         me.c.parent_provider_id == parent.c.id)
698     sel = sa.select(cols).select_from(me_to_parent)
699     sel = sel.where(me.c.id.in_(rp_ids))
700     return {
701         r[0]: ProviderIds(**dict(r)) for r in context.session.execute(sel)
702     }
703 
704 
705 def _provider_ids_from_uuid(context, uuid):
706     """Given the UUID of a resource provider, returns a namedtuple
707     (ProviderIds) with the internal ID, the UUID, the parent provider's
708     internal ID, parent provider's UUID, the root provider's internal ID and
709     the root provider UUID.
710 
711     :returns: ProviderIds object containing the internal IDs and UUIDs of the
712               provider identified by the supplied UUID
713     :param uuid: The UUID of the provider to look up
714     """
715     # SELECT
716     #   rp.id, rp.uuid,
717     #   parent.id AS parent_id, parent.uuid AS parent_uuid,
718     #   root.id AS root_id, root.uuid AS root_uuid
719     # FROM resource_providers AS rp
720     # LEFT JOIN resource_providers AS parent
721     #   ON rp.parent_provider_id = parent.id
722     # LEFT JOIN resource_providers AS root
723     #   ON rp.root_provider_id = root.id
724     me = sa.alias(_RP_TBL, name="me")
725     parent = sa.alias(_RP_TBL, name="parent")
726     root = sa.alias(_RP_TBL, name="root")
727     cols = [
728         me.c.id,
729         me.c.uuid,
730         parent.c.id.label('parent_id'),
731         parent.c.uuid.label('parent_uuid'),
732         root.c.id.label('root_id'),
733         root.c.uuid.label('root_uuid'),
734     ]
735     # TODO(jaypipes): Change this to an inner join when we are sure all
736     # root_provider_id values are NOT NULL
737     me_to_root = sa.outerjoin(me, root, me.c.root_provider_id == root.c.id)
738     me_to_parent = sa.outerjoin(me_to_root, parent,
739         me.c.parent_provider_id == parent.c.id)
740     sel = sa.select(cols).select_from(me_to_parent)
741     sel = sel.where(me.c.uuid == uuid)
742     res = context.session.execute(sel).fetchone()
743     if not res:
744         return None
745     return ProviderIds(**dict(res))
746 
747 
748 def _provider_ids_matching_aggregates(context, member_of, rp_ids=None):
749     """Given a list of lists of aggregate UUIDs, return the internal IDs of all
750     resource providers associated with the aggregates.
751 
752     :param member_of: A list containing lists of aggregate UUIDs. Each item in
753         the outer list is to be AND'd together. If that item contains multiple
754         values, they are OR'd together.
755 
756         For example, if member_of is::
757 
758             [
759                 ['agg1'],
760                 ['agg2', 'agg3'],
761             ]
762 
763         we will return all the resource providers that are
764         associated with agg1 as well as either (agg2 or agg3)
765     :param rp_ids: When present, returned resource providers are limited
766         to only those in this value
767 
768     :returns: A list of internal resource provider IDs having all required
769         aggregate associations
770     """
771     # Given a request for the following:
772     #
773     # member_of = [
774     #   [agg1],
775     #   [agg2],
776     #   [agg3, agg4]
777     # ]
778     #
779     # we need to produce the following SQL expression:
780     #
781     # SELECT
782     #   rp.id
783     # FROM resource_providers AS rp
784     # JOIN resource_provider_aggregates AS rpa1
785     #   ON rp.id = rpa1.resource_provider_id
786     #   AND rpa1.aggregate_id IN ($AGG1_ID)
787     # JOIN resource_provider_aggregates AS rpa2
788     #   ON rp.id = rpa2.resource_provider_id
789     #   AND rpa2.aggregate_id IN ($AGG2_ID)
790     # JOIN resource_provider_aggregates AS rpa3
791     #   ON rp.id = rpa3.resource_provider_id
792     #   AND rpa3.aggregate_id IN ($AGG3_ID, $AGG4_ID)
793     # # Only if we have rp_ids...
794     # WHERE rp.id IN ($RP_IDs)
795 
796     # First things first, get a map of all the aggregate UUID to internal
797     # aggregate IDs
798     agg_uuids = set()
799     for members in member_of:
800         for member in members:
801             agg_uuids.add(member)
802     agg_tbl = sa.alias(_AGG_TBL, name='aggs')
803     agg_sel = sa.select([agg_tbl.c.uuid, agg_tbl.c.id])
804     agg_sel = agg_sel.where(agg_tbl.c.uuid.in_(agg_uuids))
805     agg_uuid_map = {
806         r[0]: r[1] for r in context.session.execute(agg_sel).fetchall()
807     }
808 
809     rp_tbl = sa.alias(_RP_TBL, name='rp')
810     join_chain = rp_tbl
811 
812     for x, members in enumerate(member_of):
813         rpa_tbl = sa.alias(_RP_AGG_TBL, name='rpa%d' % x)
814 
815         agg_ids = [agg_uuid_map[member] for member in members
816                    if member in agg_uuid_map]
817         if not agg_ids:
818             # This member_of list contains only non-existent aggregate UUIDs
819             # and therefore we will always return 0 results, so short-circuit
820             return []
821 
822         join_cond = sa.and_(
823             rp_tbl.c.id == rpa_tbl.c.resource_provider_id,
824             rpa_tbl.c.aggregate_id.in_(agg_ids))
825         join_chain = sa.join(join_chain, rpa_tbl, join_cond)
826     sel = sa.select([rp_tbl.c.id]).select_from(join_chain)
827     if rp_ids:
828         sel = sel.where(rp_tbl.c.id.in_(rp_ids))
829     return [r[0] for r in context.session.execute(sel).fetchall()]
830 
831 
832 @db_api.placement_context_manager.writer
833 def _delete_rp_record(context, _id):
834     return context.session.query(models.ResourceProvider).\
835         filter(models.ResourceProvider.id == _id).\
836         delete(synchronize_session=False)
837 
838 
839 @base.VersionedObjectRegistry.register_if(False)
840 class ResourceProvider(base.VersionedObject, base.TimestampedObject):
841     SETTABLE_FIELDS = ('name', 'parent_provider_uuid')
842 
843     fields = {
844         'id': fields.IntegerField(read_only=True),
845         'uuid': fields.UUIDField(nullable=False),
846         'name': fields.StringField(nullable=False),
847         'generation': fields.IntegerField(nullable=False),
848         # UUID of the root provider in a hierarchy of providers. Will be equal
849         # to the uuid field if this provider is the root provider of a
850         # hierarchy. This field is never manually set by the user. Instead, it
851         # is automatically set to either the root provider UUID of the parent
852         # or the UUID of the provider itself if there is no parent. This field
853         # is an optimization field that allows us to very quickly query for all
854         # providers within a particular tree without doing any recursive
855         # querying.
856         'root_provider_uuid': fields.UUIDField(nullable=False),
857         # UUID of the direct parent provider, or None if this provider is a
858         # "root" provider.
859         'parent_provider_uuid': fields.UUIDField(nullable=True, default=None),
860     }
861 
862     def create(self):
863         if 'id' in self:
864             raise exception.ObjectActionError(action='create',
865                                               reason='already created')
866         if 'uuid' not in self:
867             raise exception.ObjectActionError(action='create',
868                                               reason='uuid is required')
869         if 'name' not in self:
870             raise exception.ObjectActionError(action='create',
871                                               reason='name is required')
872         if 'root_provider_uuid' in self:
873             raise exception.ObjectActionError(
874                 action='create',
875                 reason=_('root provider UUID cannot be manually set.'))
876 
877         self.obj_set_defaults()
878         updates = self.obj_get_changes()
879         self._create_in_db(self._context, updates)
880         self.obj_reset_changes()
881 
882     def destroy(self):
883         self._delete(self._context, self.id)
884 
885     def save(self):
886         updates = self.obj_get_changes()
887         if updates and any(k not in self.SETTABLE_FIELDS
888                            for k in updates.keys()):
889             raise exception.ObjectActionError(
890                 action='save',
891                 reason='Immutable fields changed')
892         self._update_in_db(self._context, self.id, updates)
893         self.obj_reset_changes()
894 
895     @classmethod
896     def get_by_uuid(cls, context, uuid):
897         """Returns a new ResourceProvider object with the supplied UUID.
898 
899         :raises NotFound if no such provider could be found
900         :param uuid: UUID of the provider to search for
901         """
902         rp_rec = _get_provider_by_uuid(context, uuid)
903         return cls._from_db_object(context, cls(), rp_rec)
904 
905     def add_inventory(self, inventory):
906         """Add one new Inventory to the resource provider.
907 
908         Fails if Inventory of the provided resource class is
909         already present.
910         """
911         _add_inventory(self._context, self, inventory)
912         self.obj_reset_changes()
913 
914     def delete_inventory(self, resource_class):
915         """Delete Inventory of provided resource_class."""
916         _delete_inventory(self._context, self, resource_class)
917         self.obj_reset_changes()
918 
919     def set_inventory(self, inv_list):
920         """Set all resource provider Inventory to be the provided list."""
921         exceeded = _set_inventory(self._context, self, inv_list)
922         for uuid, rclass in exceeded:
923             LOG.warning('Resource provider %(uuid)s is now over-'
924                         'capacity for %(resource)s',
925                         {'uuid': uuid, 'resource': rclass})
926         self.obj_reset_changes()
927 
928     def update_inventory(self, inventory):
929         """Update one existing Inventory of the same resource class.
930 
931         Fails if no Inventory of the same class is present.
932         """
933         exceeded = _update_inventory(self._context, self, inventory)
934         for uuid, rclass in exceeded:
935             LOG.warning('Resource provider %(uuid)s is now over-'
936                         'capacity for %(resource)s',
937                         {'uuid': uuid, 'resource': rclass})
938         self.obj_reset_changes()
939 
940     def get_aggregates(self):
941         """Get the aggregate uuids associated with this resource provider."""
942         return _get_aggregates_by_provider_id(self._context, self.id)
943 
944     def set_aggregates(self, aggregate_uuids, increment_generation=False):
945         """Set the aggregate uuids associated with this resource provider.
946 
947         If an aggregate does not exist, one will be created using the
948         provided uuid.
949 
950         The resource provider generation is incremented if and only if the
951         increment_generation parameter is True.
952         """
953         _set_aggregates(self._context, self, aggregate_uuids,
954                         increment_generation=increment_generation)
955 
956     def set_traits(self, traits):
957         """Replaces the set of traits associated with the resource provider
958         with the given list of Trait objects.
959 
960         :param traits: A list of Trait objects representing the traits to
961                        associate with the provider.
962         """
963         _set_traits(self._context, self, traits)
964         self.obj_reset_changes()
965 
966     @db_api.placement_context_manager.writer
967     def _create_in_db(self, context, updates):
968         parent_id = None
969         root_id = None
970         # User supplied a parent, let's make sure it exists
971         parent_uuid = updates.pop('parent_provider_uuid')
972         if parent_uuid is not None:
973             # Setting parent to ourselves doesn't make any sense
974             if parent_uuid == self.uuid:
975                 raise exception.ObjectActionError(
976                         action='create',
977                         reason=_('parent provider UUID cannot be same as '
978                                  'UUID. Please set parent provider UUID to '
979                                  'None if there is no parent.'))
980 
981             parent_ids = _provider_ids_from_uuid(context, parent_uuid)
982             if parent_ids is None:
983                 raise exception.ObjectActionError(
984                         action='create',
985                         reason=_('parent provider UUID does not exist.'))
986 
987             parent_id = parent_ids.id
988             root_id = parent_ids.root_id
989             updates['root_provider_id'] = root_id
990             updates['parent_provider_id'] = parent_id
991             self.root_provider_uuid = parent_ids.root_uuid
992 
993         db_rp = models.ResourceProvider()
994         db_rp.update(updates)
995         context.session.add(db_rp)
996         context.session.flush()
997 
998         self.id = db_rp.id
999         self.generation = db_rp.generation
1000 
1001         if root_id is None:
1002             # User did not specify a parent when creating this provider, so the
1003             # root_provider_id needs to be set to this provider's newly-created
1004             # internal ID
1005             db_rp.root_provider_id = db_rp.id
1006             context.session.add(db_rp)
1007             context.session.flush()
1008             self.root_provider_uuid = self.uuid
1009 
1010     @staticmethod
1011     @db_api.placement_context_manager.writer
1012     def _delete(context, _id):
1013         # Do a quick check to see if the provider is a parent. If it is, don't
1014         # allow deleting the provider. Note that the foreign key constraint on
1015         # resource_providers.parent_provider_id will prevent deletion of the
1016         # parent within the transaction below. This is just a quick
1017         # short-circuit outside of the transaction boundary.
1018         if _has_child_providers(context, _id):
1019             raise exception.CannotDeleteParentResourceProvider()
1020 
1021         # Don't delete the resource provider if it has allocations.
1022         rp_allocations = context.session.query(models.Allocation).\
1023              filter(models.Allocation.resource_provider_id == _id).\
1024              count()
1025         if rp_allocations:
1026             raise exception.ResourceProviderInUse()
1027         # Delete any inventory associated with the resource provider
1028         context.session.query(models.Inventory).\
1029             filter(models.Inventory.resource_provider_id == _id).\
1030             delete(synchronize_session=False)
1031         # Delete any aggregate associations for the resource provider
1032         # The name substitution on the next line is needed to satisfy pep8
1033         RPA_model = models.ResourceProviderAggregate
1034         context.session.query(RPA_model).\
1035                 filter(RPA_model.resource_provider_id == _id).delete()
1036         # delete any trait associations for the resource provider
1037         RPT_model = models.ResourceProviderTrait
1038         context.session.query(RPT_model).\
1039                 filter(RPT_model.resource_provider_id == _id).delete()
1040         # set root_provider_id to null to make deletion possible
1041         context.session.query(models.ResourceProvider).\
1042             filter(models.ResourceProvider.id == _id,
1043                    models.ResourceProvider.root_provider_id == _id).\
1044             update({'root_provider_id': None})
1045         # Now delete the RP record
1046         try:
1047             result = _delete_rp_record(context, _id)
1048         except sqla_exc.IntegrityError:
1049             # NOTE(jaypipes): Another thread snuck in and parented this
1050             # resource provider in between the above check for
1051             # _has_child_providers() and our attempt to delete the record
1052             raise exception.CannotDeleteParentResourceProvider()
1053         if not result:
1054             raise exception.NotFound()
1055 
1056     @db_api.placement_context_manager.writer
1057     def _update_in_db(self, context, id, updates):
1058         # A list of resource providers in the same tree with the
1059         # resource provider to update
1060         same_tree = []
1061         if 'parent_provider_uuid' in updates:
1062             # TODO(jaypipes): For now, "re-parenting" and "un-parenting" are
1063             # not possible. If the provider already had a parent, we don't
1064             # allow changing that parent due to various issues, including:
1065             #
1066             # * if the new parent is a descendant of this resource provider, we
1067             #   introduce the possibility of a loop in the graph, which would
1068             #   be very bad
1069             # * potentially orphaning heretofore-descendants
1070             #
1071             # So, for now, let's just prevent re-parenting...
1072             my_ids = _provider_ids_from_uuid(context, self.uuid)
1073             parent_uuid = updates.pop('parent_provider_uuid')
1074             if parent_uuid is not None:
1075                 parent_ids = _provider_ids_from_uuid(context, parent_uuid)
1076                 # User supplied a parent, let's make sure it exists
1077                 if parent_ids is None:
1078                     raise exception.ObjectActionError(
1079                             action='create',
1080                             reason=_('parent provider UUID does not exist.'))
1081                 if (my_ids.parent_id is not None and
1082                         my_ids.parent_id != parent_ids.id):
1083                     raise exception.ObjectActionError(
1084                             action='update',
1085                             reason=_('re-parenting a provider is not '
1086                                      'currently allowed.'))
1087                 if my_ids.parent_uuid is None:
1088                     # So the user specifies a parent for an RP that doesn't
1089                     # have one. We have to check that by this new parent we
1090                     # don't create a loop in the tree. Basically the new parent
1091                     # cannot be the RP itself or one of its descendants.
1092                     # However as the RP's current parent is None the above
1093                     # condition is the same as "the new parent cannot be any RP
1094                     # from the current RP tree".
1095                     same_tree = ResourceProviderList.get_all_by_filters(
1096                         context,
1097                         filters={'in_tree': self.uuid})
1098                     rp_uuids_in_the_same_tree = [rp.uuid for rp in same_tree]
1099                     if parent_uuid in rp_uuids_in_the_same_tree:
1100                         raise exception.ObjectActionError(
1101                             action='update',
1102                             reason=_('creating loop in the provider tree is '
1103                                      'not allowed.'))
1104 
1105                 updates['root_provider_id'] = parent_ids.root_id
1106                 updates['parent_provider_id'] = parent_ids.id
1107                 self.root_provider_uuid = parent_ids.root_uuid
1108             else:
1109                 if my_ids.parent_id is not None:
1110                     raise exception.ObjectActionError(
1111                             action='update',
1112                             reason=_('un-parenting a provider is not '
1113                                      'currently allowed.'))
1114 
1115         db_rp = context.session.query(models.ResourceProvider).filter_by(
1116             id=id).first()
1117         db_rp.update(updates)
1118         context.session.add(db_rp)
1119 
1120         # We should also update the root providers of resource providers
1121         # originally in the same tree. If re-parenting is supported,
1122         # this logic should be changed to update only descendents of the
1123         # re-parented resource providers, not all the providers in the tree.
1124         for rp in same_tree:
1125             # If the parent is not updated, this clause is skipped since the
1126             # `same_tree` has no element.
1127             rp.root_provider_uuid = parent_ids.root_uuid
1128             db_rp = context.session.query(
1129                 models.ResourceProvider).filter_by(id=rp.id).first()
1130             data = {'root_provider_id': parent_ids.root_id}
1131             db_rp.update(data)
1132             context.session.add(db_rp)
1133 
1134         try:
1135             context.session.flush()
1136         except sqla_exc.IntegrityError:
1137             # NOTE(jaypipes): Another thread snuck in and deleted the parent
1138             # for this resource provider in between the above check for a valid
1139             # parent provider and here...
1140             raise exception.ObjectActionError(
1141                     action='update',
1142                     reason=_('parent provider UUID does not exist.'))
1143 
1144     @staticmethod
1145     @db_api.placement_context_manager.writer  # For online data migration
1146     def _from_db_object(context, resource_provider, db_resource_provider):
1147         # Online data migration to populate root_provider_id
1148         # TODO(jaypipes): Remove when all root_provider_id values are NOT NULL
1149         if db_resource_provider['root_provider_uuid'] is None:
1150             rp_id = db_resource_provider['id']
1151             uuid = db_resource_provider['uuid']
1152             db_resource_provider['root_provider_uuid'] = uuid
1153             _set_root_provider_id(context, rp_id, rp_id)
1154         for field in resource_provider.fields:
1155             setattr(resource_provider, field, db_resource_provider[field])
1156         resource_provider._context = context
1157         resource_provider.obj_reset_changes()
1158         return resource_provider
1159 
1160 
1161 @db_api.placement_context_manager.reader
1162 def _get_providers_with_shared_capacity(ctx, rc_id, amount, member_of=None):
1163     """Returns a list of resource provider IDs (internal IDs, not UUIDs)
1164     that have capacity for a requested amount of a resource and indicate that
1165     they share resource via an aggregate association.
1166 
1167     Shared resource providers are marked with a standard trait called
1168     MISC_SHARES_VIA_AGGREGATE. This indicates that the provider allows its
1169     inventory to be consumed by other resource providers associated via an
1170     aggregate link.
1171 
1172     For example, assume we have two compute nodes, CN_1 and CN_2, each with
1173     inventory of VCPU and MEMORY_MB but not DISK_GB (in other words, these are
1174     compute nodes with no local disk). There is a resource provider called
1175     "NFS_SHARE" that has an inventory of DISK_GB and has the
1176     MISC_SHARES_VIA_AGGREGATE trait. Both the "CN_1" and "CN_2" compute node
1177     resource providers and the "NFS_SHARE" resource provider are associated
1178     with an aggregate called "AGG_1".
1179 
1180     The scheduler needs to determine the resource providers that can fulfill a
1181     request for 2 VCPU, 1024 MEMORY_MB and 100 DISK_GB.
1182 
1183     Clearly, no single provider can satisfy the request for all three
1184     resources, since neither compute node has DISK_GB inventory and the
1185     NFS_SHARE provider has no VCPU or MEMORY_MB inventories.
1186 
1187     However, if we consider the NFS_SHARE resource provider as providing
1188     inventory of DISK_GB for both CN_1 and CN_2, we can include CN_1 and CN_2
1189     as potential fits for the requested set of resources.
1190 
1191     To facilitate that matching query, this function returns all providers that
1192     indicate they share their inventory with providers in some aggregate and
1193     have enough capacity for the requested amount of a resource.
1194 
1195     To follow the example above, if we were to call
1196     _get_providers_with_shared_capacity(ctx, "DISK_GB", 100), we would want to
1197     get back the ID for the NFS_SHARE resource provider.
1198 
1199     :param rc_id: Internal ID of the requested resource class.
1200     :param amount: Amount of the requested resource.
1201     :param member_of: When present, contains a list of lists of aggregate
1202                       uuids that are used to filter the returned list of
1203                       resource providers that *directly* belong to the
1204                       aggregates referenced.
1205     """
1206     # The SQL we need to generate here looks like this:
1207     #
1208     # SELECT rp.id
1209     # FROM resource_providers AS rp
1210     #   INNER JOIN resource_provider_traits AS rpt
1211     #     ON rp.id = rpt.resource_provider_id
1212     #   INNER JOIN traits AS t
1213     #     ON rpt.trait_id = t.id
1214     #     AND t.name = "MISC_SHARES_VIA_AGGREGATE"
1215     #   INNER JOIN inventories AS inv
1216     #     ON rp.id = inv.resource_provider_id
1217     #     AND inv.resource_class_id = $rc_id
1218     #   LEFT JOIN (
1219     #     SELECT resource_provider_id, SUM(used) as used
1220     #     FROM allocations
1221     #     WHERE resource_class_id = $rc_id
1222     #     GROUP BY resource_provider_id
1223     #   ) AS usage
1224     #     ON rp.id = usage.resource_provider_id
1225     # WHERE COALESCE(usage.used, 0) + $amount <= (
1226     #   inv.total - inv.reserved) * inv.allocation_ratio
1227     # ) AND
1228     #   inv.min_unit <= $amount AND
1229     #   inv.max_unit >= $amount AND
1230     #   $amount % inv.step_size = 0
1231     # GROUP BY rp.id
1232 
1233     rp_tbl = sa.alias(_RP_TBL, name='rp')
1234     inv_tbl = sa.alias(_INV_TBL, name='inv')
1235     t_tbl = sa.alias(_TRAIT_TBL, name='t')
1236     rpt_tbl = sa.alias(_RP_TRAIT_TBL, name='rpt')
1237 
1238     rp_to_rpt_join = sa.join(
1239         rp_tbl, rpt_tbl,
1240         rp_tbl.c.id == rpt_tbl.c.resource_provider_id,
1241     )
1242 
1243     rpt_to_t_join = sa.join(
1244         rp_to_rpt_join, t_tbl,
1245         sa.and_(
1246             rpt_tbl.c.trait_id == t_tbl.c.id,
1247             # The traits table wants unicode trait names, but os_traits
1248             # presents native str, so we need to cast.
1249             t_tbl.c.name == six.text_type(os_traits.MISC_SHARES_VIA_AGGREGATE),
1250         ),
1251     )
1252 
1253     rp_to_inv_join = sa.join(
1254         rpt_to_t_join, inv_tbl,
1255         sa.and_(
1256             rpt_tbl.c.resource_provider_id == inv_tbl.c.resource_provider_id,
1257             inv_tbl.c.resource_class_id == rc_id,
1258         ),
1259     )
1260 
1261     usage = sa.select([_ALLOC_TBL.c.resource_provider_id,
1262                        sql.func.sum(_ALLOC_TBL.c.used).label('used')])
1263     usage = usage.where(_ALLOC_TBL.c.resource_class_id == rc_id)
1264     usage = usage.group_by(_ALLOC_TBL.c.resource_provider_id)
1265     usage = sa.alias(usage, name='usage')
1266 
1267     inv_to_usage_join = sa.outerjoin(
1268         rp_to_inv_join, usage,
1269         inv_tbl.c.resource_provider_id == usage.c.resource_provider_id,
1270     )
1271 
1272     where_conds = sa.and_(
1273         func.coalesce(usage.c.used, 0) + amount <= (
1274             inv_tbl.c.total - inv_tbl.c.reserved) * inv_tbl.c.allocation_ratio,
1275         inv_tbl.c.min_unit <= amount,
1276         inv_tbl.c.max_unit >= amount,
1277         amount % inv_tbl.c.step_size == 0)
1278 
1279     # If 'member_of' has values, do a separate lookup to identify the
1280     # resource providers that meet the member_of constraints.
1281     if member_of:
1282         rps_in_aggs = _provider_ids_matching_aggregates(ctx, member_of)
1283         if not rps_in_aggs:
1284             # Short-circuit. The user either asked for a non-existing
1285             # aggregate or there were no resource providers that matched
1286             # the requirements...
1287             return []
1288         where_conds.append(rp_tbl.c.id.in_(rps_in_aggs))
1289 
1290     sel = sa.select([rp_tbl.c.id]).select_from(inv_to_usage_join)
1291     sel = sel.where(where_conds)
1292     sel = sel.group_by(rp_tbl.c.id)
1293 
1294     return [r[0] for r in ctx.session.execute(sel)]
1295 
1296 
1297 @base.VersionedObjectRegistry.register_if(False)
1298 class ResourceProviderList(base.ObjectListBase, base.VersionedObject):
1299 
1300     fields = {
1301         'objects': fields.ListOfObjectsField('ResourceProvider'),
1302     }
1303 
1304     @staticmethod
1305     @db_api.placement_context_manager.reader
1306     def _get_all_by_filters_from_db(context, filters):
1307         # Eg. filters can be:
1308         #  filters = {
1309         #      'name': <name>,
1310         #      'uuid': <uuid>,
1311         #      'member_of': [[<aggregate_uuid>, <aggregate_uuid>],
1312         #                    [<aggregate_uuid>]]
1313         #      'resources': {
1314         #          'VCPU': 1,
1315         #          'MEMORY_MB': 1024
1316         #      },
1317         #      'in_tree': <uuid>,
1318         #      'required': [<trait_name>, ...]
1319         #  }
1320         if not filters:
1321             filters = {}
1322         else:
1323             # Since we modify the filters, copy them so that we don't modify
1324             # them in the calling program.
1325             filters = copy.deepcopy(filters)
1326         name = filters.pop('name', None)
1327         uuid = filters.pop('uuid', None)
1328         member_of = filters.pop('member_of', [])
1329         required = set(filters.pop('required', []))
1330         forbidden = set([trait for trait in required
1331                          if trait.startswith('!')])
1332         required = required - forbidden
1333         forbidden = set([trait.lstrip('!') for trait in forbidden])
1334 
1335         resources = filters.pop('resources', {})
1336         # NOTE(sbauza): We want to key the dict by the resource class IDs
1337         # and we want to make sure those class names aren't incorrect.
1338         resources = {_RC_CACHE.id_from_string(r_name): amount
1339                      for r_name, amount in resources.items()}
1340         rp = sa.alias(_RP_TBL, name="rp")
1341         root_rp = sa.alias(_RP_TBL, name="root_rp")
1342         parent_rp = sa.alias(_RP_TBL, name="parent_rp")
1343 
1344         cols = [
1345             rp.c.id,
1346             rp.c.uuid,
1347             rp.c.name,
1348             rp.c.generation,
1349             rp.c.updated_at,
1350             rp.c.created_at,
1351             root_rp.c.uuid.label("root_provider_uuid"),
1352             parent_rp.c.uuid.label("parent_provider_uuid"),
1353         ]
1354 
1355         # TODO(jaypipes): Convert this to an inner join once all
1356         # root_provider_id values are NOT NULL
1357         rp_to_root = sa.outerjoin(rp, root_rp,
1358             rp.c.root_provider_id == root_rp.c.id)
1359         rp_to_parent = sa.outerjoin(rp_to_root, parent_rp,
1360             rp.c.parent_provider_id == parent_rp.c.id)
1361 
1362         query = sa.select(cols).select_from(rp_to_parent)
1363 
1364         if name:
1365             query = query.where(rp.c.name == name)
1366         if uuid:
1367             query = query.where(rp.c.uuid == uuid)
1368         if 'in_tree' in filters:
1369             # The 'in_tree' parameter is the UUID of a resource provider that
1370             # the caller wants to limit the returned providers to only those
1371             # within its "provider tree". So, we look up the resource provider
1372             # having the UUID specified by the 'in_tree' parameter and grab the
1373             # root_provider_id value of that record. We can then ask for only
1374             # those resource providers having a root_provider_id of that value.
1375             tree_uuid = filters.pop('in_tree')
1376             tree_ids = _provider_ids_from_uuid(context, tree_uuid)
1377             if tree_ids is None:
1378                 # List operations should simply return an empty list when a
1379                 # non-existing resource provider UUID is given.
1380                 return []
1381             root_id = tree_ids.root_id
1382             # TODO(jaypipes): Remove this OR condition when root_provider_id
1383             # is not nullable in the database and all resource provider records
1384             # have populated the root provider ID.
1385             where_cond = sa.or_(rp.c.id == root_id,
1386                 rp.c.root_provider_id == root_id)
1387             query = query.where(where_cond)
1388 
1389         # If 'member_of' has values, do a separate lookup to identify the
1390         # resource providers that meet the member_of constraints.
1391         if member_of:
1392             rps_in_aggs = _provider_ids_matching_aggregates(context, member_of)
1393             if not rps_in_aggs:
1394                 # Short-circuit. The user either asked for a non-existing
1395                 # aggregate or there were no resource providers that matched
1396                 # the requirements...
1397                 return []
1398             query = query.where(rp.c.id.in_(rps_in_aggs))
1399 
1400         # If 'required' has values, add a filter to limit results to providers
1401         # possessing *all* of the listed traits.
1402         if required:
1403             trait_map = _trait_ids_from_names(context, required)
1404             if len(trait_map) != len(required):
1405                 missing = required - set(trait_map)
1406                 raise exception.TraitNotFound(names=', '.join(missing))
1407             rp_ids = _get_provider_ids_having_all_traits(context, trait_map)
1408             if not rp_ids:
1409                 # If no providers have the required traits, we're done
1410                 return []
1411             query = query.where(rp.c.id.in_(rp_ids))
1412 
1413         # If 'forbidden' has values, filter out those providers that have
1414         # that trait as one their traits.
1415         if forbidden:
1416             trait_map = _trait_ids_from_names(context, forbidden)
1417             if len(trait_map) != len(forbidden):
1418                 missing = forbidden - set(trait_map)
1419                 raise exception.TraitNotFound(names=', '.join(missing))
1420             rp_ids = _get_provider_ids_having_any_trait(context, trait_map)
1421             if rp_ids:
1422                 query = query.where(~rp.c.id.in_(rp_ids))
1423 
1424         if not resources:
1425             # Returns quickly the list in case we don't need to check the
1426             # resource usage
1427             res = context.session.execute(query).fetchall()
1428             return [dict(r) for r in res]
1429 
1430         # NOTE(sbauza): In case we want to look at the resource criteria, then
1431         # the SQL generated from this case looks something like:
1432         # SELECT
1433         #   rp.*
1434         # FROM resource_providers AS rp
1435         # JOIN inventories AS inv
1436         # ON rp.id = inv.resource_provider_id
1437         # LEFT JOIN (
1438         #    SELECT resource_provider_id, resource_class_id, SUM(used) AS used
1439         #    FROM allocations
1440         #    WHERE resource_class_id IN ($RESOURCE_CLASSES)
1441         #    GROUP BY resource_provider_id, resource_class_id
1442         # ) AS usage
1443         #     ON inv.resource_provider_id = usage.resource_provider_id
1444         #     AND inv.resource_class_id = usage.resource_class_id
1445         # AND (inv.resource_class_id = $X AND (used + $AMOUNT_X <= (
1446         #        total - reserved) * inv.allocation_ratio) AND
1447         #        inv.min_unit <= $AMOUNT_X AND inv.max_unit >= $AMOUNT_X AND
1448         #        $AMOUNT_X % inv.step_size == 0)
1449         #      OR (inv.resource_class_id = $Y AND (used + $AMOUNT_Y <= (
1450         #        total - reserved) * inv.allocation_ratio) AND
1451         #        inv.min_unit <= $AMOUNT_Y AND inv.max_unit >= $AMOUNT_Y AND
1452         #        $AMOUNT_Y % inv.step_size == 0)
1453         #      OR (inv.resource_class_id = $Z AND (used + $AMOUNT_Z <= (
1454         #        total - reserved) * inv.allocation_ratio) AND
1455         #        inv.min_unit <= $AMOUNT_Z AND inv.max_unit >= $AMOUNT_Z AND
1456         #        $AMOUNT_Z % inv.step_size == 0))
1457         # GROUP BY rp.id
1458         # HAVING
1459         #  COUNT(DISTINCT(inv.resource_class_id)) == len($RESOURCE_CLASSES)
1460         #
1461         # with a possible additional WHERE clause for the name and uuid that
1462         # comes from the above filters
1463 
1464         # First JOIN between inventories and RPs is here
1465         inv_join = sa.join(rp_to_parent, _INV_TBL,
1466             rp.c.id == _INV_TBL.c.resource_provider_id)
1467 
1468         # Now, below is the LEFT JOIN for getting the allocations usage
1469         usage = sa.select([_ALLOC_TBL.c.resource_provider_id,
1470                            _ALLOC_TBL.c.resource_class_id,
1471                            sql.func.sum(_ALLOC_TBL.c.used).label('used')])
1472         usage = usage.where(_ALLOC_TBL.c.resource_class_id.in_(resources))
1473         usage = usage.group_by(_ALLOC_TBL.c.resource_provider_id,
1474                                _ALLOC_TBL.c.resource_class_id)
1475         usage = sa.alias(usage, name='usage')
1476         usage_join = sa.outerjoin(inv_join, usage,
1477             sa.and_(
1478                 usage.c.resource_provider_id == (
1479                     _INV_TBL.c.resource_provider_id),
1480                 usage.c.resource_class_id == _INV_TBL.c.resource_class_id))
1481 
1482         # And finally, we verify for each resource class if the requested
1483         # amount isn't more than the left space (considering the allocation
1484         # ratio, the reserved space and the min and max amount possible sizes)
1485         where_clauses = [
1486             sa.and_(
1487                 _INV_TBL.c.resource_class_id == r_idx,
1488                 (func.coalesce(usage.c.used, 0) + amount <= (
1489                     _INV_TBL.c.total - _INV_TBL.c.reserved
1490                 ) * _INV_TBL.c.allocation_ratio),
1491                 _INV_TBL.c.min_unit <= amount,
1492                 _INV_TBL.c.max_unit >= amount,
1493                 amount % _INV_TBL.c.step_size == 0
1494             )
1495             for (r_idx, amount) in resources.items()]
1496         query = query.select_from(usage_join)
1497         query = query.where(sa.or_(*where_clauses))
1498         query = query.group_by(rp.c.id, root_rp.c.uuid, parent_rp.c.uuid)
1499         # NOTE(sbauza): Only RPs having all the asked resources can be provided
1500         query = query.having(sql.func.count(
1501             sa.distinct(_INV_TBL.c.resource_class_id)) == len(resources))
1502 
1503         res = context.session.execute(query).fetchall()
1504         return [dict(r) for r in res]
1505 
1506     @classmethod
1507     def get_all_by_filters(cls, context, filters=None):
1508         """Returns a list of `ResourceProvider` objects that have sufficient
1509         resources in their inventories to satisfy the amounts specified in the
1510         `filters` parameter.
1511 
1512         If no resource providers can be found, the function will return an
1513         empty list.
1514 
1515         :param context: `nova.context.RequestContext` that may be used to grab
1516                         a DB connection.
1517         :param filters: Can be `name`, `uuid`, `member_of`, `in_tree` or
1518                         `resources` where `member_of` is a list of list of
1519                         aggregate UUIDs, `in_tree` is a UUID of a resource
1520                         provider that we can use to find the root provider ID
1521                         of the tree of providers to filter results by and
1522                         `resources` is a dict of amounts keyed by resource
1523                         classes.
1524         :type filters: dict
1525         """
1526         resource_providers = cls._get_all_by_filters_from_db(context, filters)
1527         return base.obj_make_list(context, cls(context),
1528                                   ResourceProvider, resource_providers)
1529 
1530 
1531 @base.VersionedObjectRegistry.register_if(False)
1532 class Inventory(base.VersionedObject, base.TimestampedObject):
1533 
1534     fields = {
1535         'id': fields.IntegerField(read_only=True),
1536         'resource_provider': fields.ObjectField('ResourceProvider'),
1537         'resource_class': rc_fields.ResourceClassField(read_only=True),
1538         'total': fields.NonNegativeIntegerField(),
1539         'reserved': fields.NonNegativeIntegerField(default=0),
1540         'min_unit': fields.NonNegativeIntegerField(default=1),
1541         'max_unit': fields.NonNegativeIntegerField(default=1),
1542         'step_size': fields.NonNegativeIntegerField(default=1),
1543         'allocation_ratio': fields.NonNegativeFloatField(default=1.0),
1544     }
1545 
1546     @property
1547     def capacity(self):
1548         """Inventory capacity, adjusted by allocation_ratio."""
1549         return int((self.total - self.reserved) * self.allocation_ratio)
1550 
1551 
1552 @db_api.placement_context_manager.reader
1553 def _get_inventory_by_provider_id(ctx, rp_id):
1554     inv = sa.alias(_INV_TBL, name="i")
1555     cols = [
1556         inv.c.resource_class_id,
1557         inv.c.total,
1558         inv.c.reserved,
1559         inv.c.min_unit,
1560         inv.c.max_unit,
1561         inv.c.step_size,
1562         inv.c.allocation_ratio,
1563         inv.c.updated_at,
1564         inv.c.created_at,
1565     ]
1566     sel = sa.select(cols)
1567     sel = sel.where(inv.c.resource_provider_id == rp_id)
1568 
1569     return [dict(r) for r in ctx.session.execute(sel)]
1570 
1571 
1572 @base.VersionedObjectRegistry.register_if(False)
1573 class InventoryList(base.ObjectListBase, base.VersionedObject):
1574 
1575     fields = {
1576         'objects': fields.ListOfObjectsField('Inventory'),
1577     }
1578 
1579     def find(self, res_class):
1580         """Return the inventory record from the list of Inventory records that
1581         matches the supplied resource class, or None.
1582 
1583         :param res_class: An integer or string representing a resource
1584                           class. If the value is a string, the method first
1585                           looks up the resource class identifier from the
1586                           string.
1587         """
1588         if not isinstance(res_class, six.string_types):
1589             raise ValueError
1590 
1591         for inv_rec in self.objects:
1592             if inv_rec.resource_class == res_class:
1593                 return inv_rec
1594 
1595     @classmethod
1596     def get_all_by_resource_provider(cls, context, rp):
1597         db_inv = _get_inventory_by_provider_id(context, rp.id)
1598         # Build up a list of Inventory objects, setting the Inventory object
1599         # fields to the same-named database record field we got from
1600         # _get_inventory_by_provider_id(). We already have the ResourceProvider
1601         # object so we just pass that object to the Inventory object
1602         # constructor as-is
1603         objs = [
1604             Inventory(
1605                 context, resource_provider=rp,
1606                 resource_class=_RC_CACHE.string_from_id(
1607                     rec['resource_class_id']),
1608                 **rec)
1609             for rec in db_inv
1610         ]
1611         inv_list = cls(context, objects=objs)
1612         return inv_list
1613 
1614 
1615 @base.VersionedObjectRegistry.register_if(False)
1616 class Allocation(base.VersionedObject, base.TimestampedObject):
1617 
1618     fields = {
1619         'id': fields.IntegerField(),
1620         'resource_provider': fields.ObjectField('ResourceProvider'),
1621         'consumer': fields.ObjectField('Consumer', nullable=False),
1622         'resource_class': rc_fields.ResourceClassField(),
1623         'used': fields.IntegerField(),
1624     }
1625 
1626 
1627 @db_api.placement_context_manager.writer
1628 def _delete_allocations_for_consumer(ctx, consumer_id):
1629     """Deletes any existing allocations that correspond to the allocations to
1630     be written. This is wrapped in a transaction, so if the write subsequently
1631     fails, the deletion will also be rolled back.
1632     """
1633     del_sql = _ALLOC_TBL.delete().where(
1634         _ALLOC_TBL.c.consumer_id == consumer_id)
1635     ctx.session.execute(del_sql)
1636 
1637 
1638 @db_api.placement_context_manager.writer
1639 def _delete_allocations_by_ids(ctx, alloc_ids):
1640     """Deletes allocations having an internal id value in the set of supplied
1641     IDs
1642     """
1643     del_sql = _ALLOC_TBL.delete().where(_ALLOC_TBL.c.id.in_(alloc_ids))
1644     ctx.session.execute(del_sql)
1645 
1646 
1647 def _check_capacity_exceeded(ctx, allocs):
1648     """Checks to see if the supplied allocation records would result in any of
1649     the inventories involved having their capacity exceeded.
1650 
1651     Raises an InvalidAllocationCapacityExceeded exception if any inventory
1652     would be exhausted by the allocation. Raises an
1653     InvalidAllocationConstraintsViolated exception if any of the `step_size`,
1654     `min_unit` or `max_unit` constraints in an inventory will be violated
1655     by any one of the allocations.
1656 
1657     If no inventories would be exceeded or violated by the allocations, the
1658     function returns a list of `ResourceProvider` objects that contain the
1659     generation at the time of the check.
1660 
1661     :param ctx: `nova.context.RequestContext` that has an oslo_db Session
1662     :param allocs: List of `Allocation` objects to check
1663     """
1664     # The SQL generated below looks like this:
1665     # SELECT
1666     #   rp.id,
1667     #   rp.uuid,
1668     #   rp.generation,
1669     #   inv.resource_class_id,
1670     #   inv.total,
1671     #   inv.reserved,
1672     #   inv.allocation_ratio,
1673     #   allocs.used
1674     # FROM resource_providers AS rp
1675     # JOIN inventories AS i1
1676     # ON rp.id = i1.resource_provider_id
1677     # LEFT JOIN (
1678     #    SELECT resource_provider_id, resource_class_id, SUM(used) AS used
1679     #    FROM allocations
1680     #    WHERE resource_class_id IN ($RESOURCE_CLASSES)
1681     #    AND resource_provider_id IN ($RESOURCE_PROVIDERS)
1682     #    GROUP BY resource_provider_id, resource_class_id
1683     # ) AS allocs
1684     # ON inv.resource_provider_id = allocs.resource_provider_id
1685     # AND inv.resource_class_id = allocs.resource_class_id
1686     # WHERE rp.id IN ($RESOURCE_PROVIDERS)
1687     # AND inv.resource_class_id IN ($RESOURCE_CLASSES)
1688     #
1689     # We then take the results of the above and determine if any of the
1690     # inventory will have its capacity exceeded.
1691     rc_ids = set([_RC_CACHE.id_from_string(a.resource_class)
1692                        for a in allocs])
1693     provider_uuids = set([a.resource_provider.uuid for a in allocs])
1694     provider_ids = set([a.resource_provider.id for a in allocs])
1695     usage = sa.select([_ALLOC_TBL.c.resource_provider_id,
1696                        _ALLOC_TBL.c.resource_class_id,
1697                        sql.func.sum(_ALLOC_TBL.c.used).label('used')])
1698     usage = usage.where(
1699             sa.and_(_ALLOC_TBL.c.resource_class_id.in_(rc_ids),
1700                     _ALLOC_TBL.c.resource_provider_id.in_(provider_ids)))
1701     usage = usage.group_by(_ALLOC_TBL.c.resource_provider_id,
1702                            _ALLOC_TBL.c.resource_class_id)
1703     usage = sa.alias(usage, name='usage')
1704 
1705     inv_join = sql.join(_RP_TBL, _INV_TBL,
1706             sql.and_(_RP_TBL.c.id == _INV_TBL.c.resource_provider_id,
1707                      _INV_TBL.c.resource_class_id.in_(rc_ids)))
1708     primary_join = sql.outerjoin(inv_join, usage,
1709         sql.and_(
1710             _INV_TBL.c.resource_provider_id == usage.c.resource_provider_id,
1711             _INV_TBL.c.resource_class_id == usage.c.resource_class_id)
1712     )
1713     cols_in_output = [
1714         _RP_TBL.c.id.label('resource_provider_id'),
1715         _RP_TBL.c.uuid,
1716         _RP_TBL.c.generation,
1717         _INV_TBL.c.resource_class_id,
1718         _INV_TBL.c.total,
1719         _INV_TBL.c.reserved,
1720         _INV_TBL.c.allocation_ratio,
1721         _INV_TBL.c.min_unit,
1722         _INV_TBL.c.max_unit,
1723         _INV_TBL.c.step_size,
1724         usage.c.used,
1725     ]
1726 
1727     sel = sa.select(cols_in_output).select_from(primary_join)
1728     sel = sel.where(
1729             sa.and_(_RP_TBL.c.id.in_(provider_ids),
1730                     _INV_TBL.c.resource_class_id.in_(rc_ids)))
1731     records = ctx.session.execute(sel)
1732     # Create a map keyed by (rp_uuid, res_class) for the records in the DB
1733     usage_map = {}
1734     provs_with_inv = set()
1735     for record in records:
1736         map_key = (record['uuid'], record['resource_class_id'])
1737         if map_key in usage_map:
1738             raise KeyError("%s already in usage_map, bad query" % str(map_key))
1739         usage_map[map_key] = record
1740         provs_with_inv.add(record["uuid"])
1741     # Ensure that all providers have existing inventory
1742     missing_provs = provider_uuids - provs_with_inv
1743     if missing_provs:
1744         class_str = ', '.join([_RC_CACHE.string_from_id(rc_id)
1745                                for rc_id in rc_ids])
1746         provider_str = ', '.join(missing_provs)
1747         raise exception.InvalidInventory(resource_class=class_str,
1748                 resource_provider=provider_str)
1749 
1750     res_providers = {}
1751     rp_resource_class_sum = collections.defaultdict(
1752         lambda: collections.defaultdict(int))
1753     for alloc in allocs:
1754         rc_id = _RC_CACHE.id_from_string(alloc.resource_class)
1755         rp_uuid = alloc.resource_provider.uuid
1756         if rp_uuid not in res_providers:
1757             res_providers[rp_uuid] = alloc.resource_provider
1758         amount_needed = alloc.used
1759         rp_resource_class_sum[rp_uuid][rc_id] += amount_needed
1760         # No use checking usage if we're not asking for anything
1761         if amount_needed == 0:
1762             continue
1763         key = (rp_uuid, rc_id)
1764         try:
1765             usage = usage_map[key]
1766         except KeyError:
1767             # The resource class at rc_id is not in the usage map.
1768             raise exception.InvalidInventory(
1769                     resource_class=alloc.resource_class,
1770                     resource_provider=rp_uuid)
1771         allocation_ratio = usage['allocation_ratio']
1772         min_unit = usage['min_unit']
1773         max_unit = usage['max_unit']
1774         step_size = usage['step_size']
1775 
1776         # check min_unit, max_unit, step_size
1777         if (amount_needed < min_unit or amount_needed > max_unit or
1778                 amount_needed % step_size != 0):
1779             LOG.warning(
1780                 "Allocation for %(rc)s on resource provider %(rp)s "
1781                 "violates min_unit, max_unit, or step_size. "
1782                 "Requested: %(requested)s, min_unit: %(min_unit)s, "
1783                 "max_unit: %(max_unit)s, step_size: %(step_size)s",
1784                 {'rc': alloc.resource_class,
1785                  'rp': rp_uuid,
1786                  'requested': amount_needed,
1787                  'min_unit': min_unit,
1788                  'max_unit': max_unit,
1789                  'step_size': step_size})
1790             raise exception.InvalidAllocationConstraintsViolated(
1791                 resource_class=alloc.resource_class,
1792                 resource_provider=rp_uuid)
1793 
1794         # usage["used"] can be returned as None
1795         used = usage['used'] or 0
1796         capacity = (usage['total'] - usage['reserved']) * allocation_ratio
1797         if (capacity < (used + amount_needed) or
1798             capacity < (used + rp_resource_class_sum[rp_uuid][rc_id])):
1799             LOG.warning(
1800                 "Over capacity for %(rc)s on resource provider %(rp)s. "
1801                 "Needed: %(needed)s, Used: %(used)s, Capacity: %(cap)s",
1802                 {'rc': alloc.resource_class,
1803                  'rp': rp_uuid,
1804                  'needed': amount_needed,
1805                  'used': used,
1806                  'cap': capacity})
1807             raise exception.InvalidAllocationCapacityExceeded(
1808                 resource_class=alloc.resource_class,
1809                 resource_provider=rp_uuid)
1810     return res_providers
1811 
1812 
1813 @db_api.placement_context_manager.reader
1814 def _get_allocations_by_provider_id(ctx, rp_id):
1815     allocs = sa.alias(_ALLOC_TBL, name="a")
1816     consumers = sa.alias(_CONSUMER_TBL, name="c")
1817     projects = sa.alias(_PROJECT_TBL, name="p")
1818     users = sa.alias(_USER_TBL, name="u")
1819     cols = [
1820         allocs.c.id,
1821         allocs.c.resource_class_id,
1822         allocs.c.used,
1823         allocs.c.updated_at,
1824         allocs.c.created_at,
1825         consumers.c.id.label("consumer_id"),
1826         consumers.c.generation.label("consumer_generation"),
1827         sql.func.coalesce(
1828             consumers.c.uuid, allocs.c.consumer_id).label("consumer_uuid"),
1829         projects.c.id.label("project_id"),
1830         projects.c.external_id.label("project_external_id"),
1831         users.c.id.label("user_id"),
1832         users.c.external_id.label("user_external_id"),
1833     ]
1834     # TODO(jaypipes): change this join to be on ID not UUID
1835     consumers_join = sa.join(
1836         allocs, consumers, allocs.c.consumer_id == consumers.c.uuid)
1837     projects_join = sa.join(
1838         consumers_join, projects, consumers.c.project_id == projects.c.id)
1839     users_join = sa.join(
1840         projects_join, users, consumers.c.user_id == users.c.id)
1841     sel = sa.select(cols).select_from(users_join)
1842     sel = sel.where(allocs.c.resource_provider_id == rp_id)
1843 
1844     return [dict(r) for r in ctx.session.execute(sel)]
1845 
1846 
1847 @db_api.placement_context_manager.reader
1848 def _get_allocations_by_consumer_uuid(ctx, consumer_uuid):
1849     allocs = sa.alias(_ALLOC_TBL, name="a")
1850     rp = sa.alias(_RP_TBL, name="rp")
1851     consumer = sa.alias(_CONSUMER_TBL, name="c")
1852     project = sa.alias(_PROJECT_TBL, name="p")
1853     user = sa.alias(_USER_TBL, name="u")
1854     cols = [
1855         allocs.c.id,
1856         allocs.c.resource_provider_id,
1857         rp.c.name.label("resource_provider_name"),
1858         rp.c.uuid.label("resource_provider_uuid"),
1859         rp.c.generation.label("resource_provider_generation"),
1860         allocs.c.resource_class_id,
1861         allocs.c.used,
1862         consumer.c.id.label("consumer_id"),
1863         consumer.c.generation.label("consumer_generation"),
1864         sql.func.coalesce(
1865             consumer.c.uuid, allocs.c.consumer_id).label("consumer_uuid"),
1866         project.c.id.label("project_id"),
1867         project.c.external_id.label("project_external_id"),
1868         user.c.id.label("user_id"),
1869         user.c.external_id.label("user_external_id"),
1870     ]
1871     # Build up the joins of the five tables we need to interact with.
1872     rp_join = sa.join(allocs, rp, allocs.c.resource_provider_id == rp.c.id)
1873     consumer_join = sa.join(rp_join, consumer,
1874                             allocs.c.consumer_id == consumer.c.uuid)
1875     project_join = sa.join(consumer_join, project,
1876                            consumer.c.project_id == project.c.id)
1877     user_join = sa.join(project_join, user,
1878                         consumer.c.user_id == user.c.id)
1879 
1880     sel = sa.select(cols).select_from(user_join)
1881     sel = sel.where(allocs.c.consumer_id == consumer_uuid)
1882 
1883     return [dict(r) for r in ctx.session.execute(sel)]
1884 
1885 
1886 @db_api.placement_context_manager.writer.independent
1887 def _create_incomplete_consumers_for_provider(ctx, rp_id):
1888     # TODO(jaypipes): Remove in Stein after a blocker migration is added.
1889     """Creates consumer record if consumer relationship between allocations ->
1890     consumers table is missing for any allocation on the supplied provider
1891     internal ID, using the "incomplete consumer" project and user CONF options.
1892     """
1893     alloc_to_consumer = sa.outerjoin(
1894         _ALLOC_TBL, consumer_obj.CONSUMER_TBL,
1895         _ALLOC_TBL.c.consumer_id == consumer_obj.CONSUMER_TBL.c.uuid)
1896     sel = sa.select([_ALLOC_TBL.c.consumer_id])
1897     sel = sel.select_from(alloc_to_consumer)
1898     sel = sel.where(
1899         sa.and_(
1900             _ALLOC_TBL.c.resource_provider_id == rp_id,
1901             consumer_obj.CONSUMER_TBL.c.id.is_(None)))
1902     missing = ctx.session.execute(sel).fetchall()
1903     if missing:
1904         # Do a single INSERT for all missing consumer relationships for the
1905         # provider
1906         incomplete_proj_id = project_obj.ensure_incomplete_project(ctx)
1907         incomplete_user_id = user_obj.ensure_incomplete_user(ctx)
1908 
1909         cols = [
1910             _ALLOC_TBL.c.consumer_id,
1911             incomplete_proj_id,
1912             incomplete_user_id,
1913         ]
1914         sel = sa.select(cols)
1915         sel = sel.select_from(alloc_to_consumer)
1916         sel = sel.where(
1917             sa.and_(
1918                 _ALLOC_TBL.c.resource_provider_id == rp_id,
1919                 consumer_obj.CONSUMER_TBL.c.id.is_(None)))
1920         target_cols = ['uuid', 'project_id', 'user_id']
1921         ins_stmt = consumer_obj.CONSUMER_TBL.insert().from_select(
1922             target_cols, sel)
1923         res = ctx.session.execute(ins_stmt)
1924         if res.rowcount > 0:
1925             LOG.info("Online data migration to fix incomplete consumers "
1926                      "for resource provider %s has been run. Migrated %d "
1927                      "incomplete consumer records on the fly.", rp_id,
1928                      res.rowcount)
1929 
1930 
1931 @db_api.placement_context_manager.writer.independent
1932 def _create_incomplete_consumer(ctx, consumer_id):
1933     # TODO(jaypipes): Remove in Stein after a blocker migration is added.
1934     """Creates consumer record if consumer relationship between allocations ->
1935     consumers table is missing for the supplied consumer UUID, using the
1936     "incomplete consumer" project and user CONF options.
1937     """
1938     alloc_to_consumer = sa.outerjoin(
1939         _ALLOC_TBL, consumer_obj.CONSUMER_TBL,
1940         _ALLOC_TBL.c.consumer_id == consumer_obj.CONSUMER_TBL.c.uuid)
1941     sel = sa.select([_ALLOC_TBL.c.consumer_id])
1942     sel = sel.select_from(alloc_to_consumer)
1943     sel = sel.where(
1944         sa.and_(
1945             _ALLOC_TBL.c.consumer_id == consumer_id,
1946             consumer_obj.CONSUMER_TBL.c.id.is_(None)))
1947     missing = ctx.session.execute(sel).fetchall()
1948     if missing:
1949         incomplete_proj_id = project_obj.ensure_incomplete_project(ctx)
1950         incomplete_user_id = user_obj.ensure_incomplete_user(ctx)
1951 
1952         ins_stmt = consumer_obj.CONSUMER_TBL.insert().values(
1953             uuid=consumer_id, project_id=incomplete_proj_id,
1954             user_id=incomplete_user_id)
1955         res = ctx.session.execute(ins_stmt)
1956         if res.rowcount > 0:
1957             LOG.info("Online data migration to fix incomplete consumers "
1958                      "for consumer %s has been run. Migrated %d incomplete "
1959                      "consumer records on the fly.", consumer_id, res.rowcount)
1960 
1961 
1962 @base.VersionedObjectRegistry.register_if(False)
1963 class AllocationList(base.ObjectListBase, base.VersionedObject):
1964 
1965     # The number of times to retry set_allocations if there has
1966     # been a resource provider (not consumer) generation coflict.
1967     RP_CONFLICT_RETRY_COUNT = 10
1968 
1969     fields = {
1970         'objects': fields.ListOfObjectsField('Allocation'),
1971     }
1972 
1973     @oslo_db_api.wrap_db_retry(max_retries=5, retry_on_deadlock=True)
1974     @db_api.placement_context_manager.writer
1975     def _set_allocations(self, context, allocs):
1976         """Write a set of allocations.
1977 
1978         We must check that there is capacity for each allocation.
1979         If there is not we roll back the entire set.
1980 
1981         :raises `exception.ResourceClassNotFound` if any resource class in any
1982                 allocation in allocs cannot be found in either the standard
1983                 classes or the DB.
1984         :raises `exception.InvalidAllocationCapacityExceeded` if any inventory
1985                 would be exhausted by the allocation.
1986         :raises `InvalidAllocationConstraintsViolated` if any of the
1987                 `step_size`, `min_unit` or `max_unit` constraints in an
1988                 inventory will be violated by any one of the allocations.
1989         :raises `ConcurrentUpdateDetected` if a generation for a resource
1990                 provider or consumer failed its increment check.
1991         """
1992         # First delete any existing allocations for any consumers. This
1993         # provides a clean slate for the consumers mentioned in the list of
1994         # allocations being manipulated.
1995         consumer_ids = set(alloc.consumer.uuid for alloc in allocs)
1996         for consumer_id in consumer_ids:
1997             _delete_allocations_for_consumer(context, consumer_id)
1998 
1999         # Before writing any allocation records, we check that the submitted
2000         # allocations do not cause any inventory capacity to be exceeded for
2001         # any resource provider and resource class involved in the allocation
2002         # transaction. _check_capacity_exceeded() raises an exception if any
2003         # inventory capacity is exceeded. If capacity is not exceeeded, the
2004         # function returns a list of ResourceProvider objects containing the
2005         # generation of the resource provider at the time of the check. These
2006         # objects are used at the end of the allocation transaction as a guard
2007         # against concurrent updates.
2008         #
2009         # Don't check capacity when alloc.used is zero. Zero is not a valid
2010         # amount when making an allocation (the minimum consumption of a
2011         # resource is one) but is used in this method to indicate a need for
2012         # removal. Providing 0 is controlled at the HTTP API layer where PUT
2013         # /allocations does not allow empty allocations. When POST /allocations
2014         # is implemented it will for the special case of atomically setting and
2015         # removing different allocations in the same request.
2016         # _check_capacity_exceeded will raise a ResourceClassNotFound # if any
2017         # allocation is using a resource class that does not exist.
2018         visited_consumers = {}
2019         visited_rps = _check_capacity_exceeded(context, allocs)
2020         for alloc in allocs:
2021             if alloc.consumer.id not in visited_consumers:
2022                 visited_consumers[alloc.consumer.id] = alloc.consumer
2023 
2024             # If alloc.used is set to zero that is a signal that we don't want
2025             # to (re-)create any allocations for this resource class.
2026             # _delete_current_allocs has already wiped out allocations so just
2027             # continue
2028             if alloc.used == 0:
2029                 continue
2030             consumer_id = alloc.consumer.uuid
2031             rp = alloc.resource_provider
2032             rc_id = _RC_CACHE.id_from_string(alloc.resource_class)
2033             ins_stmt = _ALLOC_TBL.insert().values(
2034                     resource_provider_id=rp.id,
2035                     resource_class_id=rc_id,
2036                     consumer_id=consumer_id,
2037                     used=alloc.used)
2038             res = context.session.execute(ins_stmt)
2039             alloc.id = res.lastrowid
2040             alloc.obj_reset_changes()
2041 
2042         # Generation checking happens here. If the inventory for this resource
2043         # provider changed out from under us, this will raise a
2044         # ConcurrentUpdateDetected which can be caught by the caller to choose
2045         # to try again. It will also rollback the transaction so that these
2046         # changes always happen atomically.
2047         for rp in visited_rps.values():
2048             rp.generation = _increment_provider_generation(context, rp)
2049         for consumer in visited_consumers.values():
2050             consumer.increment_generation()
2051         # If any consumers involved in this transaction ended up having no
2052         # allocations, delete the consumer records. Exclude consumers that had
2053         # *some resource* in the allocation list with a total > 0 since clearly
2054         # those consumers have allocations...
2055         cons_with_allocs = set(a.consumer.uuid for a in allocs if a.used > 0)
2056         all_cons = set(c.uuid for c in visited_consumers.values())
2057         consumers_to_check = all_cons - cons_with_allocs
2058         consumer_obj.delete_consumers_if_no_allocations(
2059             context, consumers_to_check)
2060 
2061     @classmethod
2062     def get_all_by_resource_provider(cls, context, rp):
2063         _create_incomplete_consumers_for_provider(context, rp.id)
2064         db_allocs = _get_allocations_by_provider_id(context, rp.id)
2065         # Build up a list of Allocation objects, setting the Allocation object
2066         # fields to the same-named database record field we got from
2067         # _get_allocations_by_provider_id(). We already have the
2068         # ResourceProvider object so we just pass that object to the Allocation
2069         # object constructor as-is
2070         objs = []
2071         for rec in db_allocs:
2072             consumer = consumer_obj.Consumer(
2073                 context, id=rec['consumer_id'],
2074                 uuid=rec['consumer_uuid'],
2075                 generation=rec['consumer_generation'],
2076                 project=project_obj.Project(
2077                     context, id=rec['project_id'],
2078                     external_id=rec['project_external_id']),
2079                 user=user_obj.User(
2080                     context, id=rec['user_id'],
2081                     external_id=rec['user_external_id']))
2082             objs.append(
2083                 Allocation(
2084                     context, id=rec['id'], resource_provider=rp,
2085                     resource_class=_RC_CACHE.string_from_id(
2086                         rec['resource_class_id']),
2087                     consumer=consumer,
2088                     used=rec['used']))
2089         alloc_list = cls(context, objects=objs)
2090         return alloc_list
2091 
2092     @classmethod
2093     def get_all_by_consumer_id(cls, context, consumer_id):
2094         _create_incomplete_consumer(context, consumer_id)
2095         db_allocs = _get_allocations_by_consumer_uuid(context, consumer_id)
2096 
2097         if db_allocs:
2098             # Build up the Consumer object (it's the same for all allocations
2099             # since we looked up by consumer ID)
2100             db_first = db_allocs[0]
2101             consumer = consumer_obj.Consumer(
2102                 context, id=db_first['consumer_id'],
2103                 uuid=db_first['consumer_uuid'],
2104                 generation=db_first['consumer_generation'],
2105                 project=project_obj.Project(
2106                     context, id=db_first['project_id'],
2107                     external_id=db_first['project_external_id']),
2108                 user=user_obj.User(
2109                     context, id=db_first['user_id'],
2110                     external_id=db_first['user_external_id']))
2111 
2112         # Build up a list of Allocation objects, setting the Allocation object
2113         # fields to the same-named database record field we got from
2114         # _get_allocations_by_consumer_id().
2115         #
2116         # NOTE(jaypipes):  Unlike with get_all_by_resource_provider(), we do
2117         # NOT already have the ResourceProvider object so we construct a new
2118         # ResourceProvider object below by looking at the resource provider
2119         # fields returned by _get_allocations_by_consumer_id().
2120         objs = [
2121             Allocation(
2122                 context, id=rec['id'],
2123                 resource_provider=ResourceProvider(
2124                     context,
2125                     id=rec['resource_provider_id'],
2126                     uuid=rec['resource_provider_uuid'],
2127                     name=rec['resource_provider_name'],
2128                     generation=rec['resource_provider_generation']),
2129                 resource_class=_RC_CACHE.string_from_id(
2130                     rec['resource_class_id']),
2131                 consumer=consumer,
2132                 used=rec['used'])
2133             for rec in db_allocs
2134         ]
2135         alloc_list = cls(context, objects=objs)
2136         return alloc_list
2137 
2138     def replace_all(self):
2139         """Replace the supplied allocations.
2140 
2141         :note: This method always deletes all allocations for all consumers
2142                referenced in the list of Allocation objects and then replaces
2143                the consumer's allocations with the Allocation objects. In doing
2144                so, it will end up setting the Allocation.id attribute of each
2145                Allocation object.
2146         """
2147         # Retry _set_allocations server side if there is a
2148         # ResourceProviderConcurrentUpdateDetected. We don't care about
2149         # sleeping, we simply want to reset the resource provider objects
2150         # and try again. For sake of simplicity (and because we don't have
2151         # easy access to the information) we reload all the resource
2152         # providers that may be present.
2153         retries = self.RP_CONFLICT_RETRY_COUNT
2154         while retries:
2155             retries -= 1
2156             try:
2157                 self._set_allocations(self._context, self.objects)
2158                 break
2159             except exception.ResourceProviderConcurrentUpdateDetected:
2160                 LOG.debug('Retrying allocations write on resource provider '
2161                           'generation conflict')
2162                 # We only want to reload each unique resource provider once.
2163                 alloc_rp_uuids = set(
2164                     alloc.resource_provider.uuid for alloc in self.objects)
2165                 seen_rps = {}
2166                 for rp_uuid in alloc_rp_uuids:
2167                     seen_rps[rp_uuid] = ResourceProvider.get_by_uuid(
2168                         self._context, rp_uuid)
2169                 for alloc in self.objects:
2170                     rp_uuid = alloc.resource_provider.uuid
2171                     alloc.resource_provider = seen_rps[rp_uuid]
2172         else:
2173             # We ran out of retries so we need to raise again.
2174             # The log will automatically have request id info associated with
2175             # it that will allow tracing back to specific allocations.
2176             # Attempting to extract specific consumer or resource provider
2177             # information from the allocations is not coherent as this
2178             # could be multiple consumers and providers.
2179             LOG.warning('Exceeded retry limit of %d on allocations write',
2180                         self.RP_CONFLICT_RETRY_COUNT)
2181             raise exception.ResourceProviderConcurrentUpdateDetected()
2182 
2183     def delete_all(self):
2184         consumer_uuids = set(alloc.consumer.uuid for alloc in self.objects)
2185         alloc_ids = [alloc.id for alloc in self.objects]
2186         _delete_allocations_by_ids(self._context, alloc_ids)
2187         consumer_obj.delete_consumers_if_no_allocations(
2188             self._context, consumer_uuids)
2189 
2190     def __repr__(self):
2191         strings = [repr(x) for x in self.objects]
2192         return "AllocationList[" + ", ".join(strings) + "]"
2193 
2194 
2195 @base.VersionedObjectRegistry.register_if(False)
2196 class Usage(base.VersionedObject):
2197 
2198     fields = {
2199         'resource_class': rc_fields.ResourceClassField(read_only=True),
2200         'usage': fields.NonNegativeIntegerField(),
2201     }
2202 
2203     @staticmethod
2204     def _from_db_object(context, target, source):
2205         for field in target.fields:
2206             if field not in ('resource_class'):
2207                 setattr(target, field, source[field])
2208 
2209         if 'resource_class' not in target:
2210             rc_str = _RC_CACHE.string_from_id(source['resource_class_id'])
2211             target.resource_class = rc_str
2212 
2213         target._context = context
2214         target.obj_reset_changes()
2215         return target
2216 
2217 
2218 @base.VersionedObjectRegistry.register_if(False)
2219 class UsageList(base.ObjectListBase, base.VersionedObject):
2220 
2221     fields = {
2222         'objects': fields.ListOfObjectsField('Usage'),
2223     }
2224 
2225     @staticmethod
2226     @db_api.placement_context_manager.reader
2227     def _get_all_by_resource_provider_uuid(context, rp_uuid):
2228         query = (context.session.query(models.Inventory.resource_class_id,
2229                  func.coalesce(func.sum(models.Allocation.used), 0))
2230                  .join(models.ResourceProvider,
2231                        models.Inventory.resource_provider_id ==
2232                        models.ResourceProvider.id)
2233                  .outerjoin(models.Allocation,
2234                             sql.and_(models.Inventory.resource_provider_id ==
2235                                      models.Allocation.resource_provider_id,
2236                                      models.Inventory.resource_class_id ==
2237                                      models.Allocation.resource_class_id))
2238                  .filter(models.ResourceProvider.uuid == rp_uuid)
2239                  .group_by(models.Inventory.resource_class_id))
2240         result = [dict(resource_class_id=item[0], usage=item[1])
2241                   for item in query.all()]
2242         return result
2243 
2244     @staticmethod
2245     @db_api.placement_context_manager.reader
2246     def _get_all_by_project_user(context, project_id, user_id=None):
2247         query = (context.session.query(models.Allocation.resource_class_id,
2248                  func.coalesce(func.sum(models.Allocation.used), 0))
2249                  .join(models.Consumer,
2250                        models.Allocation.consumer_id == models.Consumer.uuid)
2251                  .join(models.Project,
2252                        models.Consumer.project_id == models.Project.id)
2253                  .filter(models.Project.external_id == project_id))
2254         if user_id:
2255             query = query.join(models.User,
2256                                models.Consumer.user_id == models.User.id)
2257             query = query.filter(models.User.external_id == user_id)
2258         query = query.group_by(models.Allocation.resource_class_id)
2259         result = [dict(resource_class_id=item[0], usage=item[1])
2260                   for item in query.all()]
2261         return result
2262 
2263     @classmethod
2264     def get_all_by_resource_provider_uuid(cls, context, rp_uuid):
2265         usage_list = cls._get_all_by_resource_provider_uuid(context, rp_uuid)
2266         return base.obj_make_list(context, cls(context), Usage, usage_list)
2267 
2268     @classmethod
2269     def get_all_by_project_user(cls, context, project_id, user_id=None):
2270         usage_list = cls._get_all_by_project_user(context, project_id,
2271                                                   user_id=user_id)
2272         return base.obj_make_list(context, cls(context), Usage, usage_list)
2273 
2274     def __repr__(self):
2275         strings = [repr(x) for x in self.objects]
2276         return "UsageList[" + ", ".join(strings) + "]"
2277 
2278 
2279 @base.VersionedObjectRegistry.register_if(False)
2280 class ResourceClass(base.VersionedObject, base.TimestampedObject):
2281 
2282     MIN_CUSTOM_RESOURCE_CLASS_ID = 10000
2283     """Any user-defined resource classes must have an identifier greater than
2284     or equal to this number.
2285     """
2286 
2287     # Retry count for handling possible race condition in creating resource
2288     # class. We don't ever want to hit this, as it is simply a race when
2289     # creating these classes, but this is just a stopgap to prevent a potential
2290     # infinite loop.
2291     RESOURCE_CREATE_RETRY_COUNT = 100
2292 
2293     fields = {
2294         'id': fields.IntegerField(read_only=True),
2295         'name': rc_fields.ResourceClassField(nullable=False),
2296     }
2297 
2298     @staticmethod
2299     def _from_db_object(context, target, source):
2300         for field in target.fields:
2301             setattr(target, field, source[field])
2302 
2303         target._context = context
2304         target.obj_reset_changes()
2305         return target
2306 
2307     @classmethod
2308     def get_by_name(cls, context, name):
2309         """Return a ResourceClass object with the given string name.
2310 
2311         :param name: String name of the resource class to find
2312 
2313         :raises: ResourceClassNotFound if no such resource class was found
2314         """
2315         rc = _RC_CACHE.all_from_string(name)
2316         obj = cls(context, id=rc['id'], name=rc['name'],
2317                   updated_at=rc['updated_at'], created_at=rc['created_at'])
2318         obj.obj_reset_changes()
2319         return obj
2320 
2321     @staticmethod
2322     @db_api.placement_context_manager.reader
2323     def _get_next_id(context):
2324         """Utility method to grab the next resource class identifier to use for
2325          user-defined resource classes.
2326         """
2327         query = context.session.query(func.max(models.ResourceClass.id))
2328         max_id = query.one()[0]
2329         if not max_id:
2330             return ResourceClass.MIN_CUSTOM_RESOURCE_CLASS_ID
2331         else:
2332             return max_id + 1
2333 
2334     def create(self):
2335         if 'id' in self:
2336             raise exception.ObjectActionError(action='create',
2337                                               reason='already created')
2338         if 'name' not in self:
2339             raise exception.ObjectActionError(action='create',
2340                                               reason='name is required')
2341         if self.name in rc_fields.ResourceClass.STANDARD:
2342             raise exception.ResourceClassExists(resource_class=self.name)
2343 
2344         if not self.name.startswith(rc_fields.ResourceClass.CUSTOM_NAMESPACE):
2345             raise exception.ObjectActionError(
2346                 action='create',
2347                 reason='name must start with ' +
2348                         rc_fields.ResourceClass.CUSTOM_NAMESPACE)
2349 
2350         updates = self.obj_get_changes()
2351         # There is the possibility of a race when adding resource classes, as
2352         # the ID is generated locally. This loop catches that exception, and
2353         # retries until either it succeeds, or a different exception is
2354         # encountered.
2355         retries = self.RESOURCE_CREATE_RETRY_COUNT
2356         while retries:
2357             retries -= 1
2358             try:
2359                 rc = self._create_in_db(self._context, updates)
2360                 self._from_db_object(self._context, self, rc)
2361                 break
2362             except db_exc.DBDuplicateEntry as e:
2363                 if 'id' in e.columns:
2364                     # Race condition for ID creation; try again
2365                     continue
2366                 # The duplication is on the other unique column, 'name'. So do
2367                 # not retry; raise the exception immediately.
2368                 raise exception.ResourceClassExists(resource_class=self.name)
2369         else:
2370             # We have no idea how common it will be in practice for the retry
2371             # limit to be exceeded. We set it high in the hope that we never
2372             # hit this point, but added this log message so we know that this
2373             # specific situation occurred.
2374             LOG.warning("Exceeded retry limit on ID generation while "
2375                         "creating ResourceClass %(name)s",
2376                         {'name': self.name})
2377             msg = _("creating resource class %s") % self.name
2378             raise exception.MaxDBRetriesExceeded(action=msg)
2379 
2380     @staticmethod
2381     @db_api.placement_context_manager.writer
2382     def _create_in_db(context, updates):
2383         next_id = ResourceClass._get_next_id(context)
2384         rc = models.ResourceClass()
2385         rc.update(updates)
2386         rc.id = next_id
2387         context.session.add(rc)
2388         return rc
2389 
2390     def destroy(self):
2391         if 'id' not in self:
2392             raise exception.ObjectActionError(action='destroy',
2393                                               reason='ID attribute not found')
2394         # Never delete any standard resource class, since the standard resource
2395         # classes don't even exist in the database table anyway.
2396         if self.id in (rc['id'] for rc in _RC_CACHE.STANDARDS):
2397             raise exception.ResourceClassCannotDeleteStandard(
2398                     resource_class=self.name)
2399 
2400         self._destroy(self._context, self.id, self.name)
2401         _RC_CACHE.clear()
2402 
2403     @staticmethod
2404     @db_api.placement_context_manager.writer
2405     def _destroy(context, _id, name):
2406         # Don't delete the resource class if it is referred to in the
2407         # inventories table.
2408         num_inv = context.session.query(models.Inventory).filter(
2409                 models.Inventory.resource_class_id == _id).count()
2410         if num_inv:
2411             raise exception.ResourceClassInUse(resource_class=name)
2412 
2413         res = context.session.query(models.ResourceClass).filter(
2414                 models.ResourceClass.id == _id).delete()
2415         if not res:
2416             raise exception.NotFound()
2417 
2418     def save(self):
2419         if 'id' not in self:
2420             raise exception.ObjectActionError(action='save',
2421                                               reason='ID attribute not found')
2422         updates = self.obj_get_changes()
2423         # Never update any standard resource class, since the standard resource
2424         # classes don't even exist in the database table anyway.
2425         if self.id in (rc['id'] for rc in _RC_CACHE.STANDARDS):
2426             raise exception.ResourceClassCannotUpdateStandard(
2427                     resource_class=self.name)
2428         self._save(self._context, self.id, self.name, updates)
2429         _RC_CACHE.clear()
2430 
2431     @staticmethod
2432     @db_api.placement_context_manager.writer
2433     def _save(context, id, name, updates):
2434         db_rc = context.session.query(models.ResourceClass).filter_by(
2435             id=id).first()
2436         db_rc.update(updates)
2437         try:
2438             db_rc.save(context.session)
2439         except db_exc.DBDuplicateEntry:
2440             raise exception.ResourceClassExists(resource_class=name)
2441 
2442 
2443 @base.VersionedObjectRegistry.register_if(False)
2444 class ResourceClassList(base.ObjectListBase, base.VersionedObject):
2445 
2446     fields = {
2447         'objects': fields.ListOfObjectsField('ResourceClass'),
2448     }
2449 
2450     @staticmethod
2451     @db_api.placement_context_manager.reader
2452     def _get_all(context):
2453         customs = list(context.session.query(models.ResourceClass).all())
2454         return _RC_CACHE.STANDARDS + customs
2455 
2456     @classmethod
2457     def get_all(cls, context):
2458         resource_classes = cls._get_all(context)
2459         return base.obj_make_list(context, cls(context),
2460                                   ResourceClass, resource_classes)
2461 
2462     def __repr__(self):
2463         strings = [repr(x) for x in self.objects]
2464         return "ResourceClassList[" + ", ".join(strings) + "]"
2465 
2466 
2467 @base.VersionedObjectRegistry.register_if(False)
2468 class Trait(base.VersionedObject, base.TimestampedObject):
2469 
2470     # All the user-defined traits must begin with this prefix.
2471     CUSTOM_NAMESPACE = 'CUSTOM_'
2472 
2473     fields = {
2474         'id': fields.IntegerField(read_only=True),
2475         'name': fields.StringField(nullable=False)
2476     }
2477 
2478     @staticmethod
2479     def _from_db_object(context, trait, db_trait):
2480         for key in trait.fields:
2481             setattr(trait, key, db_trait[key])
2482         trait.obj_reset_changes()
2483         trait._context = context
2484         return trait
2485 
2486     @staticmethod
2487     @db_api.placement_context_manager.writer
2488     def _create_in_db(context, updates):
2489         trait = models.Trait()
2490         trait.update(updates)
2491         context.session.add(trait)
2492         return trait
2493 
2494     def create(self):
2495         if 'id' in self:
2496             raise exception.ObjectActionError(action='create',
2497                                               reason='already created')
2498         if 'name' not in self:
2499             raise exception.ObjectActionError(action='create',
2500                                               reason='name is required')
2501 
2502         updates = self.obj_get_changes()
2503 
2504         try:
2505             db_trait = self._create_in_db(self._context, updates)
2506         except db_exc.DBDuplicateEntry:
2507             raise exception.TraitExists(name=self.name)
2508 
2509         self._from_db_object(self._context, self, db_trait)
2510 
2511     @staticmethod
2512     @db_api.placement_context_manager.writer  # trait sync can cause a write
2513     def _get_by_name_from_db(context, name):
2514         result = context.session.query(models.Trait).filter_by(
2515             name=name).first()
2516         if not result:
2517             raise exception.TraitNotFound(names=name)
2518         return result
2519 
2520     @classmethod
2521     def get_by_name(cls, context, name):
2522         db_trait = cls._get_by_name_from_db(context, six.text_type(name))
2523         return cls._from_db_object(context, cls(), db_trait)
2524 
2525     @staticmethod
2526     @db_api.placement_context_manager.writer
2527     def _destroy_in_db(context, _id, name):
2528         num = context.session.query(models.ResourceProviderTrait).filter(
2529             models.ResourceProviderTrait.trait_id == _id).count()
2530         if num:
2531             raise exception.TraitInUse(name=name)
2532 
2533         res = context.session.query(models.Trait).filter_by(
2534             name=name).delete()
2535         if not res:
2536             raise exception.TraitNotFound(names=name)
2537 
2538     def destroy(self):
2539         if 'name' not in self:
2540             raise exception.ObjectActionError(action='destroy',
2541                                               reason='name is required')
2542 
2543         if not self.name.startswith(self.CUSTOM_NAMESPACE):
2544             raise exception.TraitCannotDeleteStandard(name=self.name)
2545 
2546         if 'id' not in self:
2547             raise exception.ObjectActionError(action='destroy',
2548                                               reason='ID attribute not found')
2549 
2550         self._destroy_in_db(self._context, self.id, self.name)
2551 
2552 
2553 @base.VersionedObjectRegistry.register_if(False)
2554 class TraitList(base.ObjectListBase, base.VersionedObject):
2555 
2556     fields = {
2557         'objects': fields.ListOfObjectsField('Trait')
2558     }
2559 
2560     @staticmethod
2561     @db_api.placement_context_manager.writer  # trait sync can cause a write
2562     def _get_all_from_db(context, filters):
2563         if not filters:
2564             filters = {}
2565 
2566         query = context.session.query(models.Trait)
2567         if 'name_in' in filters:
2568             query = query.filter(models.Trait.name.in_(
2569                 [six.text_type(n) for n in filters['name_in']]
2570             ))
2571         if 'prefix' in filters:
2572             query = query.filter(
2573                 models.Trait.name.like(six.text_type(filters['prefix'] + '%')))
2574         if 'associated' in filters:
2575             if filters['associated']:
2576                 query = query.join(models.ResourceProviderTrait,
2577                     models.Trait.id == models.ResourceProviderTrait.trait_id
2578                 ).distinct()
2579             else:
2580                 query = query.outerjoin(models.ResourceProviderTrait,
2581                     models.Trait.id == models.ResourceProviderTrait.trait_id
2582                 ).filter(models.ResourceProviderTrait.trait_id == null())
2583 
2584         return query.all()
2585 
2586     @base.remotable_classmethod
2587     def get_all(cls, context, filters=None):
2588         db_traits = cls._get_all_from_db(context, filters)
2589         return base.obj_make_list(context, cls(context), Trait, db_traits)
2590 
2591     @classmethod
2592     def get_all_by_resource_provider(cls, context, rp):
2593         """Returns a TraitList containing Trait objects for any trait
2594         associated with the supplied resource provider.
2595         """
2596         db_traits = _get_traits_by_provider_id(context, rp.id)
2597         return base.obj_make_list(context, cls(context), Trait, db_traits)
2598 
2599 
2600 @base.VersionedObjectRegistry.register_if(False)
2601 class AllocationRequestResource(base.VersionedObject):
2602 
2603     fields = {
2604         'resource_provider': fields.ObjectField('ResourceProvider'),
2605         'resource_class': rc_fields.ResourceClassField(read_only=True),
2606         'amount': fields.NonNegativeIntegerField(),
2607     }
2608 
2609 
2610 @base.VersionedObjectRegistry.register_if(False)
2611 class AllocationRequest(base.VersionedObject):
2612 
2613     fields = {
2614         # UUID of (the root of the tree including) the non-sharing resource
2615         # provider associated with this AllocationRequest. Internal use only,
2616         # not included when the object is serialized for output.
2617         'anchor_root_provider_uuid': fields.UUIDField(),
2618         # Whether all AllocationRequestResources in this AllocationRequest are
2619         # required to be satisfied by the same provider (based on the
2620         # corresponding RequestGroup's use_same_provider attribute). Internal
2621         # use only, not included when the object is serialized for output.
2622         'use_same_provider': fields.BooleanField(),
2623         'resource_requests': fields.ListOfObjectsField(
2624             'AllocationRequestResource'
2625         ),
2626     }
2627 
2628     def __repr__(self):
2629         anchor = (self.anchor_root_provider_uuid[-8:]
2630                   if 'anchor_root_provider_uuid' in self else '<?>')
2631         usp = self.use_same_provider if 'use_same_provider' in self else '<?>'
2632         repr_str = ('%s(anchor=...%s, same_provider=%s, '
2633                     'resource_requests=[%s])' %
2634                     (self.obj_name(), anchor, usp,
2635                      ', '.join([str(arr) for arr in self.resource_requests])))
2636         if six.PY2:
2637             repr_str = encodeutils.safe_encode(repr_str, incoming='utf-8')
2638         return repr_str
2639 
2640 
2641 @base.VersionedObjectRegistry.register_if(False)
2642 class ProviderSummaryResource(base.VersionedObject):
2643 
2644     fields = {
2645         'resource_class': rc_fields.ResourceClassField(read_only=True),
2646         'capacity': fields.NonNegativeIntegerField(),
2647         'used': fields.NonNegativeIntegerField(),
2648         # Internal use only; not included when the object is serialized for
2649         # output.
2650         'max_unit': fields.NonNegativeIntegerField(),
2651     }
2652 
2653 
2654 @base.VersionedObjectRegistry.register_if(False)
2655 class ProviderSummary(base.VersionedObject):
2656 
2657     fields = {
2658         'resource_provider': fields.ObjectField('ResourceProvider'),
2659         'resources': fields.ListOfObjectsField('ProviderSummaryResource'),
2660         'traits': fields.ListOfObjectsField('Trait'),
2661     }
2662 
2663     @property
2664     def resource_class_names(self):
2665         """Helper property that returns a set() of resource class string names
2666         that are included in the provider summary.
2667         """
2668         return set(res.resource_class for res in self.resources)
2669 
2670 
2671 @db_api.placement_context_manager.reader
2672 def _get_usages_by_provider_tree(ctx, root_ids):
2673     """Returns a row iterator of usage records grouped by provider ID
2674     for all resource providers in all trees indicated in the ``root_ids``.
2675     """
2676     # We build up a SQL expression that looks like this:
2677     # SELECT
2678     #   rp.id as resource_provider_id
2679     # , rp.uuid as resource_provider_uuid
2680     # , inv.resource_class_id
2681     # , inv.total
2682     # , inv.reserved
2683     # , inv.allocation_ratio
2684     # , inv.max_unit
2685     # , usage.used
2686     # FROM resource_providers AS rp
2687     # LEFT JOIN inventories AS inv
2688     #  ON rp.id = inv.resource_provider_id
2689     # LEFT JOIN (
2690     #   SELECT resource_provider_id, resource_class_id, SUM(used) as used
2691     #   FROM allocations
2692     #   JOIN resource_providers
2693     #     ON allocations.resource_provider_id = resource_providers.id
2694     #     AND resource_providers.root_provider_id IN($root_ids)
2695     #   GROUP BY resource_provider_id, resource_class_id
2696     # )
2697     # AS usages
2698     #   ON inv.resource_provider_id = usage.resource_provider_id
2699     #   AND inv.resource_class_id = usage.resource_class_id
2700     # WHERE rp.root_provider_id IN ($root_ids)
2701     rpt = sa.alias(_RP_TBL, name="rp")
2702     inv = sa.alias(_INV_TBL, name="inv")
2703     # Build our derived table (subquery in the FROM clause) that sums used
2704     # amounts for resource provider and resource class
2705     derived_alloc_to_rp = sa.join(
2706         _ALLOC_TBL, _RP_TBL,
2707         sa.and_(_ALLOC_TBL.c.resource_provider_id == _RP_TBL.c.id,
2708                 _RP_TBL.c.root_provider_id.in_(root_ids)))
2709     usage = sa.alias(
2710         sa.select([
2711             _ALLOC_TBL.c.resource_provider_id,
2712             _ALLOC_TBL.c.resource_class_id,
2713             sql.func.sum(_ALLOC_TBL.c.used).label('used'),
2714         ]).select_from(derived_alloc_to_rp).group_by(
2715             _ALLOC_TBL.c.resource_provider_id,
2716             _ALLOC_TBL.c.resource_class_id
2717         ),
2718         name='usage')
2719     # Build a join between the resource providers and inventories table
2720     rpt_inv_join = sa.outerjoin(rpt, inv,
2721                                 rpt.c.id == inv.c.resource_provider_id)
2722     # And then join to the derived table of usages
2723     usage_join = sa.outerjoin(
2724         rpt_inv_join,
2725         usage,
2726         sa.and_(
2727             usage.c.resource_provider_id == inv.c.resource_provider_id,
2728             usage.c.resource_class_id == inv.c.resource_class_id,
2729         ),
2730     )
2731     query = sa.select([
2732         rpt.c.id.label("resource_provider_id"),
2733         rpt.c.uuid.label("resource_provider_uuid"),
2734         inv.c.resource_class_id,
2735         inv.c.total,
2736         inv.c.reserved,
2737         inv.c.allocation_ratio,
2738         inv.c.max_unit,
2739         usage.c.used,
2740     ]).select_from(usage_join).where(rpt.c.root_provider_id.in_(root_ids))
2741     return ctx.session.execute(query).fetchall()
2742 
2743 
2744 @db_api.placement_context_manager.reader
2745 def _get_provider_ids_having_any_trait(ctx, traits):
2746     """Returns a list of resource provider internal IDs that have ANY of the
2747     supplied traits.
2748 
2749     :param ctx: Session context to use
2750     :param traits: A map, keyed by trait string name, of trait internal IDs, at
2751                    least one of which each provider must have associated with
2752                    it.
2753     :raise ValueError: If traits is empty or None.
2754     """
2755     if not traits:
2756         raise ValueError(_('traits must not be empty'))
2757 
2758     rptt = sa.alias(_RP_TRAIT_TBL, name="rpt")
2759     sel = sa.select([rptt.c.resource_provider_id])
2760     sel = sel.where(rptt.c.trait_id.in_(traits.values()))
2761     sel = sel.group_by(rptt.c.resource_provider_id)
2762     return [r[0] for r in ctx.session.execute(sel)]
2763 
2764 
2765 @db_api.placement_context_manager.reader
2766 def _get_provider_ids_having_all_traits(ctx, required_traits):
2767     """Returns a list of resource provider internal IDs that have ALL of the
2768     required traits.
2769 
2770     NOTE: Don't call this method with no required_traits.
2771 
2772     :param ctx: Session context to use
2773     :param required_traits: A map, keyed by trait string name, of required
2774                             trait internal IDs that each provider must have
2775                             associated with it
2776     :raise ValueError: If required_traits is empty or None.
2777     """
2778     if not required_traits:
2779         raise ValueError(_('required_traits must not be empty'))
2780 
2781     rptt = sa.alias(_RP_TRAIT_TBL, name="rpt")
2782     sel = sa.select([rptt.c.resource_provider_id])
2783     sel = sel.where(rptt.c.trait_id.in_(required_traits.values()))
2784     sel = sel.group_by(rptt.c.resource_provider_id)
2785     # Only get the resource providers that have ALL the required traits, so we
2786     # need to GROUP BY the resource provider and ensure that the
2787     # COUNT(trait_id) is equal to the number of traits we are requiring
2788     num_traits = len(required_traits)
2789     cond = sa.func.count(rptt.c.trait_id) == num_traits
2790     sel = sel.having(cond)
2791     return [r[0] for r in ctx.session.execute(sel)]
2792 
2793 
2794 @db_api.placement_context_manager.reader
2795 def _has_provider_trees(ctx):
2796     """Simple method that returns whether provider trees (i.e. nested resource
2797     providers) are in use in the deployment at all. This information is used to
2798     switch code paths when attempting to retrieve allocation candidate
2799     information. The code paths are eminently easier to execute and follow for
2800     non-nested scenarios...
2801 
2802     NOTE(jaypipes): The result of this function can be cached extensively.
2803     """
2804     sel = sa.select([_RP_TBL.c.id])
2805     sel = sel.where(_RP_TBL.c.parent_provider_id.isnot(None))
2806     sel = sel.limit(1)
2807     res = ctx.session.execute(sel).fetchall()
2808     return len(res) > 0
2809 
2810 
2811 @db_api.placement_context_manager.reader
2812 def _get_provider_ids_matching(ctx, resources, required_traits,
2813         forbidden_traits, member_of=None):
2814     """Returns a list of tuples of (internal provider ID, root provider ID)
2815     that have available inventory to satisfy all the supplied requests for
2816     resources.
2817 
2818     :note: This function is used for scenarios that do NOT involve sharing
2819     providers.
2820 
2821     :param ctx: Session context to use
2822     :param resources: A dict, keyed by resource class ID, of the amount
2823                       requested of that resource class.
2824     :param required_traits: A map, keyed by trait string name, of required
2825                             trait internal IDs that each provider must have
2826                             associated with it
2827     :param forbidden_traits: A map, keyed by trait string name, of forbidden
2828                              trait internal IDs that each provider must not
2829                              have associated with it
2830     :param member_of: An optional list of list of aggregate UUIDs. If provided,
2831                       the allocation_candidates returned will only be for
2832                       resource providers that are members of one or more of the
2833                       supplied aggregates of each aggregate UUID list.
2834     """
2835     trait_rps = None
2836     forbidden_rp_ids = None
2837     if required_traits:
2838         trait_rps = _get_provider_ids_having_all_traits(ctx, required_traits)
2839         if not trait_rps:
2840             return []
2841     if forbidden_traits:
2842         forbidden_rp_ids = _get_provider_ids_having_any_trait(
2843             ctx, forbidden_traits)
2844 
2845     rpt = sa.alias(_RP_TBL, name="rp")
2846 
2847     rc_name_map = {
2848         rc_id: _RC_CACHE.string_from_id(rc_id).lower() for rc_id in resources
2849     }
2850 
2851     # Dict, keyed by resource class ID, of an aliased table object for the
2852     # inventories table winnowed to only that resource class.
2853     inv_tables = {
2854         rc_id: sa.alias(_INV_TBL, name='inv_%s' % rc_name_map[rc_id])
2855         for rc_id in resources
2856     }
2857 
2858     # Dict, keyed by resource class ID, of a derived table (subquery in the
2859     # FROM clause or JOIN) against the allocations table winnowed to only that
2860     # resource class, grouped by resource provider.
2861     usage_tables = {
2862         rc_id: sa.alias(
2863             sa.select([
2864                 _ALLOC_TBL.c.resource_provider_id,
2865                 sql.func.sum(_ALLOC_TBL.c.used).label('used'),
2866             ]).where(
2867                 _ALLOC_TBL.c.resource_class_id == rc_id
2868             ).group_by(
2869                 _ALLOC_TBL.c.resource_provider_id
2870             ),
2871             name='usage_%s' % rc_name_map[rc_id],
2872         )
2873         for rc_id in resources
2874     }
2875 
2876     sel = sa.select([rpt.c.id, rpt.c.root_provider_id])
2877 
2878     # List of the WHERE conditions we build up by iterating over the requested
2879     # resources
2880     where_conds = []
2881 
2882     # First filter by the resource providers that had all the required traits
2883     if trait_rps:
2884         where_conds.append(rpt.c.id.in_(trait_rps))
2885     # or have any forbidden trait
2886     if forbidden_rp_ids:
2887         where_conds.append(~rpt.c.id.in_(forbidden_rp_ids))
2888 
2889     # The chain of joins that we eventually pass to select_from()
2890     join_chain = rpt
2891 
2892     for rc_id, amount in resources.items():
2893         inv_by_rc = inv_tables[rc_id]
2894         usage_by_rc = usage_tables[rc_id]
2895 
2896         # We can do a more efficient INNER JOIN because we don't have shared
2897         # resource providers to deal with
2898         rp_inv_join = sa.join(
2899             join_chain, inv_by_rc,
2900             sa.and_(
2901                 inv_by_rc.c.resource_provider_id == rpt.c.id,
2902                 # Add a join condition winnowing this copy of inventories table
2903                 # to only the resource class being analyzed in this loop...
2904                 inv_by_rc.c.resource_class_id == rc_id,
2905             ),
2906         )
2907         rp_inv_usage_join = sa.outerjoin(
2908             rp_inv_join, usage_by_rc,
2909             inv_by_rc.c.resource_provider_id ==
2910                 usage_by_rc.c.resource_provider_id,
2911         )
2912         join_chain = rp_inv_usage_join
2913 
2914         usage_cond = sa.and_(
2915             (
2916             (sql.func.coalesce(usage_by_rc.c.used, 0) + amount) <=
2917             (inv_by_rc.c.total - inv_by_rc.c.reserved) *
2918                 inv_by_rc.c.allocation_ratio
2919             ),
2920             inv_by_rc.c.min_unit <= amount,
2921             inv_by_rc.c.max_unit >= amount,
2922             amount % inv_by_rc.c.step_size == 0,
2923         )
2924         where_conds.append(usage_cond)
2925 
2926     # If 'member_of' has values, do a separate lookup to identify the
2927     # resource providers that meet the member_of constraints.
2928     if member_of:
2929         rps_in_aggs = _provider_ids_matching_aggregates(ctx, member_of)
2930         if not rps_in_aggs:
2931             # Short-circuit. The user either asked for a non-existing
2932             # aggregate or there were no resource providers that matched
2933             # the requirements...
2934             return []
2935         where_conds.append(rpt.c.id.in_(rps_in_aggs))
2936 
2937     sel = sel.select_from(join_chain)
2938     sel = sel.where(sa.and_(*where_conds))
2939 
2940     return [(r[0], r[1]) for r in ctx.session.execute(sel)]
2941 
2942 
2943 @db_api.placement_context_manager.reader
2944 def _provider_aggregates(ctx, rp_ids):
2945     """Given a list of resource provider internal IDs, returns a dict,
2946     keyed by those provider IDs, of sets of aggregate ids associated
2947     with that provider.
2948 
2949     :raises: ValueError when rp_ids is empty.
2950 
2951     :param ctx: nova.context.RequestContext object
2952     :param rp_ids: list of resource provider IDs
2953     """
2954     if not rp_ids:
2955         raise ValueError(_("Expected rp_ids to be a list of resource provider "
2956                            "internal IDs, but got an empty list."))
2957 
2958     rpat = sa.alias(_RP_AGG_TBL, name='rpat')
2959     sel = sa.select([rpat.c.resource_provider_id,
2960                      rpat.c.aggregate_id])
2961     sel = sel.where(rpat.c.resource_provider_id.in_(rp_ids))
2962     res = collections.defaultdict(set)
2963     for r in ctx.session.execute(sel):
2964         res[r[0]].add(r[1])
2965     return res
2966 
2967 
2968 @db_api.placement_context_manager.reader
2969 def _get_providers_with_resource(ctx, rc_id, amount):
2970     """Returns a set of tuples of (provider ID, root provider ID) of providers
2971     that satisfy the request for a single resource class.
2972 
2973     :param ctx: Session context to use
2974     :param rc_id: Internal ID of resource class to check inventory for
2975     :param amount: Amount of resource being requested
2976     """
2977     # SELECT rp.id, rp.root_provider_id
2978     # FROM resource_providers AS rp
2979     # JOIN inventories AS inv
2980     #  ON rp.id = inv.resource_provider_id
2981     #  AND inv.resource_class_id = $RC_ID
2982     # LEFT JOIN (
2983     #  SELECT
2984     #    alloc.resource_provider_id,
2985     #    SUM(allocs.used) AS used
2986     #  FROM allocations AS alloc
2987     #  WHERE allocs.resource_class_id = $RC_ID
2988     #  GROUP BY allocs.resource_provider_id
2989     # ) AS usage
2990     #  ON inv.resource_provider_id = usage.resource_provider_id
2991     # WHERE
2992     #  used + $AMOUNT <= ((total - reserved) * inv.allocation_ratio)
2993     #  AND inv.min_unit <= $AMOUNT
2994     #  AND inv.max_unit >= $AMOUNT
2995     #  AND $AMOUNT % inv.step_size == 0
2996     rpt = sa.alias(_RP_TBL, name="rp")
2997     inv = sa.alias(_INV_TBL, name="inv")
2998     allocs = sa.alias(_ALLOC_TBL, name="alloc")
2999     usage = sa.select([
3000             allocs.c.resource_provider_id,
3001             sql.func.sum(allocs.c.used).label('used')])
3002     usage = usage.where(allocs.c.resource_class_id == rc_id)
3003     usage = usage.group_by(allocs.c.resource_provider_id)
3004     usage = sa.alias(usage, name="usage")
3005     where_conds = [
3006         sql.func.coalesce(usage.c.used, 0) + amount <= (
3007             (inv.c.total - inv.c.reserved) * inv.c.allocation_ratio),
3008         inv.c.min_unit <= amount,
3009         inv.c.max_unit >= amount,
3010         amount % inv.c.step_size == 0,
3011     ]
3012     rp_to_inv = sa.join(
3013         rpt, inv, sa.and_(
3014             rpt.c.id == inv.c.resource_provider_id,
3015             inv.c.resource_class_id == rc_id))
3016     inv_to_usage = sa.outerjoin(
3017         rp_to_inv, usage,
3018         inv.c.resource_provider_id == usage.c.resource_provider_id)
3019     sel = sa.select([rpt.c.id, rpt.c.root_provider_id])
3020     sel = sel.select_from(inv_to_usage)
3021     sel = sel.where(sa.and_(*where_conds))
3022     res = ctx.session.execute(sel).fetchall()
3023     res = set((r[0], r[1]) for r in res)
3024     return res
3025 
3026 
3027 @db_api.placement_context_manager.reader
3028 def _get_trees_with_traits(ctx, rp_ids, required_traits, forbidden_traits):
3029     """Given a list of provider IDs, filter them to return a set of tuples of
3030     (provider ID, root provider ID) of providers which belong to a tree that
3031     can satisfy trait requirements.
3032 
3033     :param ctx: Session context to use
3034     :param rp_ids: a set of resource provider IDs
3035     :param required_traits: A map, keyed by trait string name, of required
3036                             trait internal IDs that each provider TREE must
3037                             COLLECTIVELY have associated with it
3038     :param forbidden_traits: A map, keyed by trait string name, of trait
3039                              internal IDs that a resource provider must
3040                              not have.
3041     """
3042     # We now want to restrict the returned providers to only those provider
3043     # trees that have all our required traits.
3044     #
3045     # The SQL we want looks like this:
3046     #
3047     # SELECT outer_rp.id, outer_rp.root_provider_id
3048     # FROM resource_providers AS outer_rp
3049     # JOIN (
3050     #   SELECT rp.root_provider_id
3051     #   FROM resource_providers AS rp
3052     #   # Only if we have required traits...
3053     #   INNER JOIN resource_provider_traits AS rptt
3054     #   ON rp.id = rptt.resource_provider_id
3055     #   AND rptt.trait_id IN ($REQUIRED_TRAIT_IDS)
3056     #   # Only if we have forbidden_traits...
3057     #   LEFT JOIN resource_provider_traits AS rptt_forbid
3058     #   ON rp.id = rptt_forbid.resource_provider_id
3059     #   AND rptt_forbid.trait_id IN ($FORBIDDEN_TRAIT_IDS)
3060     #   WHERE rp.id IN ($RP_IDS)
3061     #   # Only if we have forbidden traits...
3062     #   AND rptt_forbid.resource_provider_id IS NULL
3063     #   GROUP BY rp.root_provider_id
3064     #   # Only if have required traits...
3065     #   HAVING COUNT(DISTINCT rptt.trait_id) == $NUM_REQUIRED_TRAITS
3066     # ) AS trees_with_traits
3067     #  ON outer_rp.root_provider_id = trees_with_traits.root_provider_id
3068     rpt = sa.alias(_RP_TBL, name="rp")
3069     cond = [rpt.c.id.in_(rp_ids)]
3070     subq = sa.select([rpt.c.root_provider_id])
3071     subq_join = None
3072     if required_traits:
3073         rptt = sa.alias(_RP_TRAIT_TBL, name="rptt")
3074         rpt_to_rptt = sa.join(
3075             rpt, rptt, sa.and_(
3076                 rpt.c.id == rptt.c.resource_provider_id,
3077                 rptt.c.trait_id.in_(required_traits.values())))
3078         subq_join = rpt_to_rptt
3079         # Only get the resource providers that have ALL the required traits,
3080         # so we need to GROUP BY the root provider and ensure that the
3081         # COUNT(trait_id) is equal to the number of traits we are requiring
3082         num_traits = len(required_traits)
3083         having_cond = sa.func.count(sa.distinct(rptt.c.trait_id)) == num_traits
3084         subq = subq.having(having_cond)
3085 
3086     # Tack on an additional LEFT JOIN clause inside the derived table if we've
3087     # got forbidden traits in the mix.
3088     if forbidden_traits:
3089         rptt_forbid = sa.alias(_RP_TRAIT_TBL, name="rptt_forbid")
3090         join_to = rpt
3091         if subq_join is not None:
3092             join_to = subq_join
3093         rpt_to_rptt_forbid = sa.outerjoin(
3094             join_to, rptt_forbid, sa.and_(
3095                 rpt.c.id == rptt_forbid.c.resource_provider_id,
3096                 rptt_forbid.c.trait_id.in_(forbidden_traits.values())))
3097         cond.append(rptt_forbid.c.resource_provider_id == sa.null())
3098         subq_join = rpt_to_rptt_forbid
3099 
3100     subq = subq.select_from(subq_join)
3101     subq = subq.where(sa.and_(*cond))
3102     subq = subq.group_by(rpt.c.root_provider_id)
3103     trees_with_traits = sa.alias(subq, name="trees_with_traits")
3104 
3105     outer_rps = sa.alias(_RP_TBL, name="outer_rps")
3106     outer_to_subq = sa.join(
3107         outer_rps, trees_with_traits,
3108         outer_rps.c.root_provider_id == trees_with_traits.c.root_provider_id)
3109     sel = sa.select([outer_rps.c.id, outer_rps.c.root_provider_id])
3110     sel = sel.select_from(outer_to_subq)
3111     res = ctx.session.execute(sel).fetchall()
3112 
3113     return [(rp_id, root_id) for rp_id, root_id in res]
3114 
3115 
3116 @db_api.placement_context_manager.reader
3117 def _get_trees_matching_all(ctx, resources, required_traits, forbidden_traits,
3118                             sharing, member_of):
3119     """Returns a list of two-tuples (provider internal ID, root provider
3120     internal ID) for providers that satisfy the request for resources.
3121 
3122     If traits are also required, this function only returns results where the
3123     set of providers within a tree that satisfy the resource request
3124     collectively have all the required traits associated with them. This means
3125     that given the following provider tree:
3126 
3127     cn1
3128      |
3129      --> pf1 (SRIOV_NET_VF:2)
3130      |
3131      --> pf2 (SRIOV_NET_VF:1, HW_NIC_OFFLOAD_GENEVE)
3132 
3133     If a user requests 1 SRIOV_NET_VF resource and no required traits will
3134     return both pf1 and pf2. However, a request for 2 SRIOV_NET_VF and required
3135     trait of HW_NIC_OFFLOAD_GENEVE will return no results (since pf1 is the
3136     only provider with enough inventory of SRIOV_NET_VF but it does not have
3137     the required HW_NIC_OFFLOAD_GENEVE trait).
3138 
3139     :note: This function is used for scenarios to get results for a
3140     RequestGroup with use_same_provider=False. In this scenario, we are able
3141     to use multiple providers within the same provider tree including sharing
3142     providers to satisfy different resources involved in a single RequestGroup.
3143 
3144     :param ctx: Session context to use
3145     :param resources: A dict, keyed by resource class ID, of the amount
3146                       requested of that resource class.
3147     :param required_traits: A map, keyed by trait string name, of required
3148                             trait internal IDs that each provider TREE must
3149                             COLLECTIVELY have associated with it
3150     :param forbidden_traits: A map, keyed by trait string name, of trait
3151                              internal IDs that a resource provider must
3152                              not have.
3153     :param sharing: dict, keyed by resource class ID, of lists of resource
3154                     provider IDs that share that resource class and can
3155                     contribute to the overall allocation request
3156     :param member_of: An optional list of lists of aggregate UUIDs. If
3157                       provided, the allocation_candidates returned will only be
3158                       for resource providers that are members of one or more of
3159                       the supplied aggregates in each aggregate UUID list.
3160     """
3161     # We first grab the provider trees that have nodes that meet the request
3162     # for each resource class.  Once we have this information, we'll then do a
3163     # followup query to winnow the set of resource providers to only those
3164     # provider *trees* that have all of the required traits.
3165     provs_with_inv = set()
3166     # provs_with_inv is a list of three-tuples with the second element being
3167     # the root provider ID and the third being resource class ID. Get the list
3168     # of root provider IDs and get all trees that collectively have all
3169     # required traits.
3170     trees_with_inv = set()
3171 
3172     for rc_id, amount in resources.items():
3173         rc_provs_with_inv = _get_providers_with_resource(ctx, rc_id, amount)
3174         if not rc_provs_with_inv:
3175             # If there's no providers that have one of the resource classes,
3176             # then we can short-circuit
3177             return []
3178         rc_trees = set(p[1] for p in rc_provs_with_inv)
3179         provs_with_inv |= set((p[0], p[1], rc_id) for p in rc_provs_with_inv)
3180 
3181         sharing_providers = sharing.get(rc_id)
3182         if sharing_providers:
3183             # There are sharing providers for this resource class, so we
3184             # should also get combinations of (sharing provider, anchor root)
3185             # in addition to (non-sharing provider, anchor root) we already
3186             # have.
3187             rc_provs_with_inv = _anchors_for_sharing_providers(
3188                                         ctx, sharing_providers, get_id=True)
3189             rc_provs_with_inv = set(
3190                 (p[0], p[1], rc_id) for p in rc_provs_with_inv)
3191             rc_trees |= set(p[1] for p in rc_provs_with_inv)
3192             provs_with_inv |= rc_provs_with_inv
3193 
3194         # Filter trees_with_inv to have only trees with enough inventories
3195         # for this resource class. Here "tree" includes sharing providers
3196         # in its terminology
3197         if trees_with_inv:
3198             trees_with_inv &= rc_trees
3199         else:
3200             trees_with_inv = rc_trees
3201 
3202         if not trees_with_inv:
3203             return []
3204 
3205     # Select only those tuples where there are providers for all requested
3206     # resource classes (trees_with_inv contains the root provider IDs of those
3207     # trees that contain all our requested resources)
3208     provs_with_inv = set(p for p in provs_with_inv if p[1] in trees_with_inv)
3209 
3210     if not provs_with_inv:
3211         return []
3212 
3213     # If 'member_of' has values, do a separate lookup to identify the
3214     # resource providers that meet the member_of constraints.
3215     if member_of:
3216         rps_in_aggs = _provider_ids_matching_aggregates(ctx, member_of,
3217                                                         rp_ids=trees_with_inv)
3218         if not rps_in_aggs:
3219             # Short-circuit. The user either asked for a non-existing
3220             # aggregate or there were no resource providers that matched
3221             # the requirements...
3222             return []
3223         provs_with_inv = set(p for p in provs_with_inv if p[1] in rps_in_aggs)
3224 
3225     if (not required_traits and not forbidden_traits) or (
3226             any(sharing.values())):
3227         # If there were no traits required, there's no difference in how we
3228         # calculate allocation requests between nested and non-nested
3229         # environments, so just short-circuit and return. Or if sharing
3230         # providers are in play, we check the trait constraints later
3231         # in _alloc_candidates_multiple_providers(), so skip.
3232         return list(provs_with_inv)
3233 
3234     # Return the providers where the providers have the available inventory
3235     # capacity and that set of providers (grouped by their tree) have all
3236     # of the required traits and none of the forbidden traits
3237     rp_ids_with_inv = set(p[0] for p in provs_with_inv)
3238     rp_tuples_with_trait = _get_trees_with_traits(
3239         ctx, rp_ids_with_inv, required_traits, forbidden_traits)
3240 
3241     ret = [rp_tuple for rp_tuple in provs_with_inv if (
3242         rp_tuple[0], rp_tuple[1]) in rp_tuples_with_trait]
3243 
3244     return ret
3245 
3246 
3247 def _build_provider_summaries(context, usages, prov_traits):
3248     """Given a list of dicts of usage information and a map of providers to
3249     their associated string traits, returns a dict, keyed by resource provider
3250     ID, of ProviderSummary objects.
3251 
3252     :param context: nova.context.RequestContext object
3253     :param usages: A list of dicts with the following format:
3254 
3255         {
3256             'resource_provider_id': <internal resource provider ID>,
3257             'resource_provider_uuid': <UUID>,
3258             'resource_class_id': <internal resource class ID>,
3259             'total': integer,
3260             'reserved': integer,
3261             'allocation_ratio': float,
3262         }
3263     :param prov_traits: A dict, keyed by internal resource provider ID, of
3264                         string trait names associated with that provider
3265     """
3266     # Before we go creating provider summary objects, first grab all the
3267     # provider information (including root, parent and UUID information) for
3268     # all providers involved in our operation
3269     rp_ids = set(usage['resource_provider_id'] for usage in usages)
3270     provider_ids = _provider_ids_from_rp_ids(context, rp_ids)
3271 
3272     # Build up a dict, keyed by internal resource provider ID, of
3273     # ProviderSummary objects containing one or more ProviderSummaryResource
3274     # objects representing the resources the provider has inventory for.
3275     summaries = {}
3276     for usage in usages:
3277         rp_id = usage['resource_provider_id']
3278         summary = summaries.get(rp_id)
3279         if not summary:
3280             pids = provider_ids[rp_id]
3281             summary = ProviderSummary(
3282                 context,
3283                 resource_provider=ResourceProvider(
3284                     context, id=pids.id, uuid=pids.uuid,
3285                     root_provider_uuid=pids.root_uuid,
3286                     parent_provider_uuid=pids.parent_uuid),
3287                 resources=[],
3288             )
3289             summaries[rp_id] = summary
3290 
3291         traits = prov_traits[rp_id]
3292         summary.traits = [Trait(context, name=tname) for tname in traits]
3293 
3294         rc_id = usage['resource_class_id']
3295         if rc_id is None:
3296             # NOTE(tetsuro): This provider doesn't have any inventory itself.
3297             # But we include this provider in summaries since another
3298             # provider in the same tree will be in the "allocation_request".
3299             # Let's skip the following and leave "ProviderSummary.resources"
3300             # field empty.
3301             continue
3302         # NOTE(jaypipes): usage['used'] may be None due to the LEFT JOIN of
3303         # the usages subquery, so we coerce NULL values to 0 here.
3304         used = usage['used'] or 0
3305         allocation_ratio = usage['allocation_ratio']
3306         cap = int((usage['total'] - usage['reserved']) * allocation_ratio)
3307         rc_name = _RC_CACHE.string_from_id(rc_id)
3308         rpsr = ProviderSummaryResource(
3309             context,
3310             resource_class=rc_name,
3311             capacity=cap,
3312             used=used,
3313             max_unit=usage['max_unit'],
3314         )
3315         summary.resources.append(rpsr)
3316     return summaries
3317 
3318 
3319 def _aggregates_associated_with_providers(a, b, prov_aggs):
3320     """quickly check if the two rps are in the same aggregates
3321 
3322     :param a: resource provider ID for first provider
3323     :param b: resource provider ID for second provider
3324     :param prov_aggs: a dict keyed by resource provider IDs, of sets
3325                       of aggregate ids associated with that provider
3326     """
3327     a_aggs = prov_aggs[a]
3328     b_aggs = prov_aggs[b]
3329     return a_aggs & b_aggs
3330 
3331 
3332 def _shared_allocation_request_resources(ctx, ns_rp_id, requested_resources,
3333                                          sharing, summaries, prov_aggs):
3334     """Returns a dict, keyed by resource class ID, of lists of
3335     AllocationRequestResource objects that represent resources that are
3336     provided by a sharing provider.
3337 
3338     :param ctx: nova.context.RequestContext object
3339     :param ns_rp_id: an internal ID of a non-sharing resource provider
3340     :param requested_resources: dict, keyed by resource class ID, of amounts
3341                                 being requested for that resource class
3342     :param sharing: dict, keyed by resource class ID, of lists of resource
3343                     provider IDs that share that resource class and can
3344                     contribute to the overall allocation request
3345     :param summaries: dict, keyed by resource provider ID, of ProviderSummary
3346                       objects containing usage and trait information for
3347                       resource providers involved in the overall request
3348     :param prov_aggs: dict, keyed by resource provider ID, of sets of
3349                       aggregate ids associated with that provider.
3350     """
3351     res_requests = collections.defaultdict(list)
3352     for rc_id in sharing:
3353         for rp_id in sharing[rc_id]:
3354             aggs_in_both = _aggregates_associated_with_providers(
3355                 ns_rp_id, rp_id, prov_aggs)
3356             if not aggs_in_both:
3357                 continue
3358             summary = summaries[rp_id]
3359             rp_uuid = summary.resource_provider.uuid
3360             res_req = AllocationRequestResource(
3361                 ctx,
3362                 resource_provider=ResourceProvider(ctx, uuid=rp_uuid),
3363                 resource_class=_RC_CACHE.string_from_id(rc_id),
3364                 amount=requested_resources[rc_id],
3365             )
3366             res_requests[rc_id].append(res_req)
3367     return res_requests
3368 
3369 
3370 def _allocation_request_for_provider(ctx, requested_resources, provider):
3371     """Returns an AllocationRequest object containing AllocationRequestResource
3372     objects for each resource class in the supplied requested resources dict.
3373 
3374     :param ctx: nova.context.RequestContext object
3375     :param requested_resources: dict, keyed by resource class ID, of amounts
3376                                 being requested for that resource class
3377     :param provider: ResourceProvider object representing the provider of the
3378                      resources.
3379     """
3380     resource_requests = [
3381         AllocationRequestResource(
3382             ctx, resource_provider=provider,
3383             resource_class=_RC_CACHE.string_from_id(rc_id),
3384             amount=amount,
3385         ) for rc_id, amount in requested_resources.items()
3386     ]
3387     # NOTE(efried): This method only produces an AllocationRequest with its
3388     # anchor in its own tree.  If the provider is a sharing provider, the
3389     # caller needs to identify the other anchors with which it might be
3390     # associated.
3391     return AllocationRequest(
3392             ctx, resource_requests=resource_requests,
3393             anchor_root_provider_uuid=provider.root_provider_uuid)
3394 
3395 
3396 def _check_traits_for_alloc_request(res_requests, summaries, prov_traits,
3397                                     required_traits, forbidden_traits):
3398     """Given a list of AllocationRequestResource objects, check if that
3399     combination can provide trait constraints. If it can, returns all
3400     resource provider internal IDs in play, else return an empty list.
3401 
3402     TODO(tetsuro): For optimization, we should move this logic to SQL in
3403                    _get_trees_matching_all().
3404 
3405     :param res_requests: a list of AllocationRequestResource objects that have
3406                          resource providers to be checked if they collectively
3407                          satisfy trait constraints in the required_traits and
3408                          forbidden_traits parameters.
3409     :param summaries: dict, keyed by resource provider ID, of ProviderSummary
3410                       objects containing usage and trait information for
3411                       resource providers involved in the overall request
3412     :param prov_traits: A dict, keyed by internal resource provider ID, of
3413                         string trait names associated with that provider
3414     :param required_traits: A map, keyed by trait string name, of required
3415                             trait internal IDs that each *allocation request's
3416                             set of providers* must *collectively* have
3417                             associated with them
3418     :param forbidden_traits: A map, keyed by trait string name, of trait
3419                              internal IDs that a resource provider must
3420                              not have.
3421     """
3422     all_prov_ids = []
3423     all_traits = set()
3424     for res_req in res_requests:
3425         rp_uuid = res_req.resource_provider.uuid
3426         for rp_id, summary in summaries.items():
3427             if summary.resource_provider.uuid == rp_uuid:
3428                 break
3429         rp_traits = set(prov_traits.get(rp_id, []))
3430 
3431         # Check if there are forbidden_traits
3432         conflict_traits = set(forbidden_traits) & set(rp_traits)
3433         if conflict_traits:
3434             LOG.debug('Excluding resource provider %s, it has '
3435                       'forbidden traits: (%s).',
3436                       rp_id, ', '.join(conflict_traits))
3437             return []
3438 
3439         all_prov_ids.append(rp_id)
3440         all_traits |= rp_traits
3441 
3442     # Check if there are missing traits
3443     missing_traits = set(required_traits) - all_traits
3444     if missing_traits:
3445         LOG.debug('Excluding a set of allocation candidate %s : '
3446                   'missing traits %s are not satisfied.',
3447                   all_prov_ids, ','.join(missing_traits))
3448         return []
3449 
3450     return all_prov_ids
3451 
3452 
3453 def _alloc_candidates_single_provider(ctx, requested_resources, rp_tuples):
3454     """Returns a tuple of (allocation requests, provider summaries) for a
3455     supplied set of requested resource amounts and resource providers. The
3456     supplied resource providers have capacity to satisfy ALL of the resources
3457     in the requested resources as well as ALL required traits that were
3458     requested by the user.
3459 
3460     This is used in two circumstances:
3461     - To get results for a RequestGroup with use_same_provider=True.
3462     - As an optimization when no sharing providers satisfy any of the requested
3463       resources, and nested providers are not in play.
3464     In these scenarios, we can more efficiently build the list of
3465     AllocationRequest and ProviderSummary objects due to not having to
3466     determine requests across multiple providers.
3467 
3468     :param ctx: nova.context.RequestContext object
3469     :param requested_resources: dict, keyed by resource class ID, of amounts
3470                                 being requested for that resource class
3471     :param rp_tuples: List of two-tuples of (provider ID, root provider ID)s
3472                       for providers that matched the requested resources
3473     """
3474     if not rp_tuples:
3475         return [], []
3476 
3477     # Get all root resource provider IDs.
3478     root_ids = set(p[1] for p in rp_tuples)
3479 
3480     # Grab usage summaries for each provider
3481     usages = _get_usages_by_provider_tree(ctx, root_ids)
3482 
3483     # Get a dict, keyed by resource provider internal ID, of trait string names
3484     # that provider has associated with it
3485     prov_traits = _get_traits_by_provider_tree(ctx, root_ids)
3486 
3487     # Get a dict, keyed by resource provider internal ID, of ProviderSummary
3488     # objects for all providers
3489     summaries = _build_provider_summaries(ctx, usages, prov_traits)
3490 
3491     # Next, build up a list of allocation requests. These allocation requests
3492     # are AllocationRequest objects, containing resource provider UUIDs,
3493     # resource class names and amounts to consume from that resource provider
3494     alloc_requests = []
3495     for rp_id, root_id in rp_tuples:
3496         rp_summary = summaries[rp_id]
3497         req_obj = _allocation_request_for_provider(
3498                 ctx, requested_resources, rp_summary.resource_provider)
3499         alloc_requests.append(req_obj)
3500         # If this is a sharing provider, we have to include an extra
3501         # AllocationRequest for every possible anchor.
3502         traits = [trait.name for trait in rp_summary.traits]
3503         if os_traits.MISC_SHARES_VIA_AGGREGATE in traits:
3504             anchors = set([p[1] for p in _anchors_for_sharing_providers(
3505                 ctx, [rp_summary.resource_provider.id])])
3506             for anchor in anchors:
3507                 # We already added self
3508                 if anchor == rp_summary.resource_provider.root_provider_uuid:
3509                     continue
3510                 req_obj = copy.deepcopy(req_obj)
3511                 req_obj.anchor_root_provider_uuid = anchor
3512                 alloc_requests.append(req_obj)
3513     return alloc_requests, list(summaries.values())
3514 
3515 
3516 def _alloc_candidates_multiple_providers(ctx, requested_resources,
3517         required_traits, forbidden_traits, rp_tuples):
3518     """Returns a tuple of (allocation requests, provider summaries) for a
3519     supplied set of requested resource amounts and tuples of
3520     (rp_id, root_id, rc_id). The supplied resource provider trees have
3521     capacity to satisfy ALL of the resources in the requested resources as
3522     well as ALL required traits that were requested by the user.
3523 
3524     This is a code path to get results for a RequestGroup with
3525     use_same_provider=False. In this scenario, we are able to use multiple
3526     providers within the same provider tree including sharing providers to
3527     satisfy different resources involved in a single request group.
3528 
3529     :param ctx: nova.context.RequestContext object
3530     :param requested_resources: dict, keyed by resource class ID, of amounts
3531                                 being requested for that resource class
3532     :param required_traits: A map, keyed by trait string name, of required
3533                             trait internal IDs that each *allocation request's
3534                             set of providers* must *collectively* have
3535                             associated with them
3536     :param forbidden_traits: A map, keyed by trait string name, of trait
3537                              internal IDs that a resource provider must
3538                              not have.
3539     :param rp_tuples: List of tuples of (provider ID, anchor root provider ID,
3540                       resource class ID)s for providers that matched the
3541                       requested resources
3542     """
3543     if not rp_tuples:
3544         return [], []
3545 
3546     # Get all the root resource provider IDs. We should include the first
3547     # values of rp_tuples because while sharing providers are root providers,
3548     # they have their "anchor" providers for the second value.
3549     root_ids = set(p[0] for p in rp_tuples) | set(p[1] for p in rp_tuples)
3550 
3551     # Grab usage summaries for each provider in the trees
3552     usages = _get_usages_by_provider_tree(ctx, root_ids)
3553 
3554     # Get a dict, keyed by resource provider internal ID, of trait string names
3555     # that provider has associated with it
3556     prov_traits = _get_traits_by_provider_tree(ctx, root_ids)
3557 
3558     # Get a dict, keyed by resource provider internal ID, of ProviderSummary
3559     # objects for all providers
3560     summaries = _build_provider_summaries(ctx, usages, prov_traits)
3561 
3562     # Get a dict, keyed by root provider internal ID, of a dict, keyed by
3563     # resource class internal ID, of lists of AllocationRequestResource objects
3564     tree_dict = collections.defaultdict(lambda: collections.defaultdict(list))
3565 
3566     for rp_id, root_id, rc_id in rp_tuples:
3567         rp_summary = summaries[rp_id]
3568         tree_dict[root_id][rc_id].append(
3569             AllocationRequestResource(
3570                 ctx, resource_provider=rp_summary.resource_provider,
3571                 resource_class=_RC_CACHE.string_from_id(rc_id),
3572                 amount=requested_resources[rc_id]))
3573 
3574     # Next, build up a list of allocation requests. These allocation requests
3575     # are AllocationRequest objects, containing resource provider UUIDs,
3576     # resource class names and amounts to consume from that resource provider
3577     alloc_requests = []
3578 
3579     # Build a list of lists of provider internal IDs that end up in
3580     # allocation request objects. This is used to ensure we don't end up
3581     # having allocation requests with duplicate sets of resource providers.
3582     alloc_prov_ids = []
3583 
3584     # Let's look into each tree
3585     for root_id, alloc_dict in tree_dict.items():
3586         # Get request_groups, which is a list of lists of
3587         # AllocationRequestResource(ARR) per requested resource class(rc).
3588         # For example, if we have the alloc_dict:
3589         # {rc1_id: [ARR(rc1, rp1), ARR(rc1, rp2)],
3590         #  rc2_id: [ARR(rc2, rp1), ARR(rc2, rp2)],
3591         #  rc3_id: [ARR(rc3, rp1)]}
3592         # then the request_groups would be something like
3593         # [[ARR(rc1, rp1), ARR(rc1, rp2)],
3594         #  [ARR(rc2, rp1), ARR(rc2, rp2)],
3595         #  [ARR(rc3, rp1)]]
3596         # , which should be ordered by the resource class id.
3597         request_groups = [val for key, val in sorted(alloc_dict.items())]
3598 
3599         root_summary = summaries[root_id]
3600         root_uuid = root_summary.resource_provider.uuid
3601 
3602         # Using itertools.product, we get all the combinations of resource
3603         # providers in a tree.
3604         # For example, the sample in the comment above becomes:
3605         # [(ARR(rc1, ss1), ARR(rc2, ss1), ARR(rc3, ss1)),
3606         #  (ARR(rc1, ss1), ARR(rc2, ss2), ARR(rc3, ss1)),
3607         #  (ARR(rc1, ss2), ARR(rc2, ss1), ARR(rc3, ss1)),
3608         #  (ARR(rc1, ss2), ARR(rc2, ss2), ARR(rc3, ss1))]
3609         for res_requests in itertools.product(*request_groups):
3610             all_prov_ids = _check_traits_for_alloc_request(res_requests,
3611                 summaries, prov_traits, required_traits, forbidden_traits)
3612             if (not all_prov_ids) or (all_prov_ids in alloc_prov_ids):
3613                 # This combination doesn't satisfy trait constraints,
3614                 # ...or we already have this permutation, which happens
3615                 # when multiple sharing providers with different resource
3616                 # classes are in one request.
3617                 continue
3618             alloc_prov_ids.append(all_prov_ids)
3619             alloc_requests.append(
3620                 AllocationRequest(ctx, resource_requests=list(res_requests),
3621                                   anchor_root_provider_uuid=root_uuid)
3622             )
3623     return alloc_requests, list(summaries.values())
3624 
3625 
3626 @db_api.placement_context_manager.reader
3627 def _get_traits_by_provider_tree(ctx, root_ids):
3628     """Returns a dict, keyed by provider IDs for all resource providers
3629     in all trees indicated in the ``root_ids``, of string trait names
3630     associated with that provider.
3631 
3632     :raises: ValueError when root_ids is empty.
3633 
3634     :param ctx: nova.context.RequestContext object
3635     :param root_ids: list of root resource provider IDs
3636     """
3637     if not root_ids:
3638         raise ValueError(_("Expected root_ids to be a list of root resource"
3639                            "provider internal IDs, but got an empty list."))
3640 
3641     rpt = sa.alias(_RP_TBL, name='rpt')
3642     rptt = sa.alias(_RP_TRAIT_TBL, name='rptt')
3643     tt = sa.alias(_TRAIT_TBL, name='t')
3644     rpt_rptt = sa.join(rpt, rptt, rpt.c.id == rptt.c.resource_provider_id)
3645     j = sa.join(rpt_rptt, tt, rptt.c.trait_id == tt.c.id)
3646     sel = sa.select([rptt.c.resource_provider_id, tt.c.name]).select_from(j)
3647     sel = sel.where(rpt.c.root_provider_id.in_(root_ids))
3648     res = collections.defaultdict(list)
3649     for r in ctx.session.execute(sel):
3650         res[r[0]].append(r[1])
3651     return res
3652 
3653 
3654 @db_api.placement_context_manager.reader
3655 def _trait_ids_from_names(ctx, names):
3656     """Given a list of string trait names, returns a dict, keyed by those
3657     string names, of the corresponding internal integer trait ID.
3658 
3659     :raises: ValueError when names is empty.
3660 
3661     :param ctx: nova.context.RequestContext object
3662     :param names: list of string trait names
3663     """
3664     if not names:
3665         raise ValueError(_("Expected names to be a list of string trait "
3666                            "names, but got an empty list."))
3667 
3668     # Avoid SAWarnings about unicode types...
3669     unames = map(six.text_type, names)
3670     tt = sa.alias(_TRAIT_TBL, name='t')
3671     sel = sa.select([tt.c.name, tt.c.id]).where(tt.c.name.in_(unames))
3672     return {r[0]: r[1] for r in ctx.session.execute(sel)}
3673 
3674 
3675 def _rp_rc_key(rp, rc):
3676     """Creates hashable key unique to a provider + resource class."""
3677     return rp.uuid, rc
3678 
3679 
3680 def _consolidate_allocation_requests(areqs):
3681     """Consolidates a list of AllocationRequest into one.
3682 
3683     :param areqs: A list containing one AllocationRequest for each input
3684             RequestGroup.  This may mean that multiple resource_requests
3685             contain resource amounts of the same class from the same provider.
3686     :return: A single consolidated AllocationRequest, containing no
3687             resource_requests with duplicated (resource_provider,
3688             resource_class).
3689     """
3690     # Construct a dict, keyed by resource provider UUID + resource class, of
3691     # AllocationRequestResource, consolidating as we go.
3692     arrs_by_rp_rc = {}
3693     # areqs must have at least one element.  Save the anchor to populate the
3694     # returned AllocationRequest.
3695     anchor_rp_uuid = areqs[0].anchor_root_provider_uuid
3696     for areq in areqs:
3697         # Sanity check: the anchor should be the same for every areq
3698         if anchor_rp_uuid != areq.anchor_root_provider_uuid:
3699             # This should never happen.  If it does, it's a dev bug.
3700             raise ValueError(
3701                 _("Expected every AllocationRequest in "
3702                   "`_consolidate_allocation_requests` to have the same "
3703                   "anchor!"))
3704         for arr in areq.resource_requests:
3705             key = _rp_rc_key(arr.resource_provider, arr.resource_class)
3706             if key not in arrs_by_rp_rc:
3707                 arrs_by_rp_rc[key] = copy.deepcopy(arr)
3708             else:
3709                 arrs_by_rp_rc[key].amount += arr.amount
3710     return AllocationRequest(
3711         resource_requests=list(arrs_by_rp_rc.values()),
3712         anchor_root_provider_uuid=anchor_rp_uuid)
3713 
3714 
3715 def _satisfies_group_policy(areqs, group_policy, num_granular_groups):
3716     """Applies group_policy to a list of AllocationRequest.
3717 
3718     Returns True or False, indicating whether this list of
3719     AllocationRequest satisfies group_policy, as follows:
3720 
3721     * "isolate": Each AllocationRequest with use_same_provider=True
3722                  is satisfied by a single resource provider.  If the "isolate"
3723                  policy is in effect, each such AllocationRequest must be
3724                  satisfied by a *unique* resource provider.
3725     * "none" or None: Always returns True.
3726 
3727     :param areqs: A list containing one AllocationRequest for each input
3728             RequestGroup.
3729     :param group_policy: String indicating how RequestGroups should interact
3730             with each other.  If the value is "isolate", we will return False
3731             if AllocationRequests that came from RequestGroups keyed by
3732             nonempty suffixes are satisfied by the same provider.
3733     :param num_granular_groups: The number of granular (use_same_provider=True)
3734             RequestGroups in the request.
3735     :return: True if areqs satisfies group_policy; False otherwise.
3736     """
3737     if group_policy != 'isolate':
3738         # group_policy="none" means no filtering
3739         return True
3740 
3741     # The number of unique resource providers referenced in the request groups
3742     # having use_same_provider=True must be equal to the number of granular
3743     # groups.
3744     num_granular_groups_in_areqs = len(set(
3745         # We can reliably use the first resource_request's provider: all the
3746         # resource_requests are satisfied by the same provider by definition
3747         # because use_same_provider is True.
3748         areq.resource_requests[0].resource_provider.uuid
3749         for areq in areqs
3750         if areq.use_same_provider))
3751     if num_granular_groups == num_granular_groups_in_areqs:
3752         return True
3753     LOG.debug('Excluding the following set of AllocationRequest because '
3754               'group_policy=isolate and the number of granular groups in the '
3755               'set (%d) does not match the number of granular groups in the '
3756               'request (%d): %s',
3757               num_granular_groups_in_areqs, num_granular_groups, str(areqs))
3758     return False
3759 
3760 
3761 def _exceeds_capacity(areq, psum_res_by_rp_rc):
3762     """Checks a (consolidated) AllocationRequest against the provider summaries
3763     to ensure that it does not exceed capacity.
3764 
3765     Exceeding capacity can mean the total amount (already used plus this
3766     allocation) exceeds the total inventory amount; or this allocation exceeds
3767     the max_unit in the inventory record.
3768 
3769     :param areq: An AllocationRequest produced by the
3770             `_consolidate_allocation_requests` method.
3771     :param psum_res_by_rp_rc: A dict, keyed by provider + resource class via
3772             _rp_rc_key, of ProviderSummaryResource.
3773     :return: True if areq exceeds capacity; False otherwise.
3774     """
3775     for arr in areq.resource_requests:
3776         key = _rp_rc_key(arr.resource_provider, arr.resource_class)
3777         psum_res = psum_res_by_rp_rc[key]
3778         if psum_res.used + arr.amount > psum_res.capacity:
3779             LOG.debug('Excluding the following AllocationRequest because used '
3780                       '(%d) + amount (%d) > capacity (%d) for resource class '
3781                       '%s: %s',
3782                       psum_res.used, arr.amount, psum_res.capacity,
3783                       arr.resource_class, str(areq))
3784             return True
3785         if arr.amount > psum_res.max_unit:
3786             LOG.debug('Excluding the following AllocationRequest because '
3787                       'amount (%d) > max_unit (%d) for resource class %s: %s',
3788                       arr.amount, psum_res.max_unit, arr.resource_class,
3789                       str(areq))
3790             return True
3791     return False
3792 
3793 
3794 def _merge_candidates(candidates, group_policy=None):
3795     """Given a dict, keyed by RequestGroup suffix, of tuples of
3796     (allocation_requests, provider_summaries), produce a single tuple of
3797     (allocation_requests, provider_summaries) that appropriately incorporates
3798     the elements from each.
3799 
3800     Each (alloc_reqs, prov_sums) in `candidates` satisfies one RequestGroup.
3801     This method creates a list of alloc_reqs, *each* of which satisfies *all*
3802     of the RequestGroups.
3803 
3804     For that merged list of alloc_reqs, a corresponding provider_summaries is
3805     produced.
3806 
3807     :param candidates: A dict, keyed by integer suffix or '', of tuples of
3808             (allocation_requests, provider_summaries) to be merged.
3809     :param group_policy: String indicating how RequestGroups should interact
3810             with each other.  If the value is "isolate", we will filter out
3811             candidates where AllocationRequests that came from RequestGroups
3812             keyed by nonempty suffixes are satisfied by the same provider.
3813     :return: A tuple of (allocation_requests, provider_summaries).
3814     """
3815     # Build a dict, keyed by anchor root provider UUID, of dicts, keyed by
3816     # suffix, of nonempty lists of AllocationRequest.  Each inner dict must
3817     # possess all of the suffix keys to be viable (i.e. contains at least
3818     # one AllocationRequest per RequestGroup).
3819     #
3820     # areq_lists_by_anchor =
3821     #   { anchor_root_provider_uuid: {
3822     #         '': [AllocationRequest, ...],   \  This dict must contain
3823     #         '1': [AllocationRequest, ...],   \ exactly one nonempty list per
3824     #         ...                              / suffix to be viable. That
3825     #         '42': [AllocationRequest, ...], /  filtering is done later.
3826     #     },
3827     #     ...
3828     #   }
3829     areq_lists_by_anchor = collections.defaultdict(
3830             lambda: collections.defaultdict(list))
3831     # Save off all the provider summaries lists - we'll use 'em later.
3832     all_psums = []
3833     # Construct a dict, keyed by resource provider + resource class, of
3834     # ProviderSummaryResource.  This will be used to do a final capacity
3835     # check/filter on each merged AllocationRequest.
3836     psum_res_by_rp_rc = {}
3837     for suffix, (areqs, psums) in candidates.items():
3838         for areq in areqs:
3839             anchor = areq.anchor_root_provider_uuid
3840             areq_lists_by_anchor[anchor][suffix].append(areq)
3841         for psum in psums:
3842             all_psums.append(psum)
3843             for psum_res in psum.resources:
3844                 key = _rp_rc_key(
3845                         psum.resource_provider, psum_res.resource_class)
3846                 psum_res_by_rp_rc[key] = psum_res
3847 
3848     # Create all combinations picking one AllocationRequest from each list
3849     # for each anchor.
3850     areqs = []
3851     all_suffixes = set(candidates)
3852     num_granular_groups = len(all_suffixes - set(['']))
3853     for areq_lists_by_suffix in areq_lists_by_anchor.values():
3854         # Filter out any entries that don't have allocation requests for
3855         # *all* suffixes (i.e. all RequestGroups)
3856         if set(areq_lists_by_suffix) != all_suffixes:
3857             continue
3858         # We're using itertools.product to go from this:
3859         # areq_lists_by_suffix = {
3860         #     '':   [areq__A,   areq__B,   ...],
3861         #     '1':  [areq_1_A,  areq_1_B,  ...],
3862         #     ...
3863         #     '42': [areq_42_A, areq_42_B, ...],
3864         # }
3865         # to this:
3866         # [ [areq__A, areq_1_A, ..., areq_42_A],  Each of these lists is one
3867         #   [areq__A, areq_1_A, ..., areq_42_B],  areq_list in the loop below.
3868         #   [areq__A, areq_1_B, ..., areq_42_A],  each areq_list contains one
3869         #   [areq__A, areq_1_B, ..., areq_42_B],  AllocationRequest from each
3870         #   [areq__B, areq_1_A, ..., areq_42_A],  RequestGroup. So taken as a
3871         #   [areq__B, areq_1_A, ..., areq_42_B],  whole, each list is a viable
3872         #   [areq__B, areq_1_B, ..., areq_42_A],  (preliminary) candidate to
3873         #   [areq__B, areq_1_B, ..., areq_42_B],  return.
3874         #   ...,
3875         # ]
3876         for areq_list in itertools.product(
3877                 *list(areq_lists_by_suffix.values())):
3878             # At this point, each AllocationRequest in areq_list is still
3879             # marked as use_same_provider. This is necessary to filter by group
3880             # policy, which enforces how these interact with each other.
3881             if not _satisfies_group_policy(
3882                     areq_list, group_policy, num_granular_groups):
3883                 continue
3884             # Now we go from this (where 'arr' is AllocationRequestResource):
3885             # [ areq__B(arrX, arrY, arrZ),
3886             #   areq_1_A(arrM, arrN),
3887             #   ...,
3888             #   areq_42_B(arrQ)
3889             # ]
3890             # to this:
3891             # areq_combined(arrX, arrY, arrZ, arrM, arrN, arrQ)
3892             # Note that this discards the information telling us which
3893             # RequestGroup led to which piece of the final AllocationRequest.
3894             # We needed that to be present for the previous filter; we need it
3895             # to be *absent* for the next one (and for the final output).
3896             areq = _consolidate_allocation_requests(areq_list)
3897             # Since we sourced this AllocationRequest from multiple
3898             # *independent* queries, it's possible that the combined result
3899             # now exceeds capacity where amounts of the same RP+RC were
3900             # folded together.  So do a final capacity check/filter.
3901             if _exceeds_capacity(areq, psum_res_by_rp_rc):
3902                 continue
3903             areqs.append(areq)
3904 
3905     # It's possible we've filtered out everything.  If so, short out.
3906     if not areqs:
3907         return [], []
3908 
3909     # Now we have to produce provider summaries.  The provider summaries in
3910     # the candidates input contain all the information; we just need to
3911     # filter it down to only the providers in trees represented by our merged
3912     # list of allocation requests.
3913     tree_uuids = set()
3914     for areq in areqs:
3915         for arr in areq.resource_requests:
3916             tree_uuids.add(arr.resource_provider.root_provider_uuid)
3917     psums = [psum for psum in all_psums if
3918              psum.resource_provider.root_provider_uuid in tree_uuids]
3919 
3920     return areqs, psums
3921 
3922 
3923 @base.VersionedObjectRegistry.register_if(False)
3924 class AllocationCandidates(base.VersionedObject):
3925     """The AllocationCandidates object is a collection of possible allocations
3926     that match some request for resources, along with some summary information
3927     about the resource providers involved in these allocation candidates.
3928     """
3929 
3930     fields = {
3931         # A collection of allocation possibilities that can be attempted by the
3932         # caller that would, at the time of calling, meet the requested
3933         # resource constraints
3934         'allocation_requests': fields.ListOfObjectsField('AllocationRequest'),
3935         # Information about usage and inventory that relate to any provider
3936         # contained in any of the AllocationRequest objects in the
3937         # allocation_requests field
3938         'provider_summaries': fields.ListOfObjectsField('ProviderSummary'),
3939     }
3940 
3941     @classmethod
3942     def get_by_requests(cls, context, requests, limit=None, group_policy=None):
3943         """Returns an AllocationCandidates object containing all resource
3944         providers matching a set of supplied resource constraints, with a set
3945         of allocation requests constructed from that list of resource
3946         providers. If CONF.placement.randomize_allocation_candidates is True
3947         (default is False) then the order of the allocation requests will
3948         be randomized.
3949 
3950         :param context: Nova RequestContext.
3951         :param requests: Dict, keyed by suffix, of
3952                          nova.api.openstack.placement.util.RequestGroup
3953         :param limit: An integer, N, representing the maximum number of
3954                       allocation candidates to return. If
3955                       CONF.placement.randomize_allocation_candidates is True
3956                       this will be a random sampling of N of the available
3957                       results. If False then the first N results, in whatever
3958                       order the database picked them, will be returned. In
3959                       either case if there are fewer than N total results,
3960                       all the results will be returned.
3961         :param group_policy: String indicating how RequestGroups with
3962                              use_same_provider=True should interact with each
3963                              other.  If the value is "isolate", we will filter
3964                              out allocation requests where any such
3965                              RequestGroups are satisfied by the same RP.
3966         :return: An instance of AllocationCandidates with allocation_requests
3967                  and provider_summaries satisfying `requests`, limited
3968                  according to `limit`.
3969         """
3970         alloc_reqs, provider_summaries = cls._get_by_requests(
3971             context, requests, limit=limit, group_policy=group_policy)
3972         return cls(
3973             context,
3974             allocation_requests=alloc_reqs,
3975             provider_summaries=provider_summaries,
3976         )
3977 
3978     @staticmethod
3979     def _get_by_one_request(context, request, sharing_providers, has_trees):
3980         """Get allocation candidates for one RequestGroup.
3981 
3982         Must be called from within an placement_context_manager.reader
3983         (or writer) context.
3984 
3985         :param context: Nova RequestContext.
3986         :param request: One nova.api.openstack.placement.util.RequestGroup
3987         :param sharing_providers: dict, keyed by resource class internal ID, of
3988                                   the set of provider IDs containing shared
3989                                   inventory of that resource class
3990         :param has_trees: bool indicating there is some level of nesting in the
3991                           environment (if there isn't, we take faster, simpler
3992                           code paths)
3993         :return: A tuple of (allocation_requests, provider_summaries)
3994                  satisfying `request`.
3995         """
3996         # Transform resource string names to internal integer IDs
3997         resources = {
3998             _RC_CACHE.id_from_string(key): value
3999             for key, value in request.resources.items()
4000         }
4001 
4002         # maps the trait name to the trait internal ID
4003         required_trait_map = {}
4004         forbidden_trait_map = {}
4005         for trait_map, traits in (
4006                 (required_trait_map, request.required_traits),
4007                 (forbidden_trait_map, request.forbidden_traits)):
4008             if traits:
4009                 trait_map.update(_trait_ids_from_names(context, traits))
4010                 # Double-check that we found a trait ID for each requested name
4011                 if len(trait_map) != len(traits):
4012                     missing = traits - set(trait_map)
4013                     raise exception.TraitNotFound(names=', '.join(missing))
4014 
4015         member_of = request.member_of
4016 
4017         any_sharing = any(sharing_providers.values())
4018         if not request.use_same_provider and (has_trees or any_sharing):
4019             # TODO(jaypipes): The check/callout to handle trees goes here.
4020             # Build a dict, keyed by resource class internal ID, of lists of
4021             # internal IDs of resource providers that share some inventory for
4022             # each resource class requested.
4023             # If there aren't any providers that have any of the
4024             # required traits, just exit early...
4025             if required_trait_map:
4026                 # TODO(cdent): Now that there is also a forbidden_trait_map
4027                 # it should be possible to further optimize this attempt at
4028                 # a quick return, but we leave that to future patches for
4029                 # now.
4030                 trait_rps = _get_provider_ids_having_any_trait(
4031                     context, required_trait_map)
4032                 if not trait_rps:
4033                     return [], []
4034             rp_tuples = _get_trees_matching_all(context, resources,
4035                 required_trait_map, forbidden_trait_map,
4036                 sharing_providers, member_of)
4037             return _alloc_candidates_multiple_providers(context, resources,
4038                 required_trait_map, forbidden_trait_map, rp_tuples)
4039 
4040         # Either we are processing a single-RP request group, or there are no
4041         # sharing providers that (help) satisfy the request.  Get a list of
4042         # resource provider IDs that have ALL the requested resources and more
4043         # efficiently construct the allocation requests.
4044         # NOTE(jaypipes): When we start handling nested providers, we may
4045         # add new code paths or modify this code path to return root
4046         # provider IDs of provider trees instead of the resource provider
4047         # IDs.
4048         rp_ids = _get_provider_ids_matching(context, resources,
4049                                             required_trait_map,
4050                                             forbidden_trait_map, member_of)
4051         return _alloc_candidates_single_provider(context, resources, rp_ids)
4052 
4053     @classmethod
4054     # TODO(efried): This is only a writer context because it accesses the
4055     # resource_providers table via ResourceProvider.get_by_uuid, which does
4056     # data migration to populate the root_provider_uuid.  Change this back to a
4057     # reader when that migration is no longer happening.
4058     @db_api.placement_context_manager.writer
4059     def _get_by_requests(cls, context, requests, limit=None,
4060                          group_policy=None):
4061         # TODO(jaypipes): Make a RequestGroupContext object and put these
4062         # pieces of information in there, passing the context to the various
4063         # internal functions handling that part of the request.
4064         sharing = {}
4065         for request in requests.values():
4066             member_of = request.member_of
4067             for rc_name, amount in request.resources.items():
4068                 rc_id = _RC_CACHE.id_from_string(rc_name)
4069                 if rc_id not in sharing:
4070                     sharing[rc_id] = _get_providers_with_shared_capacity(
4071                         context, rc_id, amount, member_of)
4072         has_trees = _has_provider_trees(context)
4073 
4074         candidates = {}
4075         for suffix, request in requests.items():
4076             alloc_reqs, summaries = cls._get_by_one_request(
4077                 context, request, sharing, has_trees)
4078             LOG.debug("%s (suffix '%s') returned %d matches",
4079                       str(request), str(suffix), len(alloc_reqs))
4080             if not alloc_reqs:
4081                 # Shortcut: If any one request resulted in no candidates, the
4082                 # whole operation is shot.
4083                 return [], []
4084             # Mark each allocation request according to whether its
4085             # corresponding RequestGroup required it to be restricted to a
4086             # single provider.  We'll need this later to evaluate group_policy.
4087             for areq in alloc_reqs:
4088                 areq.use_same_provider = request.use_same_provider
4089             candidates[suffix] = alloc_reqs, summaries
4090 
4091         # At this point, each (alloc_requests, summary_obj) in `candidates` is
4092         # independent of the others. We need to fold them together such that
4093         # each allocation request satisfies *all* the incoming `requests`.  The
4094         # `candidates` dict is guaranteed to contain entries for all suffixes,
4095         # or we would have short-circuited above.
4096         alloc_request_objs, summary_objs = _merge_candidates(
4097                 candidates, group_policy=group_policy)
4098 
4099         # Limit the number of allocation request objects. We do this after
4100         # creating all of them so that we can do a random slice without
4101         # needing to mess with the complex sql above or add additional
4102         # columns to the DB.
4103         if limit and limit <= len(alloc_request_objs):
4104             if CONF.placement.randomize_allocation_candidates:
4105                 alloc_request_objs = random.sample(alloc_request_objs, limit)
4106             else:
4107                 alloc_request_objs = alloc_request_objs[:limit]
4108         elif CONF.placement.randomize_allocation_candidates:
4109             random.shuffle(alloc_request_objs)
4110 
4111         # Limit summaries to only those mentioned in the allocation requests.
4112         if limit and limit <= len(alloc_request_objs):
4113             kept_summary_objs = []
4114             alloc_req_rp_uuids = set()
4115             # Extract resource provider uuids from the resource requests.
4116             for aro in alloc_request_objs:
4117                 for arr in aro.resource_requests:
4118                     alloc_req_rp_uuids.add(arr.resource_provider.uuid)
4119             for summary in summary_objs:
4120                 rp_uuid = summary.resource_provider.uuid
4121                 # Skip a summary if we are limiting and haven't selected an
4122                 # allocation request that uses the resource provider.
4123                 if rp_uuid not in alloc_req_rp_uuids:
4124                     continue
4125                 kept_summary_objs.append(summary)
4126         else:
4127             kept_summary_objs = summary_objs
4128 
4129         return alloc_request_objs, kept_summary_objs
4130 
4131 
4132 @db_api.placement_context_manager.writer
4133 def reshape(ctx, inventories, allocations):
4134     """The 'replace the world' strategy that is executed when we want to
4135     completely replace a set of provider inventory, allocation and consumer
4136     information in a single transaction.
4137 
4138     :note: The reason this has to be done in a single monolithic function is so
4139            we have a single top-level function on which to decorate with the
4140            @db_api.placement_context_manager.writer transaction context
4141            manager. Each time a top-level function that is decorated with this
4142            exits, the transaction is either COMMIT'd or ROLLBACK'd. We need to
4143            avoid calling two functions that are already decorated with a
4144            transaction context manager from a function that *isn't* decorated
4145            with the transaction context manager if we want all changes involved
4146            in the sub-functions to operate within a single DB transaction.
4147 
4148     :param ctx: `nova.api.openstack.placement.context.RequestContext` object
4149                 containing the DB transaction context.
4150     :param inventories: dict, keyed by resource provider UUID, of
4151                         `InventoryList` objects representing the replaced
4152                         inventory information for the provider.
4153     :param allocations: `AllocationList` object containing all allocations for
4154                         all consumers being modified by the reshape operation.
4155     :raises: `exception.ConcurrentUpdateDetected` when any resource provider or
4156              consumer generation increment fails due to concurrent changes to
4157              the same objects.
4158     """
4159     # The resource provider objects, keyed by provider UUID, that are involved
4160     # in this transaction. We keep a cache of these because as we perform the
4161     # various operations on the providers, their generations increment and we
4162     # want to "inject" the changed resource provider objects into the
4163     # AllocationList's objects before calling AllocationList.replace_all().
4164     # We start with the providers in the allocation objects, but only use one
4165     # if we don't find it in the inventories.
4166     affected_providers = {alloc.resource_provider.uuid: alloc.resource_provider
4167                           for alloc in allocations}
4168     # We have to do the inventory changes in two steps because:
4169     # - we can't delete inventories with allocations; and
4170     # - we can't create allocations on nonexistent inventories.
4171     # So in the first step we create a kind of "union" inventory for each
4172     # provider. It contains all the inventories that the request wishes to
4173     # exist in the end, PLUS any inventories that the request wished to remove
4174     # (in their original form).
4175     # Note that this can cause us to end up with an interim situation where we
4176     # have modified an inventory to have less capacity than is currently
4177     # allocated, but that's allowed by the code. If the final picture is
4178     # overcommitted, we'll get an appropriate exception when we replace the
4179     # allocations at the end.
4180     for rp_uuid, new_inv_list in inventories.items():
4181         LOG.debug("reshaping: *interim* inventory replacement for provider %s",
4182                   rp_uuid)
4183         # We need the resource provider object.
4184         if new_inv_list:
4185             # If the inventory list is nonempty, grab it from the first entry.
4186             # This overrides (and will replace, below) an object that was
4187             # prepopulated from the allocations.
4188             rp = new_inv_list[0].resource_provider
4189         elif rp_uuid in affected_providers:
4190             # No inventory object; second choice is from an allocation object.
4191             rp = affected_providers[rp_uuid]
4192         else:
4193             # No inventory or allocations for this provider - which makes sense
4194             # when we're "clearing out" a provider. Look it up.
4195             rp = ResourceProvider.get_by_uuid(ctx, rp_uuid)
4196         # Update the cache. This may be replacing an entry that came from
4197         # allocations, or adding a new entry from inventories and/or db lookup.
4198         affected_providers[rp_uuid] = rp
4199 
4200         # Optimization: If the new inventory is empty, the below would be
4201         # replacing it with itself (and incrementing the generation)
4202         # unnecessarily.
4203         if not new_inv_list:
4204             continue
4205 
4206         # A dict, keyed by resource class, of the Inventory objects. We start
4207         # with the original inventory list.
4208         inv_by_rc = {inv.resource_class: inv for inv in
4209                      InventoryList.get_all_by_resource_provider(ctx, rp)}
4210         # Now add each inventory in the new inventory list. If an inventory for
4211         # that resource class existed in the original inventory list, it is
4212         # overwritten.
4213         for inv in new_inv_list:
4214             inv_by_rc[inv.resource_class] = inv
4215         # Set the interim inventory structure.
4216         rp.set_inventory(InventoryList(objects=list(inv_by_rc.values())))
4217 
4218     # NOTE(jaypipes): The above inventory replacements will have
4219     # incremented the resource provider generations, so we need to look in
4220     # the AllocationList and swap the resource provider object with the one we
4221     # saved above that has the updated provider generation in it.
4222     for alloc in allocations:
4223         rp_uuid = alloc.resource_provider.uuid
4224         if rp_uuid in affected_providers:
4225             alloc.resource_provider = affected_providers[rp_uuid]
4226 
4227     # Now we can replace all the allocations
4228     LOG.debug("reshaping: attempting allocation replacement")
4229     allocations.replace_all()
4230 
4231     # And finally, we can set the inventories to their actual desired state.
4232     for rp_uuid, new_inv_list in inventories.items():
4233         LOG.debug("reshaping: *final* inventory replacement for provider %s",
4234                   rp_uuid)
4235         affected_providers[rp_uuid].set_inventory(new_inv_list)
