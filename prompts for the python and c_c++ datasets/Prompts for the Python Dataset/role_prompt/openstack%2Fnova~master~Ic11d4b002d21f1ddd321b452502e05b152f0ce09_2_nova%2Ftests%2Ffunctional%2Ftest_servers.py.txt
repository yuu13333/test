I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2011 Justin Santa Barbara
2 # All Rights Reserved.
3 #
4 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
5 #    not use this file except in compliance with the License. You may obtain
6 #    a copy of the License at
7 #
8 #         http://www.apache.org/licenses/LICENSE-2.0
9 #
10 #    Unless required by applicable law or agreed to in writing, software
11 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
12 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
13 #    License for the specific language governing permissions and limitations
14 #    under the License.
15 from __future__ import absolute_import
16 
17 import collections
18 import copy
19 import datetime
20 import fixtures
21 import time
22 import zlib
23 
24 from keystoneauth1 import adapter
25 import mock
26 from oslo_config import cfg
27 from oslo_log import log as logging
28 from oslo_serialization import base64
29 from oslo_serialization import jsonutils
30 from oslo_utils.fixture import uuidsentinel as uuids
31 from oslo_utils import timeutils
32 import six
33 
34 from nova.compute import api as compute_api
35 from nova.compute import instance_actions
36 from nova.compute import manager as compute_manager
37 from nova.compute import rpcapi
38 from nova import context
39 from nova import exception
40 from nova import objects
41 from nova.objects import block_device as block_device_obj
42 from nova import rc_fields
43 from nova.scheduler import weights
44 from nova import test
45 from nova.tests import fixtures as nova_fixtures
46 from nova.tests.functional.api import client
47 from nova.tests.functional import integrated_helpers
48 from nova.tests.unit.api.openstack import fakes
49 from nova.tests.unit import fake_block_device
50 from nova.tests.unit import fake_notifier
51 from nova.tests.unit import fake_requests
52 import nova.tests.unit.image.fake
53 from nova.tests.unit.objects import test_instance_info_cache
54 from nova.virt import fake
55 from nova import volume
56 
57 CONF = cfg.CONF
58 
59 LOG = logging.getLogger(__name__)
60 
61 
62 class AltHostWeigher(weights.BaseHostWeigher):
63     """Used in the alternate host tests to return a pre-determined list of
64     hosts.
65     """
66     def _weigh_object(self, host_state, weight_properties):
67         """Return a defined order of hosts."""
68         weights = {"selection": 999, "alt_host1": 888, "alt_host2": 777,
69                    "alt_host3": 666, "host1": 0, "host2": 0}
70         return weights.get(host_state.host, 0)
71 
72 
73 class ServersTestBase(integrated_helpers._IntegratedTestBase):
74     api_major_version = 'v2'
75     _force_delete_parameter = 'forceDelete'
76     _image_ref_parameter = 'imageRef'
77     _flavor_ref_parameter = 'flavorRef'
78     _access_ipv4_parameter = 'accessIPv4'
79     _access_ipv6_parameter = 'accessIPv6'
80     _return_resv_id_parameter = 'return_reservation_id'
81     _min_count_parameter = 'min_count'
82 
83     USE_NEUTRON = True
84 
85     def setUp(self):
86         self.computes = {}
87         super(ServersTestBase, self).setUp()
88         self.conductor = self.start_service(
89             'conductor', manager='nova.conductor.manager.ConductorManager')
90 
91     def _wait_for_state_change(self, server, from_status):
92         for i in range(0, 50):
93             server = self.api.get_server(server['id'])
94             if server['status'] != from_status:
95                 break
96             time.sleep(.1)
97 
98         return server
99 
100     def _wait_for_deletion(self, server_id):
101         # Wait (briefly) for deletion
102         for _retries in range(50):
103             try:
104                 found_server = self.api.get_server(server_id)
105             except client.OpenStackApiNotFoundException:
106                 found_server = None
107                 LOG.debug("Got 404, proceeding")
108                 break
109 
110             LOG.debug("Found_server=%s", found_server)
111 
112             # TODO(justinsb): Mock doesn't yet do accurate state changes
113             # if found_server['status'] != 'deleting':
114             #    break
115             time.sleep(.1)
116 
117         # Should be gone
118         self.assertFalse(found_server)
119 
120     def _delete_server(self, server_id):
121         # Delete the server
122         self.api.delete_server(server_id)
123         self._wait_for_deletion(server_id)
124 
125     def _get_access_ips_params(self):
126         return {self._access_ipv4_parameter: "172.19.0.2",
127                 self._access_ipv6_parameter: "fe80::2"}
128 
129     def _verify_access_ips(self, server):
130         self.assertEqual('172.19.0.2',
131                          server[self._access_ipv4_parameter])
132         self.assertEqual('fe80::2', server[self._access_ipv6_parameter])
133 
134 
135 class ServersTest(ServersTestBase):
136 
137     def test_get_servers(self):
138         # Simple check that listing servers works.
139         servers = self.api.get_servers()
140         for server in servers:
141             LOG.debug("server: %s", server)
142 
143     def _get_node_build_failures(self):
144         ctxt = context.get_admin_context()
145         computes = objects.ComputeNodeList.get_all(ctxt)
146         return {
147             node.hypervisor_hostname: int(node.stats.get('failed_builds', 0))
148             for node in computes}
149 
150     def _run_periodics(self):
151         """Run the update_available_resource task on every compute manager
152 
153         This runs periodics on the computes in an undefined order; some child
154         class redefined this function to force a specific order.
155         """
156 
157         if self.compute.host not in self.computes:
158             self.computes[self.compute.host] = self.compute
159 
160         ctx = context.get_admin_context()
161         for compute in self.computes.values():
162             LOG.info('Running periodic for compute (%s)',
163                 compute.manager.host)
164             compute.manager.update_available_resource(ctx)
165         LOG.info('Finished with periodics')
166 
167     def test_create_server_with_error(self):
168         # Create a server which will enter error state.
169 
170         def throw_error(*args, **kwargs):
171             raise exception.BuildAbortException(reason='',
172                     instance_uuid='fake')
173 
174         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
175 
176         server = self._build_minimal_create_server_request()
177         created_server = self.api.post_server({"server": server})
178         created_server_id = created_server['id']
179 
180         found_server = self.api.get_server(created_server_id)
181         self.assertEqual(created_server_id, found_server['id'])
182 
183         found_server = self._wait_for_state_change(found_server, 'BUILD')
184 
185         self.assertEqual('ERROR', found_server['status'])
186         self._delete_server(created_server_id)
187 
188         # We should have no (persisted) build failures until we update
189         # resources, after which we should have one
190         self.assertEqual([0], list(self._get_node_build_failures().values()))
191         self._run_periodics()
192         self.assertEqual([1], list(self._get_node_build_failures().values()))
193 
194     def _test_create_server_with_error_with_retries(self):
195         # Create a server which will enter error state.
196 
197         fake.set_nodes(['host2'])
198         self.addCleanup(fake.restore_nodes)
199         self.flags(host='host2')
200         self.compute2 = self.start_service('compute', host='host2')
201         self.computes['compute2'] = self.compute2
202 
203         fails = []
204 
205         def throw_error(*args, **kwargs):
206             fails.append('one')
207             raise test.TestingException('Please retry me')
208 
209         self.stub_out('nova.virt.fake.FakeDriver.spawn', throw_error)
210 
211         server = self._build_minimal_create_server_request()
212         created_server = self.api.post_server({"server": server})
213         created_server_id = created_server['id']
214 
215         found_server = self.api.get_server(created_server_id)
216         self.assertEqual(created_server_id, found_server['id'])
217 
218         found_server = self._wait_for_state_change(found_server, 'BUILD')
219 
220         self.assertEqual('ERROR', found_server['status'])
221         self._delete_server(created_server_id)
222 
223         return len(fails)
224 
225     def test_create_server_with_error_with_retries(self):
226         self.flags(max_attempts=2, group='scheduler')
227         fails = self._test_create_server_with_error_with_retries()
228         self.assertEqual(2, fails)
229         self._run_periodics()
230         self.assertEqual(
231             [1, 1], list(self._get_node_build_failures().values()))
232 
233     def test_create_server_with_error_with_no_retries(self):
234         self.flags(max_attempts=1, group='scheduler')
235         fails = self._test_create_server_with_error_with_retries()
236         self.assertEqual(1, fails)
237         self._run_periodics()
238         self.assertEqual(
239             [0, 1], list(sorted(self._get_node_build_failures().values())))
240 
241     def test_create_and_delete_server(self):
242         # Creates and deletes a server.
243 
244         # Create server
245         # Build the server data gradually, checking errors along the way
246         server = {}
247         good_server = self._build_minimal_create_server_request()
248 
249         post = {'server': server}
250 
251         # Without an imageRef, this throws 500.
252         # TODO(justinsb): Check whatever the spec says should be thrown here
253         self.assertRaises(client.OpenStackApiException,
254                           self.api.post_server, post)
255 
256         # With an invalid imageRef, this throws 500.
257         server[self._image_ref_parameter] = self.get_invalid_image()
258         # TODO(justinsb): Check whatever the spec says should be thrown here
259         self.assertRaises(client.OpenStackApiException,
260                           self.api.post_server, post)
261 
262         # Add a valid imageRef
263         server[self._image_ref_parameter] = good_server.get(
264             self._image_ref_parameter)
265 
266         # Without flavorRef, this throws 500
267         # TODO(justinsb): Check whatever the spec says should be thrown here
268         self.assertRaises(client.OpenStackApiException,
269                           self.api.post_server, post)
270 
271         server[self._flavor_ref_parameter] = good_server.get(
272             self._flavor_ref_parameter)
273 
274         # Without a name, this throws 500
275         # TODO(justinsb): Check whatever the spec says should be thrown here
276         self.assertRaises(client.OpenStackApiException,
277                           self.api.post_server, post)
278 
279         # Set a valid server name
280         server['name'] = good_server['name']
281 
282         created_server = self.api.post_server(post)
283         LOG.debug("created_server: %s", created_server)
284         self.assertTrue(created_server['id'])
285         created_server_id = created_server['id']
286 
287         # Check it's there
288         found_server = self.api.get_server(created_server_id)
289         self.assertEqual(created_server_id, found_server['id'])
290 
291         # It should also be in the all-servers list
292         servers = self.api.get_servers()
293         server_ids = [s['id'] for s in servers]
294         self.assertIn(created_server_id, server_ids)
295 
296         found_server = self._wait_for_state_change(found_server, 'BUILD')
297         # It should be available...
298         # TODO(justinsb): Mock doesn't yet do this...
299         self.assertEqual('ACTIVE', found_server['status'])
300         servers = self.api.get_servers(detail=True)
301         for server in servers:
302             self.assertIn("image", server)
303             self.assertIn("flavor", server)
304 
305         self._delete_server(created_server_id)
306 
307     def _force_reclaim(self):
308         # Make sure that compute manager thinks the instance is
309         # old enough to be expired
310         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
311         timeutils.set_time_override(override_time=the_past)
312         self.addCleanup(timeutils.clear_time_override)
313         ctxt = context.get_admin_context()
314         self.compute._reclaim_queued_deletes(ctxt)
315 
316     def test_deferred_delete(self):
317         # Creates, deletes and waits for server to be reclaimed.
318         self.flags(reclaim_instance_interval=1)
319 
320         # Create server
321         server = self._build_minimal_create_server_request()
322 
323         created_server = self.api.post_server({'server': server})
324         LOG.debug("created_server: %s", created_server)
325         self.assertTrue(created_server['id'])
326         created_server_id = created_server['id']
327 
328         # Wait for it to finish being created
329         found_server = self._wait_for_state_change(created_server, 'BUILD')
330 
331         # It should be available...
332         self.assertEqual('ACTIVE', found_server['status'])
333 
334         # Cannot restore unless instance is deleted
335         self.assertRaises(client.OpenStackApiException,
336                           self.api.post_server_action, created_server_id,
337                           {'restore': {}})
338 
339         # Delete the server
340         self.api.delete_server(created_server_id)
341 
342         # Wait for queued deletion
343         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
344         self.assertEqual('SOFT_DELETED', found_server['status'])
345 
346         self._force_reclaim()
347 
348         # Wait for real deletion
349         self._wait_for_deletion(created_server_id)
350 
351     def test_deferred_delete_restore(self):
352         # Creates, deletes and restores a server.
353         self.flags(reclaim_instance_interval=3600)
354 
355         # Create server
356         server = self._build_minimal_create_server_request()
357 
358         created_server = self.api.post_server({'server': server})
359         LOG.debug("created_server: %s", created_server)
360         self.assertTrue(created_server['id'])
361         created_server_id = created_server['id']
362 
363         # Wait for it to finish being created
364         found_server = self._wait_for_state_change(created_server, 'BUILD')
365 
366         # It should be available...
367         self.assertEqual('ACTIVE', found_server['status'])
368 
369         # Delete the server
370         self.api.delete_server(created_server_id)
371 
372         # Wait for queued deletion
373         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
374         self.assertEqual('SOFT_DELETED', found_server['status'])
375 
376         # Restore server
377         self.api.post_server_action(created_server_id, {'restore': {}})
378 
379         # Wait for server to become active again
380         found_server = self._wait_for_state_change(found_server, 'DELETED')
381         self.assertEqual('ACTIVE', found_server['status'])
382 
383     def test_deferred_delete_restore_overquota(self):
384         # Test that a restore that would put the user over quota fails
385         self.flags(instances=1, group='quota')
386         # Creates, deletes and restores a server.
387         self.flags(reclaim_instance_interval=3600)
388 
389         # Create server
390         server = self._build_minimal_create_server_request()
391 
392         created_server1 = self.api.post_server({'server': server})
393         LOG.debug("created_server: %s", created_server1)
394         self.assertTrue(created_server1['id'])
395         created_server_id1 = created_server1['id']
396 
397         # Wait for it to finish being created
398         found_server1 = self._wait_for_state_change(created_server1, 'BUILD')
399 
400         # It should be available...
401         self.assertEqual('ACTIVE', found_server1['status'])
402 
403         # Delete the server
404         self.api.delete_server(created_server_id1)
405 
406         # Wait for queued deletion
407         found_server1 = self._wait_for_state_change(found_server1, 'ACTIVE')
408         self.assertEqual('SOFT_DELETED', found_server1['status'])
409 
410         # Create a second server
411         server = self._build_minimal_create_server_request()
412 
413         created_server2 = self.api.post_server({'server': server})
414         LOG.debug("created_server: %s", created_server2)
415         self.assertTrue(created_server2['id'])
416 
417         # Wait for it to finish being created
418         found_server2 = self._wait_for_state_change(created_server2, 'BUILD')
419 
420         # It should be available...
421         self.assertEqual('ACTIVE', found_server2['status'])
422 
423         # Try to restore the first server, it should fail
424         ex = self.assertRaises(client.OpenStackApiException,
425                                self.api.post_server_action,
426                                created_server_id1, {'restore': {}})
427         self.assertEqual(403, ex.response.status_code)
428         self.assertEqual('SOFT_DELETED', found_server1['status'])
429 
430     def test_deferred_delete_force(self):
431         # Creates, deletes and force deletes a server.
432         self.flags(reclaim_instance_interval=3600)
433 
434         # Create server
435         server = self._build_minimal_create_server_request()
436 
437         created_server = self.api.post_server({'server': server})
438         LOG.debug("created_server: %s", created_server)
439         self.assertTrue(created_server['id'])
440         created_server_id = created_server['id']
441 
442         # Wait for it to finish being created
443         found_server = self._wait_for_state_change(created_server, 'BUILD')
444 
445         # It should be available...
446         self.assertEqual('ACTIVE', found_server['status'])
447 
448         # Delete the server
449         self.api.delete_server(created_server_id)
450 
451         # Wait for queued deletion
452         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
453         self.assertEqual('SOFT_DELETED', found_server['status'])
454 
455         # Force delete server
456         self.api.post_server_action(created_server_id,
457                                     {self._force_delete_parameter: {}})
458 
459         # Wait for real deletion
460         self._wait_for_deletion(created_server_id)
461 
462     def test_create_server_with_metadata(self):
463         # Creates a server with metadata.
464 
465         # Build the server data gradually, checking errors along the way
466         server = self._build_minimal_create_server_request()
467 
468         metadata = {}
469         for i in range(30):
470             metadata['key_%s' % i] = 'value_%s' % i
471 
472         server['metadata'] = metadata
473 
474         post = {'server': server}
475         created_server = self.api.post_server(post)
476         LOG.debug("created_server: %s", created_server)
477         self.assertTrue(created_server['id'])
478         created_server_id = created_server['id']
479 
480         found_server = self.api.get_server(created_server_id)
481         self.assertEqual(created_server_id, found_server['id'])
482         self.assertEqual(metadata, found_server.get('metadata'))
483 
484         # The server should also be in the all-servers details list
485         servers = self.api.get_servers(detail=True)
486         server_map = {server['id']: server for server in servers}
487         found_server = server_map.get(created_server_id)
488         self.assertTrue(found_server)
489         # Details do include metadata
490         self.assertEqual(metadata, found_server.get('metadata'))
491 
492         # The server should also be in the all-servers summary list
493         servers = self.api.get_servers(detail=False)
494         server_map = {server['id']: server for server in servers}
495         found_server = server_map.get(created_server_id)
496         self.assertTrue(found_server)
497         # Summary should not include metadata
498         self.assertFalse(found_server.get('metadata'))
499 
500         # Cleanup
501         self._delete_server(created_server_id)
502 
503     def test_server_metadata_actions_negative_invalid_state(self):
504         # Create server with metadata
505         server = self._build_minimal_create_server_request()
506 
507         metadata = {'key_1': 'value_1'}
508 
509         server['metadata'] = metadata
510 
511         post = {'server': server}
512         created_server = self.api.post_server(post)
513 
514         found_server = self._wait_for_state_change(created_server, 'BUILD')
515         self.assertEqual('ACTIVE', found_server['status'])
516         self.assertEqual(metadata, found_server.get('metadata'))
517         server_id = found_server['id']
518 
519         # Change status from ACTIVE to SHELVED for negative test
520         self.flags(shelved_offload_time = -1)
521         self.api.post_server_action(server_id, {'shelve': {}})
522         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
523         self.assertEqual('SHELVED', found_server['status'])
524 
525         metadata = {'key_2': 'value_2'}
526 
527         # Update Metadata item in SHELVED (not ACTIVE, etc.)
528         ex = self.assertRaises(client.OpenStackApiException,
529                                self.api.post_server_metadata,
530                                server_id, metadata)
531         self.assertEqual(409, ex.response.status_code)
532         self.assertEqual('SHELVED', found_server['status'])
533 
534         # Delete Metadata item in SHELVED (not ACTIVE, etc.)
535         ex = self.assertRaises(client.OpenStackApiException,
536                                self.api.delete_server_metadata,
537                                server_id, 'key_1')
538         self.assertEqual(409, ex.response.status_code)
539         self.assertEqual('SHELVED', found_server['status'])
540 
541         # Cleanup
542         self._delete_server(server_id)
543 
544     def test_create_and_rebuild_server(self):
545         # Rebuild a server with metadata.
546 
547         # create a server with initially has no metadata
548         server = self._build_minimal_create_server_request()
549         server_post = {'server': server}
550 
551         metadata = {}
552         for i in range(30):
553             metadata['key_%s' % i] = 'value_%s' % i
554 
555         server_post['server']['metadata'] = metadata
556 
557         created_server = self.api.post_server(server_post)
558         LOG.debug("created_server: %s", created_server)
559         self.assertTrue(created_server['id'])
560         created_server_id = created_server['id']
561 
562         created_server = self._wait_for_state_change(created_server, 'BUILD')
563 
564         # rebuild the server with metadata and other server attributes
565         post = {}
566         post['rebuild'] = {
567             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
568             "name": "blah",
569             self._access_ipv4_parameter: "172.19.0.2",
570             self._access_ipv6_parameter: "fe80::2",
571             "metadata": {'some': 'thing'},
572         }
573         post['rebuild'].update(self._get_access_ips_params())
574 
575         self.api.post_server_action(created_server_id, post)
576         LOG.debug("rebuilt server: %s", created_server)
577         self.assertTrue(created_server['id'])
578 
579         found_server = self.api.get_server(created_server_id)
580         self.assertEqual(created_server_id, found_server['id'])
581         self.assertEqual({'some': 'thing'}, found_server.get('metadata'))
582         self.assertEqual('blah', found_server.get('name'))
583         self.assertEqual(post['rebuild'][self._image_ref_parameter],
584                          found_server.get('image')['id'])
585         self._verify_access_ips(found_server)
586 
587         # rebuild the server with empty metadata and nothing else
588         post = {}
589         post['rebuild'] = {
590             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
591             "metadata": {},
592         }
593 
594         self.api.post_server_action(created_server_id, post)
595         LOG.debug("rebuilt server: %s", created_server)
596         self.assertTrue(created_server['id'])
597 
598         found_server = self.api.get_server(created_server_id)
599         self.assertEqual(created_server_id, found_server['id'])
600         self.assertEqual({}, found_server.get('metadata'))
601         self.assertEqual('blah', found_server.get('name'))
602         self.assertEqual(post['rebuild'][self._image_ref_parameter],
603                          found_server.get('image')['id'])
604         self._verify_access_ips(found_server)
605 
606         # Cleanup
607         self._delete_server(created_server_id)
608 
609     def test_rename_server(self):
610         # Test building and renaming a server.
611 
612         # Create a server
613         server = self._build_minimal_create_server_request()
614         created_server = self.api.post_server({'server': server})
615         LOG.debug("created_server: %s", created_server)
616         server_id = created_server['id']
617         self.assertTrue(server_id)
618 
619         # Rename the server to 'new-name'
620         self.api.put_server(server_id, {'server': {'name': 'new-name'}})
621 
622         # Check the name of the server
623         created_server = self.api.get_server(server_id)
624         self.assertEqual(created_server['name'], 'new-name')
625 
626         # Cleanup
627         self._delete_server(server_id)
628 
629     def test_create_multiple_servers(self):
630         # Creates multiple servers and checks for reservation_id.
631 
632         # Create 2 servers, setting 'return_reservation_id, which should
633         # return a reservation_id
634         server = self._build_minimal_create_server_request()
635         server[self._min_count_parameter] = 2
636         server[self._return_resv_id_parameter] = True
637         post = {'server': server}
638         response = self.api.post_server(post)
639         self.assertIn('reservation_id', response)
640         reservation_id = response['reservation_id']
641         self.assertNotIn(reservation_id, ['', None])
642         # Assert that the reservation_id itself has the expected format
643         self.assertRegex(reservation_id, 'r-[0-9a-zA-Z]{8}')
644 
645         # Create 1 more server, which should not return a reservation_id
646         server = self._build_minimal_create_server_request()
647         post = {'server': server}
648         created_server = self.api.post_server(post)
649         self.assertTrue(created_server['id'])
650         created_server_id = created_server['id']
651 
652         # lookup servers created by the first request.
653         servers = self.api.get_servers(detail=True,
654                 search_opts={'reservation_id': reservation_id})
655         server_map = {server['id']: server for server in servers}
656         found_server = server_map.get(created_server_id)
657         # The server from the 2nd request should not be there.
658         self.assertIsNone(found_server)
659         # Should have found 2 servers.
660         self.assertEqual(len(server_map), 2)
661 
662         # Cleanup
663         self._delete_server(created_server_id)
664         for server_id in server_map:
665             self._delete_server(server_id)
666 
667     def test_create_server_with_injected_files(self):
668         # Creates a server with injected_files.
669         personality = []
670 
671         # Inject a text file
672         data = 'Hello, World!'
673         personality.append({
674             'path': '/helloworld.txt',
675             'contents': base64.encode_as_bytes(data),
676         })
677 
678         # Inject a binary file
679         data = zlib.compress(b'Hello, World!')
680         personality.append({
681             'path': '/helloworld.zip',
682             'contents': base64.encode_as_bytes(data),
683         })
684 
685         # Create server
686         server = self._build_minimal_create_server_request()
687         server['personality'] = personality
688 
689         post = {'server': server}
690 
691         created_server = self.api.post_server(post)
692         LOG.debug("created_server: %s", created_server)
693         self.assertTrue(created_server['id'])
694         created_server_id = created_server['id']
695 
696         # Check it's there
697         found_server = self.api.get_server(created_server_id)
698         self.assertEqual(created_server_id, found_server['id'])
699 
700         found_server = self._wait_for_state_change(found_server, 'BUILD')
701         self.assertEqual('ACTIVE', found_server['status'])
702 
703         # Cleanup
704         self._delete_server(created_server_id)
705 
706     def test_stop_start_servers_negative_invalid_state(self):
707         # Create server
708         server = self._build_minimal_create_server_request()
709         created_server = self.api.post_server({"server": server})
710         created_server_id = created_server['id']
711 
712         found_server = self._wait_for_state_change(created_server, 'BUILD')
713         self.assertEqual('ACTIVE', found_server['status'])
714 
715         # Start server in ACTIVE
716         # NOTE(mkoshiya): When os-start API runs, the server status
717         # must be SHUTOFF.
718         # By returning 409, I want to confirm that the ACTIVE server does not
719         # cause unexpected behavior.
720         post = {'os-start': {}}
721         ex = self.assertRaises(client.OpenStackApiException,
722                                self.api.post_server_action,
723                                created_server_id, post)
724         self.assertEqual(409, ex.response.status_code)
725         self.assertEqual('ACTIVE', found_server['status'])
726 
727         # Stop server
728         post = {'os-stop': {}}
729         self.api.post_server_action(created_server_id, post)
730         found_server = self._wait_for_state_change(found_server, 'ACTIVE')
731         self.assertEqual('SHUTOFF', found_server['status'])
732 
733         # Stop server in SHUTOFF
734         # NOTE(mkoshiya): When os-stop API runs, the server status
735         # must be ACTIVE or ERROR.
736         # By returning 409, I want to confirm that the SHUTOFF server does not
737         # cause unexpected behavior.
738         post = {'os-stop': {}}
739         ex = self.assertRaises(client.OpenStackApiException,
740                                self.api.post_server_action,
741                                created_server_id, post)
742         self.assertEqual(409, ex.response.status_code)
743         self.assertEqual('SHUTOFF', found_server['status'])
744 
745         # Cleanup
746         self._delete_server(created_server_id)
747 
748     def test_revert_resized_server_negative_invalid_state(self):
749         # Create server
750         server = self._build_minimal_create_server_request()
751         created_server = self.api.post_server({"server": server})
752         created_server_id = created_server['id']
753         found_server = self._wait_for_state_change(created_server, 'BUILD')
754         self.assertEqual('ACTIVE', found_server['status'])
755 
756         # Revert resized server in ACTIVE
757         # NOTE(yatsumi): When revert resized server API runs,
758         # the server status must be VERIFY_RESIZE.
759         # By returning 409, I want to confirm that the ACTIVE server does not
760         # cause unexpected behavior.
761         post = {'revertResize': {}}
762         ex = self.assertRaises(client.OpenStackApiException,
763                                self.api.post_server_action,
764                                created_server_id, post)
765         self.assertEqual(409, ex.response.status_code)
766         self.assertEqual('ACTIVE', found_server['status'])
767 
768         # Cleanup
769         self._delete_server(created_server_id)
770 
771     def test_resize_server_negative_invalid_state(self):
772         # Avoid migration
773         self.flags(allow_resize_to_same_host=True)
774 
775         # Create server
776         server = self._build_minimal_create_server_request()
777         created_server = self.api.post_server({"server": server})
778         created_server_id = created_server['id']
779         found_server = self._wait_for_state_change(created_server, 'BUILD')
780         self.assertEqual('ACTIVE', found_server['status'])
781 
782         # Resize server(flavorRef: 1 -> 2)
783         post = {'resize': {"flavorRef": "2", "OS-DCF:diskConfig": "AUTO"}}
784         self.api.post_server_action(created_server_id, post)
785         found_server = self._wait_for_state_change(found_server, 'RESIZE')
786         self.assertEqual('VERIFY_RESIZE', found_server['status'])
787 
788         # Resize server in VERIFY_RESIZE(flavorRef: 2 -> 1)
789         # NOTE(yatsumi): When resize API runs, the server status
790         # must be ACTIVE or SHUTOFF.
791         # By returning 409, I want to confirm that the VERIFY_RESIZE server
792         # does not cause unexpected behavior.
793         post = {'resize': {"flavorRef": "1", "OS-DCF:diskConfig": "AUTO"}}
794         ex = self.assertRaises(client.OpenStackApiException,
795                                self.api.post_server_action,
796                                created_server_id, post)
797         self.assertEqual(409, ex.response.status_code)
798         self.assertEqual('VERIFY_RESIZE', found_server['status'])
799 
800         # Cleanup
801         self._delete_server(created_server_id)
802 
803     def test_confirm_resized_server_negative_invalid_state(self):
804         # Create server
805         server = self._build_minimal_create_server_request()
806         created_server = self.api.post_server({"server": server})
807         created_server_id = created_server['id']
808         found_server = self._wait_for_state_change(created_server, 'BUILD')
809         self.assertEqual('ACTIVE', found_server['status'])
810 
811         # Confirm resized server in ACTIVE
812         # NOTE(yatsumi): When confirm resized server API runs,
813         # the server status must be VERIFY_RESIZE.
814         # By returning 409, I want to confirm that the ACTIVE server does not
815         # cause unexpected behavior.
816         post = {'confirmResize': {}}
817         ex = self.assertRaises(client.OpenStackApiException,
818                                self.api.post_server_action,
819                                created_server_id, post)
820         self.assertEqual(409, ex.response.status_code)
821         self.assertEqual('ACTIVE', found_server['status'])
822 
823         # Cleanup
824         self._delete_server(created_server_id)
825 
826     def test_resize_server_overquota(self):
827         self.flags(cores=1, group='quota')
828         self.flags(ram=512, group='quota')
829         # Create server with default flavor, 1 core, 512 ram
830         server = self._build_minimal_create_server_request()
831         created_server = self.api.post_server({"server": server})
832         created_server_id = created_server['id']
833 
834         found_server = self._wait_for_state_change(created_server, 'BUILD')
835         self.assertEqual('ACTIVE', found_server['status'])
836 
837         # Try to resize to flavorid 2, 1 core, 2048 ram
838         post = {'resize': {'flavorRef': '2'}}
839         ex = self.assertRaises(client.OpenStackApiException,
840                                self.api.post_server_action,
841                                created_server_id, post)
842         self.assertEqual(403, ex.response.status_code)
843 
844     def test_attach_vol_maximum_disk_devices_exceeded(self):
845         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
846 
847         server = self._build_minimal_create_server_request()
848         created_server = self.api.post_server({"server": server})
849         server_id = created_server['id']
850         self._wait_for_state_change(created_server, 'BUILD')
851 
852         volume_id = '9a695496-44aa-4404-b2cc-ccab2501f87e'
853         LOG.info('Attaching volume %s to server %s', volume_id, server_id)
854 
855         # The fake driver doesn't implement get_device_name_for_instance, so
856         # we'll just raise the exception directly here, instead of simuluating
857         # an instance with 26 disk devices already attached.
858         with mock.patch.object(self.compute.driver,
859                                'get_device_name_for_instance') as mock_get:
860             mock_get.side_effect = exception.TooManyDiskDevices(maximum=26)
861             ex = self.assertRaises(
862                 client.OpenStackApiException, self.api.post_server_volume,
863                 server_id, dict(volumeAttachment=dict(volumeId=volume_id)))
864             expected = ('The maximum allowed number of disk devices (26) to '
865                         'attach to a single instance has been exceeded.')
866             self.assertEqual(403, ex.response.status_code)
867             self.assertIn(expected, six.text_type(ex))
868 
869 
870 class ServersTestV21(ServersTest):
871     api_major_version = 'v2.1'
872 
873 
874 class ServersTestV219(ServersTestBase):
875     api_major_version = 'v2.1'
876 
877     def _create_server(self, set_desc = True, desc = None):
878         server = self._build_minimal_create_server_request()
879         if set_desc:
880             server['description'] = desc
881         post = {'server': server}
882         response = self.api.api_post('/servers', post).body
883         return (server, response['server'])
884 
885     def _update_server(self, server_id, set_desc = True, desc = None):
886         new_name = integrated_helpers.generate_random_alphanumeric(8)
887         server = {'server': {'name': new_name}}
888         if set_desc:
889             server['server']['description'] = desc
890         self.api.api_put('/servers/%s' % server_id, server)
891 
892     def _rebuild_server(self, server_id, set_desc = True, desc = None):
893         new_name = integrated_helpers.generate_random_alphanumeric(8)
894         post = {}
895         post['rebuild'] = {
896             "name": new_name,
897             self._image_ref_parameter: "76fa36fc-c930-4bf3-8c8a-ea2a2420deb6",
898             self._access_ipv4_parameter: "172.19.0.2",
899             self._access_ipv6_parameter: "fe80::2",
900             "metadata": {'some': 'thing'},
901         }
902         post['rebuild'].update(self._get_access_ips_params())
903         if set_desc:
904             post['rebuild']['description'] = desc
905         self.api.api_post('/servers/%s/action' % server_id, post)
906 
907     def _create_server_and_verify(self, set_desc = True, expected_desc = None):
908         # Creates a server with a description and verifies it is
909         # in the GET responses.
910         created_server_id = self._create_server(set_desc,
911                                                 expected_desc)[1]['id']
912         self._verify_server_description(created_server_id, expected_desc)
913         self._delete_server(created_server_id)
914 
915     def _update_server_and_verify(self, server_id, set_desc = True,
916                                   expected_desc = None):
917         # Updates a server with a description and verifies it is
918         # in the GET responses.
919         self._update_server(server_id, set_desc, expected_desc)
920         self._verify_server_description(server_id, expected_desc)
921 
922     def _rebuild_server_and_verify(self, server_id, set_desc = True,
923                                   expected_desc = None):
924         # Rebuilds a server with a description and verifies it is
925         # in the GET responses.
926         self._rebuild_server(server_id, set_desc, expected_desc)
927         self._verify_server_description(server_id, expected_desc)
928 
929     def _verify_server_description(self, server_id, expected_desc = None,
930                                    desc_in_resp = True):
931         # Calls GET on the servers and verifies that the description
932         # is set as expected in the response, or not set at all.
933         response = self.api.api_get('/servers/%s' % server_id)
934         found_server = response.body['server']
935         self.assertEqual(server_id, found_server['id'])
936         if desc_in_resp:
937             # Verify the description is set as expected (can be None)
938             self.assertEqual(expected_desc, found_server.get('description'))
939         else:
940             # Verify the description is not included in the response.
941             self.assertNotIn('description', found_server)
942 
943         servers = self.api.api_get('/servers/detail').body['servers']
944         server_map = {server['id']: server for server in servers}
945         found_server = server_map.get(server_id)
946         self.assertTrue(found_server)
947         if desc_in_resp:
948             # Verify the description is set as expected (can be None)
949             self.assertEqual(expected_desc, found_server.get('description'))
950         else:
951             # Verify the description is not included in the response.
952             self.assertNotIn('description', found_server)
953 
954     def _create_assertRaisesRegex(self, desc):
955         # Verifies that a 400 error is thrown on create server
956         with self.assertRaisesRegex(client.OpenStackApiException,
957                                     ".*Unexpected status code.*") as cm:
958             self._create_server(True, desc)
959             self.assertEqual(400, cm.exception.response.status_code)
960 
961     def _update_assertRaisesRegex(self, server_id, desc):
962         # Verifies that a 400 error is thrown on update server
963         with self.assertRaisesRegex(client.OpenStackApiException,
964                                     ".*Unexpected status code.*") as cm:
965             self._update_server(server_id, True, desc)
966             self.assertEqual(400, cm.exception.response.status_code)
967 
968     def _rebuild_assertRaisesRegex(self, server_id, desc):
969         # Verifies that a 400 error is thrown on rebuild server
970         with self.assertRaisesRegex(client.OpenStackApiException,
971                                     ".*Unexpected status code.*") as cm:
972             self._rebuild_server(server_id, True, desc)
973             self.assertEqual(400, cm.exception.response.status_code)
974 
975     def test_create_server_with_description(self):
976         self.api.microversion = '2.19'
977         # Create and get a server with a description
978         self._create_server_and_verify(True, 'test description')
979         # Create and get a server with an empty description
980         self._create_server_and_verify(True, '')
981         # Create and get a server with description set to None
982         self._create_server_and_verify()
983         # Create and get a server without setting the description
984         self._create_server_and_verify(False)
985 
986     def test_update_server_with_description(self):
987         self.api.microversion = '2.19'
988         # Create a server with an initial description
989         server_id = self._create_server(True, 'test desc 1')[1]['id']
990 
991         # Update and get the server with a description
992         self._update_server_and_verify(server_id, True, 'updated desc')
993         # Update and get the server name without changing the description
994         self._update_server_and_verify(server_id, False, 'updated desc')
995         # Update and get the server with an empty description
996         self._update_server_and_verify(server_id, True, '')
997         # Update and get the server by removing the description (set to None)
998         self._update_server_and_verify(server_id)
999         # Update and get the server with a 2nd new description
1000         self._update_server_and_verify(server_id, True, 'updated desc2')
1001 
1002         # Cleanup
1003         self._delete_server(server_id)
1004 
1005     def test_rebuild_server_with_description(self):
1006         self.api.microversion = '2.19'
1007 
1008         # Create a server with an initial description
1009         server = self._create_server(True, 'test desc 1')[1]
1010         server_id = server['id']
1011         self._wait_for_state_change(server, 'BUILD')
1012 
1013         # Rebuild and get the server with a description
1014         self._rebuild_server_and_verify(server_id, True, 'updated desc')
1015         # Rebuild and get the server name without changing the description
1016         self._rebuild_server_and_verify(server_id, False, 'updated desc')
1017         # Rebuild and get the server with an empty description
1018         self._rebuild_server_and_verify(server_id, True, '')
1019         # Rebuild and get the server by removing the description (set to None)
1020         self._rebuild_server_and_verify(server_id)
1021         # Rebuild and get the server with a 2nd new description
1022         self._rebuild_server_and_verify(server_id, True, 'updated desc2')
1023 
1024         # Cleanup
1025         self._delete_server(server_id)
1026 
1027     def test_version_compatibility(self):
1028         # Create a server with microversion v2.19 and a description.
1029         self.api.microversion = '2.19'
1030         server_id = self._create_server(True, 'test desc 1')[1]['id']
1031         # Verify that the description is not included on V2.18 GETs
1032         self.api.microversion = '2.18'
1033         self._verify_server_description(server_id, desc_in_resp = False)
1034         # Verify that updating the server with description on V2.18
1035         # results in a 400 error
1036         self._update_assertRaisesRegex(server_id, 'test update 2.18')
1037         # Verify that rebuilding the server with description on V2.18
1038         # results in a 400 error
1039         self._rebuild_assertRaisesRegex(server_id, 'test rebuild 2.18')
1040 
1041         # Cleanup
1042         self._delete_server(server_id)
1043 
1044         # Create a server on V2.18 and verify that the description
1045         # defaults to the name on a V2.19 GET
1046         server_req, response = self._create_server(False)
1047         server_id = response['id']
1048         self.api.microversion = '2.19'
1049         self._verify_server_description(server_id, server_req['name'])
1050 
1051         # Cleanup
1052         self._delete_server(server_id)
1053 
1054         # Verify that creating a server with description on V2.18
1055         # results in a 400 error
1056         self.api.microversion = '2.18'
1057         self._create_assertRaisesRegex('test create 2.18')
1058 
1059     def test_description_errors(self):
1060         self.api.microversion = '2.19'
1061         # Create servers with invalid descriptions.  These throw 400.
1062         # Invalid unicode with non-printable control char
1063         self._create_assertRaisesRegex(u'invalid\0dstring')
1064         # Description is longer than 255 chars
1065         self._create_assertRaisesRegex('x' * 256)
1066 
1067         # Update and rebuild servers with invalid descriptions.
1068         # These throw 400.
1069         server_id = self._create_server(True, "desc")[1]['id']
1070         # Invalid unicode with non-printable control char
1071         self._update_assertRaisesRegex(server_id, u'invalid\u0604string')
1072         self._rebuild_assertRaisesRegex(server_id, u'invalid\u0604string')
1073         # Description is longer than 255 chars
1074         self._update_assertRaisesRegex(server_id, 'x' * 256)
1075         self._rebuild_assertRaisesRegex(server_id, 'x' * 256)
1076 
1077 
1078 class ServerTestV220(ServersTestBase):
1079     api_major_version = 'v2.1'
1080 
1081     def setUp(self):
1082         super(ServerTestV220, self).setUp()
1083         self.api.microversion = '2.20'
1084         self.ctxt = context.get_admin_context()
1085 
1086     def _create_server(self):
1087         server = self._build_minimal_create_server_request()
1088         post = {'server': server}
1089         response = self.api.api_post('/servers', post).body
1090         return (server, response['server'])
1091 
1092     def _shelve_server(self):
1093         server = self._create_server()[1]
1094         server_id = server['id']
1095         self._wait_for_state_change(server, 'BUILD')
1096         self.api.post_server_action(server_id, {'shelve': None})
1097         return self._wait_for_state_change(server, 'ACTIVE')
1098 
1099     def _get_fake_bdms(self, ctxt):
1100         return block_device_obj.block_device_make_list(self.ctxt,
1101                     [fake_block_device.FakeDbBlockDeviceDict(
1102                     {'device_name': '/dev/vda',
1103                      'source_type': 'volume',
1104                      'destination_type': 'volume',
1105                      'volume_id': '5d721593-f033-4f6d-ab6f-b5b067e61bc4'})])
1106 
1107     def test_attach_detach_vol_to_shelved_server(self):
1108         self.flags(shelved_offload_time=-1)
1109         found_server = self._shelve_server()
1110         self.assertEqual('SHELVED', found_server['status'])
1111         server_id = found_server['id']
1112 
1113         # Test attach volume
1114         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1115         with test.nested(mock.patch.object(volume.cinder,
1116                                        'is_microversion_supported'),
1117                          mock.patch.object(compute_api.API,
1118                                        '_check_attach_and_reserve_volume'),
1119                          mock.patch.object(rpcapi.ComputeAPI,
1120                                        'attach_volume')) as (mock_cinder_mv,
1121                                                              mock_reserve,
1122                                                              mock_attach):
1123             mock_cinder_mv.side_effect = \
1124                 exception.CinderAPIVersionNotAvailable(version='3.44')
1125             volume_attachment = {"volumeAttachment": {"volumeId":
1126                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1127             self.api.api_post(
1128                             '/servers/%s/os-volume_attachments' % (server_id),
1129                             volume_attachment)
1130             self.assertTrue(mock_reserve.called)
1131             self.assertTrue(mock_attach.called)
1132 
1133         # Test detach volume
1134         with test.nested(mock.patch.object(volume.cinder.API,
1135                                            'begin_detaching'),
1136                          mock.patch.object(objects.BlockDeviceMappingList,
1137                                            'get_by_instance_uuid'),
1138                          mock.patch.object(rpcapi.ComputeAPI,
1139                                            'detach_volume')
1140                          ) as (mock_check, mock_get_bdms, mock_rpc):
1141 
1142             mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)
1143             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1144 
1145             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1146                             (server_id, attachment_id))
1147             self.assertTrue(mock_check.called)
1148             self.assertTrue(mock_rpc.called)
1149 
1150         self._delete_server(server_id)
1151 
1152     def test_attach_detach_vol_to_shelved_offloaded_server(self):
1153         self.flags(shelved_offload_time=0)
1154         found_server = self._shelve_server()
1155         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1156         server_id = found_server['id']
1157 
1158         # Test attach volume
1159         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1160         with test.nested(mock.patch.object(volume.cinder,
1161                                        'is_microversion_supported'),
1162                          mock.patch.object(compute_api.API,
1163                                        '_check_attach_and_reserve_volume'),
1164                          mock.patch.object(volume.cinder.API,
1165                                        'attach')) as (mock_cinder_mv,
1166                                                       mock_reserve, mock_vol):
1167             mock_cinder_mv.side_effect = \
1168                 exception.CinderAPIVersionNotAvailable(version='3.44')
1169             volume_attachment = {"volumeAttachment": {"volumeId":
1170                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1171             attach_response = self.api.api_post(
1172                              '/servers/%s/os-volume_attachments' % (server_id),
1173                              volume_attachment).body['volumeAttachment']
1174             self.assertTrue(mock_reserve.called)
1175             self.assertTrue(mock_vol.called)
1176             self.assertIsNone(attach_response['device'])
1177 
1178         # Test detach volume
1179         with test.nested(mock.patch.object(volume.cinder.API,
1180                                            'begin_detaching'),
1181                          mock.patch.object(objects.BlockDeviceMappingList,
1182                                            'get_by_instance_uuid'),
1183                          mock.patch.object(compute_api.API,
1184                                            '_local_cleanup_bdm_volumes')
1185                          ) as (mock_check, mock_get_bdms, mock_clean_vols):
1186 
1187             mock_get_bdms.return_value = self._get_fake_bdms(self.ctxt)
1188             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1189             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1190                             (server_id, attachment_id))
1191             self.assertTrue(mock_check.called)
1192             self.assertTrue(mock_clean_vols.called)
1193 
1194         self._delete_server(server_id)
1195 
1196     def test_attach_detach_vol_to_shelved_offloaded_server_new_flow(self):
1197         self.flags(shelved_offload_time=0)
1198         found_server = self._shelve_server()
1199         self.assertEqual('SHELVED_OFFLOADED', found_server['status'])
1200         server_id = found_server['id']
1201         fake_bdms = self._get_fake_bdms(self.ctxt)
1202 
1203         # Test attach volume
1204         self.stub_out('nova.volume.cinder.API.get', fakes.stub_volume_get)
1205         with test.nested(mock.patch.object(volume.cinder,
1206                                        'is_microversion_supported'),
1207                          mock.patch.object(compute_api.API,
1208                             '_check_volume_already_attached_to_instance'),
1209                          mock.patch.object(volume.cinder.API,
1210                                         'check_availability_zone'),
1211                          mock.patch.object(volume.cinder.API,
1212                                         'attachment_create'),
1213                          mock.patch.object(volume.cinder.API,
1214                                         'attachment_complete')
1215                          ) as (mock_cinder_mv, mock_check_vol_attached,
1216                                mock_check_av_zone, mock_attach_create,
1217                                mock_attachment_complete):
1218             mock_attach_create.return_value = {'id': uuids.volume}
1219             volume_attachment = {"volumeAttachment": {"volumeId":
1220                                        "5d721593-f033-4f6d-ab6f-b5b067e61bc4"}}
1221             attach_response = self.api.api_post(
1222                              '/servers/%s/os-volume_attachments' % (server_id),
1223                              volume_attachment).body['volumeAttachment']
1224             self.assertTrue(mock_attach_create.called)
1225             mock_attachment_complete.assert_called_once_with(
1226                 mock.ANY, uuids.volume)
1227             self.assertIsNone(attach_response['device'])
1228 
1229         # Test detach volume
1230         with test.nested(mock.patch.object(objects.BlockDeviceMappingList,
1231                                            'get_by_instance_uuid'),
1232                          mock.patch.object(compute_api.API,
1233                                            '_local_cleanup_bdm_volumes')
1234                          ) as (mock_get_bdms, mock_clean_vols):
1235 
1236             mock_get_bdms.return_value = fake_bdms
1237             attachment_id = mock_get_bdms.return_value[0]['volume_id']
1238             self.api.api_delete('/servers/%s/os-volume_attachments/%s' %
1239                             (server_id, attachment_id))
1240             self.assertTrue(mock_clean_vols.called)
1241 
1242         self._delete_server(server_id)
1243 
1244 
1245 class ServerTestV269(ServersTestBase):
1246     api_major_version = 'v2.1'
1247     NUMBER_OF_CELLS = 3
1248 
1249     def setUp(self):
1250         super(ServerTestV269, self).setUp()
1251         self.api.microversion = '2.69'
1252 
1253         self.ctxt = context.get_admin_context()
1254         self.project_id = self.api.project_id
1255         self.cells = objects.CellMappingList.get_all(self.ctxt)
1256         self.down_cell_insts = []
1257         self.up_cell_insts = []
1258         self.down_cell_mappings = objects.CellMappingList()
1259         flavor = objects.Flavor(id=1, name='flavor1',
1260                                 memory_mb=256, vcpus=1,
1261                                 root_gb=1, ephemeral_gb=1,
1262                                 flavorid='1',
1263                                 swap=0, rxtx_factor=1.0,
1264                                 vcpu_weight=1,
1265                                 disabled=False,
1266                                 is_public=True,
1267                                 extra_specs={},
1268                                 projects=[])
1269         _info_cache = objects.InstanceInfoCache(context)
1270         objects.InstanceInfoCache._from_db_object(context, _info_cache,
1271             test_instance_info_cache.fake_info_cache)
1272         # cell1 and cell2 will be the down cells while
1273         # cell0 and cell3 will be the up cells.
1274         down_cell_names = ['cell1', 'cell2']
1275         for cell in self.cells:
1276             # create 2 instances and their mappings in all the 4 cells
1277             for i in range(2):
1278                 with context.target_cell(self.ctxt, cell) as cctxt:
1279                     inst = objects.Instance(
1280                         context=cctxt,
1281                         project_id=self.project_id,
1282                         user_id=self.project_id,
1283                         instance_type_id=flavor.id,
1284                         hostname='%s-inst%i' % (cell.name, i),
1285                         flavor=flavor,
1286                         info_cache=_info_cache,
1287                         display_name='server-test')
1288                     inst.create()
1289                 im = objects.InstanceMapping(context=self.ctxt,
1290                                              instance_uuid=inst.uuid,
1291                                              cell_mapping=cell,
1292                                              project_id=self.project_id,
1293                                              queued_for_delete=False)
1294                 im.create()
1295                 if cell.name in down_cell_names:
1296                     self.down_cell_insts.append(inst.uuid)
1297                 else:
1298                     self.up_cell_insts.append(inst.uuid)
1299             # In cell1 and cell3 add a third instance in a different project
1300             # to show the --all-tenants case.
1301             if cell.name == 'cell1' or cell.name == 'cell3':
1302                 with context.target_cell(self.ctxt, cell) as cctxt:
1303                     inst = objects.Instance(
1304                         context=cctxt,
1305                         project_id='faker',
1306                         user_id='faker',
1307                         instance_type_id=flavor.id,
1308                         hostname='%s-inst%i' % (cell.name, 3),
1309                         flavor=flavor,
1310                         info_cache=_info_cache,
1311                         display_name='server-test')
1312                     inst.create()
1313                 im = objects.InstanceMapping(context=self.ctxt,
1314                                              instance_uuid=inst.uuid,
1315                                              cell_mapping=cell,
1316                                              project_id='faker',
1317                                              queued_for_delete=False)
1318                 im.create()
1319             if cell.name in down_cell_names:
1320                 self.down_cell_mappings.objects.append(cell)
1321         self.useFixture(nova_fixtures.DownCellFixture(self.down_cell_mappings))
1322 
1323     def test_get_servers_with_down_cells(self):
1324         servers = self.api.get_servers(detail=False)
1325         # 4 servers from the up cells and 4 servers from the down cells
1326         self.assertEqual(8, len(servers))
1327         for server in servers:
1328             if 'name' not in server:
1329                 # server is in the down cell.
1330                 self.assertEqual('UNKNOWN', server['status'])
1331                 self.assertIn(server['id'], self.down_cell_insts)
1332                 self.assertIn('links', server)
1333                 # the partial construct will have only the above 3 keys
1334                 self.assertEqual(3, len(server))
1335             else:
1336                 # server in up cell
1337                 self.assertIn(server['id'], self.up_cell_insts)
1338                 # has all the keys
1339                 self.assertEqual(server['name'], 'server-test')
1340                 self.assertIn('links', server)
1341 
1342     def test_get_servers_detail_with_down_cells(self):
1343         servers = self.api.get_servers()
1344         # 4 servers from the up cells and 4 servers from the down cells
1345         self.assertEqual(8, len(servers))
1346         for server in servers:
1347             if 'user_id' not in server:
1348                 # server is in the down cell.
1349                 self.assertEqual('UNKNOWN', server['status'])
1350                 self.assertIn(server['id'], self.down_cell_insts)
1351                 # the partial construct will have only 5 keys:
1352                 # created, tenant_id, status, id and links.
1353                 self.assertEqual(5, len(server))
1354             else:
1355                 # server in up cell
1356                 self.assertIn(server['id'], self.up_cell_insts)
1357                 # has all the keys
1358                 self.assertEqual(server['user_id'], self.project_id)
1359                 self.assertIn('image', server)
1360 
1361     def test_get_servers_detail_limits_with_down_cells(self):
1362         servers = self.api.get_servers(search_opts={'limit': 5})
1363         # 4 servers from the up cells since we skip down cell
1364         # results by default for paging.
1365         self.assertEqual(4, len(servers), servers)
1366         for server in servers:
1367             # server in up cell
1368             self.assertIn(server['id'], self.up_cell_insts)
1369             # has all the keys
1370             self.assertEqual(server['user_id'], self.project_id)
1371             self.assertIn('image', server)
1372 
1373     def test_get_servers_detail_limits_with_down_cells_the_500_gift(self):
1374         self.flags(list_records_by_skipping_down_cells=False, group='api')
1375         # We get an API error with a 500 response code since the
1376         # list_records_by_skipping_down_cells config option is False.
1377         exp = self.assertRaises(client.OpenStackApiException,
1378                                 self.api.get_servers,
1379                                 search_opts={'limit': 5})
1380         self.assertEqual(500, exp.response.status_code)
1381         self.assertIn('NovaException', six.text_type(exp))
1382 
1383     def test_get_servers_detail_marker_in_down_cells(self):
1384         marker = self.down_cell_insts[2]
1385         # It will fail with a 500 if the marker is in the down cell.
1386         exp = self.assertRaises(client.OpenStackApiException,
1387                                 self.api.get_servers,
1388                                 search_opts={'marker': marker})
1389         self.assertEqual(500, exp.response.status_code)
1390         self.assertIn('oslo_db.exception.DBError', six.text_type(exp))
1391 
1392     def test_get_servers_detail_marker_sorting(self):
1393         marker = self.up_cell_insts[1]
1394         # It will give the results from the up cell if
1395         # list_records_by_skipping_down_cells config option is True.
1396         servers = self.api.get_servers(search_opts={'marker': marker,
1397                                                     'sort_key': "created_at",
1398                                                     'sort_dir': "asc"})
1399         # since there are 4 servers from the up cells, when giving the
1400         # second instance as marker, sorted by creation time in ascending
1401         # third and fourth instances will be returned.
1402         self.assertEqual(2, len(servers))
1403         for server in servers:
1404             self.assertIn(
1405                 server['id'], [self.up_cell_insts[2], self.up_cell_insts[3]])
1406 
1407     def test_get_servers_detail_non_admin_with_deleted_flag(self):
1408         # if list_records_by_skipping_down_cells config option is True
1409         # this deleted option should be ignored and the rest of the instances
1410         # from the up cells and the partial results from the down cells should
1411         # be returned.
1412         # Set the policy so we don't have permission to allow
1413         # all filters but are able to get server details.
1414         servers_rule = 'os_compute_api:servers:detail'
1415         extraspec_rule = 'os_compute_api:servers:allow_all_filters'
1416         self.policy.set_rules({
1417             extraspec_rule: 'rule:admin_api',
1418             servers_rule: '@'})
1419         servers = self.api.get_servers(search_opts={'deleted': True})
1420         # gets 4 results from up cells and 4 from down cells.
1421         self.assertEqual(8, len(servers))
1422         for server in servers:
1423             if "image" not in server:
1424                 self.assertIn(server['id'], self.down_cell_insts)
1425             else:
1426                 self.assertIn(server['id'], self.up_cell_insts)
1427 
1428     def test_get_servers_detail_filters(self):
1429         # We get the results only from the up cells, this ignoring the down
1430         # cells if list_records_by_skipping_down_cells config option is True.
1431         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1432             api_version='v2.1'))
1433         self.admin_api = api_fixture.admin_api
1434         self.admin_api.microversion = '2.69'
1435         servers = self.admin_api.get_servers(
1436             search_opts={'hostname': "cell3-inst0"})
1437         self.assertEqual(1, len(servers))
1438         self.assertEqual(self.up_cell_insts[2], servers[0]['id'])
1439 
1440     def test_get_servers_detail_all_tenants_with_down_cells(self):
1441         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
1442             api_version='v2.1'))
1443         self.admin_api = api_fixture.admin_api
1444         self.admin_api.microversion = '2.69'
1445         servers = self.admin_api.get_servers(search_opts={'all_tenants': True})
1446         # 4 servers from the up cells and 4 servers from the down cells
1447         # plus the 2 instances from cell1 and cell3 which are in a different
1448         # project.
1449         self.assertEqual(10, len(servers))
1450         for server in servers:
1451             if 'user_id' not in server:
1452                 # server is in the down cell.
1453                 self.assertEqual('UNKNOWN', server['status'])
1454                 if server['tenant_id'] != 'faker':
1455                     self.assertIn(server['id'], self.down_cell_insts)
1456                 # the partial construct will have only 5 keys:
1457                 # created, tenant_id, status, id and links
1458                 self.assertEqual(5, len(server))
1459             else:
1460                 # server in up cell
1461                 if server['tenant_id'] != 'faker':
1462                     self.assertIn(server['id'], self.up_cell_insts)
1463                     self.assertEqual(server['user_id'], self.project_id)
1464                 self.assertIn('image', server)
1465 
1466 
1467 class ServerRebuildTestCase(integrated_helpers._IntegratedTestBase,
1468                             integrated_helpers.InstanceHelperMixin):
1469     api_major_version = 'v2.1'
1470     # We have to cap the microversion at 2.38 because that's the max we
1471     # can use to update image metadata via our compute images proxy API.
1472     microversion = '2.38'
1473 
1474     def _disable_compute_for(self, server):
1475         # Refresh to get its host
1476         server = self.api.get_server(server['id'])
1477         host = server['OS-EXT-SRV-ATTR:host']
1478 
1479         # Disable the service it is on
1480         self.api_fixture.admin_api.put_service('disable',
1481                                                {'host': host,
1482                                                 'binary': 'nova-compute'})
1483 
1484     def test_rebuild_with_image_novalidhost(self):
1485         """Creates a server with an image that is valid for the single compute
1486         that we have. Then rebuilds the server, passing in an image with
1487         metadata that does not fit the single compute which should result in
1488         a NoValidHost error. The ImagePropertiesFilter filter is enabled by
1489         default so that should filter out the host based on the image meta.
1490         """
1491 
1492         fake.set_nodes(['host2'])
1493         self.addCleanup(fake.restore_nodes)
1494         self.flags(host='host2')
1495         self.compute2 = self.start_service('compute', host='host2')
1496 
1497         # We hard-code from a fake image since we can't get images
1498         # via the compute /images proxy API with microversion > 2.35.
1499         original_image_ref = '155d900f-4e14-4e4c-a73d-069cbf4541e6'
1500         server_req_body = {
1501             'server': {
1502                 'imageRef': original_image_ref,
1503                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1504                 'name': 'test_rebuild_with_image_novalidhost',
1505                 # We don't care about networking for this test. This requires
1506                 # microversion >= 2.37.
1507                 'networks': 'none'
1508             }
1509         }
1510         server = self.api.post_server(server_req_body)
1511         self._wait_for_state_change(self.api, server, 'ACTIVE')
1512 
1513         # Disable the host we're on so ComputeFilter would have ruled it out
1514         # normally
1515         self._disable_compute_for(server)
1516 
1517         # Now update the image metadata to be something that won't work with
1518         # the fake compute driver we're using since the fake driver has an
1519         # "x86_64" architecture.
1520         rebuild_image_ref = (
1521             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1522         self.api.put_image_meta_key(
1523             rebuild_image_ref, 'hw_architecture', 'unicore32')
1524         # Now rebuild the server with that updated image and it should result
1525         # in a NoValidHost failure from the scheduler.
1526         rebuild_req_body = {
1527             'rebuild': {
1528                 'imageRef': rebuild_image_ref
1529             }
1530         }
1531         # Since we're using the CastAsCall fixture, the NoValidHost error
1532         # should actually come back to the API and result in a 500 error.
1533         # Normally the user would get a 202 response because nova-api RPC casts
1534         # to nova-conductor which RPC calls the scheduler which raises the
1535         # NoValidHost. We can mimic the end user way to figure out the failure
1536         # by looking for the failed 'rebuild' instance action event.
1537         self.api.api_post('/servers/%s/action' % server['id'],
1538                           rebuild_req_body, check_response_status=[500])
1539         # Look for the failed rebuild action.
1540         self._wait_for_action_fail_completion(
1541             server, instance_actions.REBUILD, 'rebuild_server',
1542             # Before microversion 2.51 events are only returned for instance
1543             # actions if you're an admin.
1544             self.api_fixture.admin_api)
1545         # Assert the server image_ref was rolled back on failure.
1546         server = self.api.get_server(server['id'])
1547         self.assertEqual(original_image_ref, server['image']['id'])
1548 
1549         # The server should be in ERROR state
1550         self.assertEqual('ERROR', server['status'])
1551         self.assertIn('No valid host', server['fault']['message'])
1552 
1553         # Rebuild it again with the same bad image to make sure it's rejected
1554         # again. Since we're using CastAsCall here, there is no 202 from the
1555         # API, and the exception from conductor gets passed back through the
1556         # API.
1557         ex = self.assertRaises(
1558             client.OpenStackApiException, self.api.api_post,
1559             '/servers/%s/action' % server['id'], rebuild_req_body)
1560         self.assertIn('NoValidHost', six.text_type(ex))
1561 
1562     # A rebuild to the same host should never attempt a rebuild claim.
1563     @mock.patch('nova.compute.resource_tracker.ResourceTracker.rebuild_claim',
1564                 new_callable=mock.NonCallableMock)
1565     def test_rebuild_with_new_image(self, mock_rebuild_claim):
1566         """Rebuilds a server with a different image which will run it through
1567         the scheduler to validate the image is still OK with the compute host
1568         that the instance is running on.
1569 
1570         Validates that additional resources are not allocated against the
1571         instance.host in Placement due to the rebuild on same host.
1572         """
1573         admin_api = self.api_fixture.admin_api
1574         admin_api.microversion = '2.53'
1575 
1576         def _get_provider_uuid_by_host(host):
1577             resp = admin_api.api_get(
1578                 'os-hypervisors?hypervisor_hostname_pattern=%s' % host).body
1579             return resp['hypervisors'][0]['id']
1580 
1581         def _get_provider_usages(provider_uuid):
1582             return self.placement_api.get(
1583                 '/resource_providers/%s/usages' % provider_uuid).body['usages']
1584 
1585         def _get_allocations_by_server_uuid(server_uuid):
1586             return self.placement_api.get(
1587                 '/allocations/%s' % server_uuid).body['allocations']
1588 
1589         def _set_provider_inventory(rp_uuid, resource_class, inventory):
1590             # Get the resource provider generation for the inventory update.
1591             rp = self.placement_api.get(
1592                 '/resource_providers/%s' % rp_uuid).body
1593             inventory['resource_provider_generation'] = rp['generation']
1594             return self.placement_api.put(
1595                 '/resource_providers/%s/inventories/%s' %
1596                 (rp_uuid, resource_class), inventory).body
1597 
1598         def assertFlavorMatchesAllocation(flavor, allocation):
1599             self.assertEqual(flavor['vcpus'], allocation['VCPU'])
1600             self.assertEqual(flavor['ram'], allocation['MEMORY_MB'])
1601             self.assertEqual(flavor['disk'], allocation['DISK_GB'])
1602 
1603         nodename = self.compute.manager._get_nodename(None)
1604         rp_uuid = _get_provider_uuid_by_host(nodename)
1605         # make sure we start with no usage on the compute node
1606         rp_usages = _get_provider_usages(rp_uuid)
1607         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
1608 
1609         server_req_body = {
1610             'server': {
1611                 # We hard-code from a fake image since we can't get images
1612                 # via the compute /images proxy API with microversion > 2.35.
1613                 'imageRef': '155d900f-4e14-4e4c-a73d-069cbf4541e6',
1614                 'flavorRef': '1',   # m1.tiny from DefaultFlavorsFixture,
1615                 'name': 'test_rebuild_with_new_image',
1616                 # We don't care about networking for this test. This requires
1617                 # microversion >= 2.37.
1618                 'networks': 'none'
1619             }
1620         }
1621         server = self.api.post_server(server_req_body)
1622         self._wait_for_state_change(self.api, server, 'ACTIVE')
1623 
1624         flavor = self.api.api_get('/flavors/1').body['flavor']
1625 
1626         # make the compute node full and ensure rebuild still succeed
1627         _set_provider_inventory(rp_uuid, "VCPU", {"total": 1})
1628 
1629         # There should be usage for the server on the compute node now.
1630         rp_usages = _get_provider_usages(rp_uuid)
1631         assertFlavorMatchesAllocation(flavor, rp_usages)
1632         allocs = _get_allocations_by_server_uuid(server['id'])
1633         self.assertIn(rp_uuid, allocs)
1634         allocs = allocs[rp_uuid]['resources']
1635         assertFlavorMatchesAllocation(flavor, allocs)
1636 
1637         rebuild_image_ref = (
1638             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1639         # Now rebuild the server with a different image.
1640         rebuild_req_body = {
1641             'rebuild': {
1642                 'imageRef': rebuild_image_ref
1643             }
1644         }
1645         self.api.api_post('/servers/%s/action' % server['id'],
1646                           rebuild_req_body)
1647         self._wait_for_server_parameter(
1648             self.api, server, {'OS-EXT-STS:task_state': None})
1649 
1650         # The usage and allocations should not have changed.
1651         rp_usages = _get_provider_usages(rp_uuid)
1652         assertFlavorMatchesAllocation(flavor, rp_usages)
1653 
1654         allocs = _get_allocations_by_server_uuid(server['id'])
1655         self.assertIn(rp_uuid, allocs)
1656         allocs = allocs[rp_uuid]['resources']
1657         assertFlavorMatchesAllocation(flavor, allocs)
1658 
1659     def test_volume_backed_rebuild_different_image(self):
1660         """Tests that trying to rebuild a volume-backed instance with a
1661         different image than what is in the root disk of the root volume
1662         will result in a 400 BadRequest error.
1663         """
1664         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
1665         # First create our server as normal.
1666         server_req_body = {
1667             # There is no imageRef because this is boot from volume.
1668             'server': {
1669                 'flavorRef': '1',  # m1.tiny from DefaultFlavorsFixture,
1670                 'name': 'test_volume_backed_rebuild_different_image',
1671                 # We don't care about networking for this test. This requires
1672                 # microversion >= 2.37.
1673                 'networks': 'none',
1674                 'block_device_mapping_v2': [{
1675                     'boot_index': 0,
1676                     'uuid':
1677                     nova_fixtures.CinderFixtureNewAttachFlow.IMAGE_BACKED_VOL,
1678                     'source_type': 'volume',
1679                     'destination_type': 'volume'
1680                 }]
1681             }
1682         }
1683         server = self.api.post_server(server_req_body)
1684         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
1685         # For a volume-backed server, the image ref will be an empty string
1686         # in the server response.
1687         self.assertEqual('', server['image'])
1688 
1689         # Now rebuild the server with a different image than was used to create
1690         # our fake volume.
1691         rebuild_image_ref = (
1692             nova.tests.unit.image.fake.AUTO_DISK_CONFIG_ENABLED_IMAGE_UUID)
1693         rebuild_req_body = {
1694             'rebuild': {
1695                 'imageRef': rebuild_image_ref
1696             }
1697         }
1698         resp = self.api.api_post('/servers/%s/action' % server['id'],
1699                                  rebuild_req_body, check_response_status=[400])
1700         # Assert that we failed because of the image change and not something
1701         # else.
1702         self.assertIn('Unable to rebuild with a different image for a '
1703                       'volume-backed server', six.text_type(resp))
1704 
1705 
1706 class ProviderTreeTests(integrated_helpers.ProviderUsageBaseTestCase):
1707     compute_driver = 'fake.MediumFakeDriver'
1708 
1709     def setUp(self):
1710         super(ProviderTreeTests, self).setUp()
1711         # Before starting compute, placement has no providers registered
1712         self.assertEqual([], self._get_all_providers())
1713 
1714         # Start compute without mocking update_provider_tree. The fake driver
1715         # doesn't implement the method, so this will cause us to start with the
1716         # legacy get_available_resource()-based inventory discovery and
1717         # boostrapping of placement data.
1718         self.compute = self._start_compute(host='host1')
1719 
1720         # Mock out update_provider_tree *after* starting compute with the
1721         # (unmocked, default, unimplemented) version from the fake driver.
1722         _p = mock.patch.object(fake.MediumFakeDriver, 'update_provider_tree')
1723         self.addCleanup(_p.stop)
1724         self.mock_upt = _p.start()
1725 
1726         # The compute host should have been created in placement with
1727         # appropriate inventory and no traits
1728         rps = self._get_all_providers()
1729         self.assertEqual(1, len(rps))
1730         self.assertEqual(self.compute.host, rps[0]['name'])
1731         self.host_uuid = self._get_provider_uuid_by_host(self.compute.host)
1732         self.assertEqual({
1733             'DISK_GB': {
1734                 'total': 1028,
1735                 'allocation_ratio': 1.0,
1736                 'max_unit': 1028,
1737                 'min_unit': 1,
1738                 'reserved': 0,
1739                 'step_size': 1,
1740             },
1741             'MEMORY_MB': {
1742                 'total': 8192,
1743                 'allocation_ratio': 1.5,
1744                 'max_unit': 8192,
1745                 'min_unit': 1,
1746                 'reserved': 512,
1747                 'step_size': 1,
1748             },
1749             'VCPU': {
1750                 'total': 10,
1751                 'allocation_ratio': 16.0,
1752                 'max_unit': 10,
1753                 'min_unit': 1,
1754                 'reserved': 0,
1755                 'step_size': 1,
1756             },
1757         }, self._get_provider_inventory(self.host_uuid))
1758         self.assertEqual([], self._get_provider_traits(self.host_uuid))
1759 
1760     def _run_update_available_resource(self, startup):
1761         self.compute.rt.update_available_resource(
1762             context.get_admin_context(), self.compute.host, startup=startup)
1763 
1764     def _run_update_available_resource_and_assert_raises(
1765             self, exc=exception.ResourceProviderSyncFailed, startup=False):
1766         """Invoke ResourceTracker.update_available_resource and assert that it
1767         results in ResourceProviderSyncFailed.
1768 
1769         _run_periodicals is a little too high up in the call stack to be useful
1770         for this, because ResourceTracker.update_available_resource_for_node
1771         swallows all exceptions.
1772         """
1773         self.assertRaises(exc, self._run_update_available_resource, startup)
1774 
1775     def test_update_provider_tree_associated_info(self):
1776         """Inventory in some standard and custom resource classes.  Standard
1777         and custom traits.  Aggregates.  Custom resource class and trait get
1778         created; inventory, traits, and aggregates get set properly.
1779         """
1780         inv = {
1781             'VCPU': {
1782                 'total': 10,
1783                 'reserved': 0,
1784                 'min_unit': 1,
1785                 'max_unit': 2,
1786                 'step_size': 1,
1787                 'allocation_ratio': 10.0,
1788             },
1789             'MEMORY_MB': {
1790                 'total': 1048576,
1791                 'reserved': 2048,
1792                 'min_unit': 1024,
1793                 'max_unit': 131072,
1794                 'step_size': 1024,
1795                 'allocation_ratio': 1.0,
1796             },
1797             'CUSTOM_BANDWIDTH': {
1798                 'total': 1250000,
1799                 'reserved': 10000,
1800                 'min_unit': 5000,
1801                 'max_unit': 250000,
1802                 'step_size': 5000,
1803                 'allocation_ratio': 8.0,
1804             },
1805         }
1806         traits = set(['HW_CPU_X86_AVX', 'HW_CPU_X86_AVX2', 'CUSTOM_GOLD'])
1807         aggs = set([uuids.agg1, uuids.agg2])
1808 
1809         def update_provider_tree(prov_tree, nodename):
1810             prov_tree.update_inventory(self.compute.host, inv)
1811             prov_tree.update_traits(self.compute.host, traits)
1812             prov_tree.update_aggregates(self.compute.host, aggs)
1813         self.mock_upt.side_effect = update_provider_tree
1814 
1815         self.assertNotIn('CUSTOM_BANDWIDTH', self._get_all_resource_classes())
1816         self.assertNotIn('CUSTOM_GOLD', self._get_all_traits())
1817 
1818         self._run_periodics()
1819 
1820         self.assertIn('CUSTOM_BANDWIDTH', self._get_all_resource_classes())
1821         self.assertIn('CUSTOM_GOLD', self._get_all_traits())
1822         self.assertEqual(inv, self._get_provider_inventory(self.host_uuid))
1823         self.assertEqual(traits,
1824                          set(self._get_provider_traits(self.host_uuid)))
1825         self.assertEqual(aggs,
1826                          set(self._get_provider_aggregates(self.host_uuid)))
1827 
1828     def _update_provider_tree_multiple_providers(self, startup=False,
1829                                                  do_reshape=False):
1830         """Make update_provider_tree create multiple providers, including an
1831         additional root as a sharing provider; and some descendants in the
1832         compute node's tree.
1833 
1834                    +---------------------------+   +--------------------------+
1835                    |uuid: self.host_uuid       |   |uuid: uuids.ssp           |
1836                    |name: self.compute.host    |   |name: 'ssp'               |
1837                    |inv: (per MediumFakeDriver)|   |inv: DISK_GB=500          |
1838                    |     VCPU=10               |...|traits: [MISC_SHARES_..., |
1839                    |     MEMORY_MB=8192        |   |         STORAGE_DISK_SSD]|
1840                    |     DISK_GB=1028          |   |aggs: [uuids.agg]         |
1841                    |aggs: [uuids.agg]          |   +--------------------------+
1842                    +---------------------------+
1843                          /                   \
1844              +-----------------+          +-----------------+
1845              |uuid: uuids.numa1|          |uuid: uuids.numa2|
1846              |name: 'numa1'    |          |name: 'numa2'    |
1847              |inv: VCPU=10     |          |inv: VCPU=20     |
1848              |     MEMORY_MB=1G|          |     MEMORY_MB=2G|
1849              +-----------------+          +-----------------+
1850                  /          \                    /         \
1851         +------------+  +------------+   +------------+  +------------+
1852         |uuid:       |  |uuid:       |   |uuid:       |  |uuid:       |
1853         | uuids.pf1_1|  | uuids.pf1_2|   | uuids.pf2_1|  | uuids.pf2_2|
1854         |name:       |  |name:       |   |name:       |  |name:       |
1855         | 'pf1_1'    |  | 'pf1_2'    |   | 'pf2_1'    |  | 'pf2_2'    |
1856         |inv:        |  |inv:        |   |inv:        |  |inv:        |
1857         | ..NET_VF: 2|  | ..NET_VF: 3|   | ..NET_VF: 3|  | ..NET_VF: 4|
1858         |traits:     |  |traits:     |   |traits:     |  |traits:     |
1859         | ..PHYSNET_0|  | ..PHYSNET_1|   | ..PHYSNET_0|  | ..PHYSNET_1|
1860         +------------+  +------------+   +------------+  +------------+
1861         """
1862         def update_provider_tree(prov_tree, nodename, allocations=None):
1863             if do_reshape and allocations is None:
1864                 raise exception.ReshapeNeeded()
1865 
1866             # Create a shared storage provider as a root
1867             prov_tree.new_root('ssp', uuids.ssp)
1868             prov_tree.update_traits(
1869                 'ssp', ['MISC_SHARES_VIA_AGGREGATE', 'STORAGE_DISK_SSD'])
1870             prov_tree.update_aggregates('ssp', [uuids.agg])
1871             prov_tree.update_inventory('ssp', {'DISK_GB': {'total': 500}})
1872             # Compute node is in the same aggregate
1873             prov_tree.update_aggregates(self.compute.host, [uuids.agg])
1874             # Create two NUMA nodes as children
1875             prov_tree.new_child('numa1', self.host_uuid, uuid=uuids.numa1)
1876             prov_tree.new_child('numa2', self.host_uuid, uuid=uuids.numa2)
1877             # Give the NUMA nodes the proc/mem inventory.  NUMA 2 has twice as
1878             # much as NUMA 1 (so we can validate later that everything is where
1879             # it should be).
1880             for n in (1, 2):
1881                 inv = {
1882                     'VCPU': {
1883                         'total': 10 * n,
1884                         'reserved': 0,
1885                         'min_unit': 1,
1886                         'max_unit': 2,
1887                         'step_size': 1,
1888                         'allocation_ratio': 10.0,
1889                     },
1890                     'MEMORY_MB': {
1891                          'total': 1048576 * n,
1892                          'reserved': 2048,
1893                          'min_unit': 512,
1894                          'max_unit': 131072,
1895                          'step_size': 512,
1896                          'allocation_ratio': 1.0,
1897                      },
1898                 }
1899                 prov_tree.update_inventory('numa%d' % n, inv)
1900             # Each NUMA node has two PFs providing VF inventory on one of two
1901             # networks
1902             for n in (1, 2):
1903                 for p in (1, 2):
1904                     name = 'pf%d_%d' % (n, p)
1905                     prov_tree.new_child(
1906                         name, getattr(uuids, 'numa%d' % n),
1907                         uuid=getattr(uuids, name))
1908                     trait = 'CUSTOM_PHYSNET_%d' % ((n + p) % 2)
1909                     prov_tree.update_traits(name, [trait])
1910                     inv = {
1911                         'SRIOV_NET_VF': {
1912                             'total': n + p,
1913                             'reserved': 0,
1914                             'min_unit': 1,
1915                             'max_unit': 1,
1916                             'step_size': 1,
1917                             'allocation_ratio': 1.0,
1918                         },
1919                     }
1920                     prov_tree.update_inventory(name, inv)
1921             if do_reshape:
1922                 # Clear out the compute node's inventory. Its VCPU and
1923                 # MEMORY_MB "moved" to the NUMA RPs and its DISK_GB "moved" to
1924                 # the shared storage provider.
1925                 prov_tree.update_inventory(self.host_uuid, {})
1926                 # Move all the allocations
1927                 for consumer_uuid, alloc_info in allocations.items():
1928                     allocs = alloc_info['allocations']
1929                     # All allocations should belong to the compute node.
1930                     self.assertEqual([self.host_uuid], list(allocs))
1931                     new_allocs = {}
1932                     for rc, amt in allocs[self.host_uuid]['resources'].items():
1933                         # Move VCPU to NUMA1 and MEMORY_MB to NUMA2. Bogus, but
1934                         # lets us prove stuff ends up where we tell it to go.
1935                         if rc == 'VCPU':
1936                             rp_uuid = uuids.numa1
1937                         elif rc == 'MEMORY_MB':
1938                             rp_uuid = uuids.numa2
1939                         elif rc == 'DISK_GB':
1940                             rp_uuid = uuids.ssp
1941                         else:
1942                             self.fail("Unexpected resource on compute node: "
1943                                       "%s=%d" % (rc, amt))
1944                         new_allocs[rp_uuid] = {
1945                             'resources': {rc: amt},
1946                         }
1947                     # Add a VF for the heck of it. Again bogus, but see above.
1948                     new_allocs[uuids.pf1_1] = {
1949                         'resources': {'SRIOV_NET_VF': 1}
1950                     }
1951                     # Now replace just the allocations, leaving the other stuff
1952                     # (proj/user ID and consumer generation) alone
1953                     alloc_info['allocations'] = new_allocs
1954 
1955         self.mock_upt.side_effect = update_provider_tree
1956 
1957         if startup:
1958             self.restart_compute_service(self.compute)
1959         else:
1960             self._run_update_available_resource(False)
1961 
1962         # Create a dict, keyed by provider UUID, of all the providers
1963         rps_by_uuid = {}
1964         for rp_dict in self._get_all_providers():
1965             rps_by_uuid[rp_dict['uuid']] = rp_dict
1966 
1967         # All and only the expected providers got created.
1968         all_uuids = set([self.host_uuid, uuids.ssp, uuids.numa1, uuids.numa2,
1969                          uuids.pf1_1, uuids.pf1_2, uuids.pf2_1, uuids.pf2_2])
1970         self.assertEqual(all_uuids, set(rps_by_uuid))
1971 
1972         # Validate tree roots
1973         tree_uuids = [self.host_uuid, uuids.numa1, uuids.numa2,
1974                       uuids.pf1_1, uuids.pf1_2, uuids.pf2_1, uuids.pf2_2]
1975         for tree_uuid in tree_uuids:
1976             self.assertEqual(self.host_uuid,
1977                              rps_by_uuid[tree_uuid]['root_provider_uuid'])
1978         self.assertEqual(uuids.ssp,
1979                          rps_by_uuid[uuids.ssp]['root_provider_uuid'])
1980 
1981         # SSP has the right traits
1982         self.assertEqual(
1983             set(['MISC_SHARES_VIA_AGGREGATE', 'STORAGE_DISK_SSD']),
1984             set(self._get_provider_traits(uuids.ssp)))
1985 
1986         # SSP has the right inventory
1987         self.assertEqual(
1988             500, self._get_provider_inventory(uuids.ssp)['DISK_GB']['total'])
1989 
1990         # SSP and compute are in the same aggregate
1991         agg_uuids = set([self.host_uuid, uuids.ssp])
1992         for uuid in agg_uuids:
1993             self.assertEqual(set([uuids.agg]),
1994                              set(self._get_provider_aggregates(uuid)))
1995 
1996         # The rest aren't in aggregates
1997         for uuid in (all_uuids - agg_uuids):
1998             self.assertEqual(set(), set(self._get_provider_aggregates(uuid)))
1999 
2000         # NUMAs have the right inventory and parentage
2001         for n in (1, 2):
2002             numa_uuid = getattr(uuids, 'numa%d' % n)
2003             self.assertEqual(self.host_uuid,
2004                              rps_by_uuid[numa_uuid]['parent_provider_uuid'])
2005             inv = self._get_provider_inventory(numa_uuid)
2006             self.assertEqual(10 * n, inv['VCPU']['total'])
2007             self.assertEqual(1048576 * n, inv['MEMORY_MB']['total'])
2008 
2009         # PFs have the right inventory, physnet, and parentage
2010         self.assertEqual(uuids.numa1,
2011                          rps_by_uuid[uuids.pf1_1]['parent_provider_uuid'])
2012         self.assertEqual(['CUSTOM_PHYSNET_0'],
2013                          self._get_provider_traits(uuids.pf1_1))
2014         self.assertEqual(
2015             2,
2016             self._get_provider_inventory(uuids.pf1_1)['SRIOV_NET_VF']['total'])
2017 
2018         self.assertEqual(uuids.numa1,
2019                          rps_by_uuid[uuids.pf1_2]['parent_provider_uuid'])
2020         self.assertEqual(['CUSTOM_PHYSNET_1'],
2021                          self._get_provider_traits(uuids.pf1_2))
2022         self.assertEqual(
2023             3,
2024             self._get_provider_inventory(uuids.pf1_2)['SRIOV_NET_VF']['total'])
2025 
2026         self.assertEqual(uuids.numa2,
2027                          rps_by_uuid[uuids.pf2_1]['parent_provider_uuid'])
2028         self.assertEqual(['CUSTOM_PHYSNET_1'],
2029                          self._get_provider_traits(uuids.pf2_1))
2030         self.assertEqual(
2031             3,
2032             self._get_provider_inventory(uuids.pf2_1)['SRIOV_NET_VF']['total'])
2033 
2034         self.assertEqual(uuids.numa2,
2035                          rps_by_uuid[uuids.pf2_2]['parent_provider_uuid'])
2036         self.assertEqual(['CUSTOM_PHYSNET_0'],
2037                          self._get_provider_traits(uuids.pf2_2))
2038         self.assertEqual(
2039             4,
2040             self._get_provider_inventory(uuids.pf2_2)['SRIOV_NET_VF']['total'])
2041 
2042         # Compute and NUMAs don't have any traits
2043         for uuid in (self.host_uuid, uuids.numa1, uuids.numa2):
2044             self.assertEqual([], self._get_provider_traits(uuid))
2045 
2046     def test_update_provider_tree_multiple_providers(self):
2047         self._update_provider_tree_multiple_providers()
2048 
2049     def test_update_provider_tree_multiple_providers_startup(self):
2050         """The above works the same for startup when no reshape requested."""
2051         self._update_provider_tree_multiple_providers(startup=True)
2052 
2053     def test_update_provider_tree_bogus_resource_class(self):
2054         def update_provider_tree(prov_tree, nodename):
2055             prov_tree.update_inventory(self.compute.host, {'FOO': {}})
2056         self.mock_upt.side_effect = update_provider_tree
2057 
2058         rcs = self._get_all_resource_classes()
2059         self.assertIn('VCPU', rcs)
2060         self.assertNotIn('FOO', rcs)
2061 
2062         self._run_update_available_resource_and_assert_raises()
2063 
2064         rcs = self._get_all_resource_classes()
2065         self.assertIn('VCPU', rcs)
2066         self.assertNotIn('FOO', rcs)
2067 
2068     def test_update_provider_tree_bogus_trait(self):
2069         def update_provider_tree(prov_tree, nodename):
2070             prov_tree.update_traits(self.compute.host, ['FOO'])
2071         self.mock_upt.side_effect = update_provider_tree
2072 
2073         traits = self._get_all_traits()
2074         self.assertIn('HW_CPU_X86_AVX', traits)
2075         self.assertNotIn('FOO', traits)
2076 
2077         self._run_update_available_resource_and_assert_raises()
2078 
2079         traits = self._get_all_traits()
2080         self.assertIn('HW_CPU_X86_AVX', traits)
2081         self.assertNotIn('FOO', traits)
2082 
2083     def _create_instance(self, flavor):
2084         server_req = self._build_minimal_create_server_request(
2085             self.api, 'some-server', flavor_id=flavor['id'],
2086             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
2087             networks='none', az='nova:host1')
2088         inst = self.api.post_server({'server': server_req})
2089         return self._wait_for_state_change(self.admin_api, inst, 'ACTIVE')
2090 
2091     def test_reshape(self):
2092         """On startup, virt driver signals it needs to reshape, then does so.
2093 
2094         This test creates a couple of instances so there are allocations to be
2095         moved by the reshape operation. Then we do the reshape and make sure
2096         the inventories and allocations end up where they should.
2097         """
2098         # First let's create some instances so we have allocations to move.
2099         flavors = self.api.get_flavors()
2100         inst1 = self._create_instance(flavors[0])
2101         inst2 = self._create_instance(flavors[1])
2102 
2103         # Instance create calls RT._update, which calls
2104         # driver.update_provider_tree, which is currently mocked to a no-op.
2105         self.assertEqual(2, self.mock_upt.call_count)
2106         self.mock_upt.reset_mock()
2107 
2108         # Hit the reshape.
2109         self._update_provider_tree_multiple_providers(startup=True,
2110                                                       do_reshape=True)
2111 
2112         # Check the final allocations
2113         # The compute node provider should have *no* allocations.
2114         self.assertEqual(
2115             {}, self._get_allocations_by_provider_uuid(self.host_uuid))
2116         # And no inventory
2117         self.assertEqual({}, self._get_provider_inventory(self.host_uuid))
2118         # NUMA1 got all the VCPU
2119         self.assertEqual(
2120             {inst1['id']: {'resources': {'VCPU': 1}},
2121              inst2['id']: {'resources': {'VCPU': 1}}},
2122             self._get_allocations_by_provider_uuid(uuids.numa1))
2123         # NUMA2 got all the memory
2124         self.assertEqual(
2125             {inst1['id']: {'resources': {'MEMORY_MB': 512}},
2126              inst2['id']: {'resources': {'MEMORY_MB': 2048}}},
2127             self._get_allocations_by_provider_uuid(uuids.numa2))
2128         # Disk resource ended up on the shared storage provider
2129         self.assertEqual(
2130             {inst1['id']: {'resources': {'DISK_GB': 1}},
2131              inst2['id']: {'resources': {'DISK_GB': 20}}},
2132             self._get_allocations_by_provider_uuid(uuids.ssp))
2133         # We put VFs on the first PF in NUMA1
2134         self.assertEqual(
2135             {inst1['id']: {'resources': {'SRIOV_NET_VF': 1}},
2136              inst2['id']: {'resources': {'SRIOV_NET_VF': 1}}},
2137             self._get_allocations_by_provider_uuid(uuids.pf1_1))
2138         self.assertEqual(
2139             {}, self._get_allocations_by_provider_uuid(uuids.pf1_2))
2140         self.assertEqual(
2141             {}, self._get_allocations_by_provider_uuid(uuids.pf2_1))
2142         self.assertEqual(
2143             {}, self._get_allocations_by_provider_uuid(uuids.pf2_2))
2144         # This is *almost* redundant - but it makes sure the instances don't
2145         # have extra allocations from some other provider.
2146         self.assertEqual(
2147             {
2148                 uuids.numa1: {
2149                     'resources': {'VCPU': 1},
2150                     # Don't care about the generations - rely on placement db
2151                     # tests to validate that those behave properly.
2152                     'generation': mock.ANY,
2153                 },
2154                 uuids.numa2: {
2155                     'resources': {'MEMORY_MB': 512},
2156                     'generation': mock.ANY,
2157                 },
2158                 uuids.ssp: {
2159                     'resources': {'DISK_GB': 1},
2160                     'generation': mock.ANY,
2161                 },
2162                 uuids.pf1_1: {
2163                     'resources': {'SRIOV_NET_VF': 1},
2164                     'generation': mock.ANY,
2165                 },
2166             }, self._get_allocations_by_server_uuid(inst1['id']))
2167         self.assertEqual(
2168             {
2169                 uuids.numa1: {
2170                     'resources': {'VCPU': 1},
2171                     'generation': mock.ANY,
2172                 },
2173                 uuids.numa2: {
2174                     'resources': {'MEMORY_MB': 2048},
2175                     'generation': mock.ANY,
2176                 },
2177                 uuids.ssp: {
2178                     'resources': {'DISK_GB': 20},
2179                     'generation': mock.ANY,
2180                 },
2181                 uuids.pf1_1: {
2182                     'resources': {'SRIOV_NET_VF': 1},
2183                     'generation': mock.ANY,
2184                 },
2185             }, self._get_allocations_by_server_uuid(inst2['id']))
2186 
2187         # The first call raises ReshapeNeeded, resulting in the second.
2188         self.assertEqual(2, self.mock_upt.call_count)
2189         # The expected value of the allocations kwarg to update_provider_tree
2190         # for that second call:
2191         exp_allocs = {
2192             inst1['id']: {
2193                 'allocations': {
2194                     uuids.numa1: {'resources': {'VCPU': 1}},
2195                     uuids.numa2: {'resources': {'MEMORY_MB': 512}},
2196                     uuids.ssp: {'resources': {'DISK_GB': 1}},
2197                     uuids.pf1_1: {'resources': {'SRIOV_NET_VF': 1}},
2198                 },
2199                 'consumer_generation': mock.ANY,
2200                 'project_id': mock.ANY,
2201                 'user_id': mock.ANY,
2202             },
2203             inst2['id']: {
2204                 'allocations': {
2205                     uuids.numa1: {'resources': {'VCPU': 1}},
2206                     uuids.numa2: {'resources': {'MEMORY_MB': 2048}},
2207                     uuids.ssp: {'resources': {'DISK_GB': 20}},
2208                     uuids.pf1_1: {'resources': {'SRIOV_NET_VF': 1}},
2209                 },
2210                 'consumer_generation': mock.ANY,
2211                 'project_id': mock.ANY,
2212                 'user_id': mock.ANY,
2213             },
2214         }
2215         self.mock_upt.assert_has_calls([
2216             mock.call(mock.ANY, 'host1'),
2217             mock.call(mock.ANY, 'host1', allocations=exp_allocs),
2218         ])
2219 
2220 
2221 class ServerMovingTests(integrated_helpers.ProviderUsageBaseTestCase):
2222     """Tests moving servers while checking the resource allocations and usages
2223 
2224     These tests use two compute hosts. Boot a server on one of them then try to
2225     move the server to the other. At every step resource allocation of the
2226     server and the resource usages of the computes are queried from placement
2227     API and asserted.
2228     """
2229 
2230     REQUIRES_LOCKING = True
2231     # NOTE(danms): The test defaults to using SmallFakeDriver,
2232     # which only has one vcpu, which can't take the doubled allocation
2233     # we're now giving it. So, use the bigger MediumFakeDriver here.
2234     compute_driver = 'fake.MediumFakeDriver'
2235 
2236     def setUp(self):
2237         super(ServerMovingTests, self).setUp()
2238         fake_notifier.stub_notifier(self)
2239         self.addCleanup(fake_notifier.reset)
2240 
2241         self.compute1 = self._start_compute(host='host1')
2242         self.compute2 = self._start_compute(host='host2')
2243 
2244         flavors = self.api.get_flavors()
2245         self.flavor1 = flavors[0]
2246         self.flavor2 = flavors[1]
2247         # create flavor3 which has less MEMORY_MB but more DISK_GB than flavor2
2248         flavor_body = {'flavor':
2249                            {'name': 'test_flavor3',
2250                             'ram': int(self.flavor2['ram'] / 2),
2251                             'vcpus': 1,
2252                             'disk': self.flavor2['disk'] * 2,
2253                             'id': 'a22d5517-147c-4147-a0d1-e698df5cd4e3'
2254                             }}
2255 
2256         self.flavor3 = self.api.post_flavor(flavor_body)
2257 
2258     def _other_hostname(self, host):
2259         other_host = {'host1': 'host2',
2260                       'host2': 'host1'}
2261         return other_host[host]
2262 
2263     def _run_periodics(self):
2264         # NOTE(jaypipes): We always run periodics in the same order: first on
2265         # compute1, then on compute2. However, we want to test scenarios when
2266         # the periodics run at different times during mover operations. This is
2267         # why we have the "reverse" tests which simply switch the source and
2268         # dest host while keeping the order in which we run the
2269         # periodics. This effectively allows us to test the matrix of timing
2270         # scenarios during move operations.
2271         ctx = context.get_admin_context()
2272         LOG.info('Running periodic for compute1 (%s)',
2273             self.compute1.manager.host)
2274         self.compute1.manager.update_available_resource(ctx)
2275         LOG.info('Running periodic for compute2 (%s)',
2276             self.compute2.manager.host)
2277         self.compute2.manager.update_available_resource(ctx)
2278         LOG.info('Finished with periodics')
2279 
2280     def test_resize_revert(self):
2281         self._test_resize_revert(dest_hostname='host1')
2282 
2283     def test_resize_revert_reverse(self):
2284         self._test_resize_revert(dest_hostname='host2')
2285 
2286     def test_resize_confirm(self):
2287         self._test_resize_confirm(dest_hostname='host1')
2288 
2289     def test_resize_confirm_reverse(self):
2290         self._test_resize_confirm(dest_hostname='host2')
2291 
2292     def _resize_and_check_allocations(self, server, old_flavor, new_flavor,
2293             source_rp_uuid, dest_rp_uuid):
2294         self.flags(allow_resize_to_same_host=False)
2295         resize_req = {
2296             'resize': {
2297                 'flavorRef': new_flavor['id']
2298             }
2299         }
2300         self._move_and_check_allocations(
2301             server, request=resize_req, old_flavor=old_flavor,
2302             new_flavor=new_flavor, source_rp_uuid=source_rp_uuid,
2303             dest_rp_uuid=dest_rp_uuid)
2304 
2305     def _test_resize_revert(self, dest_hostname):
2306         source_hostname = self._other_hostname(dest_hostname)
2307         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2308         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2309 
2310         server = self._boot_and_check_allocations(self.flavor1,
2311             source_hostname)
2312 
2313         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2314             source_rp_uuid, dest_rp_uuid)
2315 
2316         # Revert the resize and check the usages
2317         post = {'revertResize': None}
2318         self.api.post_server_action(server['id'], post)
2319         self._wait_for_state_change(self.api, server, 'ACTIVE')
2320 
2321         # Make sure the RequestSpec.flavor matches the original flavor.
2322         ctxt = context.get_admin_context()
2323         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
2324         self.assertEqual(self.flavor1['id'], reqspec.flavor.flavorid)
2325 
2326         self._run_periodics()
2327 
2328         # the original host expected to have the old resource allocation
2329         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2330 
2331         self.assertRequestMatchesUsage({'VCPU': 0,
2332                                         'MEMORY_MB': 0,
2333                                         'DISK_GB': 0}, dest_rp_uuid)
2334 
2335         # Check that the server only allocates resource from the original host
2336         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2337                                            source_rp_uuid)
2338 
2339         self._delete_and_check_allocations(server)
2340 
2341     def _test_resize_confirm(self, dest_hostname):
2342         source_hostname = self._other_hostname(dest_hostname)
2343         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2344         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2345 
2346         server = self._boot_and_check_allocations(self.flavor1,
2347             source_hostname)
2348 
2349         self._resize_and_check_allocations(server, self.flavor1, self.flavor2,
2350             source_rp_uuid, dest_rp_uuid)
2351 
2352         # Confirm the resize and check the usages
2353         post = {'confirmResize': None}
2354         self.api.post_server_action(
2355             server['id'], post, check_response_status=[204])
2356         self._wait_for_state_change(self.api, server, 'ACTIVE')
2357 
2358         # After confirming, we should have an allocation only on the
2359         # destination host
2360 
2361         # The target host usage should be according to the new flavor
2362         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
2363         self.assertRequestMatchesUsage({'VCPU': 0,
2364                                         'MEMORY_MB': 0,
2365                                         'DISK_GB': 0}, source_rp_uuid)
2366 
2367         # and the target host allocation should be according to the new flavor
2368         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2369                                            dest_rp_uuid)
2370 
2371         self._run_periodics()
2372 
2373         # Check we're still accurate after running the periodics
2374 
2375         # and the target host usage should be according to the new flavor
2376         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor2)
2377         self.assertRequestMatchesUsage({'VCPU': 0,
2378                                         'MEMORY_MB': 0,
2379                                         'DISK_GB': 0}, source_rp_uuid)
2380 
2381         # and the server allocates only from the target host
2382         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2383                                            dest_rp_uuid)
2384 
2385         self._delete_and_check_allocations(server)
2386 
2387     def test_resize_revert_same_host(self):
2388         # make sure that the test only uses a single host
2389         compute2_service_id = self.admin_api.get_services(
2390             host=self.compute2.host, binary='nova-compute')[0]['id']
2391         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2392 
2393         hostname = self.compute1.manager.host
2394         rp_uuid = self._get_provider_uuid_by_host(hostname)
2395 
2396         server = self._boot_and_check_allocations(self.flavor2, hostname)
2397 
2398         self._resize_to_same_host_and_check_allocations(
2399             server, self.flavor2, self.flavor3, rp_uuid)
2400 
2401         # Revert the resize and check the usages
2402         post = {'revertResize': None}
2403         self.api.post_server_action(server['id'], post)
2404         self._wait_for_state_change(self.api, server, 'ACTIVE')
2405 
2406         self._run_periodics()
2407 
2408         # after revert only allocations due to the old flavor should remain
2409         self.assertFlavorMatchesUsage(rp_uuid, self.flavor2)
2410 
2411         self.assertFlavorMatchesAllocation(self.flavor2, server['id'],
2412                                            rp_uuid)
2413 
2414         self._delete_and_check_allocations(server)
2415 
2416     def test_resize_confirm_same_host(self):
2417         # make sure that the test only uses a single host
2418         compute2_service_id = self.admin_api.get_services(
2419             host=self.compute2.host, binary='nova-compute')[0]['id']
2420         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2421 
2422         hostname = self.compute1.manager.host
2423         rp_uuid = self._get_provider_uuid_by_host(hostname)
2424 
2425         server = self._boot_and_check_allocations(self.flavor2, hostname)
2426 
2427         self._resize_to_same_host_and_check_allocations(
2428             server, self.flavor2, self.flavor3, rp_uuid)
2429 
2430         # Confirm the resize and check the usages
2431         post = {'confirmResize': None}
2432         self.api.post_server_action(
2433             server['id'], post, check_response_status=[204])
2434         self._wait_for_state_change(self.api, server, 'ACTIVE')
2435 
2436         self._run_periodics()
2437 
2438         # after confirm only allocations due to the new flavor should remain
2439         self.assertFlavorMatchesUsage(rp_uuid, self.flavor3)
2440 
2441         self.assertFlavorMatchesAllocation(self.flavor3, server['id'],
2442                                            rp_uuid)
2443 
2444         self._delete_and_check_allocations(server)
2445 
2446     def test_resize_not_enough_resource(self):
2447         # Try to resize to a flavor that requests more VCPU than what the
2448         # compute hosts has available and expect the resize to fail
2449 
2450         flavor_body = {'flavor':
2451                            {'name': 'test_too_big_flavor',
2452                             'ram': 1024,
2453                             'vcpus': fake.MediumFakeDriver.vcpus + 1,
2454                             'disk': 20,
2455                             }}
2456 
2457         big_flavor = self.api.post_flavor(flavor_body)
2458 
2459         dest_hostname = self.compute2.host
2460         source_hostname = self._other_hostname(dest_hostname)
2461         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2462         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2463 
2464         server = self._boot_and_check_allocations(
2465             self.flavor1, source_hostname)
2466 
2467         self.flags(allow_resize_to_same_host=False)
2468         resize_req = {
2469             'resize': {
2470                 'flavorRef': big_flavor['id']
2471             }
2472         }
2473 
2474         resp = self.api.post_server_action(
2475             server['id'], resize_req, check_response_status=[400])
2476         self.assertEqual(
2477             resp['badRequest']['message'],
2478             "No valid host was found. No valid host found for resize")
2479         server = self.admin_api.get_server(server['id'])
2480         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
2481 
2482         # only the source host shall have usages after the failed resize
2483         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2484 
2485         # Check that the other provider has no usage
2486         self.assertRequestMatchesUsage(
2487             {'VCPU': 0,
2488              'MEMORY_MB': 0,
2489              'DISK_GB': 0}, dest_rp_uuid)
2490 
2491         # Check that the server only allocates resource from the host it is
2492         # booted on
2493         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2494                                            source_rp_uuid)
2495 
2496         self._delete_and_check_allocations(server)
2497 
2498     def _wait_for_notification_event_type(self, event_type, max_retries=50):
2499         retry_counter = 0
2500         while True:
2501             if len(fake_notifier.NOTIFICATIONS) > 0:
2502                 for notification in fake_notifier.NOTIFICATIONS:
2503                     if notification.event_type == event_type:
2504                         return
2505             if retry_counter == max_retries:
2506                 self.fail('Wait for notification event type (%s) failed'
2507                           % event_type)
2508             retry_counter += 1
2509             time.sleep(0.1)
2510 
2511     def test_evacuate_with_no_compute(self):
2512         source_hostname = self.compute1.host
2513         dest_hostname = self.compute2.host
2514         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2515         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2516 
2517         # Disable compute service on destination host
2518         compute2_service_id = self.admin_api.get_services(
2519             host=dest_hostname, binary='nova-compute')[0]['id']
2520         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
2521 
2522         server = self._boot_and_check_allocations(
2523             self.flavor1, source_hostname)
2524 
2525         # Force source compute down
2526         source_compute_id = self.admin_api.get_services(
2527             host=source_hostname, binary='nova-compute')[0]['id']
2528         self.compute1.stop()
2529         self.admin_api.put_service(
2530             source_compute_id, {'forced_down': 'true'})
2531 
2532         # Initialize fake_notifier
2533         fake_notifier.stub_notifier(self)
2534         fake_notifier.reset()
2535 
2536         # Initiate evacuation
2537         post = {'evacuate': {}}
2538         self.api.post_server_action(server['id'], post)
2539 
2540         # NOTE(elod.illes): Should be changed to non-polling solution when
2541         # patch https://review.openstack.org/#/c/482629/ gets merged:
2542         # fake_notifier.wait_for_versioned_notifications(
2543         #     'compute_task.rebuild_server')
2544         self._wait_for_notification_event_type('compute_task.rebuild_server')
2545 
2546         self._run_periodics()
2547 
2548         # There is no other host to evacuate to so the rebuild should put the
2549         # VM to ERROR state, but it should remain on source compute
2550         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2551                            'status': 'ERROR'}
2552         server = self._wait_for_server_parameter(self.api, server,
2553                                                  expected_params)
2554 
2555         # Check migrations
2556         migrations = self.api.get_migrations()
2557         self.assertEqual(1, len(migrations))
2558         self.assertEqual('evacuation', migrations[0]['migration_type'])
2559         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
2560         self.assertEqual(source_hostname, migrations[0]['source_compute'])
2561         self.assertEqual('error', migrations[0]['status'])
2562 
2563         # Restart source host
2564         self.admin_api.put_service(
2565             source_compute_id, {'forced_down': 'false'})
2566         self.compute1.start()
2567 
2568         self._run_periodics()
2569 
2570         # Check allocation and usages: should only use resources on source host
2571         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2572 
2573         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2574                                            source_rp_uuid)
2575         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2576         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2577 
2578         self._delete_and_check_allocations(server)
2579 
2580     def test_migrate_no_valid_host(self):
2581         source_hostname = self.compute1.host
2582         dest_hostname = self.compute2.host
2583         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2584         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2585 
2586         server = self._boot_and_check_allocations(
2587             self.flavor1, source_hostname)
2588 
2589         dest_compute_id = self.admin_api.get_services(
2590             host=dest_hostname, binary='nova-compute')[0]['id']
2591         self.compute2.stop()
2592         # force it down to avoid waiting for the service group to time out
2593         self.admin_api.put_service(
2594             dest_compute_id, {'forced_down': 'true'})
2595 
2596         # migrate the server
2597         post = {'migrate': None}
2598         ex = self.assertRaises(client.OpenStackApiException,
2599                                self.api.post_server_action,
2600                                server['id'], post)
2601         self.assertIn('No valid host', six.text_type(ex))
2602         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
2603                            'status': 'ACTIVE'}
2604         self._wait_for_server_parameter(self.api, server, expected_params)
2605 
2606         self._run_periodics()
2607 
2608         # Expect to have allocation only on source_host
2609         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2610         zero_usage = {'VCPU': 0, 'DISK_GB': 0, 'MEMORY_MB': 0}
2611         self.assertRequestMatchesUsage(zero_usage, dest_rp_uuid)
2612 
2613         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2614                                            source_rp_uuid)
2615 
2616         self._delete_and_check_allocations(server)
2617 
2618     def test_evacuate(self):
2619         source_hostname = self.compute1.host
2620         dest_hostname = self.compute2.host
2621         server = self._boot_and_check_allocations(
2622             self.flavor1, source_hostname)
2623 
2624         source_compute_id = self.admin_api.get_services(
2625             host=source_hostname, binary='nova-compute')[0]['id']
2626 
2627         self.compute1.stop()
2628         # force it down to avoid waiting for the service group to time out
2629         self.admin_api.put_service(
2630             source_compute_id, {'forced_down': 'true'})
2631 
2632         # evacuate the server
2633         post = {'evacuate': {}}
2634         self.api.post_server_action(
2635             server['id'], post)
2636         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2637                            'status': 'ACTIVE'}
2638         server = self._wait_for_server_parameter(self.api, server,
2639                                                  expected_params)
2640 
2641         # Expect to have allocation and usages on both computes as the
2642         # source compute is still down
2643         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2644         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2645 
2646         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2647 
2648         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2649 
2650         self._check_allocation_during_evacuate(
2651             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2652 
2653         # restart the source compute
2654         self.restart_compute_service(self.compute1)
2655 
2656         self.admin_api.put_service(
2657             source_compute_id, {'forced_down': 'false'})
2658 
2659         source_usages = self._get_provider_usages(source_rp_uuid)
2660         self.assertEqual({'VCPU': 0,
2661                           'MEMORY_MB': 0,
2662                           'DISK_GB': 0},
2663                          source_usages)
2664 
2665         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2666 
2667         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2668                                            dest_rp_uuid)
2669 
2670         self._delete_and_check_allocations(server)
2671 
2672     def test_evacuate_forced_host(self):
2673         """Evacuating a server with a forced host bypasses the scheduler
2674         which means conductor has to create the allocations against the
2675         destination node. This test recreates the scenarios and asserts
2676         the allocations on the source and destination nodes are as expected.
2677         """
2678         source_hostname = self.compute1.host
2679         dest_hostname = self.compute2.host
2680 
2681         # the ability to force evacuate a server is removed entirely in 2.68
2682         self.api.microversion = '2.67'
2683 
2684         server = self._boot_and_check_allocations(
2685             self.flavor1, source_hostname)
2686 
2687         source_compute_id = self.admin_api.get_services(
2688             host=source_hostname, binary='nova-compute')[0]['id']
2689 
2690         self.compute1.stop()
2691         # force it down to avoid waiting for the service group to time out
2692         self.admin_api.put_service(
2693             source_compute_id, {'forced_down': 'true'})
2694 
2695         # evacuate the server and force the destination host which bypasses
2696         # the scheduler
2697         post = {
2698             'evacuate': {
2699                 'host': dest_hostname,
2700                 'force': True
2701             }
2702         }
2703         self.api.post_server_action(server['id'], post)
2704         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2705                            'status': 'ACTIVE'}
2706         server = self._wait_for_server_parameter(self.api, server,
2707                                                  expected_params)
2708 
2709         # Run the periodics to show those don't modify allocations.
2710         self._run_periodics()
2711 
2712         # Expect to have allocation and usages on both computes as the
2713         # source compute is still down
2714         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2715         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2716 
2717         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2718 
2719         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2720 
2721         self._check_allocation_during_evacuate(
2722             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2723 
2724         # restart the source compute
2725         self.restart_compute_service(self.compute1)
2726         self.admin_api.put_service(
2727             source_compute_id, {'forced_down': 'false'})
2728 
2729         # Run the periodics again to show they don't change anything.
2730         self._run_periodics()
2731 
2732         # When the source node starts up, the instance has moved so the
2733         # ResourceTracker should cleanup allocations for the source node.
2734         source_usages = self._get_provider_usages(source_rp_uuid)
2735         self.assertEqual(
2736             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
2737 
2738         # The usages/allocations should still exist on the destination node
2739         # after the source node starts back up.
2740         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2741 
2742         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2743                                            dest_rp_uuid)
2744 
2745         self._delete_and_check_allocations(server)
2746 
2747     def test_evacuate_forced_host_v268(self):
2748         """Evacuating a server with a forced host was removed in API
2749         microversion 2.68. This test ensures that the request is rejected.
2750         """
2751         source_hostname = self.compute1.host
2752         dest_hostname = self.compute2.host
2753 
2754         server = self._boot_and_check_allocations(
2755             self.flavor1, source_hostname)
2756 
2757         # evacuate the server and force the destination host which bypasses
2758         # the scheduler
2759         post = {
2760             'evacuate': {
2761                 'host': dest_hostname,
2762                 'force': True
2763             }
2764         }
2765         ex = self.assertRaises(client.OpenStackApiException,
2766                                self.api.post_server_action,
2767                                server['id'], post)
2768         self.assertIn("'force' was unexpected", six.text_type(ex))
2769 
2770     # NOTE(gibi): there is a similar test in SchedulerOnlyChecksTargetTest but
2771     # we want this test here as well because ServerMovingTest is a parent class
2772     # of multiple test classes that run this test case with different compute
2773     # node setups.
2774     def test_evacuate_host_specified_but_not_forced(self):
2775         """Evacuating a server with a host but using the scheduler to create
2776         the allocations against the destination node. This test recreates the
2777         scenarios and asserts the allocations on the source and destination
2778         nodes are as expected.
2779         """
2780         source_hostname = self.compute1.host
2781         dest_hostname = self.compute2.host
2782 
2783         server = self._boot_and_check_allocations(
2784             self.flavor1, source_hostname)
2785 
2786         source_compute_id = self.admin_api.get_services(
2787             host=source_hostname, binary='nova-compute')[0]['id']
2788 
2789         self.compute1.stop()
2790         # force it down to avoid waiting for the service group to time out
2791         self.admin_api.put_service(
2792             source_compute_id, {'forced_down': 'true'})
2793 
2794         # evacuate the server specify the target but do not force the
2795         # destination host to use the scheduler to validate the target host
2796         post = {
2797             'evacuate': {
2798                 'host': dest_hostname,
2799             }
2800         }
2801         self.api.post_server_action(server['id'], post)
2802         expected_params = {'OS-EXT-SRV-ATTR:host': dest_hostname,
2803                            'status': 'ACTIVE'}
2804         server = self._wait_for_server_parameter(self.api, server,
2805                                                  expected_params)
2806 
2807         # Run the periodics to show those don't modify allocations.
2808         self._run_periodics()
2809 
2810         # Expect to have allocation and usages on both computes as the
2811         # source compute is still down
2812         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2813         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2814 
2815         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2816 
2817         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2818 
2819         self._check_allocation_during_evacuate(
2820             self.flavor1, server['id'], source_rp_uuid, dest_rp_uuid)
2821 
2822         # restart the source compute
2823         self.restart_compute_service(self.compute1)
2824         self.admin_api.put_service(
2825             source_compute_id, {'forced_down': 'false'})
2826 
2827         # Run the periodics again to show they don't change anything.
2828         self._run_periodics()
2829 
2830         # When the source node starts up, the instance has moved so the
2831         # ResourceTracker should cleanup allocations for the source node.
2832         source_usages = self._get_provider_usages(source_rp_uuid)
2833         self.assertEqual(
2834             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, source_usages)
2835 
2836         # The usages/allocations should still exist on the destination node
2837         # after the source node starts back up.
2838         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2839 
2840         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2841                                            dest_rp_uuid)
2842 
2843         self._delete_and_check_allocations(server)
2844 
2845     def test_evacuate_claim_on_dest_fails(self):
2846         """Tests that the allocations on the destination node are cleaned up
2847         when the rebuild move claim fails due to insufficient resources.
2848         """
2849         source_hostname = self.compute1.host
2850         dest_hostname = self.compute2.host
2851         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2852 
2853         server = self._boot_and_check_allocations(
2854             self.flavor1, source_hostname)
2855 
2856         source_compute_id = self.admin_api.get_services(
2857             host=source_hostname, binary='nova-compute')[0]['id']
2858 
2859         self.compute1.stop()
2860         # force it down to avoid waiting for the service group to time out
2861         self.admin_api.put_service(
2862             source_compute_id, {'forced_down': 'true'})
2863 
2864         # NOTE(mriedem): This isn't great, and I'd like to fake out the driver
2865         # to make the claim fail, by doing something like returning a too high
2866         # memory_mb overhead, but the limits dict passed to the claim is empty
2867         # so the claim test is considering it as unlimited and never actually
2868         # performs a claim test. Configuring the scheduler to use the RamFilter
2869         # to get the memory_mb limit at least seems like it should work but
2870         # it doesn't appear to for some reason...
2871         def fake_move_claim(*args, **kwargs):
2872             # Assert the destination node allocation exists.
2873             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2874             raise exception.ComputeResourcesUnavailable(
2875                     reason='test_evacuate_claim_on_dest_fails')
2876 
2877         with mock.patch('nova.compute.claims.MoveClaim', fake_move_claim):
2878             # evacuate the server
2879             self.api.post_server_action(server['id'], {'evacuate': {}})
2880             # the migration will fail on the dest node and the instance will
2881             # go into error state
2882             server = self._wait_for_state_change(self.api, server, 'ERROR')
2883 
2884         # Run the periodics to show those don't modify allocations.
2885         self._run_periodics()
2886 
2887         # The allocation should still exist on the source node since it's
2888         # still down, and the allocation on the destination node should be
2889         # cleaned up.
2890         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2891 
2892         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2893 
2894         self.assertRequestMatchesUsage(
2895             {'VCPU': 0,
2896              'MEMORY_MB': 0,
2897              'DISK_GB': 0}, dest_rp_uuid)
2898 
2899         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2900                                            source_rp_uuid)
2901 
2902         # restart the source compute
2903         self.restart_compute_service(self.compute1)
2904         self.admin_api.put_service(
2905             source_compute_id, {'forced_down': 'false'})
2906 
2907         # Run the periodics again to show they don't change anything.
2908         self._run_periodics()
2909 
2910         # The source compute shouldn't have cleaned up the allocation for
2911         # itself since the instance didn't move.
2912         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2913 
2914         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2915                                            source_rp_uuid)
2916 
2917     def test_evacuate_rebuild_on_dest_fails(self):
2918         """Tests that the allocations on the destination node are cleaned up
2919         automatically when the claim is made but the actual rebuild
2920         via the driver fails.
2921 
2922         """
2923         source_hostname = self.compute1.host
2924         dest_hostname = self.compute2.host
2925         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
2926 
2927         server = self._boot_and_check_allocations(
2928             self.flavor1, source_hostname)
2929 
2930         source_compute_id = self.admin_api.get_services(
2931             host=source_hostname, binary='nova-compute')[0]['id']
2932 
2933         self.compute1.stop()
2934         # force it down to avoid waiting for the service group to time out
2935         self.admin_api.put_service(
2936             source_compute_id, {'forced_down': 'true'})
2937 
2938         def fake_rebuild(*args, **kwargs):
2939             # Assert the destination node allocation exists.
2940             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
2941             raise test.TestingException('test_evacuate_rebuild_on_dest_fails')
2942 
2943         with mock.patch.object(
2944                 self.compute2.driver, 'rebuild', fake_rebuild):
2945             # evacuate the server
2946             self.api.post_server_action(server['id'], {'evacuate': {}})
2947             # the migration will fail on the dest node and the instance will
2948             # go into error state
2949             server = self._wait_for_state_change(self.api, server, 'ERROR')
2950 
2951         # Run the periodics to show those don't modify allocations.
2952         self._run_periodics()
2953 
2954         # The allocation should still exist on the source node since it's
2955         # still down, and the allocation on the destination node should be
2956         # cleaned up.
2957         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
2958 
2959         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2960 
2961         self.assertRequestMatchesUsage(
2962             {'VCPU': 0,
2963              'MEMORY_MB': 0,
2964              'DISK_GB': 0}, dest_rp_uuid)
2965 
2966         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2967                                            source_rp_uuid)
2968 
2969         # restart the source compute
2970         self.restart_compute_service(self.compute1)
2971         self.admin_api.put_service(
2972             source_compute_id, {'forced_down': 'false'})
2973 
2974         # Run the periodics again to show they don't change anything.
2975         self._run_periodics()
2976 
2977         # The source compute shouldn't have cleaned up the allocation for
2978         # itself since the instance didn't move.
2979         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
2980 
2981         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
2982                                            source_rp_uuid)
2983 
2984     def _boot_then_shelve_and_check_allocations(self, hostname, rp_uuid):
2985         # avoid automatic shelve offloading
2986         self.flags(shelved_offload_time=-1)
2987         server = self._boot_and_check_allocations(
2988             self.flavor1, hostname)
2989         req = {
2990             'shelve': {}
2991         }
2992         self.api.post_server_action(server['id'], req)
2993         self._wait_for_state_change(self.api, server, 'SHELVED')
2994         # the host should maintain the existing allocation for this instance
2995         # while the instance is shelved
2996         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
2997         # Check that the server only allocates resource from the host it is
2998         # booted on
2999         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3000                                            rp_uuid)
3001         return server
3002 
3003     def test_shelve_unshelve(self):
3004         source_hostname = self.compute1.host
3005         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3006         server = self._boot_then_shelve_and_check_allocations(
3007             source_hostname, source_rp_uuid)
3008 
3009         req = {
3010             'unshelve': {}
3011         }
3012         self.api.post_server_action(server['id'], req)
3013         self._wait_for_state_change(self.api, server, 'ACTIVE')
3014 
3015         # the host should have resource usage as the instance is ACTIVE
3016         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3017 
3018         # Check that the server only allocates resource from the host it is
3019         # booted on
3020         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3021                                            source_rp_uuid)
3022 
3023         self._delete_and_check_allocations(server)
3024 
3025     def _shelve_offload_and_check_allocations(self, server, source_rp_uuid):
3026         req = {
3027             'shelveOffload': {}
3028         }
3029         self.api.post_server_action(server['id'], req)
3030         self._wait_for_server_parameter(
3031             self.api, server, {'status': 'SHELVED_OFFLOADED',
3032                                'OS-EXT-SRV-ATTR:host': None,
3033                                'OS-EXT-AZ:availability_zone': ''})
3034         source_usages = self._get_provider_usages(source_rp_uuid)
3035         self.assertEqual({'VCPU': 0,
3036                           'MEMORY_MB': 0,
3037                           'DISK_GB': 0},
3038                          source_usages)
3039 
3040         allocations = self._get_allocations_by_server_uuid(server['id'])
3041         self.assertEqual(0, len(allocations))
3042 
3043     def test_shelve_offload_unshelve_diff_host(self):
3044         source_hostname = self.compute1.host
3045         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3046         server = self._boot_then_shelve_and_check_allocations(
3047             source_hostname, source_rp_uuid)
3048 
3049         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
3050 
3051         # unshelve after shelve offload will do scheduling. this test case
3052         # wants to test the scenario when the scheduler select a different host
3053         # to ushelve the instance. So we disable the original host.
3054         source_service_id = self.admin_api.get_services(
3055             host=source_hostname, binary='nova-compute')[0]['id']
3056         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3057 
3058         req = {
3059             'unshelve': {}
3060         }
3061         self.api.post_server_action(server['id'], req)
3062         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3063         # unshelving an offloaded instance will call the scheduler so the
3064         # instance might end up on a different host
3065         current_hostname = server['OS-EXT-SRV-ATTR:host']
3066         self.assertEqual(current_hostname, self._other_hostname(
3067             source_hostname))
3068 
3069         # the host running the instance should have resource usage
3070         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
3071         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
3072 
3073         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3074                                            current_rp_uuid)
3075 
3076         self._delete_and_check_allocations(server)
3077 
3078     def test_shelve_offload_unshelve_same_host(self):
3079         source_hostname = self.compute1.host
3080         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3081         server = self._boot_then_shelve_and_check_allocations(
3082             source_hostname, source_rp_uuid)
3083 
3084         self._shelve_offload_and_check_allocations(server, source_rp_uuid)
3085 
3086         # unshelve after shelve offload will do scheduling. this test case
3087         # wants to test the scenario when the scheduler select the same host
3088         # to ushelve the instance. So we disable the other host.
3089         source_service_id = self.admin_api.get_services(
3090             host=self._other_hostname(source_hostname),
3091             binary='nova-compute')[0]['id']
3092         self.admin_api.put_service(source_service_id, {'status': 'disabled'})
3093 
3094         req = {
3095             'unshelve': {}
3096         }
3097         self.api.post_server_action(server['id'], req)
3098         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
3099         # unshelving an offloaded instance will call the scheduler so the
3100         # instance might end up on a different host
3101         current_hostname = server['OS-EXT-SRV-ATTR:host']
3102         self.assertEqual(current_hostname, source_hostname)
3103 
3104         # the host running the instance should have resource usage
3105         current_rp_uuid = self._get_provider_uuid_by_host(current_hostname)
3106         self.assertFlavorMatchesUsage(current_rp_uuid, self.flavor1)
3107 
3108         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3109                                            current_rp_uuid)
3110 
3111         self._delete_and_check_allocations(server)
3112 
3113     def test_live_migrate_force(self):
3114         source_hostname = self.compute1.host
3115         dest_hostname = self.compute2.host
3116         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3117         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3118 
3119         # the ability to force live migrate a server is removed entirely in
3120         # 2.68
3121         self.api.microversion = '2.67'
3122 
3123         server = self._boot_and_check_allocations(
3124             self.flavor1, source_hostname)
3125 
3126         # live migrate the server and force the destination host which bypasses
3127         # the scheduler
3128         post = {
3129             'os-migrateLive': {
3130                 'host': dest_hostname,
3131                 'block_migration': True,
3132                 'force': True,
3133             }
3134         }
3135 
3136         self.api.post_server_action(server['id'], post)
3137         self._wait_for_server_parameter(self.api, server,
3138             {'OS-EXT-SRV-ATTR:host': dest_hostname,
3139              'status': 'ACTIVE'})
3140 
3141         self._run_periodics()
3142 
3143         # NOTE(danms): There should be no usage for the source
3144         self.assertRequestMatchesUsage(
3145             {'VCPU': 0,
3146              'MEMORY_MB': 0,
3147              'DISK_GB': 0}, source_rp_uuid)
3148 
3149         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3150 
3151         # the server has an allocation on only the dest node
3152         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3153                                            dest_rp_uuid)
3154 
3155         self._delete_and_check_allocations(server)
3156 
3157     def test_live_migrate_forced_v268(self):
3158         """Live migrating a server with a forced host was removed in API
3159         microversion 2.68. This test ensures that the request is rejected.
3160         """
3161         source_hostname = self.compute1.host
3162         dest_hostname = self.compute2.host
3163 
3164         server = self._boot_and_check_allocations(
3165             self.flavor1, source_hostname)
3166 
3167         # live migrate the server and force the destination host which bypasses
3168         # the scheduler
3169         post = {
3170             'os-migrateLive': {
3171                 'host': dest_hostname,
3172                 'block_migration': True,
3173                 'force': True,
3174             }
3175         }
3176 
3177         ex = self.assertRaises(client.OpenStackApiException,
3178                                self.api.post_server_action,
3179                                server['id'], post)
3180         self.assertIn("'force' was unexpected", six.text_type(ex))
3181 
3182     def test_live_migrate(self):
3183         source_hostname = self.compute1.host
3184         dest_hostname = self.compute2.host
3185         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3186         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3187 
3188         server = self._boot_and_check_allocations(
3189             self.flavor1, source_hostname)
3190         post = {
3191             'os-migrateLive': {
3192                 'host': dest_hostname,
3193                 'block_migration': True,
3194             }
3195         }
3196 
3197         self.api.post_server_action(server['id'], post)
3198         self._wait_for_server_parameter(self.api, server,
3199                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3200                                          'status': 'ACTIVE'})
3201 
3202         self._run_periodics()
3203 
3204         # NOTE(danms): There should be no usage for the source
3205         self.assertRequestMatchesUsage(
3206             {'VCPU': 0,
3207              'MEMORY_MB': 0,
3208              'DISK_GB': 0}, source_rp_uuid)
3209 
3210         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3211 
3212         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3213                                            dest_rp_uuid)
3214 
3215         self._delete_and_check_allocations(server)
3216 
3217     def test_live_migrate_pre_check_fails(self):
3218         """Tests the case that the LiveMigrationTask in conductor has
3219         called the scheduler which picked a host and created allocations
3220         against it in Placement, but then when the conductor task calls
3221         check_can_live_migrate_destination on the destination compute it
3222         fails. The allocations on the destination compute node should be
3223         cleaned up before the conductor task asks the scheduler for another
3224         host to try the live migration.
3225         """
3226         self.failed_hostname = None
3227         source_hostname = self.compute1.host
3228         dest_hostname = self.compute2.host
3229         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3230         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3231 
3232         server = self._boot_and_check_allocations(
3233             self.flavor1, source_hostname)
3234 
3235         def fake_check_can_live_migrate_destination(
3236                 context, instance, src_compute_info, dst_compute_info,
3237                 block_migration=False, disk_over_commit=False):
3238             self.failed_hostname = dst_compute_info['host']
3239             raise exception.MigrationPreCheckError(
3240                 reason='test_live_migrate_pre_check_fails')
3241 
3242         with mock.patch('nova.virt.fake.FakeDriver.'
3243                         'check_can_live_migrate_destination',
3244                         side_effect=fake_check_can_live_migrate_destination):
3245             post = {
3246                 'os-migrateLive': {
3247                     'host': dest_hostname,
3248                     'block_migration': True,
3249                 }
3250             }
3251             self.api.post_server_action(server['id'], post)
3252             # As there are only two computes and we failed to live migrate to
3253             # the only other destination host, the LiveMigrationTask raises
3254             # MaxRetriesExceeded back to the conductor manager which handles it
3255             # generically and sets the instance back to ACTIVE status and
3256             # clears the task_state. The migration record status is set to
3257             # 'error', so that's what we need to look for to know when this
3258             # is done.
3259             migration = self._wait_for_migration_status(server, ['error'])
3260 
3261         # The source_compute should be set on the migration record, but the
3262         # destination shouldn't be as we never made it to one.
3263         self.assertEqual(source_hostname, migration['source_compute'])
3264         self.assertIsNone(migration['dest_compute'])
3265         # Make sure the destination host (the only other host) is the failed
3266         # host.
3267         self.assertEqual(dest_hostname, self.failed_hostname)
3268 
3269         # Since the instance didn't move, assert the allocations are still
3270         # on the source node.
3271         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3272 
3273         # Assert the allocations, created by the scheduler, are cleaned up
3274         # after the migration pre-check error happens.
3275         self.assertRequestMatchesUsage(
3276             {'VCPU': 0,
3277              'MEMORY_MB': 0,
3278              'DISK_GB': 0}, dest_rp_uuid)
3279 
3280         # There should only be 1 allocation for the instance on the source node
3281         self.assertFlavorMatchesAllocation(
3282             self.flavor1, server['id'], source_rp_uuid)
3283 
3284         self._delete_and_check_allocations(server)
3285 
3286     @mock.patch('nova.virt.fake.FakeDriver.pre_live_migration')
3287     def test_live_migrate_rollback_cleans_dest_node_allocations(
3288             self, mock_pre_live_migration, force=False):
3289         """Tests the case that when live migration fails, either during the
3290         call to pre_live_migration on the destination, or during the actual
3291         live migration in the virt driver, the allocations on the destination
3292         node are rolled back since the instance is still on the source node.
3293         """
3294         source_hostname = self.compute1.host
3295         dest_hostname = self.compute2.host
3296         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3297         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3298 
3299         # the ability to force live migrate a server is removed entirely in
3300         # 2.68
3301         self.api.microversion = '2.67'
3302 
3303         server = self._boot_and_check_allocations(
3304             self.flavor1, source_hostname)
3305 
3306         def stub_pre_live_migration(context, instance, block_device_info,
3307                                     network_info, disk_info, migrate_data):
3308             # Make sure the source node allocations are against the migration
3309             # record and the dest node allocations are against the instance.
3310             self.assertFlavorMatchesAllocation(
3311                 self.flavor1, migrate_data.migration.uuid, source_rp_uuid)
3312 
3313             self.assertFlavorMatchesAllocation(
3314                 self.flavor1, server['id'], dest_rp_uuid)
3315             # The actual type of exception here doesn't matter. The point
3316             # is that the virt driver raised an exception from the
3317             # pre_live_migration method on the destination host.
3318             raise test.TestingException(
3319                 'test_live_migrate_rollback_cleans_dest_node_allocations')
3320 
3321         mock_pre_live_migration.side_effect = stub_pre_live_migration
3322 
3323         post = {
3324             'os-migrateLive': {
3325                 'host': dest_hostname,
3326                 'block_migration': True,
3327                 'force': force
3328             }
3329         }
3330         self.api.post_server_action(server['id'], post)
3331         # The compute manager will put the migration record into error status
3332         # when pre_live_migration fails, so wait for that to happen.
3333         migration = self._wait_for_migration_status(server, ['error'])
3334         # The _rollback_live_migration method in the compute manager will reset
3335         # the task_state on the instance, so wait for that to happen.
3336         server = self._wait_for_server_parameter(
3337             self.api, server, {'OS-EXT-STS:task_state': None})
3338 
3339         self.assertEqual(source_hostname, migration['source_compute'])
3340         self.assertEqual(dest_hostname, migration['dest_compute'])
3341 
3342         # Since the instance didn't move, assert the allocations are still
3343         # on the source node.
3344         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3345 
3346         # Assert the allocations, created by the scheduler, are cleaned up
3347         # after the rollback happens.
3348         self.assertRequestMatchesUsage(
3349             {'VCPU': 0,
3350              'MEMORY_MB': 0,
3351              'DISK_GB': 0}, dest_rp_uuid)
3352 
3353         # There should only be 1 allocation for the instance on the source node
3354         self.assertFlavorMatchesAllocation(
3355             self.flavor1, server['id'], source_rp_uuid)
3356 
3357         self._delete_and_check_allocations(server)
3358 
3359     def test_live_migrate_rollback_cleans_dest_node_allocations_forced(self):
3360         """Tests the case that when a forced host live migration fails, either
3361         during the call to pre_live_migration on the destination, or during
3362         the actual live migration in the virt driver, the allocations on the
3363         destination node are rolled back since the instance is still on the
3364         source node.
3365         """
3366         self.test_live_migrate_rollback_cleans_dest_node_allocations(
3367             force=True)
3368 
3369     def test_rescheduling_when_migrating_instance(self):
3370         """Tests that allocations are removed from the destination node by
3371         the compute service when a cold migrate / resize fails and a reschedule
3372         request is sent back to conductor.
3373         """
3374         source_hostname = self.compute1.manager.host
3375         server = self._boot_and_check_allocations(
3376             self.flavor1, source_hostname)
3377 
3378         def fake_prep_resize(*args, **kwargs):
3379             dest_hostname = self._other_hostname(source_hostname)
3380             dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3381             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3382             allocations = self._get_allocations_by_server_uuid(server['id'])
3383             self.assertIn(dest_rp_uuid, allocations)
3384 
3385             source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3386             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3387             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
3388             allocations = self._get_allocations_by_server_uuid(migration_uuid)
3389             self.assertIn(source_rp_uuid, allocations)
3390 
3391             raise test.TestingException('Simulated _prep_resize failure.')
3392 
3393         # Yes this isn't great in a functional test, but it's simple.
3394         self.stub_out('nova.compute.manager.ComputeManager._prep_resize',
3395                       fake_prep_resize)
3396 
3397         # Now migrate the server which is going to fail on the destination.
3398         self.api.post_server_action(server['id'], {'migrate': None})
3399 
3400         self._wait_for_action_fail_completion(
3401             server, instance_actions.MIGRATE, 'compute_prep_resize')
3402 
3403         dest_hostname = self._other_hostname(source_hostname)
3404         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3405 
3406         # Expects no allocation records on the failed host.
3407         self.assertRequestMatchesUsage(
3408             {'VCPU': 0,
3409              'MEMORY_MB': 0,
3410              'DISK_GB': 0}, dest_rp_uuid)
3411 
3412         # Ensure the allocation records still exist on the source host.
3413         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3414         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3415         allocations = self._get_allocations_by_server_uuid(server['id'])
3416         self.assertIn(source_rp_uuid, allocations)
3417 
3418     def _test_resize_to_same_host_instance_fails(self, failing_method,
3419                                                  event_name):
3420         """Tests that when we resize to the same host and resize fails in
3421         the given method, we cleanup the allocations before rescheduling.
3422         """
3423         # make sure that the test only uses a single host
3424         compute2_service_id = self.admin_api.get_services(
3425             host=self.compute2.host, binary='nova-compute')[0]['id']
3426         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
3427 
3428         hostname = self.compute1.manager.host
3429         rp_uuid = self._get_provider_uuid_by_host(hostname)
3430 
3431         server = self._boot_and_check_allocations(self.flavor1, hostname)
3432 
3433         def fake_resize_method(*args, **kwargs):
3434             # Ensure the allocations are doubled now before we fail.
3435             self.assertFlavorMatchesUsage(rp_uuid, self.flavor1, self.flavor2)
3436             raise test.TestingException('Simulated resize failure.')
3437 
3438         # Yes this isn't great in a functional test, but it's simple.
3439         self.stub_out(
3440             'nova.compute.manager.ComputeManager.%s' % failing_method,
3441             fake_resize_method)
3442 
3443         self.flags(allow_resize_to_same_host=True)
3444         resize_req = {
3445             'resize': {
3446                 'flavorRef': self.flavor2['id']
3447             }
3448         }
3449         self.api.post_server_action(server['id'], resize_req)
3450 
3451         self._wait_for_action_fail_completion(
3452             server, instance_actions.RESIZE, event_name)
3453 
3454         # Ensure the allocation records still exist on the host.
3455         source_rp_uuid = self._get_provider_uuid_by_host(hostname)
3456         # The new_flavor should have been subtracted from the doubled
3457         # allocation which just leaves us with the original flavor.
3458         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3459 
3460     def test_resize_to_same_host_prep_resize_fails(self):
3461         self._test_resize_to_same_host_instance_fails(
3462             '_prep_resize', 'compute_prep_resize')
3463 
3464     def test_resize_instance_fails_allocation_cleanup(self):
3465         self._test_resize_to_same_host_instance_fails(
3466             '_resize_instance', 'compute_resize_instance')
3467 
3468     def test_finish_resize_fails_allocation_cleanup(self):
3469         self._test_resize_to_same_host_instance_fails(
3470             '_finish_resize', 'compute_finish_resize')
3471 
3472     def _test_resize_reschedule_uses_host_lists(self, fails, num_alts=None):
3473         """Test that when a resize attempt fails, the retry comes from the
3474         supplied host_list, and does not call the scheduler.
3475         """
3476         server_req = self._build_minimal_create_server_request(
3477                 self.api, "some-server", flavor_id=self.flavor1["id"],
3478                 image_uuid="155d900f-4e14-4e4c-a73d-069cbf4541e6",
3479                 networks='none')
3480 
3481         created_server = self.api.post_server({"server": server_req})
3482         server = self._wait_for_state_change(self.api, created_server,
3483                 "ACTIVE")
3484         inst_host = server["OS-EXT-SRV-ATTR:host"]
3485         uuid_orig = self._get_provider_uuid_by_host(inst_host)
3486 
3487         # We will need four new compute nodes to test the resize, representing
3488         # the host selected by select_destinations(), along with 3 alternates.
3489         self._start_compute(host="selection")
3490         self._start_compute(host="alt_host1")
3491         self._start_compute(host="alt_host2")
3492         self._start_compute(host="alt_host3")
3493         uuid_sel = self._get_provider_uuid_by_host("selection")
3494         uuid_alt1 = self._get_provider_uuid_by_host("alt_host1")
3495         uuid_alt2 = self._get_provider_uuid_by_host("alt_host2")
3496         uuid_alt3 = self._get_provider_uuid_by_host("alt_host3")
3497         hosts = [{"name": "selection", "uuid": uuid_sel},
3498                  {"name": "alt_host1", "uuid": uuid_alt1},
3499                  {"name": "alt_host2", "uuid": uuid_alt2},
3500                  {"name": "alt_host3", "uuid": uuid_alt3},
3501                 ]
3502 
3503         self.flags(weight_classes=[__name__ + '.AltHostWeigher'],
3504                    group='filter_scheduler')
3505         self.scheduler_service.stop()
3506         self.scheduler_service = self.start_service('scheduler')
3507 
3508         def fake_prep_resize(*args, **kwargs):
3509             if self.num_fails < fails:
3510                 self.num_fails += 1
3511                 raise Exception("fake_prep_resize")
3512             actual_prep_resize(*args, **kwargs)
3513 
3514         # Yes this isn't great in a functional test, but it's simple.
3515         actual_prep_resize = compute_manager.ComputeManager._prep_resize
3516         self.stub_out("nova.compute.manager.ComputeManager._prep_resize",
3517                       fake_prep_resize)
3518         self.num_fails = 0
3519         num_alts = 4 if num_alts is None else num_alts
3520         # Make sure we have enough retries available for the number of
3521         # requested fails.
3522         attempts = min(fails + 2, num_alts)
3523         self.flags(max_attempts=attempts, group='scheduler')
3524         server_uuid = server["id"]
3525         data = {"resize": {"flavorRef": self.flavor2["id"]}}
3526         self.api.post_server_action(server_uuid, data)
3527 
3528         if num_alts < fails:
3529             # We will run out of alternates before populate_retry will
3530             # raise a MaxRetriesExceeded exception, so the migration will
3531             # fail and the server should be in status "ERROR"
3532             server = self._wait_for_state_change(self.api, created_server,
3533                     "ERROR")
3534             # The usage should be unchanged from the original flavor
3535             self.assertFlavorMatchesUsage(uuid_orig, self.flavor1)
3536             # There should be no usages on any of the hosts
3537             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3538             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3539             for target_uuid in target_uuids:
3540                 usage = self._get_provider_usages(target_uuid)
3541                 self.assertEqual(empty_usage, usage)
3542         else:
3543             server = self._wait_for_state_change(self.api, created_server,
3544                     "VERIFY_RESIZE")
3545             # Verify that the selected host failed, and was rescheduled to
3546             # an alternate host.
3547             new_server_host = server.get("OS-EXT-SRV-ATTR:host")
3548             expected_host = hosts[fails]["name"]
3549             self.assertEqual(expected_host, new_server_host)
3550             uuid_dest = hosts[fails]["uuid"]
3551             # The usage should match the resized flavor
3552             self.assertFlavorMatchesUsage(uuid_dest, self.flavor2)
3553             # Verify that the other host have no allocations
3554             target_uuids = (uuid_sel, uuid_alt1, uuid_alt2, uuid_alt3)
3555             empty_usage = {"VCPU": 0, "MEMORY_MB": 0, "DISK_GB": 0}
3556             for target_uuid in target_uuids:
3557                 if target_uuid == uuid_dest:
3558                     continue
3559                 usage = self._get_provider_usages(target_uuid)
3560                 self.assertEqual(empty_usage, usage)
3561 
3562             # Verify that there is only one migration record for the instance.
3563             ctxt = context.get_admin_context()
3564             filters = {"instance_uuid": server["id"]}
3565             migrations = objects.MigrationList.get_by_filters(ctxt, filters)
3566             self.assertEqual(1, len(migrations.objects))
3567 
3568     def test_resize_reschedule_uses_host_lists_1_fail(self):
3569         self._test_resize_reschedule_uses_host_lists(fails=1)
3570 
3571     def test_resize_reschedule_uses_host_lists_3_fails(self):
3572         self._test_resize_reschedule_uses_host_lists(fails=3)
3573 
3574     def test_resize_reschedule_uses_host_lists_not_enough_alts(self):
3575         self._test_resize_reschedule_uses_host_lists(fails=3, num_alts=1)
3576 
3577     def test_migrate_confirm(self):
3578         source_hostname = self.compute1.host
3579         dest_hostname = self.compute2.host
3580         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3581         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3582 
3583         server = self._boot_and_check_allocations(
3584             self.flavor1, source_hostname)
3585 
3586         self._migrate_and_check_allocations(
3587             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3588 
3589         # Confirm the move and check the usages
3590         post = {'confirmResize': None}
3591         self.api.post_server_action(
3592             server['id'], post, check_response_status=[204])
3593         self._wait_for_state_change(self.api, server, 'ACTIVE')
3594 
3595         def _check_allocation():
3596             # the target host usage should be according to the flavor
3597             self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3598             # the source host has no usage
3599             self.assertRequestMatchesUsage({'VCPU': 0,
3600                                             'MEMORY_MB': 0,
3601                                             'DISK_GB': 0}, source_rp_uuid)
3602 
3603             # and the target host allocation should be according to the flavor
3604             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3605                                                dest_rp_uuid)
3606 
3607         # After confirming, we should have an allocation only on the
3608         # destination host
3609         _check_allocation()
3610         self._run_periodics()
3611 
3612         # Check we're still accurate after running the periodics
3613         _check_allocation()
3614 
3615         self._delete_and_check_allocations(server)
3616 
3617     def test_migrate_revert(self):
3618         source_hostname = self.compute1.host
3619         dest_hostname = self.compute2.host
3620         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3621         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3622 
3623         server = self._boot_and_check_allocations(
3624             self.flavor1, source_hostname)
3625 
3626         self._migrate_and_check_allocations(
3627             server, self.flavor1, source_rp_uuid, dest_rp_uuid)
3628 
3629         # Revert the move and check the usages
3630         post = {'revertResize': None}
3631         self.api.post_server_action(server['id'], post)
3632         self._wait_for_state_change(self.api, server, 'ACTIVE')
3633 
3634         def _check_allocation():
3635             self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3636             self.assertRequestMatchesUsage({'VCPU': 0,
3637                                             'MEMORY_MB': 0,
3638                                             'DISK_GB': 0}, dest_rp_uuid)
3639 
3640             # Check that the server only allocates resource from the original
3641             # host
3642             self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3643                                                source_rp_uuid)
3644 
3645         # the original host expected to have the old resource allocation
3646         _check_allocation()
3647         self._run_periodics()
3648         _check_allocation()
3649 
3650         self._delete_and_check_allocations(server)
3651 
3652 
3653 class ServerLiveMigrateForceAndAbort(
3654         integrated_helpers.ProviderUsageBaseTestCase):
3655     """Test Server live migrations, which delete the migration or
3656     force_complete it, and check the allocations after the operations.
3657 
3658     The test are using fakedriver to handle the force_completion and deletion
3659     of live migration.
3660     """
3661 
3662     compute_driver = 'fake.FakeLiveMigrateDriver'
3663 
3664     def setUp(self):
3665         super(ServerLiveMigrateForceAndAbort, self).setUp()
3666 
3667         self.compute1 = self._start_compute(host='host1')
3668         self.compute2 = self._start_compute(host='host2')
3669 
3670         flavors = self.api.get_flavors()
3671         self.flavor1 = flavors[0]
3672 
3673     def test_live_migrate_force_complete(self):
3674         source_hostname = self.compute1.host
3675         dest_hostname = self.compute2.host
3676         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3677         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3678 
3679         server = self._boot_and_check_allocations(
3680             self.flavor1, source_hostname)
3681 
3682         post = {
3683             'os-migrateLive': {
3684                 'host': dest_hostname,
3685                 'block_migration': True,
3686             }
3687         }
3688         self.api.post_server_action(server['id'], post)
3689 
3690         migration = self._wait_for_migration_status(server, ['running'])
3691         self.api.force_complete_migration(server['id'],
3692                                           migration['id'])
3693 
3694         self._wait_for_server_parameter(self.api, server,
3695                                         {'OS-EXT-SRV-ATTR:host': dest_hostname,
3696                                          'status': 'ACTIVE'})
3697 
3698         self._run_periodics()
3699 
3700         self.assertRequestMatchesUsage(
3701             {'VCPU': 0,
3702              'MEMORY_MB': 0,
3703              'DISK_GB': 0}, source_rp_uuid)
3704 
3705         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3706         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3707                                            dest_rp_uuid)
3708 
3709         self._delete_and_check_allocations(server)
3710 
3711     def test_live_migrate_delete(self):
3712         source_hostname = self.compute1.host
3713         dest_hostname = self.compute2.host
3714         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
3715         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3716 
3717         server = self._boot_and_check_allocations(
3718             self.flavor1, source_hostname)
3719 
3720         post = {
3721             'os-migrateLive': {
3722                 'host': dest_hostname,
3723                 'block_migration': True,
3724             }
3725         }
3726         self.api.post_server_action(server['id'], post)
3727 
3728         migration = self._wait_for_migration_status(server, ['running'])
3729 
3730         self.api.delete_migration(server['id'], migration['id'])
3731         self._wait_for_server_parameter(self.api, server,
3732             {'OS-EXT-SRV-ATTR:host': source_hostname,
3733              'status': 'ACTIVE'})
3734 
3735         self._run_periodics()
3736 
3737         allocations = self._get_allocations_by_server_uuid(server['id'])
3738         self.assertNotIn(dest_rp_uuid, allocations)
3739 
3740         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
3741 
3742         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
3743                                            source_rp_uuid)
3744 
3745         self.assertRequestMatchesUsage({'VCPU': 0,
3746                                         'MEMORY_MB': 0,
3747                                         'DISK_GB': 0}, dest_rp_uuid)
3748 
3749         self._delete_and_check_allocations(server)
3750 
3751 
3752 class ServerLiveMigrateForceAndAbortWithNestedResourcesRequest(
3753         ServerLiveMigrateForceAndAbort):
3754     compute_driver = 'fake.FakeLiveMigrateDriverWithNestedCustomResources'
3755 
3756     def setUp(self):
3757         super(ServerLiveMigrateForceAndAbortWithNestedResourcesRequest,
3758               self).setUp()
3759         # modify the flavor used in the test base class to require one piece of
3760         # CUSTOM_MAGIC resource as well.
3761 
3762         self.api.post_extra_spec(
3763             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3764         # save the extra_specs in the flavor stored in the test case as
3765         # well
3766         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3767 
3768 
3769 class ServerRescheduleTests(integrated_helpers.ProviderUsageBaseTestCase):
3770     """Tests server create scenarios which trigger a reschedule during
3771     a server build and validates that allocations in Placement
3772     are properly cleaned up.
3773 
3774     Uses a fake virt driver that fails the build on the first attempt.
3775     """
3776 
3777     compute_driver = 'fake.FakeRescheduleDriver'
3778 
3779     def setUp(self):
3780         super(ServerRescheduleTests, self).setUp()
3781         self.compute1 = self._start_compute(host='host1')
3782         self.compute2 = self._start_compute(host='host2')
3783 
3784         flavors = self.api.get_flavors()
3785         self.flavor1 = flavors[0]
3786 
3787     def _other_hostname(self, host):
3788         other_host = {'host1': 'host2',
3789                       'host2': 'host1'}
3790         return other_host[host]
3791 
3792     def test_rescheduling_when_booting_instance(self):
3793         """Tests that allocations, created by the scheduler, are cleaned
3794         from the source node when the build fails on that node and is
3795         rescheduled to another node.
3796         """
3797         server_req = self._build_minimal_create_server_request(
3798                 self.api, 'some-server', flavor_id=self.flavor1['id'],
3799                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3800                 networks='none')
3801 
3802         created_server = self.api.post_server({'server': server_req})
3803         server = self._wait_for_state_change(
3804                 self.api, created_server, 'ACTIVE')
3805         dest_hostname = server['OS-EXT-SRV-ATTR:host']
3806         failed_hostname = self._other_hostname(dest_hostname)
3807 
3808         LOG.info('failed on %s', failed_hostname)
3809         LOG.info('booting on %s', dest_hostname)
3810 
3811         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
3812         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
3813 
3814         # Expects no allocation records on the failed host.
3815         self.assertRequestMatchesUsage(
3816             {'VCPU': 0,
3817              'MEMORY_MB': 0,
3818              'DISK_GB': 0}, failed_rp_uuid)
3819 
3820         # Ensure the allocation records on the destination host.
3821         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor1)
3822 
3823 
3824 class ServerRescheduleTestsWithNestedResourcesRequest(ServerRescheduleTests):
3825     compute_driver = 'fake.FakeRescheduleDriverWithNestedCustomResources'
3826 
3827     def setUp(self):
3828         super(ServerRescheduleTestsWithNestedResourcesRequest, self).setUp()
3829         # modify the flavor used in the test base class to require one piece of
3830         # CUSTOM_MAGIC resource as well.
3831 
3832         self.api.post_extra_spec(
3833             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3834         # save the extra_specs in the flavor stored in the test case as
3835         # well
3836         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3837 
3838 
3839 class ServerBuildAbortTests(integrated_helpers.ProviderUsageBaseTestCase):
3840     """Tests server create scenarios which trigger a build abort during
3841     a server build and validates that allocations in Placement
3842     are properly cleaned up.
3843 
3844     Uses a fake virt driver that aborts the build on the first attempt.
3845     """
3846 
3847     compute_driver = 'fake.FakeBuildAbortDriver'
3848 
3849     def setUp(self):
3850         super(ServerBuildAbortTests, self).setUp()
3851         # We only need one compute service/host/node for these tests.
3852         self.compute1 = self._start_compute(host='host1')
3853 
3854         flavors = self.api.get_flavors()
3855         self.flavor1 = flavors[0]
3856 
3857     def test_abort_when_booting_instance(self):
3858         """Tests that allocations, created by the scheduler, are cleaned
3859         from the source node when the build is aborted on that node.
3860         """
3861         server_req = self._build_minimal_create_server_request(
3862                 self.api, 'some-server', flavor_id=self.flavor1['id'],
3863                 image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3864                 networks='none')
3865 
3866         created_server = self.api.post_server({'server': server_req})
3867         self._wait_for_state_change(self.api, created_server, 'ERROR')
3868 
3869         failed_hostname = self.compute1.manager.host
3870 
3871         failed_rp_uuid = self._get_provider_uuid_by_host(failed_hostname)
3872         # Expects no allocation records on the failed host.
3873         self.assertRequestMatchesUsage({'VCPU': 0,
3874                                         'MEMORY_MB': 0,
3875                                         'DISK_GB': 0}, failed_rp_uuid)
3876 
3877 
3878 class ServerBuildAbortTestsWithNestedResourceRequest(ServerBuildAbortTests):
3879     compute_driver = 'fake.FakeBuildAbortDriverWithNestedCustomResources'
3880 
3881     def setUp(self):
3882         super(ServerBuildAbortTestsWithNestedResourceRequest, self).setUp()
3883         # modify the flavor used in the test base class to require one piece of
3884         # CUSTOM_MAGIC resource as well.
3885 
3886         self.api.post_extra_spec(
3887             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3888         # save the extra_specs in the flavor stored in the test case as
3889         # well
3890         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3891 
3892 
3893 class ServerUnshelveSpawnFailTests(
3894         integrated_helpers.ProviderUsageBaseTestCase):
3895     """Tests server unshelve scenarios which trigger a
3896     VirtualInterfaceCreateException during driver.spawn() and validates that
3897     allocations in Placement are properly cleaned up.
3898     """
3899 
3900     compute_driver = 'fake.FakeUnshelveSpawnFailDriver'
3901 
3902     def setUp(self):
3903         super(ServerUnshelveSpawnFailTests, self).setUp()
3904         # We only need one compute service/host/node for these tests.
3905         self.compute1 = self._start_compute('host1')
3906 
3907         flavors = self.api.get_flavors()
3908         self.flavor1 = flavors[0]
3909 
3910     def test_driver_spawn_fail_when_unshelving_instance(self):
3911         """Tests that allocations, created by the scheduler, are cleaned
3912         from the target node when the unshelve driver.spawn fails on that node.
3913         """
3914         hostname = self.compute1.manager.host
3915         rp_uuid = self._get_provider_uuid_by_host(hostname)
3916         # We start with no usages on the host.
3917         self.assertRequestMatchesUsage(
3918             {'VCPU': 0,
3919              'MEMORY_MB': 0,
3920              'DISK_GB': 0}, rp_uuid)
3921 
3922         server_req = self._build_minimal_create_server_request(
3923             self.api, 'unshelve-spawn-fail', flavor_id=self.flavor1['id'],
3924             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
3925             networks='none')
3926 
3927         server = self.api.post_server({'server': server_req})
3928         self._wait_for_state_change(self.api, server, 'ACTIVE')
3929 
3930         # assert allocations exist for the host
3931         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3932 
3933         # shelve offload the server
3934         self.flags(shelved_offload_time=0)
3935         self.api.post_server_action(server['id'], {'shelve': None})
3936         self._wait_for_server_parameter(
3937             self.api, server, {'status': 'SHELVED_OFFLOADED',
3938                                'OS-EXT-SRV-ATTR:host': None})
3939 
3940         # assert allocations were removed from the host
3941         self.assertRequestMatchesUsage(
3942             {'VCPU': 0,
3943              'MEMORY_MB': 0,
3944              'DISK_GB': 0}, rp_uuid)
3945 
3946         # unshelve the server, which should fail
3947         self.api.post_server_action(server['id'], {'unshelve': None})
3948         self._wait_for_action_fail_completion(
3949             server, instance_actions.UNSHELVE, 'compute_unshelve_instance')
3950 
3951         # assert allocations were removed from the host
3952         self.assertRequestMatchesUsage(
3953             {'VCPU': 0,
3954              'MEMORY_MB': 0,
3955              'DISK_GB': 0}, rp_uuid)
3956 
3957 
3958 class ServerUnshelveSpawnFailTestsWithNestedResourceRequest(
3959     ServerUnshelveSpawnFailTests):
3960     compute_driver = ('fake.'
3961                       'FakeUnshelveSpawnFailDriverWithNestedCustomResources')
3962 
3963     def setUp(self):
3964         super(ServerUnshelveSpawnFailTestsWithNestedResourceRequest,
3965               self).setUp()
3966         # modify the flavor used in the test base class to require one piece of
3967         # CUSTOM_MAGIC resource as well.
3968 
3969         self.api.post_extra_spec(
3970             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
3971         # save the extra_specs in the flavor stored in the test case as
3972         # well
3973         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
3974 
3975 
3976 class ServerSoftDeleteTests(integrated_helpers.ProviderUsageBaseTestCase):
3977 
3978     compute_driver = 'fake.SmallFakeDriver'
3979 
3980     def setUp(self):
3981         super(ServerSoftDeleteTests, self).setUp()
3982         # We only need one compute service/host/node for these tests.
3983         self.compute1 = self._start_compute('host1')
3984 
3985         flavors = self.api.get_flavors()
3986         self.flavor1 = flavors[0]
3987 
3988     def _soft_delete_and_check_allocation(self, server, hostname):
3989         self.api.delete_server(server['id'])
3990         server = self._wait_for_state_change(self.api, server, 'SOFT_DELETED')
3991 
3992         self._run_periodics()
3993 
3994         # in soft delete state nova should keep the resource allocation as
3995         # the instance can be restored
3996         rp_uuid = self._get_provider_uuid_by_host(hostname)
3997 
3998         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
3999 
4000         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4001                                            rp_uuid)
4002 
4003         # run the periodic reclaim but as time isn't advanced it should not
4004         # reclaim the instance
4005         ctxt = context.get_admin_context()
4006         self.compute1._reclaim_queued_deletes(ctxt)
4007 
4008         self._run_periodics()
4009 
4010         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4011 
4012         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4013                                            rp_uuid)
4014 
4015     def test_soft_delete_then_reclaim(self):
4016         """Asserts that the automatic reclaim of soft deleted instance cleans
4017         up the allocations in placement.
4018         """
4019 
4020         # make sure that instance will go to SOFT_DELETED state instead of
4021         # deleted immediately
4022         self.flags(reclaim_instance_interval=30)
4023 
4024         hostname = self.compute1.host
4025         rp_uuid = self._get_provider_uuid_by_host(hostname)
4026 
4027         server = self._boot_and_check_allocations(self.flavor1, hostname)
4028 
4029         self._soft_delete_and_check_allocation(server, hostname)
4030 
4031         # advance the time and run periodic reclaim, instance should be deleted
4032         # and resources should be freed
4033         the_past = timeutils.utcnow() + datetime.timedelta(hours=1)
4034         timeutils.set_time_override(override_time=the_past)
4035         self.addCleanup(timeutils.clear_time_override)
4036         ctxt = context.get_admin_context()
4037         self.compute1._reclaim_queued_deletes(ctxt)
4038 
4039         # Wait for real deletion
4040         self._wait_until_deleted(server)
4041 
4042         usages = self._get_provider_usages(rp_uuid)
4043         self.assertEqual({'VCPU': 0,
4044                           'MEMORY_MB': 0,
4045                           'DISK_GB': 0}, usages)
4046         allocations = self._get_allocations_by_server_uuid(server['id'])
4047         self.assertEqual(0, len(allocations))
4048 
4049     def test_soft_delete_then_restore(self):
4050         """Asserts that restoring a soft deleted instance keeps the proper
4051         allocation in placement.
4052         """
4053 
4054         # make sure that instance will go to SOFT_DELETED state instead of
4055         # deleted immediately
4056         self.flags(reclaim_instance_interval=30)
4057 
4058         hostname = self.compute1.host
4059         rp_uuid = self._get_provider_uuid_by_host(hostname)
4060 
4061         server = self._boot_and_check_allocations(
4062             self.flavor1, hostname)
4063 
4064         self._soft_delete_and_check_allocation(server, hostname)
4065 
4066         post = {'restore': {}}
4067         self.api.post_server_action(server['id'], post)
4068         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4069 
4070         # after restore the allocations should be kept
4071         self.assertFlavorMatchesUsage(rp_uuid, self.flavor1)
4072 
4073         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
4074                                            rp_uuid)
4075 
4076         # Now we want a real delete
4077         self.flags(reclaim_instance_interval=0)
4078         self._delete_and_check_allocations(server)
4079 
4080 
4081 class ServerSoftDeleteTestsWithNestedResourceRequest(ServerSoftDeleteTests):
4082     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
4083 
4084     def setUp(self):
4085         super(ServerSoftDeleteTestsWithNestedResourceRequest, self).setUp()
4086         # modify the flavor used in the test base class to require one piece of
4087         # CUSTOM_MAGIC resource as well.
4088 
4089         self.api.post_extra_spec(
4090             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
4091         # save the extra_specs in the flavor stored in the test case as
4092         # well
4093         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
4094 
4095 
4096 class VolumeBackedServerTest(integrated_helpers.ProviderUsageBaseTestCase):
4097     """Tests for volume-backed servers."""
4098 
4099     compute_driver = 'fake.SmallFakeDriver'
4100 
4101     def setUp(self):
4102         super(VolumeBackedServerTest, self).setUp()
4103         self.compute1 = self._start_compute('host1')
4104         self.flavor_id = self._create_flavor()
4105 
4106     def _create_flavor(self):
4107         body = {
4108             'flavor': {
4109                 'id': 'vbst',
4110                 'name': 'special',
4111                 'ram': 512,
4112                 'vcpus': 1,
4113                 'disk': 10,
4114                 'OS-FLV-EXT-DATA:ephemeral': 20,
4115                 'swap': 5 * 1024,
4116                 'rxtx_factor': 1.0,
4117                 'os-flavor-access:is_public': True,
4118             },
4119         }
4120         self.admin_api.post_flavor(body)
4121         return body['flavor']['id']
4122 
4123     def _create_server(self):
4124         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
4125             image_id = self.api.get_images()[0]['id']
4126         server_req = self._build_minimal_create_server_request(
4127             self.api, 'trait-based-server',
4128             image_uuid=image_id,
4129             flavor_id=self.flavor_id, networks='none')
4130         server = self.api.post_server({'server': server_req})
4131         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4132         return server
4133 
4134     def _create_volume_backed_server(self):
4135         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4136         volume_id = nova_fixtures.CinderFixtureNewAttachFlow.IMAGE_BACKED_VOL
4137         server_req_body = {
4138             # There is no imageRef because this is boot from volume.
4139             'server': {
4140                 'flavorRef': self.flavor_id,
4141                 'name': 'test_volume_backed',
4142                 # We don't care about networking for this test. This
4143                 # requires microversion >= 2.37.
4144                 'networks': 'none',
4145                 'block_device_mapping_v2': [{
4146                     'boot_index': 0,
4147                     'uuid': volume_id,
4148                     'source_type': 'volume',
4149                     'destination_type': 'volume'
4150                 }]
4151             }
4152         }
4153         server = self.api.post_server(server_req_body)
4154         server = self._wait_for_state_change(self.api, server, 'ACTIVE')
4155         return server
4156 
4157     def test_ephemeral_has_disk_allocation(self):
4158         server = self._create_server()
4159         allocs = self._get_allocations_by_server_uuid(server['id'])
4160         resources = list(allocs.values())[0]['resources']
4161         self.assertIn('MEMORY_MB', resources)
4162         # 10gb root, 20gb ephemeral, 5gb swap
4163         expected_usage = 35
4164         self.assertEqual(expected_usage, resources['DISK_GB'])
4165         # Ensure the compute node is reporting the correct disk usage
4166         self.assertEqual(
4167             expected_usage,
4168             self.admin_api.get_hypervisor_stats()['local_gb_used'])
4169 
4170     def test_volume_backed_no_disk_allocation(self):
4171         server = self._create_volume_backed_server()
4172         allocs = self._get_allocations_by_server_uuid(server['id'])
4173         resources = list(allocs.values())[0]['resources']
4174         self.assertIn('MEMORY_MB', resources)
4175         # 0gb root, 20gb ephemeral, 5gb swap
4176         expected_usage = 25
4177         self.assertEqual(expected_usage, resources['DISK_GB'])
4178         # Ensure the compute node is reporting the correct disk usage
4179         self.assertEqual(
4180             expected_usage,
4181             self.admin_api.get_hypervisor_stats()['local_gb_used'])
4182 
4183         # Now let's hack the RequestSpec.is_bfv field to mimic migrating an
4184         # old instance created before RequestSpec.is_bfv was set in the API,
4185         # move the instance and verify that the RequestSpec.is_bfv is set
4186         # and the instance still reports the same DISK_GB allocations as during
4187         # the initial create.
4188         ctxt = context.get_admin_context()
4189         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4190         # Make sure it's set.
4191         self.assertTrue(reqspec.is_bfv)
4192         del reqspec.is_bfv
4193         reqspec.save()
4194         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4195         # Make sure it's not set.
4196         self.assertNotIn('is_bfv', reqspec)
4197         # Now migrate the instance to another host and check the request spec
4198         # and allocations after the migration.
4199         self._start_compute('host2')
4200         self.admin_api.post_server_action(server['id'], {'migrate': None})
4201         # Wait for the server to complete the cold migration.
4202         server = self._wait_for_state_change(
4203             self.admin_api, server, 'VERIFY_RESIZE')
4204         self.assertEqual('host2', server['OS-EXT-SRV-ATTR:host'])
4205         # Confirm the cold migration and check usage and the request spec.
4206         self.api.post_server_action(server['id'], {'confirmResize': None})
4207         self._wait_for_state_change(self.api, server, 'ACTIVE')
4208         reqspec = objects.RequestSpec.get_by_instance_uuid(ctxt, server['id'])
4209         # Make sure it's set.
4210         self.assertTrue(reqspec.is_bfv)
4211         allocs = self._get_allocations_by_server_uuid(server['id'])
4212         resources = list(allocs.values())[0]['resources']
4213         self.assertEqual(expected_usage, resources['DISK_GB'])
4214 
4215         # Now shelve and unshelve the server to make sure root_gb DISK_GB
4216         # isn't reported for allocations after we unshelve the server.
4217         fake_notifier.stub_notifier(self)
4218         self.addCleanup(fake_notifier.reset)
4219         self.api.post_server_action(server['id'], {'shelve': None})
4220         self._wait_for_state_change(self.api, server, 'SHELVED_OFFLOADED')
4221         fake_notifier.wait_for_versioned_notifications(
4222                 'instance.shelve_offload.end')
4223         # The server should not have any allocations since it's not currently
4224         # hosted on any compute service.
4225         allocs = self._get_allocations_by_server_uuid(server['id'])
4226         self.assertDictEqual({}, allocs)
4227         # Now unshelve the server and make sure there are still no DISK_GB
4228         # allocations for the root disk.
4229         self.api.post_server_action(server['id'], {'unshelve': None})
4230         self._wait_for_state_change(self.api, server, 'ACTIVE')
4231         allocs = self._get_allocations_by_server_uuid(server['id'])
4232         resources = list(allocs.values())[0]['resources']
4233         self.assertEqual(expected_usage, resources['DISK_GB'])
4234 
4235 
4236 class TraitsBasedSchedulingTest(integrated_helpers.ProviderUsageBaseTestCase):
4237     """Tests for requesting a server with required traits in Placement"""
4238 
4239     compute_driver = 'fake.SmallFakeDriver'
4240 
4241     def setUp(self):
4242         super(TraitsBasedSchedulingTest, self).setUp()
4243         self.compute1 = self._start_compute('host1')
4244         self.compute2 = self._start_compute('host2')
4245         # Using a standard trait from the os-traits library, set a required
4246         # trait extra spec on the flavor.
4247         flavors = self.api.get_flavors()
4248         self.flavor_with_trait = flavors[0]
4249         self.admin_api.post_extra_spec(
4250             self.flavor_with_trait['id'],
4251             {'extra_specs': {'trait:HW_CPU_X86_VMX': 'required'}})
4252         self.flavor_without_trait = flavors[1]
4253         self.flavor_with_forbidden_trait = flavors[2]
4254         self.admin_api.post_extra_spec(
4255             self.flavor_with_forbidden_trait['id'],
4256             {'extra_specs': {'trait:HW_CPU_X86_SGX': 'forbidden'}})
4257 
4258         # Note that we're using v2.35 explicitly as the api returns 404
4259         # starting with 2.36
4260         with nova.utils.temporary_mutation(self.api, microversion='2.35'):
4261             images = self.api.get_images()
4262             self.image_id_with_trait = images[0]['id']
4263             self.api.api_put('/images/%s/metadata' % self.image_id_with_trait,
4264                              {'metadata': {
4265                                  'trait:HW_CPU_X86_SGX': 'required'}})
4266             self.image_id_without_trait = images[1]['id']
4267 
4268     def _create_server_with_traits(self, flavor_id, image_id):
4269         """Create a server with given flavor and image id's
4270         :param flavor_id: the flavor id
4271         :param image_id: the image id
4272         :return: create server response
4273         """
4274 
4275         server_req = self._build_minimal_create_server_request(
4276             self.api, 'trait-based-server',
4277             image_uuid=image_id,
4278             flavor_id=flavor_id, networks='none')
4279         return self.api.post_server({'server': server_req})
4280 
4281     def _create_volume_backed_server_with_traits(self, flavor_id, volume_id):
4282         """Create a server with block device mapping(volume) with the given
4283         flavor and volume id's. Either the flavor or the image backing the
4284         volume is expected to have the traits
4285         :param flavor_id: the flavor id
4286         :param volume_id: the volume id
4287         :return: create server response
4288         """
4289 
4290         server_req_body = {
4291             # There is no imageRef because this is boot from volume.
4292             'server': {
4293                 'flavorRef': flavor_id,
4294                 'name': 'test_image_trait_on_volume_backed',
4295                 # We don't care about networking for this test. This
4296                 # requires microversion >= 2.37.
4297                 'networks': 'none',
4298                 'block_device_mapping_v2': [{
4299                     'boot_index': 0,
4300                     'uuid': volume_id,
4301                     'source_type': 'volume',
4302                     'destination_type': 'volume'
4303                 }]
4304             }
4305         }
4306         server = self.api.post_server(server_req_body)
4307         return server
4308 
4309     def test_flavor_traits_based_scheduling(self):
4310         """Tests that a server create request using a required trait on flavor
4311         ends up on the single compute node resource provider that also has that
4312         trait in Placement.
4313         """
4314 
4315         # Decorate compute1 resource provider with that same trait.
4316         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4317         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4318 
4319         # Create server using only flavor trait
4320         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4321                                                  self.image_id_without_trait)
4322         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4323         # Assert the server ended up on the expected compute host that has
4324         # the required trait.
4325         self.assertEqual(self.compute1.host, server['OS-EXT-SRV-ATTR:host'])
4326 
4327     def test_image_traits_based_scheduling(self):
4328         """Tests that a server create request using a required trait on image
4329         ends up on the single compute node resource provider that also has that
4330         trait in Placement.
4331         """
4332 
4333         # Decorate compute2 resource provider with image trait.
4334         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4335         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4336 
4337         # Create server using only image trait
4338         server = self._create_server_with_traits(
4339             self.flavor_without_trait['id'], self.image_id_with_trait)
4340         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4341         # Assert the server ended up on the expected compute host that has
4342         # the required trait.
4343         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4344 
4345     def test_flavor_image_traits_based_scheduling(self):
4346         """Tests that a server create request using a required trait on flavor
4347         AND a required trait on the image ends up on the single compute node
4348         resource provider that also has that trait in Placement.
4349         """
4350 
4351         # Decorate compute2 resource provider with both flavor and image trait.
4352         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4353         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4354                                             'HW_CPU_X86_SGX'])
4355 
4356         # Create server using flavor and image trait
4357         server = self._create_server_with_traits(
4358             self.flavor_with_trait['id'], self.image_id_with_trait)
4359         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4360         # Assert the server ended up on the expected compute host that has
4361         # the required trait.
4362         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4363 
4364     def test_image_trait_on_volume_backed_instance(self):
4365         """Tests that when trying to launch a volume-backed instance with a
4366         required trait on the image metadata contained within the volume,
4367         the instance ends up on the single compute node resource provider
4368         that also has that trait in Placement.
4369         """
4370         # Decorate compute2 resource provider with volume image metadata trait.
4371         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4372         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4373 
4374         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4375         # Create our server with a volume containing the image meta data with a
4376         # required trait
4377         server = self._create_volume_backed_server_with_traits(
4378             self.flavor_without_trait['id'],
4379             nova_fixtures.CinderFixtureNewAttachFlow.
4380             IMAGE_WITH_TRAITS_BACKED_VOL)
4381 
4382         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4383         # Assert the server ended up on the expected compute host that has
4384         # the required trait.
4385         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4386 
4387     def test_flavor_image_trait_on_volume_backed_instance(self):
4388         """Tests that when trying to launch a volume-backed instance with a
4389         required trait on flavor AND a required trait on the image metadata
4390         contained within the volume, the instance ends up on the single
4391         compute node resource provider that also has those traits in Placement.
4392         """
4393         # Decorate compute2 resource provider with volume image metadata trait.
4394         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4395         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4396                                             'HW_CPU_X86_SGX'])
4397 
4398         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4399         # Create our server with a flavor trait and a volume containing the
4400         # image meta data with a required trait
4401         server = self._create_volume_backed_server_with_traits(
4402             self.flavor_with_trait['id'],
4403             nova_fixtures.CinderFixtureNewAttachFlow.
4404             IMAGE_WITH_TRAITS_BACKED_VOL)
4405 
4406         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4407         # Assert the server ended up on the expected compute host that has
4408         # the required trait.
4409         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4410 
4411     def test_flavor_traits_based_scheduling_no_valid_host(self):
4412         """Tests that a server create request using a required trait expressed
4413          in flavor fails to find a valid host since no compute node resource
4414          providers have the trait.
4415         """
4416 
4417         # Decorate compute1 resource provider with the image trait.
4418         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4419         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_SGX'])
4420 
4421         server = self._create_server_with_traits(self.flavor_with_trait['id'],
4422                                                  self.image_id_without_trait)
4423         # The server should go to ERROR state because there is no valid host.
4424         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4425         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4426         # Make sure the failure was due to NoValidHost by checking the fault.
4427         self.assertIn('fault', server)
4428         self.assertIn('No valid host', server['fault']['message'])
4429 
4430     def test_image_traits_based_scheduling_no_valid_host(self):
4431         """Tests that a server create request using a required trait expressed
4432          in image fails to find a valid host since no compute node resource
4433          providers have the trait.
4434         """
4435 
4436         # Decorate compute1 resource provider with that flavor trait.
4437         rp_uuid = self._get_provider_uuid_by_host(self.compute1.host)
4438         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4439 
4440         server = self._create_server_with_traits(
4441             self.flavor_without_trait['id'], self.image_id_with_trait)
4442         # The server should go to ERROR state because there is no valid host.
4443         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4444         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4445         # Make sure the failure was due to NoValidHost by checking the fault.
4446         self.assertIn('fault', server)
4447         self.assertIn('No valid host', server['fault']['message'])
4448 
4449     def test_flavor_image_traits_based_scheduling_no_valid_host(self):
4450         """Tests that a server create request using a required trait expressed
4451          in flavor AND a required trait expressed in the image fails to find a
4452          valid host since no compute node resource providers have the trait.
4453         """
4454 
4455         server = self._create_server_with_traits(
4456             self.flavor_with_trait['id'], self.image_id_with_trait)
4457         # The server should go to ERROR state because there is no valid host.
4458         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4459         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4460         # Make sure the failure was due to NoValidHost by checking the fault.
4461         self.assertIn('fault', server)
4462         self.assertIn('No valid host', server['fault']['message'])
4463 
4464     def test_image_trait_on_volume_backed_instance_no_valid_host(self):
4465         """Tests that when trying to launch a volume-backed instance with a
4466         required trait on the image metadata contained within the volume
4467         fails to find a valid host since no compute node resource providers
4468         have the trait.
4469         """
4470         self.useFixture(nova_fixtures.CinderFixtureNewAttachFlow(self))
4471         # Create our server with a volume
4472         server = self._create_volume_backed_server_with_traits(
4473             self.flavor_without_trait['id'],
4474             nova_fixtures.CinderFixtureNewAttachFlow.
4475             IMAGE_WITH_TRAITS_BACKED_VOL)
4476 
4477         # The server should go to ERROR state because there is no valid host.
4478         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
4479         self.assertIsNone(server['OS-EXT-SRV-ATTR:host'])
4480         # Make sure the failure was due to NoValidHost by checking the fault.
4481         self.assertIn('fault', server)
4482         self.assertIn('No valid host', server['fault']['message'])
4483 
4484     def test_rebuild_instance_with_image_traits(self):
4485         """Rebuilds a server with a different image which has traits
4486         associated with it and which will run it through the scheduler to
4487         validate the image is still OK with the compute host that the
4488         instance is running on.
4489          """
4490         # Decorate compute2 resource provider with both flavor and image trait.
4491         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4492         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4493                                             'HW_CPU_X86_SGX'])
4494         # make sure we start with no usage on the compute node
4495         rp_usages = self._get_provider_usages(rp_uuid)
4496         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4497 
4498         # create a server without traits on image and with traits on flavour
4499         server = self._create_server_with_traits(
4500             self.flavor_with_trait['id'], self.image_id_without_trait)
4501         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4502 
4503         # make the compute node full and ensure rebuild still succeed
4504         inv = {"resource_class": "VCPU",
4505                "total": 1}
4506         self._set_inventory(rp_uuid, inv)
4507 
4508         # Now rebuild the server with a different image with traits
4509         rebuild_req_body = {
4510             'rebuild': {
4511                 'imageRef': self.image_id_with_trait
4512             }
4513         }
4514         self.api.api_post('/servers/%s/action' % server['id'],
4515                           rebuild_req_body)
4516         self._wait_for_server_parameter(
4517             self.api, server, {'OS-EXT-STS:task_state': None})
4518 
4519         allocs = self._get_allocations_by_server_uuid(server['id'])
4520         self.assertIn(rp_uuid, allocs)
4521 
4522         # Assert the server ended up on the expected compute host that has
4523         # the required trait.
4524         self.assertEqual(self.compute2.host, server['OS-EXT-SRV-ATTR:host'])
4525 
4526     def test_rebuild_instance_with_image_traits_no_host(self):
4527         """Rebuilding a server with a different image which has required
4528         traits on the image fails to valid the host that this server is
4529         currently running, cause the compute host resource provider is not
4530         associated with similar trait.
4531         """
4532         # Decorate compute2 resource provider with traits on flavor
4533         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4534         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4535 
4536         # make sure we start with no usage on the compute node
4537         rp_usages = self._get_provider_usages(rp_uuid)
4538         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4539 
4540         # create a server without traits on image and with traits on flavour
4541         server = self._create_server_with_traits(
4542             self.flavor_with_trait['id'], self.image_id_without_trait)
4543         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4544 
4545         # Now rebuild the server with a different image with traits
4546         rebuild_req_body = {
4547             'rebuild': {
4548                 'imageRef': self.image_id_with_trait
4549             }
4550         }
4551 
4552         self.api.api_post('/servers/%s/action' % server['id'],
4553                           rebuild_req_body)
4554         # Look for the failed rebuild action.
4555         self._wait_for_action_fail_completion(
4556             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4557         # Assert the server image_ref was rolled back on failure.
4558         server = self.api.get_server(server['id'])
4559         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4560 
4561         # The server should be in ERROR state
4562         self.assertEqual('ERROR', server['status'])
4563         self.assertEqual("No valid host was found. Image traits cannot be "
4564                          "satisfied by the current resource providers. "
4565                          "Either specify a different image during rebuild "
4566                          "or create a new server with the specified image.",
4567                          server['fault']['message'])
4568 
4569     def test_rebuild_instance_with_image_traits_no_image_change(self):
4570         """Rebuilds a server with a same image which has traits
4571         associated with it and which will run it through the scheduler to
4572         validate the image is still OK with the compute host that the
4573         instance is running on.
4574          """
4575         # Decorate compute2 resource provider with both flavor and image trait.
4576         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4577         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX',
4578                                             'HW_CPU_X86_SGX'])
4579         # make sure we start with no usage on the compute node
4580         rp_usages = self._get_provider_usages(rp_uuid)
4581         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
4582                          rp_usages)
4583 
4584         # create a server with traits in both image and flavour
4585         server = self._create_server_with_traits(
4586             self.flavor_with_trait['id'], self.image_id_with_trait)
4587         server = self._wait_for_state_change(self.admin_api, server,
4588                                              'ACTIVE')
4589 
4590         # Now rebuild the server with a different image with traits
4591         rebuild_req_body = {
4592             'rebuild': {
4593                 'imageRef': self.image_id_with_trait
4594             }
4595         }
4596         self.api.api_post('/servers/%s/action' % server['id'],
4597                           rebuild_req_body)
4598         self._wait_for_server_parameter(
4599             self.api, server, {'OS-EXT-STS:task_state': None})
4600 
4601         allocs = self._get_allocations_by_server_uuid(server['id'])
4602         self.assertIn(rp_uuid, allocs)
4603 
4604         # Assert the server ended up on the expected compute host that has
4605         # the required trait.
4606         self.assertEqual(self.compute2.host,
4607                          server['OS-EXT-SRV-ATTR:host'])
4608 
4609     def test_rebuild_instance_with_image_traits_and_forbidden_flavor_traits(
4610                                                                         self):
4611         """Rebuilding a server with a different image which has required
4612         traits on the image fails to validate image traits because flavor
4613         associated with the current instance has the similar trait that is
4614         forbidden
4615         """
4616         # Decorate compute2 resource provider with traits on flavor
4617         rp_uuid = self._get_provider_uuid_by_host(self.compute2.host)
4618         self._set_provider_traits(rp_uuid, ['HW_CPU_X86_VMX'])
4619 
4620         # make sure we start with no usage on the compute node
4621         rp_usages = self._get_provider_usages(rp_uuid)
4622         self.assertEqual({'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0}, rp_usages)
4623 
4624         # create a server with forbidden traits on flavor and no triats on
4625         # image
4626         server = self._create_server_with_traits(
4627             self.flavor_with_forbidden_trait['id'],
4628             self.image_id_without_trait)
4629         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
4630 
4631         # Now rebuild the server with a different image with traits
4632         rebuild_req_body = {
4633             'rebuild': {
4634                 'imageRef': self.image_id_with_trait
4635             }
4636         }
4637 
4638         self.api.api_post('/servers/%s/action' % server['id'],
4639                           rebuild_req_body)
4640         # Look for the failed rebuild action.
4641         self._wait_for_action_fail_completion(
4642             server, instance_actions.REBUILD, 'rebuild_server', self.admin_api)
4643         # Assert the server image_ref was rolled back on failure.
4644         server = self.api.get_server(server['id'])
4645         self.assertEqual(self.image_id_without_trait, server['image']['id'])
4646 
4647         # The server should be in ERROR state
4648         self.assertEqual('ERROR', server['status'])
4649         self.assertEqual("No valid host was found. Image traits are part of "
4650                          "forbidden traits in flavor associated with the "
4651                          "server. Either specify a different image during "
4652                          "rebuild or create a new server with the specified "
4653                          "image and a compatible flavor.",
4654                          server['fault']['message'])
4655 
4656 
4657 class ServerTestV256Common(ServersTestBase):
4658     api_major_version = 'v2.1'
4659     microversion = '2.56'
4660     ADMIN_API = True
4661 
4662     def _setup_compute_service(self):
4663         # Set up 3 compute services in the same cell
4664         for host in ('host1', 'host2', 'host3'):
4665             fake.set_nodes([host])
4666             self.addCleanup(fake.restore_nodes)
4667             self.start_service('compute', host=host)
4668 
4669     def _create_server(self, target_host=None):
4670         server = self._build_minimal_create_server_request(
4671             image_uuid='a2459075-d96c-40d5-893e-577ff92e721c')
4672         server.update({'networks': 'auto'})
4673         if target_host is not None:
4674             server['availability_zone'] = 'nova:%s' % target_host
4675         post = {'server': server}
4676         response = self.api.api_post('/servers', post).body
4677         return response['server']
4678 
4679     @staticmethod
4680     def _get_target_and_other_hosts(host):
4681         target_other_hosts = {'host1': ['host2', 'host3'],
4682                               'host2': ['host3', 'host1'],
4683                               'host3': ['host1', 'host2']}
4684         return target_other_hosts[host]
4685 
4686 
4687 class ServerTestV256MultiCellTestCase(ServerTestV256Common):
4688     """Negative test to ensure we fail with ComputeHostNotFound if we try to
4689     target a host in another cell from where the instance lives.
4690     """
4691     NUMBER_OF_CELLS = 2
4692 
4693     def _setup_compute_service(self):
4694         # Set up 2 compute services in different cells
4695         host_to_cell_mappings = {
4696             'host1': 'cell1',
4697             'host2': 'cell2'}
4698         for host in sorted(host_to_cell_mappings):
4699             fake.set_nodes([host])
4700             self.addCleanup(fake.restore_nodes)
4701             self.start_service('compute', host=host,
4702                                cell=host_to_cell_mappings[host])
4703 
4704     def test_migrate_server_to_host_in_different_cell(self):
4705         # We target host1 specifically so that we have a predictable target for
4706         # the cold migration in cell2.
4707         server = self._create_server(target_host='host1')
4708         server = self._wait_for_state_change(server, 'BUILD')
4709 
4710         self.assertEqual('host1', server['OS-EXT-SRV-ATTR:host'])
4711         ex = self.assertRaises(client.OpenStackApiException,
4712                                self.api.post_server_action,
4713                                server['id'],
4714                                {'migrate': {'host': 'host2'}})
4715         # When the API pulls the instance out of cell1, the context is targeted
4716         # to cell1, so when the compute API resize() method attempts to lookup
4717         # the target host in cell1, it will result in a ComputeHostNotFound
4718         # error.
4719         self.assertEqual(400, ex.response.status_code)
4720         self.assertIn('Compute host host2 could not be found',
4721                       six.text_type(ex))
4722 
4723 
4724 class ServerTestV256SingleCellMultiHostTestCase(ServerTestV256Common):
4725     """Happy path test where we create a server on one host, migrate it to
4726     another host of our choosing and ensure it lands there.
4727     """
4728     def test_migrate_server_to_host_in_same_cell(self):
4729         server = self._create_server()
4730         server = self._wait_for_state_change(server, 'BUILD')
4731         source_host = server['OS-EXT-SRV-ATTR:host']
4732         target_host = self._get_target_and_other_hosts(source_host)[0]
4733         self.api.post_server_action(server['id'],
4734                                     {'migrate': {'host': target_host}})
4735         # Assert the server is now on the target host.
4736         server = self.api.get_server(server['id'])
4737         self.assertEqual(target_host, server['OS-EXT-SRV-ATTR:host'])
4738 
4739 
4740 class ServerTestV256RescheduleTestCase(ServerTestV256Common):
4741 
4742     @mock.patch.object(compute_manager.ComputeManager, '_prep_resize',
4743                        side_effect=exception.MigrationError(
4744                            reason='Test Exception'))
4745     def test_migrate_server_not_reschedule(self, mock_prep_resize):
4746         server = self._create_server()
4747         found_server = self._wait_for_state_change(server, 'BUILD')
4748 
4749         target_host, other_host = self._get_target_and_other_hosts(
4750             found_server['OS-EXT-SRV-ATTR:host'])
4751 
4752         self.assertRaises(client.OpenStackApiException,
4753                           self.api.post_server_action,
4754                           server['id'],
4755                           {'migrate': {'host': target_host}})
4756         self.assertEqual(1, mock_prep_resize.call_count)
4757         found_server = self.api.get_server(server['id'])
4758         # Check that rescheduling is not occurred.
4759         self.assertNotEqual(other_host, found_server['OS-EXT-SRV-ATTR:host'])
4760 
4761 
4762 class ConsumerGenerationConflictTest(
4763         integrated_helpers.ProviderUsageBaseTestCase):
4764 
4765     # we need the medium driver to be able to allocate resource not just for
4766     # a single instance
4767     compute_driver = 'fake.MediumFakeDriver'
4768 
4769     def setUp(self):
4770         super(ConsumerGenerationConflictTest, self).setUp()
4771         flavors = self.api.get_flavors()
4772         self.flavor = flavors[0]
4773         self.other_flavor = flavors[1]
4774         self.compute1 = self._start_compute('compute1')
4775         self.compute2 = self._start_compute('compute2')
4776 
4777     def test_create_server_fails_as_placement_reports_consumer_conflict(self):
4778         server_req = self._build_minimal_create_server_request(
4779             self.api, 'some-server', flavor_id=self.flavor['id'],
4780             image_uuid='155d900f-4e14-4e4c-a73d-069cbf4541e6',
4781             networks='none')
4782 
4783         # We cannot pre-create a consumer with the uuid of the instance created
4784         # below as that uuid is generated. Instead we have to simulate that
4785         # Placement returns 409, consumer generation conflict for the PUT
4786         # /allocation request the scheduler does for the instance.
4787         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
4788             rsp = fake_requests.FakeResponse(
4789                 409,
4790                 jsonutils.dumps(
4791                     {'errors': [
4792                         {'code': 'placement.concurrent_update',
4793                          'detail': 'consumer generation conflict'}]}))
4794             mock_put.return_value = rsp
4795 
4796             created_server = self.api.post_server({'server': server_req})
4797             server = self._wait_for_state_change(
4798                 self.admin_api, created_server, 'ERROR')
4799 
4800         # This is not a conflict that the API user can ever resolve. It is a
4801         # serious inconsistency in our database or a bug in the scheduler code
4802         # doing the claim.
4803         self.assertEqual(500, server['fault']['code'])
4804         self.assertIn('Failed to update allocations for consumer',
4805                       server['fault']['message'])
4806 
4807         allocations = self._get_allocations_by_server_uuid(server['id'])
4808         self.assertEqual(0, len(allocations))
4809 
4810         self._delete_and_check_allocations(server)
4811 
4812     def test_migrate_claim_on_dest_fails(self):
4813         source_hostname = self.compute1.host
4814         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4815 
4816         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4817 
4818         # We have to simulate that Placement returns 409, consumer generation
4819         # conflict for the PUT /allocation request the scheduler does on the
4820         # destination host for the instance.
4821         with mock.patch('keystoneauth1.adapter.Adapter.put') as mock_put:
4822             rsp = fake_requests.FakeResponse(
4823                 409,
4824                 jsonutils.dumps(
4825                     {'errors': [
4826                         {'code': 'placement.concurrent_update',
4827                          'detail': 'consumer generation conflict'}]}))
4828             mock_put.return_value = rsp
4829 
4830             request = {'migrate': None}
4831             exception = self.assertRaises(client.OpenStackApiException,
4832                                           self.api.post_server_action,
4833                                           server['id'], request)
4834 
4835         # I know that HTTP 500 is harsh code but I think this conflict case
4836         # signals either a serious db inconsistency or a bug in nova's
4837         # claim code.
4838         self.assertEqual(500, exception.response.status_code)
4839 
4840         # The migration is aborted so the instance is ACTIVE on the source
4841         # host instead of being in VERIFY_RESIZE state.
4842         server = self.api.get_server(server['id'])
4843         self.assertEqual('ACTIVE', server['status'])
4844         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
4845 
4846         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4847 
4848         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4849                                            source_rp_uuid)
4850 
4851         self._delete_and_check_allocations(server)
4852 
4853     def test_migrate_move_allocation_fails_due_to_conflict(self):
4854         source_hostname = self.compute1.host
4855         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4856 
4857         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4858 
4859         rsp = fake_requests.FakeResponse(
4860             409,
4861             jsonutils.dumps(
4862                 {'errors': [
4863                     {'code': 'placement.concurrent_update',
4864                      'detail': 'consumer generation conflict'}]}))
4865 
4866         with mock.patch('keystoneauth1.adapter.Adapter.post',
4867                         autospec=True) as mock_post:
4868             mock_post.return_value = rsp
4869 
4870             request = {'migrate': None}
4871             exception = self.assertRaises(client.OpenStackApiException,
4872                                           self.api.post_server_action,
4873                                           server['id'], request)
4874 
4875         self.assertEqual(1, mock_post.call_count)
4876 
4877         self.assertEqual(409, exception.response.status_code)
4878         self.assertIn('Failed to move allocations', exception.response.text)
4879 
4880         migrations = self.api.get_migrations()
4881         self.assertEqual(1, len(migrations))
4882         self.assertEqual('migration', migrations[0]['migration_type'])
4883         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4884         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4885         self.assertEqual('error', migrations[0]['status'])
4886 
4887         # The migration is aborted so the instance is ACTIVE on the source
4888         # host instead of being in VERIFY_RESIZE state.
4889         server = self.api.get_server(server['id'])
4890         self.assertEqual('ACTIVE', server['status'])
4891         self.assertEqual(source_hostname, server['OS-EXT-SRV-ATTR:host'])
4892 
4893         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
4894 
4895         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
4896                                            source_rp_uuid)
4897 
4898         self._delete_and_check_allocations(server)
4899 
4900     def test_confirm_migrate_delete_alloc_on_source_fails(self):
4901         source_hostname = self.compute1.host
4902         dest_hostname = self.compute2.host
4903         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4904         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4905 
4906         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4907         self._migrate_and_check_allocations(
4908             server, self.flavor, source_rp_uuid, dest_rp_uuid)
4909 
4910         rsp = fake_requests.FakeResponse(
4911             409,
4912             jsonutils.dumps(
4913                 {'errors': [
4914                     {'code': 'placement.concurrent_update',
4915                      'detail': 'consumer generation conflict'}]}))
4916 
4917         with mock.patch('keystoneauth1.adapter.Adapter.put',
4918                         autospec=True) as mock_put:
4919             mock_put.return_value = rsp
4920 
4921             post = {'confirmResize': None}
4922             self.api.post_server_action(
4923                 server['id'], post, check_response_status=[204])
4924             server = self._wait_for_state_change(self.api, server, 'ERROR')
4925             self.assertIn('Failed to delete allocations',
4926                           server['fault']['message'])
4927 
4928         self.assertEqual(1, mock_put.call_count)
4929 
4930         migrations = self.api.get_migrations()
4931         self.assertEqual(1, len(migrations))
4932         self.assertEqual('migration', migrations[0]['migration_type'])
4933         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4934         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4935         # NOTE(gibi): it might be better to mark the migration as error
4936         self.assertEqual('confirmed', migrations[0]['status'])
4937 
4938         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4939         # after the instance is deleted. At least nova logs a fat ERROR.
4940         self.assertIn('Deleting allocation in placement for migration %s '
4941                       'failed. The instance %s will be put to ERROR state but '
4942                       'the allocation held by the migration is leaked.' %
4943                       (migrations[0]['uuid'], server['id']),
4944                       self.stdlog.logger.output)
4945         self.api.delete_server(server['id'])
4946         self._wait_until_deleted(server)
4947         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4948 
4949         allocations = self._get_allocations_by_server_uuid(
4950             migrations[0]['uuid'])
4951         self.assertEqual(1, len(allocations))
4952 
4953     def test_revert_migrate_delete_dest_allocation_fails_due_to_conflict(self):
4954         source_hostname = self.compute1.host
4955         dest_hostname = self.compute2.host
4956         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
4957         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
4958 
4959         server = self._boot_and_check_allocations(self.flavor, source_hostname)
4960         self._migrate_and_check_allocations(
4961             server, self.flavor, source_rp_uuid, dest_rp_uuid)
4962 
4963         rsp = fake_requests.FakeResponse(
4964             409,
4965             jsonutils.dumps(
4966                 {'errors': [
4967                     {'code': 'placement.concurrent_update',
4968                      'detail': 'consumer generation conflict'}]}))
4969 
4970         with mock.patch('keystoneauth1.adapter.Adapter.post',
4971                         autospec=True) as mock_post:
4972             mock_post.return_value = rsp
4973 
4974             post = {'revertResize': None}
4975             self.api.post_server_action(server['id'], post)
4976             server = self._wait_for_state_change(self.api, server, 'ERROR')
4977 
4978         self.assertEqual(1, mock_post.call_count)
4979 
4980         migrations = self.api.get_migrations()
4981         self.assertEqual(1, len(migrations))
4982         self.assertEqual('migration', migrations[0]['migration_type'])
4983         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
4984         self.assertEqual(source_hostname, migrations[0]['source_compute'])
4985         self.assertEqual('error', migrations[0]['status'])
4986 
4987         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
4988         # after the instance is deleted. At least nova logs a fat ERROR.
4989         self.assertIn('Reverting allocation in placement for migration %s '
4990                       'failed. The instance %s will be put into ERROR state '
4991                       'but the allocation held by the migration is leaked.' %
4992                       (migrations[0]['uuid'], server['id']),
4993                       self.stdlog.logger.output)
4994         self.api.delete_server(server['id'])
4995         self._wait_until_deleted(server)
4996         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
4997 
4998         allocations = self._get_allocations_by_server_uuid(
4999             migrations[0]['uuid'])
5000         self.assertEqual(1, len(allocations))
5001 
5002     def test_revert_resize_same_host_delete_dest_fails_due_to_conflict(self):
5003         # make sure that the test only uses a single host
5004         compute2_service_id = self.admin_api.get_services(
5005             host=self.compute2.host, binary='nova-compute')[0]['id']
5006         self.admin_api.put_service(compute2_service_id, {'status': 'disabled'})
5007 
5008         hostname = self.compute1.manager.host
5009         rp_uuid = self._get_provider_uuid_by_host(hostname)
5010 
5011         server = self._boot_and_check_allocations(self.flavor, hostname)
5012 
5013         self._resize_to_same_host_and_check_allocations(
5014             server, self.flavor, self.other_flavor, rp_uuid)
5015 
5016         rsp = fake_requests.FakeResponse(
5017             409,
5018             jsonutils.dumps(
5019                 {'errors': [
5020                     {'code': 'placement.concurrent_update',
5021                      'detail': 'consumer generation conflict'}]}))
5022         with mock.patch('keystoneauth1.adapter.Adapter.post',
5023                         autospec=True) as mock_post:
5024             mock_post.return_value = rsp
5025 
5026             post = {'revertResize': None}
5027             self.api.post_server_action(server['id'], post)
5028             server = self._wait_for_state_change(self.api, server, 'ERROR',)
5029 
5030         self.assertEqual(1, mock_post.call_count)
5031 
5032         migrations = self.api.get_migrations()
5033         self.assertEqual(1, len(migrations))
5034         self.assertEqual('resize', migrations[0]['migration_type'])
5035         self.assertEqual(server['id'], migrations[0]['instance_uuid'])
5036         self.assertEqual(hostname, migrations[0]['source_compute'])
5037         self.assertEqual('error', migrations[0]['status'])
5038 
5039         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5040         # after the instance is deleted. At least nova logs a fat ERROR.
5041         self.assertIn('Reverting allocation in placement for migration %s '
5042                       'failed. The instance %s will be put into ERROR state '
5043                       'but the allocation held by the migration is leaked.' %
5044                       (migrations[0]['uuid'], server['id']),
5045                       self.stdlog.logger.output)
5046         self.api.delete_server(server['id'])
5047         self._wait_until_deleted(server)
5048         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5049 
5050         allocations = self._get_allocations_by_server_uuid(
5051             migrations[0]['uuid'])
5052         self.assertEqual(1, len(allocations))
5053 
5054     def test_force_live_migrate_claim_on_dest_fails(self):
5055         # Normal live migrate moves source allocation from instance to
5056         # migration like a normal migrate tested above.
5057         # Normal live migrate claims on dest like a normal boot tested above.
5058         source_hostname = self.compute1.host
5059         dest_hostname = self.compute2.host
5060 
5061         # the ability to force live migrate a server is removed entirely in
5062         # 2.68
5063         self.api.microversion = '2.67'
5064 
5065         server = self._boot_and_check_allocations(
5066             self.flavor, source_hostname)
5067 
5068         rsp = fake_requests.FakeResponse(
5069             409,
5070             jsonutils.dumps(
5071                 {'errors': [
5072                     {'code': 'placement.concurrent_update',
5073                      'detail': 'consumer generation conflict'}]}))
5074         with mock.patch('keystoneauth1.adapter.Adapter.put',
5075                         autospec=True) as mock_put:
5076             mock_put.return_value = rsp
5077 
5078             post = {
5079                 'os-migrateLive': {
5080                     'host': dest_hostname,
5081                     'block_migration': True,
5082                     'force': True,
5083                 }
5084             }
5085 
5086             self.api.post_server_action(server['id'], post)
5087             server = self._wait_for_state_change(self.api, server, 'ERROR')
5088 
5089         self.assertEqual(1, mock_put.call_count)
5090 
5091         # This is not a conflict that the API user can ever resolve. It is a
5092         # serious inconsistency in our database or a bug in the scheduler code
5093         # doing the claim.
5094         self.assertEqual(500, server['fault']['code'])
5095         # The instance is in ERROR state so the allocations are in limbo but
5096         # at least we expect that when the instance is deleted the allocations
5097         # are cleaned up properly.
5098         self._delete_and_check_allocations(server)
5099 
5100     def test_live_migrate_drop_allocation_on_source_fails(self):
5101         source_hostname = self.compute1.host
5102         dest_hostname = self.compute2.host
5103         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5104         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5105 
5106         # the ability to force live migrate a server is removed entirely in
5107         # 2.68
5108         self.api.microversion = '2.67'
5109 
5110         server = self._boot_and_check_allocations(
5111             self.flavor, source_hostname)
5112 
5113         fake_notifier.stub_notifier(self)
5114         self.addCleanup(fake_notifier.reset)
5115 
5116         orig_put = adapter.Adapter.put
5117 
5118         rsp = fake_requests.FakeResponse(
5119             409,
5120             jsonutils.dumps(
5121                 {'errors': [
5122                     {'code': 'placement.concurrent_update',
5123                      'detail': 'consumer generation conflict'}]}))
5124 
5125         def fake_put(_self, url, *args, **kwargs):
5126             migration_uuid = self.get_migration_uuid_for_instance(server['id'])
5127             if url == '/allocations/%s' % migration_uuid:
5128                 return rsp
5129             else:
5130                 return orig_put(_self, url, *args, **kwargs)
5131 
5132         with mock.patch('keystoneauth1.adapter.Adapter.put',
5133                         autospec=True) as mock_put:
5134             mock_put.side_effect = fake_put
5135 
5136             post = {
5137                 'os-migrateLive': {
5138                     'host': dest_hostname,
5139                     'block_migration': True,
5140                     'force': True,
5141                 }
5142             }
5143 
5144             self.api.post_server_action(server['id'], post)
5145 
5146             # nova does the source host cleanup _after_ setting the migration
5147             # to completed and sending end notifications so we have to wait
5148             # here a bit.
5149             time.sleep(1)
5150 
5151             # Nova failed to clean up on the source host. This right now puts
5152             # the instance to ERROR state and fails the migration.
5153             server = self._wait_for_server_parameter(self.api, server,
5154                 {'OS-EXT-SRV-ATTR:host': dest_hostname,
5155                  'status': 'ERROR'})
5156             self._wait_for_migration_status(server, ['error'])
5157             fake_notifier.wait_for_versioned_notifications(
5158                 'instance.live_migration_post.end')
5159 
5160         # 1 claim on destination, 1 normal delete on dest that fails,
5161         self.assertEqual(2, mock_put.call_count)
5162 
5163         # As the cleanup on the source host failed Nova leaks the allocation
5164         # held by the migration.
5165         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5166         migration_uuid = self.get_migration_uuid_for_instance(server['id'])
5167         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
5168                                            source_rp_uuid)
5169 
5170         self.assertFlavorMatchesUsage(dest_rp_uuid, self.flavor)
5171 
5172         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5173                                            dest_rp_uuid)
5174 
5175         # NOTE(gibi): Nova leaks the allocation held by the migration_uuid even
5176         # after the instance is deleted. At least nova logs a fat ERROR.
5177         self.assertIn('Deleting allocation in placement for migration %s '
5178                       'failed. The instance %s will be put to ERROR state but '
5179                       'the allocation held by the migration is leaked.' %
5180                       (migration_uuid, server['id']),
5181                       self.stdlog.logger.output)
5182 
5183         self.api.delete_server(server['id'])
5184         self._wait_until_deleted(server)
5185         fake_notifier.wait_for_versioned_notifications('instance.delete.end')
5186 
5187         self.assertFlavorMatchesAllocation(self.flavor, migration_uuid,
5188                                            source_rp_uuid)
5189 
5190     def _test_evacuate_fails_allocating_on_dest_host(self, force):
5191         source_hostname = self.compute1.host
5192         dest_hostname = self.compute2.host
5193 
5194         # the ability to force evacuate a server is removed entirely in 2.68
5195         self.api.microversion = '2.67'
5196 
5197         server = self._boot_and_check_allocations(
5198             self.flavor, source_hostname)
5199 
5200         source_compute_id = self.admin_api.get_services(
5201             host=source_hostname, binary='nova-compute')[0]['id']
5202 
5203         self.compute1.stop()
5204         # force it down to avoid waiting for the service group to time out
5205         self.admin_api.put_service(
5206             source_compute_id, {'forced_down': 'true'})
5207 
5208         rsp = fake_requests.FakeResponse(
5209             409,
5210             jsonutils.dumps(
5211                 {'errors': [
5212                     {'code': 'placement.concurrent_update',
5213                      'detail': 'consumer generation conflict'}]}))
5214 
5215         with mock.patch('keystoneauth1.adapter.Adapter.put',
5216                         autospec=True) as mock_put:
5217             mock_put.return_value = rsp
5218             post = {
5219                 'evacuate': {
5220                     'force': force
5221                 }
5222             }
5223             if force:
5224                 post['evacuate']['host'] = dest_hostname
5225 
5226             self.api.post_server_action(server['id'], post)
5227             server = self._wait_for_state_change(self.api, server, 'ERROR')
5228 
5229         self.assertEqual(1, mock_put.call_count)
5230 
5231         # As nova failed to allocate on the dest host we only expect allocation
5232         # on the source
5233         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5234         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5235 
5236         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5237 
5238         self.assertRequestMatchesUsage({'VCPU': 0,
5239                                         'MEMORY_MB': 0,
5240                                         'DISK_GB': 0}, dest_rp_uuid)
5241 
5242         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5243                                            source_rp_uuid)
5244 
5245         self._delete_and_check_allocations(server)
5246 
5247     def test_force_evacuate_fails_allocating_on_dest_host(self):
5248         self._test_evacuate_fails_allocating_on_dest_host(force=True)
5249 
5250     def test_evacuate_fails_allocating_on_dest_host(self):
5251         self._test_evacuate_fails_allocating_on_dest_host(force=False)
5252 
5253     def test_server_delete_fails_due_to_conflict(self):
5254         source_hostname = self.compute1.host
5255 
5256         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5257 
5258         rsp = fake_requests.FakeResponse(
5259             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5260 
5261         with mock.patch('keystoneauth1.adapter.Adapter.put',
5262                         autospec=True) as mock_put:
5263             mock_put.return_value = rsp
5264 
5265             self.api.delete_server(server['id'])
5266             server = self._wait_for_state_change(self.admin_api, server,
5267                                                  'ERROR')
5268             self.assertEqual(1, mock_put.call_count)
5269 
5270         # We still have the allocations as deletion failed
5271         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5272         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5273 
5274         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5275                                            source_rp_uuid)
5276 
5277         # retry the delete to make sure that allocations are removed this time
5278         self._delete_and_check_allocations(server)
5279 
5280     def test_server_local_delete_fails_due_to_conflict(self):
5281         source_hostname = self.compute1.host
5282 
5283         server = self._boot_and_check_allocations(self.flavor, source_hostname)
5284         source_compute_id = self.admin_api.get_services(
5285             host=self.compute1.host, binary='nova-compute')[0]['id']
5286         self.compute1.stop()
5287         self.admin_api.put_service(
5288             source_compute_id, {'forced_down': 'true'})
5289 
5290         rsp = fake_requests.FakeResponse(
5291             409, jsonutils.dumps({'text': 'consumer generation conflict'}))
5292 
5293         with mock.patch('keystoneauth1.adapter.Adapter.put',
5294                         autospec=True) as mock_put:
5295             mock_put.return_value = rsp
5296 
5297             ex = self.assertRaises(client.OpenStackApiException,
5298                                    self.api.delete_server, server['id'])
5299             self.assertEqual(409, ex.response.status_code)
5300             self.assertIn('Failed to delete allocations for consumer',
5301                           jsonutils.loads(ex.response.content)[
5302                               'conflictingRequest']['message'])
5303             self.assertEqual(1, mock_put.call_count)
5304 
5305         # We still have the allocations as deletion failed
5306         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5307         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor)
5308 
5309         self.assertFlavorMatchesAllocation(self.flavor, server['id'],
5310                                            source_rp_uuid)
5311 
5312         # retry the delete to make sure that allocations are removed this time
5313         self._delete_and_check_allocations(server)
5314 
5315 
5316 class ServerMovingTestsWithNestedComputes(ServerMovingTests):
5317     """Runs all the server moving tests while the computes have nested trees.
5318     The servers still do not request resources from any child provider though.
5319     """
5320     compute_driver = 'fake.MediumFakeDriverWithNestedCustomResources'
5321 
5322 
5323 class ServerMovingTestsWithNestedResourceRequests(
5324     ServerMovingTestsWithNestedComputes):
5325     """Runs all the server moving tests while the computes have nested trees.
5326     The servers also request resources from child providers.
5327     """
5328 
5329     def setUp(self):
5330         super(ServerMovingTestsWithNestedResourceRequests, self).setUp()
5331         # modify the flavors used in the ServerMoving test base class to
5332         # require one piece of CUSTOM_MAGIC resource as well.
5333 
5334         for flavor in [self.flavor1, self.flavor2, self.flavor3]:
5335             self.api.post_extra_spec(
5336                 flavor['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5337             # save the extra_specs in the flavor stored in the test case as
5338             # well
5339             flavor['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5340 
5341     def _check_allocation_during_evacuate(
5342             self, flavor, server_uuid, source_root_rp_uuid, dest_root_rp_uuid):
5343         # NOTE(gibi): evacuate is the only case when the same consumer has
5344         # allocation from two different RP trees so we need a special check
5345         # here.
5346         allocations = self._get_allocations_by_server_uuid(server_uuid)
5347         source_rps = self._get_all_rp_uuids_in_a_tree(source_root_rp_uuid)
5348         dest_rps = self._get_all_rp_uuids_in_a_tree(dest_root_rp_uuid)
5349 
5350         self.assertEqual(set(source_rps + dest_rps), set(allocations))
5351 
5352         total_source_allocation = collections.defaultdict(int)
5353         total_dest_allocation = collections.defaultdict(int)
5354         for rp, alloc in allocations.items():
5355             for rc, value in alloc['resources'].items():
5356                 if rp in source_rps:
5357                     total_source_allocation[rc] += value
5358                 else:
5359                     total_dest_allocation[rc] += value
5360 
5361         self.assertEqual(
5362             self._resources_from_flavor(flavor), total_source_allocation)
5363         self.assertEqual(
5364             self._resources_from_flavor(flavor), total_dest_allocation)
5365 
5366     def test_live_migrate_force(self):
5367         # Nova intentionally does not support force live-migrating server
5368         # with nested allocations.
5369 
5370         source_hostname = self.compute1.host
5371         dest_hostname = self.compute2.host
5372         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5373         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5374 
5375         # the ability to force live migrate a server is removed entirely in
5376         # 2.68
5377         self.api.microversion = '2.67'
5378 
5379         server = self._boot_and_check_allocations(
5380             self.flavor1, source_hostname)
5381         post = {
5382             'os-migrateLive': {
5383                 'host': dest_hostname,
5384                 'block_migration': True,
5385                 'force': True,
5386             }
5387         }
5388 
5389         self.api.post_server_action(server['id'], post)
5390         self._wait_for_migration_status(server, ['error'])
5391         self._wait_for_server_parameter(self.api, server,
5392             {'OS-EXT-SRV-ATTR:host': source_hostname,
5393              'status': 'ACTIVE'})
5394         self.assertIn('Unable to move instance %s to host host2. The instance '
5395                       'has complex allocations on the source host so move '
5396                       'cannot be forced.' %
5397                       server['id'],
5398                       self.stdlog.logger.output)
5399 
5400         self._run_periodics()
5401 
5402         # NOTE(danms): There should be no usage for the dest
5403         self.assertRequestMatchesUsage(
5404             {'VCPU': 0,
5405              'MEMORY_MB': 0,
5406              'DISK_GB': 0}, dest_rp_uuid)
5407 
5408         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5409 
5410         # the server has an allocation on only the source node
5411         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5412                                            source_rp_uuid)
5413 
5414         self._delete_and_check_allocations(server)
5415 
5416     def test_evacuate_forced_host(self):
5417         # Nova intentionally does not support force evacuating server
5418         # with nested allocations.
5419 
5420         source_hostname = self.compute1.host
5421         dest_hostname = self.compute2.host
5422 
5423         # the ability to force evacuate a server is removed entirely in 2.68
5424         self.api.microversion = '2.67'
5425 
5426         server = self._boot_and_check_allocations(
5427             self.flavor1, source_hostname)
5428 
5429         source_compute_id = self.admin_api.get_services(
5430             host=source_hostname, binary='nova-compute')[0]['id']
5431 
5432         self.compute1.stop()
5433         # force it down to avoid waiting for the service group to time out
5434         self.admin_api.put_service(
5435             source_compute_id, {'forced_down': 'true'})
5436 
5437         # evacuate the server and force the destination host which bypasses
5438         # the scheduler
5439         post = {
5440             'evacuate': {
5441                 'host': dest_hostname,
5442                 'force': True
5443             }
5444         }
5445         self.api.post_server_action(server['id'], post)
5446         self._wait_for_migration_status(server, ['error'])
5447         expected_params = {'OS-EXT-SRV-ATTR:host': source_hostname,
5448                            'status': 'ACTIVE'}
5449         server = self._wait_for_server_parameter(self.api, server,
5450                                                  expected_params)
5451         self.assertIn('Unable to move instance %s to host host2. The instance '
5452                       'has complex allocations on the source host so move '
5453                       'cannot be forced.' %
5454                       server['id'],
5455                       self.stdlog.logger.output)
5456 
5457         # Run the periodics to show those don't modify allocations.
5458         self._run_periodics()
5459 
5460         source_rp_uuid = self._get_provider_uuid_by_host(source_hostname)
5461         dest_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
5462 
5463         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5464 
5465         self.assertRequestMatchesUsage(
5466             {'VCPU': 0,
5467              'MEMORY_MB': 0,
5468              'DISK_GB': 0}, dest_rp_uuid)
5469 
5470         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5471                                            source_rp_uuid)
5472 
5473         # restart the source compute
5474         self.restart_compute_service(self.compute1)
5475         self.admin_api.put_service(
5476             source_compute_id, {'forced_down': 'false'})
5477 
5478         # Run the periodics again to show they don't change anything.
5479         self._run_periodics()
5480 
5481         # When the source node starts up nothing should change as the
5482         # evacuation failed
5483         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5484 
5485         self.assertRequestMatchesUsage(
5486             {'VCPU': 0,
5487              'MEMORY_MB': 0,
5488              'DISK_GB': 0}, dest_rp_uuid)
5489 
5490         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5491                                            source_rp_uuid)
5492 
5493         self._delete_and_check_allocations(server)
5494 
5495 
5496 # NOTE(gibi): There is another case NestedToFlat but that leads to the same
5497 # code path that NestedToNested as in both cases the instance will have
5498 # complex allocation on the source host which is already covered in
5499 # ServerMovingTestsWithNestedResourceRequests
5500 class ServerMovingTestsFromFlatToNested(
5501         integrated_helpers.ProviderUsageBaseTestCase):
5502     """Tests trying to move servers from a compute with a flat RP tree to a
5503     compute with a nested RP tree and assert that the blind allocation copy
5504     fails cleanly.
5505     """
5506 
5507     REQUIRES_LOCKING = True
5508     compute_driver = 'fake.MediumFakeDriver'
5509 
5510     def setUp(self):
5511         super(ServerMovingTestsFromFlatToNested, self).setUp()
5512         flavors = self.api.get_flavors()
5513         self.flavor1 = flavors[0]
5514         self.api.post_extra_spec(
5515             self.flavor1['id'], {'extra_specs': {'resources:CUSTOM_MAGIC': 1}})
5516         self.flavor1['extra_specs'] = {'resources:CUSTOM_MAGIC': 1}
5517 
5518     def test_force_live_migrate_from_flat_to_nested(self):
5519         # first compute will start with the flat RP tree but we add
5520         # CUSTOM_MAGIC inventory to the root compute RP
5521         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5522 
5523         # the ability to force live migrate a server is removed entirely in
5524         # 2.68
5525         self.api.microversion = '2.67'
5526 
5527         def stub_update_provider_tree(self, provider_tree, nodename,
5528                                       allocations=None):
5529             # do the regular inventory update
5530             orig_update_provider_tree(
5531                 self, provider_tree, nodename, allocations)
5532             if nodename == 'host1':
5533                 # add the extra resource
5534                 inv = provider_tree.data(nodename).inventory
5535                 inv['CUSTOM_MAGIC'] = {
5536                     'total': 10,
5537                     'reserved': 0,
5538                     'min_unit': 1,
5539                     'max_unit': 10,
5540                     'step_size': 1,
5541                     'allocation_ratio': 1,
5542                 }
5543                 provider_tree.update_inventory(nodename, inv)
5544 
5545         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5546                       stub_update_provider_tree)
5547         self.compute1 = self._start_compute(host='host1')
5548         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5549 
5550         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5551         # start the second compute with nested RP tree
5552         self.flags(
5553             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5554         self.compute2 = self._start_compute(host='host2')
5555 
5556         # try to force live migrate from flat to nested.
5557         post = {
5558             'os-migrateLive': {
5559                 'host': 'host2',
5560                 'block_migration': True,
5561                 'force': True,
5562             }
5563         }
5564 
5565         self.api.post_server_action(server['id'], post)
5566         # We expect that the migration will fail as force migrate tries to
5567         # blindly copy the source allocation to the destination but on the
5568         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5569         # provider as that resource is reported on a child provider.
5570         self._wait_for_server_parameter(self.api, server,
5571             {'OS-EXT-SRV-ATTR:host': 'host1',
5572              'status': 'ACTIVE'})
5573 
5574         migration = self._wait_for_migration_status(server, ['error'])
5575         self.assertEqual('host1', migration['source_compute'])
5576         self.assertEqual('host2', migration['dest_compute'])
5577 
5578         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5579         # from the dest node root RP and placement rejects the that allocation.
5580         self.assertIn("Unable to allocate inventory: Inventory for "
5581                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5582         self.assertIn('No valid host was found. Unable to move instance %s to '
5583                       'host host2. There is not enough capacity on the host '
5584                       'for the instance.' % server['id'],
5585                       self.stdlog.logger.output)
5586 
5587         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5588 
5589         # There should be no usage for the dest
5590         self.assertRequestMatchesUsage(
5591             {'VCPU': 0,
5592              'MEMORY_MB': 0,
5593              'DISK_GB': 0}, dest_rp_uuid)
5594 
5595         # and everything stays at the source
5596         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5597         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5598                                            source_rp_uuid)
5599 
5600         self._delete_and_check_allocations(server)
5601 
5602     def test_force_evacuate_from_flat_to_nested(self):
5603         # first compute will start with the flat RP tree but we add
5604         # CUSTOM_MAGIC inventory to the root compute RP
5605         orig_update_provider_tree = fake.MediumFakeDriver.update_provider_tree
5606 
5607         # the ability to force evacuate a server is removed entirely in 2.68
5608         self.api.microversion = '2.67'
5609 
5610         def stub_update_provider_tree(self, provider_tree, nodename,
5611                                       allocations=None):
5612             # do the regular inventory update
5613             orig_update_provider_tree(
5614                 self, provider_tree, nodename, allocations)
5615             if nodename == 'host1':
5616                 # add the extra resource
5617                 inv = provider_tree.data(nodename).inventory
5618                 inv['CUSTOM_MAGIC'] = {
5619                     'total': 10,
5620                     'reserved': 0,
5621                     'min_unit': 1,
5622                     'max_unit': 10,
5623                     'step_size': 1,
5624                     'allocation_ratio': 1,
5625                 }
5626                 provider_tree.update_inventory(nodename, inv)
5627 
5628         self.stub_out('nova.virt.fake.FakeDriver.update_provider_tree',
5629                       stub_update_provider_tree)
5630         self.compute1 = self._start_compute(host='host1')
5631         source_rp_uuid = self._get_provider_uuid_by_host('host1')
5632 
5633         server = self._boot_and_check_allocations(self.flavor1, 'host1')
5634         # start the second compute with nested RP tree
5635         self.flags(
5636             compute_driver='fake.MediumFakeDriverWithNestedCustomResources')
5637         self.compute2 = self._start_compute(host='host2')
5638 
5639         source_compute_id = self.admin_api.get_services(
5640             host='host1', binary='nova-compute')[0]['id']
5641         self.compute1.stop()
5642         # force it down to avoid waiting for the service group to time out
5643         self.admin_api.put_service(
5644             source_compute_id, {'forced_down': 'true'})
5645 
5646         # try to force evacuate from flat to nested.
5647         post = {
5648             'evacuate': {
5649                 'host': 'host2',
5650                 'force': True,
5651             }
5652         }
5653 
5654         self.api.post_server_action(server['id'], post)
5655         # We expect that the evacuation will fail as force evacuate tries to
5656         # blindly copy the source allocation to the destination but on the
5657         # destination there is no inventory of CUSTOM_MAGIC on the compute node
5658         # provider as that resource is reported on a child provider.
5659         self._wait_for_server_parameter(self.api, server,
5660             {'OS-EXT-SRV-ATTR:host': 'host1',
5661              'status': 'ACTIVE'})
5662 
5663         migration = self._wait_for_migration_status(server, ['error'])
5664         self.assertEqual('host1', migration['source_compute'])
5665         self.assertEqual('host2', migration['dest_compute'])
5666 
5667         # Nova fails the migration because it ties to allocation CUSTOM_MAGIC
5668         # from the dest node root RP and placement rejects the that allocation.
5669         self.assertIn("Unable to allocate inventory: Inventory for "
5670                       "'CUSTOM_MAGIC'", self.stdlog.logger.output)
5671         self.assertIn('No valid host was found. Unable to move instance %s to '
5672                       'host host2. There is not enough capacity on the host '
5673                       'for the instance.' % server['id'],
5674                       self.stdlog.logger.output)
5675 
5676         dest_rp_uuid = self._get_provider_uuid_by_host('host2')
5677 
5678         # There should be no usage for the dest
5679         self.assertRequestMatchesUsage(
5680             {'VCPU': 0,
5681              'MEMORY_MB': 0,
5682              'DISK_GB': 0}, dest_rp_uuid)
5683 
5684         # and everything stays at the source
5685         self.assertFlavorMatchesUsage(source_rp_uuid, self.flavor1)
5686         self.assertFlavorMatchesAllocation(self.flavor1, server['id'],
5687                                            source_rp_uuid)
5688 
5689         self._delete_and_check_allocations(server)
5690 
5691 
5692 class PortResourceRequestBasedSchedulingTestBase(
5693         integrated_helpers.ProviderUsageBaseTestCase):
5694 
5695     compute_driver = 'fake.FakeDriverWithPciResources'
5696 
5697     CUSTOM_VNIC_TYPE_NORMAL = 'CUSTOM_VNIC_TYPE_NORMAL'
5698     CUSTOM_VNIC_TYPE_DIRECT = 'CUSTOM_VNIC_TYPE_DIRECT'
5699     CUSTOM_PHYSNET1 = 'CUSTOM_PHYSNET1'
5700     CUSTOM_PHYSNET2 = 'CUSTOM_PHYSNET2'
5701     CUSTOM_PHYSNET3 = 'CUSTOM_PHYSNET3'
5702 
5703     def setUp(self):
5704         # enable PciPassthroughFilter to support SRIOV before the base class
5705         # starts the scheduler
5706         if 'PciPassthroughFilter' not in CONF.filter_scheduler.enabled_filters:
5707             self.flags(
5708                 enabled_filters=CONF.filter_scheduler.enabled_filters
5709                                 + ['PciPassthroughFilter'],
5710                 group='filter_scheduler')
5711 
5712         self.useFixture(
5713             fake.FakeDriverWithPciResources.
5714                 FakeDriverWithPciResourcesConfigFixture())
5715 
5716         super(PortResourceRequestBasedSchedulingTestBase, self).setUp()
5717         self.compute1 = self._start_compute('host1')
5718         self.compute1_rp_uuid = self._get_provider_uuid_by_host('host1')
5719         self.ovs_bridge_rp_per_host = {}
5720         self.flavor = self.api.get_flavors()[0]
5721         self.flavor_with_group_policy = self.api.get_flavors()[1]
5722 
5723         # Setting group policy for placement. This is mandatory when more than
5724         # one request group is included in the allocation candidate request and
5725         # we have tests with two ports both having resource request modelled as
5726         # two separate request groups.
5727         self.admin_api.post_extra_spec(
5728             self.flavor_with_group_policy['id'],
5729             {'extra_specs': {'group_policy': 'isolate'}})
5730 
5731         self._create_networking_rp_tree(self.compute1_rp_uuid)
5732 
5733         # add an extra port and the related network to the neutron fixture
5734         # specifically for these tests. It cannot be added globally in the
5735         # fixture init as it adds a second network that makes auto allocation
5736         # based test to fail due to ambiguous networks.
5737         self.neutron._ports[
5738             self.neutron.port_with_sriov_resource_request['id']] = \
5739             copy.deepcopy(self.neutron.port_with_sriov_resource_request)
5740         self.neutron._networks[
5741             self.neutron.network_2['id']] = self.neutron.network_2
5742         self.neutron._subnets[
5743             self.neutron.subnet_2['id']] = self.neutron.subnet_2
5744 
5745     def _create_server(self, flavor, networks):
5746         server_req = self._build_minimal_create_server_request(
5747             self.api, 'bandwidth-aware-server',
5748             image_uuid='76fa36fc-c930-4bf3-8c8a-ea2a2420deb6',
5749             flavor_id=flavor['id'], networks=networks)
5750         return self.api.post_server({'server': server_req})
5751 
5752     def _set_provider_inventories(self, rp_uuid, inventories):
5753         rp = self.placement_api.get(
5754             '/resource_providers/%s' % rp_uuid).body
5755         inventories['resource_provider_generation'] = rp['generation']
5756         return self._update_inventory(rp_uuid, inventories)
5757 
5758     def _create_ovs_networking_rp_tree(self, compute_rp_uuid):
5759         # we need uuid sentinel for the test to make pep8 happy but we need a
5760         # unique one per compute so here is some ugliness
5761         ovs_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'ovs agent')
5762         agent_rp_req = {
5763             "name": ovs_agent_rp_uuid,
5764             "uuid": ovs_agent_rp_uuid,
5765             "parent_provider_uuid": compute_rp_uuid
5766         }
5767         self.placement_api.post('/resource_providers',
5768                                 body=agent_rp_req,
5769                                 version='1.20')
5770         ovs_bridge_rp_uuid = getattr(uuids, ovs_agent_rp_uuid + 'ovs br')
5771         ovs_bridge_req = {
5772             "name": ovs_bridge_rp_uuid,
5773             "uuid": ovs_bridge_rp_uuid,
5774             "parent_provider_uuid": ovs_agent_rp_uuid
5775         }
5776         self.placement_api.post('/resource_providers',
5777                                 body=ovs_bridge_req,
5778                                 version='1.20')
5779         self.ovs_bridge_rp_per_host[compute_rp_uuid] = ovs_bridge_rp_uuid
5780 
5781         self._set_provider_inventories(
5782             ovs_bridge_rp_uuid,
5783             {"inventories": {
5784                 rc_fields.ResourceClass.NET_BW_IGR_KILOBIT_PER_SEC:
5785                     {"total": 10000},
5786                 rc_fields.ResourceClass.NET_BW_EGR_KILOBIT_PER_SEC:
5787                     {"total": 10000},
5788             }})
5789 
5790         self._create_trait(self.CUSTOM_VNIC_TYPE_NORMAL)
5791         self._create_trait(self.CUSTOM_PHYSNET2)
5792 
5793         self._set_provider_traits(
5794             ovs_bridge_rp_uuid,
5795             [self.CUSTOM_VNIC_TYPE_NORMAL, self.CUSTOM_PHYSNET2])
5796 
5797     def _create_pf_device_rp(
5798             self, device_rp_uuid, parent_rp_uuid, inventories, traits,
5799             device_rp_name=None):
5800         """Create a RP in placement for a physical function network device with
5801         traits and inventories.
5802         """
5803 
5804         if not device_rp_name:
5805             device_rp_name = device_rp_uuid
5806 
5807         sriov_pf_req = {
5808             "name": device_rp_name,
5809             "uuid": device_rp_uuid,
5810             "parent_provider_uuid": parent_rp_uuid
5811         }
5812         self.placement_api.post('/resource_providers',
5813                                 body=sriov_pf_req,
5814                                 version='1.20')
5815 
5816         self._set_provider_inventories(
5817             device_rp_uuid,
5818             {"inventories": inventories})
5819 
5820         for trait in traits:
5821             self._create_trait(trait)
5822 
5823         self._set_provider_traits(
5824             device_rp_uuid,
5825             traits)
5826 
5827     def _create_sriov_networking_rp_tree(self, compute_rp_uuid):
5828         # Create a matching RP tree in placement for the PCI devices added to
5829         # the passthrough_whitelist config during setUp() and PCI devices
5830         # present in the FakeDriverWithPciResources virt driver.
5831         #
5832         # * PF1 represents the PCI device 0000:01:00, it will be mapped to
5833         # physnet1 and it will have bandwidth inventory.
5834         # * PF2 represents the PCI device 0000:02:00, it will be mapped to
5835         # physnet2 it will have bandwidth inventory.
5836         # * PF3 represents the PCI device 0000:03:00 and, it will be mapped to
5837         # physnet2 but it will not have bandwidth inventory.
5838 
5839         compute_name = compute_rp_uuid
5840         sriov_agent_rp_uuid = getattr(uuids, compute_rp_uuid + 'sriov agent')
5841         agent_rp_req = {
5842             "name": "%s:NIC Switch agent" % compute_name,
5843             "uuid": sriov_agent_rp_uuid,
5844             "parent_provider_uuid": compute_rp_uuid
5845         }
5846         self.placement_api.post('/resource_providers',
5847                                 body=agent_rp_req,
5848                                 version='1.20')
5849 
5850         self.sriov_pf1_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF1')
5851         inventories = {
5852             rc_fields.ResourceClass.NET_BW_IGR_KILOBIT_PER_SEC:
5853                 {"total": 100000},
5854             rc_fields.ResourceClass.NET_BW_EGR_KILOBIT_PER_SEC:
5855                 {"total": 100000},
5856         }
5857         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET1]
5858         self._create_pf_device_rp(
5859             self.sriov_pf1_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5860             device_rp_name="%s:NIC Switch agent:ens1" % compute_name)
5861 
5862         self.sriov_pf2_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF2')
5863         inventories = {
5864             rc_fields.ResourceClass.NET_BW_IGR_KILOBIT_PER_SEC:
5865                 {"total": 100000},
5866             rc_fields.ResourceClass.NET_BW_EGR_KILOBIT_PER_SEC:
5867                 {"total": 100000},
5868         }
5869         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
5870         self._create_pf_device_rp(
5871             self.sriov_pf2_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5872             device_rp_name="%s:NIC Switch agent:ens2" % compute_name)
5873 
5874         self.sriov_pf3_rp_uuid = getattr(uuids, sriov_agent_rp_uuid + 'PF3')
5875         inventories = {}
5876         traits = [self.CUSTOM_VNIC_TYPE_DIRECT, self.CUSTOM_PHYSNET2]
5877         self._create_pf_device_rp(
5878             self.sriov_pf3_rp_uuid, sriov_agent_rp_uuid, inventories, traits,
5879             device_rp_name="%s:NIC Switch agent:ens3" % compute_name)
5880 
5881     def _create_networking_rp_tree(self, compute_rp_uuid):
5882         # let's simulate what the neutron would do
5883         self._create_ovs_networking_rp_tree(compute_rp_uuid)
5884         self._create_sriov_networking_rp_tree(compute_rp_uuid)
5885 
5886     def assertPortMatchesAllocation(self, port, allocations):
5887         port_request = port['resource_request']['resources']
5888         for rc, amount in allocations.items():
5889             self.assertEqual(port_request[rc], amount,
5890                              'port %s requested %d %s '
5891                              'resources but got allocation %d' %
5892                              (port['id'], port_request[rc], rc,
5893                               amount))
5894 
5895 
5896 class PortResourceRequestBasedSchedulingTest(
5897         PortResourceRequestBasedSchedulingTestBase):
5898     """Tests for handling servers with ports having resource requests """
5899 
5900     def _add_resource_request_to_a_bound_port(self, port_id):
5901         # NOTE(gibi): self.neutron._ports contains a copy of each neutron port
5902         # defined on class level in the fixture. So modifying what is in the
5903         # _ports list is safe as it is re-created for each Neutron fixture
5904         # instance therefore for each individual test using that fixture.
5905         bound_port = self.neutron._ports[port_id]
5906         bound_port['resource_request'] = (
5907             self.neutron.port_with_resource_request['resource_request'])
5908 
5909     def test_interface_attach_with_port_resource_request(self):
5910         # create a server
5911         server = self._create_server(
5912             flavor=self.flavor,
5913             networks=[{'port': self.neutron.port_1['id']}])
5914         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5915 
5916         # try to add a port with resource request
5917         post = {
5918             'interfaceAttachment': {
5919                 'port_id': self.neutron.port_with_resource_request['id']
5920         }}
5921         ex = self.assertRaises(client.OpenStackApiException,
5922                                self.api.attach_interface,
5923                                server['id'], post)
5924         self.assertEqual(400, ex.response.status_code)
5925         self.assertIn('Attaching interfaces with QoS policy is '
5926                       'not supported for instance',
5927                       six.text_type(ex))
5928 
5929     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
5930     def test_interface_attach_with_network_create_port_has_resource_request(
5931             self, mock_neutron_create_port):
5932         # create a server
5933         server = self._create_server(
5934             flavor=self.flavor,
5935             networks=[{'port': self.neutron.port_1['id']}])
5936         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5937 
5938         # the interfaceAttach operation below will result in a new port being
5939         # created in the network that is attached. Make sure that neutron
5940         # returns a port that has resource request.
5941         mock_neutron_create_port.return_value = (
5942             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
5943 
5944         # try to attach a network
5945         post = {
5946             'interfaceAttachment': {
5947                 'net_id': self.neutron.network_1['id']
5948         }}
5949         ex = self.assertRaises(client.OpenStackApiException,
5950                                self.api.attach_interface,
5951                                server['id'], post)
5952         self.assertEqual(400, ex.response.status_code)
5953         self.assertIn('Using networks with QoS policy is not supported for '
5954                       'instance',
5955                       six.text_type(ex))
5956 
5957     @mock.patch('nova.tests.fixtures.NeutronFixture.create_port')
5958     def test_create_server_with_network_create_port_has_resource_request(
5959             self, mock_neutron_create_port):
5960         # the server create operation below will result in a new port being
5961         # created in the network. Make sure that neutron returns a port that
5962         # has resource request.
5963         mock_neutron_create_port.return_value = (
5964             {'port': copy.deepcopy(self.neutron.port_with_resource_request)})
5965 
5966         server = self._create_server(
5967             flavor=self.flavor,
5968             networks=[{'uuid': self.neutron.network_1['id']}])
5969         server = self._wait_for_state_change(self.admin_api, server, 'ERROR')
5970 
5971         self.assertEqual(500, server['fault']['code'])
5972         self.assertIn('Failed to allocate the network',
5973                       server['fault']['message'])
5974 
5975     def test_create_server_with_port_resource_request_old_microversion(self):
5976         ex = self.assertRaises(
5977             client.OpenStackApiException, self._create_server,
5978             flavor=self.flavor,
5979             networks=[{'port': self.neutron.port_with_resource_request['id']}])
5980 
5981         self.assertEqual(400, ex.response.status_code)
5982         self.assertIn(
5983             "Creating servers with ports having resource requests, like a "
5984             "port with a QoS minimum bandwidth policy, is not supported with "
5985             "this microversion", six.text_type(ex))
5986 
5987     def test_resize_server_with_port_resource_request_old_microversion(self):
5988         server = self._create_server(
5989             flavor=self.flavor,
5990             networks=[{'port': self.neutron.port_1['id']}])
5991         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
5992 
5993         # We need to simulate that the above server has a port that has
5994         # resource request; we cannot boot with such a port but legacy servers
5995         # can exist with such a port.
5996         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
5997 
5998         resize_req = {
5999             'resize': {
6000                 'flavorRef': self.flavor['id']
6001             }
6002         }
6003         ex = self.assertRaises(
6004             client.OpenStackApiException,
6005             self.api.post_server_action, server['id'], resize_req)
6006 
6007         self.assertEqual(400, ex.response.status_code)
6008         self.assertIn(
6009             'The resize action on a server with ports having resource '
6010             'requests', six.text_type(ex))
6011 
6012     def test_migrate_server_with_port_resource_request_old_microversion(self):
6013         server = self._create_server(
6014             flavor=self.flavor,
6015             networks=[{'port': self.neutron.port_1['id']}])
6016         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6017 
6018         # We need to simulate that the above server has a port that has
6019         # resource request; we cannot boot with such a port but legacy servers
6020         # can exist with such a port.
6021         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6022 
6023         ex = self.assertRaises(
6024             client.OpenStackApiException,
6025             self.api.post_server_action, server['id'], {'migrate': None})
6026 
6027         self.assertEqual(400, ex.response.status_code)
6028         self.assertIn(
6029             'The migrate action on a server with ports having resource '
6030             'requests', six.text_type(ex))
6031 
6032     def test_live_migrate_server_with_port_resource_request_old_microversion(
6033             self):
6034         server = self._create_server(
6035             flavor=self.flavor,
6036             networks=[{'port': self.neutron.port_1['id']}])
6037         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6038 
6039         # We need to simulate that the above server has a port that has
6040         # resource request; we cannot boot with such a port but legacy servers
6041         # can exist with such a port.
6042         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6043 
6044         post = {
6045             'os-migrateLive': {
6046                 'host': None,
6047                 'block_migration': False,
6048             }
6049         }
6050         ex = self.assertRaises(
6051             client.OpenStackApiException,
6052             self.api.post_server_action, server['id'], post)
6053 
6054         self.assertEqual(400, ex.response.status_code)
6055         self.assertIn(
6056             'The os-migrateLive action on a server with ports having resource '
6057             'requests', six.text_type(ex))
6058 
6059     def test_evacuate_server_with_port_resource_request_old_microversion(
6060             self):
6061         server = self._create_server(
6062             flavor=self.flavor,
6063             networks=[{'port': self.neutron.port_1['id']}])
6064         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6065 
6066         # We need to simulate that the above server has a port that has
6067         # resource request; we cannot boot with such a port but legacy servers
6068         # can exist with such a port.
6069         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6070 
6071         ex = self.assertRaises(
6072             client.OpenStackApiException,
6073             self.api.post_server_action, server['id'], {'evacuate': {}})
6074 
6075         self.assertEqual(400, ex.response.status_code)
6076         self.assertIn(
6077             'The evacuate action on a server with ports having resource '
6078             'requests', six.text_type(ex))
6079 
6080     def test_unshelve_offloaded_server_with_port_resource_request_old_version(
6081             self):
6082         server = self._create_server(
6083             flavor=self.flavor,
6084             networks=[{'port': self.neutron.port_1['id']}])
6085         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6086 
6087         # with default config shelve means immediate offload as well
6088         req = {
6089             'shelve': {}
6090         }
6091         self.api.post_server_action(server['id'], req)
6092         self._wait_for_server_parameter(
6093             self.api, server, {'status': 'SHELVED_OFFLOADED'})
6094 
6095         # We need to simulate that the above server has a port that has
6096         # resource request; we cannot boot with such a port but legacy servers
6097         # can exist with such a port.
6098         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6099 
6100         ex = self.assertRaises(
6101             client.OpenStackApiException,
6102             self.api.post_server_action, server['id'], {'unshelve': {}})
6103 
6104         self.assertEqual(400, ex.response.status_code)
6105         self.assertIn(
6106             'The unshelve action on a server with ports having resource '
6107             'requests', six.text_type(ex))
6108 
6109     def test_unshelve_not_offloaded_server_with_port_resource_request(
6110             self):
6111         """If the server is not offloaded then unshelving does not cause a new
6112         resource allocation therefore having port resource request is
6113         irrelevant. This test asserts that such unshelve request is not
6114         rejected.
6115         """
6116         server = self._create_server(
6117             flavor=self.flavor,
6118             networks=[{'port': self.neutron.port_1['id']}])
6119         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6120 
6121         # avoid automatic shelve offloading
6122         self.flags(shelved_offload_time=-1)
6123         req = {
6124             'shelve': {}
6125         }
6126         self.api.post_server_action(server['id'], req)
6127         self._wait_for_server_parameter(
6128             self.api, server, {'status': 'SHELVED'})
6129 
6130         # We need to simulate that the above server has a port that has
6131         # resource request; we cannot boot with such a port but legacy servers
6132         # can exist with such a port.
6133         self._add_resource_request_to_a_bound_port(self.neutron.port_1['id'])
6134 
6135         self.api.post_server_action(server['id'], {'unshelve': {}})
6136         self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6137 
6138 
6139 class PortResourceRequestBasedSchedulingTestIgnoreMicroversionCheck(
6140         PortResourceRequestBasedSchedulingTestBase):
6141     """Tests creating a server with a pre-existing port that has a resource
6142     request for a QoS minimum bandwidth policy. Stubs out the
6143     supports_port_resource_request control method in the API in order to
6144     test the functionality between the API and scheduler before the
6145     microversion is added.
6146     """
6147 
6148     def setUp(self):
6149         super(
6150             PortResourceRequestBasedSchedulingTestIgnoreMicroversionCheck,
6151             self).setUp()
6152 
6153         # NOTE(gibi): This mock turns off the api microversion that prevents
6154         # handling of instances operations if the request involves ports with
6155         # resource request with old microversion. The new microversion does not
6156         # exists yet as the whole feature is not read for end user consumption.
6157         # This functional tests however would like to prove that some use cases
6158         # already work.
6159         self.useFixture(
6160             fixtures.MockPatch(
6161                 'nova.api.openstack.common.'
6162                 'supports_port_resource_request',
6163                 return_value=True))
6164 
6165     def test_boot_server_with_two_ports_one_having_resource_request(self):
6166         non_qos_port = self.neutron.port_1
6167         qos_port = self.neutron.port_with_resource_request
6168 
6169         server = self._create_server(
6170             flavor=self.flavor,
6171             networks=[{'port': non_qos_port['id']},
6172                       {'port': qos_port['id']}])
6173         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6174         updated_non_qos_port = self.neutron.show_port(
6175             non_qos_port['id'])['port']
6176         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6177 
6178         allocations = self.placement_api.get(
6179             '/allocations/%s' % server['id']).body['allocations']
6180 
6181         # We expect one set of allocations for the compute resources on the
6182         # compute rp and one set for the networking resources on the ovs bridge
6183         # rp due to the qos_port resource request
6184         self.assertEqual(2, len(allocations))
6185         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6186         network_allocations = allocations[
6187             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6188 
6189         self.assertEqual(self._resources_from_flavor(self.flavor),
6190                          compute_allocations)
6191         self.assertPortMatchesAllocation(qos_port, network_allocations)
6192 
6193         # We expect that only the RP uuid of the networking RP having the port
6194         # allocation is sent in the port binding for the port having resource
6195         # request
6196         qos_binding_profile = updated_qos_port['binding:profile']
6197         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6198                          qos_binding_profile['allocation'])
6199 
6200         # And we expect not to have any allocation set in the port binding for
6201         # the port that doesn't have resource request
6202         self.assertNotIn('binding:profile', updated_non_qos_port)
6203 
6204         self._delete_and_check_allocations(server)
6205 
6206         # assert that unbind removes the allocation from the binding of the
6207         # port that got allocation during the bind
6208         updated_qos_port = self.neutron.show_port(qos_port['id'])['port']
6209         binding_profile = updated_qos_port['binding:profile']
6210         self.assertNotIn('allocation', binding_profile)
6211 
6212     def test_one_ovs_one_sriov_port(self):
6213         ovs_port = self.neutron.port_with_resource_request
6214         sriov_port = self.neutron.port_with_sriov_resource_request
6215 
6216         server = self._create_server(flavor=self.flavor_with_group_policy,
6217                                      networks=[{'port': ovs_port['id']},
6218                                                {'port': sriov_port['id']}])
6219 
6220         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6221 
6222         ovs_port = self.neutron.show_port(ovs_port['id'])['port']
6223         sriov_port = self.neutron.show_port(sriov_port['id'])['port']
6224 
6225         allocations = self.placement_api.get(
6226             '/allocations/%s' % server['id']).body['allocations']
6227 
6228         # We expect one set of allocations for the compute resources on the
6229         # compute rp and one set for the networking resources on the ovs bridge
6230         # rp and on the sriov PF rp.
6231         self.assertEqual(3, len(allocations))
6232         compute_allocations = allocations[self.compute1_rp_uuid]['resources']
6233         ovs_allocations = allocations[
6234             self.ovs_bridge_rp_per_host[self.compute1_rp_uuid]]['resources']
6235         sriov_allocations = allocations[self.sriov_pf2_rp_uuid]['resources']
6236 
6237         self.assertEqual(
6238             self._resources_from_flavor(self.flavor_with_group_policy),
6239             compute_allocations)
6240 
6241         self.assertPortMatchesAllocation(ovs_port, ovs_allocations)
6242         self.assertPortMatchesAllocation(sriov_port, sriov_allocations)
6243 
6244         # We expect that only the RP uuid of the networking RP having the port
6245         # allocation is sent in the port binding for the port having resource
6246         # request
6247         ovs_binding = ovs_port['binding:profile']
6248         self.assertEqual(self.ovs_bridge_rp_per_host[self.compute1_rp_uuid],
6249                          ovs_binding['allocation'])
6250         sriov_binding = sriov_port['binding:profile']
6251         self.assertEqual(self.sriov_pf2_rp_uuid,
6252                          sriov_binding['allocation'])
6253 
6254 
6255 class PortResourceRequestReSchedulingTestIgnoreMicroversionCheck(
6256         PortResourceRequestBasedSchedulingTestBase):
6257     """Similar to PortResourceRequestBasedSchedulingTestIgnoreMicroversionCheck
6258     except this test uses FakeRescheduleDriver which will test reschedules
6259     during server create work as expected, i.e. that the resource request
6260     allocations are moved from the initially selected compute to the
6261     alternative compute.
6262     """
6263 
6264     compute_driver = 'fake.FakeRescheduleDriver'
6265 
6266     def setUp(self):
6267         super(
6268             PortResourceRequestReSchedulingTestIgnoreMicroversionCheck,
6269             self).setUp()
6270         self.compute2 = self._start_compute('host2')
6271         self.compute2_rp_uuid = self._get_provider_uuid_by_host('host2')
6272         self._create_networking_rp_tree(self.compute2_rp_uuid)
6273 
6274         # NOTE(gibi): This mock turns off the api microversion that prevents
6275         # handling of instances operations if the request involves ports with
6276         # resource request with old microversion. The new microversion does not
6277         # exists yet as the whole feature is not read for end user consumption.
6278         # This functional tests however would like to prove that some use cases
6279         # already work.
6280         self.useFixture(
6281             fixtures.MockPatch(
6282                 'nova.api.openstack.common.'
6283                 'supports_port_resource_request',
6284                 return_value=True))
6285 
6286     def _create_networking_rp_tree(self, compute_rp_uuid):
6287         # let's simulate what the neutron would do
6288         self._create_ovs_networking_rp_tree(compute_rp_uuid)
6289 
6290     def test_boot_reschedule_success(self):
6291         port = self.neutron.port_with_resource_request
6292 
6293         server = self._create_server(
6294             flavor=self.flavor,
6295             networks=[{'port': port['id']}])
6296         server = self._wait_for_state_change(self.admin_api, server, 'ACTIVE')
6297         updated_port = self.neutron.show_port(port['id'])['port']
6298 
6299         dest_hostname = server['OS-EXT-SRV-ATTR:host']
6300         dest_compute_rp_uuid = self._get_provider_uuid_by_host(dest_hostname)
6301 
6302         failed_compute_rp = (self.compute1_rp_uuid
6303                              if dest_compute_rp_uuid == self.compute2_rp_uuid
6304                              else self.compute2_rp_uuid)
6305 
6306         allocations = self.placement_api.get(
6307             '/allocations/%s' % server['id']).body['allocations']
6308 
6309         # We expect one set of allocations for the compute resources on the
6310         # compute rp and one set for the networking resources on the ovs bridge
6311         # rp
6312         self.assertEqual(2, len(allocations))
6313         compute_allocations = allocations[dest_compute_rp_uuid]['resources']
6314         network_allocations = allocations[
6315             self.ovs_bridge_rp_per_host[dest_compute_rp_uuid]]['resources']
6316 
6317         self.assertEqual(self._resources_from_flavor(self.flavor),
6318                          compute_allocations)
6319         self.assertPortMatchesAllocation(port, network_allocations)
6320 
6321         # assert that the allocations against the host where the spawn
6322         # failed are cleaned up properly
6323         self.assertEqual(
6324             {'VCPU': 0, 'MEMORY_MB': 0, 'DISK_GB': 0},
6325             self._get_provider_usages(failed_compute_rp))
6326         self.assertEqual(
6327             {'NET_BW_EGR_KILOBIT_PER_SEC': 0, 'NET_BW_IGR_KILOBIT_PER_SEC': 0},
6328             self._get_provider_usages(
6329                 self.ovs_bridge_rp_per_host[failed_compute_rp]))
6330 
6331         # We expect that only the RP uuid of the networking RP having the port
6332         # allocation is sent in the port binding
6333         binding_profile = updated_port['binding:profile']
6334         self.assertEqual(self.ovs_bridge_rp_per_host[dest_compute_rp_uuid],
6335                          binding_profile['allocation'])
6336 
6337         self._delete_and_check_allocations(server)
6338 
6339         # assert that unbind removes the allocation from the binding
6340         updated_port = self.neutron.show_port(port['id'])['port']
6341         binding_profile = updated_port['binding:profile']
6342         self.assertNotIn('allocation', binding_profile)
6343 
6344     def test_boot_reschedule_fill_provider_mapping_raises(self):
6345         """Verify that if the  _fill_provider_mapping raises during re-schedule
6346         then the instance is properly put into ERROR state.
6347         """
6348 
6349         port = self.neutron.port_with_resource_request
6350 
6351         server = self._create_server(
6352             flavor=self.flavor,
6353             networks=[{'port': port['id']}])
6354 
6355         # First call is during boot, we want that to succeed normally. Then the
6356         # fake virt driver triggers a re-schedule. During that re-schedule the
6357         # is fill called again, and we simulate that call raises.
6358         from nova.conductor import manager
6359         fill = manager.ComputeTaskManager._fill_provider_mapping
6360         calls = []
6361 
6362         def fake_fill(*args, **kwargs):
6363             if not calls:
6364                 calls.append('called')
6365                 return fill(*args, **kwargs)
6366             else:
6367                 raise exception.ConsumerAllocationRetrievalFailed(
6368                     consumer_uuid=uuids.inst1, error='testing')
6369 
6370         with mock.patch(
6371                 'nova.conductor.manager.ComputeTaskManager.'
6372                 '_fill_provider_mapping', side_effect=fake_fill,
6373                 autospec=True):
6374             server = self._wait_for_state_change(
6375                 self.admin_api, server, 'ERROR')
6376 
6377         self.assertIn(
6378             'Failed to retrieve allocations for consumer',
6379             server['fault']['message'])
6380 
6381         self._delete_and_check_allocations(server)
6382 
6383         # assert that unbind removes the allocation from the binding
6384         updated_port = self.neutron.show_port(port['id'])['port']
6385         binding_profile = updated_port['binding:profile']
6386         self.assertNotIn('allocation', binding_profile)
