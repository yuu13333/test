I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import copy
32 import functools
33 import inspect
34 import sys
35 import time
36 import traceback
37 
38 from cinderclient import exceptions as cinder_exception
39 from cursive import exception as cursive_exception
40 import eventlet.event
41 from eventlet import greenthread
42 import eventlet.semaphore
43 import eventlet.timeout
44 import futurist
45 from keystoneauth1 import exceptions as keystone_exception
46 import os_traits
47 from oslo_log import log as logging
48 import oslo_messaging as messaging
49 from oslo_serialization import jsonutils
50 from oslo_service import loopingcall
51 from oslo_service import periodic_task
52 from oslo_utils import excutils
53 from oslo_utils import strutils
54 from oslo_utils import timeutils
55 from oslo_utils import units
56 import six
57 from six.moves import range
58 
59 from nova import block_device
60 from nova.compute import api as compute
61 from nova.compute import build_results
62 from nova.compute import claims
63 from nova.compute import power_state
64 from nova.compute import resource_tracker
65 from nova.compute import rpcapi as compute_rpcapi
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 from nova.compute.utils import wrap_instance_event
69 from nova.compute import vm_states
70 from nova import conductor
71 import nova.conf
72 from nova.console import rpcapi as console_rpcapi
73 import nova.context
74 from nova import exception
75 from nova import exception_wrapper
76 from nova import hooks
77 from nova.i18n import _
78 from nova import image
79 from nova import manager
80 from nova import network
81 from nova.network import base_api as base_net_api
82 from nova.network import model as network_model
83 from nova.network.security_group import openstack_driver
84 from nova import objects
85 from nova.objects import base as obj_base
86 from nova.objects import external_event as external_event_obj
87 from nova.objects import fields
88 from nova.objects import instance as obj_instance
89 from nova.objects import migrate_data as migrate_data_obj
90 from nova.pci import request as pci_req_module
91 from nova.pci import whitelist
92 from nova import rpc
93 from nova import safe_utils
94 from nova.scheduler.client import query
95 from nova.scheduler.client import report
96 from nova.scheduler import utils as scheduler_utils
97 from nova import utils
98 from nova.virt import block_device as driver_block_device
99 from nova.virt import configdrive
100 from nova.virt import driver
101 from nova.virt import event as virtevent
102 from nova.virt import hardware
103 from nova.virt import storage_users
104 from nova.virt import virtapi
105 from nova.volume import cinder
106 
107 CONF = nova.conf.CONF
108 
109 LOG = logging.getLogger(__name__)
110 
111 get_notifier = functools.partial(rpc.get_notifier, service='compute')
112 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
113                                    get_notifier=get_notifier,
114                                    binary='nova-compute')
115 
116 
117 @contextlib.contextmanager
118 def errors_out_migration_ctxt(migration):
119     """Context manager to error out migration on failure."""
120 
121     try:
122         yield
123     except Exception:
124         with excutils.save_and_reraise_exception():
125             if migration:
126                 # We may have been passed None for our migration if we're
127                 # receiving from an older client. The migration will be
128                 # errored via the legacy path.
129                 migration.status = 'error'
130                 try:
131                     migration.save()
132                 except Exception:
133                     LOG.debug(
134                         'Error setting migration status for instance %s.',
135                         migration.instance_uuid, exc_info=True)
136 
137 
138 @utils.expects_func_args('migration')
139 def errors_out_migration(function):
140     """Decorator to error out migration on failure."""
141 
142     @functools.wraps(function)
143     def decorated_function(self, context, *args, **kwargs):
144         wrapped_func = safe_utils.get_wrapped_function(function)
145         keyed_args = inspect.getcallargs(wrapped_func, self, context,
146                                          *args, **kwargs)
147         migration = keyed_args['migration']
148         with errors_out_migration_ctxt(migration):
149             return function(self, context, *args, **kwargs)
150 
151     return decorated_function
152 
153 
154 @utils.expects_func_args('instance')
155 def reverts_task_state(function):
156     """Decorator to revert task_state on failure."""
157 
158     @functools.wraps(function)
159     def decorated_function(self, context, *args, **kwargs):
160         try:
161             return function(self, context, *args, **kwargs)
162         except exception.UnexpectedTaskStateError as e:
163             # Note(maoy): unexpected task state means the current
164             # task is preempted. Do not clear task state in this
165             # case.
166             with excutils.save_and_reraise_exception():
167                 LOG.info("Task possibly preempted: %s",
168                          e.format_message())
169         except Exception:
170             with excutils.save_and_reraise_exception():
171                 wrapped_func = safe_utils.get_wrapped_function(function)
172                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
173                                                  *args, **kwargs)
174                 # NOTE(mriedem): 'instance' must be in keyed_args because we
175                 # have utils.expects_func_args('instance') decorating this
176                 # method.
177                 instance = keyed_args['instance']
178                 original_task_state = instance.task_state
179                 try:
180                     self._instance_update(context, instance, task_state=None)
181                     LOG.info("Successfully reverted task state from %s on "
182                              "failure for instance.",
183                              original_task_state, instance=instance)
184                 except exception.InstanceNotFound:
185                     # We might delete an instance that failed to build shortly
186                     # after it errored out this is an expected case and we
187                     # should not trace on it.
188                     pass
189                 except Exception as e:
190                     LOG.warning("Failed to revert task state for instance. "
191                                 "Error: %s", e, instance=instance)
192 
193     return decorated_function
194 
195 
196 @utils.expects_func_args('instance')
197 def wrap_instance_fault(function):
198     """Wraps a method to catch exceptions related to instances.
199 
200     This decorator wraps a method to catch any exceptions having to do with
201     an instance that may get thrown. It then logs an instance fault in the db.
202     """
203 
204     @functools.wraps(function)
205     def decorated_function(self, context, *args, **kwargs):
206         try:
207             return function(self, context, *args, **kwargs)
208         except exception.InstanceNotFound:
209             raise
210         except Exception as e:
211             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
212             # we will get a KeyError exception which will cover up the real
213             # exception. So, we update kwargs with the values from args first.
214             # then, we can get 'instance' from kwargs easily.
215             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
216 
217             with excutils.save_and_reraise_exception():
218                 compute_utils.add_instance_fault_from_exc(context,
219                         kwargs['instance'], e, sys.exc_info())
220 
221     return decorated_function
222 
223 
224 @utils.expects_func_args('image_id', 'instance')
225 def delete_image_on_error(function):
226     """Used for snapshot related method to ensure the image created in
227     compute.api is deleted when an error occurs.
228     """
229 
230     @functools.wraps(function)
231     def decorated_function(self, context, image_id, instance,
232                            *args, **kwargs):
233         try:
234             return function(self, context, image_id, instance,
235                             *args, **kwargs)
236         except Exception:
237             with excutils.save_and_reraise_exception():
238                 compute_utils.delete_image(
239                     context, instance, self.image_api, image_id,
240                     log_exc_info=True)
241 
242     return decorated_function
243 
244 
245 # TODO(danms): Remove me after Icehouse
246 # TODO(alaski): Actually remove this after Newton, assuming a major RPC bump
247 # NOTE(mikal): if the method being decorated has more than one decorator, then
248 # put this one first. Otherwise the various exception handling decorators do
249 # not function correctly.
250 def object_compat(function):
251     """Wraps a method that expects a new-world instance
252 
253     This provides compatibility for callers passing old-style dict
254     instances.
255     """
256 
257     @functools.wraps(function)
258     def decorated_function(self, context, *args, **kwargs):
259         def _load_instance(instance_or_dict):
260             if isinstance(instance_or_dict, dict):
261                 # try to get metadata and system_metadata for most cases but
262                 # only attempt to load those if the db instance already has
263                 # those fields joined
264                 metas = [meta for meta in ('metadata', 'system_metadata')
265                          if meta in instance_or_dict]
266                 instance = objects.Instance._from_db_object(
267                     context, objects.Instance(), instance_or_dict,
268                     expected_attrs=metas)
269                 instance._context = context
270                 return instance
271             return instance_or_dict
272 
273         try:
274             kwargs['instance'] = _load_instance(kwargs['instance'])
275         except KeyError:
276             args = (_load_instance(args[0]),) + args[1:]
277 
278         migration = kwargs.get('migration')
279         if isinstance(migration, dict):
280             migration = objects.Migration._from_db_object(
281                     context.elevated(), objects.Migration(),
282                     migration)
283             kwargs['migration'] = migration
284 
285         return function(self, context, *args, **kwargs)
286 
287     return decorated_function
288 
289 
290 class InstanceEvents(object):
291     def __init__(self):
292         self._events = {}
293 
294     @staticmethod
295     def _lock_name(instance):
296         return '%s-%s' % (instance.uuid, 'events')
297 
298     def prepare_for_instance_event(self, instance, name, tag):
299         """Prepare to receive an event for an instance.
300 
301         This will register an event for the given instance that we will
302         wait on later. This should be called before initiating whatever
303         action will trigger the event. The resulting eventlet.event.Event
304         object should be wait()'d on to ensure completion.
305 
306         :param instance: the instance for which the event will be generated
307         :param name: the name of the event we're expecting
308         :param tag: the tag associated with the event we're expecting
309         :returns: an event object that should be wait()'d on
310         """
311         if self._events is None:
312             # NOTE(danms): We really should have a more specific error
313             # here, but this is what we use for our default error case
314             raise exception.NovaException('In shutdown, no new events '
315                                           'can be scheduled')
316 
317         @utils.synchronized(self._lock_name(instance))
318         def _create_or_get_event():
319             instance_events = self._events.setdefault(instance.uuid, {})
320             return instance_events.setdefault((name, tag),
321                                               eventlet.event.Event())
322         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
323                   {'name': name, 'tag': tag}, instance=instance)
324         return _create_or_get_event()
325 
326     def pop_instance_event(self, instance, event):
327         """Remove a pending event from the wait list.
328 
329         This will remove a pending event from the wait list so that it
330         can be used to signal the waiters to wake up.
331 
332         :param instance: the instance for which the event was generated
333         :param event: the nova.objects.external_event.InstanceExternalEvent
334                       that describes the event
335         :returns: the eventlet.event.Event object on which the waiters
336                   are blocked
337         """
338         no_events_sentinel = object()
339         no_matching_event_sentinel = object()
340 
341         @utils.synchronized(self._lock_name(instance))
342         def _pop_event():
343             if self._events is None:
344                 LOG.debug('Unexpected attempt to pop events during shutdown',
345                           instance=instance)
346                 return no_events_sentinel
347             events = self._events.get(instance.uuid)
348             if not events:
349                 return no_events_sentinel
350             _event = events.pop((event.name, event.tag), None)
351             if not events:
352                 del self._events[instance.uuid]
353             if _event is None:
354                 return no_matching_event_sentinel
355             return _event
356 
357         result = _pop_event()
358         if result is no_events_sentinel:
359             LOG.debug('No waiting events found dispatching %(event)s',
360                       {'event': event.key},
361                       instance=instance)
362             return None
363         elif result is no_matching_event_sentinel:
364             LOG.debug('No event matching %(event)s in %(events)s',
365                       {'event': event.key,
366                        'events': self._events.get(instance.uuid, {}).keys()},
367                       instance=instance)
368             return None
369         else:
370             return result
371 
372     def clear_events_for_instance(self, instance):
373         """Remove all pending events for an instance.
374 
375         This will remove all events currently pending for an instance
376         and return them (indexed by event name).
377 
378         :param instance: the instance for which events should be purged
379         :returns: a dictionary of {event_name: eventlet.event.Event}
380         """
381         @utils.synchronized(self._lock_name(instance))
382         def _clear_events():
383             if self._events is None:
384                 LOG.debug('Unexpected attempt to clear events during shutdown',
385                           instance=instance)
386                 return dict()
387             # NOTE(danms): We have historically returned the raw internal
388             # format here, which is {event.key: [events, ...])} so just
389             # trivially convert it here.
390             return {'%s-%s' % k: e
391                     for k, e in self._events.pop(instance.uuid, {}).items()}
392         return _clear_events()
393 
394     def cancel_all_events(self):
395         if self._events is None:
396             LOG.debug('Unexpected attempt to cancel events during shutdown.')
397             return
398         our_events = self._events
399         # NOTE(danms): Block new events
400         self._events = None
401 
402         for instance_uuid, events in our_events.items():
403             for (name, tag), eventlet_event in events.items():
404                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
405                           'instance %(instance_uuid)s',
406                           {'name': name,
407                            'tag': tag,
408                            'instance_uuid': instance_uuid})
409                 event = objects.InstanceExternalEvent(
410                     instance_uuid=instance_uuid,
411                     name=name, status='failed',
412                     tag=tag, data={})
413                 eventlet_event.send(event)
414 
415 
416 class ComputeVirtAPI(virtapi.VirtAPI):
417     def __init__(self, compute):
418         super(ComputeVirtAPI, self).__init__()
419         self._compute = compute
420         self.reportclient = compute.reportclient
421 
422     def _default_error_callback(self, event_name, instance):
423         raise exception.NovaException(_('Instance event failed'))
424 
425     @contextlib.contextmanager
426     def wait_for_instance_event(self, instance, event_names, deadline=300,
427                                 error_callback=None):
428         """Plan to wait for some events, run some code, then wait.
429 
430         This context manager will first create plans to wait for the
431         provided event_names, yield, and then wait for all the scheduled
432         events to complete.
433 
434         Note that this uses an eventlet.timeout.Timeout to bound the
435         operation, so callers should be prepared to catch that
436         failure and handle that situation appropriately.
437 
438         If the event is not received by the specified timeout deadline,
439         eventlet.timeout.Timeout is raised.
440 
441         If the event is received but did not have a 'completed'
442         status, a NovaException is raised.  If an error_callback is
443         provided, instead of raising an exception as detailed above
444         for the failure case, the callback will be called with the
445         event_name and instance, and can return True to continue
446         waiting for the rest of the events, False to stop processing,
447         or raise an exception which will bubble up to the waiter.
448 
449         :param instance: The instance for which an event is expected
450         :param event_names: A list of event names. Each element is a
451                             tuple of strings to indicate (name, tag),
452                             where name is required, but tag may be None.
453         :param deadline: Maximum number of seconds we should wait for all
454                          of the specified events to arrive.
455         :param error_callback: A function to be called if an event arrives
456 
457         """
458 
459         if error_callback is None:
460             error_callback = self._default_error_callback
461         events = {}
462         for event_name in event_names:
463             name, tag = event_name
464             event_name = objects.InstanceExternalEvent.make_key(name, tag)
465             try:
466                 events[event_name] = (
467                     self._compute.instance_events.prepare_for_instance_event(
468                         instance, name, tag))
469             except exception.NovaException:
470                 error_callback(event_name, instance)
471                 # NOTE(danms): Don't wait for any of the events. They
472                 # should all be canceled and fired immediately below,
473                 # but don't stick around if not.
474                 deadline = 0
475         yield
476         with eventlet.timeout.Timeout(deadline):
477             for event_name, event in events.items():
478                 actual_event = event.wait()
479                 if actual_event.status == 'completed':
480                     continue
481                 decision = error_callback(event_name, instance)
482                 if decision is False:
483                     break
484 
485     def update_compute_provider_status(self, context, rp_uuid, enabled):
486         """Used to add/remove the COMPUTE_STATUS_DISABLED trait on the provider
487 
488         :param context: nova auth RequestContext
489         :param rp_uuid: UUID of a compute node resource provider in Placement
490         :param enabled: True if the node is enabled in which case the trait
491             would be removed, False if the node is disabled in which case
492             the trait would be added.
493         :raises: ResourceProviderTraitRetrievalFailed
494         :raises: ResourceProviderUpdateConflict
495         :raises: ResourceProviderUpdateFailed
496         :raises: TraitRetrievalFailed
497         :raises: keystoneauth1.exceptions.ClientException
498         """
499         trait_name = os_traits.COMPUTE_STATUS_DISABLED
500         # Get the current traits (and generation) for the provider.
501         # TODO(mriedem): Leverage the ProviderTree cache in get_provider_traits
502         trait_info = self.reportclient.get_provider_traits(context, rp_uuid)
503         # If the host is enabled, remove the trait (if set), else add
504         # the trait if it doesn't already exist.
505         original_traits = trait_info.traits
506         new_traits = None
507         if enabled and trait_name in original_traits:
508             new_traits = original_traits - {trait_name}
509             LOG.debug('Removing trait %s from compute node resource '
510                       'provider %s in placement.', trait_name, rp_uuid)
511         elif not enabled and trait_name not in original_traits:
512             new_traits = original_traits | {trait_name}
513             LOG.debug('Adding trait %s to compute node resource '
514                       'provider %s in placement.', trait_name, rp_uuid)
515 
516         if new_traits is not None:
517             self.reportclient.set_traits_for_provider(
518                 context, rp_uuid, new_traits)
519 
520 
521 class ComputeManager(manager.Manager):
522     """Manages the running instances from creation to destruction."""
523 
524     target = messaging.Target(version='5.7')
525 
526     def __init__(self, compute_driver=None, *args, **kwargs):
527         """Load configuration options and connect to the hypervisor."""
528         # We want the ComputeManager, ResourceTracker and ComputeVirtAPI all
529         # using the same instance of SchedulerReportClient which has the
530         # ProviderTree cache for this compute service.
531         self.reportclient = report.SchedulerReportClient()
532         self.virtapi = ComputeVirtAPI(self)
533         self.network_api = network.API()
534         self.volume_api = cinder.API()
535         self.image_api = image.API()
536         self._last_bw_usage_poll = 0
537         self._bw_usage_supported = True
538         self.compute_api = compute.API()
539         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
540         self.compute_task_api = conductor.ComputeTaskAPI()
541         self.is_neutron_security_groups = (
542             openstack_driver.is_neutron_security_groups())
543         self.query_client = query.SchedulerQueryClient()
544         self.instance_events = InstanceEvents()
545         self._sync_power_pool = eventlet.GreenPool(
546             size=CONF.sync_power_state_pool_size)
547         self._syncs_in_progress = {}
548         self.send_instance_updates = (
549             CONF.filter_scheduler.track_instance_changes)
550         if CONF.max_concurrent_builds != 0:
551             self._build_semaphore = eventlet.semaphore.Semaphore(
552                 CONF.max_concurrent_builds)
553         else:
554             self._build_semaphore = compute_utils.UnlimitedSemaphore()
555         if CONF.max_concurrent_live_migrations > 0:
556             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
557                 max_workers=CONF.max_concurrent_live_migrations)
558         else:
559             # CONF.max_concurrent_live_migrations is 0 (unlimited)
560             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
561         # This is a dict, keyed by instance uuid, to a two-item tuple of
562         # migration object and Future for the queued live migration.
563         self._waiting_live_migrations = {}
564 
565         super(ComputeManager, self).__init__(service_name="compute",
566                                              *args, **kwargs)
567 
568         # NOTE(russellb) Load the driver last.  It may call back into the
569         # compute manager via the virtapi, so we want it to be fully
570         # initialized before that happens.
571         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
572         self.use_legacy_block_device_info = \
573                             self.driver.need_legacy_block_device_info
574         self.rt = resource_tracker.ResourceTracker(
575             self.host, self.driver, reportclient=self.reportclient)
576 
577     def reset(self):
578         LOG.info('Reloading compute RPC API')
579         compute_rpcapi.reset_globals()
580         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
581         self.reportclient.clear_provider_cache()
582 
583     def _update_resource_tracker(self, context, instance):
584         """Let the resource tracker know that an instance has changed state."""
585 
586         if instance.host == self.host:
587             self.rt.update_usage(context, instance, instance.node)
588 
589     def _instance_update(self, context, instance, **kwargs):
590         """Update an instance in the database using kwargs as value."""
591 
592         for k, v in kwargs.items():
593             setattr(instance, k, v)
594         instance.save()
595         self._update_resource_tracker(context, instance)
596 
597     def _nil_out_instance_obj_host_and_node(self, instance):
598         # NOTE(jwcroppe): We don't do instance.save() here for performance
599         # reasons; a call to this is expected to be immediately followed by
600         # another call that does instance.save(), thus avoiding two writes
601         # to the database layer.
602         instance.host = None
603         instance.node = None
604         # ResourceTracker._set_instance_host_and_node also sets launched_on
605         # to the same value as host and is really only ever used by legacy
606         # nova-network code, but we should also null it out to avoid confusion
607         # if there is an instance in the database with no host set but
608         # launched_on is set. Note that we do not care about using launched_on
609         # as some kind of debug helper if diagnosing a build failure, that is
610         # what instance action events are for.
611         instance.launched_on = None
612         # If the instance is not on a host, it's not in an aggregate and
613         # therefore is not in an availability zone.
614         instance.availability_zone = None
615 
616     def _set_instance_obj_error_state(self, context, instance,
617                                       clean_task_state=False):
618         try:
619             instance.vm_state = vm_states.ERROR
620             if clean_task_state:
621                 instance.task_state = None
622             instance.save()
623         except exception.InstanceNotFound:
624             LOG.debug('Instance has been destroyed from under us while '
625                       'trying to set it to ERROR', instance=instance)
626 
627     def _get_instances_on_driver(self, context, filters=None):
628         """Return a list of instance records for the instances found
629         on the hypervisor which satisfy the specified filters. If filters=None
630         return a list of instance records for all the instances found on the
631         hypervisor.
632         """
633         if not filters:
634             filters = {}
635         try:
636             driver_uuids = self.driver.list_instance_uuids()
637             if len(driver_uuids) == 0:
638                 # Short circuit, don't waste a DB call
639                 return objects.InstanceList()
640             filters['uuid'] = driver_uuids
641             local_instances = objects.InstanceList.get_by_filters(
642                 context, filters, use_slave=True)
643             return local_instances
644         except NotImplementedError:
645             pass
646 
647         # The driver doesn't support uuids listing, so we'll have
648         # to brute force.
649         driver_instances = self.driver.list_instances()
650         # NOTE(mjozefcz): In this case we need to apply host filter.
651         # Without this all instance data would be fetched from db.
652         filters['host'] = self.host
653         instances = objects.InstanceList.get_by_filters(context, filters,
654                                                         use_slave=True)
655         name_map = {instance.name: instance for instance in instances}
656         local_instances = []
657         for driver_instance in driver_instances:
658             instance = name_map.get(driver_instance)
659             if not instance:
660                 continue
661             local_instances.append(instance)
662         return local_instances
663 
664     def _destroy_evacuated_instances(self, context, node_cache):
665         """Destroys evacuated instances.
666 
667         While nova-compute was down, the instances running on it could be
668         evacuated to another host. This method looks for evacuation migration
669         records where this is the source host and which were either started
670         (accepted), in-progress (pre-migrating) or migrated (done). From those
671         migration records, local instances reported by the hypervisor are
672         compared to the instances for the migration records and those local
673         guests are destroyed, along with instance allocation records in
674         Placement for this node.
675         Then allocations are removed from Placement for every instance that is
676         evacuated from this host regardless if the instance is reported by the
677         hypervisor or not.
678 
679         :param context: The request context
680         :param node_cache: A dict of ComputeNode objects keyed by the UUID of
681             the compute node
682         :return: A dict keyed by instance uuid mapped to Migration objects
683             for instances that were migrated away from this host
684         """
685         filters = {
686             'source_compute': self.host,
687             # NOTE(mriedem): Migration records that have been accepted are
688             # included in case the source node comes back up while instances
689             # are being evacuated to another host. We don't want the same
690             # instance being reported from multiple hosts.
691             # NOTE(lyarwood): pre-migrating is also included here as the
692             # source compute can come back online shortly after the RT
693             # claims on the destination that in-turn moves the migration to
694             # pre-migrating. If the evacuate fails on the destination host,
695             # the user can rebuild the instance (in ERROR state) on the source
696             # host.
697             'status': ['accepted', 'pre-migrating', 'done'],
698             'migration_type': 'evacuation',
699         }
700         with utils.temporary_mutation(context, read_deleted='yes'):
701             evacuations = objects.MigrationList.get_by_filters(context,
702                                                                filters)
703         if not evacuations:
704             return {}
705         evacuations = {mig.instance_uuid: mig for mig in evacuations}
706 
707         # TODO(mriedem): We could optimize by pre-loading the joined fields
708         # we know we'll use, like info_cache and flavor.
709         local_instances = self._get_instances_on_driver(context)
710         evacuated_local_instances = {inst.uuid: inst
711                                      for inst in local_instances
712                                      if inst.uuid in evacuations}
713 
714         for instance in evacuated_local_instances.values():
715             LOG.info('Destroying instance as it has been evacuated from '
716                      'this host but still exists in the hypervisor',
717                      instance=instance)
718             try:
719                 network_info = self.network_api.get_instance_nw_info(
720                     context, instance)
721                 bdi = self._get_instance_block_device_info(context,
722                                                            instance)
723                 destroy_disks = not (self._is_instance_storage_shared(
724                     context, instance))
725             except exception.InstanceNotFound:
726                 network_info = network_model.NetworkInfo()
727                 bdi = {}
728                 LOG.info('Instance has been marked deleted already, '
729                          'removing it from the hypervisor.',
730                          instance=instance)
731                 # always destroy disks if the instance was deleted
732                 destroy_disks = True
733             self.driver.destroy(context, instance,
734                                 network_info,
735                                 bdi, destroy_disks)
736 
737         hostname_to_cn_uuid = {
738             cn.hypervisor_hostname: cn.uuid
739             for cn in node_cache.values()}
740 
741         for instance_uuid, migration in evacuations.items():
742             try:
743                 if instance_uuid in evacuated_local_instances:
744                     # Avoid the db call if we already have the instance loaded
745                     # above
746                     instance = evacuated_local_instances[instance_uuid]
747                 else:
748                     instance = objects.Instance.get_by_uuid(
749                         context, instance_uuid)
750             except exception.InstanceNotFound:
751                 # The instance already deleted so we expect that every
752                 # allocation of that instance has already been cleaned up
753                 continue
754 
755             LOG.info('Cleaning up allocations of the instance as it has been '
756                      'evacuated from this host',
757                      instance=instance)
758             if migration.source_node not in hostname_to_cn_uuid:
759                 LOG.error("Failed to clean allocation of evacuated "
760                           "instance as the source node %s is not found",
761                           migration.source_node, instance=instance)
762                 continue
763             cn_uuid = hostname_to_cn_uuid[migration.source_node]
764 
765             # If the instance was deleted in the interim, assume its
766             # allocations were properly cleaned up (either by its hosting
767             # compute service or the API).
768             if (not instance.deleted and
769                     not self.reportclient.
770                         remove_provider_tree_from_instance_allocation(
771                             context, instance.uuid, cn_uuid)):
772                 LOG.error("Failed to clean allocation of evacuated instance "
773                           "on the source node %s",
774                           cn_uuid, instance=instance)
775 
776             migration.status = 'completed'
777             migration.save()
778         return evacuations
779 
780     def _is_instance_storage_shared(self, context, instance, host=None):
781         shared_storage = True
782         data = None
783         try:
784             data = self.driver.check_instance_shared_storage_local(context,
785                                                        instance)
786             if data:
787                 shared_storage = (self.compute_rpcapi.
788                                   check_instance_shared_storage(context,
789                                   instance, data, host=host))
790         except NotImplementedError:
791             LOG.debug('Hypervisor driver does not support '
792                       'instance shared storage check, '
793                       'assuming it\'s not on shared storage',
794                       instance=instance)
795             shared_storage = False
796         except Exception:
797             LOG.exception('Failed to check if instance shared',
798                           instance=instance)
799         finally:
800             if data:
801                 self.driver.check_instance_shared_storage_cleanup(context,
802                                                                   data)
803         return shared_storage
804 
805     def _complete_partial_deletion(self, context, instance):
806         """Complete deletion for instances in DELETED status but not marked as
807         deleted in the DB
808         """
809         instance.destroy()
810         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
811                 context, instance.uuid)
812         self._complete_deletion(context,
813                                 instance)
814         self._notify_about_instance_usage(context, instance, "delete.end")
815         compute_utils.notify_about_instance_action(context, instance,
816                 self.host, action=fields.NotificationAction.DELETE,
817                 phase=fields.NotificationPhase.END, bdms=bdms)
818 
819     def _complete_deletion(self, context, instance):
820         self._update_resource_tracker(context, instance)
821 
822         self.reportclient.delete_allocation_for_instance(context,
823                                                          instance.uuid)
824 
825         self._clean_instance_console_tokens(context, instance)
826         self._delete_scheduler_instance_info(context, instance.uuid)
827 
828     def _validate_pinning_configuration(self, instances):
829         if not self.driver.capabilities.get('supports_pcpus', False):
830             return
831 
832         for instance in instances:
833             # ignore deleted instances
834             if instance.deleted:
835                 continue
836 
837             # if this is an unpinned instance and the host only has
838             # 'cpu_dedicated_set' configured, we need to tell the operator to
839             # correct their configuration
840             if not (instance.numa_topology and
841                         instance.numa_topology.cpu_pinning_requested):
842 
843                 # we don't need to check 'vcpu_pin_set' since it can't coexist
844                 # alongside 'cpu_dedicated_set'
845                 if (CONF.compute.cpu_dedicated_set and
846                         not CONF.compute.cpu_shared_set):
847                     msg = _("This host has unpinned instances but has no CPUs "
848                             "set aside for this purpose; configure '[compute] "
849                             "cpu_shared_set' instead of, or in addition to, "
850                             "'[compute] cpu_dedicated_set'")
851                     raise exception.InvalidConfiguration(msg)
852 
853                 continue
854 
855             # ditto for pinned instances if only 'cpu_shared_set' is configured
856             if (CONF.compute.cpu_shared_set and
857                     not CONF.compute.cpu_dedicated_set and
858                     not CONF.vcpu_pin_set):
859                 msg = _("This host has pinned instances but has no CPUs "
860                         "set aside for this purpose; configure '[compute] "
861                         "cpu_dedicated_set' instead of, or in addition to, "
862                         "'[compute] cpu_shared_set'")
863                 raise exception.InvalidConfiguration(msg)
864 
865             # also check to make sure the operator hasn't accidentally
866             # dropped some cores that instances are currently using
867             available_dedicated_cpus = (hardware.get_vcpu_pin_set() or
868                                         hardware.get_cpu_dedicated_set())
869             pinned_cpus = instance.numa_topology.cpu_pinning
870             if available_dedicated_cpus and (
871                     pinned_cpus - available_dedicated_cpus):
872                 # we can't raise an exception because of bug #1289064,
873                 # which meant we didn't recalculate CPU pinning information
874                 # when we live migrated a pinned instance
875                 LOG.warning(
876                     "Instance is pinned to host CPUs %(cpus)s "
877                     "but one or more of these CPUs are not included in "
878                     "either '[compute] cpu_dedicated_set' or "
879                     "'vcpu_pin_set'; you should update these "
880                     "configuration options to include the missing CPUs "
881                     "or rebuild or cold migrate this instance.",
882                     {'cpus': list(pinned_cpus)},
883                     instance=instance)
884 
885     def _init_instance(self, context, instance):
886         """Initialize this instance during service init."""
887 
888         # NOTE(danms): If the instance appears to not be owned by this
889         # host, it may have been evacuated away, but skipped by the
890         # evacuation cleanup code due to configuration. Thus, if that
891         # is a possibility, don't touch the instance in any way, but
892         # log the concern. This will help avoid potential issues on
893         # startup due to misconfiguration.
894         if instance.host != self.host:
895             LOG.warning('Instance %(uuid)s appears to not be owned '
896                         'by this host, but by %(host)s. Startup '
897                         'processing is being skipped.',
898                         {'uuid': instance.uuid,
899                          'host': instance.host})
900             return
901 
902         # Instances that are shut down, or in an error state can not be
903         # initialized and are not attempted to be recovered. The exception
904         # to this are instances that are in RESIZE_MIGRATING or DELETING,
905         # which are dealt with further down.
906         if (instance.vm_state == vm_states.SOFT_DELETED or
907             (instance.vm_state == vm_states.ERROR and
908             instance.task_state not in
909             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
910             LOG.debug("Instance is in %s state.",
911                       instance.vm_state, instance=instance)
912             return
913 
914         if instance.vm_state == vm_states.DELETED:
915             try:
916                 self._complete_partial_deletion(context, instance)
917             except Exception:
918                 # we don't want that an exception blocks the init_host
919                 LOG.exception('Failed to complete a deletion',
920                               instance=instance)
921             return
922 
923         if (instance.vm_state == vm_states.BUILDING or
924             instance.task_state in [task_states.SCHEDULING,
925                                     task_states.BLOCK_DEVICE_MAPPING,
926                                     task_states.NETWORKING,
927                                     task_states.SPAWNING]):
928             # NOTE(dave-mcnally) compute stopped before instance was fully
929             # spawned so set to ERROR state. This is safe to do as the state
930             # may be set by the api but the host is not so if we get here the
931             # instance has already been scheduled to this particular host.
932             LOG.debug("Instance failed to spawn correctly, "
933                       "setting to ERROR state", instance=instance)
934             self._set_instance_obj_error_state(
935                 context, instance, clean_task_state=True)
936             return
937 
938         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
939             instance.task_state in [task_states.REBUILDING,
940                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
941                                     task_states.REBUILD_SPAWNING]):
942             # NOTE(jichenjc) compute stopped before instance was fully
943             # spawned so set to ERROR state. This is consistent to BUILD
944             LOG.debug("Instance failed to rebuild correctly, "
945                       "setting to ERROR state", instance=instance)
946             self._set_instance_obj_error_state(
947                 context, instance, clean_task_state=True)
948             return
949 
950         if (instance.vm_state != vm_states.ERROR and
951             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
952                                     task_states.IMAGE_PENDING_UPLOAD,
953                                     task_states.IMAGE_UPLOADING,
954                                     task_states.IMAGE_SNAPSHOT]):
955             LOG.debug("Instance in transitional state %s at start-up "
956                       "clearing task state",
957                       instance.task_state, instance=instance)
958             try:
959                 self._post_interrupted_snapshot_cleanup(context, instance)
960             except Exception:
961                 # we don't want that an exception blocks the init_host
962                 LOG.exception('Failed to cleanup snapshot.', instance=instance)
963             instance.task_state = None
964             instance.save()
965 
966         if (instance.vm_state != vm_states.ERROR and
967             instance.task_state in [task_states.RESIZE_PREP]):
968             LOG.debug("Instance in transitional state %s at start-up "
969                       "clearing task state",
970                       instance['task_state'], instance=instance)
971             instance.task_state = None
972             instance.save()
973 
974         if instance.task_state == task_states.DELETING:
975             try:
976                 LOG.info('Service started deleting the instance during '
977                          'the previous run, but did not finish. Restarting'
978                          ' the deletion now.', instance=instance)
979                 instance.obj_load_attr('metadata')
980                 instance.obj_load_attr('system_metadata')
981                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
982                         context, instance.uuid)
983                 self._delete_instance(context, instance, bdms)
984             except Exception:
985                 # we don't want that an exception blocks the init_host
986                 LOG.exception('Failed to complete a deletion',
987                               instance=instance)
988                 self._set_instance_obj_error_state(context, instance)
989             return
990 
991         current_power_state = self._get_power_state(context, instance)
992         try_reboot, reboot_type = self._retry_reboot(context, instance,
993                                                      current_power_state)
994 
995         if try_reboot:
996             LOG.debug("Instance in transitional state (%(task_state)s) at "
997                       "start-up and power state is (%(power_state)s), "
998                       "triggering reboot",
999                       {'task_state': instance.task_state,
1000                        'power_state': current_power_state},
1001                       instance=instance)
1002 
1003             # NOTE(mikal): if the instance was doing a soft reboot that got as
1004             # far as shutting down the instance but not as far as starting it
1005             # again, then we've just become a hard reboot. That means the
1006             # task state for the instance needs to change so that we're in one
1007             # of the expected task states for a hard reboot.
1008             if (instance.task_state in task_states.soft_reboot_states and
1009                 reboot_type == 'HARD'):
1010                 instance.task_state = task_states.REBOOT_PENDING_HARD
1011                 instance.save()
1012 
1013             self.reboot_instance(context, instance, block_device_info=None,
1014                                  reboot_type=reboot_type)
1015             return
1016 
1017         elif (current_power_state == power_state.RUNNING and
1018               instance.task_state in [task_states.REBOOT_STARTED,
1019                                       task_states.REBOOT_STARTED_HARD,
1020                                       task_states.PAUSING,
1021                                       task_states.UNPAUSING]):
1022             LOG.warning("Instance in transitional state "
1023                         "(%(task_state)s) at start-up and power state "
1024                         "is (%(power_state)s), clearing task state",
1025                         {'task_state': instance.task_state,
1026                          'power_state': current_power_state},
1027                         instance=instance)
1028             instance.task_state = None
1029             instance.vm_state = vm_states.ACTIVE
1030             instance.save()
1031         elif (current_power_state == power_state.PAUSED and
1032               instance.task_state == task_states.UNPAUSING):
1033             LOG.warning("Instance in transitional state "
1034                         "(%(task_state)s) at start-up and power state "
1035                         "is (%(power_state)s), clearing task state "
1036                         "and unpausing the instance",
1037                         {'task_state': instance.task_state,
1038                          'power_state': current_power_state},
1039                         instance=instance)
1040             try:
1041                 self.unpause_instance(context, instance)
1042             except NotImplementedError:
1043                 # Some virt driver didn't support pause and unpause
1044                 pass
1045             except Exception:
1046                 LOG.exception('Failed to unpause instance', instance=instance)
1047             return
1048 
1049         if instance.task_state == task_states.POWERING_OFF:
1050             try:
1051                 LOG.debug("Instance in transitional state %s at start-up "
1052                           "retrying stop request",
1053                           instance.task_state, instance=instance)
1054                 self.stop_instance(context, instance, True)
1055             except Exception:
1056                 # we don't want that an exception blocks the init_host
1057                 LOG.exception('Failed to stop instance', instance=instance)
1058             return
1059 
1060         if instance.task_state == task_states.POWERING_ON:
1061             try:
1062                 LOG.debug("Instance in transitional state %s at start-up "
1063                           "retrying start request",
1064                           instance.task_state, instance=instance)
1065                 self.start_instance(context, instance)
1066             except Exception:
1067                 # we don't want that an exception blocks the init_host
1068                 LOG.exception('Failed to start instance', instance=instance)
1069             return
1070 
1071         net_info = instance.get_network_info()
1072         try:
1073             self.driver.plug_vifs(instance, net_info)
1074         except NotImplementedError as e:
1075             LOG.debug(e, instance=instance)
1076         except exception.VirtualInterfacePlugException:
1077             # NOTE(mriedem): If we get here, it could be because the vif_type
1078             # in the cache is "binding_failed" or "unbound".
1079             # The periodic task _heal_instance_info_cache checks for this
1080             # condition. It should fix this by binding the ports again when
1081             # it gets to this instance.
1082             LOG.exception('Virtual interface plugging failed for instance. '
1083                           'The port binding:host_id may need to be manually '
1084                           'updated.', instance=instance)
1085             self._set_instance_obj_error_state(context, instance)
1086             return
1087 
1088         if instance.task_state == task_states.RESIZE_MIGRATING:
1089             # We crashed during resize/migration, so roll back for safety
1090             try:
1091                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
1092                 # not in system_metadata we default to True for backwards
1093                 # compatibility
1094                 power_on = (instance.system_metadata.get('old_vm_state') !=
1095                             vm_states.STOPPED)
1096 
1097                 block_dev_info = self._get_instance_block_device_info(context,
1098                                                                       instance)
1099 
1100                 migration = objects.Migration.get_by_id_and_instance(
1101                     context, instance.migration_context.migration_id,
1102                     instance.uuid)
1103                 self.driver.finish_revert_migration(context, instance,
1104                     net_info, migration, block_dev_info, power_on)
1105 
1106             except Exception:
1107                 LOG.exception('Failed to revert crashed migration',
1108                               instance=instance)
1109             finally:
1110                 LOG.info('Instance found in migrating state during '
1111                          'startup. Resetting task_state',
1112                          instance=instance)
1113                 instance.task_state = None
1114                 instance.save()
1115         if instance.task_state == task_states.MIGRATING:
1116             # Live migration did not complete, but instance is on this
1117             # host, so reset the state.
1118             instance.task_state = None
1119             instance.save(expected_task_state=[task_states.MIGRATING])
1120 
1121         db_state = instance.power_state
1122         drv_state = self._get_power_state(context, instance)
1123         expect_running = (db_state == power_state.RUNNING and
1124                           drv_state != db_state)
1125 
1126         LOG.debug('Current state is %(drv_state)s, state in DB is '
1127                   '%(db_state)s.',
1128                   {'drv_state': drv_state, 'db_state': db_state},
1129                   instance=instance)
1130 
1131         if expect_running and CONF.resume_guests_state_on_host_boot:
1132             self._resume_guests_state(context, instance, net_info)
1133         elif drv_state == power_state.RUNNING:
1134             # VMwareAPI drivers will raise an exception
1135             try:
1136                 self.driver.ensure_filtering_rules_for_instance(
1137                                        instance, net_info)
1138             except NotImplementedError:
1139                 LOG.debug('Hypervisor driver does not support '
1140                           'firewall rules', instance=instance)
1141 
1142     def _resume_guests_state(self, context, instance, net_info):
1143         LOG.info('Rebooting instance after nova-compute restart.',
1144                  instance=instance)
1145         block_device_info = \
1146             self._get_instance_block_device_info(context, instance)
1147 
1148         try:
1149             self.driver.resume_state_on_host_boot(
1150                 context, instance, net_info, block_device_info)
1151         except NotImplementedError:
1152             LOG.warning('Hypervisor driver does not support '
1153                         'resume guests', instance=instance)
1154         except Exception:
1155             # NOTE(vish): The instance failed to resume, so we set the
1156             #             instance to error and attempt to continue.
1157             LOG.warning('Failed to resume instance',
1158                         instance=instance)
1159             self._set_instance_obj_error_state(context, instance)
1160 
1161     def _retry_reboot(self, context, instance, current_power_state):
1162         current_task_state = instance.task_state
1163         retry_reboot = False
1164         reboot_type = compute_utils.get_reboot_type(current_task_state,
1165                                                     current_power_state)
1166 
1167         pending_soft = (
1168             current_task_state == task_states.REBOOT_PENDING and
1169             instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1170         pending_hard = (
1171             current_task_state == task_states.REBOOT_PENDING_HARD and
1172             instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1173         started_not_running = (current_task_state in
1174                                [task_states.REBOOT_STARTED,
1175                                 task_states.REBOOT_STARTED_HARD] and
1176                                current_power_state != power_state.RUNNING)
1177 
1178         if pending_soft or pending_hard or started_not_running:
1179             retry_reboot = True
1180 
1181         return retry_reboot, reboot_type
1182 
1183     def handle_lifecycle_event(self, event):
1184         LOG.info("VM %(state)s (Lifecycle Event)",
1185                  {'state': event.get_name()},
1186                  instance_uuid=event.get_instance_uuid())
1187         context = nova.context.get_admin_context(read_deleted='yes')
1188         vm_power_state = None
1189         event_transition = event.get_transition()
1190         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1191             vm_power_state = power_state.SHUTDOWN
1192         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1193             vm_power_state = power_state.RUNNING
1194         elif event_transition in (
1195                 virtevent.EVENT_LIFECYCLE_PAUSED,
1196                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1197                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1198             vm_power_state = power_state.PAUSED
1199         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1200             vm_power_state = power_state.RUNNING
1201         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1202             vm_power_state = power_state.SUSPENDED
1203         else:
1204             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1205 
1206         migrate_finish_statuses = {
1207             # This happens on the source node and indicates live migration
1208             # entered post-copy mode.
1209             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1210             # Suspended for offline migration.
1211             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1212         }
1213 
1214         expected_attrs = []
1215         if event_transition in migrate_finish_statuses:
1216             # Join on info_cache since that's needed in migrate_instance_start.
1217             expected_attrs.append('info_cache')
1218         instance = objects.Instance.get_by_uuid(context,
1219                                                 event.get_instance_uuid(),
1220                                                 expected_attrs=expected_attrs)
1221 
1222         # Note(lpetrut): The event may be delayed, thus not reflecting
1223         # the current instance power state. In that case, ignore the event.
1224         current_power_state = self._get_power_state(context, instance)
1225         if current_power_state == vm_power_state:
1226             LOG.debug('Synchronizing instance power state after lifecycle '
1227                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1228                       'current task_state: %(task_state)s, current DB '
1229                       'power_state: %(db_power_state)s, VM power_state: '
1230                       '%(vm_power_state)s',
1231                       {'event': event.get_name(),
1232                        'vm_state': instance.vm_state,
1233                        'task_state': instance.task_state,
1234                        'db_power_state': instance.power_state,
1235                        'vm_power_state': vm_power_state},
1236                       instance_uuid=instance.uuid)
1237             self._sync_instance_power_state(context,
1238                                             instance,
1239                                             vm_power_state)
1240 
1241         # The following checks are for live migration. We want to activate
1242         # the port binding for the destination host before the live migration
1243         # is resumed on the destination host in order to reduce network
1244         # downtime. Otherwise the ports are bound to the destination host
1245         # in post_live_migration_at_destination.
1246         # TODO(danms): Explore options for using a different live migration
1247         # specific callback for this instead of piggy-backing on the
1248         # handle_lifecycle_event callback.
1249         if (instance.task_state == task_states.MIGRATING and
1250                 event_transition in migrate_finish_statuses):
1251             status = migrate_finish_statuses[event_transition]
1252             try:
1253                 migration = objects.Migration.get_by_instance_and_status(
1254                             context, instance.uuid, status)
1255                 LOG.debug('Binding ports to destination host: %s',
1256                           migration.dest_compute, instance=instance)
1257                 # For neutron, migrate_instance_start will activate the
1258                 # destination host port bindings, if there are any created by
1259                 # conductor before live migration started.
1260                 self.network_api.migrate_instance_start(
1261                     context, instance, migration)
1262             except exception.MigrationNotFoundByStatus:
1263                 LOG.warning("Unable to find migration record with status "
1264                             "'%s' for instance. Port binding will happen in "
1265                             "post live migration.", status, instance=instance)
1266 
1267     def handle_events(self, event):
1268         if isinstance(event, virtevent.LifecycleEvent):
1269             try:
1270                 self.handle_lifecycle_event(event)
1271             except exception.InstanceNotFound:
1272                 LOG.debug("Event %s arrived for non-existent instance. The "
1273                           "instance was probably deleted.", event)
1274         else:
1275             LOG.debug("Ignoring event %s", event)
1276 
1277     def init_virt_events(self):
1278         if CONF.workarounds.handle_virt_lifecycle_events:
1279             self.driver.register_event_listener(self.handle_events)
1280         else:
1281             # NOTE(mriedem): If the _sync_power_states periodic task is
1282             # disabled we should emit a warning in the logs.
1283             if CONF.sync_power_state_interval < 0:
1284                 LOG.warning('Instance lifecycle events from the compute '
1285                             'driver have been disabled. Note that lifecycle '
1286                             'changes to an instance outside of the compute '
1287                             'service will not be synchronized '
1288                             'automatically since the _sync_power_states '
1289                             'periodic task is also disabled.')
1290             else:
1291                 LOG.info('Instance lifecycle events from the compute '
1292                          'driver have been disabled. Note that lifecycle '
1293                          'changes to an instance outside of the compute '
1294                          'service will only be synchronized by the '
1295                          '_sync_power_states periodic task.')
1296 
1297     def _get_nodes(self, context):
1298         """Queried the ComputeNode objects from the DB that are reported by the
1299         hypervisor.
1300 
1301         :param context: the request context
1302         :return: a dict of ComputeNode objects keyed by the UUID of the given
1303             node.
1304         """
1305         nodes_by_uuid = {}
1306         try:
1307             node_names = self.driver.get_available_nodes()
1308         except exception.VirtDriverNotReady:
1309             LOG.warning(
1310                 "Virt driver is not ready. If this is the first time this "
1311                 "service is starting on this host, then you can ignore this "
1312                 "warning.")
1313             return {}
1314 
1315         for node_name in node_names:
1316             try:
1317                 node = objects.ComputeNode.get_by_host_and_nodename(
1318                     context, self.host, node_name)
1319                 nodes_by_uuid[node.uuid] = node
1320             except exception.ComputeHostNotFound:
1321                 LOG.warning(
1322                     "Compute node %s not found in the database. If this is "
1323                     "the first time this service is starting on this host, "
1324                     "then you can ignore this warning.", node_name)
1325         return nodes_by_uuid
1326 
1327     def init_host(self):
1328         """Initialization for a standalone compute service."""
1329 
1330         if CONF.pci.passthrough_whitelist:
1331             # Simply loading the PCI passthrough whitelist will do a bunch of
1332             # validation that would otherwise wait until the PciDevTracker is
1333             # constructed when updating available resources for the compute
1334             # node(s) in the resource tracker, effectively killing that task.
1335             # So load up the whitelist when starting the compute service to
1336             # flush any invalid configuration early so we can kill the service
1337             # if the configuration is wrong.
1338             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1339 
1340         nova.conf.neutron.register_dynamic_opts(CONF)
1341 
1342         # Override the number of concurrent disk operations allowed if the
1343         # user has specified a limit.
1344         if CONF.compute.max_concurrent_disk_ops != 0:
1345             compute_utils.disk_ops_semaphore = \
1346                 eventlet.semaphore.BoundedSemaphore(
1347                     CONF.compute.max_concurrent_disk_ops)
1348 
1349         self.driver.init_host(host=self.host)
1350         context = nova.context.get_admin_context()
1351         instances = objects.InstanceList.get_by_host(
1352             context, self.host,
1353             expected_attrs=['info_cache', 'metadata', 'numa_topology'])
1354 
1355         if CONF.defer_iptables_apply:
1356             self.driver.filter_defer_apply_on()
1357 
1358         self.init_virt_events()
1359 
1360         self._validate_pinning_configuration(instances)
1361 
1362         # NOTE(gibi): At this point the compute_nodes of the resource tracker
1363         # has not been populated yet so we cannot rely on the resource tracker
1364         # here.
1365         # NOTE(gibi): If ironic and vcenter virt driver slow start time
1366         # becomes problematic here then we should consider adding a config
1367         # option or a driver flag to tell us if we should thread
1368         # _destroy_evacuated_instances and
1369         # _error_out_instances_whose_build_was_interrupted out in the
1370         # background on startup
1371         nodes_by_uuid = self._get_nodes(context)
1372 
1373         try:
1374             # checking that instance was not already evacuated to other host
1375             evacuated_instances = self._destroy_evacuated_instances(
1376                 context, nodes_by_uuid)
1377 
1378             # Initialise instances on the host that are not evacuating
1379             for instance in instances:
1380                 if instance.uuid not in evacuated_instances:
1381                     self._init_instance(context, instance)
1382 
1383             # NOTE(gibi): collect all the instance uuids that is in some way
1384             # was already handled above. Either by init_instance or by
1385             # _destroy_evacuated_instances. This way we can limit the scope of
1386             # the _error_out_instances_whose_build_was_interrupted call to look
1387             # only for instances that have allocations on this node and not
1388             # handled by the above calls.
1389             already_handled = {instance.uuid for instance in instances}.union(
1390                 evacuated_instances)
1391             self._error_out_instances_whose_build_was_interrupted(
1392                 context, already_handled, nodes_by_uuid.keys())
1393 
1394         finally:
1395             if CONF.defer_iptables_apply:
1396                 self.driver.filter_defer_apply_off()
1397             if instances:
1398                 # We only send the instance info to the scheduler on startup
1399                 # if there is anything to send, otherwise this host might
1400                 # not be mapped yet in a cell and the scheduler may have
1401                 # issues dealing with the information. Later changes to
1402                 # instances on this host will update the scheduler, or the
1403                 # _sync_scheduler_instance_info periodic task will.
1404                 self._update_scheduler_instance_info(context, instances)
1405 
1406     def _error_out_instances_whose_build_was_interrupted(
1407             self, context, already_handled_instances, node_uuids):
1408         """If there are instances in BUILDING state that are not
1409         assigned to this host but have allocations in placement towards
1410         this compute that means the nova-compute service was
1411         restarted while those instances waited for the resource claim
1412         to finish and the _set_instance_host_and_node() to update the
1413         instance.host field. We need to push them to ERROR state here to
1414         prevent keeping them in BUILDING state forever.
1415 
1416         :param context: The request context
1417         :param already_handled_instances: The set of instance UUIDs that the
1418             host initialization process already handled in some way.
1419         :param node_uuids: The list of compute node uuids handled by this
1420             service
1421         """
1422 
1423         # Strategy:
1424         # 1) Get the allocations from placement for our compute node(s)
1425         # 2) Remove the already handled instances from the consumer list;
1426         #    they are either already initialized or need to be skipped.
1427         # 3) Check which remaining consumer is an instance in BUILDING state
1428         #    and push it to ERROR state.
1429 
1430         LOG.info(
1431             "Looking for unclaimed instances stuck in BUILDING status for "
1432             "nodes managed by this host")
1433         for cn_uuid in node_uuids:
1434             try:
1435                 f = self.reportclient.get_allocations_for_resource_provider
1436                 allocations = f(context, cn_uuid).allocations
1437             except (exception.ResourceProviderAllocationRetrievalFailed,
1438                     keystone_exception.ClientException) as e:
1439                 LOG.error(
1440                     "Could not retrieve compute node resource provider %s and "
1441                     "therefore unable to error out any instances stuck in "
1442                     "BUILDING state. Error: %s", cn_uuid, six.text_type(e))
1443                 continue
1444 
1445             not_handled_consumers = (set(allocations) -
1446                                      already_handled_instances)
1447 
1448             if not not_handled_consumers:
1449                 continue
1450 
1451             filters = {
1452                 'vm_state': vm_states.BUILDING,
1453                 'uuid': not_handled_consumers
1454             }
1455 
1456             instances = objects.InstanceList.get_by_filters(
1457                 context, filters, expected_attrs=[])
1458 
1459             for instance in instances:
1460                 LOG.debug(
1461                     "Instance spawn was interrupted before instance_claim, "
1462                     "setting instance to ERROR state", instance=instance)
1463                 self._set_instance_obj_error_state(
1464                     context, instance, clean_task_state=True)
1465 
1466     def cleanup_host(self):
1467         self.driver.register_event_listener(None)
1468         self.instance_events.cancel_all_events()
1469         self.driver.cleanup_host(host=self.host)
1470         self._cleanup_live_migrations_in_pool()
1471 
1472     def _cleanup_live_migrations_in_pool(self):
1473         # Shutdown the pool so we don't get new requests.
1474         self._live_migration_executor.shutdown(wait=False)
1475         # For any queued migrations, cancel the migration and update
1476         # its status.
1477         for migration, future in self._waiting_live_migrations.values():
1478             # If we got here before the Future was submitted then we need
1479             # to move on since there isn't anything we can do.
1480             if future is None:
1481                 continue
1482             if future.cancel():
1483                 self._set_migration_status(migration, 'cancelled')
1484                 LOG.info('Successfully cancelled queued live migration.',
1485                          instance_uuid=migration.instance_uuid)
1486             else:
1487                 LOG.warning('Unable to cancel live migration.',
1488                             instance_uuid=migration.instance_uuid)
1489         self._waiting_live_migrations.clear()
1490 
1491     def pre_start_hook(self):
1492         """After the service is initialized, but before we fully bring
1493         the service up by listening on RPC queues, make sure to update
1494         our available resources (and indirectly our available nodes).
1495         """
1496         self.update_available_resource(nova.context.get_admin_context(),
1497                                        startup=True)
1498 
1499     def _get_power_state(self, context, instance):
1500         """Retrieve the power state for the given instance."""
1501         LOG.debug('Checking state', instance=instance)
1502         try:
1503             return self.driver.get_info(instance, use_cache=False).state
1504         except exception.InstanceNotFound:
1505             return power_state.NOSTATE
1506 
1507     def get_console_topic(self, context):
1508         """Retrieves the console host for a project on this host.
1509 
1510         Currently this is just set in the flags for each compute host.
1511 
1512         """
1513         # TODO(mdragon): perhaps make this variable by console_type?
1514         return '%s.%s' % (console_rpcapi.RPC_TOPIC, CONF.console_host)
1515 
1516     @wrap_exception()
1517     def get_console_pool_info(self, context, console_type):
1518         return self.driver.get_console_pool_info(console_type)
1519 
1520     @wrap_exception()
1521     def refresh_instance_security_rules(self, context, instance):
1522         """Tell the virtualization driver to refresh security rules for
1523         an instance.
1524 
1525         Passes straight through to the virtualization driver.
1526 
1527         Synchronize the call because we may still be in the middle of
1528         creating the instance.
1529         """
1530         @utils.synchronized(instance.uuid)
1531         def _sync_refresh():
1532             try:
1533                 return self.driver.refresh_instance_security_rules(instance)
1534             except NotImplementedError:
1535                 LOG.debug('Hypervisor driver does not support '
1536                           'security groups.', instance=instance)
1537 
1538         return _sync_refresh()
1539 
1540     def _await_block_device_map_created(self, context, vol_id):
1541         # TODO(yamahata): creating volume simultaneously
1542         #                 reduces creation time?
1543         # TODO(yamahata): eliminate dumb polling
1544         start = time.time()
1545         retries = CONF.block_device_allocate_retries
1546         # (1) if the configured value is 0, one attempt should be made
1547         # (2) if the configured value is > 0, then the total number attempts
1548         #      is (retries + 1)
1549         attempts = 1
1550         if retries >= 1:
1551             attempts = retries + 1
1552         for attempt in range(1, attempts + 1):
1553             volume = self.volume_api.get(context, vol_id)
1554             volume_status = volume['status']
1555             if volume_status not in ['creating', 'downloading']:
1556                 if volume_status == 'available':
1557                     return attempt
1558                 LOG.warning("Volume id: %(vol_id)s finished being "
1559                             "created but its status is %(vol_status)s.",
1560                             {'vol_id': vol_id,
1561                              'vol_status': volume_status})
1562                 break
1563             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1564         raise exception.VolumeNotCreated(volume_id=vol_id,
1565                                          seconds=int(time.time() - start),
1566                                          attempts=attempt,
1567                                          volume_status=volume_status)
1568 
1569     def _decode_files(self, injected_files):
1570         """Base64 decode the list of files to inject."""
1571         if not injected_files:
1572             return []
1573 
1574         def _decode(f):
1575             path, contents = f
1576             # Py3 raises binascii.Error instead of TypeError as in Py27
1577             try:
1578                 decoded = base64.b64decode(contents)
1579                 return path, decoded
1580             except (TypeError, binascii.Error):
1581                 raise exception.Base64Exception(path=path)
1582 
1583         return [_decode(f) for f in injected_files]
1584 
1585     def _validate_instance_group_policy(self, context, instance,
1586                                         scheduler_hints):
1587         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1588         # However, there is a race condition with the enforcement of
1589         # the policy.  Since more than one instance may be scheduled at the
1590         # same time, it's possible that more than one instance with an
1591         # anti-affinity policy may end up here.  It's also possible that
1592         # multiple instances with an affinity policy could end up on different
1593         # hosts.  This is a validation step to make sure that starting the
1594         # instance here doesn't violate the policy.
1595         group_hint = scheduler_hints.get('group')
1596         if not group_hint:
1597             return
1598 
1599         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1600         # to check the type on the value and pull the single entry out. The
1601         # API request schema validates that the 'group' hint is a single value.
1602         if isinstance(group_hint, list):
1603             group_hint = group_hint[0]
1604 
1605         @utils.synchronized(group_hint)
1606         def _do_validation(context, instance, group_hint):
1607             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1608             if group.policy and 'anti-affinity' == group.policy:
1609                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1610                     context, self.host)
1611                 ins_on_host = set(instances_uuids)
1612                 members = set(group.members)
1613                 # Determine the set of instance group members on this host
1614                 # which are not the instance in question. This is used to
1615                 # determine how many other members from the same anti-affinity
1616                 # group can be on this host.
1617                 members_on_host = ins_on_host & members - set([instance.uuid])
1618                 rules = group.rules
1619                 if rules and 'max_server_per_host' in rules:
1620                     max_server = rules['max_server_per_host']
1621                 else:
1622                     max_server = 1
1623                 if len(members_on_host) >= max_server:
1624                     msg = _("Anti-affinity instance group policy "
1625                             "was violated.")
1626                     raise exception.RescheduledException(
1627                             instance_uuid=instance.uuid,
1628                             reason=msg)
1629             elif group.policy and 'affinity' == group.policy:
1630                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1631                 if group_hosts and self.host not in group_hosts:
1632                     msg = _("Affinity instance group policy was violated.")
1633                     raise exception.RescheduledException(
1634                             instance_uuid=instance.uuid,
1635                             reason=msg)
1636 
1637         if not CONF.workarounds.disable_group_policy_check_upcall:
1638             _do_validation(context, instance, group_hint)
1639 
1640     def _log_original_error(self, exc_info, instance_uuid):
1641         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1642                   exc_info=exc_info)
1643 
1644     @periodic_task.periodic_task
1645     def _check_instance_build_time(self, context):
1646         """Ensure that instances are not stuck in build."""
1647         timeout = CONF.instance_build_timeout
1648         if timeout == 0:
1649             return
1650 
1651         filters = {'vm_state': vm_states.BUILDING,
1652                    'host': self.host}
1653 
1654         building_insts = objects.InstanceList.get_by_filters(context,
1655                            filters, expected_attrs=[], use_slave=True)
1656 
1657         for instance in building_insts:
1658             if timeutils.is_older_than(instance.created_at, timeout):
1659                 self._set_instance_obj_error_state(context, instance)
1660                 LOG.warning("Instance build timed out. Set to error "
1661                             "state.", instance=instance)
1662 
1663     def _check_instance_exists(self, context, instance):
1664         """Ensure an instance with the same name is not already present."""
1665         if self.driver.instance_exists(instance):
1666             raise exception.InstanceExists(name=instance.name)
1667 
1668     def _allocate_network_async(self, context, instance, requested_networks,
1669                                 security_groups, is_vpn,
1670                                 resource_provider_mapping):
1671         """Method used to allocate networks in the background.
1672 
1673         Broken out for testing.
1674         """
1675         # First check to see if we're specifically not supposed to allocate
1676         # networks because if so, we can exit early.
1677         if requested_networks and requested_networks.no_allocate:
1678             LOG.debug("Not allocating networking since 'none' was specified.",
1679                       instance=instance)
1680             return network_model.NetworkInfo([])
1681 
1682         LOG.debug("Allocating IP information in the background.",
1683                   instance=instance)
1684         retries = CONF.network_allocate_retries
1685         attempts = retries + 1
1686         retry_time = 1
1687         bind_host_id = self.driver.network_binding_host_id(context, instance)
1688         for attempt in range(1, attempts + 1):
1689             try:
1690                 nwinfo = self.network_api.allocate_for_instance(
1691                         context, instance, vpn=is_vpn,
1692                         requested_networks=requested_networks,
1693                         security_groups=security_groups,
1694                         bind_host_id=bind_host_id,
1695                         resource_provider_mapping=resource_provider_mapping)
1696                 LOG.debug('Instance network_info: |%s|', nwinfo,
1697                           instance=instance)
1698                 instance.system_metadata['network_allocated'] = 'True'
1699                 # NOTE(JoshNang) do not save the instance here, as it can cause
1700                 # races. The caller shares a reference to instance and waits
1701                 # for this async greenthread to finish before calling
1702                 # instance.save().
1703                 return nwinfo
1704             except Exception:
1705                 exc_info = sys.exc_info()
1706                 log_info = {'attempt': attempt,
1707                             'attempts': attempts}
1708                 if attempt == attempts:
1709                     LOG.exception('Instance failed network setup '
1710                                   'after %(attempts)d attempt(s)',
1711                                   log_info)
1712                     six.reraise(*exc_info)
1713                 LOG.warning('Instance failed network setup '
1714                             '(attempt %(attempt)d of %(attempts)d)',
1715                             log_info, instance=instance)
1716                 time.sleep(retry_time)
1717                 retry_time *= 2
1718                 if retry_time > 30:
1719                     retry_time = 30
1720         # Not reached.
1721 
1722     def _build_networks_for_instance(self, context, instance,
1723             requested_networks, security_groups, resource_provider_mapping):
1724 
1725         # If we're here from a reschedule the network may already be allocated.
1726         if strutils.bool_from_string(
1727                 instance.system_metadata.get('network_allocated', 'False')):
1728             # NOTE(alex_xu): The network_allocated is True means the network
1729             # resource already allocated at previous scheduling, and the
1730             # network setup is cleanup at previous. After rescheduling, the
1731             # network resource need setup on the new host.
1732             self.network_api.setup_instance_network_on_host(
1733                 context, instance, instance.host)
1734             return self.network_api.get_instance_nw_info(context, instance)
1735 
1736         if not self.is_neutron_security_groups:
1737             security_groups = []
1738 
1739         network_info = self._allocate_network(context, instance,
1740                 requested_networks, security_groups,
1741                 resource_provider_mapping)
1742 
1743         return network_info
1744 
1745     def _allocate_network(self, context, instance, requested_networks,
1746                           security_groups, resource_provider_mapping):
1747         """Start network allocation asynchronously.  Return an instance
1748         of NetworkInfoAsyncWrapper that can be used to retrieve the
1749         allocated networks when the operation has finished.
1750         """
1751         # NOTE(comstud): Since we're allocating networks asynchronously,
1752         # this task state has little meaning, as we won't be in this
1753         # state for very long.
1754         instance.vm_state = vm_states.BUILDING
1755         instance.task_state = task_states.NETWORKING
1756         instance.save(expected_task_state=[None])
1757 
1758         is_vpn = False
1759         return network_model.NetworkInfoAsyncWrapper(
1760                 self._allocate_network_async, context, instance,
1761                 requested_networks, security_groups, is_vpn,
1762                 resource_provider_mapping)
1763 
1764     def _default_root_device_name(self, instance, image_meta, root_bdm):
1765         """Gets a default root device name from the driver.
1766 
1767         :param nova.objects.Instance instance:
1768             The instance for which to get the root device name.
1769         :param nova.objects.ImageMeta image_meta:
1770             The metadata of the image of the instance.
1771         :param nova.objects.BlockDeviceMapping root_bdm:
1772             The description of the root device.
1773         :returns: str -- The default root device name.
1774         :raises: InternalError, TooManyDiskDevices
1775         """
1776         try:
1777             return self.driver.default_root_device_name(instance,
1778                                                         image_meta,
1779                                                         root_bdm)
1780         except NotImplementedError:
1781             return compute_utils.get_next_device_name(instance, [])
1782 
1783     def _default_device_names_for_instance(self, instance,
1784                                            root_device_name,
1785                                            *block_device_lists):
1786         """Default the missing device names in the BDM from the driver.
1787 
1788         :param nova.objects.Instance instance:
1789             The instance for which to get default device names.
1790         :param str root_device_name: The root device name.
1791         :param list block_device_lists: List of block device mappings.
1792         :returns: None
1793         :raises: InternalError, TooManyDiskDevices
1794         """
1795         try:
1796             self.driver.default_device_names_for_instance(instance,
1797                                                           root_device_name,
1798                                                           *block_device_lists)
1799         except NotImplementedError:
1800             compute_utils.default_device_names_for_instance(
1801                 instance, root_device_name, *block_device_lists)
1802 
1803     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1804         """Get the next device name from the driver, based on the BDM.
1805 
1806         :param nova.objects.Instance instance:
1807             The instance whose volume is requesting a device name.
1808         :param nova.objects.BlockDeviceMappingList bdms:
1809             The block device mappings for the instance.
1810         :param nova.objects.BlockDeviceMapping block_device_obj:
1811             A block device mapping containing info about the requested block
1812             device.
1813         :returns: The next device name.
1814         :raises: InternalError, TooManyDiskDevices
1815         """
1816         # NOTE(ndipanov): Copy obj to avoid changing the original
1817         block_device_obj = block_device_obj.obj_clone()
1818         try:
1819             return self.driver.get_device_name_for_instance(
1820                 instance, bdms, block_device_obj)
1821         except NotImplementedError:
1822             return compute_utils.get_device_name_for_instance(
1823                 instance, bdms, block_device_obj.get("device_name"))
1824 
1825     def _default_block_device_names(self, instance, image_meta, block_devices):
1826         """Verify that all the devices have the device_name set. If not,
1827         provide a default name.
1828 
1829         It also ensures that there is a root_device_name and is set to the
1830         first block device in the boot sequence (boot_index=0).
1831         """
1832         root_bdm = block_device.get_root_bdm(block_devices)
1833         if not root_bdm:
1834             return
1835 
1836         # Get the root_device_name from the root BDM or the instance
1837         root_device_name = None
1838         update_root_bdm = False
1839 
1840         if root_bdm.device_name:
1841             root_device_name = root_bdm.device_name
1842             instance.root_device_name = root_device_name
1843         elif instance.root_device_name:
1844             root_device_name = instance.root_device_name
1845             root_bdm.device_name = root_device_name
1846             update_root_bdm = True
1847         else:
1848             root_device_name = self._default_root_device_name(instance,
1849                                                               image_meta,
1850                                                               root_bdm)
1851 
1852             instance.root_device_name = root_device_name
1853             root_bdm.device_name = root_device_name
1854             update_root_bdm = True
1855 
1856         if update_root_bdm:
1857             root_bdm.save()
1858 
1859         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1860                             block_devices))
1861         swap = list(filter(block_device.new_format_is_swap,
1862                       block_devices))
1863         block_device_mapping = list(filter(
1864               driver_block_device.is_block_device_mapping, block_devices))
1865 
1866         self._default_device_names_for_instance(instance,
1867                                                 root_device_name,
1868                                                 ephemerals,
1869                                                 swap,
1870                                                 block_device_mapping)
1871 
1872     def _block_device_info_to_legacy(self, block_device_info):
1873         """Convert BDI to the old format for drivers that need it."""
1874 
1875         if self.use_legacy_block_device_info:
1876             ephemerals = driver_block_device.legacy_block_devices(
1877                 driver.block_device_info_get_ephemerals(block_device_info))
1878             mapping = driver_block_device.legacy_block_devices(
1879                 driver.block_device_info_get_mapping(block_device_info))
1880             swap = block_device_info['swap']
1881             if swap:
1882                 swap = swap.legacy()
1883 
1884             block_device_info.update({
1885                 'ephemerals': ephemerals,
1886                 'swap': swap,
1887                 'block_device_mapping': mapping})
1888 
1889     def _add_missing_dev_names(self, bdms, instance):
1890         for bdm in bdms:
1891             if bdm.device_name is not None:
1892                 continue
1893 
1894             device_name = self._get_device_name_for_instance(instance,
1895                                                              bdms, bdm)
1896             values = {'device_name': device_name}
1897             bdm.update(values)
1898             bdm.save()
1899 
1900     def _prep_block_device(self, context, instance, bdms):
1901         """Set up the block device for an instance with error logging."""
1902         try:
1903             self._add_missing_dev_names(bdms, instance)
1904             block_device_info = driver.get_block_device_info(instance, bdms)
1905             mapping = driver.block_device_info_get_mapping(block_device_info)
1906             driver_block_device.attach_block_devices(
1907                 mapping, context, instance, self.volume_api, self.driver,
1908                 wait_func=self._await_block_device_map_created)
1909 
1910             self._block_device_info_to_legacy(block_device_info)
1911             return block_device_info
1912 
1913         except exception.OverQuota as e:
1914             LOG.warning('Failed to create block device for instance due'
1915                         ' to exceeding volume related resource quota.'
1916                         ' Error: %s', e.message, instance=instance)
1917             raise
1918 
1919         except Exception as ex:
1920             LOG.exception('Instance failed block device setup',
1921                           instance=instance)
1922             # InvalidBDM will eventually result in a BuildAbortException when
1923             # booting from volume, and will be recorded as an instance fault.
1924             # Maintain the original exception message which most likely has
1925             # useful details which the standard InvalidBDM error message lacks.
1926             raise exception.InvalidBDM(six.text_type(ex))
1927 
1928     def _update_instance_after_spawn(self, context, instance,
1929                                      vm_state=vm_states.ACTIVE):
1930         instance.power_state = self._get_power_state(context, instance)
1931         instance.vm_state = vm_state
1932         instance.task_state = None
1933         # NOTE(sean-k-mooney): configdrive.update_instance checks
1934         # instance.launched_at to determine if it is the first or
1935         # subsequent spawn of an instance. We need to call update_instance
1936         # first before setting instance.launched_at or instance.config_drive
1937         # will never be set to true based on the value of force_config_drive.
1938         # As a result the config drive will be lost on a hard reboot of the
1939         # instance even when force_config_drive=true. see bug #1835822.
1940         configdrive.update_instance(instance)
1941         instance.launched_at = timeutils.utcnow()
1942 
1943     def _update_scheduler_instance_info(self, context, instance):
1944         """Sends an InstanceList with created or updated Instance objects to
1945         the Scheduler client.
1946 
1947         In the case of init_host, the value passed will already be an
1948         InstanceList. Other calls will send individual Instance objects that
1949         have been created or resized. In this case, we create an InstanceList
1950         object containing that Instance.
1951         """
1952         if not self.send_instance_updates:
1953             return
1954         if isinstance(instance, obj_instance.Instance):
1955             instance = objects.InstanceList(objects=[instance])
1956         context = context.elevated()
1957         self.query_client.update_instance_info(context, self.host,
1958                                                instance)
1959 
1960     def _delete_scheduler_instance_info(self, context, instance_uuid):
1961         """Sends the uuid of the deleted Instance to the Scheduler client."""
1962         if not self.send_instance_updates:
1963             return
1964         context = context.elevated()
1965         self.query_client.delete_instance_info(context, self.host,
1966                                                instance_uuid)
1967 
1968     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
1969     def _sync_scheduler_instance_info(self, context):
1970         if not self.send_instance_updates:
1971             return
1972         context = context.elevated()
1973         instances = objects.InstanceList.get_by_host(context, self.host,
1974                                                      expected_attrs=[],
1975                                                      use_slave=True)
1976         uuids = [instance.uuid for instance in instances]
1977         self.query_client.sync_instance_info(context, self.host, uuids)
1978 
1979     def _notify_about_instance_usage(self, context, instance, event_suffix,
1980                                      network_info=None, extra_usage_info=None,
1981                                      fault=None):
1982         compute_utils.notify_about_instance_usage(
1983             self.notifier, context, instance, event_suffix,
1984             network_info=network_info,
1985             extra_usage_info=extra_usage_info, fault=fault)
1986 
1987     def _deallocate_network(self, context, instance,
1988                             requested_networks=None):
1989         # If we were told not to allocate networks let's save ourselves
1990         # the trouble of calling the network API.
1991         if requested_networks and requested_networks.no_allocate:
1992             LOG.debug("Skipping network deallocation for instance since "
1993                       "networking was not requested.", instance=instance)
1994             return
1995 
1996         LOG.debug('Deallocating network for instance', instance=instance)
1997         with timeutils.StopWatch() as timer:
1998             self.network_api.deallocate_for_instance(
1999                 context, instance, requested_networks=requested_networks)
2000         # nova-network does an rpc call so we're OK tracking time spent here
2001         LOG.info('Took %0.2f seconds to deallocate network for instance.',
2002                  timer.elapsed(), instance=instance)
2003 
2004     def _get_instance_block_device_info(self, context, instance,
2005                                         refresh_conn_info=False,
2006                                         bdms=None):
2007         """Transform block devices to the driver block_device format."""
2008 
2009         if bdms is None:
2010             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2011                     context, instance.uuid)
2012         block_device_info = driver.get_block_device_info(instance, bdms)
2013 
2014         if not refresh_conn_info:
2015             # if the block_device_mapping has no value in connection_info
2016             # (returned as None), don't include in the mapping
2017             block_device_info['block_device_mapping'] = [
2018                 bdm for bdm in driver.block_device_info_get_mapping(
2019                                     block_device_info)
2020                 if bdm.get('connection_info')]
2021         else:
2022             driver_block_device.refresh_conn_infos(
2023                 driver.block_device_info_get_mapping(block_device_info),
2024                 context, instance, self.volume_api, self.driver)
2025 
2026         self._block_device_info_to_legacy(block_device_info)
2027 
2028         return block_device_info
2029 
2030     def _build_failed(self, node):
2031         if CONF.compute.consecutive_build_service_disable_threshold:
2032             # NOTE(danms): Update our counter, but wait for the next
2033             # update_available_resource() periodic to flush it to the DB
2034             self.rt.build_failed(node)
2035 
2036     def _build_succeeded(self, node):
2037         self.rt.build_succeeded(node)
2038 
2039     @wrap_exception()
2040     @reverts_task_state
2041     @wrap_instance_fault
2042     def build_and_run_instance(self, context, instance, image, request_spec,
2043                      filter_properties, admin_password=None,
2044                      injected_files=None, requested_networks=None,
2045                      security_groups=None, block_device_mapping=None,
2046                      node=None, limits=None, host_list=None):
2047 
2048         @utils.synchronized(instance.uuid)
2049         def _locked_do_build_and_run_instance(*args, **kwargs):
2050             # NOTE(danms): We grab the semaphore with the instance uuid
2051             # locked because we could wait in line to build this instance
2052             # for a while and we want to make sure that nothing else tries
2053             # to do anything with this instance while we wait.
2054             with self._build_semaphore:
2055                 try:
2056                     result = self._do_build_and_run_instance(*args, **kwargs)
2057                 except Exception:
2058                     # NOTE(mriedem): This should really only happen if
2059                     # _decode_files in _do_build_and_run_instance fails, and
2060                     # that's before a guest is spawned so it's OK to remove
2061                     # allocations for the instance for this node from Placement
2062                     # below as there is no guest consuming resources anyway.
2063                     # The _decode_files case could be handled more specifically
2064                     # but that's left for another day.
2065                     result = build_results.FAILED
2066                     raise
2067                 finally:
2068                     if result == build_results.FAILED:
2069                         # Remove the allocation records from Placement for the
2070                         # instance if the build failed. The instance.host is
2071                         # likely set to None in _do_build_and_run_instance
2072                         # which means if the user deletes the instance, it
2073                         # will be deleted in the API, not the compute service.
2074                         # Setting the instance.host to None in
2075                         # _do_build_and_run_instance means that the
2076                         # ResourceTracker will no longer consider this instance
2077                         # to be claiming resources against it, so we want to
2078                         # reflect that same thing in Placement.  No need to
2079                         # call this for a reschedule, as the allocations will
2080                         # have already been removed in
2081                         # self._do_build_and_run_instance().
2082                         self.reportclient.delete_allocation_for_instance(
2083                             context, instance.uuid)
2084 
2085                     if result in (build_results.FAILED,
2086                                   build_results.RESCHEDULED):
2087                         self._build_failed(node)
2088                     else:
2089                         self._build_succeeded(node)
2090 
2091         # NOTE(danms): We spawn here to return the RPC worker thread back to
2092         # the pool. Since what follows could take a really long time, we don't
2093         # want to tie up RPC workers.
2094         utils.spawn_n(_locked_do_build_and_run_instance,
2095                       context, instance, image, request_spec,
2096                       filter_properties, admin_password, injected_files,
2097                       requested_networks, security_groups,
2098                       block_device_mapping, node, limits, host_list)
2099 
2100     def _check_device_tagging(self, requested_networks, block_device_mapping):
2101         tagging_requested = False
2102         if requested_networks:
2103             for net in requested_networks:
2104                 if 'tag' in net and net.tag is not None:
2105                     tagging_requested = True
2106                     break
2107         if block_device_mapping and not tagging_requested:
2108             for bdm in block_device_mapping:
2109                 if 'tag' in bdm and bdm.tag is not None:
2110                     tagging_requested = True
2111                     break
2112         if (tagging_requested and
2113                 not self.driver.capabilities.get('supports_device_tagging',
2114                                                  False)):
2115             raise exception.BuildAbortException('Attempt to boot guest with '
2116                                                 'tagged devices on host that '
2117                                                 'does not support tagging.')
2118 
2119     def _check_trusted_certs(self, instance):
2120         if (instance.trusted_certs and
2121                 not self.driver.capabilities.get('supports_trusted_certs',
2122                                                  False)):
2123             raise exception.BuildAbortException(
2124                 'Trusted image certificates provided on host that does not '
2125                 'support certificate validation.')
2126 
2127     @hooks.add_hook('build_instance')
2128     @wrap_exception()
2129     @reverts_task_state
2130     @wrap_instance_event(prefix='compute')
2131     @wrap_instance_fault
2132     def _do_build_and_run_instance(self, context, instance, image,
2133             request_spec, filter_properties, admin_password, injected_files,
2134             requested_networks, security_groups, block_device_mapping,
2135             node=None, limits=None, host_list=None):
2136 
2137         try:
2138             LOG.debug('Starting instance...', instance=instance)
2139             instance.vm_state = vm_states.BUILDING
2140             instance.task_state = None
2141             instance.save(expected_task_state=
2142                     (task_states.SCHEDULING, None))
2143         except exception.InstanceNotFound:
2144             msg = 'Instance disappeared before build.'
2145             LOG.debug(msg, instance=instance)
2146             return build_results.FAILED
2147         except exception.UnexpectedTaskStateError as e:
2148             LOG.debug(e.format_message(), instance=instance)
2149             return build_results.FAILED
2150 
2151         # b64 decode the files to inject:
2152         decoded_files = self._decode_files(injected_files)
2153 
2154         if limits is None:
2155             limits = {}
2156 
2157         if node is None:
2158             node = self._get_nodename(instance, refresh=True)
2159 
2160         try:
2161             with timeutils.StopWatch() as timer:
2162                 self._build_and_run_instance(context, instance, image,
2163                         decoded_files, admin_password, requested_networks,
2164                         security_groups, block_device_mapping, node, limits,
2165                         filter_properties, request_spec)
2166             LOG.info('Took %0.2f seconds to build instance.',
2167                      timer.elapsed(), instance=instance)
2168             return build_results.ACTIVE
2169         except exception.RescheduledException as e:
2170             retry = filter_properties.get('retry')
2171             if not retry:
2172                 # no retry information, do not reschedule.
2173                 LOG.debug("Retry info not present, will not reschedule",
2174                     instance=instance)
2175                 self._cleanup_allocated_networks(context, instance,
2176                     requested_networks)
2177                 self._cleanup_volumes(context, instance,
2178                     block_device_mapping, raise_exc=False)
2179                 compute_utils.add_instance_fault_from_exc(context,
2180                         instance, e, sys.exc_info(),
2181                         fault_message=e.kwargs['reason'])
2182                 self._nil_out_instance_obj_host_and_node(instance)
2183                 self._set_instance_obj_error_state(context, instance,
2184                                                    clean_task_state=True)
2185                 return build_results.FAILED
2186             LOG.debug(e.format_message(), instance=instance)
2187             # This will be used for logging the exception
2188             retry['exc'] = traceback.format_exception(*sys.exc_info())
2189             # This will be used for setting the instance fault message
2190             retry['exc_reason'] = e.kwargs['reason']
2191             # NOTE(comstud): Deallocate networks if the driver wants
2192             # us to do so.
2193             # NOTE(mriedem): Always deallocate networking when using Neutron.
2194             # This is to unbind any ports that the user supplied in the server
2195             # create request, or delete any ports that nova created which were
2196             # meant to be bound to this host. This check intentionally bypasses
2197             # the result of deallocate_networks_on_reschedule because the
2198             # default value in the driver is False, but that method was really
2199             # only meant for Ironic and should be removed when nova-network is
2200             # removed (since is_neutron() will then always be True).
2201             # NOTE(vladikr): SR-IOV ports should be deallocated to
2202             # allow new sriov pci devices to be allocated on a new host.
2203             # Otherwise, if devices with pci addresses are already allocated
2204             # on the destination host, the instance will fail to spawn.
2205             # info_cache.network_info should be present at this stage.
2206             if (self.driver.deallocate_networks_on_reschedule(instance) or
2207                 utils.is_neutron() or
2208                 self.deallocate_sriov_ports_on_reschedule(instance)):
2209                 self._cleanup_allocated_networks(context, instance,
2210                         requested_networks)
2211             else:
2212                 # NOTE(alex_xu): Network already allocated and we don't
2213                 # want to deallocate them before rescheduling. But we need
2214                 # to cleanup those network resources setup on this host before
2215                 # rescheduling.
2216                 self.network_api.cleanup_instance_network_on_host(
2217                     context, instance, self.host)
2218 
2219             self._nil_out_instance_obj_host_and_node(instance)
2220             instance.task_state = task_states.SCHEDULING
2221             instance.save()
2222             # The instance will have already claimed resources from this host
2223             # before this build was attempted. Now that it has failed, we need
2224             # to unclaim those resources before casting to the conductor, so
2225             # that if there are alternate hosts available for a retry, it can
2226             # claim resources on that new host for the instance.
2227             self.reportclient.delete_allocation_for_instance(context,
2228                                                              instance.uuid)
2229 
2230             self.compute_task_api.build_instances(context, [instance],
2231                     image, filter_properties, admin_password,
2232                     injected_files, requested_networks, security_groups,
2233                     block_device_mapping, request_spec=request_spec,
2234                     host_lists=[host_list])
2235             return build_results.RESCHEDULED
2236         except (exception.InstanceNotFound,
2237                 exception.UnexpectedDeletingTaskStateError):
2238             msg = 'Instance disappeared during build.'
2239             LOG.debug(msg, instance=instance)
2240             self._cleanup_allocated_networks(context, instance,
2241                     requested_networks)
2242             return build_results.FAILED
2243         except Exception as e:
2244             if isinstance(e, exception.BuildAbortException):
2245                 LOG.error(e.format_message(), instance=instance)
2246             else:
2247                 # Should not reach here.
2248                 LOG.exception('Unexpected build failure, not rescheduling '
2249                               'build.', instance=instance)
2250             self._cleanup_allocated_networks(context, instance,
2251                     requested_networks)
2252             self._cleanup_volumes(context, instance,
2253                     block_device_mapping, raise_exc=False)
2254             compute_utils.add_instance_fault_from_exc(context, instance,
2255                     e, sys.exc_info())
2256             self._nil_out_instance_obj_host_and_node(instance)
2257             self._set_instance_obj_error_state(context, instance,
2258                                                clean_task_state=True)
2259             return build_results.FAILED
2260 
2261     def deallocate_sriov_ports_on_reschedule(self, instance):
2262         """Determine if networks are needed to be deallocated before reschedule
2263 
2264         Check the cached network info for any assigned SR-IOV ports.
2265         SR-IOV ports should be deallocated prior to rescheduling
2266         in order to allow new sriov pci devices to be allocated on a new host.
2267         """
2268         info_cache = instance.info_cache
2269 
2270         def _has_sriov_port(vif):
2271             return vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV
2272 
2273         if (info_cache and info_cache.network_info):
2274             for vif in info_cache.network_info:
2275                 if _has_sriov_port(vif):
2276                     return True
2277         return False
2278 
2279     @staticmethod
2280     def _get_scheduler_hints(filter_properties, request_spec=None):
2281         """Helper method to get scheduler hints.
2282 
2283         This method prefers to get the hints out of the request spec, but that
2284         might not be provided. Conductor will pass request_spec down to the
2285         first compute chosen for a build but older computes will not pass
2286         the request_spec to conductor's build_instances method for a
2287         a reschedule, so if we're on a host via a retry, request_spec may not
2288         be provided so we need to fallback to use the filter_properties
2289         to get scheduler hints.
2290         """
2291         hints = {}
2292         if request_spec is not None and 'scheduler_hints' in request_spec:
2293             hints = request_spec.scheduler_hints
2294         if not hints:
2295             hints = filter_properties.get('scheduler_hints') or {}
2296         return hints
2297 
2298     @staticmethod
2299     def _get_request_group_mapping(request_spec):
2300         """Return request group resource - provider mapping. This is currently
2301         used for Neutron ports that have resource request due to the port
2302         having QoS minimum bandwidth policy rule attached.
2303 
2304         :param request_spec: A RequestSpec object
2305         :returns: A dict keyed by RequestGroup requester_id, currently Neutron
2306         port_id, to resource provider UUID that provides resource for that
2307         RequestGroup.
2308         """
2309 
2310         if (request_spec and
2311                 'requested_resources' in request_spec and
2312                 request_spec.requested_resources is not None):
2313             return {
2314                 group.requester_id: group.provider_uuids
2315                 for group in request_spec.requested_resources
2316             }
2317         else:
2318             return None
2319 
2320     def _update_pci_request_spec_with_allocated_interface_name(
2321             self, context, instance, request_group_resource_providers_mapping):
2322         if not instance.pci_requests:
2323             return
2324 
2325         def needs_update(pci_request, mapping):
2326             return (pci_request.requester_id and
2327                     pci_request.requester_id in mapping)
2328 
2329         modified = False
2330         for pci_request in instance.pci_requests.requests:
2331             if needs_update(
2332                     pci_request, request_group_resource_providers_mapping):
2333 
2334                 provider_uuids = request_group_resource_providers_mapping[
2335                     pci_request.requester_id]
2336 
2337                 if len(provider_uuids) != 1:
2338                     reason = (
2339                         'Allocating resources from more than one resource '
2340                         'providers %(providers)s for a single pci request '
2341                         '%(requester)s is not supported.' %
2342                         {'providers': provider_uuids,
2343                          'requester': pci_request.requester_id})
2344                     raise exception.BuildAbortException(
2345                         instance_uuid=instance.uuid,
2346                         reason=reason)
2347 
2348                 dev_rp_name = self.reportclient.get_resource_provider_name(
2349                     context,
2350                     provider_uuids[0])
2351 
2352                 # NOTE(gibi): the device RP name reported by neutron is
2353                 # structured like <hostname>:<agentname>:<interfacename>
2354                 rp_name_pieces = dev_rp_name.split(':')
2355                 if len(rp_name_pieces) != 3:
2356                     reason = (
2357                         'Resource provider %(provider)s used to allocate '
2358                         'resources for the pci request %(requester)s does not '
2359                         'have properly formatted name. Expected name format '
2360                         'is <hostname>:<agentname>:<interfacename>, but got '
2361                         '%(provider_name)s' %
2362                         {'provider': provider_uuids[0],
2363                          'requester': pci_request.requester_id,
2364                          'provider_name': dev_rp_name})
2365                     raise exception.BuildAbortException(
2366                         instance_uuid=instance.uuid,
2367                         reason=reason)
2368 
2369                 for spec in pci_request.spec:
2370                     spec['parent_ifname'] = rp_name_pieces[2]
2371                     modified = True
2372         if modified:
2373             instance.save()
2374 
2375     def _build_and_run_instance(self, context, instance, image, injected_files,
2376             admin_password, requested_networks, security_groups,
2377             block_device_mapping, node, limits, filter_properties,
2378             request_spec=None):
2379 
2380         image_name = image.get('name')
2381         self._notify_about_instance_usage(context, instance, 'create.start',
2382                 extra_usage_info={'image_name': image_name})
2383         compute_utils.notify_about_instance_create(
2384             context, instance, self.host,
2385             phase=fields.NotificationPhase.START,
2386             bdms=block_device_mapping)
2387 
2388         # NOTE(mikal): cache the keystone roles associated with the instance
2389         # at boot time for later reference
2390         instance.system_metadata.update(
2391             {'boot_roles': ','.join(context.roles)})
2392 
2393         self._check_device_tagging(requested_networks, block_device_mapping)
2394         self._check_trusted_certs(instance)
2395 
2396         request_group_resource_providers_mapping = \
2397             self._get_request_group_mapping(request_spec)
2398 
2399         if request_group_resource_providers_mapping:
2400             self._update_pci_request_spec_with_allocated_interface_name(
2401                 context, instance, request_group_resource_providers_mapping)
2402 
2403         # TODO(Luyao) cut over to get_allocs_for_consumer
2404         allocs = self.reportclient.get_allocations_for_consumer(
2405                 context, instance.uuid)
2406 
2407         try:
2408             scheduler_hints = self._get_scheduler_hints(filter_properties,
2409                                                         request_spec)
2410             with self.rt.instance_claim(context, instance, node, allocs,
2411                                         limits):
2412                 # NOTE(russellb) It's important that this validation be done
2413                 # *after* the resource tracker instance claim, as that is where
2414                 # the host is set on the instance.
2415                 self._validate_instance_group_policy(context, instance,
2416                                                      scheduler_hints)
2417                 image_meta = objects.ImageMeta.from_dict(image)
2418 
2419                 request_group_resource_providers_mapping = \
2420                     self._get_request_group_mapping(request_spec)
2421 
2422                 with self._build_resources(context, instance,
2423                         requested_networks, security_groups, image_meta,
2424                         block_device_mapping,
2425                         request_group_resource_providers_mapping) as resources:
2426                     instance.vm_state = vm_states.BUILDING
2427                     instance.task_state = task_states.SPAWNING
2428                     # NOTE(JoshNang) This also saves the changes to the
2429                     # instance from _allocate_network_async, as they aren't
2430                     # saved in that function to prevent races.
2431                     instance.save(expected_task_state=
2432                             task_states.BLOCK_DEVICE_MAPPING)
2433                     block_device_info = resources['block_device_info']
2434                     network_info = resources['network_info']
2435                     LOG.debug('Start spawning the instance on the hypervisor.',
2436                               instance=instance)
2437                     with timeutils.StopWatch() as timer:
2438                         self.driver.spawn(context, instance, image_meta,
2439                                           injected_files, admin_password,
2440                                           allocs, network_info=network_info,
2441                                           block_device_info=block_device_info)
2442                     LOG.info('Took %0.2f seconds to spawn the instance on '
2443                              'the hypervisor.', timer.elapsed(),
2444                              instance=instance)
2445         except (exception.InstanceNotFound,
2446                 exception.UnexpectedDeletingTaskStateError) as e:
2447             with excutils.save_and_reraise_exception():
2448                 self._notify_about_instance_usage(context, instance,
2449                     'create.error', fault=e)
2450                 tb = traceback.format_exc()
2451                 compute_utils.notify_about_instance_create(
2452                     context, instance, self.host,
2453                     phase=fields.NotificationPhase.ERROR, exception=e,
2454                     bdms=block_device_mapping, tb=tb)
2455         except exception.ComputeResourcesUnavailable as e:
2456             LOG.debug(e.format_message(), instance=instance)
2457             self._notify_about_instance_usage(context, instance,
2458                     'create.error', fault=e)
2459             tb = traceback.format_exc()
2460             compute_utils.notify_about_instance_create(
2461                     context, instance, self.host,
2462                     phase=fields.NotificationPhase.ERROR, exception=e,
2463                     bdms=block_device_mapping, tb=tb)
2464             raise exception.RescheduledException(
2465                     instance_uuid=instance.uuid, reason=e.format_message())
2466         except exception.BuildAbortException as e:
2467             with excutils.save_and_reraise_exception():
2468                 LOG.debug(e.format_message(), instance=instance)
2469                 self._notify_about_instance_usage(context, instance,
2470                     'create.error', fault=e)
2471                 tb = traceback.format_exc()
2472                 compute_utils.notify_about_instance_create(
2473                     context, instance, self.host,
2474                     phase=fields.NotificationPhase.ERROR, exception=e,
2475                     bdms=block_device_mapping, tb=tb)
2476         except (exception.FixedIpLimitExceeded,
2477                 exception.NoMoreNetworks, exception.NoMoreFixedIps) as e:
2478             LOG.warning('No more network or fixed IP to be allocated',
2479                         instance=instance)
2480             self._notify_about_instance_usage(context, instance,
2481                     'create.error', fault=e)
2482             tb = traceback.format_exc()
2483             compute_utils.notify_about_instance_create(
2484                     context, instance, self.host,
2485                     phase=fields.NotificationPhase.ERROR, exception=e,
2486                     bdms=block_device_mapping, tb=tb)
2487             msg = _('Failed to allocate the network(s) with error %s, '
2488                     'not rescheduling.') % e.format_message()
2489             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2490                     reason=msg)
2491         except (exception.VirtualInterfaceCreateException,
2492                 exception.VirtualInterfaceMacAddressException,
2493                 exception.FixedIpInvalidOnHost,
2494                 exception.UnableToAutoAllocateNetwork,
2495                 exception.NetworksWithQoSPolicyNotSupported) as e:
2496             LOG.exception('Failed to allocate network(s)',
2497                           instance=instance)
2498             self._notify_about_instance_usage(context, instance,
2499                     'create.error', fault=e)
2500             tb = traceback.format_exc()
2501             compute_utils.notify_about_instance_create(
2502                     context, instance, self.host,
2503                     phase=fields.NotificationPhase.ERROR, exception=e,
2504                     bdms=block_device_mapping, tb=tb)
2505             msg = _('Failed to allocate the network(s), not rescheduling.')
2506             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2507                     reason=msg)
2508         except (exception.FlavorDiskTooSmall,
2509                 exception.FlavorMemoryTooSmall,
2510                 exception.ImageNotActive,
2511                 exception.ImageUnacceptable,
2512                 exception.InvalidDiskInfo,
2513                 exception.InvalidDiskFormat,
2514                 cursive_exception.SignatureVerificationError,
2515                 exception.CertificateValidationFailed,
2516                 exception.VolumeEncryptionNotSupported,
2517                 exception.InvalidInput,
2518                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2519                 # in the API during server create and rebuild.
2520                 exception.RequestedVRamTooHigh) as e:
2521             self._notify_about_instance_usage(context, instance,
2522                     'create.error', fault=e)
2523             tb = traceback.format_exc()
2524             compute_utils.notify_about_instance_create(
2525                     context, instance, self.host,
2526                     phase=fields.NotificationPhase.ERROR, exception=e,
2527                     bdms=block_device_mapping, tb=tb)
2528             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2529                     reason=e.format_message())
2530         except Exception as e:
2531             LOG.exception('Failed to build and run instance',
2532                           instance=instance)
2533             self._notify_about_instance_usage(context, instance,
2534                     'create.error', fault=e)
2535             tb = traceback.format_exc()
2536             compute_utils.notify_about_instance_create(
2537                     context, instance, self.host,
2538                     phase=fields.NotificationPhase.ERROR, exception=e,
2539                     bdms=block_device_mapping, tb=tb)
2540             raise exception.RescheduledException(
2541                     instance_uuid=instance.uuid, reason=six.text_type(e))
2542 
2543         # NOTE(alaski): This is only useful during reschedules, remove it now.
2544         instance.system_metadata.pop('network_allocated', None)
2545 
2546         # If CONF.default_access_ip_network_name is set, grab the
2547         # corresponding network and set the access ip values accordingly.
2548         network_name = CONF.default_access_ip_network_name
2549         if (network_name and not instance.access_ip_v4 and
2550                 not instance.access_ip_v6):
2551             # Note that when there are multiple ips to choose from, an
2552             # arbitrary one will be chosen.
2553             for vif in network_info:
2554                 if vif['network']['label'] == network_name:
2555                     for ip in vif.fixed_ips():
2556                         if not instance.access_ip_v4 and ip['version'] == 4:
2557                             instance.access_ip_v4 = ip['address']
2558                         if not instance.access_ip_v6 and ip['version'] == 6:
2559                             instance.access_ip_v6 = ip['address']
2560                     break
2561 
2562         self._update_instance_after_spawn(context, instance)
2563 
2564         try:
2565             instance.save(expected_task_state=task_states.SPAWNING)
2566         except (exception.InstanceNotFound,
2567                 exception.UnexpectedDeletingTaskStateError) as e:
2568             with excutils.save_and_reraise_exception():
2569                 self._notify_about_instance_usage(context, instance,
2570                     'create.error', fault=e)
2571                 tb = traceback.format_exc()
2572                 compute_utils.notify_about_instance_create(
2573                     context, instance, self.host,
2574                     phase=fields.NotificationPhase.ERROR, exception=e,
2575                     bdms=block_device_mapping, tb=tb)
2576 
2577         self._update_scheduler_instance_info(context, instance)
2578         self._notify_about_instance_usage(context, instance, 'create.end',
2579                 extra_usage_info={'message': _('Success')},
2580                 network_info=network_info)
2581         compute_utils.notify_about_instance_create(context, instance,
2582                 self.host, phase=fields.NotificationPhase.END,
2583                 bdms=block_device_mapping)
2584 
2585     @contextlib.contextmanager
2586     def _build_resources(self, context, instance, requested_networks,
2587                          security_groups, image_meta, block_device_mapping,
2588                          resource_provider_mapping):
2589         resources = {}
2590         network_info = None
2591         try:
2592             LOG.debug('Start building networks asynchronously for instance.',
2593                       instance=instance)
2594             network_info = self._build_networks_for_instance(context, instance,
2595                     requested_networks, security_groups,
2596                     resource_provider_mapping)
2597             resources['network_info'] = network_info
2598         except (exception.InstanceNotFound,
2599                 exception.UnexpectedDeletingTaskStateError):
2600             raise
2601         except exception.UnexpectedTaskStateError as e:
2602             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2603                     reason=e.format_message())
2604         except Exception:
2605             # Because this allocation is async any failures are likely to occur
2606             # when the driver accesses network_info during spawn().
2607             LOG.exception('Failed to allocate network(s)',
2608                           instance=instance)
2609             msg = _('Failed to allocate the network(s), not rescheduling.')
2610             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2611                     reason=msg)
2612 
2613         try:
2614             # Perform any driver preparation work for the driver.
2615             self.driver.prepare_for_spawn(instance)
2616 
2617             # Depending on a virt driver, some network configuration is
2618             # necessary before preparing block devices.
2619             self.driver.prepare_networks_before_block_device_mapping(
2620                 instance, network_info)
2621 
2622             # Verify that all the BDMs have a device_name set and assign a
2623             # default to the ones missing it with the help of the driver.
2624             self._default_block_device_names(instance, image_meta,
2625                                              block_device_mapping)
2626 
2627             LOG.debug('Start building block device mappings for instance.',
2628                       instance=instance)
2629             instance.vm_state = vm_states.BUILDING
2630             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2631             instance.save()
2632 
2633             block_device_info = self._prep_block_device(context, instance,
2634                     block_device_mapping)
2635             resources['block_device_info'] = block_device_info
2636         except (exception.InstanceNotFound,
2637                 exception.UnexpectedDeletingTaskStateError):
2638             with excutils.save_and_reraise_exception():
2639                 # Make sure the async call finishes
2640                 if network_info is not None:
2641                     network_info.wait(do_raise=False)
2642                     self.driver.clean_networks_preparation(instance,
2643                                                            network_info)
2644                 self.driver.failed_spawn_cleanup(instance)
2645         except (exception.UnexpectedTaskStateError,
2646                 exception.OverQuota, exception.InvalidBDM) as e:
2647             # Make sure the async call finishes
2648             if network_info is not None:
2649                 network_info.wait(do_raise=False)
2650                 self.driver.clean_networks_preparation(instance, network_info)
2651             self.driver.failed_spawn_cleanup(instance)
2652             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2653                     reason=e.format_message())
2654         except Exception:
2655             LOG.exception('Failure prepping block device',
2656                           instance=instance)
2657             # Make sure the async call finishes
2658             if network_info is not None:
2659                 network_info.wait(do_raise=False)
2660                 self.driver.clean_networks_preparation(instance, network_info)
2661             self.driver.failed_spawn_cleanup(instance)
2662             msg = _('Failure prepping block device.')
2663             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2664                     reason=msg)
2665 
2666         try:
2667             yield resources
2668         except Exception as exc:
2669             with excutils.save_and_reraise_exception() as ctxt:
2670                 if not isinstance(exc, (
2671                         exception.InstanceNotFound,
2672                         exception.UnexpectedDeletingTaskStateError)):
2673                     LOG.exception('Instance failed to spawn',
2674                                   instance=instance)
2675                 # Make sure the async call finishes
2676                 if network_info is not None:
2677                     network_info.wait(do_raise=False)
2678                 # if network_info is empty we're likely here because of
2679                 # network allocation failure. Since nothing can be reused on
2680                 # rescheduling it's better to deallocate network to eliminate
2681                 # the chance of orphaned ports in neutron
2682                 deallocate_networks = False if network_info else True
2683                 try:
2684                     self._shutdown_instance(context, instance,
2685                             block_device_mapping, requested_networks,
2686                             try_deallocate_networks=deallocate_networks)
2687                 except Exception as exc2:
2688                     ctxt.reraise = False
2689                     LOG.warning('Could not clean up failed build,'
2690                                 ' not rescheduling. Error: %s',
2691                                 six.text_type(exc2))
2692                     raise exception.BuildAbortException(
2693                             instance_uuid=instance.uuid,
2694                             reason=six.text_type(exc))
2695 
2696     def _cleanup_allocated_networks(self, context, instance,
2697             requested_networks):
2698         try:
2699             self._deallocate_network(context, instance, requested_networks)
2700         except Exception:
2701             LOG.exception('Failed to deallocate networks', instance=instance)
2702             return
2703 
2704         instance.system_metadata['network_allocated'] = 'False'
2705         try:
2706             instance.save()
2707         except exception.InstanceNotFound:
2708             # NOTE(alaski): It's possible that we're cleaning up the networks
2709             # because the instance was deleted.  If that's the case then this
2710             # exception will be raised by instance.save()
2711             pass
2712 
2713     def _try_deallocate_network(self, context, instance,
2714                                 requested_networks=None):
2715 
2716         # During auto-scale cleanup, we could be deleting a large number
2717         # of servers at the same time and overloading parts of the system,
2718         # so we retry a few times in case of connection failures to the
2719         # networking service.
2720         @loopingcall.RetryDecorator(
2721             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2722             exceptions=(keystone_exception.connection.ConnectFailure,))
2723         def _deallocate_network_with_retries():
2724             try:
2725                 self._deallocate_network(
2726                     context, instance, requested_networks)
2727             except keystone_exception.connection.ConnectFailure as e:
2728                 # Provide a warning that something is amiss.
2729                 with excutils.save_and_reraise_exception():
2730                     LOG.warning('Failed to deallocate network for instance; '
2731                                 'retrying. Error: %s', six.text_type(e),
2732                                 instance=instance)
2733 
2734         try:
2735             # tear down allocated network structure
2736             _deallocate_network_with_retries()
2737         except Exception as ex:
2738             with excutils.save_and_reraise_exception():
2739                 LOG.error('Failed to deallocate network for instance. '
2740                           'Error: %s', ex, instance=instance)
2741                 self._set_instance_obj_error_state(context, instance)
2742 
2743     def _get_power_off_values(self, context, instance, clean_shutdown):
2744         """Get the timing configuration for powering down this instance."""
2745         if clean_shutdown:
2746             timeout = compute_utils.get_value_from_system_metadata(instance,
2747                           key='image_os_shutdown_timeout', type=int,
2748                           default=CONF.shutdown_timeout)
2749             retry_interval = CONF.compute.shutdown_retry_interval
2750         else:
2751             timeout = 0
2752             retry_interval = 0
2753 
2754         return timeout, retry_interval
2755 
2756     def _power_off_instance(self, context, instance, clean_shutdown=True):
2757         """Power off an instance on this host."""
2758         timeout, retry_interval = self._get_power_off_values(context,
2759                                         instance, clean_shutdown)
2760         self.driver.power_off(instance, timeout, retry_interval)
2761 
2762     def _shutdown_instance(self, context, instance,
2763                            bdms, requested_networks=None, notify=True,
2764                            try_deallocate_networks=True):
2765         """Shutdown an instance on this host.
2766 
2767         :param:context: security context
2768         :param:instance: a nova.objects.Instance object
2769         :param:bdms: the block devices for the instance to be torn
2770                      down
2771         :param:requested_networks: the networks on which the instance
2772                                    has ports
2773         :param:notify: true if a final usage notification should be
2774                        emitted
2775         :param:try_deallocate_networks: false if we should avoid
2776                                         trying to teardown networking
2777         """
2778         context = context.elevated()
2779         LOG.info('Terminating instance', instance=instance)
2780 
2781         if notify:
2782             self._notify_about_instance_usage(context, instance,
2783                                               "shutdown.start")
2784             compute_utils.notify_about_instance_action(context, instance,
2785                     self.host, action=fields.NotificationAction.SHUTDOWN,
2786                     phase=fields.NotificationPhase.START, bdms=bdms)
2787 
2788         network_info = instance.get_network_info()
2789 
2790         # NOTE(arnaudmorin) to avoid nova destroying the instance without
2791         # unplugging the interface, refresh network_info if it is empty.
2792         if not network_info:
2793             network_info = self.network_api.get_instance_nw_info(
2794                 context, instance)
2795 
2796         # NOTE(vish) get bdms before destroying the instance
2797         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2798         block_device_info = self._get_instance_block_device_info(
2799             context, instance, bdms=bdms)
2800 
2801         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2802         #                want to keep ip allocated for certain failures
2803         try:
2804             LOG.debug('Start destroying the instance on the hypervisor.',
2805                       instance=instance)
2806             with timeutils.StopWatch() as timer:
2807                 self.driver.destroy(context, instance, network_info,
2808                                     block_device_info)
2809             LOG.info('Took %0.2f seconds to destroy the instance on the '
2810                      'hypervisor.', timer.elapsed(), instance=instance)
2811         except exception.InstancePowerOffFailure:
2812             # if the instance can't power off, don't release the ip
2813             with excutils.save_and_reraise_exception():
2814                 pass
2815         except Exception:
2816             with excutils.save_and_reraise_exception():
2817                 # deallocate ip and fail without proceeding to
2818                 # volume api calls, preserving current behavior
2819                 if try_deallocate_networks:
2820                     self._try_deallocate_network(context, instance,
2821                                                  requested_networks)
2822 
2823         if try_deallocate_networks:
2824             self._try_deallocate_network(context, instance, requested_networks)
2825 
2826         timer.restart()
2827         for bdm in vol_bdms:
2828             try:
2829                 if bdm.attachment_id:
2830                     self.volume_api.attachment_delete(context,
2831                                                       bdm.attachment_id)
2832                 else:
2833                     # NOTE(vish): actual driver detach done in driver.destroy,
2834                     #             so just tell cinder that we are done with it.
2835                     connector = self.driver.get_volume_connector(instance)
2836                     self.volume_api.terminate_connection(context,
2837                                                          bdm.volume_id,
2838                                                          connector)
2839                     self.volume_api.detach(context, bdm.volume_id,
2840                                            instance.uuid)
2841 
2842             except exception.VolumeAttachmentNotFound as exc:
2843                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2844                           instance=instance)
2845             except exception.DiskNotFound as exc:
2846                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2847                           instance=instance)
2848             except exception.VolumeNotFound as exc:
2849                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2850                           instance=instance)
2851             except (cinder_exception.EndpointNotFound,
2852                     keystone_exception.EndpointNotFound) as exc:
2853                 LOG.warning('Ignoring EndpointNotFound for '
2854                             'volume %(volume_id)s: %(exc)s',
2855                             {'exc': exc, 'volume_id': bdm.volume_id},
2856                             instance=instance)
2857             except cinder_exception.ClientException as exc:
2858                 LOG.warning('Ignoring unknown cinder exception for '
2859                             'volume %(volume_id)s: %(exc)s',
2860                             {'exc': exc, 'volume_id': bdm.volume_id},
2861                             instance=instance)
2862             except Exception as exc:
2863                 LOG.warning('Ignoring unknown exception for '
2864                             'volume %(volume_id)s: %(exc)s',
2865                             {'exc': exc, 'volume_id': bdm.volume_id},
2866                             instance=instance)
2867         if vol_bdms:
2868             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2869                      'for instance.',
2870                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2871                      instance=instance)
2872 
2873         if notify:
2874             self._notify_about_instance_usage(context, instance,
2875                                               "shutdown.end")
2876             compute_utils.notify_about_instance_action(context, instance,
2877                     self.host, action=fields.NotificationAction.SHUTDOWN,
2878                     phase=fields.NotificationPhase.END, bdms=bdms)
2879 
2880     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2881                          detach=True):
2882         exc_info = None
2883         for bdm in bdms:
2884             if detach and bdm.volume_id:
2885                 try:
2886                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2887                               instance_uuid=instance.uuid)
2888                     destroy = bdm.delete_on_termination
2889                     self._detach_volume(context, bdm, instance,
2890                                         destroy_bdm=destroy)
2891                 except Exception as exc:
2892                     exc_info = sys.exc_info()
2893                     LOG.warning('Failed to detach volume: %(volume_id)s '
2894                                 'due to %(exc)s',
2895                                 {'volume_id': bdm.volume_id, 'exc': exc})
2896 
2897             if bdm.volume_id and bdm.delete_on_termination:
2898                 try:
2899                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2900                               instance_uuid=instance.uuid)
2901                     self.volume_api.delete(context, bdm.volume_id)
2902                 except Exception as exc:
2903                     exc_info = sys.exc_info()
2904                     LOG.warning('Failed to delete volume: %(volume_id)s '
2905                                 'due to %(exc)s',
2906                                 {'volume_id': bdm.volume_id, 'exc': exc})
2907         if exc_info is not None and raise_exc:
2908             six.reraise(exc_info[0], exc_info[1], exc_info[2])
2909 
2910     @hooks.add_hook("delete_instance")
2911     def _delete_instance(self, context, instance, bdms):
2912         """Delete an instance on this host.
2913 
2914         :param context: nova request context
2915         :param instance: nova.objects.instance.Instance object
2916         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2917         """
2918         events = self.instance_events.clear_events_for_instance(instance)
2919         if events:
2920             LOG.debug('Events pending at deletion: %(events)s',
2921                       {'events': ','.join(events.keys())},
2922                       instance=instance)
2923         self._notify_about_instance_usage(context, instance,
2924                                           "delete.start")
2925         compute_utils.notify_about_instance_action(context, instance,
2926                 self.host, action=fields.NotificationAction.DELETE,
2927                 phase=fields.NotificationPhase.START, bdms=bdms)
2928 
2929         self._shutdown_instance(context, instance, bdms)
2930 
2931         # NOTE(vish): We have already deleted the instance, so we have
2932         #             to ignore problems cleaning up the volumes. It
2933         #             would be nice to let the user know somehow that
2934         #             the volume deletion failed, but it is not
2935         #             acceptable to have an instance that can not be
2936         #             deleted. Perhaps this could be reworked in the
2937         #             future to set an instance fault the first time
2938         #             and to only ignore the failure if the instance
2939         #             is already in ERROR.
2940 
2941         # NOTE(ameeda): The volumes already detached during the above
2942         #               _shutdown_instance() call and this is why
2943         #               detach is not requested from _cleanup_volumes()
2944         #               in this case
2945 
2946         self._cleanup_volumes(context, instance, bdms,
2947                 raise_exc=False, detach=False)
2948         # if a delete task succeeded, always update vm state and task
2949         # state without expecting task state to be DELETING
2950         instance.vm_state = vm_states.DELETED
2951         instance.task_state = None
2952         instance.power_state = power_state.NOSTATE
2953         instance.terminated_at = timeutils.utcnow()
2954         instance.save()
2955 
2956         self._complete_deletion(context, instance)
2957         # only destroy the instance in the db if the _complete_deletion
2958         # doesn't raise and therefore allocation is successfully
2959         # deleted in placement
2960         instance.destroy()
2961 
2962         self._notify_about_instance_usage(context, instance, "delete.end")
2963         compute_utils.notify_about_instance_action(context, instance,
2964                 self.host, action=fields.NotificationAction.DELETE,
2965                 phase=fields.NotificationPhase.END, bdms=bdms)
2966 
2967     @wrap_exception()
2968     @reverts_task_state
2969     @wrap_instance_event(prefix='compute')
2970     @wrap_instance_fault
2971     def terminate_instance(self, context, instance, bdms):
2972         """Terminate an instance on this host."""
2973         @utils.synchronized(instance.uuid)
2974         def do_terminate_instance(instance, bdms):
2975             # NOTE(mriedem): If we are deleting the instance while it was
2976             # booting from volume, we could be racing with a database update of
2977             # the BDM volume_id. Since the compute API passes the BDMs over RPC
2978             # to compute here, the BDMs may be stale at this point. So check
2979             # for any volume BDMs that don't have volume_id set and if we
2980             # detect that, we need to refresh the BDM list before proceeding.
2981             # TODO(mriedem): Move this into _delete_instance and make the bdms
2982             # parameter optional.
2983             for bdm in list(bdms):
2984                 if bdm.is_volume and not bdm.volume_id:
2985                     LOG.debug('There are potentially stale BDMs during '
2986                               'delete, refreshing the BlockDeviceMappingList.',
2987                               instance=instance)
2988                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2989                         context, instance.uuid)
2990                     break
2991             try:
2992                 self._delete_instance(context, instance, bdms)
2993             except exception.InstanceNotFound:
2994                 LOG.info("Instance disappeared during terminate",
2995                          instance=instance)
2996             except Exception:
2997                 # As we're trying to delete always go to Error if something
2998                 # goes wrong that _delete_instance can't handle.
2999                 with excutils.save_and_reraise_exception():
3000                     LOG.exception('Setting instance vm_state to ERROR',
3001                                   instance=instance)
3002                     self._set_instance_obj_error_state(context, instance)
3003 
3004         do_terminate_instance(instance, bdms)
3005 
3006     # NOTE(johannes): This is probably better named power_off_instance
3007     # so it matches the driver method, but because of other issues, we
3008     # can't use that name in grizzly.
3009     @wrap_exception()
3010     @reverts_task_state
3011     @wrap_instance_event(prefix='compute')
3012     @wrap_instance_fault
3013     def stop_instance(self, context, instance, clean_shutdown):
3014         """Stopping an instance on this host."""
3015 
3016         @utils.synchronized(instance.uuid)
3017         def do_stop_instance():
3018             current_power_state = self._get_power_state(context, instance)
3019             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
3020                       'current task_state: %(task_state)s, current DB '
3021                       'power_state: %(db_power_state)s, current VM '
3022                       'power_state: %(current_power_state)s',
3023                       {'vm_state': instance.vm_state,
3024                        'task_state': instance.task_state,
3025                        'db_power_state': instance.power_state,
3026                        'current_power_state': current_power_state},
3027                       instance_uuid=instance.uuid)
3028 
3029             # NOTE(mriedem): If the instance is already powered off, we are
3030             # possibly tearing down and racing with other operations, so we can
3031             # expect the task_state to be None if something else updates the
3032             # instance and we're not locking it.
3033             expected_task_state = [task_states.POWERING_OFF]
3034             # The list of power states is from _sync_instance_power_state.
3035             if current_power_state in (power_state.NOSTATE,
3036                                        power_state.SHUTDOWN,
3037                                        power_state.CRASHED):
3038                 LOG.info('Instance is already powered off in the '
3039                          'hypervisor when stop is called.',
3040                          instance=instance)
3041                 expected_task_state.append(None)
3042 
3043             self._notify_about_instance_usage(context, instance,
3044                                               "power_off.start")
3045 
3046             compute_utils.notify_about_instance_action(context, instance,
3047                         self.host, action=fields.NotificationAction.POWER_OFF,
3048                         phase=fields.NotificationPhase.START)
3049 
3050             self._power_off_instance(context, instance, clean_shutdown)
3051             instance.power_state = self._get_power_state(context, instance)
3052             instance.vm_state = vm_states.STOPPED
3053             instance.task_state = None
3054             instance.save(expected_task_state=expected_task_state)
3055             self._notify_about_instance_usage(context, instance,
3056                                               "power_off.end")
3057 
3058             compute_utils.notify_about_instance_action(context, instance,
3059                         self.host, action=fields.NotificationAction.POWER_OFF,
3060                         phase=fields.NotificationPhase.END)
3061 
3062         do_stop_instance()
3063 
3064     def _power_on(self, context, instance):
3065         network_info = self.network_api.get_instance_nw_info(context, instance)
3066         block_device_info = self._get_instance_block_device_info(context,
3067                                                                  instance)
3068         self.driver.power_on(context, instance,
3069                              network_info,
3070                              block_device_info)
3071 
3072     def _delete_snapshot_of_shelved_instance(self, context, instance,
3073                                              snapshot_id):
3074         """Delete snapshot of shelved instance."""
3075         try:
3076             self.image_api.delete(context, snapshot_id)
3077         except (exception.ImageNotFound,
3078                 exception.ImageNotAuthorized) as exc:
3079             LOG.warning("Failed to delete snapshot "
3080                         "from shelved instance (%s).",
3081                         exc.format_message(), instance=instance)
3082         except Exception:
3083             LOG.exception("Something wrong happened when trying to "
3084                           "delete snapshot from shelved instance.",
3085                           instance=instance)
3086 
3087     # NOTE(johannes): This is probably better named power_on_instance
3088     # so it matches the driver method, but because of other issues, we
3089     # can't use that name in grizzly.
3090     @wrap_exception()
3091     @reverts_task_state
3092     @wrap_instance_event(prefix='compute')
3093     @wrap_instance_fault
3094     def start_instance(self, context, instance):
3095         """Starting an instance on this host."""
3096         self._notify_about_instance_usage(context, instance, "power_on.start")
3097         compute_utils.notify_about_instance_action(context, instance,
3098             self.host, action=fields.NotificationAction.POWER_ON,
3099             phase=fields.NotificationPhase.START)
3100         self._power_on(context, instance)
3101         instance.power_state = self._get_power_state(context, instance)
3102         instance.vm_state = vm_states.ACTIVE
3103         instance.task_state = None
3104 
3105         # Delete an image(VM snapshot) for a shelved instance
3106         snapshot_id = instance.system_metadata.get('shelved_image_id')
3107         if snapshot_id:
3108             self._delete_snapshot_of_shelved_instance(context, instance,
3109                                                       snapshot_id)
3110 
3111         # Delete system_metadata for a shelved instance
3112         compute_utils.remove_shelved_keys_from_system_metadata(instance)
3113 
3114         instance.save(expected_task_state=task_states.POWERING_ON)
3115         self._notify_about_instance_usage(context, instance, "power_on.end")
3116         compute_utils.notify_about_instance_action(context, instance,
3117             self.host, action=fields.NotificationAction.POWER_ON,
3118             phase=fields.NotificationPhase.END)
3119 
3120     @messaging.expected_exceptions(NotImplementedError,
3121                                    exception.TriggerCrashDumpNotSupported,
3122                                    exception.InstanceNotRunning)
3123     @wrap_exception()
3124     @wrap_instance_event(prefix='compute')
3125     @wrap_instance_fault
3126     def trigger_crash_dump(self, context, instance):
3127         """Trigger crash dump in an instance."""
3128 
3129         self._notify_about_instance_usage(context, instance,
3130                                           "trigger_crash_dump.start")
3131         compute_utils.notify_about_instance_action(context, instance,
3132                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3133                 phase=fields.NotificationPhase.START)
3134 
3135         # This method does not change task_state and power_state because the
3136         # effect of a trigger depends on user's configuration.
3137         self.driver.trigger_crash_dump(instance)
3138 
3139         self._notify_about_instance_usage(context, instance,
3140                                           "trigger_crash_dump.end")
3141         compute_utils.notify_about_instance_action(context, instance,
3142                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3143                 phase=fields.NotificationPhase.END)
3144 
3145     @wrap_exception()
3146     @reverts_task_state
3147     @wrap_instance_event(prefix='compute')
3148     @wrap_instance_fault
3149     def soft_delete_instance(self, context, instance):
3150         """Soft delete an instance on this host."""
3151         with compute_utils.notify_about_instance_delete(
3152                 self.notifier, context, instance, 'soft_delete',
3153                 source=fields.NotificationSource.COMPUTE):
3154             try:
3155                 self.driver.soft_delete(instance)
3156             except NotImplementedError:
3157                 # Fallback to just powering off the instance if the
3158                 # hypervisor doesn't implement the soft_delete method
3159                 self.driver.power_off(instance)
3160             instance.power_state = self._get_power_state(context, instance)
3161             instance.vm_state = vm_states.SOFT_DELETED
3162             instance.task_state = None
3163             instance.save(expected_task_state=[task_states.SOFT_DELETING])
3164 
3165     @wrap_exception()
3166     @reverts_task_state
3167     @wrap_instance_event(prefix='compute')
3168     @wrap_instance_fault
3169     def restore_instance(self, context, instance):
3170         """Restore a soft-deleted instance on this host."""
3171         self._notify_about_instance_usage(context, instance, "restore.start")
3172         compute_utils.notify_about_instance_action(context, instance,
3173             self.host, action=fields.NotificationAction.RESTORE,
3174             phase=fields.NotificationPhase.START)
3175         try:
3176             self.driver.restore(instance)
3177         except NotImplementedError:
3178             # Fallback to just powering on the instance if the hypervisor
3179             # doesn't implement the restore method
3180             self._power_on(context, instance)
3181         instance.power_state = self._get_power_state(context, instance)
3182         instance.vm_state = vm_states.ACTIVE
3183         instance.task_state = None
3184         instance.save(expected_task_state=task_states.RESTORING)
3185         self._notify_about_instance_usage(context, instance, "restore.end")
3186         compute_utils.notify_about_instance_action(context, instance,
3187             self.host, action=fields.NotificationAction.RESTORE,
3188             phase=fields.NotificationPhase.END)
3189 
3190     @staticmethod
3191     def _set_migration_status(migration, status):
3192         """Set the status, and guard against a None being passed in.
3193 
3194         This is useful as some of the compute RPC calls will not pass
3195         a migration object in older versions. The check can be removed when
3196         we move past 4.x major version of the RPC API.
3197         """
3198         if migration:
3199             migration.status = status
3200             migration.save()
3201 
3202     def _rebuild_default_impl(self, context, instance, image_meta,
3203                               injected_files, admin_password, allocations,
3204                               bdms, detach_block_devices, attach_block_devices,
3205                               network_info=None,
3206                               evacuate=False, block_device_info=None,
3207                               preserve_ephemeral=False):
3208         if preserve_ephemeral:
3209             # The default code path does not support preserving ephemeral
3210             # partitions.
3211             raise exception.PreserveEphemeralNotSupported()
3212 
3213         if evacuate:
3214             detach_block_devices(context, bdms)
3215         else:
3216             self._power_off_instance(context, instance, clean_shutdown=True)
3217             detach_block_devices(context, bdms)
3218             self.driver.destroy(context, instance,
3219                                 network_info=network_info,
3220                                 block_device_info=block_device_info)
3221 
3222         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
3223         instance.save(expected_task_state=[task_states.REBUILDING])
3224 
3225         new_block_device_info = attach_block_devices(context, instance, bdms)
3226 
3227         instance.task_state = task_states.REBUILD_SPAWNING
3228         instance.save(
3229             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
3230 
3231         with instance.mutated_migration_context():
3232             self.driver.spawn(context, instance, image_meta, injected_files,
3233                               admin_password, allocations,
3234                               network_info=network_info,
3235                               block_device_info=new_block_device_info)
3236 
3237     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
3238         tb = traceback.format_exc()
3239         self._notify_about_instance_usage(context, instance,
3240                                           'rebuild.error', fault=error)
3241         compute_utils.notify_about_instance_rebuild(
3242             context, instance, self.host,
3243             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms,
3244             tb=tb)
3245 
3246     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
3247     @wrap_exception()
3248     @reverts_task_state
3249     @wrap_instance_event(prefix='compute')
3250     @wrap_instance_fault
3251     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
3252                          injected_files, new_pass, orig_sys_metadata,
3253                          bdms, recreate, on_shared_storage,
3254                          preserve_ephemeral, migration,
3255                          scheduled_node, limits, request_spec):
3256         """Destroy and re-make this instance.
3257 
3258         A 'rebuild' effectively purges all existing data from the system and
3259         remakes the VM with given 'metadata' and 'personalities'.
3260 
3261         :param context: `nova.RequestContext` object
3262         :param instance: Instance object
3263         :param orig_image_ref: Original image_ref before rebuild
3264         :param image_ref: New image_ref for rebuild
3265         :param injected_files: Files to inject
3266         :param new_pass: password to set on rebuilt instance
3267         :param orig_sys_metadata: instance system metadata from pre-rebuild
3268         :param bdms: block-device-mappings to use for rebuild
3269         :param recreate: True if the instance is being evacuated (e.g. the
3270             hypervisor it was on failed) - cleanup of old state will be
3271             skipped.
3272         :param on_shared_storage: True if instance files on shared storage.
3273                                   If not provided then information from the
3274                                   driver will be used to decide if the instance
3275                                   files are available or not on the target host
3276         :param preserve_ephemeral: True if the default ephemeral storage
3277                                    partition must be preserved on rebuild
3278         :param migration: a Migration object if one was created for this
3279                           rebuild operation (if it's a part of evacuate)
3280         :param scheduled_node: A node of the host chosen by the scheduler. If a
3281                                host was specified by the user, this will be
3282                                None
3283         :param limits: Overcommit limits set by the scheduler. If a host was
3284                        specified by the user, this will be None
3285         :param request_spec: a RequestSpec object used to schedule the instance
3286 
3287         """
3288         # recreate=True means the instance is being evacuated from a failed
3289         # host to a new destination host (this host). The 'recreate' variable
3290         # name is confusing, so rename it to evacuate here at the top, which
3291         # is simpler than renaming a parameter in an RPC versioned method.
3292         evacuate = recreate
3293         context = context.elevated()
3294 
3295         if evacuate:
3296             LOG.info("Evacuating instance", instance=instance)
3297         else:
3298             LOG.info("Rebuilding instance", instance=instance)
3299 
3300         if evacuate:
3301             # This is an evacuation to a new host, so we need to perform a
3302             # resource claim.
3303             rebuild_claim = self.rt.rebuild_claim
3304         else:
3305             # This is a rebuild to the same host, so we don't need to make
3306             # a claim since the instance is already on this host.
3307             rebuild_claim = claims.NopClaim
3308 
3309         if image_ref:
3310             image_meta = objects.ImageMeta.from_image_ref(
3311                 context, self.image_api, image_ref)
3312         elif evacuate:
3313             # For evacuate the API does not send down the image_ref since the
3314             # image does not change so just get it from what was stashed in
3315             # the instance system_metadata when the instance was created (or
3316             # last rebuilt). This also works for volume-backed instances.
3317             image_meta = instance.image_meta
3318         else:
3319             image_meta = objects.ImageMeta()
3320 
3321         # NOTE(mriedem): On an evacuate, we need to update
3322         # the instance's host and node properties to reflect it's
3323         # destination node for the evacuate.
3324         if not scheduled_node:
3325             if evacuate:
3326                 try:
3327                     compute_node = self._get_compute_info(context, self.host)
3328                     scheduled_node = compute_node.hypervisor_hostname
3329                 except exception.ComputeHostNotFound:
3330                     LOG.exception('Failed to get compute_info for %s',
3331                                   self.host)
3332             else:
3333                 scheduled_node = instance.node
3334 
3335         allocs = self.reportclient.get_allocations_for_consumer(
3336                     context, instance.uuid)
3337 
3338         # If the resource claim or group policy validation fails before we
3339         # do anything to the guest or its networking/volumes we want to keep
3340         # the current status rather than put the instance into ERROR status.
3341         instance_state = instance.vm_state
3342         with self._error_out_instance_on_exception(
3343                 context, instance, instance_state=instance_state):
3344             try:
3345                 self._do_rebuild_instance_with_claim(
3346                     context, instance, orig_image_ref,
3347                     image_meta, injected_files, new_pass, orig_sys_metadata,
3348                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3349                     migration, request_spec, allocs, rebuild_claim,
3350                     scheduled_node, limits)
3351             except (exception.ComputeResourcesUnavailable,
3352                     exception.RescheduledException) as e:
3353                 if isinstance(e, exception.ComputeResourcesUnavailable):
3354                     LOG.debug("Could not rebuild instance on this host, not "
3355                               "enough resources available.", instance=instance)
3356                 else:
3357                     # RescheduledException is raised by the late server group
3358                     # policy check during evacuation if a parallel scheduling
3359                     # violated the policy.
3360                     # We catch the RescheduledException here but we don't have
3361                     # the plumbing to do an actual reschedule so we abort the
3362                     # operation.
3363                     LOG.debug("Could not rebuild instance on this host, "
3364                               "late server group check failed.",
3365                               instance=instance)
3366                 # NOTE(ndipanov): We just abort the build for now and leave a
3367                 # migration record for potential cleanup later
3368                 self._set_migration_status(migration, 'failed')
3369                 # Since the claim failed, we need to remove the allocation
3370                 # created against the destination node. Note that we can only
3371                 # get here when evacuating to a destination node. Rebuilding
3372                 # on the same host (not evacuate) uses the NopClaim which will
3373                 # not raise ComputeResourcesUnavailable.
3374                 self.rt.delete_allocation_for_evacuated_instance(
3375                     context, instance, scheduled_node, node_type='destination')
3376                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3377                 # Wrap this in InstanceFaultRollback so that the
3378                 # _error_out_instance_on_exception context manager keeps the
3379                 # vm_state unchanged.
3380                 raise exception.InstanceFaultRollback(
3381                     inner_exception=exception.BuildAbortException(
3382                         instance_uuid=instance.uuid,
3383                         reason=e.format_message()))
3384             except (exception.InstanceNotFound,
3385                     exception.UnexpectedDeletingTaskStateError) as e:
3386                 LOG.debug('Instance was deleted while rebuilding',
3387                           instance=instance)
3388                 self._set_migration_status(migration, 'failed')
3389                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3390             except Exception as e:
3391                 self._set_migration_status(migration, 'failed')
3392                 if evacuate or scheduled_node is not None:
3393                     self.rt.delete_allocation_for_evacuated_instance(
3394                         context, instance, scheduled_node,
3395                         node_type='destination')
3396                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3397                 raise
3398             else:
3399                 instance.apply_migration_context()
3400                 # NOTE (ndipanov): This save will now update the host and node
3401                 # attributes making sure that next RT pass is consistent since
3402                 # it will be based on the instance and not the migration DB
3403                 # entry.
3404                 instance.host = self.host
3405                 instance.node = scheduled_node
3406                 instance.save()
3407                 instance.drop_migration_context()
3408 
3409                 # NOTE (ndipanov): Mark the migration as done only after we
3410                 # mark the instance as belonging to this host.
3411                 self._set_migration_status(migration, 'done')
3412 
3413     def _do_rebuild_instance_with_claim(
3414             self, context, instance, orig_image_ref, image_meta,
3415             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3416             on_shared_storage, preserve_ephemeral, migration, request_spec,
3417             allocations, rebuild_claim, scheduled_node, limits):
3418         """Helper to avoid deep nesting in the top-level method."""
3419 
3420         request_group_resource_providers_mapping = None
3421         if evacuate:
3422             request_group_resource_providers_mapping = \
3423                 self._get_request_group_mapping(request_spec)
3424 
3425             if request_group_resource_providers_mapping:
3426                 self._update_pci_request_spec_with_allocated_interface_name(
3427                     context, instance,
3428                     request_group_resource_providers_mapping)
3429 
3430         claim_context = rebuild_claim(
3431             context, instance, scheduled_node, allocations,
3432             limits=limits, image_meta=image_meta, migration=migration)
3433 
3434         with claim_context:
3435             self._do_rebuild_instance(
3436                 context, instance, orig_image_ref, image_meta, injected_files,
3437                 new_pass, orig_sys_metadata, bdms, evacuate, on_shared_storage,
3438                 preserve_ephemeral, migration, request_spec, allocations,
3439                 request_group_resource_providers_mapping)
3440 
3441     @staticmethod
3442     def _get_image_name(image_meta):
3443         if image_meta.obj_attr_is_set("name"):
3444             return image_meta.name
3445         else:
3446             return ''
3447 
3448     def _do_rebuild_instance(self, context, instance, orig_image_ref,
3449                              image_meta, injected_files, new_pass,
3450                              orig_sys_metadata, bdms, evacuate,
3451                              on_shared_storage, preserve_ephemeral,
3452                              migration, request_spec, allocations,
3453                              request_group_resource_providers_mapping):
3454         orig_vm_state = instance.vm_state
3455 
3456         if evacuate:
3457             if request_spec:
3458                 # NOTE(gibi): Do a late check of server group policy as
3459                 # parallel scheduling could violate such policy. This will
3460                 # cause the evacuate to fail as rebuild does not implement
3461                 # reschedule.
3462                 hints = self._get_scheduler_hints({}, request_spec)
3463                 self._validate_instance_group_policy(context, instance, hints)
3464 
3465             if not self.driver.capabilities.get("supports_evacuate", False):
3466                 raise exception.InstanceEvacuateNotSupported
3467 
3468             self._check_instance_exists(context, instance)
3469 
3470             if on_shared_storage is None:
3471                 LOG.debug('on_shared_storage is not provided, using driver '
3472                           'information to decide if the instance needs to '
3473                           'be evacuated')
3474                 on_shared_storage = self.driver.instance_on_disk(instance)
3475 
3476             elif (on_shared_storage !=
3477                     self.driver.instance_on_disk(instance)):
3478                 # To cover case when admin expects that instance files are
3479                 # on shared storage, but not accessible and vice versa
3480                 raise exception.InvalidSharedStorage(
3481                         _("Invalid state of instance files on shared"
3482                             " storage"))
3483 
3484             if on_shared_storage:
3485                 LOG.info('disk on shared storage, evacuating using'
3486                          ' existing disk')
3487             elif instance.image_ref:
3488                 orig_image_ref = instance.image_ref
3489                 LOG.info("disk not on shared storage, evacuating from "
3490                          "image: '%s'", str(orig_image_ref))
3491             else:
3492                 LOG.info('disk on volume, evacuating using existing '
3493                          'volume')
3494 
3495         # We check trusted certs capabilities for both evacuate (rebuild on
3496         # another host) and rebuild (rebuild on the same host) because for
3497         # evacuate we need to make sure an instance with trusted certs can
3498         # have the image verified with those certs during rebuild, and for
3499         # rebuild we could be rebuilding a server that started out with no
3500         # trusted certs on this host, and then was rebuilt with trusted certs
3501         # for a new image, in which case we need to validate that new image
3502         # with the trusted certs during the rebuild.
3503         self._check_trusted_certs(instance)
3504 
3505         # This instance.exists message should contain the original
3506         # image_ref, not the new one.  Since the DB has been updated
3507         # to point to the new one... we have to override it.
3508         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3509                                                                context)
3510         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3511         compute_utils.notify_usage_exists(
3512                 self.notifier, context, instance, self.host,
3513                 current_period=True, system_metadata=orig_sys_metadata,
3514                 extra_usage_info=extra_usage_info)
3515 
3516         # This message should contain the new image_ref
3517         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3518         self._notify_about_instance_usage(context, instance,
3519                 "rebuild.start", extra_usage_info=extra_usage_info)
3520         # NOTE: image_name is not included in the versioned notification
3521         # because we already provide the image_uuid in the notification
3522         # payload and the image details can be looked up via the uuid.
3523         compute_utils.notify_about_instance_rebuild(
3524             context, instance, self.host,
3525             phase=fields.NotificationPhase.START,
3526             bdms=bdms)
3527 
3528         instance.power_state = self._get_power_state(context, instance)
3529         instance.task_state = task_states.REBUILDING
3530         instance.save(expected_task_state=[task_states.REBUILDING])
3531 
3532         if evacuate:
3533             self.network_api.setup_networks_on_host(
3534                     context, instance, self.host)
3535             # For nova-network this is needed to move floating IPs
3536             # For neutron this updates the host in the port binding
3537             # TODO(cfriesen): this network_api call and the one above
3538             # are so similar, we should really try to unify them.
3539             self.network_api.setup_instance_network_on_host(
3540                 context, instance, self.host, migration,
3541                 provider_mappings=request_group_resource_providers_mapping)
3542             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3543             # with @base_api.refresh_cache and then we wouldn't need this
3544             # explicit call to get_instance_nw_info.
3545             network_info = self.network_api.get_instance_nw_info(context,
3546                                                                  instance)
3547         else:
3548             network_info = instance.get_network_info()
3549 
3550         if bdms is None:
3551             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3552                     context, instance.uuid)
3553 
3554         block_device_info = \
3555             self._get_instance_block_device_info(
3556                     context, instance, bdms=bdms)
3557 
3558         def detach_block_devices(context, bdms):
3559             for bdm in bdms:
3560                 if bdm.is_volume:
3561                     # NOTE (ildikov): Having the attachment_id set in the BDM
3562                     # means that it's the new Cinder attach/detach flow
3563                     # (available from v3.44). In that case we explicitly
3564                     # attach and detach the volumes through attachment level
3565                     # operations. In this scenario _detach_volume will delete
3566                     # the existing attachment which would make the volume
3567                     # status change to 'available' if we don't pre-create
3568                     # another empty attachment before deleting the old one.
3569                     attachment_id = None
3570                     if bdm.attachment_id:
3571                         attachment_id = self.volume_api.attachment_create(
3572                             context, bdm['volume_id'], instance.uuid)['id']
3573                     self._detach_volume(context, bdm, instance,
3574                                         destroy_bdm=False)
3575                     if attachment_id:
3576                         bdm.attachment_id = attachment_id
3577                         bdm.save()
3578 
3579         files = self._decode_files(injected_files)
3580 
3581         kwargs = dict(
3582             context=context,
3583             instance=instance,
3584             image_meta=image_meta,
3585             injected_files=files,
3586             admin_password=new_pass,
3587             allocations=allocations,
3588             bdms=bdms,
3589             detach_block_devices=detach_block_devices,
3590             attach_block_devices=self._prep_block_device,
3591             block_device_info=block_device_info,
3592             network_info=network_info,
3593             preserve_ephemeral=preserve_ephemeral,
3594             evacuate=evacuate)
3595         try:
3596             with instance.mutated_migration_context():
3597                 self.driver.rebuild(**kwargs)
3598         except NotImplementedError:
3599             # NOTE(rpodolyaka): driver doesn't provide specialized version
3600             # of rebuild, fall back to the default implementation
3601             self._rebuild_default_impl(**kwargs)
3602         self._update_instance_after_spawn(context, instance)
3603         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3604 
3605         if orig_vm_state == vm_states.STOPPED:
3606             LOG.info("bringing vm to original state: '%s'",
3607                      orig_vm_state, instance=instance)
3608             instance.vm_state = vm_states.ACTIVE
3609             instance.task_state = task_states.POWERING_OFF
3610             instance.progress = 0
3611             instance.save()
3612             self.stop_instance(context, instance, False)
3613         # TODO(melwitt): We should clean up instance console tokens here in the
3614         # case of evacuate. The instance is on a new host and will need to
3615         # establish a new console connection.
3616         self._update_scheduler_instance_info(context, instance)
3617         self._notify_about_instance_usage(
3618                 context, instance, "rebuild.end",
3619                 network_info=network_info,
3620                 extra_usage_info=extra_usage_info)
3621         compute_utils.notify_about_instance_rebuild(
3622             context, instance, self.host,
3623             phase=fields.NotificationPhase.END,
3624             bdms=bdms)
3625 
3626     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3627                                      block_device_info):
3628         """Handle cases where the virt-layer had to detach non-working volumes
3629         in order to complete an operation.
3630         """
3631         for bdm in block_device_info['block_device_mapping']:
3632             if bdm.get('mount_device') in bad_devices:
3633                 try:
3634                     volume_id = bdm['connection_info']['data']['volume_id']
3635                 except KeyError:
3636                     continue
3637 
3638                 # NOTE(sirp): ideally we'd just call
3639                 # `compute_api.detach_volume` here but since that hits the
3640                 # DB directly, that's off limits from within the
3641                 # compute-manager.
3642                 #
3643                 # API-detach
3644                 LOG.info("Detaching from volume api: %s", volume_id)
3645                 self.volume_api.begin_detaching(context, volume_id)
3646 
3647                 # Manager-detach
3648                 self.detach_volume(context, volume_id, instance)
3649 
3650     @wrap_exception()
3651     @reverts_task_state
3652     @wrap_instance_event(prefix='compute')
3653     @wrap_instance_fault
3654     def reboot_instance(self, context, instance, block_device_info,
3655                         reboot_type):
3656         """Reboot an instance on this host."""
3657         # acknowledge the request made it to the manager
3658         if reboot_type == "SOFT":
3659             instance.task_state = task_states.REBOOT_PENDING
3660             expected_states = task_states.soft_reboot_states
3661         else:
3662             instance.task_state = task_states.REBOOT_PENDING_HARD
3663             expected_states = task_states.hard_reboot_states
3664 
3665         context = context.elevated()
3666         LOG.info("Rebooting instance", instance=instance)
3667 
3668         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3669             context, instance.uuid)
3670         block_device_info = self._get_instance_block_device_info(
3671             context, instance, bdms=bdms)
3672 
3673         network_info = self.network_api.get_instance_nw_info(context, instance)
3674 
3675         self._notify_about_instance_usage(context, instance, "reboot.start")
3676         compute_utils.notify_about_instance_action(
3677             context, instance, self.host,
3678             action=fields.NotificationAction.REBOOT,
3679             phase=fields.NotificationPhase.START,
3680             bdms=bdms
3681         )
3682 
3683         instance.power_state = self._get_power_state(context, instance)
3684         instance.save(expected_task_state=expected_states)
3685 
3686         if instance.power_state != power_state.RUNNING:
3687             state = instance.power_state
3688             running = power_state.RUNNING
3689             LOG.warning('trying to reboot a non-running instance:'
3690                         ' (state: %(state)s expected: %(running)s)',
3691                         {'state': state, 'running': running},
3692                         instance=instance)
3693 
3694         def bad_volumes_callback(bad_devices):
3695             self._handle_bad_volumes_detached(
3696                     context, instance, bad_devices, block_device_info)
3697 
3698         try:
3699             # Don't change it out of rescue mode
3700             if instance.vm_state == vm_states.RESCUED:
3701                 new_vm_state = vm_states.RESCUED
3702             else:
3703                 new_vm_state = vm_states.ACTIVE
3704             new_power_state = None
3705             if reboot_type == "SOFT":
3706                 instance.task_state = task_states.REBOOT_STARTED
3707                 expected_state = task_states.REBOOT_PENDING
3708             else:
3709                 instance.task_state = task_states.REBOOT_STARTED_HARD
3710                 expected_state = task_states.REBOOT_PENDING_HARD
3711             instance.save(expected_task_state=expected_state)
3712             self.driver.reboot(context, instance,
3713                                network_info,
3714                                reboot_type,
3715                                block_device_info=block_device_info,
3716                                bad_volumes_callback=bad_volumes_callback)
3717 
3718         except Exception as error:
3719             with excutils.save_and_reraise_exception() as ctxt:
3720                 exc_info = sys.exc_info()
3721                 # if the reboot failed but the VM is running don't
3722                 # put it into an error state
3723                 new_power_state = self._get_power_state(context, instance)
3724                 if new_power_state == power_state.RUNNING:
3725                     LOG.warning('Reboot failed but instance is running',
3726                                 instance=instance)
3727                     compute_utils.add_instance_fault_from_exc(context,
3728                             instance, error, exc_info)
3729                     self._notify_about_instance_usage(context, instance,
3730                             'reboot.error', fault=error)
3731                     tb = traceback.format_exc()
3732                     compute_utils.notify_about_instance_action(
3733                         context, instance, self.host,
3734                         action=fields.NotificationAction.REBOOT,
3735                         phase=fields.NotificationPhase.ERROR,
3736                         exception=error, bdms=bdms, tb=tb
3737                     )
3738                     ctxt.reraise = False
3739                 else:
3740                     LOG.error('Cannot reboot instance: %s', error,
3741                               instance=instance)
3742                     self._set_instance_obj_error_state(context, instance)
3743 
3744         if not new_power_state:
3745             new_power_state = self._get_power_state(context, instance)
3746         try:
3747             instance.power_state = new_power_state
3748             instance.vm_state = new_vm_state
3749             instance.task_state = None
3750             instance.save()
3751         except exception.InstanceNotFound:
3752             LOG.warning("Instance disappeared during reboot",
3753                         instance=instance)
3754 
3755         self._notify_about_instance_usage(context, instance, "reboot.end")
3756         compute_utils.notify_about_instance_action(
3757             context, instance, self.host,
3758             action=fields.NotificationAction.REBOOT,
3759             phase=fields.NotificationPhase.END,
3760             bdms=bdms
3761         )
3762 
3763     @delete_image_on_error
3764     def _do_snapshot_instance(self, context, image_id, instance):
3765         self._snapshot_instance(context, image_id, instance,
3766                                 task_states.IMAGE_BACKUP)
3767 
3768     @wrap_exception()
3769     @reverts_task_state
3770     @wrap_instance_event(prefix='compute')
3771     @wrap_instance_fault
3772     def backup_instance(self, context, image_id, instance, backup_type,
3773                         rotation):
3774         """Backup an instance on this host.
3775 
3776         :param backup_type: daily | weekly
3777         :param rotation: int representing how many backups to keep around
3778         """
3779         self._do_snapshot_instance(context, image_id, instance)
3780         self._rotate_backups(context, instance, backup_type, rotation)
3781 
3782     @wrap_exception()
3783     @reverts_task_state
3784     @wrap_instance_event(prefix='compute')
3785     @wrap_instance_fault
3786     @delete_image_on_error
3787     def snapshot_instance(self, context, image_id, instance):
3788         """Snapshot an instance on this host.
3789 
3790         :param context: security context
3791         :param image_id: glance.db.sqlalchemy.models.Image.Id
3792         :param instance: a nova.objects.instance.Instance object
3793         """
3794         # NOTE(dave-mcnally) the task state will already be set by the api
3795         # but if the compute manager has crashed/been restarted prior to the
3796         # request getting here the task state may have been cleared so we set
3797         # it again and things continue normally
3798         try:
3799             instance.task_state = task_states.IMAGE_SNAPSHOT
3800             instance.save(
3801                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3802         except exception.InstanceNotFound:
3803             # possibility instance no longer exists, no point in continuing
3804             LOG.debug("Instance not found, could not set state %s "
3805                       "for instance.",
3806                       task_states.IMAGE_SNAPSHOT, instance=instance)
3807             return
3808 
3809         except exception.UnexpectedDeletingTaskStateError:
3810             LOG.debug("Instance being deleted, snapshot cannot continue",
3811                       instance=instance)
3812             return
3813 
3814         self._snapshot_instance(context, image_id, instance,
3815                                 task_states.IMAGE_SNAPSHOT)
3816 
3817     def _snapshot_instance(self, context, image_id, instance,
3818                            expected_task_state):
3819         context = context.elevated()
3820 
3821         instance.power_state = self._get_power_state(context, instance)
3822         try:
3823             instance.save()
3824 
3825             LOG.info('instance snapshotting', instance=instance)
3826 
3827             if instance.power_state != power_state.RUNNING:
3828                 state = instance.power_state
3829                 running = power_state.RUNNING
3830                 LOG.warning('trying to snapshot a non-running instance: '
3831                             '(state: %(state)s expected: %(running)s)',
3832                             {'state': state, 'running': running},
3833                             instance=instance)
3834 
3835             self._notify_about_instance_usage(
3836                 context, instance, "snapshot.start")
3837             compute_utils.notify_about_instance_snapshot(context, instance,
3838                 self.host, phase=fields.NotificationPhase.START,
3839                 snapshot_image_id=image_id)
3840 
3841             def update_task_state(task_state,
3842                                   expected_state=expected_task_state):
3843                 instance.task_state = task_state
3844                 instance.save(expected_task_state=expected_state)
3845 
3846             with timeutils.StopWatch() as timer:
3847                 self.driver.snapshot(context, instance, image_id,
3848                                      update_task_state)
3849             LOG.info('Took %0.2f seconds to snapshot the instance on '
3850                      'the hypervisor.', timer.elapsed(), instance=instance)
3851 
3852             instance.task_state = None
3853             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3854 
3855             self._notify_about_instance_usage(context, instance,
3856                                               "snapshot.end")
3857             compute_utils.notify_about_instance_snapshot(context, instance,
3858                 self.host, phase=fields.NotificationPhase.END,
3859                 snapshot_image_id=image_id)
3860         except (exception.InstanceNotFound,
3861                 exception.UnexpectedDeletingTaskStateError):
3862             # the instance got deleted during the snapshot
3863             # Quickly bail out of here
3864             msg = 'Instance disappeared during snapshot'
3865             LOG.debug(msg, instance=instance)
3866             try:
3867                 image = self.image_api.get(context, image_id)
3868                 if image['status'] != 'active':
3869                     self.image_api.delete(context, image_id)
3870             except exception.ImageNotFound:
3871                 LOG.debug('Image not found during clean up %s', image_id)
3872             except Exception:
3873                 LOG.warning("Error while trying to clean up image %s",
3874                             image_id, instance=instance)
3875         except exception.ImageNotFound:
3876             instance.task_state = None
3877             instance.save()
3878             LOG.warning("Image not found during snapshot", instance=instance)
3879 
3880     def _post_interrupted_snapshot_cleanup(self, context, instance):
3881         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3882 
3883     @messaging.expected_exceptions(NotImplementedError)
3884     @wrap_exception()
3885     def volume_snapshot_create(self, context, instance, volume_id,
3886                                create_info):
3887         self.driver.volume_snapshot_create(context, instance, volume_id,
3888                                            create_info)
3889 
3890     @messaging.expected_exceptions(NotImplementedError)
3891     @wrap_exception()
3892     def volume_snapshot_delete(self, context, instance, volume_id,
3893                                snapshot_id, delete_info):
3894         self.driver.volume_snapshot_delete(context, instance, volume_id,
3895                                            snapshot_id, delete_info)
3896 
3897     @wrap_instance_fault
3898     def _rotate_backups(self, context, instance, backup_type, rotation):
3899         """Delete excess backups associated to an instance.
3900 
3901         Instances are allowed a fixed number of backups (the rotation number);
3902         this method deletes the oldest backups that exceed the rotation
3903         threshold.
3904 
3905         :param context: security context
3906         :param instance: Instance dict
3907         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3908         :param rotation: int representing how many backups to keep around;
3909             None if rotation shouldn't be used (as in the case of snapshots)
3910         """
3911         filters = {'property-image_type': 'backup',
3912                    'property-backup_type': backup_type,
3913                    'property-instance_uuid': instance.uuid}
3914 
3915         images = self.image_api.get_all(context, filters=filters,
3916                                         sort_key='created_at', sort_dir='desc')
3917         num_images = len(images)
3918         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3919                   {'num_images': num_images, 'rotation': rotation},
3920                   instance=instance)
3921 
3922         if num_images > rotation:
3923             # NOTE(sirp): this deletes all backups that exceed the rotation
3924             # limit
3925             excess = len(images) - rotation
3926             LOG.debug("Rotating out %d backups", excess,
3927                       instance=instance)
3928             for i in range(excess):
3929                 image = images.pop()
3930                 image_id = image['id']
3931                 LOG.debug("Deleting image %s", image_id,
3932                           instance=instance)
3933                 try:
3934                     self.image_api.delete(context, image_id)
3935                 except exception.ImageNotFound:
3936                     LOG.info("Failed to find image %(image_id)s to "
3937                              "delete", {'image_id': image_id},
3938                              instance=instance)
3939                 except (exception.ImageDeleteConflict, Exception) as exc:
3940                     LOG.info("Failed to delete image %(image_id)s during "
3941                              "deleting excess backups. "
3942                              "Continuing for next image.. %(exc)s",
3943                              {'image_id': image_id, 'exc': exc},
3944                              instance=instance)
3945 
3946     @wrap_exception()
3947     @reverts_task_state
3948     @wrap_instance_event(prefix='compute')
3949     @wrap_instance_fault
3950     def set_admin_password(self, context, instance, new_pass):
3951         """Set the root/admin password for an instance on this host.
3952 
3953         This is generally only called by API password resets after an
3954         image has been built.
3955 
3956         @param context: Nova auth context.
3957         @param instance: Nova instance object.
3958         @param new_pass: The admin password for the instance.
3959         """
3960 
3961         context = context.elevated()
3962         current_power_state = self._get_power_state(context, instance)
3963         expected_state = power_state.RUNNING
3964 
3965         if current_power_state != expected_state:
3966             instance.task_state = None
3967             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
3968             _msg = _('instance %s is not running') % instance.uuid
3969             raise exception.InstancePasswordSetFailed(
3970                 instance=instance.uuid, reason=_msg)
3971 
3972         try:
3973             self.driver.set_admin_password(instance, new_pass)
3974             LOG.info("Admin password set", instance=instance)
3975             instance.task_state = None
3976             instance.save(
3977                 expected_task_state=task_states.UPDATING_PASSWORD)
3978         except exception.InstanceAgentNotEnabled:
3979             with excutils.save_and_reraise_exception():
3980                 LOG.debug('Guest agent is not enabled for the instance.',
3981                           instance=instance)
3982                 instance.task_state = None
3983                 instance.save(
3984                     expected_task_state=task_states.UPDATING_PASSWORD)
3985         except exception.SetAdminPasswdNotSupported:
3986             with excutils.save_and_reraise_exception():
3987                 LOG.info('set_admin_password is not supported '
3988                          'by this driver or guest instance.',
3989                          instance=instance)
3990                 instance.task_state = None
3991                 instance.save(
3992                     expected_task_state=task_states.UPDATING_PASSWORD)
3993         except NotImplementedError:
3994             LOG.warning('set_admin_password is not implemented '
3995                         'by this driver or guest instance.',
3996                         instance=instance)
3997             instance.task_state = None
3998             instance.save(
3999                 expected_task_state=task_states.UPDATING_PASSWORD)
4000             raise NotImplementedError(_('set_admin_password is not '
4001                                         'implemented by this driver or guest '
4002                                         'instance.'))
4003         except exception.UnexpectedTaskStateError:
4004             # interrupted by another (most likely delete) task
4005             # do not retry
4006             raise
4007         except Exception:
4008             # Catch all here because this could be anything.
4009             LOG.exception('set_admin_password failed', instance=instance)
4010             # We create a new exception here so that we won't
4011             # potentially reveal password information to the
4012             # API caller.  The real exception is logged above
4013             _msg = _('error setting admin password')
4014             raise exception.InstancePasswordSetFailed(
4015                 instance=instance.uuid, reason=_msg)
4016 
4017     @wrap_exception()
4018     @reverts_task_state
4019     @wrap_instance_fault
4020     def inject_file(self, context, path, file_contents, instance):
4021         """Write a file to the specified path in an instance on this host."""
4022         # NOTE(russellb) Remove this method, as well as the underlying virt
4023         # driver methods, when the compute rpc interface is bumped to 4.x
4024         # as it is no longer used.
4025         context = context.elevated()
4026         current_power_state = self._get_power_state(context, instance)
4027         expected_state = power_state.RUNNING
4028         if current_power_state != expected_state:
4029             LOG.warning('trying to inject a file into a non-running '
4030                         '(state: %(current_state)s expected: '
4031                         '%(expected_state)s)',
4032                         {'current_state': current_power_state,
4033                          'expected_state': expected_state},
4034                         instance=instance)
4035         LOG.info('injecting file to %s', path, instance=instance)
4036         self.driver.inject_file(instance, path, file_contents)
4037 
4038     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
4039         """Determine what image should be used to boot the rescue VM."""
4040         # 1. If rescue_image_ref is passed in, use that for rescue.
4041         # 2. Else, use the base image associated with instance's current image.
4042         #       The idea here is to provide the customer with a rescue
4043         #       environment which they are familiar with.
4044         #       So, if they built their instance off of a Debian image,
4045         #       their rescue VM will also be Debian.
4046         # 3. As a last resort, use instance's current image.
4047         if not rescue_image_ref:
4048             system_meta = utils.instance_sys_meta(instance)
4049             rescue_image_ref = system_meta.get('image_base_image_ref')
4050 
4051         if not rescue_image_ref:
4052             LOG.warning('Unable to find a different image to use for '
4053                         'rescue VM, using instance\'s current image',
4054                         instance=instance)
4055             rescue_image_ref = instance.image_ref
4056 
4057         return objects.ImageMeta.from_image_ref(
4058             context, self.image_api, rescue_image_ref)
4059 
4060     @wrap_exception()
4061     @reverts_task_state
4062     @wrap_instance_event(prefix='compute')
4063     @wrap_instance_fault
4064     def rescue_instance(self, context, instance, rescue_password,
4065                         rescue_image_ref, clean_shutdown):
4066         context = context.elevated()
4067         LOG.info('Rescuing', instance=instance)
4068 
4069         admin_password = (rescue_password if rescue_password else
4070                       utils.generate_password())
4071 
4072         network_info = self.network_api.get_instance_nw_info(context, instance)
4073 
4074         rescue_image_meta = self._get_rescue_image(context, instance,
4075                                                    rescue_image_ref)
4076 
4077         extra_usage_info = {'rescue_image_name':
4078                             self._get_image_name(rescue_image_meta)}
4079         self._notify_about_instance_usage(context, instance,
4080                 "rescue.start", extra_usage_info=extra_usage_info,
4081                 network_info=network_info)
4082         compute_utils.notify_about_instance_rescue_action(
4083             context, instance, self.host, rescue_image_ref,
4084             phase=fields.NotificationPhase.START)
4085 
4086         try:
4087             self._power_off_instance(context, instance, clean_shutdown)
4088 
4089             self.driver.rescue(context, instance,
4090                                network_info,
4091                                rescue_image_meta, admin_password)
4092         except Exception as e:
4093             LOG.exception("Error trying to Rescue Instance",
4094                           instance=instance)
4095             self._set_instance_obj_error_state(context, instance)
4096             raise exception.InstanceNotRescuable(
4097                 instance_id=instance.uuid,
4098                 reason=_("Driver Error: %s") % e)
4099 
4100         compute_utils.notify_usage_exists(self.notifier, context, instance,
4101                                           self.host, current_period=True)
4102 
4103         instance.vm_state = vm_states.RESCUED
4104         instance.task_state = None
4105         instance.power_state = self._get_power_state(context, instance)
4106         instance.launched_at = timeutils.utcnow()
4107         instance.save(expected_task_state=task_states.RESCUING)
4108 
4109         self._notify_about_instance_usage(context, instance,
4110                 "rescue.end", extra_usage_info=extra_usage_info,
4111                 network_info=network_info)
4112         compute_utils.notify_about_instance_rescue_action(
4113             context, instance, self.host, rescue_image_ref,
4114             phase=fields.NotificationPhase.END)
4115 
4116     @wrap_exception()
4117     @reverts_task_state
4118     @wrap_instance_event(prefix='compute')
4119     @wrap_instance_fault
4120     def unrescue_instance(self, context, instance):
4121         context = context.elevated()
4122         LOG.info('Unrescuing', instance=instance)
4123 
4124         network_info = self.network_api.get_instance_nw_info(context, instance)
4125         self._notify_about_instance_usage(context, instance,
4126                 "unrescue.start", network_info=network_info)
4127         compute_utils.notify_about_instance_action(context, instance,
4128             self.host, action=fields.NotificationAction.UNRESCUE,
4129             phase=fields.NotificationPhase.START)
4130 
4131         with self._error_out_instance_on_exception(context, instance):
4132             self.driver.unrescue(instance,
4133                                  network_info)
4134 
4135         instance.vm_state = vm_states.ACTIVE
4136         instance.task_state = None
4137         instance.power_state = self._get_power_state(context, instance)
4138         instance.save(expected_task_state=task_states.UNRESCUING)
4139 
4140         self._notify_about_instance_usage(context,
4141                                           instance,
4142                                           "unrescue.end",
4143                                           network_info=network_info)
4144         compute_utils.notify_about_instance_action(context, instance,
4145             self.host, action=fields.NotificationAction.UNRESCUE,
4146             phase=fields.NotificationPhase.END)
4147 
4148     @wrap_exception()
4149     @wrap_instance_fault
4150     def change_instance_metadata(self, context, diff, instance):
4151         """Update the metadata published to the instance."""
4152         LOG.debug("Changing instance metadata according to %r",
4153                   diff, instance=instance)
4154         self.driver.change_instance_metadata(context, instance, diff)
4155 
4156     @wrap_exception()
4157     @wrap_instance_event(prefix='compute')
4158     @errors_out_migration
4159     @wrap_instance_fault
4160     def confirm_resize(self, context, instance, migration):
4161         """Confirms a migration/resize and deletes the 'old' instance.
4162 
4163         This is called from the API and runs on the source host.
4164 
4165         Nothing needs to happen on the destination host at this point since
4166         the instance is already running there. This routine just cleans up the
4167         source host.
4168         """
4169         @utils.synchronized(instance.uuid)
4170         def do_confirm_resize(context, instance, migration_id):
4171             # NOTE(wangpan): Get the migration status from db, if it has been
4172             #                confirmed, we do nothing and return here
4173             LOG.debug("Going to confirm migration %s", migration_id,
4174                       instance=instance)
4175             try:
4176                 # TODO(russellb) Why are we sending the migration object just
4177                 # to turn around and look it up from the db again?
4178                 migration = objects.Migration.get_by_id(
4179                                     context.elevated(), migration_id)
4180             except exception.MigrationNotFound:
4181                 LOG.error("Migration %s is not found during confirmation",
4182                           migration_id, instance=instance)
4183                 return
4184 
4185             if migration.status == 'confirmed':
4186                 LOG.info("Migration %s is already confirmed",
4187                          migration_id, instance=instance)
4188                 return
4189             elif migration.status not in ('finished', 'confirming'):
4190                 LOG.warning("Unexpected confirmation status '%(status)s' "
4191                             "of migration %(id)s, exit confirmation process",
4192                             {"status": migration.status, "id": migration_id},
4193                             instance=instance)
4194                 return
4195 
4196             # NOTE(wangpan): Get the instance from db, if it has been
4197             #                deleted, we do nothing and return here
4198             expected_attrs = ['metadata', 'system_metadata', 'flavor']
4199             try:
4200                 instance = objects.Instance.get_by_uuid(
4201                         context, instance.uuid,
4202                         expected_attrs=expected_attrs)
4203             except exception.InstanceNotFound:
4204                 LOG.info("Instance is not found during confirmation",
4205                          instance=instance)
4206                 return
4207 
4208             with self._error_out_instance_on_exception(context, instance):
4209                 try:
4210                     self._confirm_resize(
4211                         context, instance, migration=migration)
4212                 except Exception:
4213                     # Something failed when cleaning up the source host so
4214                     # log a traceback and leave a hint about hard rebooting
4215                     # the server to correct its state in the DB.
4216                     with excutils.save_and_reraise_exception(logger=LOG):
4217                         LOG.exception(
4218                             'Confirm resize failed on source host %s. '
4219                             'Resource allocations in the placement service '
4220                             'will be removed regardless because the instance '
4221                             'is now on the destination host %s. You can try '
4222                             'hard rebooting the instance to correct its '
4223                             'state.', self.host, migration.dest_compute,
4224                             instance=instance)
4225                 finally:
4226                     # Whether an error occurred or not, at this point the
4227                     # instance is on the dest host so to avoid leaking
4228                     # allocations in placement, delete them here.
4229                     self._delete_allocation_after_move(
4230                         context, instance, migration)
4231 
4232         do_confirm_resize(context, instance, migration.id)
4233 
4234     def _get_updated_nw_info_with_pci_mapping(self, nw_info, pci_mapping):
4235         # NOTE(adrianc): This method returns a copy of nw_info if modifications
4236         # are made else it returns the original nw_info.
4237         updated_nw_info = nw_info
4238         if nw_info and pci_mapping:
4239             updated_nw_info = copy.deepcopy(nw_info)
4240             for vif in updated_nw_info:
4241                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
4242                     try:
4243                         vif_pci_addr = vif['profile']['pci_slot']
4244                         new_addr = pci_mapping[vif_pci_addr].address
4245                         vif['profile']['pci_slot'] = new_addr
4246                         LOG.debug("Updating VIF's PCI address for VIF %(id)s. "
4247                                   "Original value %(orig_val)s, "
4248                                   "new value %(new_val)s",
4249                                   {'id': vif['id'],
4250                                    'orig_val': vif_pci_addr,
4251                                    'new_val': new_addr})
4252                     except (KeyError, AttributeError):
4253                         with excutils.save_and_reraise_exception():
4254                             # NOTE(adrianc): This should never happen. If we
4255                             # get here it means there is some inconsistency
4256                             # with either 'nw_info' or 'pci_mapping'.
4257                             LOG.error("Unexpected error when updating network "
4258                                       "information with PCI mapping.")
4259         return updated_nw_info
4260 
4261     def _confirm_resize(self, context, instance, migration=None):
4262         """Destroys the source instance."""
4263         self._notify_about_instance_usage(context, instance,
4264                                           "resize.confirm.start")
4265         compute_utils.notify_about_instance_action(context, instance,
4266             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4267             phase=fields.NotificationPhase.START)
4268 
4269         # NOTE(danms): delete stashed migration information
4270         old_instance_type = instance.old_flavor
4271         instance.old_flavor = None
4272         instance.new_flavor = None
4273         instance.system_metadata.pop('old_vm_state', None)
4274         instance.save()
4275 
4276         # NOTE(tr3buchet): tear down networks on source host
4277         self.network_api.setup_networks_on_host(context, instance,
4278                            migration.source_compute, teardown=True)
4279         network_info = self.network_api.get_instance_nw_info(context,
4280                                                              instance)
4281 
4282         # NOTE(adrianc): Populate old PCI device in VIF profile
4283         # to allow virt driver to properly unplug it from Hypervisor.
4284         pci_mapping = (instance.migration_context.
4285                        get_pci_mapping_for_migration(True))
4286         network_info = self._get_updated_nw_info_with_pci_mapping(
4287             network_info, pci_mapping)
4288 
4289         self.driver.confirm_migration(context, migration, instance,
4290                                       network_info)
4291 
4292         migration.status = 'confirmed'
4293         migration.save()
4294 
4295         # NOTE(mriedem): drop_move_claim relies on
4296         # instance.migration_context so make sure to not call
4297         # instance.drop_migration_context() until after drop_move_claim
4298         # is called.
4299         self.rt.drop_move_claim(context, instance, migration.source_node,
4300                                 old_instance_type, prefix='old_')
4301         instance.drop_migration_context()
4302 
4303         # NOTE(mriedem): The old_vm_state could be STOPPED but the user
4304         # might have manually powered up the instance to confirm the
4305         # resize/migrate, so we need to check the current power state
4306         # on the instance and set the vm_state appropriately. We default
4307         # to ACTIVE because if the power state is not SHUTDOWN, we
4308         # assume _sync_instance_power_state will clean it up.
4309         p_state = instance.power_state
4310         vm_state = None
4311         if p_state == power_state.SHUTDOWN:
4312             vm_state = vm_states.STOPPED
4313             LOG.debug("Resized/migrated instance is powered off. "
4314                       "Setting vm_state to '%s'.", vm_state,
4315                       instance=instance)
4316         else:
4317             vm_state = vm_states.ACTIVE
4318 
4319         instance.vm_state = vm_state
4320         instance.task_state = None
4321         instance.save(expected_task_state=[None, task_states.DELETING,
4322                                            task_states.SOFT_DELETING])
4323 
4324         self._notify_about_instance_usage(
4325             context, instance, "resize.confirm.end",
4326             network_info=network_info)
4327         compute_utils.notify_about_instance_action(context, instance,
4328                self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4329                phase=fields.NotificationPhase.END)
4330 
4331     def _delete_allocation_after_move(self, context, instance, migration):
4332         """Deletes resource allocations held by the migration record against
4333         the source compute node resource provider after a confirmed cold /
4334         successful live migration.
4335         """
4336         try:
4337             # NOTE(danms): We're finishing on the source node, so try
4338             # to delete the allocation based on the migration uuid
4339             self.reportclient.delete_allocation_for_instance(
4340                 context, migration.uuid, consumer_type='migration')
4341         except exception.AllocationDeleteFailed:
4342             LOG.error('Deleting allocation in placement for migration '
4343                       '%(migration_uuid)s failed. The instance '
4344                       '%(instance_uuid)s will be put to ERROR state '
4345                       'but the allocation held by the migration is '
4346                       'leaked.',
4347                       {'instance_uuid': instance.uuid,
4348                        'migration_uuid': migration.uuid})
4349             raise
4350 
4351     @wrap_exception()
4352     @reverts_task_state
4353     @wrap_instance_event(prefix='compute')
4354     @errors_out_migration
4355     @wrap_instance_fault
4356     def revert_resize(self, context, instance, migration, request_spec=None):
4357         """Destroys the new instance on the destination machine.
4358 
4359         Reverts the model changes, and powers on the old instance on the
4360         source machine.
4361 
4362         """
4363         # NOTE(comstud): A revert_resize is essentially a resize back to
4364         # the old size, so we need to send a usage event here.
4365         compute_utils.notify_usage_exists(self.notifier, context, instance,
4366                                           self.host, current_period=True)
4367 
4368         with self._error_out_instance_on_exception(context, instance):
4369             # NOTE(tr3buchet): tear down networks on destination host
4370             self.network_api.setup_networks_on_host(context, instance,
4371                                                     teardown=True)
4372 
4373             self.network_api.migrate_instance_start(context,
4374                                                     instance,
4375                                                     migration)
4376 
4377             network_info = self.network_api.get_instance_nw_info(context,
4378                                                                  instance)
4379             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4380                     context, instance.uuid)
4381             block_device_info = self._get_instance_block_device_info(
4382                                 context, instance, bdms=bdms)
4383 
4384             destroy_disks = not self._is_instance_storage_shared(
4385                 context, instance, host=migration.source_compute)
4386             self.driver.destroy(context, instance, network_info,
4387                                 block_device_info, destroy_disks)
4388 
4389             self._terminate_volume_connections(context, instance, bdms)
4390 
4391             migration.status = 'reverted'
4392             migration.save()
4393 
4394             # NOTE(ndipanov): We need to do this here because dropping the
4395             # claim means we lose the migration_context data. We really should
4396             # fix this by moving the drop_move_claim call to the
4397             # finish_revert_resize method as this is racy (revert is dropped,
4398             # but instance resources will be tracked with the new flavor until
4399             # it gets rolled back in finish_revert_resize, which is
4400             # potentially wrong for a period of time).
4401             instance.revert_migration_context()
4402             instance.save()
4403 
4404             self.rt.drop_move_claim(context, instance, instance.node)
4405 
4406             # RPC cast back to the source host to finish the revert there.
4407             self.compute_rpcapi.finish_revert_resize(context, instance,
4408                     migration, migration.source_compute, request_spec)
4409 
4410     def _finish_revert_resize_network_migrate_finish(
4411             self, context, instance, migration, provider_mappings):
4412         """Causes port binding to be updated. In some Neutron or port
4413         configurations - see NetworkModel.get_bind_time_events() - we
4414         expect the vif-plugged event from Neutron immediately and wait for it.
4415         The rest of the time, the event is expected further along in the
4416         virt driver, so we don't wait here.
4417 
4418         :param context: The request context.
4419         :param instance: The instance undergoing the revert resize.
4420         :param migration: The Migration object of the resize being reverted.
4421         :param provider_mappings: a dict of list of resource provider uuids
4422             keyed by port uuid
4423         :raises: eventlet.timeout.Timeout or
4424                  exception.VirtualInterfacePlugException.
4425         """
4426         network_info = instance.get_network_info()
4427         events = []
4428         deadline = CONF.vif_plugging_timeout
4429         if deadline and utils.is_neutron() and network_info:
4430             events = network_info.get_bind_time_events(migration)
4431             if events:
4432                 LOG.debug('Will wait for bind-time events: %s', events)
4433         error_cb = self._neutron_failed_migration_callback
4434         try:
4435             with self.virtapi.wait_for_instance_event(instance, events,
4436                                                       deadline=deadline,
4437                                                       error_callback=error_cb):
4438                 # NOTE(hanrong): we need to change migration.dest_compute to
4439                 # source host temporarily.
4440                 # "network_api.migrate_instance_finish" will setup the network
4441                 # for the instance on the destination host. For revert resize,
4442                 # the instance will back to the source host, the setup of the
4443                 # network for instance should be on the source host. So set
4444                 # the migration.dest_compute to source host at here.
4445                 with utils.temporary_mutation(
4446                         migration, dest_compute=migration.source_compute):
4447                     self.network_api.migrate_instance_finish(
4448                         context, instance, migration, provider_mappings)
4449         except eventlet.timeout.Timeout:
4450             with excutils.save_and_reraise_exception():
4451                 LOG.error('Timeout waiting for Neutron events: %s', events,
4452                           instance=instance)
4453 
4454     @wrap_exception()
4455     @reverts_task_state
4456     @wrap_instance_event(prefix='compute')
4457     @errors_out_migration
4458     @wrap_instance_fault
4459     def finish_revert_resize(
4460             self, context, instance, migration, request_spec=None):
4461         """Finishes the second half of reverting a resize on the source host.
4462 
4463         Bring the original source instance state back (active/shutoff) and
4464         revert the resized attributes in the database.
4465 
4466         """
4467         with self._error_out_instance_on_exception(context, instance):
4468             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4469                 context, instance.uuid)
4470             self._notify_about_instance_usage(
4471                     context, instance, "resize.revert.start")
4472             compute_utils.notify_about_instance_action(context, instance,
4473                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4474                     phase=fields.NotificationPhase.START, bdms=bdms)
4475 
4476             # NOTE(mriedem): delete stashed old_vm_state information; we
4477             # default to ACTIVE for backwards compatibility if old_vm_state
4478             # is not set
4479             old_vm_state = instance.system_metadata.pop('old_vm_state',
4480                                                         vm_states.ACTIVE)
4481 
4482             self._set_instance_info(instance, instance.old_flavor)
4483             instance.old_flavor = None
4484             instance.new_flavor = None
4485             instance.host = migration.source_compute
4486             instance.node = migration.source_node
4487             instance.save()
4488 
4489             try:
4490                 source_allocations = self._revert_allocation(
4491                     context, instance, migration)
4492             except exception.AllocationMoveFailed:
4493                 LOG.error('Reverting allocation in placement for migration '
4494                           '%(migration_uuid)s failed. The instance '
4495                           '%(instance_uuid)s will be put into ERROR state but '
4496                           'the allocation held by the migration is leaked.',
4497                           {'instance_uuid': instance.uuid,
4498                            'migration_uuid': migration.uuid})
4499                 raise
4500 
4501             provider_mappings = self._fill_provider_mapping_based_on_allocs(
4502                 context, source_allocations, request_spec)
4503 
4504             self.network_api.setup_networks_on_host(context, instance,
4505                                                     migration.source_compute)
4506             self._finish_revert_resize_network_migrate_finish(
4507                 context, instance, migration, provider_mappings)
4508             network_info = self.network_api.get_instance_nw_info(context,
4509                                                                  instance)
4510 
4511             # revert_resize deleted any volume attachments for the instance
4512             # and created new ones to be used on this host, but we
4513             # have to update those attachments with the host connector so the
4514             # BDM.connection_info will get set in the call to
4515             # _get_instance_block_device_info below with refresh_conn_info=True
4516             # and then the volumes can be re-connected via the driver on this
4517             # host.
4518             self._update_volume_attachments(context, instance, bdms)
4519 
4520             block_device_info = self._get_instance_block_device_info(
4521                     context, instance, refresh_conn_info=True, bdms=bdms)
4522 
4523             power_on = old_vm_state != vm_states.STOPPED
4524             self.driver.finish_revert_migration(
4525                 context, instance, network_info, migration, block_device_info,
4526                 power_on)
4527 
4528             instance.drop_migration_context()
4529             instance.launched_at = timeutils.utcnow()
4530             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4531 
4532             # Complete any volume attachments so the volumes are in-use.
4533             self._complete_volume_attachments(context, bdms)
4534 
4535             # if the original vm state was STOPPED, set it back to STOPPED
4536             LOG.info("Updating instance to original state: '%s'",
4537                      old_vm_state, instance=instance)
4538             if power_on:
4539                 instance.vm_state = vm_states.ACTIVE
4540                 instance.task_state = None
4541                 instance.save()
4542             else:
4543                 instance.task_state = task_states.POWERING_OFF
4544                 instance.save()
4545                 self.stop_instance(context, instance=instance,
4546                                    clean_shutdown=True)
4547 
4548             self._notify_about_instance_usage(
4549                     context, instance, "resize.revert.end")
4550             compute_utils.notify_about_instance_action(context, instance,
4551                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4552                     phase=fields.NotificationPhase.END, bdms=bdms)
4553 
4554     def _fill_provider_mapping_based_on_allocs(
4555             self, context, allocations, request_spec):
4556         """Fills and returns the request group - resource provider mapping
4557         based on the allocation passed in.
4558 
4559         :param context: The security context
4560         :param allocation: allocation dict keyed by RP UUID.
4561         :param request_spec: The RequestSpec object associated with the
4562             operation
4563         :returns: None if the request_spec is None. Otherwise a mapping
4564             between RequestGroup requester_id, currently Neutron port_id,
4565             and a list of resource provider UUIDs providing resource for
4566             that RequestGroup.
4567         """
4568         if request_spec:
4569             # NOTE(gibi): We need to re-calculate the resource provider -
4570             # port mapping as we have to have the neutron ports allocate
4571             # from the source compute after revert.
4572             scheduler_utils.fill_provider_mapping_based_on_allocation(
4573                 context, self.reportclient, request_spec, allocations)
4574             provider_mappings = self._get_request_group_mapping(
4575                 request_spec)
4576         else:
4577             # NOTE(gibi): The compute RPC is pinned to be older than 5.2
4578             # and therefore request_spec is not sent. We cannot calculate
4579             # the provider mappings. If the instance has ports with
4580             # resource request then the port update will fail in
4581             # _update_port_binding_for_instance() called via
4582             # _finish_revert_resize_network_migrate_finish() in
4583             # finish_revert_resize.
4584             provider_mappings = None
4585         return provider_mappings
4586 
4587     def _revert_allocation(self, context, instance, migration):
4588         """Revert an allocation that is held by migration to our instance."""
4589 
4590         # Fetch the original allocation that the instance had on the source
4591         # node, which are now held by the migration
4592         orig_alloc = self.reportclient.get_allocations_for_consumer(
4593             context, migration.uuid)
4594         if not orig_alloc:
4595             LOG.error('Did not find resource allocations for migration '
4596                       '%s on source node %s. Unable to revert source node '
4597                       'allocations back to the instance.',
4598                       migration.uuid, migration.source_node, instance=instance)
4599             return False
4600 
4601         LOG.info('Swapping old allocation on %(rp_uuids)s held by migration '
4602                  '%(mig)s for instance',
4603                  {'rp_uuids': orig_alloc.keys(), 'mig': migration.uuid},
4604                  instance=instance)
4605         # FIXME(gibi): This method is flawed in that it does not handle
4606         # allocations against sharing providers in any special way. This leads
4607         # to duplicate allocations against the sharing provider during
4608         # migration.
4609         # TODO(cdent): Should we be doing anything with return values here?
4610         self.reportclient.move_allocations(context, migration.uuid,
4611                                            instance.uuid)
4612         return orig_alloc
4613 
4614     def _prep_resize(self, context, image, instance, instance_type,
4615                      filter_properties, node, migration, request_spec,
4616                      clean_shutdown=True):
4617 
4618         if not filter_properties:
4619             filter_properties = {}
4620 
4621         if not instance.host:
4622             self._set_instance_obj_error_state(context, instance)
4623             msg = _('Instance has no source host')
4624             raise exception.MigrationError(reason=msg)
4625 
4626         same_host = instance.host == self.host
4627         # if the flavor IDs match, it's migrate; otherwise resize
4628         if same_host and instance_type.id == instance['instance_type_id']:
4629             # check driver whether support migrate to same host
4630             if not self.driver.capabilities.get(
4631                     'supports_migrate_to_same_host', False):
4632                 # Raise InstanceFaultRollback so that the
4633                 # _error_out_instance_on_exception context manager in
4634                 # prep_resize will set the instance.vm_state properly.
4635                 raise exception.InstanceFaultRollback(
4636                     inner_exception=exception.UnableToMigrateToSelf(
4637                         instance_id=instance.uuid, host=self.host))
4638 
4639         # NOTE(danms): Stash the new instance_type to avoid having to
4640         # look it up in the database later
4641         instance.new_flavor = instance_type
4642         # NOTE(mriedem): Stash the old vm_state so we can set the
4643         # resized/reverted instance back to the same state later.
4644         vm_state = instance.vm_state
4645         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
4646         instance.system_metadata['old_vm_state'] = vm_state
4647         instance.save()
4648 
4649         if not isinstance(request_spec, objects.RequestSpec):
4650             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
4651             # version of the request spec to compute and since Stein compute
4652             # could be sending that back to conductor on reschedule, so if we
4653             # got a dict convert it to an object.
4654             # TODO(mriedem): We can drop this compat code when we only support
4655             # compute RPC API >=6.0.
4656             request_spec = objects.RequestSpec.from_primitives(
4657                 context, request_spec, filter_properties)
4658             # We don't have to set the new flavor on the request spec because
4659             # if we got here it was due to a reschedule from the compute and
4660             # the request spec would already have the new flavor in it from the
4661             # else block below.
4662 
4663         request_group_resource_providers_mapping = \
4664             self._get_request_group_mapping(request_spec)
4665 
4666         if request_group_resource_providers_mapping:
4667             self._update_pci_request_spec_with_allocated_interface_name(
4668                 context, instance, request_group_resource_providers_mapping)
4669 
4670         limits = filter_properties.get('limits', {})
4671         allocs = self.reportclient.get_allocations_for_consumer(
4672             context, instance.uuid)
4673         with self.rt.resize_claim(context, instance, instance_type, node,
4674                                   migration, allocs, image_meta=image,
4675                                   limits=limits) as claim:
4676             LOG.info('Migrating', instance=instance)
4677             # RPC cast to the source host to start the actual resize/migration.
4678             self.compute_rpcapi.resize_instance(
4679                     context, instance, claim.migration, image,
4680                     instance_type, request_spec, clean_shutdown)
4681 
4682     def _send_prep_resize_notifications(
4683             self, context, instance, phase, flavor):
4684         """Send "resize.prep.*" notifications.
4685 
4686         :param context: nova auth request context
4687         :param instance: The instance being resized
4688         :param phase: The phase of the action (NotificationPhase enum)
4689         :param flavor: The (new) flavor for the resize (same as existing
4690             instance.flavor for a cold migration)
4691         """
4692         # Only send notify_usage_exists if it's the "start" phase.
4693         if phase == fields.NotificationPhase.START:
4694             compute_utils.notify_usage_exists(
4695                 self.notifier, context, instance, self.host,
4696                 current_period=True)
4697 
4698         # Send extra usage info about the flavor if it's the "end" phase for
4699         # the legacy unversioned notification.
4700         extra_usage_info = None
4701         if phase == fields.NotificationPhase.END:
4702             extra_usage_info = dict(
4703                 new_instance_type=flavor.name,
4704                 new_instance_type_id=flavor.id)
4705         self._notify_about_instance_usage(
4706             context, instance, "resize.prep.%s" % phase,
4707             extra_usage_info=extra_usage_info)
4708 
4709         # Send the versioned notification.
4710         compute_utils.notify_about_resize_prep_instance(
4711             context, instance, self.host, phase, flavor)
4712 
4713     @wrap_exception()
4714     @reverts_task_state
4715     @wrap_instance_event(prefix='compute')
4716     @wrap_instance_fault
4717     def prep_resize(self, context, image, instance, instance_type,
4718                     request_spec, filter_properties, node,
4719                     clean_shutdown, migration, host_list):
4720         """Initiates the process of moving a running instance to another host.
4721 
4722         Possibly changes the VCPU, RAM and disk size in the process.
4723 
4724         This is initiated from conductor and runs on the destination host.
4725 
4726         The main purpose of this method is performing some checks on the
4727         destination host and making a claim for resources. If the claim fails
4728         then a reschedule to another host may be attempted which involves
4729         calling back to conductor to start the process over again.
4730         """
4731         if node is None:
4732             node = self._get_nodename(instance, refresh=True)
4733 
4734         # Pass instance_state=instance.vm_state because we can resize
4735         # a STOPPED server and we don't want to set it back to ACTIVE
4736         # in case _prep_resize fails.
4737         instance_state = instance.vm_state
4738         with self._error_out_instance_on_exception(
4739                 context, instance, instance_state=instance_state),\
4740                 errors_out_migration_ctxt(migration):
4741             self._send_prep_resize_notifications(
4742                 context, instance, fields.NotificationPhase.START,
4743                 instance_type)
4744             try:
4745                 self._prep_resize(context, image, instance,
4746                                   instance_type, filter_properties,
4747                                   node, migration, request_spec,
4748                                   clean_shutdown)
4749             except exception.BuildAbortException:
4750                 # NOTE(gibi): We failed
4751                 # _update_pci_request_spec_with_allocated_interface_name so
4752                 # there is no reason to re-schedule. Just revert the allocation
4753                 # and fail the migration.
4754                 with excutils.save_and_reraise_exception():
4755                     self._revert_allocation(context, instance, migration)
4756             except Exception:
4757                 # Since we hit a failure, we're either rescheduling or dead
4758                 # and either way we need to cleanup any allocations created
4759                 # by the scheduler for the destination node.
4760                 self._revert_allocation(context, instance, migration)
4761                 # try to re-schedule the resize elsewhere:
4762                 exc_info = sys.exc_info()
4763                 self._reschedule_resize_or_reraise(context, instance,
4764                         exc_info, instance_type, request_spec,
4765                         filter_properties, host_list)
4766             finally:
4767                 self._send_prep_resize_notifications(
4768                     context, instance, fields.NotificationPhase.END,
4769                     instance_type)
4770 
4771     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
4772             instance_type, request_spec, filter_properties, host_list):
4773         """Try to re-schedule the resize or re-raise the original error to
4774         error out the instance.
4775         """
4776         if not filter_properties:
4777             filter_properties = {}
4778 
4779         rescheduled = False
4780         instance_uuid = instance.uuid
4781 
4782         try:
4783             retry = filter_properties.get('retry')
4784             if retry:
4785                 LOG.debug('Rescheduling, attempt %d', retry['num_attempts'],
4786                           instance_uuid=instance_uuid)
4787 
4788                 # reset the task state
4789                 task_state = task_states.RESIZE_PREP
4790                 self._instance_update(context, instance, task_state=task_state)
4791 
4792                 if exc_info:
4793                     # stringify to avoid circular ref problem in json
4794                     # serialization
4795                     retry['exc'] = traceback.format_exception_only(
4796                         exc_info[0], exc_info[1])
4797 
4798                 scheduler_hint = {'filter_properties': filter_properties}
4799 
4800                 self.compute_task_api.resize_instance(
4801                     context, instance, scheduler_hint, instance_type,
4802                     request_spec=request_spec, host_list=host_list)
4803 
4804                 rescheduled = True
4805             else:
4806                 # no retry information, do not reschedule.
4807                 LOG.debug('Retry info not present, will not reschedule',
4808                           instance_uuid=instance_uuid)
4809                 rescheduled = False
4810         except Exception as error:
4811             rescheduled = False
4812             LOG.exception("Error trying to reschedule",
4813                           instance_uuid=instance_uuid)
4814             compute_utils.add_instance_fault_from_exc(context,
4815                     instance, error,
4816                     exc_info=sys.exc_info())
4817             self._notify_about_instance_usage(context, instance,
4818                     'resize.error', fault=error)
4819             compute_utils.notify_about_instance_action(
4820                 context, instance, self.host,
4821                 action=fields.NotificationAction.RESIZE,
4822                 phase=fields.NotificationPhase.ERROR,
4823                 exception=error,
4824                 tb=','.join(traceback.format_exception(*exc_info)))
4825 
4826         if rescheduled:
4827             self._log_original_error(exc_info, instance_uuid)
4828             compute_utils.add_instance_fault_from_exc(context,
4829                     instance, exc_info[1], exc_info=exc_info)
4830             self._notify_about_instance_usage(context, instance,
4831                     'resize.error', fault=exc_info[1])
4832             compute_utils.notify_about_instance_action(
4833                 context, instance, self.host,
4834                 action=fields.NotificationAction.RESIZE,
4835                 phase=fields.NotificationPhase.ERROR,
4836                 exception=exc_info[1],
4837                 tb=','.join(traceback.format_exception(*exc_info)))
4838         else:
4839             # not re-scheduling
4840             six.reraise(*exc_info)
4841 
4842     @messaging.expected_exceptions(exception.MigrationPreCheckError)
4843     @wrap_exception()
4844     @wrap_instance_event(prefix='compute')
4845     @wrap_instance_fault
4846     def prep_snapshot_based_resize_at_dest(
4847             self, ctxt, instance, flavor, nodename, migration, limits,
4848             request_spec):
4849         """Performs pre-cross-cell resize resource claim on the dest host.
4850 
4851         This runs on the destination host in a cross-cell resize operation
4852         before the resize is actually started.
4853 
4854         Performs a resize_claim for resources that are not claimed in placement
4855         like PCI devices and NUMA topology.
4856 
4857         Note that this is different from same-cell prep_resize in that this:
4858 
4859         * Does not RPC cast to the source compute, that is orchestrated from
4860           conductor.
4861         * This does not reschedule on failure, conductor handles that since
4862           conductor is synchronously RPC calling this method. As such, the
4863           reverts_task_state decorator is not used on this method.
4864 
4865         :param ctxt: user auth request context
4866         :param instance: the instance being resized
4867         :param flavor: the flavor being resized to (unchanged for cold migrate)
4868         :param nodename: Name of the target compute node
4869         :param migration: nova.objects.Migration object for the operation
4870         :param limits: nova.objects.SchedulerLimits object of resource limits
4871         :param request_spec: nova.objects.RequestSpec object for the operation
4872         :returns: nova.objects.MigrationContext; the migration context created
4873             on the destination host during the resize_claim.
4874         :raises: nova.exception.MigrationPreCheckError if the pre-check
4875             validation fails for the given host selection
4876         """
4877         LOG.debug('Checking if we can cross-cell migrate instance to this '
4878                   'host (%s).', self.host, instance=instance)
4879         self._send_prep_resize_notifications(
4880             ctxt, instance, fields.NotificationPhase.START, flavor)
4881         # TODO(mriedem): _update_pci_request_spec_with_allocated_interface_name
4882         # should be called here if the request spec has request group mappings,
4883         # e.g. for things like QoS ports with resource requests. Do it outside
4884         # the try/except so if it raises BuildAbortException we do not attempt
4885         # to reschedule.
4886         try:
4887             # Get the allocations within the try/except block in case we get
4888             # an error so MigrationPreCheckError is raised up.
4889             allocations = self.reportclient.get_allocs_for_consumer(
4890                 ctxt, instance.uuid)['allocations']
4891             # Claim resources on this target host using the new flavor which
4892             # will create the MigrationContext object. Note that in the future
4893             # if we want to do other validation here we should do it within
4894             # the MoveClaim context so we can drop the claim if anything fails.
4895             self.rt.resize_claim(
4896                 ctxt, instance, flavor, nodename, migration, allocations,
4897                 image_meta=instance.image_meta, limits=limits)
4898         except Exception as ex:
4899             err = six.text_type(ex)
4900             LOG.warning(
4901                 'Cross-cell resize pre-checks failed for this host (%s). '
4902                 'Cleaning up. Failure: %s', self.host, err,
4903                 instance=instance, exc_info=True)
4904             raise exception.MigrationPreCheckError(
4905                 reason=(_("Pre-checks failed on host '%(host)s'. "
4906                           "Error: %(error)s") %
4907                         {'host': self.host, 'error': err}))
4908         finally:
4909             self._send_prep_resize_notifications(
4910                 ctxt, instance, fields.NotificationPhase.END, flavor)
4911 
4912         # ResourceTracker.resize_claim() sets instance.migration_context.
4913         return instance.migration_context
4914 
4915     @messaging.expected_exceptions(exception.InstancePowerOffFailure)
4916     @wrap_exception()
4917     @reverts_task_state
4918     @wrap_instance_event(prefix='compute')
4919     @errors_out_migration
4920     @wrap_instance_fault
4921     def prep_snapshot_based_resize_at_source(
4922             self, ctxt, instance, migration, snapshot_id=None):
4923         """Prepares the instance at the source host for cross-cell resize
4924 
4925         Performs actions like powering off the guest, upload snapshot data if
4926         the instance is not volume-backed, disconnecting volumes, unplugging
4927         VIFs and activating the destination host port bindings.
4928 
4929         :param ctxt: user auth request context targeted at source cell
4930         :param instance: nova.objects.Instance; the instance being resized.
4931             The expected instance.task_state is "resize_migrating" when calling
4932             this method, and the expected task_state upon successful completion
4933             is "resize_migrated".
4934         :param migration: nova.objects.Migration object for the operation.
4935             The expected migration.status is "pre-migrating" when calling this
4936             method and the expected status upon successful completion is
4937             "post-migrating".
4938         :param snapshot_id: ID of the image snapshot to upload if not a
4939             volume-backed instance
4940         :raises: nova.exception.InstancePowerOffFailure if stopping the
4941             instance fails
4942         """
4943         # Note that if anything fails here, the migration-based allocations
4944         # created in conductor should be reverted by conductor as well,
4945         # see MigrationTask.rollback.
4946         self._prep_snapshot_based_resize_at_source(
4947             ctxt, instance, migration, snapshot_id=snapshot_id)
4948 
4949     @delete_image_on_error
4950     def _snapshot_for_resize(self, ctxt, image_id, instance):
4951         """Uploads snapshot for the instance during a snapshot-based resize
4952 
4953         If the snapshot operation fails the image will be deleted.
4954 
4955         :param ctxt: the nova auth request context for the resize operation
4956         :param image_id: the snapshot image ID
4957         :param instance: the instance to snapshot/resize
4958         """
4959         LOG.debug('Uploading snapshot data for image %s', image_id,
4960                   instance=instance)
4961         # Note that we do not track the snapshot phase task states
4962         # during resize since we do not want to reflect those into the
4963         # actual instance.task_state.
4964         update_task_state = lambda *args, **kwargs: None
4965         with timeutils.StopWatch() as timer:
4966             self.driver.snapshot(ctxt, instance, image_id, update_task_state)
4967             LOG.debug('Took %0.2f seconds to snapshot the instance on '
4968                       'the hypervisor.', timer.elapsed(), instance=instance)
4969 
4970     def _prep_snapshot_based_resize_at_source(
4971             self, ctxt, instance, migration, snapshot_id=None):
4972         """Private method for prep_snapshot_based_resize_at_source so calling
4973         code can handle errors and perform rollbacks as necessary.
4974         """
4975         # Fetch and update the instance.info_cache.
4976         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4977         # Get the BDMs attached to this instance on this source host.
4978         bdms = instance.get_bdms()
4979         # Send the resize.start notification.
4980         self._send_resize_instance_notifications(
4981             ctxt, instance, bdms, network_info, fields.NotificationPhase.START)
4982         # Update the migration status from "pre-migrating" to "migrating".
4983         migration.status = 'migrating'
4984         migration.save()
4985 
4986         # Since the instance is going to be left on the source host during the
4987         # resize, we need to power it off so we do not have the instance
4988         # potentially running in two places.
4989         LOG.debug('Stopping instance', instance=instance)
4990         try:
4991             self._power_off_instance(ctxt, instance)
4992         except Exception as e:
4993             LOG.exception('Failed to power off instance.', instance=instance)
4994             raise exception.InstancePowerOffFailure(reason=six.text_type(e))
4995         instance.power_state = self._get_power_state(ctxt, instance)
4996 
4997         # If a snapshot image ID was provided, we need to snapshot the guest
4998         # disk image and upload it to the image service.
4999         if snapshot_id:
5000             self._snapshot_for_resize(ctxt, snapshot_id, instance)
5001 
5002         block_device_info = self._get_instance_block_device_info(
5003             ctxt, instance, bdms=bdms)
5004 
5005         # If something fails at this point the instance must go to ERROR
5006         # status for operator intervention or to reboot/rebuild the instance.
5007         with self._error_out_instance_on_exception(
5008                 ctxt, instance, instance_state=vm_states.ERROR):
5009 
5010             # Destroy the guest on the source host which will disconnect
5011             # volumes and unplug VIFs. Note that we DO NOT destroy disks since
5012             # we want to leave those on the source host in case of a later
5013             # failure and disks are needed to recover the guest or in case the
5014             # resize is reverted.
5015             LOG.debug('Destroying guest on source host but retaining disks.',
5016                       instance=instance)
5017             self.driver.destroy(
5018                 ctxt, instance, network_info,
5019                 block_device_info=block_device_info, destroy_disks=False)
5020 
5021             # At this point the volumes are disconnected from this source host.
5022             # Delete the old volume attachment records and create new empty
5023             # ones which will be used later if the resize is reverted.
5024             LOG.debug('Deleting volume attachments for the source host.',
5025                       instance=instance)
5026             self._terminate_volume_connections(ctxt, instance, bdms)
5027 
5028             # At this point the VIFs are unplugged from this source host.
5029             # Activate the dest host port bindings created by conductor.
5030             self.network_api.migrate_instance_start(ctxt, instance, migration)
5031 
5032             # Update the migration status from "migrating" to "post-migrating".
5033             migration.status = 'post-migrating'
5034             migration.save()
5035 
5036             # At this point, the traditional resize_instance would update the
5037             # instance host/node values to point at the dest host/node because
5038             # that is where the disk is transferred during resize_instance, but
5039             # with cross-cell resize the instance is not yet at the dest host
5040             # so we do not make that update here.
5041             instance.task_state = task_states.RESIZE_MIGRATED
5042             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5043 
5044         self._send_resize_instance_notifications(
5045             ctxt, instance, bdms, network_info,
5046             fields.NotificationPhase.END)
5047         self.instance_events.clear_events_for_instance(instance)
5048 
5049     @wrap_exception()
5050     @reverts_task_state
5051     @wrap_instance_event(prefix='compute')
5052     @wrap_instance_fault
5053     def resize_instance(self, context, instance, image,
5054                         migration, instance_type, clean_shutdown,
5055                         request_spec=None):
5056         """Starts the migration of a running instance to another host.
5057 
5058         This is initiated from the destination host's ``prep_resize`` routine
5059         and runs on the source host.
5060         """
5061         try:
5062             self._resize_instance(context, instance, image, migration,
5063                                   instance_type, clean_shutdown, request_spec)
5064         except Exception:
5065             with excutils.save_and_reraise_exception():
5066                 self._revert_allocation(context, instance, migration)
5067 
5068     def _resize_instance(self, context, instance, image,
5069                          migration, instance_type, clean_shutdown,
5070                          request_spec):
5071         with self._error_out_instance_on_exception(context, instance), \
5072              errors_out_migration_ctxt(migration):
5073             network_info = self.network_api.get_instance_nw_info(context,
5074                                                                  instance)
5075 
5076             migration.status = 'migrating'
5077             migration.save()
5078 
5079             instance.task_state = task_states.RESIZE_MIGRATING
5080             instance.save(expected_task_state=task_states.RESIZE_PREP)
5081 
5082             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5083                     context, instance.uuid)
5084             self._send_resize_instance_notifications(
5085                 context, instance, bdms, network_info,
5086                 fields.NotificationPhase.START)
5087 
5088             block_device_info = self._get_instance_block_device_info(
5089                                 context, instance, bdms=bdms)
5090 
5091             timeout, retry_interval = self._get_power_off_values(context,
5092                                             instance, clean_shutdown)
5093             disk_info = self.driver.migrate_disk_and_power_off(
5094                     context, instance, migration.dest_host,
5095                     instance_type, network_info,
5096                     block_device_info,
5097                     timeout, retry_interval)
5098 
5099             self._terminate_volume_connections(context, instance, bdms)
5100 
5101             self.network_api.migrate_instance_start(context,
5102                                                     instance,
5103                                                     migration)
5104 
5105             migration.status = 'post-migrating'
5106             migration.save()
5107 
5108             instance.host = migration.dest_compute
5109             instance.node = migration.dest_node
5110             instance.task_state = task_states.RESIZE_MIGRATED
5111             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5112 
5113             # RPC cast to the destination host to finish the resize/migration.
5114             self.compute_rpcapi.finish_resize(context, instance,
5115                 migration, image, disk_info, migration.dest_compute,
5116                 request_spec)
5117 
5118         self._send_resize_instance_notifications(
5119             context, instance, bdms, network_info,
5120             fields.NotificationPhase.END)
5121         self.instance_events.clear_events_for_instance(instance)
5122 
5123     def _send_resize_instance_notifications(
5124             self, context, instance, bdms, network_info, phase):
5125         """Send "resize.(start|end)" notifications.
5126 
5127         :param context: nova auth request context
5128         :param instance: The instance being resized
5129         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5130             instance
5131         :param network_info: NetworkInfo for the instance info cache of ports
5132         :param phase: The phase of the action (NotificationPhase enum, either
5133             ``start`` or ``end``)
5134         """
5135         action = fields.NotificationAction.RESIZE
5136         # Send the legacy unversioned notification.
5137         self._notify_about_instance_usage(
5138             context, instance, "%s.%s" % (action, phase),
5139             network_info=network_info)
5140         # Send the versioned notification.
5141         compute_utils.notify_about_instance_action(
5142             context, instance, self.host, action=action, phase=phase,
5143             bdms=bdms)
5144 
5145     def _terminate_volume_connections(self, context, instance, bdms):
5146         connector = None
5147         for bdm in bdms:
5148             if bdm.is_volume:
5149                 if bdm.attachment_id:
5150                     # NOTE(jdg): So here's the thing, the idea behind the new
5151                     # attach API's was to have a new code fork/path that we
5152                     # followed, we're not going to do that so we have to do
5153                     # some extra work in here to make it *behave* just like the
5154                     # old code. Cinder doesn't allow disconnect/reconnect (you
5155                     # just delete the attachment and get a new one)
5156                     # attachments in the new attach code so we have to do
5157                     # a delete and create without a connector (reserve),
5158                     # in other words, beware
5159                     attachment_id = self.volume_api.attachment_create(
5160                         context, bdm.volume_id, instance.uuid)['id']
5161                     self.volume_api.attachment_delete(context,
5162                                                       bdm.attachment_id)
5163                     bdm.attachment_id = attachment_id
5164                     bdm.save()
5165 
5166                 else:
5167                     if connector is None:
5168                         connector = self.driver.get_volume_connector(instance)
5169                     self.volume_api.terminate_connection(context,
5170                                                          bdm.volume_id,
5171                                                          connector)
5172 
5173     @staticmethod
5174     def _set_instance_info(instance, instance_type):
5175         instance.instance_type_id = instance_type.id
5176         instance.memory_mb = instance_type.memory_mb
5177         instance.vcpus = instance_type.vcpus
5178         instance.root_gb = instance_type.root_gb
5179         instance.ephemeral_gb = instance_type.ephemeral_gb
5180         instance.flavor = instance_type
5181 
5182     def _update_volume_attachments(self, context, instance, bdms):
5183         """Updates volume attachments using the virt driver host connector.
5184 
5185         :param context: nova.context.RequestContext - user request context
5186         :param instance: nova.objects.Instance
5187         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5188                      device mappings for the given instance
5189         """
5190         if bdms:
5191             connector = None
5192             for bdm in bdms:
5193                 if bdm.is_volume and bdm.attachment_id:
5194                     if connector is None:
5195                         connector = self.driver.get_volume_connector(instance)
5196                     self.volume_api.attachment_update(
5197                         context, bdm.attachment_id, connector, bdm.device_name)
5198 
5199     def _complete_volume_attachments(self, context, bdms):
5200         """Completes volume attachments for the instance
5201 
5202         :param context: nova.context.RequestContext - user request context
5203         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5204                      device mappings for the given instance
5205         """
5206         if bdms:
5207             for bdm in bdms:
5208                 if bdm.is_volume and bdm.attachment_id:
5209                     self.volume_api.attachment_complete(
5210                         context, bdm.attachment_id)
5211 
5212     def _finish_resize(self, context, instance, migration, disk_info,
5213                        image_meta, bdms, request_spec):
5214         resize_instance = False  # indicates disks have been resized
5215         old_instance_type_id = migration['old_instance_type_id']
5216         new_instance_type_id = migration['new_instance_type_id']
5217         old_flavor = instance.flavor  # the current flavor is now old
5218         # NOTE(mriedem): Get the old_vm_state so we know if we should
5219         # power on the instance. If old_vm_state is not set we need to default
5220         # to ACTIVE for backwards compatibility
5221         old_vm_state = instance.system_metadata.get('old_vm_state',
5222                                                     vm_states.ACTIVE)
5223         instance.old_flavor = old_flavor
5224 
5225         if old_instance_type_id != new_instance_type_id:
5226             new_flavor = instance.new_flavor  # this is set in _prep_resize
5227             # Set the flavor-related fields on the instance object including
5228             # making instance.flavor = new_flavor.
5229             self._set_instance_info(instance, new_flavor)
5230             for key in ('root_gb', 'swap', 'ephemeral_gb'):
5231                 if old_flavor[key] != new_flavor[key]:
5232                     resize_instance = True
5233                     break
5234         instance.apply_migration_context()
5235 
5236         # NOTE(tr3buchet): setup networks on destination host
5237         self.network_api.setup_networks_on_host(context, instance,
5238                                                 migration.dest_compute)
5239         provider_mappings = self._get_request_group_mapping(request_spec)
5240 
5241         # For neutron, migrate_instance_finish updates port bindings for this
5242         # host including any PCI devices claimed for SR-IOV ports.
5243         self.network_api.migrate_instance_finish(
5244             context, instance, migration, provider_mappings)
5245 
5246         network_info = self.network_api.get_instance_nw_info(context, instance)
5247 
5248         instance.task_state = task_states.RESIZE_FINISH
5249         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5250 
5251         self._send_finish_resize_notifications(
5252             context, instance, bdms, network_info,
5253             fields.NotificationPhase.START)
5254 
5255         # We need to update any volume attachments using the destination
5256         # host connector so that we can update the BDM.connection_info
5257         # before calling driver.finish_migration otherwise the driver
5258         # won't know how to connect the volumes to this host.
5259         # Note that _get_instance_block_device_info with
5260         # refresh_conn_info=True will update the BDM.connection_info value
5261         # in the database so we must do this before calling that method.
5262         self._update_volume_attachments(context, instance, bdms)
5263 
5264         block_device_info = self._get_instance_block_device_info(
5265             context, instance, refresh_conn_info=True, bdms=bdms)
5266 
5267         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
5268         # automatically power on the instance after it's migrated
5269         power_on = old_vm_state != vm_states.STOPPED
5270 
5271         try:
5272             self.driver.finish_migration(context, migration, instance,
5273                                          disk_info,
5274                                          network_info,
5275                                          image_meta, resize_instance,
5276                                          block_device_info, power_on)
5277         except Exception:
5278             # Note that we do not rollback port bindings to the source host
5279             # because resize_instance (on the source host) updated the
5280             # instance.host to point to *this* host (the destination host)
5281             # so the port bindings pointing at this host are correct even
5282             # though we failed to create the guest.
5283             with excutils.save_and_reraise_exception():
5284                 # If we failed to create the guest on this host, reset the
5285                 # instance flavor-related fields to the old flavor. An
5286                 # error handler like reverts_task_state will save the changes.
5287                 if old_instance_type_id != new_instance_type_id:
5288                     self._set_instance_info(instance, old_flavor)
5289 
5290         # Now complete any volume attachments that were previously updated.
5291         self._complete_volume_attachments(context, bdms)
5292 
5293         migration.status = 'finished'
5294         migration.save()
5295 
5296         instance.vm_state = vm_states.RESIZED
5297         instance.task_state = None
5298         instance.launched_at = timeutils.utcnow()
5299         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5300 
5301         return network_info
5302 
5303     @wrap_exception()
5304     @reverts_task_state
5305     @wrap_instance_event(prefix='compute')
5306     @errors_out_migration
5307     @wrap_instance_fault
5308     def finish_resize(self, context, disk_info, image, instance,
5309                       migration, request_spec=None):
5310         """Completes the migration process.
5311 
5312         Sets up the newly transferred disk and turns on the instance at its
5313         new host machine.
5314 
5315         """
5316         try:
5317             self._finish_resize_helper(context, disk_info, image, instance,
5318                                        migration, request_spec)
5319         except Exception:
5320             with excutils.save_and_reraise_exception():
5321                 # At this point, resize_instance (which runs on the source) has
5322                 # already updated the instance host/node values to point to
5323                 # this (the dest) compute, so we need to leave the allocations
5324                 # against the dest node resource provider intact and drop the
5325                 # allocations against the source node resource provider. If the
5326                 # user tries to recover the server by hard rebooting it, it
5327                 # will happen on this host so that's where the allocations
5328                 # should go. Note that this is the same method called from
5329                 # confirm_resize to cleanup the source node allocations held
5330                 # by the migration record.
5331                 LOG.info('Deleting allocations for old flavor on source node '
5332                          '%s after finish_resize failure. You may be able to '
5333                          'recover the instance by hard rebooting it.',
5334                          migration.source_compute, instance=instance)
5335                 self._delete_allocation_after_move(
5336                     context, instance, migration)
5337 
5338     def _finish_resize_helper(self, context, disk_info, image, instance,
5339                               migration, request_spec):
5340         """Completes the migration process.
5341 
5342         The caller must revert the instance's allocations if the migration
5343         process failed.
5344         """
5345         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5346             context, instance.uuid)
5347 
5348         with self._error_out_instance_on_exception(context, instance):
5349             image_meta = objects.ImageMeta.from_dict(image)
5350             network_info = self._finish_resize(context, instance, migration,
5351                                                disk_info, image_meta, bdms,
5352                                                request_spec)
5353 
5354         # TODO(melwitt): We should clean up instance console tokens here. The
5355         # instance is on a new host and will need to establish a new console
5356         # connection.
5357         self._update_scheduler_instance_info(context, instance)
5358         self._send_finish_resize_notifications(
5359             context, instance, bdms, network_info,
5360             fields.NotificationPhase.END)
5361 
5362     def _send_finish_resize_notifications(
5363             self, context, instance, bdms, network_info, phase):
5364         """Send notifications for the finish_resize flow.
5365 
5366         :param context: nova auth request context
5367         :param instance: The instance being resized
5368         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5369             instance
5370         :param network_info: NetworkInfo for the instance info cache of ports
5371         :param phase: The phase of the action (NotificationPhase enum, either
5372             ``start`` or ``end``)
5373         """
5374         # Send the legacy unversioned notification.
5375         self._notify_about_instance_usage(
5376             context, instance, "finish_resize.%s" % phase,
5377             network_info=network_info)
5378         # Send the versioned notification.
5379         compute_utils.notify_about_instance_action(
5380             context, instance, self.host,
5381             action=fields.NotificationAction.RESIZE_FINISH, phase=phase,
5382             bdms=bdms)
5383 
5384     @wrap_exception()
5385     @reverts_task_state
5386     @wrap_instance_event(prefix='compute')
5387     @errors_out_migration
5388     @wrap_instance_fault
5389     def finish_snapshot_based_resize_at_dest(
5390             self, ctxt, instance, migration, snapshot_id, request_spec):
5391         """Finishes the snapshot-based resize at the destination compute.
5392 
5393         Sets up block devices and networking on the destination compute and
5394         spawns the guest.
5395 
5396         :param ctxt: nova auth request context targeted at the target cell DB
5397         :param instance: The Instance object being resized with the
5398             ``migration_context`` field set. Upon successful completion of this
5399             method the vm_state should be "resized", the task_state should be
5400             None, and migration context, host/node and flavor-related fields
5401             should be set on the instance.
5402         :param migration: The Migration object for this resize operation. Upon
5403             successful completion of this method the migration status should
5404             be "finished".
5405         :param snapshot_id: ID of the image snapshot created for a
5406             non-volume-backed instance, else None.
5407         :param request_spec: nova.objects.RequestSpec object for the operation
5408         """
5409         LOG.info('Finishing snapshot based resize on destination host %s.',
5410                  self.host, instance=instance)
5411         with self._error_out_instance_on_exception(ctxt, instance):
5412             # Note that if anything fails here, the migration-based allocations
5413             # created in conductor should be reverted by conductor as well,
5414             # see MigrationTask.rollback.
5415             self._finish_snapshot_based_resize_at_dest(
5416                 ctxt, instance, migration, snapshot_id)
5417 
5418     def _finish_snapshot_based_resize_at_dest(
5419             self, ctxt, instance, migration, snapshot_id):
5420         """Private variant of finish_snapshot_based_resize_at_dest so the
5421         caller can handle reverting resource allocations on failure and perform
5422         other generic error handling.
5423         """
5424         # Figure out the image metadata to use when spawning the guest.
5425         if snapshot_id:
5426             image_meta = objects.ImageMeta.from_image_ref(
5427                 ctxt, self.image_api, snapshot_id)
5428         else:
5429             # Just use what is already on the volume-backed instance.
5430             image_meta = instance.image_meta
5431 
5432         resize = migration.migration_type == 'resize'
5433         instance.old_flavor = instance.flavor
5434         if resize:
5435             flavor = instance.new_flavor
5436             # If we are resizing to a new flavor we need to set the
5437             # flavor-related fields on the instance.
5438             # NOTE(mriedem): This is likely where storing old/new_flavor on
5439             # the MigrationContext would make this cleaner.
5440             self._set_instance_info(instance, flavor)
5441 
5442         instance.apply_migration_context()
5443         instance.task_state = task_states.RESIZE_FINISH
5444         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5445 
5446         # This seems a bit late to be sending the start notification but
5447         # it is what traditional resize has always done as well and it does
5448         # contain the changes to the instance with the new_flavor and
5449         # task_state.
5450         bdms = instance.get_bdms()
5451         network_info = instance.get_network_info()
5452         self._send_finish_resize_notifications(
5453             ctxt, instance, bdms, network_info,
5454             fields.NotificationPhase.START)
5455 
5456         # Setup volumes and networking and spawn the guest in the hypervisor.
5457         self._finish_snapshot_based_resize_at_dest_spawn(
5458             ctxt, instance, migration, image_meta, bdms)
5459 
5460         # If we spawned from a temporary snapshot image we can delete that now,
5461         # similar to how unshelve works.
5462         if snapshot_id:
5463             # FIXME(mriedem): Need to deal with bug 1653953 for libvirt with
5464             # the rbd image backend. I think the cleanest thing we can do is
5465             # from the driver check to see if instance.migration_context is not
5466             # None and if so, get the Migration record for that context
5467             # (instance.migration_context.migration_id) and from that check the
5468             # Migration.cross_cell_move flag and if True, then flatten the
5469             # image.
5470             compute_utils.delete_image(
5471                 ctxt, instance, self.image_api, snapshot_id)
5472 
5473         migration.status = 'finished'
5474         migration.save()
5475 
5476         self._update_instance_after_spawn(
5477             ctxt, instance, vm_state=vm_states.RESIZED)
5478         # Setting the host/node values will make the ResourceTracker continue
5479         # to track usage for this instance on this host.
5480         instance.host = migration.dest_compute
5481         instance.node = migration.dest_node
5482         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5483 
5484         # Broadcast to all schedulers that the instance is on this host.
5485         self._update_scheduler_instance_info(ctxt, instance)
5486         self._send_finish_resize_notifications(
5487             ctxt, instance, bdms, network_info,
5488             fields.NotificationPhase.END)
5489 
5490     def _finish_snapshot_based_resize_at_dest_spawn(
5491             self, ctxt, instance, migration, image_meta, bdms):
5492         """Sets up volumes and networking and spawns the guest on the dest host
5493 
5494         If the instance was stopped when the resize was initiated the guest
5495         will be created but remain in a shutdown power state.
5496 
5497         If the spawn fails, port bindings are rolled back to the source host
5498         and volume connections are terminated for this dest host.
5499 
5500         :param ctxt: nova auth request context
5501         :param instance: Instance object being migrated
5502         :param migration: Migration object for the operation
5503         :param image_meta: ImageMeta object used during driver.spawn
5504         :param bdms: BlockDeviceMappingList of BDMs for the instance
5505         """
5506         # Update the volume attachments using this host's connector.
5507         # That will update the BlockDeviceMapping.connection_info which
5508         # will be used to connect the volumes on this host during spawn().
5509         block_device_info = self._prep_block_device(ctxt, instance, bdms)
5510 
5511         allocations = self.reportclient.get_allocations_for_consumer(
5512             ctxt, instance.uuid)
5513 
5514         # We do not call self.network_api.setup_networks_on_host here because
5515         # for neutron that sets up the port migration profile which is only
5516         # used during live migration with DVR. Yes it is gross knowing what
5517         # that method does internally. We could change this when bug 1814837
5518         # is fixed if setup_networks_on_host is made smarter by passing the
5519         # migration record and the method checks the migration_type.
5520 
5521         # Activate the port bindings for this host.
5522         # FIXME(mriedem): We're going to have the same issue as bug 1813789
5523         # here because this will update the port bindings and send the
5524         # network-vif-plugged event and that means when driver.spawn waits for
5525         # it we might have already gotten the event and neutron won't send
5526         # another one so we could timeout.
5527         # TODO(mriedem): Calculate provider mappings when we support cross-cell
5528         # resize/migrate with ports having resource requests.
5529         self.network_api.migrate_instance_finish(
5530             ctxt, instance, migration, provider_mappings=None)
5531         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5532 
5533         # If the original vm_state was STOPPED, we do not automatically
5534         # power on the instance after it is migrated.
5535         power_on = instance.system_metadata['old_vm_state'] == vm_states.ACTIVE
5536         try:
5537             # NOTE(mriedem): If this instance uses a config drive, it will get
5538             # rebuilt here which means any personality files will be lost,
5539             # similar to unshelve. If the instance is not using a config drive
5540             # and getting metadata from the metadata API service, personality
5541             # files would be lost regardless of the move operation.
5542             self.driver.spawn(
5543                 ctxt, instance, image_meta, injected_files=[],
5544                 admin_password=None, allocations=allocations,
5545                 network_info=network_info, block_device_info=block_device_info,
5546                 power_on=power_on)
5547         except Exception:
5548             with excutils.save_and_reraise_exception(logger=LOG):
5549                 # Rollback port bindings to the source host.
5550                 try:
5551                     # This is gross but migrate_instance_start looks at the
5552                     # migration.dest_compute to determine where to activate the
5553                     # port bindings and we want the source compute port
5554                     # bindings to be re-activated. Remember at this point the
5555                     # instance.host is still pointing at the source compute.
5556                     # TODO(mriedem): Maybe we should be calling
5557                     # setup_instance_network_on_host here to deal with pci
5558                     # devices?
5559                     with utils.temporary_mutation(
5560                             migration, dest_compute=migration.source_compute):
5561                         self.network_api.migrate_instance_start(
5562                             ctxt, instance, migration)
5563                 except Exception:
5564                     LOG.exception(
5565                         'Failed to activate port bindings on the source '
5566                         'host: %s', migration.source_compute,
5567                         instance=instance)
5568 
5569                 # Rollback volume connections on this host.
5570                 for bdm in bdms:
5571                     if bdm.is_volume:
5572                         try:
5573                             self._remove_volume_connection(
5574                                 ctxt, bdm, instance, delete_attachment=True)
5575                         except Exception:
5576                             LOG.exception('Failed to remove volume connection '
5577                                           'on this host %s for volume %s.',
5578                                           self.host, bdm.volume_id,
5579                                           instance=instance)
5580 
5581     @wrap_exception()
5582     @wrap_instance_fault
5583     def add_fixed_ip_to_instance(self, context, network_id, instance):
5584         """Calls network_api to add new fixed_ip to instance
5585         then injects the new network info and resets instance networking.
5586 
5587         """
5588         self._notify_about_instance_usage(
5589                 context, instance, "create_ip.start")
5590 
5591         network_info = self.network_api.add_fixed_ip_to_instance(context,
5592                                                                  instance,
5593                                                                  network_id)
5594         self._inject_network_info(context, instance, network_info)
5595         self.reset_network(context, instance)
5596 
5597         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
5598         instance.updated_at = timeutils.utcnow()
5599         instance.save()
5600 
5601         self._notify_about_instance_usage(
5602             context, instance, "create_ip.end", network_info=network_info)
5603 
5604     @wrap_exception()
5605     @wrap_instance_fault
5606     def remove_fixed_ip_from_instance(self, context, address, instance):
5607         """Calls network_api to remove existing fixed_ip from instance
5608         by injecting the altered network info and resetting
5609         instance networking.
5610         """
5611         self._notify_about_instance_usage(
5612                 context, instance, "delete_ip.start")
5613 
5614         network_info = self.network_api.remove_fixed_ip_from_instance(context,
5615                                                                       instance,
5616                                                                       address)
5617         self._inject_network_info(context, instance, network_info)
5618         self.reset_network(context, instance)
5619 
5620         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
5621         instance.updated_at = timeutils.utcnow()
5622         instance.save()
5623 
5624         self._notify_about_instance_usage(
5625             context, instance, "delete_ip.end", network_info=network_info)
5626 
5627     @wrap_exception()
5628     @reverts_task_state
5629     @wrap_instance_event(prefix='compute')
5630     @wrap_instance_fault
5631     def pause_instance(self, context, instance):
5632         """Pause an instance on this host."""
5633         context = context.elevated()
5634         LOG.info('Pausing', instance=instance)
5635         self._notify_about_instance_usage(context, instance, 'pause.start')
5636         compute_utils.notify_about_instance_action(context, instance,
5637                self.host, action=fields.NotificationAction.PAUSE,
5638                phase=fields.NotificationPhase.START)
5639         self.driver.pause(instance)
5640         instance.power_state = self._get_power_state(context, instance)
5641         instance.vm_state = vm_states.PAUSED
5642         instance.task_state = None
5643         instance.save(expected_task_state=task_states.PAUSING)
5644         self._notify_about_instance_usage(context, instance, 'pause.end')
5645         compute_utils.notify_about_instance_action(context, instance,
5646                self.host, action=fields.NotificationAction.PAUSE,
5647                phase=fields.NotificationPhase.END)
5648 
5649     @wrap_exception()
5650     @reverts_task_state
5651     @wrap_instance_event(prefix='compute')
5652     @wrap_instance_fault
5653     def unpause_instance(self, context, instance):
5654         """Unpause a paused instance on this host."""
5655         context = context.elevated()
5656         LOG.info('Unpausing', instance=instance)
5657         self._notify_about_instance_usage(context, instance, 'unpause.start')
5658         compute_utils.notify_about_instance_action(context, instance,
5659             self.host, action=fields.NotificationAction.UNPAUSE,
5660             phase=fields.NotificationPhase.START)
5661         self.driver.unpause(instance)
5662         instance.power_state = self._get_power_state(context, instance)
5663         instance.vm_state = vm_states.ACTIVE
5664         instance.task_state = None
5665         instance.save(expected_task_state=task_states.UNPAUSING)
5666         self._notify_about_instance_usage(context, instance, 'unpause.end')
5667         compute_utils.notify_about_instance_action(context, instance,
5668             self.host, action=fields.NotificationAction.UNPAUSE,
5669             phase=fields.NotificationPhase.END)
5670 
5671     @wrap_exception()
5672     def host_power_action(self, context, action):
5673         """Reboots, shuts down or powers up the host."""
5674         return self.driver.host_power_action(action)
5675 
5676     @wrap_exception()
5677     def host_maintenance_mode(self, context, host, mode):
5678         """Start/Stop host maintenance window. On start, it triggers
5679         guest VMs evacuation.
5680         """
5681         return self.driver.host_maintenance_mode(host, mode)
5682 
5683     def _update_compute_provider_status(self, context, enabled):
5684         """Adds or removes the COMPUTE_STATUS_DISABLED trait for this host.
5685 
5686         For each ComputeNode managed by this service, adds or removes the
5687         COMPUTE_STATUS_DISABLED traits to/from the associated resource provider
5688         in Placement.
5689 
5690         :param context: nova auth RequestContext
5691         :param enabled: True if the node is enabled in which case the trait
5692             would be removed, False if the node is disabled in which case
5693             the trait would be added.
5694         :raises: ComputeHostNotFound if there are no compute nodes found in
5695             the ResourceTracker for this service.
5696         """
5697         # Get the compute node(s) on this host. Remember that ironic can be
5698         # managing more than one compute node.
5699         nodes = self.rt.compute_nodes.values()
5700         if not nodes:
5701             raise exception.ComputeHostNotFound(host=self.host)
5702         # For each node, we want to add (or remove) the COMPUTE_STATUS_DISABLED
5703         # trait on the related resource provider in placement so the scheduler
5704         # (pre-)filters the provider based on its status.
5705         for node in nodes:
5706             try:
5707                 self.virtapi.update_compute_provider_status(
5708                     context, node.uuid, enabled)
5709             except (exception.ResourceProviderTraitRetrievalFailed,
5710                     exception.ResourceProviderUpdateConflict,
5711                     exception.ResourceProviderUpdateFailed,
5712                     exception.TraitRetrievalFailed) as e:
5713                 # This is best effort so just log a warning and continue.
5714                 LOG.warning('An error occurred while updating '
5715                             'COMPUTE_STATUS_DISABLED trait on compute node '
5716                             'resource provider %s. The trait will be '
5717                             'synchronized when the update_available_resource '
5718                             'periodic task runs. Error: %s',
5719                             node.uuid, e.format_message())
5720             except Exception:
5721                 LOG.exception('An error occurred while updating '
5722                               'COMPUTE_STATUS_DISABLED trait on compute node '
5723                               'resource provider %s. The trait will be '
5724                               'synchronized when the '
5725                               'update_available_resource periodic task runs.',
5726                               node.uuid)
5727 
5728     @wrap_exception()
5729     def set_host_enabled(self, context, enabled):
5730         """Sets the specified host's ability to accept new instances.
5731 
5732         This method will add or remove the COMPUTE_STATUS_DISABLED trait
5733         to/from the associated compute node resource provider(s) for this
5734         compute service.
5735         """
5736         try:
5737             self._update_compute_provider_status(context, enabled)
5738         except exception.ComputeHostNotFound:
5739             LOG.warning('Unable to add/remove trait COMPUTE_STATUS_DISABLED. '
5740                         'No ComputeNode(s) found for host: %s', self.host)
5741 
5742         try:
5743             return self.driver.set_host_enabled(enabled)
5744         except NotImplementedError:
5745             # Only the xenapi driver implements set_host_enabled but we don't
5746             # want NotImplementedError to get raised back to the API. We still
5747             # need to honor the compute RPC API contract and return 'enabled'
5748             # or 'disabled' though.
5749             return 'enabled' if enabled else 'disabled'
5750 
5751     @wrap_exception()
5752     def get_host_uptime(self, context):
5753         """Returns the result of calling "uptime" on the target host."""
5754         return self.driver.get_host_uptime()
5755 
5756     @wrap_exception()
5757     @wrap_instance_fault
5758     def get_diagnostics(self, context, instance):
5759         """Retrieve diagnostics for an instance on this host."""
5760         current_power_state = self._get_power_state(context, instance)
5761         if current_power_state == power_state.RUNNING:
5762             LOG.info("Retrieving diagnostics", instance=instance)
5763             return self.driver.get_diagnostics(instance)
5764         else:
5765             raise exception.InstanceInvalidState(
5766                 attr='power state',
5767                 instance_uuid=instance.uuid,
5768                 state=power_state.STATE_MAP[instance.power_state],
5769                 method='get_diagnostics')
5770 
5771     @wrap_exception()
5772     @wrap_instance_fault
5773     def get_instance_diagnostics(self, context, instance):
5774         """Retrieve diagnostics for an instance on this host."""
5775         current_power_state = self._get_power_state(context, instance)
5776         if current_power_state == power_state.RUNNING:
5777             LOG.info("Retrieving diagnostics", instance=instance)
5778             return self.driver.get_instance_diagnostics(instance)
5779         else:
5780             raise exception.InstanceInvalidState(
5781                 attr='power state',
5782                 instance_uuid=instance.uuid,
5783                 state=power_state.STATE_MAP[instance.power_state],
5784                 method='get_diagnostics')
5785 
5786     @wrap_exception()
5787     @reverts_task_state
5788     @wrap_instance_event(prefix='compute')
5789     @wrap_instance_fault
5790     def suspend_instance(self, context, instance):
5791         """Suspend the given instance."""
5792         context = context.elevated()
5793 
5794         # Store the old state
5795         instance.system_metadata['old_vm_state'] = instance.vm_state
5796         self._notify_about_instance_usage(context, instance, 'suspend.start')
5797         compute_utils.notify_about_instance_action(context, instance,
5798                 self.host, action=fields.NotificationAction.SUSPEND,
5799                 phase=fields.NotificationPhase.START)
5800         with self._error_out_instance_on_exception(context, instance,
5801              instance_state=instance.vm_state):
5802             self.driver.suspend(context, instance)
5803         instance.power_state = self._get_power_state(context, instance)
5804         instance.vm_state = vm_states.SUSPENDED
5805         instance.task_state = None
5806         instance.save(expected_task_state=task_states.SUSPENDING)
5807         self._notify_about_instance_usage(context, instance, 'suspend.end')
5808         compute_utils.notify_about_instance_action(context, instance,
5809                 self.host, action=fields.NotificationAction.SUSPEND,
5810                 phase=fields.NotificationPhase.END)
5811 
5812     @wrap_exception()
5813     @reverts_task_state
5814     @wrap_instance_event(prefix='compute')
5815     @wrap_instance_fault
5816     def resume_instance(self, context, instance):
5817         """Resume the given suspended instance."""
5818         context = context.elevated()
5819         LOG.info('Resuming', instance=instance)
5820 
5821         self._notify_about_instance_usage(context, instance, 'resume.start')
5822 
5823         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5824             context, instance.uuid)
5825         block_device_info = self._get_instance_block_device_info(
5826             context, instance, bdms=bdms)
5827 
5828         compute_utils.notify_about_instance_action(context, instance,
5829             self.host, action=fields.NotificationAction.RESUME,
5830             phase=fields.NotificationPhase.START, bdms=bdms)
5831 
5832         network_info = self.network_api.get_instance_nw_info(context, instance)
5833 
5834         with self._error_out_instance_on_exception(context, instance,
5835              instance_state=instance.vm_state):
5836             self.driver.resume(context, instance, network_info,
5837                                block_device_info)
5838 
5839         instance.power_state = self._get_power_state(context, instance)
5840 
5841         # We default to the ACTIVE state for backwards compatibility
5842         instance.vm_state = instance.system_metadata.pop('old_vm_state',
5843                                                          vm_states.ACTIVE)
5844 
5845         instance.task_state = None
5846         instance.save(expected_task_state=task_states.RESUMING)
5847         self._notify_about_instance_usage(context, instance, 'resume.end')
5848         compute_utils.notify_about_instance_action(context, instance,
5849             self.host, action=fields.NotificationAction.RESUME,
5850             phase=fields.NotificationPhase.END, bdms=bdms)
5851 
5852     @wrap_exception()
5853     @reverts_task_state
5854     @wrap_instance_event(prefix='compute')
5855     @wrap_instance_fault
5856     def shelve_instance(self, context, instance, image_id,
5857                         clean_shutdown):
5858         """Shelve an instance.
5859 
5860         This should be used when you want to take a snapshot of the instance.
5861         It also adds system_metadata that can be used by a periodic task to
5862         offload the shelved instance after a period of time.
5863 
5864         :param context: request context
5865         :param instance: an Instance object
5866         :param image_id: an image id to snapshot to.
5867         :param clean_shutdown: give the GuestOS a chance to stop
5868         """
5869 
5870         @utils.synchronized(instance.uuid)
5871         def do_shelve_instance():
5872             self._shelve_instance(context, instance, image_id, clean_shutdown)
5873         do_shelve_instance()
5874 
5875     def _shelve_instance(self, context, instance, image_id,
5876                          clean_shutdown):
5877         LOG.info('Shelving', instance=instance)
5878         offload = CONF.shelved_offload_time == 0
5879         if offload:
5880             # Get the BDMs early so we can pass them into versioned
5881             # notifications since _shelve_offload_instance needs the
5882             # BDMs anyway.
5883             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5884                 context, instance.uuid)
5885         else:
5886             bdms = None
5887         compute_utils.notify_usage_exists(self.notifier, context, instance,
5888                                           self.host, current_period=True)
5889         self._notify_about_instance_usage(context, instance, 'shelve.start')
5890         compute_utils.notify_about_instance_action(context, instance,
5891                 self.host, action=fields.NotificationAction.SHELVE,
5892                 phase=fields.NotificationPhase.START, bdms=bdms)
5893 
5894         def update_task_state(task_state, expected_state=task_states.SHELVING):
5895             shelving_state_map = {
5896                     task_states.IMAGE_PENDING_UPLOAD:
5897                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
5898                     task_states.IMAGE_UPLOADING:
5899                         task_states.SHELVING_IMAGE_UPLOADING,
5900                     task_states.SHELVING: task_states.SHELVING}
5901             task_state = shelving_state_map[task_state]
5902             expected_state = shelving_state_map[expected_state]
5903             instance.task_state = task_state
5904             instance.save(expected_task_state=expected_state)
5905         # Do not attempt a clean shutdown of a paused guest since some
5906         # hypervisors will fail the clean shutdown if the guest is not
5907         # running.
5908         if instance.power_state == power_state.PAUSED:
5909             clean_shutdown = False
5910         self._power_off_instance(context, instance, clean_shutdown)
5911         self.driver.snapshot(context, instance, image_id, update_task_state)
5912 
5913         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
5914         instance.system_metadata['shelved_image_id'] = image_id
5915         instance.system_metadata['shelved_host'] = self.host
5916         instance.vm_state = vm_states.SHELVED
5917         instance.task_state = None
5918         if CONF.shelved_offload_time == 0:
5919             instance.task_state = task_states.SHELVING_OFFLOADING
5920         instance.power_state = self._get_power_state(context, instance)
5921         instance.save(expected_task_state=[
5922                 task_states.SHELVING,
5923                 task_states.SHELVING_IMAGE_UPLOADING])
5924 
5925         self._notify_about_instance_usage(context, instance, 'shelve.end')
5926         compute_utils.notify_about_instance_action(context, instance,
5927                 self.host, action=fields.NotificationAction.SHELVE,
5928                 phase=fields.NotificationPhase.END, bdms=bdms)
5929 
5930         if offload:
5931             self._shelve_offload_instance(context, instance,
5932                                           clean_shutdown=False, bdms=bdms)
5933 
5934     @wrap_exception()
5935     @reverts_task_state
5936     @wrap_instance_event(prefix='compute')
5937     @wrap_instance_fault
5938     def shelve_offload_instance(self, context, instance, clean_shutdown):
5939         """Remove a shelved instance from the hypervisor.
5940 
5941         This frees up those resources for use by other instances, but may lead
5942         to slower unshelve times for this instance.  This method is used by
5943         volume backed instances since restoring them doesn't involve the
5944         potentially large download of an image.
5945 
5946         :param context: request context
5947         :param instance: nova.objects.instance.Instance
5948         :param clean_shutdown: give the GuestOS a chance to stop
5949         """
5950 
5951         @utils.synchronized(instance.uuid)
5952         def do_shelve_offload_instance():
5953             self._shelve_offload_instance(context, instance, clean_shutdown)
5954         do_shelve_offload_instance()
5955 
5956     def _shelve_offload_instance(self, context, instance, clean_shutdown,
5957                                  bdms=None):
5958         LOG.info('Shelve offloading', instance=instance)
5959         if bdms is None:
5960             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5961                 context, instance.uuid)
5962         self._notify_about_instance_usage(context, instance,
5963                 'shelve_offload.start')
5964         compute_utils.notify_about_instance_action(context, instance,
5965                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
5966                 phase=fields.NotificationPhase.START, bdms=bdms)
5967 
5968         self._power_off_instance(context, instance, clean_shutdown)
5969         current_power_state = self._get_power_state(context, instance)
5970 
5971         self.network_api.cleanup_instance_network_on_host(context, instance,
5972                                                           instance.host)
5973         network_info = self.network_api.get_instance_nw_info(context, instance)
5974 
5975         block_device_info = self._get_instance_block_device_info(context,
5976                                                                  instance,
5977                                                                  bdms=bdms)
5978         self.driver.destroy(context, instance, network_info,
5979                 block_device_info)
5980 
5981         # the instance is going to be removed from the host so we want to
5982         # terminate all the connections with the volume server and the host
5983         self._terminate_volume_connections(context, instance, bdms)
5984 
5985         # Free up the resource allocations in the placement service.
5986         # This should happen *before* the vm_state is changed to
5987         # SHELVED_OFFLOADED in case client-side code is polling the API to
5988         # schedule more instances (or unshelve) once this server is offloaded.
5989         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
5990                                                                 instance)
5991 
5992         instance.power_state = current_power_state
5993         # NOTE(mriedem): The vm_state has to be set before updating the
5994         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
5995         # values cannot be nulled out until after updating the resource tracker
5996         # though.
5997         instance.vm_state = vm_states.SHELVED_OFFLOADED
5998         instance.task_state = None
5999         instance.save(expected_task_state=[task_states.SHELVING,
6000                                            task_states.SHELVING_OFFLOADING])
6001 
6002         # NOTE(ndipanov): Free resources from the resource tracker
6003         self._update_resource_tracker(context, instance)
6004 
6005         # NOTE(sfinucan): RPC calls should no longer be attempted against this
6006         # instance, so ensure any calls result in errors
6007         self._nil_out_instance_obj_host_and_node(instance)
6008         instance.save(expected_task_state=None)
6009 
6010         # TODO(melwitt): We should clean up instance console tokens here. The
6011         # instance has no host at this point and will need to establish a new
6012         # console connection in the future after it is unshelved.
6013         self._delete_scheduler_instance_info(context, instance.uuid)
6014         self._notify_about_instance_usage(context, instance,
6015                 'shelve_offload.end')
6016         compute_utils.notify_about_instance_action(context, instance,
6017                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6018                 phase=fields.NotificationPhase.END, bdms=bdms)
6019 
6020     @wrap_exception()
6021     @reverts_task_state
6022     @wrap_instance_event(prefix='compute')
6023     @wrap_instance_fault
6024     def unshelve_instance(self, context, instance, image,
6025                           filter_properties, node, request_spec=None):
6026         """Unshelve the instance.
6027 
6028         :param context: request context
6029         :param instance: a nova.objects.instance.Instance object
6030         :param image: an image to build from.  If None we assume a
6031             volume backed instance.
6032         :param filter_properties: dict containing limits, retry info etc.
6033         :param node: target compute node
6034         :param request_spec: the RequestSpec object used to schedule the
6035             instance
6036         """
6037         if filter_properties is None:
6038             filter_properties = {}
6039 
6040         @utils.synchronized(instance.uuid)
6041         def do_unshelve_instance():
6042             self._unshelve_instance(context, instance, image,
6043                                     filter_properties, node)
6044         do_unshelve_instance()
6045 
6046     def _unshelve_instance_key_scrub(self, instance):
6047         """Remove data from the instance that may cause side effects."""
6048         cleaned_keys = dict(
6049                 key_data=instance.key_data,
6050                 auto_disk_config=instance.auto_disk_config)
6051         instance.key_data = None
6052         instance.auto_disk_config = False
6053         return cleaned_keys
6054 
6055     def _unshelve_instance_key_restore(self, instance, keys):
6056         """Restore previously scrubbed keys before saving the instance."""
6057         instance.update(keys)
6058 
6059     def _unshelve_instance(self, context, instance, image, filter_properties,
6060                            node):
6061         LOG.info('Unshelving', instance=instance)
6062         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6063                 context, instance.uuid)
6064 
6065         self._notify_about_instance_usage(context, instance, 'unshelve.start')
6066         compute_utils.notify_about_instance_action(context, instance,
6067                 self.host, action=fields.NotificationAction.UNSHELVE,
6068                 phase=fields.NotificationPhase.START, bdms=bdms)
6069 
6070         instance.task_state = task_states.SPAWNING
6071         instance.save()
6072 
6073         block_device_info = self._prep_block_device(context, instance, bdms)
6074         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
6075 
6076         if node is None:
6077             node = self._get_nodename(instance)
6078 
6079         limits = filter_properties.get('limits', {})
6080 
6081         allocations = self.reportclient.get_allocations_for_consumer(
6082             context, instance.uuid)
6083 
6084         shelved_image_ref = instance.image_ref
6085         if image:
6086             instance.image_ref = image['id']
6087             image_meta = objects.ImageMeta.from_dict(image)
6088         else:
6089             image_meta = objects.ImageMeta.from_dict(
6090                 utils.get_image_from_system_metadata(
6091                     instance.system_metadata))
6092 
6093         self.network_api.setup_instance_network_on_host(context, instance,
6094                                                         self.host)
6095         network_info = self.network_api.get_instance_nw_info(context, instance)
6096         try:
6097             with self.rt.instance_claim(context, instance, node, allocations,
6098                                         limits):
6099                 self.driver.spawn(context, instance, image_meta,
6100                                   injected_files=[],
6101                                   admin_password=None,
6102                                   allocations=allocations,
6103                                   network_info=network_info,
6104                                   block_device_info=block_device_info)
6105         except Exception:
6106             with excutils.save_and_reraise_exception(logger=LOG):
6107                 LOG.exception('Instance failed to spawn',
6108                               instance=instance)
6109                 # Cleanup allocations created by the scheduler on this host
6110                 # since we failed to spawn the instance. We do this both if
6111                 # the instance claim failed with ComputeResourcesUnavailable
6112                 # or if we did claim but the spawn failed, because aborting the
6113                 # instance claim will not remove the allocations.
6114                 self.reportclient.delete_allocation_for_instance(context,
6115                                                                  instance.uuid)
6116                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
6117                 self._terminate_volume_connections(context, instance, bdms)
6118                 # The reverts_task_state decorator on unshelve_instance will
6119                 # eventually save these updates.
6120                 self._nil_out_instance_obj_host_and_node(instance)
6121 
6122         if image:
6123             instance.image_ref = shelved_image_ref
6124             self._delete_snapshot_of_shelved_instance(context, instance,
6125                                                       image['id'])
6126 
6127         self._unshelve_instance_key_restore(instance, scrubbed_keys)
6128         self._update_instance_after_spawn(context, instance)
6129         # Delete system_metadata for a shelved instance
6130         compute_utils.remove_shelved_keys_from_system_metadata(instance)
6131 
6132         instance.save(expected_task_state=task_states.SPAWNING)
6133         self._update_scheduler_instance_info(context, instance)
6134         self._notify_about_instance_usage(context, instance, 'unshelve.end')
6135         compute_utils.notify_about_instance_action(context, instance,
6136                 self.host, action=fields.NotificationAction.UNSHELVE,
6137                 phase=fields.NotificationPhase.END, bdms=bdms)
6138 
6139     @messaging.expected_exceptions(NotImplementedError)
6140     @wrap_instance_fault
6141     def reset_network(self, context, instance):
6142         """Reset networking on the given instance."""
6143         LOG.debug('Reset network', instance=instance)
6144         self.driver.reset_network(instance)
6145 
6146     def _inject_network_info(self, context, instance, network_info):
6147         """Inject network info for the given instance."""
6148         LOG.debug('Inject network info', instance=instance)
6149         LOG.debug('network_info to inject: |%s|', network_info,
6150                   instance=instance)
6151 
6152         self.driver.inject_network_info(instance,
6153                                         network_info)
6154 
6155     @wrap_instance_fault
6156     def inject_network_info(self, context, instance):
6157         """Inject network info, but don't return the info."""
6158         network_info = self.network_api.get_instance_nw_info(context, instance)
6159         self._inject_network_info(context, instance, network_info)
6160 
6161     @messaging.expected_exceptions(NotImplementedError,
6162                                    exception.ConsoleNotAvailable,
6163                                    exception.InstanceNotFound)
6164     @wrap_exception()
6165     @wrap_instance_fault
6166     def get_console_output(self, context, instance, tail_length):
6167         """Send the console output for the given instance."""
6168         context = context.elevated()
6169         LOG.info("Get console output", instance=instance)
6170         output = self.driver.get_console_output(context, instance)
6171 
6172         if type(output) is six.text_type:
6173             output = six.b(output)
6174 
6175         if tail_length is not None:
6176             output = self._tail_log(output, tail_length)
6177 
6178         return output.decode('ascii', 'replace')
6179 
6180     def _tail_log(self, log, length):
6181         try:
6182             length = int(length)
6183         except ValueError:
6184             length = 0
6185 
6186         if length == 0:
6187             return b''
6188         else:
6189             return b'\n'.join(log.split(b'\n')[-int(length):])
6190 
6191     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6192                                    exception.InstanceNotReady,
6193                                    exception.InstanceNotFound,
6194                                    exception.ConsoleTypeUnavailable,
6195                                    NotImplementedError)
6196     @wrap_exception()
6197     @wrap_instance_fault
6198     def get_vnc_console(self, context, console_type, instance):
6199         """Return connection information for a vnc console."""
6200         context = context.elevated()
6201         LOG.debug("Getting vnc console", instance=instance)
6202 
6203         if not CONF.vnc.enabled:
6204             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6205 
6206         if console_type == 'novnc':
6207             # For essex, novncproxy_base_url must include the full path
6208             # including the html file (like http://myhost/vnc_auto.html)
6209             access_url_base = CONF.vnc.novncproxy_base_url
6210         elif console_type == 'xvpvnc':
6211             access_url_base = CONF.vnc.xvpvncproxy_base_url
6212         else:
6213             raise exception.ConsoleTypeInvalid(console_type=console_type)
6214 
6215         try:
6216             # Retrieve connect info from driver, and then decorate with our
6217             # access info token
6218             console = self.driver.get_vnc_console(context, instance)
6219             console_auth = objects.ConsoleAuthToken(
6220                 context=context,
6221                 console_type=console_type,
6222                 host=console.host,
6223                 port=console.port,
6224                 internal_access_path=console.internal_access_path,
6225                 instance_uuid=instance.uuid,
6226                 access_url_base=access_url_base,
6227             )
6228             console_auth.authorize(CONF.consoleauth.token_ttl)
6229             connect_info = console.get_connection_info(
6230                 console_auth.token, console_auth.access_url)
6231 
6232         except exception.InstanceNotFound:
6233             if instance.vm_state != vm_states.BUILDING:
6234                 raise
6235             raise exception.InstanceNotReady(instance_id=instance.uuid)
6236 
6237         return connect_info
6238 
6239     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6240                                    exception.InstanceNotReady,
6241                                    exception.InstanceNotFound,
6242                                    exception.ConsoleTypeUnavailable,
6243                                    NotImplementedError)
6244     @wrap_exception()
6245     @wrap_instance_fault
6246     def get_spice_console(self, context, console_type, instance):
6247         """Return connection information for a spice console."""
6248         context = context.elevated()
6249         LOG.debug("Getting spice console", instance=instance)
6250 
6251         if not CONF.spice.enabled:
6252             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6253 
6254         if console_type != 'spice-html5':
6255             raise exception.ConsoleTypeInvalid(console_type=console_type)
6256 
6257         try:
6258             # Retrieve connect info from driver, and then decorate with our
6259             # access info token
6260             console = self.driver.get_spice_console(context, instance)
6261             console_auth = objects.ConsoleAuthToken(
6262                 context=context,
6263                 console_type=console_type,
6264                 host=console.host,
6265                 port=console.port,
6266                 internal_access_path=console.internal_access_path,
6267                 instance_uuid=instance.uuid,
6268                 access_url_base=CONF.spice.html5proxy_base_url,
6269             )
6270             console_auth.authorize(CONF.consoleauth.token_ttl)
6271             connect_info = console.get_connection_info(
6272                 console_auth.token, console_auth.access_url)
6273 
6274         except exception.InstanceNotFound:
6275             if instance.vm_state != vm_states.BUILDING:
6276                 raise
6277             raise exception.InstanceNotReady(instance_id=instance.uuid)
6278 
6279         return connect_info
6280 
6281     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6282                                    exception.InstanceNotReady,
6283                                    exception.InstanceNotFound,
6284                                    exception.ConsoleTypeUnavailable,
6285                                    NotImplementedError)
6286     @wrap_exception()
6287     @wrap_instance_fault
6288     def get_rdp_console(self, context, console_type, instance):
6289         """Return connection information for a RDP console."""
6290         context = context.elevated()
6291         LOG.debug("Getting RDP console", instance=instance)
6292 
6293         if not CONF.rdp.enabled:
6294             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6295 
6296         if console_type != 'rdp-html5':
6297             raise exception.ConsoleTypeInvalid(console_type=console_type)
6298 
6299         try:
6300             # Retrieve connect info from driver, and then decorate with our
6301             # access info token
6302             console = self.driver.get_rdp_console(context, instance)
6303             console_auth = objects.ConsoleAuthToken(
6304                 context=context,
6305                 console_type=console_type,
6306                 host=console.host,
6307                 port=console.port,
6308                 internal_access_path=console.internal_access_path,
6309                 instance_uuid=instance.uuid,
6310                 access_url_base=CONF.rdp.html5_proxy_base_url,
6311             )
6312             console_auth.authorize(CONF.consoleauth.token_ttl)
6313             connect_info = console.get_connection_info(
6314                 console_auth.token, console_auth.access_url)
6315 
6316         except exception.InstanceNotFound:
6317             if instance.vm_state != vm_states.BUILDING:
6318                 raise
6319             raise exception.InstanceNotReady(instance_id=instance.uuid)
6320 
6321         return connect_info
6322 
6323     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6324                                    exception.InstanceNotReady,
6325                                    exception.InstanceNotFound,
6326                                    exception.ConsoleTypeUnavailable,
6327                                    NotImplementedError)
6328     @wrap_exception()
6329     @wrap_instance_fault
6330     def get_mks_console(self, context, console_type, instance):
6331         """Return connection information for a MKS console."""
6332         context = context.elevated()
6333         LOG.debug("Getting MKS console", instance=instance)
6334 
6335         if not CONF.mks.enabled:
6336             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6337 
6338         if console_type != 'webmks':
6339             raise exception.ConsoleTypeInvalid(console_type=console_type)
6340 
6341         try:
6342             # Retrieve connect info from driver, and then decorate with our
6343             # access info token
6344             console = self.driver.get_mks_console(context, instance)
6345             console_auth = objects.ConsoleAuthToken(
6346                 context=context,
6347                 console_type=console_type,
6348                 host=console.host,
6349                 port=console.port,
6350                 internal_access_path=console.internal_access_path,
6351                 instance_uuid=instance.uuid,
6352                 access_url_base=CONF.mks.mksproxy_base_url,
6353             )
6354             console_auth.authorize(CONF.consoleauth.token_ttl)
6355             connect_info = console.get_connection_info(
6356                 console_auth.token, console_auth.access_url)
6357 
6358         except exception.InstanceNotFound:
6359             if instance.vm_state != vm_states.BUILDING:
6360                 raise
6361             raise exception.InstanceNotReady(instance_id=instance.uuid)
6362 
6363         return connect_info
6364 
6365     @messaging.expected_exceptions(
6366         exception.ConsoleTypeInvalid,
6367         exception.InstanceNotReady,
6368         exception.InstanceNotFound,
6369         exception.ConsoleTypeUnavailable,
6370         exception.SocketPortRangeExhaustedException,
6371         exception.ImageSerialPortNumberInvalid,
6372         exception.ImageSerialPortNumberExceedFlavorValue,
6373         NotImplementedError)
6374     @wrap_exception()
6375     @wrap_instance_fault
6376     def get_serial_console(self, context, console_type, instance):
6377         """Returns connection information for a serial console."""
6378 
6379         LOG.debug("Getting serial console", instance=instance)
6380 
6381         if not CONF.serial_console.enabled:
6382             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6383 
6384         context = context.elevated()
6385 
6386         try:
6387             # Retrieve connect info from driver, and then decorate with our
6388             # access info token
6389             console = self.driver.get_serial_console(context, instance)
6390             console_auth = objects.ConsoleAuthToken(
6391                 context=context,
6392                 console_type=console_type,
6393                 host=console.host,
6394                 port=console.port,
6395                 internal_access_path=console.internal_access_path,
6396                 instance_uuid=instance.uuid,
6397                 access_url_base=CONF.serial_console.base_url,
6398             )
6399             console_auth.authorize(CONF.consoleauth.token_ttl)
6400             connect_info = console.get_connection_info(
6401                 console_auth.token, console_auth.access_url)
6402 
6403         except exception.InstanceNotFound:
6404             if instance.vm_state != vm_states.BUILDING:
6405                 raise
6406             raise exception.InstanceNotReady(instance_id=instance.uuid)
6407 
6408         return connect_info
6409 
6410     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6411                                    exception.InstanceNotReady,
6412                                    exception.InstanceNotFound)
6413     @wrap_exception()
6414     @wrap_instance_fault
6415     def validate_console_port(self, ctxt, instance, port, console_type):
6416         if console_type == "spice-html5":
6417             console_info = self.driver.get_spice_console(ctxt, instance)
6418         elif console_type == "rdp-html5":
6419             console_info = self.driver.get_rdp_console(ctxt, instance)
6420         elif console_type == "serial":
6421             console_info = self.driver.get_serial_console(ctxt, instance)
6422         elif console_type == "webmks":
6423             console_info = self.driver.get_mks_console(ctxt, instance)
6424         else:
6425             console_info = self.driver.get_vnc_console(ctxt, instance)
6426 
6427         # Some drivers may return an int on console_info.port but the port
6428         # variable in this method is a string, so cast to be sure we are
6429         # comparing the correct types.
6430         return str(console_info.port) == port
6431 
6432     @wrap_exception()
6433     @reverts_task_state
6434     @wrap_instance_fault
6435     def reserve_block_device_name(self, context, instance, device,
6436                                   volume_id, disk_bus, device_type, tag,
6437                                   multiattach):
6438         if (tag and not
6439                 self.driver.capabilities.get('supports_tagged_attach_volume',
6440                                              False)):
6441             raise exception.VolumeTaggedAttachNotSupported()
6442 
6443         if (multiattach and not
6444                 self.driver.capabilities.get('supports_multiattach', False)):
6445             raise exception.MultiattachNotSupportedByVirtDriver(
6446                 volume_id=volume_id)
6447 
6448         @utils.synchronized(instance.uuid)
6449         def do_reserve():
6450             bdms = (
6451                 objects.BlockDeviceMappingList.get_by_instance_uuid(
6452                     context, instance.uuid))
6453 
6454             # NOTE(ndipanov): We need to explicitly set all the fields on the
6455             #                 object so that obj_load_attr does not fail
6456             new_bdm = objects.BlockDeviceMapping(
6457                     context=context,
6458                     source_type='volume', destination_type='volume',
6459                     instance_uuid=instance.uuid, boot_index=None,
6460                     volume_id=volume_id,
6461                     device_name=device, guest_format=None,
6462                     disk_bus=disk_bus, device_type=device_type, tag=tag)
6463 
6464             new_bdm.device_name = self._get_device_name_for_instance(
6465                     instance, bdms, new_bdm)
6466 
6467             # NOTE(vish): create bdm here to avoid race condition
6468             new_bdm.create()
6469             return new_bdm
6470 
6471         return do_reserve()
6472 
6473     @wrap_exception()
6474     @wrap_instance_event(prefix='compute')
6475     @wrap_instance_fault
6476     def attach_volume(self, context, instance, bdm):
6477         """Attach a volume to an instance."""
6478         driver_bdm = driver_block_device.convert_volume(bdm)
6479 
6480         @utils.synchronized(instance.uuid)
6481         def do_attach_volume(context, instance, driver_bdm):
6482             try:
6483                 return self._attach_volume(context, instance, driver_bdm)
6484             except Exception:
6485                 with excutils.save_and_reraise_exception():
6486                     bdm.destroy()
6487 
6488         do_attach_volume(context, instance, driver_bdm)
6489 
6490     def _attach_volume(self, context, instance, bdm):
6491         context = context.elevated()
6492         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
6493                  {'volume_id': bdm.volume_id,
6494                   'mountpoint': bdm['mount_device']},
6495                  instance=instance)
6496         compute_utils.notify_about_volume_attach_detach(
6497             context, instance, self.host,
6498             action=fields.NotificationAction.VOLUME_ATTACH,
6499             phase=fields.NotificationPhase.START,
6500             volume_id=bdm.volume_id)
6501         try:
6502             bdm.attach(context, instance, self.volume_api, self.driver,
6503                        do_driver_attach=True)
6504         except Exception as e:
6505             with excutils.save_and_reraise_exception():
6506                 LOG.exception("Failed to attach %(volume_id)s "
6507                               "at %(mountpoint)s",
6508                               {'volume_id': bdm.volume_id,
6509                                'mountpoint': bdm['mount_device']},
6510                               instance=instance)
6511                 if bdm['attachment_id']:
6512                     # Try to delete the attachment to make the volume
6513                     # available again. Note that DriverVolumeBlockDevice
6514                     # may have already deleted the attachment so ignore
6515                     # VolumeAttachmentNotFound.
6516                     try:
6517                         self.volume_api.attachment_delete(
6518                             context, bdm['attachment_id'])
6519                     except exception.VolumeAttachmentNotFound as exc:
6520                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
6521                                   exc, instance=instance)
6522                 else:
6523                     self.volume_api.unreserve_volume(context, bdm.volume_id)
6524                 tb = traceback.format_exc()
6525                 compute_utils.notify_about_volume_attach_detach(
6526                     context, instance, self.host,
6527                     action=fields.NotificationAction.VOLUME_ATTACH,
6528                     phase=fields.NotificationPhase.ERROR,
6529                     exception=e,
6530                     volume_id=bdm.volume_id, tb=tb)
6531 
6532         info = {'volume_id': bdm.volume_id}
6533         self._notify_about_instance_usage(
6534             context, instance, "volume.attach", extra_usage_info=info)
6535         compute_utils.notify_about_volume_attach_detach(
6536             context, instance, self.host,
6537             action=fields.NotificationAction.VOLUME_ATTACH,
6538             phase=fields.NotificationPhase.END,
6539             volume_id=bdm.volume_id)
6540 
6541     def _notify_volume_usage_detach(self, context, instance, bdm):
6542         if CONF.volume_usage_poll_interval <= 0:
6543             return
6544 
6545         mp = bdm.device_name
6546         # Handle bootable volumes which will not contain /dev/
6547         if '/dev/' in mp:
6548             mp = mp[5:]
6549         try:
6550             vol_stats = self.driver.block_stats(instance, mp)
6551             if vol_stats is None:
6552                 return
6553         except NotImplementedError:
6554             return
6555 
6556         LOG.debug("Updating volume usage cache with totals", instance=instance)
6557         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
6558         vol_usage = objects.VolumeUsage(context)
6559         vol_usage.volume_id = bdm.volume_id
6560         vol_usage.instance_uuid = instance.uuid
6561         vol_usage.project_id = instance.project_id
6562         vol_usage.user_id = instance.user_id
6563         vol_usage.availability_zone = instance.availability_zone
6564         vol_usage.curr_reads = rd_req
6565         vol_usage.curr_read_bytes = rd_bytes
6566         vol_usage.curr_writes = wr_req
6567         vol_usage.curr_write_bytes = wr_bytes
6568         vol_usage.save(update_totals=True)
6569         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
6570         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
6571 
6572     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
6573                        attachment_id=None):
6574         """Detach a volume from an instance.
6575 
6576         :param context: security context
6577         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
6578         :param instance: the Instance object to detach the volume from
6579         :param destroy_bdm: if True, the corresponding BDM entry will be marked
6580                             as deleted. Disabling this is useful for operations
6581                             like rebuild, when we don't want to destroy BDM
6582         :param attachment_id: The volume attachment_id for the given instance
6583                               and volume.
6584         """
6585         volume_id = bdm.volume_id
6586         compute_utils.notify_about_volume_attach_detach(
6587             context, instance, self.host,
6588             action=fields.NotificationAction.VOLUME_DETACH,
6589             phase=fields.NotificationPhase.START,
6590             volume_id=volume_id)
6591 
6592         self._notify_volume_usage_detach(context, instance, bdm)
6593 
6594         LOG.info('Detaching volume %(volume_id)s',
6595                  {'volume_id': volume_id}, instance=instance)
6596 
6597         driver_bdm = driver_block_device.convert_volume(bdm)
6598         driver_bdm.detach(context, instance, self.volume_api, self.driver,
6599                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
6600 
6601         info = dict(volume_id=volume_id)
6602         self._notify_about_instance_usage(
6603             context, instance, "volume.detach", extra_usage_info=info)
6604         compute_utils.notify_about_volume_attach_detach(
6605             context, instance, self.host,
6606             action=fields.NotificationAction.VOLUME_DETACH,
6607             phase=fields.NotificationPhase.END,
6608             volume_id=volume_id)
6609 
6610         if 'tag' in bdm and bdm.tag:
6611             self._delete_disk_metadata(instance, bdm)
6612         if destroy_bdm:
6613             bdm.destroy()
6614 
6615     def _delete_disk_metadata(self, instance, bdm):
6616         for device in instance.device_metadata.devices:
6617             if isinstance(device, objects.DiskMetadata):
6618                 if 'serial' in device:
6619                     if device.serial == bdm.volume_id:
6620                         instance.device_metadata.devices.remove(device)
6621                         instance.save()
6622                         break
6623                 else:
6624                     # NOTE(artom) We log the entire device object because all
6625                     # fields are nullable and may not be set
6626                     LOG.warning('Unable to determine whether to clean up '
6627                                 'device metadata for disk %s', device,
6628                                 instance=instance)
6629 
6630     @wrap_exception()
6631     @wrap_instance_event(prefix='compute')
6632     @wrap_instance_fault
6633     def detach_volume(self, context, volume_id, instance, attachment_id):
6634         """Detach a volume from an instance.
6635 
6636         :param context: security context
6637         :param volume_id: the volume id
6638         :param instance: the Instance object to detach the volume from
6639         :param attachment_id: The volume attachment_id for the given instance
6640                               and volume.
6641 
6642         """
6643         @utils.synchronized(instance.uuid)
6644         def do_detach_volume(context, volume_id, instance, attachment_id):
6645             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
6646                     context, volume_id, instance.uuid)
6647             self._detach_volume(context, bdm, instance,
6648                                 attachment_id=attachment_id)
6649 
6650         do_detach_volume(context, volume_id, instance, attachment_id)
6651 
6652     def _init_volume_connection(self, context, new_volume,
6653                                 old_volume_id, connector, bdm,
6654                                 new_attachment_id, mountpoint):
6655         new_volume_id = new_volume['id']
6656         if new_attachment_id is None:
6657             # We're dealing with an old-style attachment so initialize the
6658             # connection so we can get the connection_info.
6659             new_cinfo = self.volume_api.initialize_connection(context,
6660                                                               new_volume_id,
6661                                                               connector)
6662         else:
6663             # Check for multiattach on the new volume and if True, check to
6664             # see if the virt driver supports multiattach.
6665             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
6666             # and should be consolidated into some common code at some point.
6667             vol_multiattach = new_volume.get('multiattach', False)
6668             virt_multiattach = self.driver.capabilities.get(
6669                 'supports_multiattach', False)
6670             if vol_multiattach and not virt_multiattach:
6671                 raise exception.MultiattachNotSupportedByVirtDriver(
6672                     volume_id=new_volume_id)
6673 
6674             # This is a new style attachment and the API created the new
6675             # volume attachment and passed the id to the compute over RPC.
6676             # At this point we need to update the new volume attachment with
6677             # the host connector, which will give us back the new attachment
6678             # connection_info.
6679             new_cinfo = self.volume_api.attachment_update(
6680                 context, new_attachment_id, connector,
6681                 mountpoint)['connection_info']
6682 
6683             if vol_multiattach:
6684                 # This will be used by the volume driver to determine the
6685                 # proper disk configuration.
6686                 new_cinfo['multiattach'] = True
6687 
6688         old_cinfo = jsonutils.loads(bdm['connection_info'])
6689         if old_cinfo and 'serial' not in old_cinfo:
6690             old_cinfo['serial'] = old_volume_id
6691         # NOTE(lyarwood): serial is not always present in the returned
6692         # connection_info so set it if it is missing as we do in
6693         # DriverVolumeBlockDevice.attach().
6694         if 'serial' not in new_cinfo:
6695             new_cinfo['serial'] = new_volume_id
6696         return (old_cinfo, new_cinfo)
6697 
6698     def _swap_volume(self, context, instance, bdm, connector,
6699                      old_volume_id, new_volume, resize_to,
6700                      new_attachment_id, is_cinder_migration):
6701         new_volume_id = new_volume['id']
6702         mountpoint = bdm['device_name']
6703         failed = False
6704         new_cinfo = None
6705         try:
6706             old_cinfo, new_cinfo = self._init_volume_connection(
6707                 context, new_volume, old_volume_id, connector,
6708                 bdm, new_attachment_id, mountpoint)
6709             # NOTE(lyarwood): The Libvirt driver, the only virt driver
6710             # currently implementing swap_volume, will modify the contents of
6711             # new_cinfo when connect_volume is called. This is then saved to
6712             # the BDM in swap_volume for future use outside of this flow.
6713             msg = ("swap_volume: Calling driver volume swap with "
6714                    "connection infos: new: %(new_cinfo)s; "
6715                    "old: %(old_cinfo)s" %
6716                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
6717             # Both new and old info might contain password
6718             LOG.debug(strutils.mask_password(msg), instance=instance)
6719 
6720             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
6721                                     mountpoint, resize_to)
6722             if new_attachment_id:
6723                 self.volume_api.attachment_complete(context, new_attachment_id)
6724             msg = ("swap_volume: Driver volume swap returned, new "
6725                    "connection_info is now : %(new_cinfo)s" %
6726                    {'new_cinfo': new_cinfo})
6727             LOG.debug(strutils.mask_password(msg))
6728         except Exception as ex:
6729             failed = True
6730             with excutils.save_and_reraise_exception():
6731                 tb = traceback.format_exc()
6732                 compute_utils.notify_about_volume_swap(
6733                     context, instance, self.host,
6734                     fields.NotificationPhase.ERROR,
6735                     old_volume_id, new_volume_id, ex, tb)
6736                 if new_cinfo:
6737                     msg = ("Failed to swap volume %(old_volume_id)s "
6738                            "for %(new_volume_id)s")
6739                     LOG.exception(msg, {'old_volume_id': old_volume_id,
6740                                         'new_volume_id': new_volume_id},
6741                                   instance=instance)
6742                 else:
6743                     msg = ("Failed to connect to volume %(volume_id)s "
6744                            "with volume at %(mountpoint)s")
6745                     LOG.exception(msg, {'volume_id': new_volume_id,
6746                                         'mountpoint': bdm['device_name']},
6747                                   instance=instance)
6748 
6749                 # The API marked the volume as 'detaching' for the old volume
6750                 # so we need to roll that back so the volume goes back to
6751                 # 'in-use' state.
6752                 self.volume_api.roll_detaching(context, old_volume_id)
6753 
6754                 if new_attachment_id is None:
6755                     # The API reserved the new volume so it would be in
6756                     # 'attaching' status, so we need to unreserve it so it
6757                     # goes back to 'available' status.
6758                     self.volume_api.unreserve_volume(context, new_volume_id)
6759                 else:
6760                     # This is a new style attachment for the new volume, which
6761                     # was created in the API. We just need to delete it here
6762                     # to put the new volume back into 'available' status.
6763                     self.volume_api.attachment_delete(
6764                         context, new_attachment_id)
6765         finally:
6766             # TODO(mriedem): This finally block is terribly confusing and is
6767             # trying to do too much. We should consider removing the finally
6768             # block and move whatever needs to happen on success and failure
6769             # into the blocks above for clarity, even if it means a bit of
6770             # redundant code.
6771             conn_volume = new_volume_id if failed else old_volume_id
6772             if new_cinfo:
6773                 LOG.debug("swap_volume: removing Cinder connection "
6774                           "for volume %(volume)s", {'volume': conn_volume},
6775                           instance=instance)
6776                 if bdm.attachment_id is None:
6777                     # This is the pre-3.44 flow for new-style volume
6778                     # attachments so just terminate the connection.
6779                     self.volume_api.terminate_connection(context,
6780                                                          conn_volume,
6781                                                          connector)
6782                 else:
6783                     # This is a new style volume attachment. If we failed, then
6784                     # the new attachment was already deleted above in the
6785                     # exception block and we have nothing more to do here. If
6786                     # swap_volume was successful in the driver, then we need to
6787                     # "detach" the original attachment by deleting it.
6788                     if not failed:
6789                         self.volume_api.attachment_delete(
6790                             context, bdm.attachment_id)
6791 
6792             # Need to make some decisions based on whether this was
6793             # a Cinder initiated migration or not. The callback to
6794             # migration completion isn't needed in the case of a
6795             # nova initiated simple swap of two volume
6796             # "volume-update" call so skip that. The new attachment
6797             # scenarios will give us a new attachment record and
6798             # that's what we want.
6799             if bdm.attachment_id and not is_cinder_migration:
6800                 # we don't callback to cinder
6801                 comp_ret = {'save_volume_id': new_volume_id}
6802             else:
6803                 # NOTE(lyarwood): The following call to
6804                 # os-migrate-volume-completion returns a dict containing
6805                 # save_volume_id, this volume id has two possible values :
6806                 # 1. old_volume_id if we are migrating (retyping) volumes
6807                 # 2. new_volume_id if we are swapping between two existing
6808                 #    volumes
6809                 # This volume id is later used to update the volume_id and
6810                 # connection_info['serial'] of the BDM.
6811                 comp_ret = self.volume_api.migrate_volume_completion(
6812                                                           context,
6813                                                           old_volume_id,
6814                                                           new_volume_id,
6815                                                           error=failed)
6816                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
6817                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
6818                           instance=instance)
6819 
6820         return (comp_ret, new_cinfo)
6821 
6822     @wrap_exception()
6823     @wrap_instance_event(prefix='compute')
6824     @wrap_instance_fault
6825     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
6826                     new_attachment_id):
6827         """Swap volume for an instance."""
6828         context = context.elevated()
6829 
6830         compute_utils.notify_about_volume_swap(
6831             context, instance, self.host,
6832             fields.NotificationPhase.START,
6833             old_volume_id, new_volume_id)
6834 
6835         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
6836                 context, old_volume_id, instance.uuid)
6837         connector = self.driver.get_volume_connector(instance)
6838 
6839         resize_to = 0
6840         old_volume = self.volume_api.get(context, old_volume_id)
6841         # Yes this is a tightly-coupled state check of what's going on inside
6842         # cinder, but we need this while we still support old (v1/v2) and
6843         # new style attachments (v3.44). Once we drop support for old style
6844         # attachments we could think about cleaning up the cinder-initiated
6845         # swap volume API flows.
6846         is_cinder_migration = False
6847         if 'migration_status' in old_volume:
6848             is_cinder_migration = old_volume['migration_status'] == 'migrating'
6849         old_vol_size = old_volume['size']
6850         new_volume = self.volume_api.get(context, new_volume_id)
6851         new_vol_size = new_volume['size']
6852         if new_vol_size > old_vol_size:
6853             resize_to = new_vol_size
6854 
6855         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
6856                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
6857                  instance=instance)
6858         comp_ret, new_cinfo = self._swap_volume(context,
6859                                                 instance,
6860                                                 bdm,
6861                                                 connector,
6862                                                 old_volume_id,
6863                                                 new_volume,
6864                                                 resize_to,
6865                                                 new_attachment_id,
6866                                                 is_cinder_migration)
6867 
6868         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
6869         # correct volume_id returned by Cinder.
6870         save_volume_id = comp_ret['save_volume_id']
6871         new_cinfo['serial'] = save_volume_id
6872         values = {
6873             'connection_info': jsonutils.dumps(new_cinfo),
6874             'source_type': 'volume',
6875             'destination_type': 'volume',
6876             'snapshot_id': None,
6877             'volume_id': save_volume_id,
6878             'no_device': None}
6879 
6880         if resize_to:
6881             values['volume_size'] = resize_to
6882 
6883         if new_attachment_id is not None:
6884             # This was a volume swap for a new-style attachment so we
6885             # need to update the BDM attachment_id for the new attachment.
6886             values['attachment_id'] = new_attachment_id
6887 
6888         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
6889                   "%(updates)s", {'volume_id': bdm.volume_id,
6890                                   'updates': values},
6891                   instance=instance)
6892         bdm.update(values)
6893         bdm.save()
6894 
6895         compute_utils.notify_about_volume_swap(
6896             context, instance, self.host,
6897             fields.NotificationPhase.END,
6898             old_volume_id, new_volume_id)
6899 
6900     @wrap_exception()
6901     def remove_volume_connection(self, context, volume_id, instance):
6902         """Remove the volume connection on this host
6903 
6904         Detach the volume from this instance on this host, and if this is
6905         the cinder v2 flow, call cinder to terminate the connection.
6906         """
6907         try:
6908             # NOTE(mriedem): If the BDM was just passed directly we would not
6909             # need to do this DB query, but this is an RPC interface so
6910             # changing that requires some care.
6911             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
6912                     context, volume_id, instance.uuid)
6913             # NOTE(mriedem): Normally we would pass delete_attachment=True to
6914             # _remove_volume_connection to delete a v3 style volume attachment,
6915             # but this method is RPC called from _rollback_live_migration which
6916             # already deletes the attachment, so because of that tight coupling
6917             # we cannot simply delete a v3 style attachment here without
6918             # needing to do some behavior modification of that
6919             # _rollback_live_migration flow which gets messy.
6920             self._remove_volume_connection(context, bdm, instance)
6921         except exception.NotFound:
6922             pass
6923 
6924     def _remove_volume_connection(self, context, bdm, instance,
6925                                   delete_attachment=False):
6926         """Remove the volume connection on this host
6927 
6928         Detach the volume from this instance on this host.
6929 
6930         :param context: nova auth request context
6931         :param bdm: BlockDeviceMapping object for a volume attached to the
6932             instance
6933         :param instance: Instance object with a volume attached represented
6934             by ``bdm``
6935         :param delete_attachment: If ``bdm.attachment_id`` is not None the
6936             attachment was made as a cinder v3 style attachment and if True,
6937             then deletes the volume attachment, otherwise just terminates
6938             the connection for a cinder legacy style connection.
6939         """
6940         driver_bdm = driver_block_device.convert_volume(bdm)
6941         driver_bdm.driver_detach(context, instance,
6942                                  self.volume_api, self.driver)
6943         if bdm.attachment_id is None:
6944             # cinder v2 api flow
6945             connector = self.driver.get_volume_connector(instance)
6946             self.volume_api.terminate_connection(context, bdm.volume_id,
6947                                                  connector)
6948         elif delete_attachment:
6949             # cinder v3 api flow
6950             self.volume_api.attachment_delete(context, bdm.attachment_id)
6951 
6952     def _deallocate_port_for_instance(self, context, instance, port_id,
6953                                       raise_on_failure=False):
6954         try:
6955             result = self.network_api.deallocate_port_for_instance(
6956                 context, instance, port_id)
6957             __, port_allocation = result
6958         except Exception as ex:
6959             with excutils.save_and_reraise_exception(
6960                     reraise=raise_on_failure):
6961                 LOG.warning('Failed to deallocate port %(port_id)s '
6962                             'for instance. Error: %(error)s',
6963                             {'port_id': port_id, 'error': ex},
6964                             instance=instance)
6965         else:
6966             if port_allocation:
6967                 # Deallocate the resources in placement that were used by the
6968                 # detached port.
6969                 try:
6970                     client = self.reportclient
6971                     client.remove_resources_from_instance_allocation(
6972                         context, instance.uuid, port_allocation)
6973                 except Exception as ex:
6974                     # We always raise here as it is not a race condition where
6975                     # somebody has already deleted the port we want to cleanup.
6976                     # Here we see that the port exists, the allocation exists,
6977                     # but we cannot clean it up so we will actually leak
6978                     # allocations.
6979                     with excutils.save_and_reraise_exception():
6980                         LOG.warning('Failed to remove resource allocation '
6981                                     'of port %(port_id)s for instance. Error: '
6982                                     '%(error)s',
6983                                     {'port_id': port_id, 'error': ex},
6984                                     instance=instance)
6985 
6986     # TODO(mriedem): There are likely race failures which can result in
6987     # NotFound and QuotaError exceptions getting traced as well.
6988     @messaging.expected_exceptions(
6989         # Do not log a traceback for user errors. We use Invalid generically
6990         # since this method can raise lots of different exceptions:
6991         # AttachInterfaceNotSupported
6992         # NetworkInterfaceTaggedAttachNotSupported
6993         # NetworkAmbiguous
6994         # PortNotUsable
6995         # PortInUse
6996         # PortNotUsableDNS
6997         # AttachSRIOVPortNotSupported
6998         # NetworksWithQoSPolicyNotSupported
6999         exception.Invalid)
7000     @wrap_exception()
7001     @wrap_instance_event(prefix='compute')
7002     @wrap_instance_fault
7003     def attach_interface(self, context, instance, network_id, port_id,
7004                          requested_ip, tag):
7005         """Use hotplug to add an network adapter to an instance."""
7006         if not self.driver.capabilities.get('supports_attach_interface',
7007                                             False):
7008             raise exception.AttachInterfaceNotSupported(
7009                 instance_uuid=instance.uuid)
7010         if (tag and not
7011             self.driver.capabilities.get('supports_tagged_attach_interface',
7012                                          False)):
7013             raise exception.NetworkInterfaceTaggedAttachNotSupported()
7014 
7015         compute_utils.notify_about_instance_action(
7016             context, instance, self.host,
7017             action=fields.NotificationAction.INTERFACE_ATTACH,
7018             phase=fields.NotificationPhase.START)
7019 
7020         bind_host_id = self.driver.network_binding_host_id(context, instance)
7021         network_info = self.network_api.allocate_port_for_instance(
7022             context, instance, port_id, network_id, requested_ip,
7023             bind_host_id=bind_host_id, tag=tag)
7024         if len(network_info) != 1:
7025             LOG.error('allocate_port_for_instance returned %(ports)s '
7026                       'ports', {'ports': len(network_info)})
7027             # TODO(elod.illes): an instance.interface_attach.error notification
7028             # should be sent here
7029             raise exception.InterfaceAttachFailed(
7030                     instance_uuid=instance.uuid)
7031         image_meta = objects.ImageMeta.from_instance(instance)
7032 
7033         try:
7034             self.driver.attach_interface(context, instance, image_meta,
7035                                          network_info[0])
7036         except exception.NovaException as ex:
7037             port_id = network_info[0].get('id')
7038             LOG.warning("attach interface failed , try to deallocate "
7039                         "port %(port_id)s, reason: %(msg)s",
7040                         {'port_id': port_id, 'msg': ex},
7041                         instance=instance)
7042             self._deallocate_port_for_instance(context, instance, port_id)
7043 
7044             tb = traceback.format_exc()
7045             compute_utils.notify_about_instance_action(
7046                 context, instance, self.host,
7047                 action=fields.NotificationAction.INTERFACE_ATTACH,
7048                 phase=fields.NotificationPhase.ERROR,
7049                 exception=ex, tb=tb)
7050 
7051             raise exception.InterfaceAttachFailed(
7052                 instance_uuid=instance.uuid)
7053 
7054         compute_utils.notify_about_instance_action(
7055             context, instance, self.host,
7056             action=fields.NotificationAction.INTERFACE_ATTACH,
7057             phase=fields.NotificationPhase.END)
7058 
7059         return network_info[0]
7060 
7061     @wrap_exception()
7062     @wrap_instance_event(prefix='compute')
7063     @wrap_instance_fault
7064     def detach_interface(self, context, instance, port_id):
7065         """Detach a network adapter from an instance."""
7066         network_info = instance.info_cache.network_info
7067         condemned = None
7068         for vif in network_info:
7069             if vif['id'] == port_id:
7070                 condemned = vif
7071                 break
7072         if condemned is None:
7073             raise exception.PortNotFound(_("Port %s is not "
7074                                            "attached") % port_id)
7075 
7076         compute_utils.notify_about_instance_action(
7077             context, instance, self.host,
7078             action=fields.NotificationAction.INTERFACE_DETACH,
7079             phase=fields.NotificationPhase.START)
7080 
7081         try:
7082             self.driver.detach_interface(context, instance, condemned)
7083         except exception.NovaException as ex:
7084             # If the instance was deleted before the interface was detached,
7085             # just log it at debug.
7086             log_level = (logging.DEBUG
7087                          if isinstance(ex, exception.InstanceNotFound)
7088                          else logging.WARNING)
7089             LOG.log(log_level,
7090                     "Detach interface failed, port_id=%(port_id)s, reason: "
7091                     "%(msg)s", {'port_id': port_id, 'msg': ex},
7092                     instance=instance)
7093             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
7094         else:
7095             self._deallocate_port_for_instance(
7096                 context, instance, port_id, raise_on_failure=True)
7097 
7098         compute_utils.notify_about_instance_action(
7099             context, instance, self.host,
7100             action=fields.NotificationAction.INTERFACE_DETACH,
7101             phase=fields.NotificationPhase.END)
7102 
7103     def _get_compute_info(self, context, host):
7104         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
7105             context, host)
7106 
7107     @wrap_exception()
7108     def check_instance_shared_storage(self, ctxt, instance, data):
7109         """Check if the instance files are shared
7110 
7111         :param ctxt: security context
7112         :param instance: dict of instance data
7113         :param data: result of driver.check_instance_shared_storage_local
7114 
7115         Returns True if instance disks located on shared storage and
7116         False otherwise.
7117         """
7118         return self.driver.check_instance_shared_storage_remote(ctxt, data)
7119 
7120     def _dest_can_numa_live_migrate(self, dest_check_data, migration):
7121         # TODO(artom) If we have a libvirt driver we expect it to set
7122         # dst_supports_numa_live_migration, but we have to remove it if we
7123         # did not get a migration from the conductor, indicating that it
7124         # cannot send RPC 5.3. This check can be removed in RPC 6.0.
7125         if ('dst_supports_numa_live_migration' in dest_check_data and
7126                 dest_check_data.dst_supports_numa_live_migration and
7127                 not migration):
7128             delattr(dest_check_data, 'dst_supports_numa_live_migration')
7129         return dest_check_data
7130 
7131     @wrap_exception()
7132     @wrap_instance_event(prefix='compute')
7133     @wrap_instance_fault
7134     def check_can_live_migrate_destination(self, ctxt, instance,
7135                                            block_migration, disk_over_commit,
7136                                            migration=None, limits=None):
7137         """Check if it is possible to execute live migration.
7138 
7139         This runs checks on the destination host, and then calls
7140         back to the source host to check the results.
7141 
7142         :param context: security context
7143         :param instance: dict of instance data
7144         :param block_migration: if true, prepare for block migration
7145                                 if None, calculate it in driver
7146         :param disk_over_commit: if true, allow disk over commit
7147                                  if None, ignore disk usage checking
7148         :param migration: objects.Migration object for this live migration.
7149         :param limits: objects.SchedulerLimits object for this live migration.
7150         :returns: a LiveMigrateData object (hypervisor-dependent)
7151         """
7152         src_compute_info = obj_base.obj_to_primitive(
7153             self._get_compute_info(ctxt, instance.host))
7154         dst_compute_info = obj_base.obj_to_primitive(
7155             self._get_compute_info(ctxt, CONF.host))
7156         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
7157             instance, src_compute_info, dst_compute_info,
7158             block_migration, disk_over_commit)
7159         dest_check_data = self._dest_can_numa_live_migrate(dest_check_data,
7160                                                            migration)
7161         LOG.debug('destination check data is %s', dest_check_data)
7162         try:
7163             migrate_data = self.compute_rpcapi.check_can_live_migrate_source(
7164                 ctxt, instance, dest_check_data)
7165             if ('src_supports_numa_live_migration' in migrate_data and
7166                     migrate_data.src_supports_numa_live_migration):
7167                 migrate_data = self._live_migration_claim(
7168                     ctxt, instance, migrate_data, migration, limits)
7169             elif 'dst_supports_numa_live_migration' in dest_check_data:
7170                 LOG.info('Destination was ready for NUMA live migration, '
7171                          'but source is either too old, or is set to an '
7172                          'older upgrade level.', instance=instance)
7173             # Create migrate_data vifs
7174             migrate_data.vifs = \
7175                 migrate_data_obj.VIFMigrateData.create_skeleton_migrate_vifs(
7176                     instance.get_network_info())
7177             # Claim PCI devices for VIFs on destination (if needed)
7178             port_id_to_pci = self._claim_pci_for_instance_vifs(ctxt, instance)
7179             # Update migrate VIFs with the newly claimed PCI devices
7180             self._update_migrate_vifs_profile_with_pci(migrate_data.vifs,
7181                                                        port_id_to_pci)
7182         finally:
7183             self.driver.cleanup_live_migration_destination_check(ctxt,
7184                     dest_check_data)
7185         return migrate_data
7186 
7187     def _live_migration_claim(self, ctxt, instance, migrate_data,
7188                               migration, limits):
7189         """Runs on the destination and does a resources claim, if necessary.
7190         Currently, only NUMA live migrations require it.
7191 
7192         :param ctxt: Request context
7193         :param instance: The Instance being live migrated
7194         :param migrate_data: The MigrateData object for this live migration
7195         :param migration: The Migration object for this live migration
7196         :param limits: The SchedulerLimits object for this live migration
7197         :returns: migrate_data with dst_numa_info set if necessary
7198         """
7199         try:
7200             # NOTE(artom) We might have gotten here from _find_destination() in
7201             # the conductor live migrate task. At that point,
7202             # migration.dest_node is not set yet (nor should it be, we're still
7203             # looking for a destination, after all). Therefore, we cannot use
7204             # migration.dest_node here and must use self._get_nodename().
7205             claim = self.rt.live_migration_claim(
7206                 ctxt, instance, self._get_nodename(instance), migration,
7207                 limits)
7208             LOG.debug('Created live migration claim.', instance=instance)
7209         except exception.ComputeResourcesUnavailable as e:
7210             raise exception.MigrationPreCheckError(
7211                 reason=e.format_message())
7212         return self.driver.post_claim_migrate_data(ctxt, instance,
7213                                                    migrate_data, claim)
7214 
7215     def _source_can_numa_live_migrate(self, ctxt, dest_check_data,
7216                                       source_check_data):
7217         # TODO(artom) Our virt driver may have told us that it supports NUMA
7218         # live migration. However, the following other conditions must be met
7219         # for a NUMA live migration to happen:
7220         # 1. We got a True dst_supports_numa_live_migration in
7221         #    dest_check_data, indicating that the dest virt driver supports
7222         #    NUMA live migration and that the conductor can send RPC 5.3 and
7223         #    that the destination compute manager can receive it.
7224         # 2. Ourselves, the source, can send RPC 5.3. There's no
7225         #    sentinel/parameter for this, so we just ask our rpcapi directly.
7226         # If any of these are not met, we need to remove the
7227         # src_supports_numa_live_migration flag from source_check_data to avoid
7228         # incorrectly initiating a NUMA live migration.
7229         # All of this can be removed in RPC 6.0/objects 2.0.
7230         can_numa_live_migrate = (
7231             'dst_supports_numa_live_migration' in dest_check_data and
7232             dest_check_data.dst_supports_numa_live_migration and
7233             self.compute_rpcapi.supports_numa_live_migration(ctxt))
7234         if ('src_supports_numa_live_migration' in source_check_data and
7235                 source_check_data.src_supports_numa_live_migration and
7236                 not can_numa_live_migrate):
7237             delattr(source_check_data, 'src_supports_numa_live_migration')
7238         return source_check_data
7239 
7240     @wrap_exception()
7241     @wrap_instance_event(prefix='compute')
7242     @wrap_instance_fault
7243     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
7244         """Check if it is possible to execute live migration.
7245 
7246         This checks if the live migration can succeed, based on the
7247         results from check_can_live_migrate_destination.
7248 
7249         :param ctxt: security context
7250         :param instance: dict of instance data
7251         :param dest_check_data: result of check_can_live_migrate_destination
7252         :returns: a LiveMigrateData object
7253         """
7254         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7255             ctxt, instance.uuid)
7256         is_volume_backed = compute_utils.is_volume_backed_instance(
7257             ctxt, instance, bdms)
7258         dest_check_data.is_volume_backed = is_volume_backed
7259         block_device_info = self._get_instance_block_device_info(
7260                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
7261         result = self.driver.check_can_live_migrate_source(ctxt, instance,
7262                                                            dest_check_data,
7263                                                            block_device_info)
7264         result = self._source_can_numa_live_migrate(ctxt, dest_check_data,
7265                                                     result)
7266         LOG.debug('source check data is %s', result)
7267         return result
7268 
7269     # TODO(mriedem): Remove the block_migration argument in v6.0 of the compute
7270     # RPC API.
7271     @wrap_exception()
7272     @wrap_instance_event(prefix='compute')
7273     @wrap_instance_fault
7274     def pre_live_migration(self, context, instance, block_migration, disk,
7275                            migrate_data):
7276         """Preparations for live migration at dest host.
7277 
7278         :param context: security context
7279         :param instance: dict of instance data
7280         :param block_migration: if true, prepare for block migration
7281         :param disk: disk info of instance
7282         :param migrate_data: A dict or LiveMigrateData object holding data
7283                              required for live migration without shared
7284                              storage.
7285         :returns: migrate_data containing additional migration info
7286         """
7287         LOG.debug('pre_live_migration data is %s', migrate_data)
7288 
7289         migrate_data.old_vol_attachment_ids = {}
7290         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7291             context, instance.uuid)
7292         network_info = self.network_api.get_instance_nw_info(context, instance)
7293         self._notify_about_instance_usage(
7294             context, instance, "live_migration.pre.start",
7295             network_info=network_info)
7296         compute_utils.notify_about_instance_action(
7297             context, instance, self.host,
7298             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7299             phase=fields.NotificationPhase.START, bdms=bdms)
7300 
7301         connector = self.driver.get_volume_connector(instance)
7302         try:
7303             for bdm in bdms:
7304                 if bdm.is_volume and bdm.attachment_id is not None:
7305                     # This bdm uses the new cinder v3.44 API.
7306                     # We will create a new attachment for this
7307                     # volume on this migration destination host. The old
7308                     # attachment will be deleted on the source host
7309                     # when the migration succeeds. The old attachment_id
7310                     # is stored in dict with the key being the bdm.volume_id
7311                     # so it can be restored on rollback.
7312                     #
7313                     # Also note that attachment_update is not needed as we
7314                     # are providing the connector in the create call.
7315                     attach_ref = self.volume_api.attachment_create(
7316                         context, bdm.volume_id, bdm.instance_uuid,
7317                         connector=connector, mountpoint=bdm.device_name)
7318 
7319                     # save current attachment so we can detach it on success,
7320                     # or restore it on a rollback.
7321                     # NOTE(mdbooth): This data is no longer used by the source
7322                     # host since change Ibe9215c0. We can't remove it until we
7323                     # are sure the source host has been upgraded.
7324                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
7325                         bdm.attachment_id
7326 
7327                     # update the bdm with the new attachment_id.
7328                     bdm.attachment_id = attach_ref['id']
7329                     bdm.save()
7330 
7331             block_device_info = self._get_instance_block_device_info(
7332                                 context, instance, refresh_conn_info=True,
7333                                 bdms=bdms)
7334 
7335             # The driver pre_live_migration will plug vifs on the host. We call
7336             # plug_vifs before calling ensure_filtering_rules_for_instance, to
7337             # ensure bridge is set up.
7338             migrate_data = self.driver.pre_live_migration(context,
7339                                            instance,
7340                                            block_device_info,
7341                                            network_info,
7342                                            disk,
7343                                            migrate_data)
7344             LOG.debug('driver pre_live_migration data is %s', migrate_data)
7345             # driver.pre_live_migration is what plugs vifs on the destination
7346             # host so now we can set the wait_for_vif_plugged flag in the
7347             # migrate_data object which the source compute will use to
7348             # determine if it should wait for a 'network-vif-plugged' event
7349             # from neutron before starting the actual guest transfer in the
7350             # hypervisor
7351             migrate_data.wait_for_vif_plugged = (
7352                 CONF.compute.live_migration_wait_for_vif_plug)
7353 
7354             # NOTE(tr3buchet): setup networks on destination host
7355             self.network_api.setup_networks_on_host(context, instance,
7356                                                              self.host)
7357 
7358             # Creating filters to hypervisors and firewalls.
7359             # An example is that nova-instance-instance-xxx,
7360             # which is written to libvirt.xml(Check "virsh nwfilter-list")
7361             # This nwfilter is necessary on the destination host.
7362             # In addition, this method is creating filtering rule
7363             # onto destination host.
7364             self.driver.ensure_filtering_rules_for_instance(instance,
7365                                                 network_info)
7366         except Exception:
7367             # If we raise, migrate_data with the updated attachment ids
7368             # will not be returned to the source host for rollback.
7369             # So we need to rollback new attachments here.
7370             with excutils.save_and_reraise_exception():
7371                 old_attachments = migrate_data.old_vol_attachment_ids
7372                 for bdm in bdms:
7373                     if (bdm.is_volume and bdm.attachment_id is not None and
7374                             bdm.volume_id in old_attachments):
7375                         self.volume_api.attachment_delete(context,
7376                                                           bdm.attachment_id)
7377                         bdm.attachment_id = old_attachments[bdm.volume_id]
7378                         bdm.save()
7379 
7380         # Volume connections are complete, tell cinder that all the
7381         # attachments have completed.
7382         for bdm in bdms:
7383             if bdm.is_volume and bdm.attachment_id is not None:
7384                 self.volume_api.attachment_complete(context,
7385                                                     bdm.attachment_id)
7386 
7387         self._notify_about_instance_usage(
7388                      context, instance, "live_migration.pre.end",
7389                      network_info=network_info)
7390         compute_utils.notify_about_instance_action(
7391             context, instance, self.host,
7392             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7393             phase=fields.NotificationPhase.END, bdms=bdms)
7394 
7395         LOG.debug('pre_live_migration result data is %s', migrate_data)
7396         return migrate_data
7397 
7398     @staticmethod
7399     def _neutron_failed_migration_callback(event_name, instance):
7400         msg = ('Neutron reported failure during migration '
7401                'with %(event)s for instance %(uuid)s')
7402         msg_args = {'event': event_name, 'uuid': instance.uuid}
7403         if CONF.vif_plugging_is_fatal:
7404             raise exception.VirtualInterfacePlugException(msg % msg_args)
7405         LOG.error(msg, msg_args)
7406 
7407     @staticmethod
7408     def _get_neutron_events_for_live_migration(instance):
7409         # We don't generate events if CONF.vif_plugging_timeout=0
7410         # meaning that the operator disabled using them.
7411         if CONF.vif_plugging_timeout and utils.is_neutron():
7412             return [('network-vif-plugged', vif['id'])
7413                     for vif in instance.get_network_info()]
7414         else:
7415             return []
7416 
7417     def _cleanup_pre_live_migration(self, context, dest, instance,
7418                                     migration, migrate_data, source_bdms):
7419         """Helper method for when pre_live_migration fails
7420 
7421         Sets the migration status to "error" and rolls back the live migration
7422         setup on the destination host.
7423 
7424         :param context: The user request context.
7425         :type context: nova.context.RequestContext
7426         :param dest: The live migration destination hostname.
7427         :type dest: str
7428         :param instance: The instance being live migrated.
7429         :type instance: nova.objects.Instance
7430         :param migration: The migration record tracking this live migration.
7431         :type migration: nova.objects.Migration
7432         :param migrate_data: Data about the live migration, populated from
7433                              the destination host.
7434         :type migrate_data: Subclass of nova.objects.LiveMigrateData
7435         :param source_bdms: BDMs prior to modification by the destination
7436                             compute host. Set by _do_live_migration and not
7437                             part of the callback interface, so this is never
7438                             None
7439         """
7440         self._set_migration_status(migration, 'error')
7441         # Make sure we set this for _rollback_live_migration()
7442         # so it can find it, as expected if it was called later
7443         migrate_data.migration = migration
7444         self._rollback_live_migration(context, instance, dest,
7445                                       migrate_data=migrate_data,
7446                                       source_bdms=source_bdms)
7447 
7448     def _do_pre_live_migration_from_source(self, context, dest, instance,
7449                                            block_migration, migration,
7450                                            migrate_data, source_bdms):
7451         """Prepares for pre-live-migration on the source host and calls dest
7452 
7453         Will setup a callback networking event handler (if configured) and
7454         then call the dest host's pre_live_migration method to prepare the
7455         dest host for live migration (plugs vifs, connect volumes, etc).
7456 
7457         _rollback_live_migration (on the source) will be called if
7458         pre_live_migration (on the dest) fails.
7459 
7460         :param context: nova auth request context for this operation
7461         :param dest: name of the destination compute service host
7462         :param instance: Instance object being live migrated
7463         :param block_migration: If true, prepare for block migration.
7464         :param migration: Migration object tracking this operation
7465         :param migrate_data: MigrateData object for this operation populated
7466             by the destination host compute driver as part of the
7467             check_can_live_migrate_destination call.
7468         :param source_bdms: BlockDeviceMappingList of BDMs currently attached
7469             to the instance from the source host.
7470         :returns: MigrateData object which is a modified version of the
7471             ``migrate_data`` argument from the compute driver on the dest
7472             host during the ``pre_live_migration`` call.
7473         :raises: MigrationError if waiting for the network-vif-plugged event
7474             timed out and is fatal.
7475         """
7476         class _BreakWaitForInstanceEvent(Exception):
7477             """Used as a signal to stop waiting for the network-vif-plugged
7478             event when we discover that
7479             [compute]/live_migration_wait_for_vif_plug is not set on the
7480             destination.
7481             """
7482             pass
7483 
7484         events = self._get_neutron_events_for_live_migration(instance)
7485         try:
7486             if ('block_migration' in migrate_data and
7487                     migrate_data.block_migration):
7488                 block_device_info = self._get_instance_block_device_info(
7489                     context, instance, bdms=source_bdms)
7490                 disk = self.driver.get_instance_disk_info(
7491                     instance, block_device_info=block_device_info)
7492             else:
7493                 disk = None
7494 
7495             deadline = CONF.vif_plugging_timeout
7496             error_cb = self._neutron_failed_migration_callback
7497             # In order to avoid a race with the vif plugging that the virt
7498             # driver does on the destination host, we register our events
7499             # to wait for before calling pre_live_migration. Then if the
7500             # dest host reports back that we shouldn't wait, we can break
7501             # out of the context manager using _BreakWaitForInstanceEvent.
7502             with self.virtapi.wait_for_instance_event(
7503                     instance, events, deadline=deadline,
7504                     error_callback=error_cb):
7505                 with timeutils.StopWatch() as timer:
7506                     # TODO(mriedem): The "block_migration" parameter passed
7507                     # here is not actually used in pre_live_migration but it
7508                     # is not optional in the RPC interface either.
7509                     migrate_data = self.compute_rpcapi.pre_live_migration(
7510                         context, instance,
7511                         block_migration, disk, dest, migrate_data)
7512                 LOG.info('Took %0.2f seconds for pre_live_migration on '
7513                          'destination host %s.',
7514                          timer.elapsed(), dest, instance=instance)
7515                 wait_for_vif_plugged = (
7516                     'wait_for_vif_plugged' in migrate_data and
7517                     migrate_data.wait_for_vif_plugged)
7518                 if events and not wait_for_vif_plugged:
7519                     raise _BreakWaitForInstanceEvent
7520         except _BreakWaitForInstanceEvent:
7521             if events:
7522                 LOG.debug('Not waiting for events after pre_live_migration: '
7523                           '%s. ', events, instance=instance)
7524             # This is a bit weird, but we need to clear sys.exc_info() so that
7525             # oslo.log formatting does not inadvertently use it later if an
7526             # error message is logged without an explicit exc_info. This is
7527             # only a problem with python 2.
7528             if six.PY2:
7529                 sys.exc_clear()
7530         except exception.VirtualInterfacePlugException:
7531             with excutils.save_and_reraise_exception():
7532                 LOG.exception('Failed waiting for network virtual interfaces '
7533                               'to be plugged on the destination host %s.',
7534                               dest, instance=instance)
7535                 self._cleanup_pre_live_migration(
7536                     context, dest, instance, migration, migrate_data,
7537                     source_bdms)
7538         except eventlet.timeout.Timeout:
7539             # We only get here if wait_for_vif_plugged is True which means
7540             # live_migration_wait_for_vif_plug=True on the destination host.
7541             msg = (
7542                 'Timed out waiting for events: %(events)s. If these timeouts '
7543                 'are a persistent issue it could mean the networking backend '
7544                 'on host %(dest)s does not support sending these events '
7545                 'unless there are port binding host changes which does not '
7546                 'happen at this point in the live migration process. You may '
7547                 'need to disable the live_migration_wait_for_vif_plug option '
7548                 'on host %(dest)s.')
7549             subs = {'events': events, 'dest': dest}
7550             LOG.warning(msg, subs, instance=instance)
7551             if CONF.vif_plugging_is_fatal:
7552                 self._cleanup_pre_live_migration(
7553                     context, dest, instance, migration, migrate_data,
7554                     source_bdms)
7555                 raise exception.MigrationError(reason=msg % subs)
7556         except Exception:
7557             with excutils.save_and_reraise_exception():
7558                 LOG.exception('Pre live migration failed at %s',
7559                               dest, instance=instance)
7560                 self._cleanup_pre_live_migration(
7561                     context, dest, instance, migration, migrate_data,
7562                     source_bdms)
7563         return migrate_data
7564 
7565     def _do_live_migration(self, context, dest, instance, block_migration,
7566                            migration, migrate_data):
7567         # NOTE(danms): We should enhance the RT to account for migrations
7568         # and use the status field to denote when the accounting has been
7569         # done on source/destination. For now, this is just here for status
7570         # reporting
7571         self._set_migration_status(migration, 'preparing')
7572         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7573                 context, instance.uuid)
7574 
7575         migrate_data = self._do_pre_live_migration_from_source(
7576             context, dest, instance, block_migration, migration, migrate_data,
7577             source_bdms)
7578 
7579         # Set migrate_data.migration because that is how _post_live_migration
7580         # and _rollback_live_migration get the migration object for cleanup.
7581         # Yes this is gross but changing the _post_live_migration and
7582         # _rollback_live_migration interfaces would also mean changing how the
7583         # virt drivers call them from the driver.live_migration method, i.e.
7584         # we would have to pass the migration object through the driver (or
7585         # consider using a partial but some do not like that pattern).
7586         migrate_data.migration = migration
7587 
7588         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
7589         # if it exist in the queue, then we are good to moving on, if
7590         # not, some other process must have aborted it, then we should
7591         # rollback.
7592         try:
7593             self._waiting_live_migrations.pop(instance.uuid)
7594         except KeyError:
7595             LOG.debug('Migration %s aborted by another process, rollback.',
7596                       migration.uuid, instance=instance)
7597             self._rollback_live_migration(context, instance, dest,
7598                                           migrate_data, 'cancelled',
7599                                           source_bdms=source_bdms)
7600             self._notify_live_migrate_abort_end(context, instance)
7601             return
7602 
7603         self._set_migration_status(migration, 'running')
7604 
7605         # NOTE(mdbooth): pre_live_migration will update connection_info and
7606         # attachment_id on all volume BDMS to reflect the new destination
7607         # host attachment. We fetch BDMs before that to retain connection_info
7608         # and attachment_id relating to the source host for post migration
7609         # cleanup.
7610         post_live_migration = functools.partial(self._post_live_migration,
7611                                                 source_bdms=source_bdms)
7612         rollback_live_migration = functools.partial(
7613             self._rollback_live_migration, source_bdms=source_bdms)
7614 
7615         LOG.debug('live_migration data is %s', migrate_data)
7616         try:
7617             self.driver.live_migration(context, instance, dest,
7618                                        post_live_migration,
7619                                        rollback_live_migration,
7620                                        block_migration, migrate_data)
7621         except Exception:
7622             LOG.exception('Live migration failed.', instance=instance)
7623             with excutils.save_and_reraise_exception():
7624                 # Put instance and migration into error state,
7625                 # as its almost certainly too late to rollback
7626                 self._set_migration_status(migration, 'error')
7627                 # first refresh instance as it may have got updated by
7628                 # post_live_migration_at_destination
7629                 instance.refresh()
7630                 self._set_instance_obj_error_state(context, instance,
7631                                                    clean_task_state=True)
7632 
7633     @wrap_exception()
7634     @wrap_instance_event(prefix='compute')
7635     @errors_out_migration
7636     @wrap_instance_fault
7637     def live_migration(self, context, dest, instance, block_migration,
7638                        migration, migrate_data):
7639         """Executing live migration.
7640 
7641         :param context: security context
7642         :param dest: destination host
7643         :param instance: a nova.objects.instance.Instance object
7644         :param block_migration: if true, prepare for block migration
7645         :param migration: an nova.objects.Migration object
7646         :param migrate_data: implementation specific params
7647 
7648         """
7649         self._set_migration_status(migration, 'queued')
7650         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
7651         # put the returned Future object into dict mapped with migration.uuid
7652         # in order to be able to track and abort it in the future.
7653         self._waiting_live_migrations[instance.uuid] = (None, None)
7654         try:
7655             future = self._live_migration_executor.submit(
7656                 self._do_live_migration, context, dest, instance,
7657                 block_migration, migration, migrate_data)
7658             self._waiting_live_migrations[instance.uuid] = (migration, future)
7659         except RuntimeError:
7660             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
7661             # pool is shutdown, which happens in
7662             # _cleanup_live_migrations_in_pool.
7663             LOG.info('Migration %s failed to submit as the compute service '
7664                      'is shutting down.', migration.uuid, instance=instance)
7665             raise exception.LiveMigrationNotSubmitted(
7666                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
7667 
7668     @wrap_exception()
7669     @wrap_instance_event(prefix='compute')
7670     @wrap_instance_fault
7671     def live_migration_force_complete(self, context, instance):
7672         """Force live migration to complete.
7673 
7674         :param context: Security context
7675         :param instance: The instance that is being migrated
7676         """
7677 
7678         self._notify_about_instance_usage(
7679             context, instance, 'live.migration.force.complete.start')
7680         compute_utils.notify_about_instance_action(
7681             context, instance, self.host,
7682             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
7683             phase=fields.NotificationPhase.START)
7684         self.driver.live_migration_force_complete(instance)
7685         self._notify_about_instance_usage(
7686             context, instance, 'live.migration.force.complete.end')
7687         compute_utils.notify_about_instance_action(
7688             context, instance, self.host,
7689             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
7690             phase=fields.NotificationPhase.END)
7691 
7692     def _notify_live_migrate_abort_end(self, context, instance):
7693         self._notify_about_instance_usage(
7694             context, instance, 'live.migration.abort.end')
7695         compute_utils.notify_about_instance_action(
7696             context, instance, self.host,
7697             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
7698             phase=fields.NotificationPhase.END)
7699 
7700     @wrap_exception()
7701     @wrap_instance_event(prefix='compute')
7702     @wrap_instance_fault
7703     def live_migration_abort(self, context, instance, migration_id):
7704         """Abort an in-progress live migration.
7705 
7706         :param context: Security context
7707         :param instance: The instance that is being migrated
7708         :param migration_id: ID of in-progress live migration
7709 
7710         """
7711         self._notify_about_instance_usage(
7712             context, instance, 'live.migration.abort.start')
7713         compute_utils.notify_about_instance_action(
7714             context, instance, self.host,
7715             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
7716             phase=fields.NotificationPhase.START)
7717         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
7718         # lead to 3 scenarios:
7719         # 1. The selected migration is still in queue, and the future.cancel()
7720         #    succeed, then the abort action is succeed, mark the migration
7721         #    status to 'cancelled'.
7722         # 2. The selected migration is still in queue, but the future.cancel()
7723         #    failed, then the _do_live_migration() has started executing, and
7724         #    the migration status is 'preparing', then we just pop it from the
7725         #    queue, and the migration process will handle it later. And the
7726         #    migration status couldn't be 'running' in this scenario because
7727         #    if _do_live_migration has started executing and we've already
7728         #    popped it from the queue and set the migration status to
7729         #    'running' at this point, popping it here will raise KeyError at
7730         #    which point we check if it's running and if so, we abort the old
7731         #    way.
7732         # 3. The selected migration is not in the queue, then the migration
7733         #    status is 'running', let the driver handle it.
7734         try:
7735             migration, future = (
7736                 self._waiting_live_migrations.pop(instance.uuid))
7737             if future and future.cancel():
7738                 # If we got here, we've successfully aborted the queued
7739                 # migration and _do_live_migration won't run so we need
7740                 # to set the migration status to cancelled and send the
7741                 # notification. If Future.cancel() fails, it means
7742                 # _do_live_migration is running and the migration status
7743                 # is preparing, and _do_live_migration() itself will attempt
7744                 # to pop the queued migration, hit a KeyError, and rollback,
7745                 # set the migration to cancelled and send the
7746                 # live.migration.abort.end notification.
7747                 self._set_migration_status(migration, 'cancelled')
7748         except KeyError:
7749             migration = objects.Migration.get_by_id(context, migration_id)
7750             if migration.status != 'running':
7751                 raise exception.InvalidMigrationState(
7752                     migration_id=migration_id, instance_uuid=instance.uuid,
7753                     state=migration.status, method='abort live migration')
7754             self.driver.live_migration_abort(instance)
7755         self._notify_live_migrate_abort_end(context, instance)
7756 
7757     def _live_migration_cleanup_flags(self, migrate_data):
7758         """Determine whether disks or instance path need to be cleaned up after
7759         live migration (at source on success, at destination on rollback)
7760 
7761         Block migration needs empty image at destination host before migration
7762         starts, so if any failure occurs, any empty images has to be deleted.
7763 
7764         Also Volume backed live migration w/o shared storage needs to delete
7765         newly created instance-xxx dir on the destination as a part of its
7766         rollback process
7767 
7768         :param migrate_data: implementation specific data
7769         :returns: (bool, bool) -- do_cleanup, destroy_disks
7770         """
7771         # NOTE(pkoniszewski): block migration specific params are set inside
7772         # migrate_data objects for drivers that expose block live migration
7773         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
7774         # cleanup is not needed.
7775         do_cleanup = False
7776         destroy_disks = False
7777         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
7778             # No instance booting at source host, but instance dir
7779             # must be deleted for preparing next block migration
7780             # must be deleted for preparing next live migration w/o shared
7781             # storage
7782             do_cleanup = not migrate_data.is_shared_instance_path
7783             destroy_disks = not migrate_data.is_shared_block_storage
7784         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
7785             do_cleanup = migrate_data.block_migration
7786             destroy_disks = migrate_data.block_migration
7787         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
7788             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
7789             do_cleanup = True
7790             destroy_disks = not migrate_data.is_shared_instance_path
7791 
7792         return (do_cleanup, destroy_disks)
7793 
7794     def _post_live_migration_remove_source_vol_connections(
7795             self, context, instance, source_bdms):
7796         """Disconnect volume connections from the source host during
7797         _post_live_migration.
7798 
7799         :param context: nova auth RequestContext
7800         :param instance: Instance object being live migrated
7801         :param source_bdms: BlockDeviceMappingList representing the attached
7802             volumes with connection_info set for the source host
7803         """
7804         # Detaching volumes.
7805         connector = self.driver.get_volume_connector(instance)
7806         for bdm in source_bdms:
7807             if bdm.is_volume:
7808                 # Detaching volumes is a call to an external API that can fail.
7809                 # If it does, we need to handle it gracefully so that the call
7810                 # to post_live_migration_at_destination - where we set instance
7811                 # host and task state - still happens. We need to rethink the
7812                 # current approach of setting instance host and task state
7813                 # AFTER a whole bunch of things that could fail in unhandled
7814                 # ways, but that is left as a TODO(artom).
7815                 try:
7816                     if bdm.attachment_id is None:
7817                         # Prior to cinder v3.44:
7818                         # We don't want to actually mark the volume detached,
7819                         # or delete the bdm, just remove the connection from
7820                         # this host.
7821                         #
7822                         # remove the volume connection without detaching from
7823                         # hypervisor because the instance is not running
7824                         # anymore on the current host
7825                         self.volume_api.terminate_connection(context,
7826                                                              bdm.volume_id,
7827                                                              connector)
7828                     else:
7829                         # cinder v3.44 api flow - delete the old attachment
7830                         # for the source host
7831                         self.volume_api.attachment_delete(context,
7832                                                           bdm.attachment_id)
7833 
7834                 except Exception as e:
7835                     if bdm.attachment_id is None:
7836                         LOG.error('Connection for volume %s not terminated on '
7837                                   'source host %s during post_live_migration: '
7838                                   '%s', bdm.volume_id, self.host,
7839                                   six.text_type(e), instance=instance)
7840                     else:
7841                         LOG.error('Volume attachment %s not deleted on source '
7842                                   'host %s during post_live_migration: %s',
7843                                   bdm.attachment_id, self.host,
7844                                   six.text_type(e), instance=instance)
7845 
7846     @wrap_exception()
7847     @wrap_instance_fault
7848     def _post_live_migration(self, ctxt, instance, dest,
7849                              block_migration=False, migrate_data=None,
7850                              source_bdms=None):
7851         """Post operations for live migration.
7852 
7853         This method is called from live_migration
7854         and mainly updating database record.
7855 
7856         :param ctxt: security context
7857         :param instance: instance dict
7858         :param dest: destination host
7859         :param block_migration: if true, prepare for block migration
7860         :param migrate_data: if not None, it is a dict which has data
7861         :param source_bdms: BDMs prior to modification by the destination
7862                             compute host. Set by _do_live_migration and not
7863                             part of the callback interface, so this is never
7864                             None
7865         required for live migration without shared storage
7866 
7867         """
7868         LOG.info('_post_live_migration() is started..',
7869                  instance=instance)
7870 
7871         # Cleanup source host post live-migration
7872         block_device_info = self._get_instance_block_device_info(
7873                             ctxt, instance, bdms=source_bdms)
7874         self.driver.post_live_migration(ctxt, instance, block_device_info,
7875                                         migrate_data)
7876 
7877         # Disconnect volumes from this (the source) host.
7878         self._post_live_migration_remove_source_vol_connections(
7879             ctxt, instance, source_bdms)
7880 
7881         # Releasing vlan.
7882         # (not necessary in current implementation?)
7883 
7884         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
7885 
7886         self._notify_about_instance_usage(ctxt, instance,
7887                                           "live_migration._post.start",
7888                                           network_info=network_info)
7889         compute_utils.notify_about_instance_action(
7890             ctxt, instance, self.host,
7891             action=fields.NotificationAction.LIVE_MIGRATION_POST,
7892             phase=fields.NotificationPhase.START)
7893         # Releasing security group ingress rule.
7894         LOG.debug('Calling driver.unfilter_instance from _post_live_migration',
7895                   instance=instance)
7896         self.driver.unfilter_instance(instance,
7897                                       network_info)
7898 
7899         migration = {'source_compute': self.host,
7900                      'dest_compute': dest, }
7901         # For neutron, migrate_instance_start will activate the destination
7902         # host port bindings, if there are any created by conductor before live
7903         # migration started.
7904         self.network_api.migrate_instance_start(ctxt,
7905                                                 instance,
7906                                                 migration)
7907 
7908         destroy_vifs = False
7909         try:
7910             # It's possible that the vif type changed on the destination
7911             # host and is already bound and active, so we need to use the
7912             # stashed source vifs in migrate_data.vifs (if present) to unplug
7913             # on the source host.
7914             unplug_nw_info = network_info
7915             if migrate_data and 'vifs' in migrate_data:
7916                 nw_info = []
7917                 for migrate_vif in migrate_data.vifs:
7918                     nw_info.append(migrate_vif.source_vif)
7919                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
7920                 LOG.debug('Calling driver.post_live_migration_at_source '
7921                           'with original source VIFs from migrate_data: %s',
7922                           unplug_nw_info, instance=instance)
7923             self.driver.post_live_migration_at_source(ctxt, instance,
7924                                                       unplug_nw_info)
7925         except NotImplementedError as ex:
7926             LOG.debug(ex, instance=instance)
7927             # For all hypervisors other than libvirt, there is a possibility
7928             # they are unplugging networks from source node in the cleanup
7929             # method
7930             destroy_vifs = True
7931 
7932         # Free instance allocations on source before claims are allocated on
7933         # destination node
7934         self.rt.free_pci_device_allocations_for_instance(ctxt, instance)
7935         # NOTE(danms): Save source node before calling post method on
7936         # destination, which will update it
7937         source_node = instance.node
7938 
7939         # Define domain at destination host, without doing it,
7940         # pause/suspend/terminate do not work.
7941         post_at_dest_success = True
7942         try:
7943             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
7944                     instance, block_migration, dest)
7945         except Exception as error:
7946             post_at_dest_success = False
7947             # We don't want to break _post_live_migration() if
7948             # post_live_migration_at_destination() fails as it should never
7949             # affect cleaning up source node.
7950             LOG.exception("Post live migration at destination %s failed",
7951                           dest, instance=instance, error=error)
7952 
7953         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
7954                 migrate_data)
7955 
7956         if do_cleanup:
7957             # NOTE(artom) By this time post_live_migration_at_destination()
7958             # will have applied the migration context and saved the instance,
7959             # writing a new instance NUMA topology in the process (if the
7960             # intance has one). Here on the source, some drivers will call
7961             # instance.save() in their cleanup() method, which would clobber
7962             # the new instance NUMA topology saved by the destination with the
7963             # old fields in our instance object. To prevent this, refresh our
7964             # instance.
7965             instance.refresh()
7966             LOG.debug('Calling driver.cleanup from _post_live_migration',
7967                       instance=instance)
7968             self.driver.cleanup(ctxt, instance, unplug_nw_info,
7969                                 destroy_disks=destroy_disks,
7970                                 migrate_data=migrate_data,
7971                                 destroy_vifs=destroy_vifs)
7972 
7973         self.instance_events.clear_events_for_instance(instance)
7974 
7975         # NOTE(timello): make sure we update available resources on source
7976         # host even before next periodic task.
7977         self.update_available_resource(ctxt)
7978 
7979         self._update_scheduler_instance_info(ctxt, instance)
7980         self._notify_about_instance_usage(ctxt, instance,
7981                                           "live_migration._post.end",
7982                                           network_info=network_info)
7983         compute_utils.notify_about_instance_action(
7984             ctxt, instance, self.host,
7985             action=fields.NotificationAction.LIVE_MIGRATION_POST,
7986             phase=fields.NotificationPhase.END)
7987         if post_at_dest_success:
7988             LOG.info('Migrating instance to %s finished successfully.',
7989                      dest, instance=instance)
7990 
7991         self._clean_instance_console_tokens(ctxt, instance)
7992         if migrate_data and migrate_data.obj_attr_is_set('migration'):
7993             migrate_data.migration.status = 'completed'
7994             migrate_data.migration.save()
7995             self._delete_allocation_after_move(ctxt,
7996                                                instance,
7997                                                migrate_data.migration)
7998         else:
7999             # We didn't have data on a migration, which means we can't
8000             # look up to see if we had new-style migration-based
8001             # allocations. This should really only happen in cases of
8002             # a buggy virt driver. Log a warning so we know it happened.
8003             LOG.warning('Live migration ended with no migrate_data '
8004                         'record. Unable to clean up migration-based '
8005                         'allocations for node %s which is almost certainly '
8006                         'not an expected situation.', source_node,
8007                         instance=instance)
8008 
8009     def _consoles_enabled(self):
8010         """Returns whether a console is enable."""
8011         return (CONF.vnc.enabled or CONF.spice.enabled or
8012                 CONF.rdp.enabled or CONF.serial_console.enabled or
8013                 CONF.mks.enabled)
8014 
8015     def _clean_instance_console_tokens(self, ctxt, instance):
8016         """Clean console tokens stored for an instance."""
8017         # If the database backend isn't in use, don't bother trying to clean
8018         # tokens.
8019         if self._consoles_enabled():
8020             objects.ConsoleAuthToken.\
8021                 clean_console_auths_for_instance(ctxt, instance.uuid)
8022 
8023     @wrap_exception()
8024     @wrap_instance_event(prefix='compute')
8025     @wrap_instance_fault
8026     def post_live_migration_at_destination(self, context, instance,
8027                                            block_migration):
8028         """Post operations for live migration .
8029 
8030         :param context: security context
8031         :param instance: Instance dict
8032         :param block_migration: if true, prepare for block migration
8033 
8034         """
8035         LOG.info('Post operation of migration started',
8036                  instance=instance)
8037 
8038         # NOTE(tr3buchet): setup networks on destination host
8039         #                  this is called a second time because
8040         #                  multi_host does not create the bridge in
8041         #                  plug_vifs
8042         # NOTE(mriedem): This is a no-op for neutron.
8043         self.network_api.setup_networks_on_host(context, instance,
8044                                                          self.host)
8045         migration = objects.Migration(source_compute=instance.host,
8046                                       dest_compute=self.host,
8047                                       migration_type='live-migration')
8048         # TODO(gibi): calculate and pass resource_provider_mapping
8049         self.network_api.migrate_instance_finish(
8050             context, instance, migration, provider_mappings=None)
8051 
8052         network_info = self.network_api.get_instance_nw_info(context, instance)
8053         self._notify_about_instance_usage(
8054                      context, instance, "live_migration.post.dest.start",
8055                      network_info=network_info)
8056         compute_utils.notify_about_instance_action(context, instance,
8057                 self.host,
8058                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8059                 phase=fields.NotificationPhase.START)
8060         block_device_info = self._get_instance_block_device_info(context,
8061                                                                  instance)
8062         # Allocate the claimed PCI resources at destination.
8063         self.rt.allocate_pci_devices_for_instance(context, instance)
8064 
8065         try:
8066             self.driver.post_live_migration_at_destination(
8067                 context, instance, network_info, block_migration,
8068                 block_device_info)
8069         except Exception:
8070             with excutils.save_and_reraise_exception():
8071                 instance.vm_state = vm_states.ERROR
8072                 LOG.error('Unexpected error during post live migration at '
8073                           'destination host.', instance=instance)
8074         finally:
8075             # Restore instance state and update host
8076             current_power_state = self._get_power_state(context, instance)
8077             node_name = None
8078             prev_host = instance.host
8079             try:
8080                 compute_node = self._get_compute_info(context, self.host)
8081                 node_name = compute_node.hypervisor_hostname
8082             except exception.ComputeHostNotFound:
8083                 LOG.exception('Failed to get compute_info for %s', self.host)
8084             finally:
8085                 # NOTE(artom) We need to apply the migration context here
8086                 # regardless of whether the driver's
8087                 # post_live_migration_at_destination succeeded or not: the
8088                 # instance is on the destination, potentially with a new NUMA
8089                 # topology and resource usage. We need to persist that.
8090                 # NOTE(artom) Apply followed by drop looks weird, but apply
8091                 # just saves the new fields while drop actually removes the
8092                 # migration context from the instance.
8093                 instance.apply_migration_context()
8094                 instance.drop_migration_context()
8095                 instance.host = self.host
8096                 instance.power_state = current_power_state
8097                 instance.task_state = None
8098                 instance.node = node_name
8099                 instance.progress = 0
8100                 instance.save(expected_task_state=task_states.MIGRATING)
8101 
8102         # NOTE(tr3buchet): tear down networks on source host (nova-net)
8103         # NOTE(mriedem): For neutron, this will delete any inactive source
8104         # host port bindings.
8105         try:
8106             self.network_api.setup_networks_on_host(context, instance,
8107                                                     prev_host, teardown=True)
8108         except exception.PortBindingDeletionFailed as e:
8109             # Removing the inactive port bindings from the source host is not
8110             # critical so just log an error but don't fail.
8111             LOG.error('Network cleanup failed for source host %s during post '
8112                       'live migration. You may need to manually clean up '
8113                       'resources in the network service. Error: %s',
8114                       prev_host, six.text_type(e))
8115         # NOTE(vish): this is necessary to update dhcp for nova-network
8116         # NOTE(mriedem): This is a no-op for neutron.
8117         self.network_api.setup_networks_on_host(context, instance, self.host)
8118         self._notify_about_instance_usage(
8119                      context, instance, "live_migration.post.dest.end",
8120                      network_info=network_info)
8121         compute_utils.notify_about_instance_action(context, instance,
8122                 self.host,
8123                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8124                 phase=fields.NotificationPhase.END)
8125 
8126     def _remove_remote_volume_connections(self, context, dest, bdms, instance):
8127         """Rollback remote volume connections on the dest"""
8128         for bdm in bdms:
8129             try:
8130                 # remove the connection on the destination host
8131                 # NOTE(lyarwood): This actually calls the cinderv2
8132                 # os-terminate_connection API if required.
8133                 self.compute_rpcapi.remove_volume_connection(
8134                         context, instance, bdm.volume_id, dest)
8135             except Exception:
8136                 LOG.warning("Ignoring exception while attempting "
8137                             "to rollback volume connections for "
8138                             "volume %s on host %s.", bdm.volume_id,
8139                             dest, instance=instance)
8140 
8141     def _rollback_volume_bdms(self, context, bdms, original_bdms, instance):
8142         """Rollback the connection_info and attachment_id for each bdm"""
8143         original_bdms_by_volid = {bdm.volume_id: bdm for bdm in original_bdms
8144                                   if bdm.is_volume}
8145         for bdm in bdms:
8146             try:
8147                 original_bdm = original_bdms_by_volid[bdm.volume_id]
8148                 if bdm.attachment_id and original_bdm.attachment_id:
8149                     # NOTE(lyarwood): 3.44 cinder api flow. Delete the
8150                     # attachment used by the bdm and reset it to that of
8151                     # the original bdm.
8152                     self.volume_api.attachment_delete(context,
8153                                                       bdm.attachment_id)
8154                     bdm.attachment_id = original_bdm.attachment_id
8155                 # NOTE(lyarwood): Reset the connection_info to the original
8156                 bdm.connection_info = original_bdm.connection_info
8157                 bdm.save()
8158             except cinder_exception.ClientException:
8159                 LOG.warning("Ignoring cinderclient exception when "
8160                             "attempting to delete attachment %s for volume "
8161                             "%s while rolling back volume bdms.",
8162                             bdm.attachment_id, bdm.volume_id,
8163                             instance=instance)
8164             except Exception:
8165                 with excutils.save_and_reraise_exception():
8166                     LOG.exception("Exception while attempting to rollback "
8167                                   "BDM for volume %s.", bdm.volume_id,
8168                                   instance=instance)
8169 
8170     @wrap_exception()
8171     @wrap_instance_fault
8172     def _rollback_live_migration(self, context, instance,
8173                                  dest, migrate_data=None,
8174                                  migration_status='error',
8175                                  source_bdms=None):
8176         """Recovers Instance/volume state from migrating -> running.
8177 
8178         :param context: security context
8179         :param instance: nova.objects.instance.Instance object
8180         :param dest:
8181             This method is called from live migration src host.
8182             This param specifies destination host.
8183         :param migrate_data:
8184             if not none, contains implementation specific data.
8185         :param migration_status:
8186             Contains the status we want to set for the migration object
8187         :param source_bdms: BDMs prior to modification by the destination
8188                             compute host. Set by _do_live_migration and not
8189                             part of the callback interface, so this is never
8190                             None
8191 
8192         """
8193         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
8194               migrate_data.obj_attr_is_set('migration')):
8195             migration = migrate_data.migration
8196         else:
8197             migration = None
8198 
8199         if migration:
8200             # Remove allocations created in Placement for the dest node.
8201             # If migration is None, the virt driver didn't pass it which is
8202             # a bug.
8203             self._revert_allocation(context, instance, migration)
8204         else:
8205             LOG.error('Unable to revert allocations during live migration '
8206                       'rollback; compute driver did not provide migrate_data',
8207                       instance=instance)
8208 
8209         # TODO(artom) drop_move_claim_at_destination() is new in RPC 5.3, only
8210         # call it if we performed a NUMA-aware live migration (which implies us
8211         # being able to send RPC 5.3). To check this, we can use the
8212         # src_supports_numa_live_migration flag, as it will be set if and only
8213         # if:
8214         # - dst_supports_numa_live_migration made its way to the source
8215         #   (meaning both dest and source are new and conductor can speak
8216         #   RPC 5.3)
8217         # - src_supports_numa_live_migration was set by the source driver and
8218         #   passed the send-RPC-5.3 check.
8219         # This check can be removed in RPC 6.0.
8220         if ('src_supports_numa_live_migration' in migrate_data and
8221                 migrate_data.src_supports_numa_live_migration):
8222             LOG.debug('Calling destination to drop move claim.',
8223                       instance=instance)
8224             self.compute_rpcapi.drop_move_claim_at_destination(context,
8225                                                                instance, dest)
8226         instance.task_state = None
8227         instance.progress = 0
8228         instance.drop_migration_context()
8229         instance.save(expected_task_state=[task_states.MIGRATING])
8230 
8231         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
8232         #                  for nova-network)
8233         # NOTE(mriedem): This is a no-op for neutron.
8234         self.network_api.setup_networks_on_host(context, instance, self.host)
8235         self.driver.rollback_live_migration_at_source(context, instance,
8236                                                       migrate_data)
8237 
8238         # NOTE(lyarwood): Fetch the current list of BDMs, disconnect any
8239         # connected volumes from the dest and delete any volume attachments
8240         # used by the destination host before rolling back to the original
8241         # still valid source host volume attachments.
8242         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8243                 context, instance.uuid)
8244         # TODO(lyarwood): Turn the following into a lookup method within
8245         # BlockDeviceMappingList.
8246         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
8247         self._remove_remote_volume_connections(context, dest, vol_bdms,
8248                                                instance)
8249         self._rollback_volume_bdms(context, vol_bdms, source_bdms, instance)
8250 
8251         self._notify_about_instance_usage(context, instance,
8252                                           "live_migration._rollback.start")
8253         compute_utils.notify_about_instance_action(context, instance,
8254                 self.host,
8255                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8256                 phase=fields.NotificationPhase.START,
8257                 bdms=bdms)
8258 
8259         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8260                 migrate_data)
8261 
8262         if do_cleanup:
8263             self.compute_rpcapi.rollback_live_migration_at_destination(
8264                     context, instance, dest, destroy_disks=destroy_disks,
8265                     migrate_data=migrate_data)
8266         elif utils.is_neutron():
8267             # The port binding profiles need to be cleaned up.
8268             with errors_out_migration_ctxt(migration):
8269                 try:
8270                     # This call will delete any inactive destination host
8271                     # port bindings.
8272                     self.network_api.setup_networks_on_host(
8273                         context, instance, host=dest, teardown=True)
8274                 except exception.PortBindingDeletionFailed as e:
8275                     # Removing the inactive port bindings from the destination
8276                     # host is not critical so just log an error but don't fail.
8277                     LOG.error(
8278                         'Network cleanup failed for destination host %s '
8279                         'during live migration rollback. You may need to '
8280                         'manually clean up resources in the network service. '
8281                         'Error: %s', dest, six.text_type(e))
8282                 except Exception:
8283                     with excutils.save_and_reraise_exception():
8284                         LOG.exception(
8285                             'An error occurred while cleaning up networking '
8286                             'during live migration rollback.',
8287                             instance=instance)
8288 
8289         self._notify_about_instance_usage(context, instance,
8290                                           "live_migration._rollback.end")
8291         compute_utils.notify_about_instance_action(context, instance,
8292                 self.host,
8293                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8294                 phase=fields.NotificationPhase.END,
8295                 bdms=bdms)
8296 
8297         self._set_migration_status(migration, migration_status)
8298 
8299     @wrap_exception()
8300     @wrap_instance_fault
8301     def drop_move_claim_at_destination(self, context, instance):
8302         """Called by the source of a live migration during rollback to ask the
8303         destination to drop the MoveClaim object that was created for the live
8304         migration on the destination.
8305         """
8306         nodename = self._get_nodename(instance)
8307         LOG.debug('Dropping live migration resource claim on destination '
8308                   'node %s', nodename, instance=instance)
8309         self.rt.drop_move_claim(
8310             context, instance, nodename, instance_type=instance.flavor)
8311 
8312     @wrap_exception()
8313     @wrap_instance_event(prefix='compute')
8314     @wrap_instance_fault
8315     def rollback_live_migration_at_destination(self, context, instance,
8316                                                destroy_disks,
8317                                                migrate_data):
8318         """Cleaning up image directory that is created pre_live_migration.
8319 
8320         :param context: security context
8321         :param instance: a nova.objects.instance.Instance object sent over rpc
8322         :param destroy_disks: whether to destroy volumes or not
8323         :param migrate_data: contains migration info
8324         """
8325         network_info = self.network_api.get_instance_nw_info(context, instance)
8326         self._notify_about_instance_usage(
8327                       context, instance, "live_migration.rollback.dest.start",
8328                       network_info=network_info)
8329         compute_utils.notify_about_instance_action(
8330             context, instance, self.host,
8331             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8332             phase=fields.NotificationPhase.START)
8333         try:
8334             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
8335             # NOTE(mriedem): For neutron, this call will delete any
8336             # destination host port bindings.
8337             # TODO(mriedem): We should eventually remove this call from
8338             # this method (rollback_live_migration_at_destination) since this
8339             # method is only called conditionally based on whether or not the
8340             # instance is running on shared storage. _rollback_live_migration
8341             # already calls this method for neutron if we are running on
8342             # shared storage.
8343             self.network_api.setup_networks_on_host(context, instance,
8344                                                     self.host, teardown=True)
8345         except exception.PortBindingDeletionFailed as e:
8346             # Removing the inactive port bindings from the destination
8347             # host is not critical so just log an error but don't fail.
8348             LOG.error(
8349                 'Network cleanup failed for destination host %s '
8350                 'during live migration rollback. You may need to '
8351                 'manually clean up resources in the network service. '
8352                 'Error: %s', self.host, six.text_type(e))
8353         except Exception:
8354             with excutils.save_and_reraise_exception():
8355                 # NOTE(tdurakov): even if teardown networks fails driver
8356                 # should try to rollback live migration on destination.
8357                 LOG.exception('An error occurred while deallocating network.',
8358                               instance=instance)
8359         finally:
8360             # always run this even if setup_networks_on_host fails
8361             # NOTE(vish): The mapping is passed in so the driver can disconnect
8362             #             from remote volumes if necessary
8363             block_device_info = self._get_instance_block_device_info(context,
8364                                                                      instance)
8365             # free any instance PCI claims done on destination during
8366             # check_can_live_migrate_destination()
8367             self.rt.free_pci_device_claims_for_instance(context, instance)
8368 
8369             self.driver.rollback_live_migration_at_destination(
8370                 context, instance, network_info, block_device_info,
8371                 destroy_disks=destroy_disks, migrate_data=migrate_data)
8372 
8373         self._notify_about_instance_usage(
8374                         context, instance, "live_migration.rollback.dest.end",
8375                         network_info=network_info)
8376         compute_utils.notify_about_instance_action(
8377             context, instance, self.host,
8378             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8379             phase=fields.NotificationPhase.END)
8380 
8381     def _require_nw_info_update(self, context, instance):
8382         """Detect whether there is a mismatch in binding:host_id, or
8383         binding_failed or unbound binding:vif_type for any of the instances
8384         ports.
8385         """
8386         # Only update port bindings if compute manager does manage port
8387         # bindings instead of the compute driver. For example IronicDriver
8388         # manages the port binding for baremetal instance ports, hence,
8389         # external intervention with the binding is not desired.
8390         if (not utils.is_neutron() or
8391                 self.driver.manages_network_binding_host_id()):
8392             return False
8393 
8394         search_opts = {'device_id': instance.uuid,
8395                        'fields': ['binding:host_id', 'binding:vif_type']}
8396         ports = self.network_api.list_ports(context, **search_opts)
8397         for p in ports['ports']:
8398             if p.get('binding:host_id') != self.host:
8399                 return True
8400             vif_type = p.get('binding:vif_type')
8401             if (vif_type == network_model.VIF_TYPE_UNBOUND or
8402                     vif_type == network_model.VIF_TYPE_BINDING_FAILED):
8403                 return True
8404         return False
8405 
8406     @periodic_task.periodic_task(
8407         spacing=CONF.heal_instance_info_cache_interval)
8408     def _heal_instance_info_cache(self, context):
8409         """Called periodically.  On every call, try to update the
8410         info_cache's network information for another instance by
8411         calling to the network manager.
8412 
8413         This is implemented by keeping a cache of uuids of instances
8414         that live on this host.  On each call, we pop one off of a
8415         list, pull the DB record, and try the call to the network API.
8416         If anything errors don't fail, as it's possible the instance
8417         has been deleted, etc.
8418         """
8419         heal_interval = CONF.heal_instance_info_cache_interval
8420         if not heal_interval:
8421             return
8422 
8423         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
8424         instance = None
8425 
8426         LOG.debug('Starting heal instance info cache')
8427 
8428         if not instance_uuids:
8429             # The list of instances to heal is empty so rebuild it
8430             LOG.debug('Rebuilding the list of instances to heal')
8431             db_instances = objects.InstanceList.get_by_host(
8432                 context, self.host, expected_attrs=[], use_slave=True)
8433             for inst in db_instances:
8434                 # We don't want to refresh the cache for instances
8435                 # which are building or deleting so don't put them
8436                 # in the list. If they are building they will get
8437                 # added to the list next time we build it.
8438                 if (inst.vm_state == vm_states.BUILDING):
8439                     LOG.debug('Skipping network cache update for instance '
8440                               'because it is Building.', instance=inst)
8441                     continue
8442                 if (inst.task_state == task_states.DELETING):
8443                     LOG.debug('Skipping network cache update for instance '
8444                               'because it is being deleted.', instance=inst)
8445                     continue
8446 
8447                 if not instance:
8448                     # Save the first one we find so we don't
8449                     # have to get it again
8450                     instance = inst
8451                 else:
8452                     instance_uuids.append(inst['uuid'])
8453 
8454             self._instance_uuids_to_heal = instance_uuids
8455         else:
8456             # Find the next valid instance on the list
8457             while instance_uuids:
8458                 try:
8459                     inst = objects.Instance.get_by_uuid(
8460                             context, instance_uuids.pop(0),
8461                             expected_attrs=['system_metadata', 'info_cache',
8462                                             'flavor'],
8463                             use_slave=True)
8464                 except exception.InstanceNotFound:
8465                     # Instance is gone.  Try to grab another.
8466                     continue
8467 
8468                 # Check the instance hasn't been migrated
8469                 if inst.host != self.host:
8470                     LOG.debug('Skipping network cache update for instance '
8471                               'because it has been migrated to another '
8472                               'host.', instance=inst)
8473                 # Check the instance isn't being deleting
8474                 elif inst.task_state == task_states.DELETING:
8475                     LOG.debug('Skipping network cache update for instance '
8476                               'because it is being deleted.', instance=inst)
8477                 else:
8478                     instance = inst
8479                     break
8480 
8481         if instance:
8482             # We have an instance now to refresh
8483             try:
8484                 # Fix potential mismatch in port binding if evacuation failed
8485                 # after reassigning the port binding to the dest host but
8486                 # before the instance host is changed.
8487                 # Do this only when instance has no pending task.
8488                 if instance.task_state is None and \
8489                         self._require_nw_info_update(context, instance):
8490                     LOG.info("Updating ports in neutron", instance=instance)
8491                     self.network_api.setup_instance_network_on_host(
8492                         context, instance, self.host)
8493                 # Call to network API to get instance info.. this will
8494                 # force an update to the instance's info_cache
8495                 self.network_api.get_instance_nw_info(
8496                     context, instance, force_refresh=True)
8497                 LOG.debug('Updated the network info_cache for instance',
8498                           instance=instance)
8499             except exception.InstanceNotFound:
8500                 # Instance is gone.
8501                 LOG.debug('Instance no longer exists. Unable to refresh',
8502                           instance=instance)
8503                 return
8504             except exception.InstanceInfoCacheNotFound:
8505                 # InstanceInfoCache is gone.
8506                 LOG.debug('InstanceInfoCache no longer exists. '
8507                           'Unable to refresh', instance=instance)
8508             except Exception:
8509                 LOG.error('An error occurred while refreshing the network '
8510                           'cache.', instance=instance, exc_info=True)
8511         else:
8512             LOG.debug("Didn't find any instances for network info cache "
8513                       "update.")
8514 
8515     @periodic_task.periodic_task
8516     def _poll_rebooting_instances(self, context):
8517         if CONF.reboot_timeout > 0:
8518             filters = {'task_state':
8519                        [task_states.REBOOTING,
8520                         task_states.REBOOT_STARTED,
8521                         task_states.REBOOT_PENDING],
8522                        'host': self.host}
8523             rebooting = objects.InstanceList.get_by_filters(
8524                 context, filters, expected_attrs=[], use_slave=True)
8525 
8526             to_poll = []
8527             for instance in rebooting:
8528                 if timeutils.is_older_than(instance.updated_at,
8529                                            CONF.reboot_timeout):
8530                     to_poll.append(instance)
8531 
8532             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
8533 
8534     @periodic_task.periodic_task
8535     def _poll_rescued_instances(self, context):
8536         if CONF.rescue_timeout > 0:
8537             filters = {'vm_state': vm_states.RESCUED,
8538                        'host': self.host}
8539             rescued_instances = objects.InstanceList.get_by_filters(
8540                 context, filters, expected_attrs=["system_metadata"],
8541                 use_slave=True)
8542 
8543             to_unrescue = []
8544             for instance in rescued_instances:
8545                 if timeutils.is_older_than(instance.launched_at,
8546                                            CONF.rescue_timeout):
8547                     to_unrescue.append(instance)
8548 
8549             for instance in to_unrescue:
8550                 self.compute_api.unrescue(context, instance)
8551 
8552     @periodic_task.periodic_task
8553     def _poll_unconfirmed_resizes(self, context):
8554         if CONF.resize_confirm_window == 0:
8555             return
8556 
8557         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
8558                 context, CONF.resize_confirm_window, self.host,
8559                 use_slave=True)
8560 
8561         migrations_info = dict(migration_count=len(migrations),
8562                 confirm_window=CONF.resize_confirm_window)
8563 
8564         if migrations_info["migration_count"] > 0:
8565             LOG.info("Found %(migration_count)d unconfirmed migrations "
8566                      "older than %(confirm_window)d seconds",
8567                      migrations_info)
8568 
8569         def _set_migration_to_error(migration, reason, **kwargs):
8570             LOG.warning("Setting migration %(migration_id)s to error: "
8571                         "%(reason)s",
8572                         {'migration_id': migration['id'], 'reason': reason},
8573                         **kwargs)
8574             migration.status = 'error'
8575             migration.save()
8576 
8577         for migration in migrations:
8578             instance_uuid = migration.instance_uuid
8579             LOG.info("Automatically confirming migration "
8580                      "%(migration_id)s for instance %(instance_uuid)s",
8581                      {'migration_id': migration.id,
8582                       'instance_uuid': instance_uuid})
8583             expected_attrs = ['metadata', 'system_metadata']
8584             try:
8585                 instance = objects.Instance.get_by_uuid(context,
8586                             instance_uuid, expected_attrs=expected_attrs,
8587                             use_slave=True)
8588             except exception.InstanceNotFound:
8589                 reason = (_("Instance %s not found") %
8590                           instance_uuid)
8591                 _set_migration_to_error(migration, reason)
8592                 continue
8593             if instance.vm_state == vm_states.ERROR:
8594                 reason = _("In ERROR state")
8595                 _set_migration_to_error(migration, reason,
8596                                         instance=instance)
8597                 continue
8598             # race condition: The instance in DELETING state should not be
8599             # set the migration state to error, otherwise the instance in
8600             # to be deleted which is in RESIZED state
8601             # will not be able to confirm resize
8602             if instance.task_state in [task_states.DELETING,
8603                                        task_states.SOFT_DELETING]:
8604                 msg = ("Instance being deleted or soft deleted during resize "
8605                        "confirmation. Skipping.")
8606                 LOG.debug(msg, instance=instance)
8607                 continue
8608 
8609             # race condition: This condition is hit when this method is
8610             # called between the save of the migration record with a status of
8611             # finished and the save of the instance object with a state of
8612             # RESIZED. The migration record should not be set to error.
8613             if instance.task_state == task_states.RESIZE_FINISH:
8614                 msg = ("Instance still resizing during resize "
8615                        "confirmation. Skipping.")
8616                 LOG.debug(msg, instance=instance)
8617                 continue
8618 
8619             vm_state = instance.vm_state
8620             task_state = instance.task_state
8621             if vm_state != vm_states.RESIZED or task_state is not None:
8622                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
8623                            "RESIZED/None") %
8624                           {'vm_state': vm_state,
8625                            'task_state': task_state})
8626                 _set_migration_to_error(migration, reason,
8627                                         instance=instance)
8628                 continue
8629             try:
8630                 self.compute_api.confirm_resize(context, instance,
8631                                                 migration=migration)
8632             except Exception as e:
8633                 LOG.info("Error auto-confirming resize: %s. "
8634                          "Will retry later.", e, instance=instance)
8635 
8636     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
8637     def _poll_shelved_instances(self, context):
8638 
8639         if CONF.shelved_offload_time <= 0:
8640             return
8641 
8642         filters = {'vm_state': vm_states.SHELVED,
8643                    'task_state': None,
8644                    'host': self.host}
8645         shelved_instances = objects.InstanceList.get_by_filters(
8646             context, filters=filters, expected_attrs=['system_metadata'],
8647             use_slave=True)
8648 
8649         to_gc = []
8650         for instance in shelved_instances:
8651             sys_meta = instance.system_metadata
8652             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
8653             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
8654                 to_gc.append(instance)
8655 
8656         for instance in to_gc:
8657             try:
8658                 instance.task_state = task_states.SHELVING_OFFLOADING
8659                 instance.save(expected_task_state=(None,))
8660                 self.shelve_offload_instance(context, instance,
8661                                              clean_shutdown=False)
8662             except Exception:
8663                 LOG.exception('Periodic task failed to offload instance.',
8664                               instance=instance)
8665 
8666     @periodic_task.periodic_task
8667     def _instance_usage_audit(self, context):
8668         if not CONF.instance_usage_audit:
8669             return
8670 
8671         begin, end = utils.last_completed_audit_period()
8672         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
8673                                self.host):
8674             return
8675 
8676         instances = objects.InstanceList.get_active_by_window_joined(
8677             context, begin, end, host=self.host,
8678             expected_attrs=['system_metadata', 'info_cache', 'metadata',
8679                             'flavor'],
8680             use_slave=True)
8681         num_instances = len(instances)
8682         errors = 0
8683         successes = 0
8684         LOG.info("Running instance usage audit for host %(host)s "
8685                  "from %(begin_time)s to %(end_time)s. "
8686                  "%(number_instances)s instances.",
8687                  {'host': self.host,
8688                   'begin_time': begin,
8689                   'end_time': end,
8690                   'number_instances': num_instances})
8691         start_time = time.time()
8692         task_log = objects.TaskLog(context)
8693         task_log.task_name = 'instance_usage_audit'
8694         task_log.period_beginning = begin
8695         task_log.period_ending = end
8696         task_log.host = self.host
8697         task_log.task_items = num_instances
8698         task_log.message = 'Instance usage audit started...'
8699         task_log.begin_task()
8700         for instance in instances:
8701             try:
8702                 compute_utils.notify_usage_exists(
8703                     self.notifier, context, instance, self.host,
8704                     ignore_missing_network_data=False)
8705                 successes += 1
8706             except Exception:
8707                 LOG.exception('Failed to generate usage '
8708                               'audit for instance '
8709                               'on host %s', self.host,
8710                               instance=instance)
8711                 errors += 1
8712         task_log.errors = errors
8713         task_log.message = (
8714             'Instance usage audit ran for host %s, %s instances in %s seconds.'
8715             % (self.host, num_instances, time.time() - start_time))
8716         task_log.end_task()
8717 
8718     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
8719     def _poll_bandwidth_usage(self, context):
8720 
8721         if not self._bw_usage_supported:
8722             return
8723 
8724         prev_time, start_time = utils.last_completed_audit_period()
8725 
8726         curr_time = time.time()
8727         if (curr_time - self._last_bw_usage_poll >
8728                 CONF.bandwidth_poll_interval):
8729             self._last_bw_usage_poll = curr_time
8730             LOG.info("Updating bandwidth usage cache")
8731 
8732             instances = objects.InstanceList.get_by_host(context,
8733                                                               self.host,
8734                                                               use_slave=True)
8735             try:
8736                 bw_counters = self.driver.get_all_bw_counters(instances)
8737             except NotImplementedError:
8738                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
8739                 # implemented yet.  If they don't it doesn't break anything,
8740                 # they just don't get the info in the usage events.
8741                 # NOTE(PhilDay): Record that its not supported so we can
8742                 # skip fast on future calls rather than waste effort getting
8743                 # the list of instances.
8744                 LOG.info("Bandwidth usage not supported by %(driver)s.",
8745                          {'driver': CONF.compute_driver})
8746                 self._bw_usage_supported = False
8747                 return
8748 
8749             refreshed = timeutils.utcnow()
8750             for bw_ctr in bw_counters:
8751                 # Allow switching of greenthreads between queries.
8752                 greenthread.sleep(0)
8753                 bw_in = 0
8754                 bw_out = 0
8755                 last_ctr_in = None
8756                 last_ctr_out = None
8757                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
8758                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
8759                     start_period=start_time, use_slave=True)
8760                 if usage:
8761                     bw_in = usage.bw_in
8762                     bw_out = usage.bw_out
8763                     last_ctr_in = usage.last_ctr_in
8764                     last_ctr_out = usage.last_ctr_out
8765                 else:
8766                     usage = (objects.BandwidthUsage.
8767                              get_by_instance_uuid_and_mac(
8768                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
8769                         start_period=prev_time, use_slave=True))
8770                     if usage:
8771                         last_ctr_in = usage.last_ctr_in
8772                         last_ctr_out = usage.last_ctr_out
8773 
8774                 if last_ctr_in is not None:
8775                     if bw_ctr['bw_in'] < last_ctr_in:
8776                         # counter rollover
8777                         bw_in += bw_ctr['bw_in']
8778                     else:
8779                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
8780 
8781                 if last_ctr_out is not None:
8782                     if bw_ctr['bw_out'] < last_ctr_out:
8783                         # counter rollover
8784                         bw_out += bw_ctr['bw_out']
8785                     else:
8786                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
8787 
8788                 objects.BandwidthUsage(context=context).create(
8789                                               bw_ctr['uuid'],
8790                                               bw_ctr['mac_address'],
8791                                               bw_in,
8792                                               bw_out,
8793                                               bw_ctr['bw_in'],
8794                                               bw_ctr['bw_out'],
8795                                               start_period=start_time,
8796                                               last_refreshed=refreshed)
8797 
8798     def _get_host_volume_bdms(self, context, use_slave=False):
8799         """Return all block device mappings on a compute host."""
8800         compute_host_bdms = []
8801         instances = objects.InstanceList.get_by_host(context, self.host,
8802             use_slave=use_slave)
8803         for instance in instances:
8804             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8805                     context, instance.uuid, use_slave=use_slave)
8806             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
8807             compute_host_bdms.append(dict(instance=instance,
8808                                           instance_bdms=instance_bdms))
8809 
8810         return compute_host_bdms
8811 
8812     def _update_volume_usage_cache(self, context, vol_usages):
8813         """Updates the volume usage cache table with a list of stats."""
8814         for usage in vol_usages:
8815             # Allow switching of greenthreads between queries.
8816             greenthread.sleep(0)
8817             vol_usage = objects.VolumeUsage(context)
8818             vol_usage.volume_id = usage['volume']
8819             vol_usage.instance_uuid = usage['instance'].uuid
8820             vol_usage.project_id = usage['instance'].project_id
8821             vol_usage.user_id = usage['instance'].user_id
8822             vol_usage.availability_zone = usage['instance'].availability_zone
8823             vol_usage.curr_reads = usage['rd_req']
8824             vol_usage.curr_read_bytes = usage['rd_bytes']
8825             vol_usage.curr_writes = usage['wr_req']
8826             vol_usage.curr_write_bytes = usage['wr_bytes']
8827             vol_usage.save()
8828             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
8829             compute_utils.notify_about_volume_usage(context, vol_usage,
8830                                                     self.host)
8831 
8832     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
8833     def _poll_volume_usage(self, context):
8834         if CONF.volume_usage_poll_interval == 0:
8835             return
8836 
8837         compute_host_bdms = self._get_host_volume_bdms(context,
8838                                                        use_slave=True)
8839         if not compute_host_bdms:
8840             return
8841 
8842         LOG.debug("Updating volume usage cache")
8843         try:
8844             vol_usages = self.driver.get_all_volume_usage(context,
8845                                                           compute_host_bdms)
8846         except NotImplementedError:
8847             return
8848 
8849         self._update_volume_usage_cache(context, vol_usages)
8850 
8851     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
8852                                  run_immediately=True)
8853     def _sync_power_states(self, context):
8854         """Align power states between the database and the hypervisor.
8855 
8856         To sync power state data we make a DB call to get the number of
8857         virtual machines known by the hypervisor and if the number matches the
8858         number of virtual machines known by the database, we proceed in a lazy
8859         loop, one database record at a time, checking if the hypervisor has the
8860         same power state as is in the database.
8861         """
8862         db_instances = objects.InstanceList.get_by_host(context, self.host,
8863                                                         expected_attrs=[],
8864                                                         use_slave=True)
8865 
8866         try:
8867             num_vm_instances = self.driver.get_num_instances()
8868         except exception.VirtDriverNotReady as e:
8869             # If the virt driver is not ready, like ironic-api not being up
8870             # yet in the case of ironic, just log it and exit.
8871             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
8872             return
8873 
8874         num_db_instances = len(db_instances)
8875 
8876         if num_vm_instances != num_db_instances:
8877             LOG.warning("While synchronizing instance power states, found "
8878                         "%(num_db_instances)s instances in the database "
8879                         "and %(num_vm_instances)s instances on the "
8880                         "hypervisor.",
8881                         {'num_db_instances': num_db_instances,
8882                          'num_vm_instances': num_vm_instances})
8883 
8884         def _sync(db_instance):
8885             # NOTE(melwitt): This must be synchronized as we query state from
8886             #                two separate sources, the driver and the database.
8887             #                They are set (in stop_instance) and read, in sync.
8888             @utils.synchronized(db_instance.uuid)
8889             def query_driver_power_state_and_sync():
8890                 self._query_driver_power_state_and_sync(context, db_instance)
8891 
8892             try:
8893                 query_driver_power_state_and_sync()
8894             except Exception:
8895                 LOG.exception("Periodic sync_power_state task had an "
8896                               "error while processing an instance.",
8897                               instance=db_instance)
8898 
8899             self._syncs_in_progress.pop(db_instance.uuid)
8900 
8901         for db_instance in db_instances:
8902             # process syncs asynchronously - don't want instance locking to
8903             # block entire periodic task thread
8904             uuid = db_instance.uuid
8905             if uuid in self._syncs_in_progress:
8906                 LOG.debug('Sync already in progress for %s', uuid)
8907             else:
8908                 LOG.debug('Triggering sync for uuid %s', uuid)
8909                 self._syncs_in_progress[uuid] = True
8910                 self._sync_power_pool.spawn_n(_sync, db_instance)
8911 
8912     def _query_driver_power_state_and_sync(self, context, db_instance):
8913         if db_instance.task_state is not None:
8914             LOG.info("During sync_power_state the instance has a "
8915                      "pending task (%(task)s). Skip.",
8916                      {'task': db_instance.task_state}, instance=db_instance)
8917             return
8918         # No pending tasks. Now try to figure out the real vm_power_state.
8919         try:
8920             vm_instance = self.driver.get_info(db_instance)
8921             vm_power_state = vm_instance.state
8922         except exception.InstanceNotFound:
8923             vm_power_state = power_state.NOSTATE
8924         # Note(maoy): the above get_info call might take a long time,
8925         # for example, because of a broken libvirt driver.
8926         try:
8927             self._sync_instance_power_state(context,
8928                                             db_instance,
8929                                             vm_power_state,
8930                                             use_slave=True)
8931         except exception.InstanceNotFound:
8932             # NOTE(hanlind): If the instance gets deleted during sync,
8933             # silently ignore.
8934             pass
8935 
8936     def _stop_unexpected_shutdown_instance(self, context, vm_state,
8937                                            db_instance, orig_db_power_state):
8938         # this is an exceptional case; make sure our data is up
8939         # to date before slamming through a power off
8940         vm_instance = self.driver.get_info(db_instance,
8941                                            use_cache=False)
8942         vm_power_state = vm_instance.state
8943 
8944         # if it still looks off, go ahead and call stop()
8945         if vm_power_state in (power_state.SHUTDOWN,
8946                               power_state.CRASHED):
8947 
8948             LOG.warning("Instance shutdown by itself. Calling the "
8949                         "stop API. Current vm_state: %(vm_state)s, "
8950                         "current task_state: %(task_state)s, "
8951                         "original DB power_state: %(db_power_state)s, "
8952                         "current VM power_state: %(vm_power_state)s",
8953                         {'vm_state': vm_state,
8954                          'task_state': db_instance.task_state,
8955                          'db_power_state': orig_db_power_state,
8956                          'vm_power_state': vm_power_state},
8957                         instance=db_instance)
8958             try:
8959                 # Note(maoy): here we call the API instead of
8960                 # brutally updating the vm_state in the database
8961                 # to allow all the hooks and checks to be performed.
8962                 if db_instance.shutdown_terminate:
8963                     self.compute_api.delete(context, db_instance)
8964                 else:
8965                     self.compute_api.stop(context, db_instance)
8966             except Exception:
8967                 # Note(maoy): there is no need to propagate the error
8968                 # because the same power_state will be retrieved next
8969                 # time and retried.
8970                 # For example, there might be another task scheduled.
8971                 LOG.exception("error during stop() in sync_power_state.",
8972                               instance=db_instance)
8973 
8974     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
8975                                    use_slave=False):
8976         """Align instance power state between the database and hypervisor.
8977 
8978         If the instance is not found on the hypervisor, but is in the database,
8979         then a stop() API will be called on the instance.
8980         """
8981 
8982         # We re-query the DB to get the latest instance info to minimize
8983         # (not eliminate) race condition.
8984         db_instance.refresh(use_slave=use_slave)
8985         db_power_state = db_instance.power_state
8986         vm_state = db_instance.vm_state
8987 
8988         if self.host != db_instance.host:
8989             # on the sending end of nova-compute _sync_power_state
8990             # may have yielded to the greenthread performing a live
8991             # migration; this in turn has changed the resident-host
8992             # for the VM; However, the instance is still active, it
8993             # is just in the process of migrating to another host.
8994             # This implies that the compute source must relinquish
8995             # control to the compute destination.
8996             LOG.info("During the sync_power process the "
8997                      "instance has moved from "
8998                      "host %(src)s to host %(dst)s",
8999                      {'src': db_instance.host,
9000                       'dst': self.host},
9001                      instance=db_instance)
9002             return
9003         elif db_instance.task_state is not None:
9004             # on the receiving end of nova-compute, it could happen
9005             # that the DB instance already report the new resident
9006             # but the actual VM has not showed up on the hypervisor
9007             # yet. In this case, let's allow the loop to continue
9008             # and run the state sync in a later round
9009             LOG.info("During sync_power_state the instance has a "
9010                      "pending task (%(task)s). Skip.",
9011                      {'task': db_instance.task_state},
9012                      instance=db_instance)
9013             return
9014 
9015         orig_db_power_state = db_power_state
9016         if vm_power_state != db_power_state:
9017             LOG.info('During _sync_instance_power_state the DB '
9018                      'power_state (%(db_power_state)s) does not match '
9019                      'the vm_power_state from the hypervisor '
9020                      '(%(vm_power_state)s). Updating power_state in the '
9021                      'DB to match the hypervisor.',
9022                      {'db_power_state': db_power_state,
9023                       'vm_power_state': vm_power_state},
9024                      instance=db_instance)
9025             # power_state is always updated from hypervisor to db
9026             db_instance.power_state = vm_power_state
9027             db_instance.save()
9028             db_power_state = vm_power_state
9029 
9030         # Note(maoy): Now resolve the discrepancy between vm_state and
9031         # vm_power_state. We go through all possible vm_states.
9032         if vm_state in (vm_states.BUILDING,
9033                         vm_states.RESCUED,
9034                         vm_states.RESIZED,
9035                         vm_states.SUSPENDED,
9036                         vm_states.ERROR):
9037             # TODO(maoy): we ignore these vm_state for now.
9038             pass
9039         elif vm_state == vm_states.ACTIVE:
9040             # The only rational power state should be RUNNING
9041             if vm_power_state in (power_state.SHUTDOWN,
9042                                   power_state.CRASHED):
9043                 self._stop_unexpected_shutdown_instance(
9044                     context, vm_state, db_instance, orig_db_power_state)
9045             elif vm_power_state == power_state.SUSPENDED:
9046                 LOG.warning("Instance is suspended unexpectedly. Calling "
9047                             "the stop API.", instance=db_instance)
9048                 try:
9049                     self.compute_api.stop(context, db_instance)
9050                 except Exception:
9051                     LOG.exception("error during stop() in sync_power_state.",
9052                                   instance=db_instance)
9053             elif vm_power_state == power_state.PAUSED:
9054                 # Note(maoy): a VM may get into the paused state not only
9055                 # because the user request via API calls, but also
9056                 # due to (temporary) external instrumentations.
9057                 # Before the virt layer can reliably report the reason,
9058                 # we simply ignore the state discrepancy. In many cases,
9059                 # the VM state will go back to running after the external
9060                 # instrumentation is done. See bug 1097806 for details.
9061                 LOG.warning("Instance is paused unexpectedly. Ignore.",
9062                             instance=db_instance)
9063             elif vm_power_state == power_state.NOSTATE:
9064                 # Occasionally, depending on the status of the hypervisor,
9065                 # which could be restarting for example, an instance may
9066                 # not be found.  Therefore just log the condition.
9067                 LOG.warning("Instance is unexpectedly not found. Ignore.",
9068                             instance=db_instance)
9069         elif vm_state == vm_states.STOPPED:
9070             if vm_power_state not in (power_state.NOSTATE,
9071                                       power_state.SHUTDOWN,
9072                                       power_state.CRASHED):
9073                 LOG.warning("Instance is not stopped. Calling "
9074                             "the stop API. Current vm_state: %(vm_state)s,"
9075                             " current task_state: %(task_state)s, "
9076                             "original DB power_state: %(db_power_state)s, "
9077                             "current VM power_state: %(vm_power_state)s",
9078                             {'vm_state': vm_state,
9079                              'task_state': db_instance.task_state,
9080                              'db_power_state': orig_db_power_state,
9081                              'vm_power_state': vm_power_state},
9082                             instance=db_instance)
9083                 try:
9084                     # NOTE(russellb) Force the stop, because normally the
9085                     # compute API would not allow an attempt to stop a stopped
9086                     # instance.
9087                     self.compute_api.force_stop(context, db_instance)
9088                 except Exception:
9089                     LOG.exception("error during stop() in sync_power_state.",
9090                                   instance=db_instance)
9091         elif vm_state == vm_states.PAUSED:
9092             if vm_power_state in (power_state.SHUTDOWN,
9093                                   power_state.CRASHED):
9094                 LOG.warning("Paused instance shutdown by itself. Calling "
9095                             "the stop API.", instance=db_instance)
9096                 try:
9097                     self.compute_api.force_stop(context, db_instance)
9098                 except Exception:
9099                     LOG.exception("error during stop() in sync_power_state.",
9100                                   instance=db_instance)
9101         elif vm_state in (vm_states.SOFT_DELETED,
9102                           vm_states.DELETED):
9103             if vm_power_state not in (power_state.NOSTATE,
9104                                       power_state.SHUTDOWN):
9105                 # Note(maoy): this should be taken care of periodically in
9106                 # _cleanup_running_deleted_instances().
9107                 LOG.warning("Instance is not (soft-)deleted.",
9108                             instance=db_instance)
9109 
9110     @periodic_task.periodic_task
9111     def _reclaim_queued_deletes(self, context):
9112         """Reclaim instances that are queued for deletion."""
9113         interval = CONF.reclaim_instance_interval
9114         if interval <= 0:
9115             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
9116             return
9117 
9118         filters = {'vm_state': vm_states.SOFT_DELETED,
9119                    'task_state': None,
9120                    'host': self.host}
9121         instances = objects.InstanceList.get_by_filters(
9122             context, filters,
9123             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
9124             use_slave=True)
9125         for instance in instances:
9126             if self._deleted_old_enough(instance, interval):
9127                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9128                         context, instance.uuid)
9129                 LOG.info('Reclaiming deleted instance', instance=instance)
9130                 try:
9131                     self._delete_instance(context, instance, bdms)
9132                 except Exception as e:
9133                     LOG.warning("Periodic reclaim failed to delete "
9134                                 "instance: %s",
9135                                 e, instance=instance)
9136 
9137     def _get_nodename(self, instance, refresh=False):
9138         """Helper method to get the name of the first available node
9139         on this host. This method should not be used with any operations
9140         on ironic instances since it does not handle multiple nodes.
9141         """
9142         node = self.driver.get_available_nodes(refresh=refresh)[0]
9143         LOG.debug("No node specified, defaulting to %s", node,
9144                   instance=instance)
9145         return node
9146 
9147     def _update_available_resource_for_node(self, context, nodename,
9148                                             startup=False):
9149 
9150         try:
9151             self.rt.update_available_resource(context, nodename,
9152                                               startup=startup)
9153         except exception.ComputeHostNotFound:
9154             LOG.warning("Compute node '%s' not found in "
9155                         "update_available_resource.", nodename)
9156         except exception.ReshapeFailed:
9157             # We're only supposed to get here on startup, if a reshape was
9158             # needed, was attempted, and failed. We want to kill the service.
9159             with excutils.save_and_reraise_exception():
9160                 LOG.critical("Resource provider data migration failed "
9161                              "fatally during startup for node %s.", nodename)
9162         except exception.ReshapeNeeded:
9163             # This exception should only find its way here if the virt driver's
9164             # update_provider_tree raised it incorrectly: either
9165             # a) After the resource tracker already caught it once and
9166             # reinvoked update_provider_tree with allocations. At this point
9167             # the driver is just supposed to *do* the reshape, so if it raises
9168             # ReshapeNeeded, it's a bug, and we want to kill the compute
9169             # service.
9170             # b) On periodic rather than startup (we only allow reshapes to
9171             # happen on startup). In this case we'll just make the logs red and
9172             # go again at the next periodic interval, where the same thing may
9173             # or may not happen again. Depending on the previous and intended
9174             # shape of the providers/inventories, this may not actually cause
9175             # any immediately visible symptoms (in terms of scheduling, etc.)
9176             # If this becomes a problem, we may wish to make it pop immediately
9177             # (e.g. disable the service).
9178             with excutils.save_and_reraise_exception():
9179                 LOG.exception("ReshapeNeeded exception is unexpected here!")
9180         except Exception:
9181             LOG.exception("Error updating resources for node %(node)s.",
9182                           {'node': nodename})
9183 
9184     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
9185     def update_available_resource(self, context, startup=False):
9186         """See driver.get_available_resource()
9187 
9188         Periodic process that keeps that the compute host's understanding of
9189         resource availability and usage in sync with the underlying hypervisor.
9190 
9191         :param context: security context
9192         :param startup: True if this is being called when the nova-compute
9193             service is starting, False otherwise.
9194         """
9195         try:
9196             nodenames = set(self.driver.get_available_nodes())
9197         except exception.VirtDriverNotReady:
9198             LOG.warning("Virt driver is not ready.")
9199             return
9200 
9201         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
9202                                                             nodenames,
9203                                                             use_slave=True,
9204                                                             startup=startup)
9205 
9206         # Delete orphan compute node not reported by driver but still in db
9207         for cn in compute_nodes_in_db:
9208             if cn.hypervisor_hostname not in nodenames:
9209                 LOG.info("Deleting orphan compute node %(id)s "
9210                          "hypervisor host is %(hh)s, "
9211                          "nodes are %(nodes)s",
9212                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
9213                           'nodes': nodenames})
9214                 cn.destroy()
9215                 self.rt.remove_node(cn.hypervisor_hostname)
9216                 # Delete the corresponding resource provider in placement,
9217                 # along with any associated allocations.
9218                 try:
9219                     self.reportclient.delete_resource_provider(context, cn,
9220                                                                cascade=True)
9221                 except keystone_exception.ClientException as e:
9222                     LOG.error(
9223                         "Failed to delete compute node resource provider "
9224                         "for compute node %s: %s", cn.uuid, six.text_type(e))
9225 
9226         for nodename in nodenames:
9227             self._update_available_resource_for_node(context, nodename,
9228                                                      startup=startup)
9229 
9230     def _get_compute_nodes_in_db(self, context, nodenames, use_slave=False,
9231                                  startup=False):
9232         try:
9233             return objects.ComputeNodeList.get_all_by_host(context, self.host,
9234                                                            use_slave=use_slave)
9235         except exception.NotFound:
9236             # If the driver is not reporting any nodenames we should not
9237             # expect there to be compute nodes so we just return in that case.
9238             # For example, this could be an ironic compute and it is not
9239             # managing any nodes yet.
9240             if nodenames:
9241                 if startup:
9242                     LOG.warning(
9243                         "No compute node record found for host %s. If this is "
9244                         "the first time this service is starting on this "
9245                         "host, then you can ignore this warning.", self.host)
9246                 else:
9247                     LOG.error("No compute node record for host %s", self.host)
9248             return []
9249 
9250     @periodic_task.periodic_task(
9251         spacing=CONF.running_deleted_instance_poll_interval,
9252         run_immediately=True)
9253     def _cleanup_running_deleted_instances(self, context):
9254         """Cleanup any instances which are erroneously still running after
9255         having been deleted.
9256 
9257         Valid actions to take are:
9258 
9259             1. noop - do nothing
9260             2. log - log which instances are erroneously running
9261             3. reap - shutdown and cleanup any erroneously running instances
9262             4. shutdown - power off *and disable* any erroneously running
9263                           instances
9264 
9265         The use-case for this cleanup task is: for various reasons, it may be
9266         possible for the database to show an instance as deleted but for that
9267         instance to still be running on a host machine (see bug
9268         https://bugs.launchpad.net/nova/+bug/911366).
9269 
9270         This cleanup task is a cross-hypervisor utility for finding these
9271         zombied instances and either logging the discrepancy (likely what you
9272         should do in production), or automatically reaping the instances (more
9273         appropriate for dev environments).
9274         """
9275         action = CONF.running_deleted_instance_action
9276 
9277         if action == "noop":
9278             return
9279 
9280         # NOTE(sirp): admin contexts don't ordinarily return deleted records
9281         with utils.temporary_mutation(context, read_deleted="yes"):
9282 
9283             try:
9284                 instances = self._running_deleted_instances(context)
9285             except exception.VirtDriverNotReady:
9286                 # Since this task runs immediately on startup, if the
9287                 # hypervisor is not yet ready handle it gracefully.
9288                 LOG.debug('Unable to check for running deleted instances '
9289                           'at this time since the hypervisor is not ready.')
9290                 return
9291 
9292             for instance in instances:
9293                 if action == "log":
9294                     LOG.warning("Detected instance with name label "
9295                                 "'%s' which is marked as "
9296                                 "DELETED but still present on host.",
9297                                 instance.name, instance=instance)
9298 
9299                 elif action == 'shutdown':
9300                     LOG.info("Powering off instance with name label "
9301                              "'%s' which is marked as "
9302                              "DELETED but still present on host.",
9303                              instance.name, instance=instance)
9304                     try:
9305                         try:
9306                             # disable starting the instance
9307                             self.driver.set_bootable(instance, False)
9308                         except NotImplementedError:
9309                             LOG.debug("set_bootable is not implemented "
9310                                       "for the current driver")
9311                         # and power it off
9312                         self.driver.power_off(instance)
9313                     except Exception:
9314                         LOG.warning("Failed to power off instance",
9315                                     instance=instance, exc_info=True)
9316 
9317                 elif action == 'reap':
9318                     LOG.info("Destroying instance with name label "
9319                              "'%s' which is marked as "
9320                              "DELETED but still present on host.",
9321                              instance.name, instance=instance)
9322                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9323                         context, instance.uuid, use_slave=True)
9324                     self.instance_events.clear_events_for_instance(instance)
9325                     try:
9326                         self._shutdown_instance(context, instance, bdms,
9327                                                 notify=False)
9328                         self._cleanup_volumes(context, instance, bdms,
9329                                               detach=False)
9330                     except Exception as e:
9331                         LOG.warning("Periodic cleanup failed to delete "
9332                                     "instance: %s",
9333                                     e, instance=instance)
9334                 else:
9335                     raise Exception(_("Unrecognized value '%s'"
9336                                       " for CONF.running_deleted_"
9337                                       "instance_action") % action)
9338 
9339     def _running_deleted_instances(self, context):
9340         """Returns a list of instances nova thinks is deleted,
9341         but the hypervisor thinks is still running.
9342         """
9343         timeout = CONF.running_deleted_instance_timeout
9344         filters = {'deleted': True,
9345                    'soft_deleted': False}
9346         instances = self._get_instances_on_driver(context, filters)
9347         return [i for i in instances if self._deleted_old_enough(i, timeout)]
9348 
9349     def _deleted_old_enough(self, instance, timeout):
9350         deleted_at = instance.deleted_at
9351         if deleted_at:
9352             deleted_at = deleted_at.replace(tzinfo=None)
9353         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
9354 
9355     @contextlib.contextmanager
9356     def _error_out_instance_on_exception(self, context, instance,
9357                                          instance_state=vm_states.ACTIVE):
9358         """Context manager to set instance.vm_state after some operation raises
9359 
9360         Used to handle NotImplementedError and InstanceFaultRollback errors
9361         and reset the instance vm_state and task_state. The vm_state is set
9362         to the $instance_state parameter and task_state is set to None.
9363         For all other types of exceptions, the vm_state is set to ERROR and
9364         the task_state is left unchanged (although most callers will have the
9365         @reverts_task_state decorator which will set the task_state to None).
9366 
9367         Re-raises the original exception *except* in the case of
9368         InstanceFaultRollback in which case the wrapped `inner_exception` is
9369         re-raised.
9370 
9371         :param context: The nova auth request context for the operation.
9372         :param instance: The instance to update. The vm_state will be set by
9373             this context manager when an exception is raised.
9374         :param instance_state: For NotImplementedError and
9375             InstanceFaultRollback this is the vm_state to set the instance to
9376             when handling one of those types of exceptions. By default the
9377             instance will be set to ACTIVE, but the caller should control this
9378             in case there have been no changes to the running state of the
9379             instance. For example, resizing a stopped server where prep_resize
9380             fails early and does not change the power state of the guest should
9381             not set the instance status to ACTIVE but remain STOPPED.
9382             This parameter is ignored for all other types of exceptions and the
9383             instance vm_state is set to ERROR.
9384         """
9385         # NOTE(mriedem): Why doesn't this method just save off the
9386         # original instance.vm_state here rather than use a parameter? Or use
9387         # instance_state=None as an override but default to the current
9388         # vm_state when rolling back.
9389         instance_uuid = instance.uuid
9390         try:
9391             yield
9392         except (NotImplementedError, exception.InstanceFaultRollback) as error:
9393             # Use reraise=False to determine if we want to raise the original
9394             # exception or something else.
9395             with excutils.save_and_reraise_exception(reraise=False) as ctxt:
9396                 LOG.info("Setting instance back to %(state)s after: %(error)s",
9397                          {'state': instance_state, 'error': error},
9398                          instance_uuid=instance_uuid)
9399                 self._instance_update(context, instance,
9400                                       vm_state=instance_state,
9401                                       task_state=None)
9402                 if isinstance(error, exception.InstanceFaultRollback):
9403                     # Raise the wrapped exception.
9404                     raise error.inner_exception
9405                 # Else re-raise the NotImplementedError.
9406                 ctxt.reraise = True
9407         except Exception:
9408             LOG.exception('Setting instance vm_state to ERROR',
9409                           instance_uuid=instance_uuid)
9410             with excutils.save_and_reraise_exception():
9411                 # NOTE(mriedem): Why don't we pass clean_task_state=True here?
9412                 self._set_instance_obj_error_state(context, instance)
9413 
9414     @wrap_exception()
9415     def add_aggregate_host(self, context, aggregate, host, slave_info):
9416         """Notify hypervisor of change (for hypervisor pools)."""
9417         try:
9418             self.driver.add_to_aggregate(context, aggregate, host,
9419                                          slave_info=slave_info)
9420         except NotImplementedError:
9421             LOG.debug('Hypervisor driver does not support '
9422                       'add_aggregate_host')
9423         except exception.AggregateError:
9424             with excutils.save_and_reraise_exception():
9425                 self.driver.undo_aggregate_operation(
9426                                     context,
9427                                     aggregate.delete_host,
9428                                     aggregate, host)
9429 
9430     @wrap_exception()
9431     def remove_aggregate_host(self, context, host, slave_info, aggregate):
9432         """Removes a host from a physical hypervisor pool."""
9433         try:
9434             self.driver.remove_from_aggregate(context, aggregate, host,
9435                                               slave_info=slave_info)
9436         except NotImplementedError:
9437             LOG.debug('Hypervisor driver does not support '
9438                       'remove_aggregate_host')
9439         except (exception.AggregateError,
9440                 exception.InvalidAggregateAction) as e:
9441             with excutils.save_and_reraise_exception():
9442                 self.driver.undo_aggregate_operation(
9443                                     context,
9444                                     aggregate.add_host,
9445                                     aggregate, host,
9446                                     isinstance(e, exception.AggregateError))
9447 
9448     def _process_instance_event(self, instance, event):
9449         _event = self.instance_events.pop_instance_event(instance, event)
9450         if _event:
9451             LOG.debug('Processing event %(event)s',
9452                       {'event': event.key}, instance=instance)
9453             _event.send(event)
9454         else:
9455             # If it's a network-vif-unplugged event and the instance is being
9456             # deleted or live migrated then we don't need to make this a
9457             # warning as it's expected. There are other expected things which
9458             # could trigger this event like detaching an interface, but we
9459             # don't have a task state for that.
9460             # TODO(mriedem): We have other move operations and things like
9461             # hard reboot (probably rebuild as well) which trigger this event
9462             # but nothing listens for network-vif-unplugged. We should either
9463             # handle those other known cases or consider just not logging a
9464             # warning if we get this event and the instance is undergoing some
9465             # task state transition.
9466             if (event.name == 'network-vif-unplugged' and
9467                     instance.task_state in (
9468                         task_states.DELETING, task_states.MIGRATING)):
9469                 LOG.debug('Received event %s for instance with task_state %s.',
9470                           event.key, instance.task_state, instance=instance)
9471             else:
9472                 LOG.warning('Received unexpected event %(event)s for '
9473                             'instance with vm_state %(vm_state)s and '
9474                             'task_state %(task_state)s.',
9475                             {'event': event.key,
9476                              'vm_state': instance.vm_state,
9477                              'task_state': instance.task_state},
9478                             instance=instance)
9479 
9480     def _process_instance_vif_deleted_event(self, context, instance,
9481                                             deleted_vif_id):
9482         # If an attached port is deleted by neutron, it needs to
9483         # be detached from the instance.
9484         # And info cache needs to be updated.
9485         network_info = instance.info_cache.network_info
9486         for index, vif in enumerate(network_info):
9487             if vif['id'] == deleted_vif_id:
9488                 LOG.info('Neutron deleted interface %(intf)s; '
9489                          'detaching it from the instance and '
9490                          'deleting it from the info cache',
9491                          {'intf': vif['id']},
9492                          instance=instance)
9493                 profile = vif.get('profile', {}) or {}  # profile can be None
9494                 if profile.get('allocation'):
9495                     LOG.error(
9496                         'The bound port %(port_id)s is deleted in Neutron but '
9497                         'the resource allocation on the resource provider '
9498                         '%(rp_uuid)s is leaked until the server '
9499                         '%(server_uuid)s is deleted.',
9500                         {'port_id': vif['id'],
9501                          'rp_uuid': vif['profile']['allocation'],
9502                          'server_uuid': instance.uuid})
9503 
9504                 del network_info[index]
9505                 base_net_api.update_instance_cache_with_nw_info(
9506                                  self.network_api, context,
9507                                  instance,
9508                                  nw_info=network_info)
9509                 try:
9510                     self.driver.detach_interface(context, instance, vif)
9511                 except NotImplementedError:
9512                     # Not all virt drivers support attach/detach of interfaces
9513                     # yet (like Ironic), so just ignore this.
9514                     pass
9515                 except exception.NovaException as ex:
9516                     # If the instance was deleted before the interface was
9517                     # detached, just log it at debug.
9518                     log_level = (logging.DEBUG
9519                                  if isinstance(ex, exception.InstanceNotFound)
9520                                  else logging.WARNING)
9521                     LOG.log(log_level,
9522                             "Detach interface failed, "
9523                             "port_id=%(port_id)s, reason: %(msg)s",
9524                             {'port_id': deleted_vif_id, 'msg': ex},
9525                             instance=instance)
9526                 break
9527 
9528     @wrap_instance_event(prefix='compute')
9529     @wrap_instance_fault
9530     def extend_volume(self, context, instance, extended_volume_id):
9531 
9532         # If an attached volume is extended by cinder, it needs to
9533         # be extended by virt driver so host can detect its new size.
9534         # And bdm needs to be updated.
9535         LOG.debug('Handling volume-extended event for volume %(vol)s',
9536                   {'vol': extended_volume_id}, instance=instance)
9537 
9538         try:
9539             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
9540                    context, extended_volume_id, instance.uuid)
9541         except exception.NotFound:
9542             LOG.warning('Extend volume failed, '
9543                         'volume %(vol)s is not attached to instance.',
9544                         {'vol': extended_volume_id},
9545                         instance=instance)
9546             return
9547 
9548         LOG.info('Cinder extended volume %(vol)s; '
9549                  'extending it to detect new size',
9550                  {'vol': extended_volume_id},
9551                  instance=instance)
9552         volume = self.volume_api.get(context, bdm.volume_id)
9553 
9554         if bdm.connection_info is None:
9555             LOG.warning('Extend volume failed, '
9556                         'attached volume %(vol)s has no connection_info',
9557                         {'vol': extended_volume_id},
9558                         instance=instance)
9559             return
9560 
9561         connection_info = jsonutils.loads(bdm.connection_info)
9562         bdm.volume_size = volume['size']
9563         bdm.save()
9564 
9565         if not self.driver.capabilities.get('supports_extend_volume', False):
9566             raise exception.ExtendVolumeNotSupported()
9567 
9568         try:
9569             self.driver.extend_volume(connection_info,
9570                                       instance,
9571                                       bdm.volume_size * units.Gi)
9572         except Exception as ex:
9573             LOG.warning('Extend volume failed, '
9574                         'volume_id=%(volume_id)s, reason: %(msg)s',
9575                         {'volume_id': extended_volume_id, 'msg': ex},
9576                         instance=instance)
9577             raise
9578 
9579     @staticmethod
9580     def _is_state_valid_for_power_update_event(instance, target_power_state):
9581         """Check if the current state of the instance allows it to be
9582         a candidate for the power-update event.
9583 
9584         :param instance: The nova instance object.
9585         :param target_power_state: The desired target power state; this should
9586                                    either be "POWER_ON" or "POWER_OFF".
9587         :returns Boolean: True if the instance can be subjected to the
9588                           power-update event.
9589         """
9590         if ((target_power_state == external_event_obj.POWER_ON and
9591                 instance.task_state is None and
9592                 instance.vm_state == vm_states.STOPPED and
9593                 instance.power_state == power_state.SHUTDOWN) or
9594             (target_power_state == external_event_obj.POWER_OFF and
9595                 instance.task_state is None and
9596                 instance.vm_state == vm_states.ACTIVE and
9597                 instance.power_state == power_state.RUNNING)):
9598             return True
9599         return False
9600 
9601     @wrap_exception()
9602     @reverts_task_state
9603     @wrap_instance_event(prefix='compute')
9604     @wrap_instance_fault
9605     def power_update(self, context, instance, target_power_state):
9606         """Power update of an instance prompted by an external event.
9607         :param context: The API request context.
9608         :param instance: The nova instance object.
9609         :param target_power_state: The desired target power state;
9610                                    this should either be "POWER_ON" or
9611                                    "POWER_OFF".
9612         """
9613 
9614         @utils.synchronized(instance.uuid)
9615         def do_power_update():
9616             LOG.debug('Handling power-update event with target_power_state %s '
9617                       'for instance', target_power_state, instance=instance)
9618             if not self._is_state_valid_for_power_update_event(
9619                     instance, target_power_state):
9620                 pow_state = fields.InstancePowerState.from_index(
9621                     instance.power_state)
9622                 LOG.info('The power-update %(tag)s event for instance '
9623                          '%(uuid)s is a no-op since the instance is in '
9624                          'vm_state %(vm_state)s, task_state '
9625                          '%(task_state)s and power_state '
9626                          '%(power_state)s.',
9627                          {'tag': target_power_state, 'uuid': instance.uuid,
9628                          'vm_state': instance.vm_state,
9629                          'task_state': instance.task_state,
9630                          'power_state': pow_state})
9631                 return
9632             LOG.debug("Trying to %s instance",
9633                       target_power_state, instance=instance)
9634             if target_power_state == external_event_obj.POWER_ON:
9635                 action = fields.NotificationAction.POWER_ON
9636                 notification_name = "power_on."
9637                 instance.task_state = task_states.POWERING_ON
9638             else:
9639                 # It's POWER_OFF
9640                 action = fields.NotificationAction.POWER_OFF
9641                 notification_name = "power_off."
9642                 instance.task_state = task_states.POWERING_OFF
9643                 instance.progress = 0
9644 
9645             try:
9646                 # Note that the task_state is set here rather than the API
9647                 # because this is a best effort operation and deferring
9648                 # updating the task_state until we get to the compute service
9649                 # avoids error handling in the API and needing to account for
9650                 # older compute services during rolling upgrades from Stein.
9651                 # If we lose a race, UnexpectedTaskStateError is handled
9652                 # below.
9653                 instance.save(expected_task_state=[None])
9654                 self._notify_about_instance_usage(context, instance,
9655                                                   notification_name + "start")
9656                 compute_utils.notify_about_instance_action(context, instance,
9657                     self.host, action=action,
9658                     phase=fields.NotificationPhase.START)
9659                 # UnexpectedTaskStateError raised from the driver will be
9660                 # handled below and not result in a fault, error notification
9661                 # or failure of the instance action. Other driver errors like
9662                 # NotImplementedError will be record a fault, send an error
9663                 # notification and mark the instance action as failed.
9664                 self.driver.power_update_event(instance, target_power_state)
9665                 self._notify_about_instance_usage(context, instance,
9666                                                   notification_name + "end")
9667                 compute_utils.notify_about_instance_action(context, instance,
9668                     self.host, action=action,
9669                     phase=fields.NotificationPhase.END)
9670             except exception.UnexpectedTaskStateError as e:
9671                 # Handling the power-update event is best effort and if we lost
9672                 # a race with some other action happening to the instance we
9673                 # just log it and return rather than fail the action.
9674                 LOG.info("The power-update event was possibly preempted: %s ",
9675                          e.format_message(), instance=instance)
9676                 return
9677         do_power_update()
9678 
9679     @wrap_exception()
9680     def external_instance_event(self, context, instances, events):
9681         # NOTE(danms): Some event types are handled by the manager, such
9682         # as when we're asked to update the instance's info_cache. If it's
9683         # not one of those, look for some thread(s) waiting for the event and
9684         # unblock them if so.
9685         for event in events:
9686             instance = [inst for inst in instances
9687                         if inst.uuid == event.instance_uuid][0]
9688             LOG.debug('Received event %(event)s',
9689                       {'event': event.key},
9690                       instance=instance)
9691             if event.name == 'network-changed':
9692                 try:
9693                     LOG.debug('Refreshing instance network info cache due to '
9694                               'event %s.', event.key, instance=instance)
9695                     self.network_api.get_instance_nw_info(
9696                         context, instance, refresh_vif_id=event.tag)
9697                 except exception.NotFound as e:
9698                     LOG.info('Failed to process external instance event '
9699                              '%(event)s due to: %(error)s',
9700                              {'event': event.key, 'error': six.text_type(e)},
9701                              instance=instance)
9702             elif event.name == 'network-vif-deleted':
9703                 try:
9704                     self._process_instance_vif_deleted_event(context,
9705                                                              instance,
9706                                                              event.tag)
9707                 except exception.NotFound as e:
9708                     LOG.info('Failed to process external instance event '
9709                              '%(event)s due to: %(error)s',
9710                              {'event': event.key, 'error': six.text_type(e)},
9711                              instance=instance)
9712             elif event.name == 'volume-extended':
9713                 self.extend_volume(context, instance, event.tag)
9714             elif event.name == 'power-update':
9715                 self.power_update(context, instance, event.tag)
9716             else:
9717                 self._process_instance_event(instance, event)
9718 
9719     @periodic_task.periodic_task(spacing=CONF.image_cache.manager_interval,
9720                                  external_process_ok=True)
9721     def _run_image_cache_manager_pass(self, context):
9722         """Run a single pass of the image cache manager."""
9723 
9724         if not self.driver.capabilities.get("has_imagecache", False):
9725             return
9726 
9727         # Determine what other nodes use this storage
9728         storage_users.register_storage_use(CONF.instances_path, CONF.host)
9729         nodes = storage_users.get_storage_users(CONF.instances_path)
9730 
9731         # Filter all_instances to only include those nodes which share this
9732         # storage path.
9733         # TODO(mikal): this should be further refactored so that the cache
9734         # cleanup code doesn't know what those instances are, just a remote
9735         # count, and then this logic should be pushed up the stack.
9736         filters = {'deleted': False,
9737                    'soft_deleted': True,
9738                    'host': nodes}
9739         filtered_instances = objects.InstanceList.get_by_filters(context,
9740                                  filters, expected_attrs=[], use_slave=True)
9741 
9742         self.driver.manage_image_cache(context, filtered_instances)
9743 
9744     def cache_images(self, context, image_ids):
9745         """Ask the virt driver to pre-cache a set of base images.
9746 
9747         :param context: The RequestContext
9748         :param image_ids: The image IDs to be cached
9749         :return: A dict, keyed by image-id where the values are one of:
9750                  'cached' if the image was downloaded,
9751                  'existing' if the image was already in the cache,
9752                  'unsupported' if the virt driver does not support caching,
9753                  'error' if the virt driver raised an exception.
9754         """
9755 
9756         results = {}
9757 
9758         LOG.info('Caching %i image(s) by request', len(image_ids))
9759         for image_id in image_ids:
9760             try:
9761                 cached = self.driver.cache_image(context, image_id)
9762                 if cached:
9763                     results[image_id] = 'cached'
9764                 else:
9765                     results[image_id] = 'existing'
9766             except NotImplementedError:
9767                 LOG.warning('Virt driver does not support image pre-caching;'
9768                             ' ignoring request')
9769                 # NOTE(danms): Yes, technically we could short-circuit here to
9770                 # avoid trying the rest of the images, but it's very cheap to
9771                 # just keep hitting the NotImplementedError to keep the logic
9772                 # clean.
9773                 results[image_id] = 'unsupported'
9774             except Exception as e:
9775                 results[image_id] = 'error'
9776                 LOG.error('Failed to cache image %(image_id)s: %(err)s',
9777                           {'image_id': image_id,
9778                            'err': e})
9779 
9780         return results
9781 
9782     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
9783     def _run_pending_deletes(self, context):
9784         """Retry any pending instance file deletes."""
9785         LOG.debug('Cleaning up deleted instances')
9786         filters = {'deleted': True,
9787                    'soft_deleted': False,
9788                    'host': CONF.host,
9789                    'cleaned': False}
9790         attrs = ['system_metadata']
9791         with utils.temporary_mutation(context, read_deleted='yes'):
9792             instances = objects.InstanceList.get_by_filters(
9793                 context, filters, expected_attrs=attrs, use_slave=True)
9794         LOG.debug('There are %d instances to clean', len(instances))
9795 
9796         for instance in instances:
9797             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
9798             LOG.debug('Instance has had %(attempts)s of %(max)s '
9799                       'cleanup attempts',
9800                       {'attempts': attempts,
9801                        'max': CONF.maximum_instance_delete_attempts},
9802                       instance=instance)
9803             if attempts < CONF.maximum_instance_delete_attempts:
9804                 success = self.driver.delete_instance_files(instance)
9805 
9806                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
9807                 if success:
9808                     instance.cleaned = True
9809                 with utils.temporary_mutation(context, read_deleted='yes'):
9810                     instance.save()
9811 
9812     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
9813     def _cleanup_incomplete_migrations(self, context):
9814         """Delete instance files on failed resize/revert-resize operation
9815 
9816         During resize/revert-resize operation, if that instance gets deleted
9817         in-between then instance files might remain either on source or
9818         destination compute node because of race condition.
9819         """
9820         LOG.debug('Cleaning up deleted instances with incomplete migration ')
9821         migration_filters = {'host': CONF.host,
9822                              'status': 'error'}
9823         migrations = objects.MigrationList.get_by_filters(context,
9824                                                           migration_filters)
9825 
9826         if not migrations:
9827             return
9828 
9829         inst_uuid_from_migrations = set([migration.instance_uuid for migration
9830                                          in migrations])
9831 
9832         inst_filters = {'deleted': True, 'soft_deleted': False,
9833                         'uuid': inst_uuid_from_migrations}
9834         attrs = ['info_cache', 'security_groups', 'system_metadata']
9835         with utils.temporary_mutation(context, read_deleted='yes'):
9836             instances = objects.InstanceList.get_by_filters(
9837                 context, inst_filters, expected_attrs=attrs, use_slave=True)
9838 
9839         for instance in instances:
9840             if instance.host != CONF.host:
9841                 for migration in migrations:
9842                     if instance.uuid == migration.instance_uuid:
9843                         # Delete instance files if not cleanup properly either
9844                         # from the source or destination compute nodes when
9845                         # the instance is deleted during resizing.
9846                         self.driver.delete_instance_files(instance)
9847                         try:
9848                             migration.status = 'failed'
9849                             migration.save()
9850                         except exception.MigrationNotFound:
9851                             LOG.warning("Migration %s is not found.",
9852                                         migration.id,
9853                                         instance=instance)
9854                         break
9855 
9856     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
9857                                    exception.QemuGuestAgentNotEnabled,
9858                                    exception.NovaException,
9859                                    NotImplementedError)
9860     @wrap_exception()
9861     def quiesce_instance(self, context, instance):
9862         """Quiesce an instance on this host."""
9863         context = context.elevated()
9864         image_meta = objects.ImageMeta.from_instance(instance)
9865         self.driver.quiesce(context, instance, image_meta)
9866 
9867     def _wait_for_snapshots_completion(self, context, mapping):
9868         for mapping_dict in mapping:
9869             if mapping_dict.get('source_type') == 'snapshot':
9870 
9871                 def _wait_snapshot():
9872                     snapshot = self.volume_api.get_snapshot(
9873                         context, mapping_dict['snapshot_id'])
9874                     if snapshot.get('status') != 'creating':
9875                         raise loopingcall.LoopingCallDone()
9876 
9877                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
9878                 timer.start(interval=0.5).wait()
9879 
9880     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
9881                                    exception.QemuGuestAgentNotEnabled,
9882                                    exception.NovaException,
9883                                    NotImplementedError)
9884     @wrap_exception()
9885     def unquiesce_instance(self, context, instance, mapping=None):
9886         """Unquiesce an instance on this host.
9887 
9888         If snapshots' image mapping is provided, it waits until snapshots are
9889         completed before unqueiscing.
9890         """
9891         context = context.elevated()
9892         if mapping:
9893             try:
9894                 self._wait_for_snapshots_completion(context, mapping)
9895             except Exception as error:
9896                 LOG.exception("Exception while waiting completion of "
9897                               "volume snapshots: %s",
9898                               error, instance=instance)
9899         image_meta = objects.ImageMeta.from_instance(instance)
9900         self.driver.unquiesce(context, instance, image_meta)
9901 
9902     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
9903     def _cleanup_expired_console_auth_tokens(self, context):
9904         """Remove all expired console auth tokens.
9905 
9906         Console authorization tokens and their connection data are stored
9907         in the database when a user asks for a console connection to an
9908         instance. After a time they expire. We periodically remove any expired
9909         tokens from the database.
9910         """
9911         objects.ConsoleAuthToken.clean_expired_console_auths(context)
9912 
9913     def _claim_pci_for_instance_vifs(self, ctxt, instance):
9914         """Claim PCI devices for the instance's VIFs on the compute node
9915 
9916         :param ctxt: Context
9917         :param instance: Instance object
9918         :return: <port ID: PciDevice> mapping for the VIFs that yielded a
9919                 PCI claim on the compute node
9920         """
9921         pci_req_id_to_port_id = {}
9922         pci_reqs = []
9923         port_id_to_pci_dev = {}
9924 
9925         for vif in instance.get_network_info():
9926             pci_req = pci_req_module.get_instance_pci_request_from_vif(
9927                 ctxt,
9928                 instance,
9929                 vif)
9930             if pci_req:
9931                 pci_req_id_to_port_id[pci_req.request_id] = vif['id']
9932                 pci_reqs.append(pci_req)
9933 
9934         if pci_reqs:
9935             # Create PCI requests and claim against PCI resource tracker
9936             # NOTE(adrianc): We claim against the same requests as on the
9937             # source node.
9938             vif_pci_requests = objects.InstancePCIRequests(
9939                 requests=pci_reqs,
9940                 instance_uuid=instance.uuid)
9941 
9942             claimed_pci_devices_objs = self.rt.claim_pci_devices(
9943                 ctxt,
9944                 vif_pci_requests)
9945 
9946             # Update VIFMigrateData profile with the newly claimed PCI
9947             # device
9948             for pci_dev in claimed_pci_devices_objs:
9949                 LOG.debug("PCI device: %s Claimed on destination node",
9950                           pci_dev.address)
9951                 port_id = pci_req_id_to_port_id[pci_dev.request_id]
9952                 port_id_to_pci_dev[port_id] = pci_dev
9953 
9954         return port_id_to_pci_dev
9955 
9956     def _update_migrate_vifs_profile_with_pci(self,
9957                                               migrate_vifs,
9958                                               port_id_to_pci_dev):
9959         """Update migrate vifs profile with the claimed PCI devices
9960 
9961         :param migrate_vifs: list of VIFMigrateData objects
9962         :param port_id_to_pci_dev: a <port_id: PciDevice> mapping
9963         :return: None.
9964         """
9965         for mig_vif in migrate_vifs:
9966             port_id = mig_vif.port_id
9967             if port_id not in port_id_to_pci_dev:
9968                 continue
9969 
9970             pci_dev = port_id_to_pci_dev[port_id]
9971             profile = copy.deepcopy(mig_vif.source_vif['profile'])
9972             profile['pci_slot'] = pci_dev.address
9973             profile['pci_vendor_info'] = ':'.join([pci_dev.vendor_id,
9974                                                    pci_dev.product_id])
9975             mig_vif.profile = profile
9976             LOG.debug("Updating migrate VIF profile for port %(port_id)s:"
9977                       "%(profile)s", {'port_id': port_id,
9978                                       'profile': profile})
