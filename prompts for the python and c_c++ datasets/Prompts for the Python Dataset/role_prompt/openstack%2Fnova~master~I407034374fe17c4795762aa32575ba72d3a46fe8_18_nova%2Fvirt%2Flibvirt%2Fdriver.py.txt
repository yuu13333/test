I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import collections
29 from collections import deque
30 import contextlib
31 import errno
32 import functools
33 import glob
34 import itertools
35 import operator
36 import os
37 import pwd
38 import shutil
39 import tempfile
40 import time
41 import uuid
42 
43 from castellan import key_manager
44 import eventlet
45 from eventlet import greenthread
46 from eventlet import tpool
47 from lxml import etree
48 from os_brick import encryptors
49 from os_brick import exception as brick_exception
50 from os_brick.initiator import connector
51 from oslo_concurrency import processutils
52 from oslo_log import log as logging
53 from oslo_serialization import jsonutils
54 from oslo_service import loopingcall
55 from oslo_utils import encodeutils
56 from oslo_utils import excutils
57 from oslo_utils import fileutils
58 from oslo_utils import importutils
59 from oslo_utils import strutils
60 from oslo_utils import timeutils
61 from oslo_utils import units
62 from oslo_utils import uuidutils
63 import six
64 from six.moves import range
65 
66 from nova.api.metadata import base as instance_metadata
67 from nova import block_device
68 from nova.compute import power_state
69 from nova.compute import task_states
70 from nova.compute import utils as compute_utils
71 import nova.conf
72 from nova.console import serial as serial_console
73 from nova.console import type as ctype
74 from nova import context as nova_context
75 from nova import exception
76 from nova.i18n import _
77 from nova import image
78 from nova.network import model as network_model
79 from nova import objects
80 from nova.objects import diagnostics as diagnostics_obj
81 from nova.objects import fields
82 from nova.objects import migrate_data as migrate_data_obj
83 from nova.pci import manager as pci_manager
84 from nova.pci import utils as pci_utils
85 import nova.privsep.libvirt
86 import nova.privsep.path
87 from nova import utils
88 from nova import version
89 from nova.virt import block_device as driver_block_device
90 from nova.virt import configdrive
91 from nova.virt.disk import api as disk_api
92 from nova.virt.disk.vfs import guestfs
93 from nova.virt import driver
94 from nova.virt import firewall
95 from nova.virt import hardware
96 from nova.virt.image import model as imgmodel
97 from nova.virt import images
98 from nova.virt.libvirt import blockinfo
99 from nova.virt.libvirt import config as vconfig
100 from nova.virt.libvirt import firewall as libvirt_firewall
101 from nova.virt.libvirt import guest as libvirt_guest
102 from nova.virt.libvirt import host
103 from nova.virt.libvirt import imagebackend
104 from nova.virt.libvirt import imagecache
105 from nova.virt.libvirt import instancejobtracker
106 from nova.virt.libvirt import migration as libvirt_migrate
107 from nova.virt.libvirt.storage import dmcrypt
108 from nova.virt.libvirt.storage import lvm
109 from nova.virt.libvirt.storage import rbd_utils
110 from nova.virt.libvirt import utils as libvirt_utils
111 from nova.virt.libvirt import vif as libvirt_vif
112 from nova.virt.libvirt.volume import mount
113 from nova.virt.libvirt.volume import remotefs
114 from nova.virt import netutils
115 from nova.volume import cinder
116 
117 libvirt = None
118 
119 uefi_logged = False
120 
121 LOG = logging.getLogger(__name__)
122 
123 CONF = nova.conf.CONF
124 
125 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
126     libvirt_firewall.__name__,
127     libvirt_firewall.IptablesFirewallDriver.__name__)
128 
129 DEFAULT_UEFI_LOADER_PATH = {
130     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
131     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
132 }
133 
134 MAX_CONSOLE_BYTES = 100 * units.Ki
135 
136 # The libvirt driver will prefix any disable reason codes with this string.
137 DISABLE_PREFIX = 'AUTO: '
138 # Disable reason for the service which was enabled or disabled without reason
139 DISABLE_REASON_UNDEFINED = None
140 
141 # Guest config console string
142 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
143 
144 GuestNumaConfig = collections.namedtuple(
145     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
146 
147 InjectionInfo = collections.namedtuple(
148     'InjectionInfo', ['network_info', 'files', 'admin_pass'])
149 
150 libvirt_volume_drivers = [
151     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
152     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
153     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
154     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
155     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
156     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
157     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
158     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
159     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
160     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
161     'fibre_channel='
162         'nova.virt.libvirt.volume.fibrechannel.'
163         'LibvirtFibreChannelVolumeDriver',
164     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
165     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
166     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
167     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
168     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
169     'vzstorage='
170         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
171     'veritas_hyperscale='
172         'nova.virt.libvirt.volume.vrtshyperscale.'
173         'LibvirtHyperScaleVolumeDriver',
174 ]
175 
176 
177 def patch_tpool_proxy():
178     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
179     or __repr__() calls. See bug #962840 for details.
180     We perform a monkey patch to replace those two instance methods.
181     """
182     def str_method(self):
183         return str(self._obj)
184 
185     def repr_method(self):
186         return repr(self._obj)
187 
188     tpool.Proxy.__str__ = str_method
189     tpool.Proxy.__repr__ = repr_method
190 
191 
192 patch_tpool_proxy()
193 
194 # For information about when MIN_LIBVIRT_VERSION and
195 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
196 #
197 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
198 #
199 # Currently this is effectively the min version for i686/x86_64
200 # + KVM/QEMU, as other architectures/hypervisors require newer
201 # versions. Over time, this will become a common min version
202 # for all architectures/hypervisors, as this value rises to
203 # meet them.
204 MIN_LIBVIRT_VERSION = (1, 2, 9)
205 MIN_QEMU_VERSION = (2, 1, 0)
206 # TODO(berrange): Re-evaluate this at start of each release cycle
207 # to decide if we want to plan a future min version bump.
208 # MIN_LIBVIRT_VERSION can be updated to match this after
209 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
210 # one cycle
211 NEXT_MIN_LIBVIRT_VERSION = (1, 3, 1)
212 NEXT_MIN_QEMU_VERSION = (2, 5, 0)
213 
214 # When the above version matches/exceeds this version
215 # delete it & corresponding code using it
216 # Libvirt version 1.2.17 is required for successful block live migration
217 # of vm booted from image with attached devices
218 MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION = (1, 2, 17)
219 # PowerPC based hosts that support NUMA using libvirt
220 MIN_LIBVIRT_NUMA_VERSION_PPC = (1, 2, 19)
221 # Versions of libvirt with known NUMA topology issues
222 # See bug #1449028
223 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
224 # Versions of libvirt with broken cpu pinning support. This excludes
225 # versions of libvirt with broken NUMA support since pinning needs
226 # NUMA
227 # See bug #1438226
228 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
229 
230 # Virtuozzo driver support
231 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
232 MIN_LIBVIRT_VIRTUOZZO_VERSION = (1, 2, 12)
233 
234 # Ability to set the user guest password with Qemu
235 MIN_LIBVIRT_SET_ADMIN_PASSWD = (1, 2, 16)
236 
237 # Ability to set the user guest password with parallels
238 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
239 
240 # s/390 & s/390x architectures with KVM
241 MIN_LIBVIRT_KVM_S390_VERSION = (1, 2, 13)
242 MIN_QEMU_S390_VERSION = (2, 3, 0)
243 
244 # libvirt < 1.3 reported virt_functions capability
245 # only when VFs are enabled.
246 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
247 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
248 
249 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
250 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
251 MIN_QEMU_VIRTLOGD = (2, 7, 0)
252 
253 # ppc64/ppc64le architectures with KVM
254 # NOTE(rfolco): Same levels for Libvirt/Qemu on Big Endian and Little
255 # Endian giving the nuance around guest vs host architectures
256 MIN_LIBVIRT_KVM_PPC64_VERSION = (1, 2, 12)
257 
258 # Names of the types that do not get compressed during migration
259 NO_COMPRESSION_TYPES = ('qcow2',)
260 
261 
262 # number of serial console limit
263 QEMU_MAX_SERIAL_PORTS = 4
264 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
265 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
266 
267 # realtime support
268 MIN_LIBVIRT_REALTIME_VERSION = (1, 2, 13)
269 
270 # libvirt postcopy support
271 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
272 
273 # qemu postcopy support
274 MIN_QEMU_POSTCOPY_VERSION = (2, 5, 0)
275 
276 MIN_LIBVIRT_OTHER_ARCH = {
277     fields.Architecture.S390: MIN_LIBVIRT_KVM_S390_VERSION,
278     fields.Architecture.S390X: MIN_LIBVIRT_KVM_S390_VERSION,
279     fields.Architecture.PPC: MIN_LIBVIRT_KVM_PPC64_VERSION,
280     fields.Architecture.PPC64: MIN_LIBVIRT_KVM_PPC64_VERSION,
281     fields.Architecture.PPC64LE: MIN_LIBVIRT_KVM_PPC64_VERSION,
282 }
283 
284 MIN_QEMU_OTHER_ARCH = {
285     fields.Architecture.S390: MIN_QEMU_S390_VERSION,
286     fields.Architecture.S390X: MIN_QEMU_S390_VERSION,
287 }
288 
289 # perf events support
290 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
291 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
292 
293 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
294                                 'mbml': 'mbm_local',
295                                 'mbmt': 'mbm_total',
296                                }
297 
298 # Mediated devices support
299 MIN_LIBVIRT_MDEV_SUPPORT = (3, 4, 0)
300 
301 
302 class LibvirtDriver(driver.ComputeDriver):
303     capabilities = {
304         "has_imagecache": True,
305         "supports_recreate": True,
306         "supports_migrate_to_same_host": False,
307         "supports_attach_interface": True,
308         "supports_device_tagging": True,
309         "supports_tagged_attach_interface": True,
310         "supports_tagged_attach_volume": True,
311         "supports_extend_volume": True,
312     }
313 
314     def __init__(self, virtapi, read_only=False):
315         super(LibvirtDriver, self).__init__(virtapi)
316 
317         global libvirt
318         if libvirt is None:
319             libvirt = importutils.import_module('libvirt')
320             libvirt_migrate.libvirt = libvirt
321 
322         self._host = host.Host(self._uri(), read_only,
323                                lifecycle_event_handler=self.emit_event,
324                                conn_event_handler=self._handle_conn_event)
325         self._initiator = None
326         self._fc_wwnns = None
327         self._fc_wwpns = None
328         self._caps = None
329         self._supported_perf_events = []
330         self.firewall_driver = firewall.load_driver(
331             DEFAULT_FIREWALL_DRIVER,
332             host=self._host)
333 
334         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
335 
336         # TODO(mriedem): Long-term we should load up the volume drivers on
337         # demand as needed rather than doing this on startup, as there might
338         # be unsupported volume drivers in this list based on the underlying
339         # platform.
340         self.volume_drivers = self._get_volume_drivers()
341 
342         self._disk_cachemode = None
343         self.image_cache_manager = imagecache.ImageCacheManager()
344         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
345 
346         self.disk_cachemodes = {}
347 
348         self.valid_cachemodes = ["default",
349                                  "none",
350                                  "writethrough",
351                                  "writeback",
352                                  "directsync",
353                                  "unsafe",
354                                 ]
355         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
356                                                                       'qemu')
357 
358         for mode_str in CONF.libvirt.disk_cachemodes:
359             disk_type, sep, cache_mode = mode_str.partition('=')
360             if cache_mode not in self.valid_cachemodes:
361                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
362                             'for disk type %(disk_type)s.',
363                             {'cache_mode': cache_mode, 'disk_type': disk_type})
364                 continue
365             self.disk_cachemodes[disk_type] = cache_mode
366 
367         self._volume_api = cinder.API()
368         self._image_api = image.API()
369 
370         sysinfo_serial_funcs = {
371             'none': lambda: None,
372             'hardware': self._get_host_sysinfo_serial_hardware,
373             'os': self._get_host_sysinfo_serial_os,
374             'auto': self._get_host_sysinfo_serial_auto,
375         }
376 
377         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
378             CONF.libvirt.sysinfo_serial)
379 
380         self.job_tracker = instancejobtracker.InstanceJobTracker()
381         self._remotefs = remotefs.RemoteFilesystem()
382 
383         self._live_migration_flags = self._block_migration_flags = 0
384         self.active_migrations = {}
385 
386         # Compute reserved hugepages from conf file at the very
387         # beginning to ensure any syntax error will be reported and
388         # avoid any re-calculation when computing resources.
389         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
390 
391     def _get_volume_drivers(self):
392         driver_registry = dict()
393 
394         for driver_str in libvirt_volume_drivers:
395             driver_type, _sep, driver = driver_str.partition('=')
396             driver_class = importutils.import_class(driver)
397             try:
398                 driver_registry[driver_type] = driver_class(self._host)
399             except brick_exception.InvalidConnectorProtocol:
400                 LOG.debug('Unable to load volume driver %s. It is not '
401                           'supported on this host.', driver)
402 
403         return driver_registry
404 
405     @property
406     def disk_cachemode(self):
407         if self._disk_cachemode is None:
408             # We prefer 'none' for consistent performance, host crash
409             # safety & migration correctness by avoiding host page cache.
410             # Some filesystems don't support O_DIRECT though. For those we
411             # fallback to 'writethrough' which gives host crash safety, and
412             # is safe for migration provided the filesystem is cache coherent
413             # (cluster filesystems typically are, but things like NFS are not).
414             self._disk_cachemode = "none"
415             if not utils.supports_direct_io(CONF.instances_path):
416                 self._disk_cachemode = "writethrough"
417         return self._disk_cachemode
418 
419     def _set_cache_mode(self, conf):
420         """Set cache mode on LibvirtConfigGuestDisk object."""
421         try:
422             source_type = conf.source_type
423             driver_cache = conf.driver_cache
424         except AttributeError:
425             return
426 
427         cache_mode = self.disk_cachemodes.get(source_type,
428                                               driver_cache)
429         conf.driver_cache = cache_mode
430 
431     def _do_quality_warnings(self):
432         """Warn about untested driver configurations.
433 
434         This will log a warning message about untested driver or host arch
435         configurations to indicate to administrators that the quality is
436         unknown. Currently, only qemu or kvm on intel 32- or 64-bit systems
437         is tested upstream.
438         """
439         caps = self._host.get_capabilities()
440         hostarch = caps.host.cpu.arch
441         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
442             hostarch not in (fields.Architecture.I686,
443                              fields.Architecture.X86_64)):
444             LOG.warning('The libvirt driver is not tested on '
445                         '%(type)s/%(arch)s by the OpenStack project and '
446                         'thus its quality can not be ensured. For more '
447                         'information, see: https://docs.openstack.org/'
448                         'nova/latest/user/support-matrix.html',
449                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
450 
451     def _handle_conn_event(self, enabled, reason):
452         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
453                  {'enabled': enabled, 'reason': reason})
454         self._set_host_enabled(enabled, reason)
455 
456     def _version_to_string(self, version):
457         return '.'.join([str(x) for x in version])
458 
459     def init_host(self, host):
460         self._host.initialize()
461 
462         self._do_quality_warnings()
463 
464         self._parse_migration_flags()
465 
466         self._supported_perf_events = self._get_supported_perf_events()
467 
468         if (CONF.libvirt.virt_type == 'lxc' and
469                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
470             LOG.warning("Running libvirt-lxc without user namespaces is "
471                         "dangerous. Containers spawned by Nova will be run "
472                         "as the host's root user. It is highly suggested "
473                         "that user namespaces be used in a public or "
474                         "multi-tenant environment.")
475 
476         # Stop libguestfs using KVM unless we're also configured
477         # to use this. This solves problem where people need to
478         # stop Nova use of KVM because nested-virt is broken
479         if CONF.libvirt.virt_type != "kvm":
480             guestfs.force_tcg()
481 
482         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
483             raise exception.InternalError(
484                 _('Nova requires libvirt version %s or greater.') %
485                 self._version_to_string(MIN_LIBVIRT_VERSION))
486 
487         if CONF.libvirt.virt_type in ("qemu", "kvm"):
488             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
489                 # "qemu-img info" calls are version dependent, so we need to
490                 # store the version in the images module.
491                 images.QEMU_VERSION = self._host.get_connection().getVersion()
492             else:
493                 raise exception.InternalError(
494                     _('Nova requires QEMU version %s or greater.') %
495                     self._version_to_string(MIN_QEMU_VERSION))
496 
497         if CONF.libvirt.virt_type == 'parallels':
498             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
499                 raise exception.InternalError(
500                     _('Nova requires Virtuozzo version %s or greater.') %
501                     self._version_to_string(MIN_VIRTUOZZO_VERSION))
502             if not self._host.has_min_version(MIN_LIBVIRT_VIRTUOZZO_VERSION):
503                 raise exception.InternalError(
504                     _('Running Nova with parallels virt_type requires '
505                       'libvirt version %s') %
506                     self._version_to_string(MIN_LIBVIRT_VIRTUOZZO_VERSION))
507 
508         # Give the cloud admin a heads up if we are intending to
509         # change the MIN_LIBVIRT_VERSION in the next release.
510         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
511             LOG.warning('Running Nova with a libvirt version less than '
512                         '%(version)s is deprecated. The required minimum '
513                         'version of libvirt will be raised to %(version)s '
514                         'in the next release.',
515                         {'version': self._version_to_string(
516                             NEXT_MIN_LIBVIRT_VERSION)})
517         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
518             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
519             LOG.warning('Running Nova with a QEMU version less than '
520                         '%(version)s is deprecated. The required minimum '
521                         'version of QEMU will be raised to %(version)s '
522                         'in the next release.',
523                         {'version': self._version_to_string(
524                             NEXT_MIN_QEMU_VERSION)})
525 
526         kvm_arch = fields.Architecture.from_host()
527         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
528             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
529                 not self._host.has_min_version(
530                                         MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch),
531                                         MIN_QEMU_OTHER_ARCH.get(kvm_arch))):
532             if MIN_QEMU_OTHER_ARCH.get(kvm_arch):
533                 raise exception.InternalError(
534                     _('Running Nova with qemu/kvm virt_type on %(arch)s '
535                       'requires libvirt version %(libvirt_ver)s and '
536                       'qemu version %(qemu_ver)s, or greater') %
537                     {'arch': kvm_arch,
538                      'libvirt_ver': self._version_to_string(
539                          MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch)),
540                      'qemu_ver': self._version_to_string(
541                          MIN_QEMU_OTHER_ARCH.get(kvm_arch))})
542             # no qemu version in the error message
543             raise exception.InternalError(
544                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
545                   'requires libvirt version %(libvirt_ver)s or greater') %
546                 {'arch': kvm_arch,
547                  'libvirt_ver': self._version_to_string(
548                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
549 
550     def _prepare_migration_flags(self):
551         migration_flags = 0
552 
553         migration_flags |= libvirt.VIR_MIGRATE_LIVE
554 
555         # Adding p2p flag only if xen is not in use, because xen does not
556         # support p2p migrations
557         if CONF.libvirt.virt_type != 'xen':
558             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
559 
560         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
561         # instance will remain defined on the source host
562         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
563 
564         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
565         # destination host
566         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
567 
568         live_migration_flags = block_migration_flags = migration_flags
569 
570         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
571         # will be live-migrations instead
572         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
573 
574         return (live_migration_flags, block_migration_flags)
575 
576     def _handle_live_migration_tunnelled(self, migration_flags):
577         if (CONF.libvirt.live_migration_tunnelled is None or
578                 CONF.libvirt.live_migration_tunnelled):
579             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
580         return migration_flags
581 
582     def _is_post_copy_available(self):
583         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION,
584                                       hv_ver=MIN_QEMU_POSTCOPY_VERSION):
585             return True
586         return False
587 
588     def _is_virtlogd_available(self):
589         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
590                                           MIN_QEMU_VIRTLOGD)
591 
592     def _handle_live_migration_post_copy(self, migration_flags):
593         if CONF.libvirt.live_migration_permit_post_copy:
594             if self._is_post_copy_available():
595                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
596             else:
597                 LOG.info('The live_migration_permit_post_copy is set '
598                          'to True, but it is not supported.')
599         return migration_flags
600 
601     def _handle_live_migration_auto_converge(self, migration_flags):
602         if (self._is_post_copy_available() and
603                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
604             LOG.info('The live_migration_permit_post_copy is set to '
605                      'True and post copy live migration is available '
606                      'so auto-converge will not be in use.')
607         elif CONF.libvirt.live_migration_permit_auto_converge:
608             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
609         return migration_flags
610 
611     def _parse_migration_flags(self):
612         (live_migration_flags,
613             block_migration_flags) = self._prepare_migration_flags()
614 
615         live_migration_flags = self._handle_live_migration_tunnelled(
616             live_migration_flags)
617         block_migration_flags = self._handle_live_migration_tunnelled(
618             block_migration_flags)
619 
620         live_migration_flags = self._handle_live_migration_post_copy(
621             live_migration_flags)
622         block_migration_flags = self._handle_live_migration_post_copy(
623             block_migration_flags)
624 
625         live_migration_flags = self._handle_live_migration_auto_converge(
626             live_migration_flags)
627         block_migration_flags = self._handle_live_migration_auto_converge(
628             block_migration_flags)
629 
630         self._live_migration_flags = live_migration_flags
631         self._block_migration_flags = block_migration_flags
632 
633     # TODO(sahid): This method is targeted for removal when the tests
634     # have been updated to avoid its use
635     #
636     # All libvirt API calls on the libvirt.Connect object should be
637     # encapsulated by methods on the nova.virt.libvirt.host.Host
638     # object, rather than directly invoking the libvirt APIs. The goal
639     # is to avoid a direct dependency on the libvirt API from the
640     # driver.py file.
641     def _get_connection(self):
642         return self._host.get_connection()
643 
644     _conn = property(_get_connection)
645 
646     @staticmethod
647     def _uri():
648         if CONF.libvirt.virt_type == 'uml':
649             uri = CONF.libvirt.connection_uri or 'uml:///system'
650         elif CONF.libvirt.virt_type == 'xen':
651             uri = CONF.libvirt.connection_uri or 'xen:///'
652         elif CONF.libvirt.virt_type == 'lxc':
653             uri = CONF.libvirt.connection_uri or 'lxc:///'
654         elif CONF.libvirt.virt_type == 'parallels':
655             uri = CONF.libvirt.connection_uri or 'parallels:///system'
656         else:
657             uri = CONF.libvirt.connection_uri or 'qemu:///system'
658         return uri
659 
660     @staticmethod
661     def _live_migration_uri(dest):
662         uris = {
663             'kvm': 'qemu+%s://%s/system',
664             'qemu': 'qemu+%s://%s/system',
665             'xen': 'xenmigr://%s/system',
666             'parallels': 'parallels+tcp://%s/system',
667         }
668         virt_type = CONF.libvirt.virt_type
669         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
670         uri = CONF.libvirt.live_migration_uri
671         if uri:
672             return uri % dest
673 
674         uri = uris.get(virt_type)
675         if uri is None:
676             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
677 
678         str_format = (dest,)
679         if virt_type in ('kvm', 'qemu'):
680             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
681             str_format = (scheme, dest)
682         return uris.get(virt_type) % str_format
683 
684     @staticmethod
685     def _migrate_uri(dest):
686         uri = None
687         # Only QEMU live migrations supports migrate-uri parameter
688         virt_type = CONF.libvirt.virt_type
689         if virt_type in ('qemu', 'kvm'):
690             # QEMU accept two schemes: tcp and rdma.  By default
691             # libvirt build the URI using the remote hostname and the
692             # tcp schema.
693             uri = 'tcp://%s' % dest
694         # Because dest might be of type unicode, here we might return value of
695         # type unicode as well which is not acceptable by libvirt python
696         # binding when Python 2.7 is in use, so let's convert it explicitly
697         # back to string. When Python 3.x is in use, libvirt python binding
698         # accepts unicode type so it is completely fine to do a no-op str(uri)
699         # conversion which will return value of type unicode.
700         return uri and str(uri)
701 
702     def instance_exists(self, instance):
703         """Efficient override of base instance_exists method."""
704         try:
705             self._host.get_guest(instance)
706             return True
707         except (exception.InternalError, exception.InstanceNotFound):
708             return False
709 
710     def estimate_instance_overhead(self, instance_info):
711         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
712             instance_info)
713         if isinstance(instance_info, objects.Flavor):
714             # A flavor object is passed during case of migrate
715             # TODO(sahid): We do not have any way to retrieve the
716             # image meta related to the instance so if the cpu_policy
717             # has been set in image_meta we will get an
718             # exception. Until we fix it we specifically set the
719             # cpu_policy in dedicated in an ImageMeta object so if the
720             # emulator threads has been requested nothing is going to
721             # fail.
722             image_meta = objects.ImageMeta.from_dict({"properties": {
723                 "hw_cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
724             }})
725             if (hardware.get_emulator_threads_constraint(
726                     instance_info, image_meta)
727                 == fields.CPUEmulatorThreadsPolicy.ISOLATE):
728                 overhead['vcpus'] += 1
729         else:
730             # An instance object is passed during case of spawing or a
731             # dict is passed when computing resource for an instance
732             numa_topology = hardware.instance_topology_from_instance(
733                 instance_info)
734             if numa_topology and numa_topology.emulator_threads_isolated:
735                 overhead['vcpus'] += 1
736         return overhead
737 
738     def list_instances(self):
739         names = []
740         for guest in self._host.list_guests(only_running=False):
741             names.append(guest.name)
742 
743         return names
744 
745     def list_instance_uuids(self):
746         uuids = []
747         for guest in self._host.list_guests(only_running=False):
748             uuids.append(guest.uuid)
749 
750         return uuids
751 
752     def plug_vifs(self, instance, network_info):
753         """Plug VIFs into networks."""
754         for vif in network_info:
755             self.vif_driver.plug(instance, vif)
756 
757     def _unplug_vifs(self, instance, network_info, ignore_errors):
758         """Unplug VIFs from networks."""
759         for vif in network_info:
760             try:
761                 self.vif_driver.unplug(instance, vif)
762             except exception.NovaException:
763                 if not ignore_errors:
764                     raise
765 
766     def unplug_vifs(self, instance, network_info):
767         self._unplug_vifs(instance, network_info, False)
768 
769     def _teardown_container(self, instance):
770         inst_path = libvirt_utils.get_instance_path(instance)
771         container_dir = os.path.join(inst_path, 'rootfs')
772         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
773         LOG.debug('Attempting to teardown container at path %(dir)s with '
774                   'root device: %(rootfs_dev)s',
775                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
776                   instance=instance)
777         disk_api.teardown_container(container_dir, rootfs_dev)
778 
779     def _destroy(self, instance, attempt=1):
780         try:
781             guest = self._host.get_guest(instance)
782             if CONF.serial_console.enabled:
783                 # This method is called for several events: destroy,
784                 # rebuild, hard-reboot, power-off - For all of these
785                 # events we want to release the serial ports acquired
786                 # for the guest before destroying it.
787                 serials = self._get_serial_ports_from_guest(guest)
788                 for hostname, port in serials:
789                     serial_console.release_port(host=hostname, port=port)
790         except exception.InstanceNotFound:
791             guest = None
792 
793         # If the instance is already terminated, we're still happy
794         # Otherwise, destroy it
795         old_domid = -1
796         if guest is not None:
797             try:
798                 old_domid = guest.id
799                 guest.poweroff()
800 
801             except libvirt.libvirtError as e:
802                 is_okay = False
803                 errcode = e.get_error_code()
804                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
805                     # Domain already gone. This can safely be ignored.
806                     is_okay = True
807                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
808                     # If the instance is already shut off, we get this:
809                     # Code=55 Error=Requested operation is not valid:
810                     # domain is not running
811 
812                     state = guest.get_power_state(self._host)
813                     if state == power_state.SHUTDOWN:
814                         is_okay = True
815                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
816                     errmsg = e.get_error_message()
817                     if (CONF.libvirt.virt_type == 'lxc' and
818                         errmsg == 'internal error: '
819                                   'Some processes refused to die'):
820                         # Some processes in the container didn't die
821                         # fast enough for libvirt. The container will
822                         # eventually die. For now, move on and let
823                         # the wait_for_destroy logic take over.
824                         is_okay = True
825                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
826                     LOG.warning("Cannot destroy instance, operation time out",
827                                 instance=instance)
828                     reason = _("operation time out")
829                     raise exception.InstancePowerOffFailure(reason=reason)
830                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
831                     if e.get_int1() == errno.EBUSY:
832                         # NOTE(danpb): When libvirt kills a process it sends it
833                         # SIGTERM first and waits 10 seconds. If it hasn't gone
834                         # it sends SIGKILL and waits another 5 seconds. If it
835                         # still hasn't gone then you get this EBUSY error.
836                         # Usually when a QEMU process fails to go away upon
837                         # SIGKILL it is because it is stuck in an
838                         # uninterruptible kernel sleep waiting on I/O from
839                         # some non-responsive server.
840                         # Given the CPU load of the gate tests though, it is
841                         # conceivable that the 15 second timeout is too short,
842                         # particularly if the VM running tempest has a high
843                         # steal time from the cloud host. ie 15 wallclock
844                         # seconds may have passed, but the VM might have only
845                         # have a few seconds of scheduled run time.
846                         LOG.warning('Error from libvirt during destroy. '
847                                     'Code=%(errcode)s Error=%(e)s; '
848                                     'attempt %(attempt)d of 3',
849                                     {'errcode': errcode, 'e': e,
850                                      'attempt': attempt},
851                                     instance=instance)
852                         with excutils.save_and_reraise_exception() as ctxt:
853                             # Try up to 3 times before giving up.
854                             if attempt < 3:
855                                 ctxt.reraise = False
856                                 self._destroy(instance, attempt + 1)
857                                 return
858 
859                 if not is_okay:
860                     with excutils.save_and_reraise_exception():
861                         LOG.error('Error from libvirt during destroy. '
862                                   'Code=%(errcode)s Error=%(e)s',
863                                   {'errcode': errcode, 'e': e},
864                                   instance=instance)
865 
866         def _wait_for_destroy(expected_domid):
867             """Called at an interval until the VM is gone."""
868             # NOTE(vish): If the instance disappears during the destroy
869             #             we ignore it so the cleanup can still be
870             #             attempted because we would prefer destroy to
871             #             never fail.
872             try:
873                 dom_info = self.get_info(instance)
874                 state = dom_info.state
875                 new_domid = dom_info.internal_id
876             except exception.InstanceNotFound:
877                 LOG.debug("During wait destroy, instance disappeared.",
878                           instance=instance)
879                 state = power_state.SHUTDOWN
880 
881             if state == power_state.SHUTDOWN:
882                 LOG.info("Instance destroyed successfully.", instance=instance)
883                 raise loopingcall.LoopingCallDone()
884 
885             # NOTE(wangpan): If the instance was booted again after destroy,
886             #                this may be an endless loop, so check the id of
887             #                domain here, if it changed and the instance is
888             #                still running, we should destroy it again.
889             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
890             if new_domid != expected_domid:
891                 LOG.info("Instance may be started again.", instance=instance)
892                 kwargs['is_running'] = True
893                 raise loopingcall.LoopingCallDone()
894 
895         kwargs = {'is_running': False}
896         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
897                                                      old_domid)
898         timer.start(interval=0.5).wait()
899         if kwargs['is_running']:
900             LOG.info("Going to destroy instance again.", instance=instance)
901             self._destroy(instance)
902         else:
903             # NOTE(GuanQiang): teardown container to avoid resource leak
904             if CONF.libvirt.virt_type == 'lxc':
905                 self._teardown_container(instance)
906 
907     def destroy(self, context, instance, network_info, block_device_info=None,
908                 destroy_disks=True):
909         self._destroy(instance)
910         self.cleanup(context, instance, network_info, block_device_info,
911                      destroy_disks)
912 
913     def _undefine_domain(self, instance):
914         try:
915             guest = self._host.get_guest(instance)
916             try:
917                 support_uefi = self._has_uefi_support()
918                 guest.delete_configuration(support_uefi)
919             except libvirt.libvirtError as e:
920                 with excutils.save_and_reraise_exception() as ctxt:
921                     errcode = e.get_error_code()
922                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
923                         LOG.debug("Called undefine, but domain already gone.",
924                                   instance=instance)
925                         ctxt.reraise = False
926                     else:
927                         LOG.error('Error from libvirt during undefine. '
928                                   'Code=%(errcode)s Error=%(e)s',
929                                   {'errcode': errcode,
930                                    'e': encodeutils.exception_to_unicode(e)},
931                                   instance=instance)
932         except exception.InstanceNotFound:
933             pass
934 
935     def cleanup(self, context, instance, network_info, block_device_info=None,
936                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
937         if destroy_vifs:
938             self._unplug_vifs(instance, network_info, True)
939 
940         # Continue attempting to remove firewall filters for the instance
941         # until it's done or there is a failure to remove the filters. If
942         # unfilter fails because the instance is not yet shutdown, try to
943         # destroy the guest again and then retry the unfilter.
944         while True:
945             try:
946                 self.unfilter_instance(instance, network_info)
947                 break
948             except libvirt.libvirtError as e:
949                 try:
950                     state = self.get_info(instance).state
951                 except exception.InstanceNotFound:
952                     state = power_state.SHUTDOWN
953 
954                 if state != power_state.SHUTDOWN:
955                     LOG.warning("Instance may be still running, destroy "
956                                 "it again.", instance=instance)
957                     self._destroy(instance)
958                 else:
959                     errcode = e.get_error_code()
960                     LOG.exception(_('Error from libvirt during unfilter. '
961                                     'Code=%(errcode)s Error=%(e)s'),
962                                   {'errcode': errcode, 'e': e},
963                                   instance=instance)
964                     reason = _("Error unfiltering instance.")
965                     raise exception.InstanceTerminationFailure(reason=reason)
966             except Exception:
967                 raise
968 
969         # FIXME(wangpan): if the instance is booted again here, such as the
970         #                 soft reboot operation boot it here, it will become
971         #                 "running deleted", should we check and destroy it
972         #                 at the end of this method?
973 
974         # NOTE(vish): we disconnect from volumes regardless
975         block_device_mapping = driver.block_device_info_get_mapping(
976             block_device_info)
977         for vol in block_device_mapping:
978             connection_info = vol['connection_info']
979             disk_dev = vol['mount_device']
980             if disk_dev is not None:
981                 disk_dev = disk_dev.rpartition("/")[2]
982 
983             if ('data' in connection_info and
984                     'volume_id' in connection_info['data']):
985                 volume_id = connection_info['data']['volume_id']
986                 encryption = encryptors.get_encryption_metadata(
987                     context, self._volume_api, volume_id, connection_info)
988 
989                 if encryption:
990                     # The volume must be detached from the VM before
991                     # disconnecting it from its encryptor. Otherwise, the
992                     # encryptor may report that the volume is still in use.
993                     encryptor = self._get_volume_encryptor(connection_info,
994                                                            encryption)
995                     encryptor.detach_volume(**encryption)
996 
997             try:
998                 self._disconnect_volume(connection_info, instance)
999             except Exception as exc:
1000                 with excutils.save_and_reraise_exception() as ctxt:
1001                     if destroy_disks:
1002                         # Don't block on Volume errors if we're trying to
1003                         # delete the instance as we may be partially created
1004                         # or deleted
1005                         ctxt.reraise = False
1006                         LOG.warning(
1007                             "Ignoring Volume Error on vol %(vol_id)s "
1008                             "during delete %(exc)s",
1009                             {'vol_id': vol.get('volume_id'),
1010                              'exc': encodeutils.exception_to_unicode(exc)},
1011                             instance=instance)
1012 
1013         if destroy_disks:
1014             # NOTE(haomai): destroy volumes if needed
1015             if CONF.libvirt.images_type == 'lvm':
1016                 self._cleanup_lvm(instance, block_device_info)
1017             if CONF.libvirt.images_type == 'rbd':
1018                 self._cleanup_rbd(instance)
1019 
1020         is_shared_block_storage = False
1021         if migrate_data and 'is_shared_block_storage' in migrate_data:
1022             is_shared_block_storage = migrate_data.is_shared_block_storage
1023         if destroy_disks or is_shared_block_storage:
1024             attempts = int(instance.system_metadata.get('clean_attempts',
1025                                                         '0'))
1026             success = self.delete_instance_files(instance)
1027             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1028             # task in the compute manager. The tight coupling is not great...
1029             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1030             if success:
1031                 instance.cleaned = True
1032             instance.save()
1033 
1034         self._undefine_domain(instance)
1035 
1036     def _detach_encrypted_volumes(self, instance, block_device_info):
1037         """Detaches encrypted volumes attached to instance."""
1038         disks = self._get_instance_disk_info(instance, block_device_info)
1039         encrypted_volumes = filter(dmcrypt.is_encrypted,
1040                                    [disk['path'] for disk in disks])
1041         for path in encrypted_volumes:
1042             dmcrypt.delete_volume(path)
1043 
1044     def _get_serial_ports_from_guest(self, guest, mode=None):
1045         """Returns an iterator over serial port(s) configured on guest.
1046 
1047         :param mode: Should be a value in (None, bind, connect)
1048         """
1049         xml = guest.get_xml_desc()
1050         tree = etree.fromstring(xml)
1051 
1052         # The 'serial' device is the base for x86 platforms. Other platforms
1053         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1054         xpath_mode = "[@mode='%s']" % mode if mode else ""
1055         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1056         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1057 
1058         tcp_devices = tree.findall(serial_tcp)
1059         if len(tcp_devices) == 0:
1060             tcp_devices = tree.findall(console_tcp)
1061         for source in tcp_devices:
1062             yield (source.get("host"), int(source.get("service")))
1063 
1064     def _get_scsi_controller_max_unit(self, guest):
1065         """Returns the max disk unit used by scsi controller"""
1066         xml = guest.get_xml_desc()
1067         tree = etree.fromstring(xml)
1068         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1069 
1070         ret = []
1071         for obj in tree.findall(addrs):
1072             ret.append(int(obj.get('unit', 0)))
1073         return max(ret)
1074 
1075     @staticmethod
1076     def _get_rbd_driver():
1077         return rbd_utils.RBDDriver(
1078                 pool=CONF.libvirt.images_rbd_pool,
1079                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1080                 rbd_user=CONF.libvirt.rbd_user)
1081 
1082     def _cleanup_rbd(self, instance):
1083         # NOTE(nic): On revert_resize, the cleanup steps for the root
1084         # volume are handled with an "rbd snap rollback" command,
1085         # and none of this is needed (and is, in fact, harmful) so
1086         # filter out non-ephemerals from the list
1087         if instance.task_state == task_states.RESIZE_REVERTING:
1088             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1089                                       disk.endswith('disk.local'))
1090         else:
1091             filter_fn = lambda disk: disk.startswith(instance.uuid)
1092         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1093 
1094     def _cleanup_lvm(self, instance, block_device_info):
1095         """Delete all LVM disks for given instance object."""
1096         if instance.get('ephemeral_key_uuid') is not None:
1097             self._detach_encrypted_volumes(instance, block_device_info)
1098 
1099         disks = self._lvm_disks(instance)
1100         if disks:
1101             lvm.remove_volumes(disks)
1102 
1103     def _lvm_disks(self, instance):
1104         """Returns all LVM disks for given instance object."""
1105         if CONF.libvirt.images_volume_group:
1106             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1107             if not os.path.exists(vg):
1108                 return []
1109             pattern = '%s_' % instance.uuid
1110 
1111             def belongs_to_instance(disk):
1112                 return disk.startswith(pattern)
1113 
1114             def fullpath(name):
1115                 return os.path.join(vg, name)
1116 
1117             logical_volumes = lvm.list_volumes(vg)
1118 
1119             disks = [fullpath(disk) for disk in logical_volumes
1120                      if belongs_to_instance(disk)]
1121             return disks
1122         return []
1123 
1124     def get_volume_connector(self, instance):
1125         root_helper = utils.get_root_helper()
1126         return connector.get_connector_properties(
1127             root_helper, CONF.my_block_storage_ip,
1128             CONF.libvirt.volume_use_multipath,
1129             enforce_multipath=True,
1130             host=CONF.host)
1131 
1132     def _cleanup_resize(self, context, instance, network_info):
1133         inst_base = libvirt_utils.get_instance_path(instance)
1134         target = inst_base + '_resize'
1135 
1136         if os.path.exists(target):
1137             # Deletion can fail over NFS, so retry the deletion as required.
1138             # Set maximum attempt as 5, most test can remove the directory
1139             # for the second time.
1140             utils.execute('rm', '-rf', target, delay_on_retry=True,
1141                           attempts=5)
1142 
1143         root_disk = self.image_backend.by_name(instance, 'disk')
1144         # TODO(nic): Set ignore_errors=False in a future release.
1145         # It is set to True here to avoid any upgrade issues surrounding
1146         # instances being in pending resize state when the software is updated;
1147         # in that case there will be no snapshot to remove.  Once it can be
1148         # reasonably assumed that no such instances exist in the wild
1149         # anymore, it should be set back to False (the default) so it will
1150         # throw errors, like it should.
1151         if root_disk.exists():
1152             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1153                                   ignore_errors=True)
1154 
1155         # NOTE(mjozefcz):
1156         # self.image_backend.image for some backends recreates instance
1157         # directory and image disk.info - remove it here if exists
1158         # Do not remove inst_base for volume-backed instances since that
1159         # could potentially remove the files on the destination host
1160         # if using shared storage.
1161         if (os.path.exists(inst_base) and not root_disk.exists() and
1162                 not compute_utils.is_volume_backed_instance(
1163                     context, instance)):
1164             try:
1165                 shutil.rmtree(inst_base)
1166             except OSError as e:
1167                 if e.errno != errno.ENOENT:
1168                     raise
1169 
1170         if instance.host != CONF.host:
1171             self._undefine_domain(instance)
1172             self.unplug_vifs(instance, network_info)
1173             self.unfilter_instance(instance, network_info)
1174 
1175     def _get_volume_driver(self, connection_info):
1176         driver_type = connection_info.get('driver_volume_type')
1177         if driver_type not in self.volume_drivers:
1178             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1179         return self.volume_drivers[driver_type]
1180 
1181     def _connect_volume(self, connection_info, instance):
1182         vol_driver = self._get_volume_driver(connection_info)
1183         vol_driver.connect_volume(connection_info, instance)
1184 
1185     def _disconnect_volume(self, connection_info, instance):
1186         vol_driver = self._get_volume_driver(connection_info)
1187         vol_driver.disconnect_volume(connection_info, instance)
1188 
1189     def _extend_volume(self, connection_info, instance):
1190         vol_driver = self._get_volume_driver(connection_info)
1191         return vol_driver.extend_volume(connection_info, instance)
1192 
1193     def _get_volume_config(self, connection_info, disk_info):
1194         vol_driver = self._get_volume_driver(connection_info)
1195         conf = vol_driver.get_config(connection_info, disk_info)
1196         self._set_cache_mode(conf)
1197         return conf
1198 
1199     def _get_volume_encryptor(self, connection_info, encryption):
1200         root_helper = utils.get_root_helper()
1201         return encryptors.get_volume_encryptor(root_helper=root_helper,
1202                                                keymgr=key_manager.API(CONF),
1203                                                connection_info=connection_info,
1204                                                **encryption)
1205 
1206     def _check_discard_for_attach_volume(self, conf, instance):
1207         """Perform some checks for volumes configured for discard support.
1208 
1209         If discard is configured for the volume, and the guest is using a
1210         configuration known to not work, we will log a message explaining
1211         the reason why.
1212         """
1213         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1214             LOG.debug('Attempting to attach volume %(id)s with discard '
1215                       'support enabled to an instance using an '
1216                       'unsupported configuration. target_bus = '
1217                       '%(bus)s. Trim commands will not be issued to '
1218                       'the storage device.',
1219                       {'bus': conf.target_bus,
1220                        'id': conf.serial},
1221                       instance=instance)
1222 
1223     def attach_volume(self, context, connection_info, instance, mountpoint,
1224                       disk_bus=None, device_type=None, encryption=None):
1225         guest = self._host.get_guest(instance)
1226 
1227         disk_dev = mountpoint.rpartition("/")[2]
1228         bdm = {
1229             'device_name': disk_dev,
1230             'disk_bus': disk_bus,
1231             'device_type': device_type}
1232 
1233         # Note(cfb): If the volume has a custom block size, check that
1234         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1235         #            presence of a block size is considered mandatory by
1236         #            cinder so we fail if we can't honor the request.
1237         data = {}
1238         if ('data' in connection_info):
1239             data = connection_info['data']
1240         if ('logical_block_size' in data or 'physical_block_size' in data):
1241             if ((CONF.libvirt.virt_type != "kvm" and
1242                  CONF.libvirt.virt_type != "qemu")):
1243                 msg = _("Volume sets block size, but the current "
1244                         "libvirt hypervisor '%s' does not support custom "
1245                         "block size") % CONF.libvirt.virt_type
1246                 raise exception.InvalidHypervisorType(msg)
1247 
1248         self._connect_volume(connection_info, instance)
1249         disk_info = blockinfo.get_info_from_bdm(
1250             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1251         if disk_info['bus'] == 'scsi':
1252             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1253 
1254         conf = self._get_volume_config(connection_info, disk_info)
1255 
1256         self._check_discard_for_attach_volume(conf, instance)
1257 
1258         try:
1259             state = guest.get_power_state(self._host)
1260             live = state in (power_state.RUNNING, power_state.PAUSED)
1261 
1262             if encryption:
1263                 encryptor = self._get_volume_encryptor(connection_info,
1264                                                        encryption)
1265                 encryptor.attach_volume(context, **encryption)
1266 
1267             guest.attach_device(conf, persistent=True, live=live)
1268             # NOTE(artom) If we're attaching with a device role tag, we need to
1269             # rebuild device_metadata. If we're attaching without a role
1270             # tag, we're rebuilding it here needlessly anyways. This isn't a
1271             # massive deal, and it helps reduce code complexity by not having
1272             # to indicate to the virt driver that the attach is tagged. The
1273             # really important optimization of not calling the database unless
1274             # device_metadata has actually changed is done for us by
1275             # instance.save().
1276             instance.device_metadata = self._build_device_metadata(
1277                 context, instance)
1278             instance.save()
1279         except Exception:
1280             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1281                           mountpoint, instance=instance)
1282             with excutils.save_and_reraise_exception():
1283                 self._disconnect_volume(connection_info, instance)
1284 
1285     def _swap_volume(self, guest, disk_path, conf, resize_to):
1286         """Swap existing disk with a new block device."""
1287         dev = guest.get_block_device(disk_path)
1288 
1289         # Save a copy of the domain's persistent XML file. We'll use this
1290         # to redefine the domain if anything fails during the volume swap.
1291         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1292 
1293         # Abort is an idempotent operation, so make sure any block
1294         # jobs which may have failed are ended.
1295         try:
1296             dev.abort_job()
1297         except Exception:
1298             pass
1299 
1300         try:
1301             # NOTE (rmk): blockRebase cannot be executed on persistent
1302             #             domains, so we need to temporarily undefine it.
1303             #             If any part of this block fails, the domain is
1304             #             re-defined regardless.
1305             if guest.has_persistent_configuration():
1306                 support_uefi = self._has_uefi_support()
1307                 guest.delete_configuration(support_uefi)
1308 
1309             try:
1310                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1311                 # allow writing to existing external volume file. Use
1312                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1313                 # make sure XML is generated correctly (bug 1691195)
1314                 copy_dev = conf.source_type == 'block'
1315                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1316                            copy_dev=copy_dev)
1317                 while not dev.is_job_complete():
1318                     time.sleep(0.5)
1319 
1320                 dev.abort_job(pivot=True)
1321 
1322             except Exception as exc:
1323                 LOG.exception("Failure rebasing volume %(new_path)s on "
1324                     "%(old_path)s.", {'new_path': conf.source_path,
1325                                       'old_path': disk_path})
1326                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1327 
1328             if resize_to:
1329                 dev.resize(resize_to * units.Gi / units.Ki)
1330 
1331             # Make sure we will redefine the domain using the updated
1332             # configuration after the volume was swapped. The dump_inactive
1333             # keyword arg controls whether we pull the inactive (persistent)
1334             # or active (live) config from the domain. We want to pull the
1335             # live config after the volume was updated to use when we redefine
1336             # the domain.
1337             xml = guest.get_xml_desc(dump_inactive=False, dump_sensitive=True)
1338         finally:
1339             self._host.write_instance_config(xml)
1340 
1341     def swap_volume(self, old_connection_info,
1342                     new_connection_info, instance, mountpoint, resize_to):
1343 
1344         guest = self._host.get_guest(instance)
1345 
1346         disk_dev = mountpoint.rpartition("/")[2]
1347         if not guest.get_disk(disk_dev):
1348             raise exception.DiskNotFound(location=disk_dev)
1349         disk_info = {
1350             'dev': disk_dev,
1351             'bus': blockinfo.get_disk_bus_for_disk_dev(
1352                 CONF.libvirt.virt_type, disk_dev),
1353             'type': 'disk',
1354             }
1355         # NOTE (lyarwood): new_connection_info will be modified by the
1356         # following _connect_volume call down into the volume drivers. The
1357         # majority of the volume drivers will add a device_path that is in turn
1358         # used by _get_volume_config to set the source_path of the
1359         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1360         # this to the BDM here as the upper compute swap_volume method will
1361         # eventually do this for us.
1362         self._connect_volume(new_connection_info, instance)
1363         conf = self._get_volume_config(new_connection_info, disk_info)
1364         if not conf.source_path:
1365             self._disconnect_volume(new_connection_info, instance)
1366             raise NotImplementedError(_("Swap only supports host devices"))
1367 
1368         try:
1369             self._swap_volume(guest, disk_dev, conf, resize_to)
1370         except exception.VolumeRebaseFailed:
1371             with excutils.save_and_reraise_exception():
1372                 self._disconnect_volume(new_connection_info, instance)
1373 
1374         self._disconnect_volume(old_connection_info, instance)
1375 
1376     def _get_existing_domain_xml(self, instance, network_info,
1377                                  block_device_info=None):
1378         try:
1379             guest = self._host.get_guest(instance)
1380             xml = guest.get_xml_desc()
1381         except exception.InstanceNotFound:
1382             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1383                                                 instance,
1384                                                 instance.image_meta,
1385                                                 block_device_info)
1386             xml = self._get_guest_xml(nova_context.get_admin_context(),
1387                                       instance, network_info, disk_info,
1388                                       instance.image_meta,
1389                                       block_device_info=block_device_info)
1390         return xml
1391 
1392     def detach_volume(self, connection_info, instance, mountpoint,
1393                       encryption=None):
1394         disk_dev = mountpoint.rpartition("/")[2]
1395         try:
1396             guest = self._host.get_guest(instance)
1397 
1398             state = guest.get_power_state(self._host)
1399             live = state in (power_state.RUNNING, power_state.PAUSED)
1400 
1401             # The volume must be detached from the VM before disconnecting it
1402             # from its encryptor. Otherwise, the encryptor may report that the
1403             # volume is still in use.
1404             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1405                                                              disk_dev,
1406                                                              live=live)
1407             wait_for_detach()
1408 
1409             if encryption:
1410                 encryptor = self._get_volume_encryptor(connection_info,
1411                                                        encryption)
1412                 encryptor.detach_volume(**encryption)
1413 
1414         except exception.InstanceNotFound:
1415             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1416             #                will throw InstanceNotFound exception. Need to
1417             #                disconnect volume under this circumstance.
1418             LOG.warning("During detach_volume, instance disappeared.",
1419                         instance=instance)
1420         except exception.DeviceNotFound:
1421             raise exception.DiskNotFound(location=disk_dev)
1422         except libvirt.libvirtError as ex:
1423             # NOTE(vish): This is called to cleanup volumes after live
1424             #             migration, so we should still disconnect even if
1425             #             the instance doesn't exist here anymore.
1426             error_code = ex.get_error_code()
1427             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1428                 # NOTE(vish):
1429                 LOG.warning("During detach_volume, instance disappeared.",
1430                             instance=instance)
1431             else:
1432                 raise
1433 
1434         self._disconnect_volume(connection_info, instance)
1435 
1436     def extend_volume(self, connection_info, instance):
1437         try:
1438             new_size = self._extend_volume(connection_info, instance)
1439         except NotImplementedError:
1440             raise exception.ExtendVolumeNotSupported()
1441 
1442         # Resize the device in QEMU so its size is updated and
1443         # detected by the instance without rebooting.
1444         try:
1445             guest = self._host.get_guest(instance)
1446             state = guest.get_power_state(self._host)
1447             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1448             if active_state:
1449                 disk_path = connection_info['data']['device_path']
1450                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1451                           {'dev': disk_path, 'size': new_size})
1452                 dev = guest.get_block_device(disk_path)
1453                 dev.resize(new_size // units.Ki)
1454             else:
1455                 LOG.debug('Skipping block device resize, guest is not running',
1456                           instance=instance)
1457         except exception.InstanceNotFound:
1458             with excutils.save_and_reraise_exception():
1459                 LOG.warning('During extend_volume, instance disappeared.',
1460                             instance=instance)
1461         except libvirt.libvirtError:
1462             with excutils.save_and_reraise_exception():
1463                 LOG.exception('resizing block device failed.',
1464                               instance=instance)
1465 
1466     def attach_interface(self, context, instance, image_meta, vif):
1467         guest = self._host.get_guest(instance)
1468 
1469         self.vif_driver.plug(instance, vif)
1470         self.firewall_driver.setup_basic_filtering(instance, [vif])
1471         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1472                                          instance.flavor,
1473                                          CONF.libvirt.virt_type,
1474                                          self._host)
1475         try:
1476             state = guest.get_power_state(self._host)
1477             live = state in (power_state.RUNNING, power_state.PAUSED)
1478             guest.attach_device(cfg, persistent=True, live=live)
1479         except libvirt.libvirtError:
1480             LOG.error('attaching network adapter failed.',
1481                       instance=instance, exc_info=True)
1482             self.vif_driver.unplug(instance, vif)
1483             raise exception.InterfaceAttachFailed(
1484                     instance_uuid=instance.uuid)
1485         try:
1486             # NOTE(artom) If we're attaching with a device role tag, we need to
1487             # rebuild device_metadata. If we're attaching without a role
1488             # tag, we're rebuilding it here needlessly anyways. This isn't a
1489             # massive deal, and it helps reduce code complexity by not having
1490             # to indicate to the virt driver that the attach is tagged. The
1491             # really important optimization of not calling the database unless
1492             # device_metadata has actually changed is done for us by
1493             # instance.save().
1494             instance.device_metadata = self._build_device_metadata(
1495                 context, instance)
1496             instance.save()
1497         except Exception:
1498             # NOTE(artom) If we fail here it means the interface attached
1499             # successfully but building and/or saving the device metadata
1500             # failed. Just unplugging the vif is therefore not enough cleanup,
1501             # we need to detach the interface.
1502             with excutils.save_and_reraise_exception(reraise=False):
1503                 LOG.error('Interface attached successfully but building '
1504                           'and/or saving device metadata failed.',
1505                           instance=instance, exc_info=True)
1506                 self.detach_interface(context, instance, vif)
1507                 raise exception.InterfaceAttachFailed(
1508                     instance_uuid=instance.uuid)
1509 
1510     def detach_interface(self, context, instance, vif):
1511         guest = self._host.get_guest(instance)
1512         cfg = self.vif_driver.get_config(instance, vif,
1513                                          instance.image_meta,
1514                                          instance.flavor,
1515                                          CONF.libvirt.virt_type, self._host)
1516         interface = guest.get_interface_by_cfg(cfg)
1517         try:
1518             self.vif_driver.unplug(instance, vif)
1519             # NOTE(mriedem): When deleting an instance and using Neutron,
1520             # we can be racing against Neutron deleting the port and
1521             # sending the vif-deleted event which then triggers a call to
1522             # detach the interface, so if the interface is not found then
1523             # we can just log it as a warning.
1524             if not interface:
1525                 mac = vif.get('address')
1526                 # The interface is gone so just log it as a warning.
1527                 LOG.warning('Detaching interface %(mac)s failed because '
1528                             'the device is no longer found on the guest.',
1529                             {'mac': mac}, instance=instance)
1530                 return
1531 
1532             state = guest.get_power_state(self._host)
1533             live = state in (power_state.RUNNING, power_state.PAUSED)
1534             # Now we are going to loop until the interface is detached or we
1535             # timeout.
1536             wait_for_detach = guest.detach_device_with_retry(
1537                 guest.get_interface_by_cfg, cfg, live=live,
1538                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1539             wait_for_detach()
1540         except exception.DeviceDetachFailed:
1541             # We failed to detach the device even with the retry loop, so let's
1542             # dump some debug information to the logs before raising back up.
1543             with excutils.save_and_reraise_exception():
1544                 devname = self.vif_driver.get_vif_devname(vif)
1545                 interface = guest.get_interface_by_cfg(cfg)
1546                 if interface:
1547                     LOG.warning(
1548                         'Failed to detach interface %(devname)s after '
1549                         'repeated attempts. Final interface xml:\n'
1550                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1551                         {'devname': devname,
1552                          'interface_xml': interface.to_xml(),
1553                          'guest_xml': guest.get_xml_desc()},
1554                         instance=instance)
1555         except exception.DeviceNotFound:
1556             # The interface is gone so just log it as a warning.
1557             LOG.warning('Detaching interface %(mac)s failed because '
1558                         'the device is no longer found on the guest.',
1559                         {'mac': vif.get('address')}, instance=instance)
1560         except libvirt.libvirtError as ex:
1561             error_code = ex.get_error_code()
1562             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1563                 LOG.warning("During detach_interface, instance disappeared.",
1564                             instance=instance)
1565             else:
1566                 # NOTE(mriedem): When deleting an instance and using Neutron,
1567                 # we can be racing against Neutron deleting the port and
1568                 # sending the vif-deleted event which then triggers a call to
1569                 # detach the interface, so we might have failed because the
1570                 # network device no longer exists. Libvirt will fail with
1571                 # "operation failed: no matching network device was found"
1572                 # which unfortunately does not have a unique error code so we
1573                 # need to look up the interface by config and if it's not found
1574                 # then we can just log it as a warning rather than tracing an
1575                 # error.
1576                 mac = vif.get('address')
1577                 interface = guest.get_interface_by_cfg(cfg)
1578                 if interface:
1579                     LOG.error('detaching network adapter failed.',
1580                               instance=instance, exc_info=True)
1581                     raise exception.InterfaceDetachFailed(
1582                             instance_uuid=instance.uuid)
1583 
1584                 # The interface is gone so just log it as a warning.
1585                 LOG.warning('Detaching interface %(mac)s failed because '
1586                             'the device is no longer found on the guest.',
1587                             {'mac': mac}, instance=instance)
1588 
1589     def _create_snapshot_metadata(self, image_meta, instance,
1590                                   img_fmt, snp_name):
1591         metadata = {'is_public': False,
1592                     'status': 'active',
1593                     'name': snp_name,
1594                     'properties': {
1595                                    'kernel_id': instance.kernel_id,
1596                                    'image_location': 'snapshot',
1597                                    'image_state': 'available',
1598                                    'owner_id': instance.project_id,
1599                                    'ramdisk_id': instance.ramdisk_id,
1600                                    }
1601                     }
1602         if instance.os_type:
1603             metadata['properties']['os_type'] = instance.os_type
1604 
1605         # NOTE(vish): glance forces ami disk format to be ami
1606         if image_meta.disk_format == 'ami':
1607             metadata['disk_format'] = 'ami'
1608         else:
1609             metadata['disk_format'] = img_fmt
1610 
1611         if image_meta.obj_attr_is_set("container_format"):
1612             metadata['container_format'] = image_meta.container_format
1613         else:
1614             metadata['container_format'] = "bare"
1615 
1616         return metadata
1617 
1618     def snapshot(self, context, instance, image_id, update_task_state):
1619         """Create snapshot from a running VM instance.
1620 
1621         This command only works with qemu 0.14+
1622         """
1623         try:
1624             guest = self._host.get_guest(instance)
1625 
1626             # TODO(sahid): We are converting all calls from a
1627             # virDomain object to use nova.virt.libvirt.Guest.
1628             # We should be able to remove virt_dom at the end.
1629             virt_dom = guest._domain
1630         except exception.InstanceNotFound:
1631             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1632 
1633         snapshot = self._image_api.get(context, image_id)
1634 
1635         # source_format is an on-disk format
1636         # source_type is a backend type
1637         disk_path, source_format = libvirt_utils.find_disk(guest)
1638         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1639 
1640         # We won't have source_type for raw or qcow2 disks, because we can't
1641         # determine that from the path. We should have it from the libvirt
1642         # xml, though.
1643         if source_type is None:
1644             source_type = source_format
1645         # For lxc instances we won't have it either from libvirt xml
1646         # (because we just gave libvirt the mounted filesystem), or the path,
1647         # so source_type is still going to be None. In this case,
1648         # root_disk is going to default to CONF.libvirt.images_type
1649         # below, which is still safe.
1650 
1651         image_format = CONF.libvirt.snapshot_image_format or source_type
1652 
1653         # NOTE(bfilippov): save lvm and rbd as raw
1654         if image_format == 'lvm' or image_format == 'rbd':
1655             image_format = 'raw'
1656 
1657         metadata = self._create_snapshot_metadata(instance.image_meta,
1658                                                   instance,
1659                                                   image_format,
1660                                                   snapshot['name'])
1661 
1662         snapshot_name = uuidutils.generate_uuid(dashed=False)
1663 
1664         state = guest.get_power_state(self._host)
1665 
1666         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1667         #               cold snapshots. Currently, checking for encryption is
1668         #               redundant because LVM supports only cold snapshots.
1669         #               It is necessary in case this situation changes in the
1670         #               future.
1671         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1672              and source_type not in ('lvm')
1673              and not CONF.ephemeral_storage_encryption.enabled
1674              and not CONF.workarounds.disable_libvirt_livesnapshot):
1675             live_snapshot = True
1676             # Abort is an idempotent operation, so make sure any block
1677             # jobs which may have failed are ended. This operation also
1678             # confirms the running instance, as opposed to the system as a
1679             # whole, has a new enough version of the hypervisor (bug 1193146).
1680             try:
1681                 guest.get_block_device(disk_path).abort_job()
1682             except libvirt.libvirtError as ex:
1683                 error_code = ex.get_error_code()
1684                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1685                     live_snapshot = False
1686                 else:
1687                     pass
1688         else:
1689             live_snapshot = False
1690 
1691         # NOTE(rmk): We cannot perform live snapshots when a managedSave
1692         #            file is present, so we will use the cold/legacy method
1693         #            for instances which are shutdown.
1694         if state == power_state.SHUTDOWN:
1695             live_snapshot = False
1696 
1697         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1698                                           instance)
1699 
1700         root_disk = self.image_backend.by_libvirt_path(
1701             instance, disk_path, image_type=source_type)
1702 
1703         if live_snapshot:
1704             LOG.info("Beginning live snapshot process", instance=instance)
1705         else:
1706             LOG.info("Beginning cold snapshot process", instance=instance)
1707 
1708         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1709 
1710         try:
1711             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1712                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1713             metadata['location'] = root_disk.direct_snapshot(
1714                 context, snapshot_name, image_format, image_id,
1715                 instance.image_ref)
1716             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1717                                   instance)
1718             self._image_api.update(context, image_id, metadata,
1719                                    purge_props=False)
1720         except (NotImplementedError, exception.ImageUnacceptable,
1721                 exception.Forbidden) as e:
1722             if type(e) != NotImplementedError:
1723                 LOG.warning('Performing standard snapshot because direct '
1724                             'snapshot failed: %(error)s',
1725                             {'error': encodeutils.exception_to_unicode(e)})
1726             failed_snap = metadata.pop('location', None)
1727             if failed_snap:
1728                 failed_snap = {'url': str(failed_snap)}
1729             root_disk.cleanup_direct_snapshot(failed_snap,
1730                                                   also_destroy_volume=True,
1731                                                   ignore_errors=True)
1732             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1733                               expected_state=task_states.IMAGE_UPLOADING)
1734 
1735             # TODO(nic): possibly abstract this out to the root_disk
1736             if source_type == 'rbd' and live_snapshot:
1737                 # Standard snapshot uses qemu-img convert from RBD which is
1738                 # not safe to run with live_snapshot.
1739                 live_snapshot = False
1740                 # Suspend the guest, so this is no longer a live snapshot
1741                 self._prepare_domain_for_snapshot(context, live_snapshot,
1742                                                   state, instance)
1743 
1744             snapshot_directory = CONF.libvirt.snapshots_directory
1745             fileutils.ensure_tree(snapshot_directory)
1746             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1747                 try:
1748                     out_path = os.path.join(tmpdir, snapshot_name)
1749                     if live_snapshot:
1750                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1751                         os.chmod(tmpdir, 0o701)
1752                         self._live_snapshot(context, instance, guest,
1753                                             disk_path, out_path, source_format,
1754                                             image_format, instance.image_meta)
1755                     else:
1756                         root_disk.snapshot_extract(out_path, image_format)
1757                     LOG.info("Snapshot extracted, beginning image upload",
1758                              instance=instance)
1759                 finally:
1760                     self._snapshot_domain(context, live_snapshot, virt_dom,
1761                                           state, instance)
1762 
1763                 # Upload that image to the image service
1764                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1765                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1766                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
1767                     self._image_api.update(context,
1768                                            image_id,
1769                                            metadata,
1770                                            image_file)
1771         except Exception:
1772             with excutils.save_and_reraise_exception():
1773                 LOG.exception(_("Failed to snapshot image"))
1774                 failed_snap = metadata.pop('location', None)
1775                 if failed_snap:
1776                     failed_snap = {'url': str(failed_snap)}
1777                 root_disk.cleanup_direct_snapshot(
1778                         failed_snap, also_destroy_volume=True,
1779                         ignore_errors=True)
1780 
1781         LOG.info("Snapshot image upload complete", instance=instance)
1782 
1783     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1784                                      instance):
1785         # NOTE(dkang): managedSave does not work for LXC
1786         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1787             if state == power_state.RUNNING or state == power_state.PAUSED:
1788                 self.suspend(context, instance)
1789 
1790     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1791                          instance):
1792         guest = None
1793         # NOTE(dkang): because previous managedSave is not called
1794         #              for LXC, _create_domain must not be called.
1795         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1796             if state == power_state.RUNNING:
1797                 guest = self._create_domain(domain=virt_dom)
1798             elif state == power_state.PAUSED:
1799                 guest = self._create_domain(domain=virt_dom, pause=True)
1800 
1801             if guest is not None:
1802                 self._attach_pci_devices(
1803                     guest, pci_manager.get_instance_pci_devs(instance))
1804                 self._attach_direct_passthrough_ports(
1805                     context, instance, guest)
1806 
1807     def _can_set_admin_password(self, image_meta):
1808 
1809         if CONF.libvirt.virt_type == 'parallels':
1810             if not self._host.has_min_version(
1811                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
1812                 raise exception.SetAdminPasswdNotSupported()
1813         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
1814             if not self._host.has_min_version(
1815                    MIN_LIBVIRT_SET_ADMIN_PASSWD):
1816                 raise exception.SetAdminPasswdNotSupported()
1817             if not image_meta.properties.get('hw_qemu_guest_agent', False):
1818                 raise exception.QemuGuestAgentNotEnabled()
1819         else:
1820             raise exception.SetAdminPasswdNotSupported()
1821 
1822     def set_admin_password(self, instance, new_pass):
1823         self._can_set_admin_password(instance.image_meta)
1824 
1825         guest = self._host.get_guest(instance)
1826         user = instance.image_meta.properties.get("os_admin_user")
1827         if not user:
1828             if instance.os_type == "windows":
1829                 user = "Administrator"
1830             else:
1831                 user = "root"
1832         try:
1833             guest.set_user_password(user, new_pass)
1834         except libvirt.libvirtError as ex:
1835             error_code = ex.get_error_code()
1836             if error_code == libvirt.VIR_ERR_AGENT_UNRESPONSIVE:
1837                 LOG.debug('Failed to set password: QEMU agent unresponsive',
1838                           instance_uuid=instance.uuid)
1839                 raise NotImplementedError()
1840 
1841             err_msg = encodeutils.exception_to_unicode(ex)
1842             msg = (_('Error from libvirt while set password for username '
1843                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
1844                    % {'user': user, 'error_code': error_code, 'ex': err_msg})
1845             raise exception.InternalError(msg)
1846 
1847     def _can_quiesce(self, instance, image_meta):
1848         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
1849             raise exception.InstanceQuiesceNotSupported(
1850                 instance_id=instance.uuid)
1851 
1852         if not image_meta.properties.get('hw_qemu_guest_agent', False):
1853             raise exception.QemuGuestAgentNotEnabled()
1854 
1855     def _requires_quiesce(self, image_meta):
1856         return image_meta.properties.get('os_require_quiesce', False)
1857 
1858     def _set_quiesced(self, context, instance, image_meta, quiesced):
1859         self._can_quiesce(instance, image_meta)
1860         try:
1861             guest = self._host.get_guest(instance)
1862             if quiesced:
1863                 guest.freeze_filesystems()
1864             else:
1865                 guest.thaw_filesystems()
1866         except libvirt.libvirtError as ex:
1867             error_code = ex.get_error_code()
1868             err_msg = encodeutils.exception_to_unicode(ex)
1869             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
1870                      '[Error Code %(error_code)s] %(ex)s')
1871                    % {'instance_name': instance.name,
1872                       'error_code': error_code, 'ex': err_msg})
1873             raise exception.InternalError(msg)
1874 
1875     def quiesce(self, context, instance, image_meta):
1876         """Freeze the guest filesystems to prepare for snapshot.
1877 
1878         The qemu-guest-agent must be setup to execute fsfreeze.
1879         """
1880         self._set_quiesced(context, instance, image_meta, True)
1881 
1882     def unquiesce(self, context, instance, image_meta):
1883         """Thaw the guest filesystems after snapshot."""
1884         self._set_quiesced(context, instance, image_meta, False)
1885 
1886     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
1887                        source_format, image_format, image_meta):
1888         """Snapshot an instance without downtime."""
1889         dev = guest.get_block_device(disk_path)
1890 
1891         # Save a copy of the domain's persistent XML file
1892         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1893 
1894         # Abort is an idempotent operation, so make sure any block
1895         # jobs which may have failed are ended.
1896         try:
1897             dev.abort_job()
1898         except Exception:
1899             pass
1900 
1901         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
1902         #             in QEMU 1.3. In order to do this, we need to create
1903         #             a destination image with the original backing file
1904         #             and matching size of the instance root disk.
1905         src_disk_size = libvirt_utils.get_disk_size(disk_path,
1906                                                     format=source_format)
1907         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
1908                                                         format=source_format,
1909                                                         basename=False)
1910         disk_delta = out_path + '.delta'
1911         libvirt_utils.create_cow_image(src_back_path, disk_delta,
1912                                        src_disk_size)
1913 
1914         quiesced = False
1915         try:
1916             self._set_quiesced(context, instance, image_meta, True)
1917             quiesced = True
1918         except exception.NovaException as err:
1919             if self._requires_quiesce(image_meta):
1920                 raise
1921             LOG.info('Skipping quiescing instance: %(reason)s.',
1922                      {'reason': err}, instance=instance)
1923 
1924         try:
1925             # NOTE (rmk): blockRebase cannot be executed on persistent
1926             #             domains, so we need to temporarily undefine it.
1927             #             If any part of this block fails, the domain is
1928             #             re-defined regardless.
1929             if guest.has_persistent_configuration():
1930                 support_uefi = self._has_uefi_support()
1931                 guest.delete_configuration(support_uefi)
1932 
1933             # NOTE (rmk): Establish a temporary mirror of our root disk and
1934             #             issue an abort once we have a complete copy.
1935             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
1936 
1937             while not dev.is_job_complete():
1938                 time.sleep(0.5)
1939 
1940             dev.abort_job()
1941             nova.privsep.path.chown(disk_delta, uid=os.getuid())
1942         finally:
1943             self._host.write_instance_config(xml)
1944             if quiesced:
1945                 self._set_quiesced(context, instance, image_meta, False)
1946 
1947         # Convert the delta (CoW) image with a backing file to a flat
1948         # image with no backing file.
1949         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
1950                                        out_path, image_format)
1951 
1952     def _volume_snapshot_update_status(self, context, snapshot_id, status):
1953         """Send a snapshot status update to Cinder.
1954 
1955         This method captures and logs exceptions that occur
1956         since callers cannot do anything useful with these exceptions.
1957 
1958         Operations on the Cinder side waiting for this will time out if
1959         a failure occurs sending the update.
1960 
1961         :param context: security context
1962         :param snapshot_id: id of snapshot being updated
1963         :param status: new status value
1964 
1965         """
1966 
1967         try:
1968             self._volume_api.update_snapshot_status(context,
1969                                                     snapshot_id,
1970                                                     status)
1971         except Exception:
1972             LOG.exception(_('Failed to send updated snapshot status '
1973                             'to volume service.'))
1974 
1975     def _volume_snapshot_create(self, context, instance, guest,
1976                                 volume_id, new_file):
1977         """Perform volume snapshot.
1978 
1979            :param guest: VM that volume is attached to
1980            :param volume_id: volume UUID to snapshot
1981            :param new_file: relative path to new qcow2 file present on share
1982 
1983         """
1984         xml = guest.get_xml_desc()
1985         xml_doc = etree.fromstring(xml)
1986 
1987         device_info = vconfig.LibvirtConfigGuest()
1988         device_info.parse_dom(xml_doc)
1989 
1990         disks_to_snap = []          # to be snapshotted by libvirt
1991         network_disks_to_snap = []  # network disks (netfs, etc.)
1992         disks_to_skip = []          # local disks not snapshotted
1993 
1994         for guest_disk in device_info.devices:
1995             if (guest_disk.root_name != 'disk'):
1996                 continue
1997 
1998             if (guest_disk.target_dev is None):
1999                 continue
2000 
2001             if (guest_disk.serial is None or guest_disk.serial != volume_id):
2002                 disks_to_skip.append(guest_disk.target_dev)
2003                 continue
2004 
2005             # disk is a Cinder volume with the correct volume_id
2006 
2007             disk_info = {
2008                 'dev': guest_disk.target_dev,
2009                 'serial': guest_disk.serial,
2010                 'current_file': guest_disk.source_path,
2011                 'source_protocol': guest_disk.source_protocol,
2012                 'source_name': guest_disk.source_name,
2013                 'source_hosts': guest_disk.source_hosts,
2014                 'source_ports': guest_disk.source_ports
2015             }
2016 
2017             # Determine path for new_file based on current path
2018             if disk_info['current_file'] is not None:
2019                 current_file = disk_info['current_file']
2020                 new_file_path = os.path.join(os.path.dirname(current_file),
2021                                              new_file)
2022                 disks_to_snap.append((current_file, new_file_path))
2023             # NOTE(mriedem): This used to include a check for gluster in
2024             # addition to netfs since they were added together. Support for
2025             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2026             # however, if other volume drivers rely on the netfs disk source
2027             # protocol.
2028             elif disk_info['source_protocol'] == 'netfs':
2029                 network_disks_to_snap.append((disk_info, new_file))
2030 
2031         if not disks_to_snap and not network_disks_to_snap:
2032             msg = _('Found no disk to snapshot.')
2033             raise exception.InternalError(msg)
2034 
2035         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2036 
2037         for current_name, new_filename in disks_to_snap:
2038             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2039             snap_disk.name = current_name
2040             snap_disk.source_path = new_filename
2041             snap_disk.source_type = 'file'
2042             snap_disk.snapshot = 'external'
2043             snap_disk.driver_name = 'qcow2'
2044 
2045             snapshot.add_disk(snap_disk)
2046 
2047         for disk_info, new_filename in network_disks_to_snap:
2048             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2049             snap_disk.name = disk_info['dev']
2050             snap_disk.source_type = 'network'
2051             snap_disk.source_protocol = disk_info['source_protocol']
2052             snap_disk.snapshot = 'external'
2053             snap_disk.source_path = new_filename
2054             old_dir = disk_info['source_name'].split('/')[0]
2055             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2056             snap_disk.source_hosts = disk_info['source_hosts']
2057             snap_disk.source_ports = disk_info['source_ports']
2058 
2059             snapshot.add_disk(snap_disk)
2060 
2061         for dev in disks_to_skip:
2062             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2063             snap_disk.name = dev
2064             snap_disk.snapshot = 'no'
2065 
2066             snapshot.add_disk(snap_disk)
2067 
2068         snapshot_xml = snapshot.to_xml()
2069         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2070 
2071         image_meta = instance.image_meta
2072         try:
2073             # Check to see if we can quiesce the guest before taking the
2074             # snapshot.
2075             self._can_quiesce(instance, image_meta)
2076             try:
2077                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2078                                reuse_ext=True, quiesce=True)
2079                 return
2080             except libvirt.libvirtError:
2081                 # If the image says that quiesce is required then we fail.
2082                 if self._requires_quiesce(image_meta):
2083                     raise
2084                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2085                                 'attempting again with quiescing disabled.'),
2086                               instance=instance)
2087         except (exception.InstanceQuiesceNotSupported,
2088                 exception.QemuGuestAgentNotEnabled) as err:
2089             # If the image says that quiesce is required then we need to fail.
2090             if self._requires_quiesce(image_meta):
2091                 raise
2092             LOG.info('Skipping quiescing instance: %(reason)s.',
2093                      {'reason': err}, instance=instance)
2094 
2095         try:
2096             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2097                            reuse_ext=True, quiesce=False)
2098         except libvirt.libvirtError:
2099             LOG.exception(_('Unable to create VM snapshot, '
2100                             'failing volume_snapshot operation.'),
2101                           instance=instance)
2102 
2103             raise
2104 
2105     def _volume_refresh_connection_info(self, context, instance, volume_id):
2106         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2107                   context, volume_id, instance.uuid)
2108 
2109         driver_bdm = driver_block_device.convert_volume(bdm)
2110         if driver_bdm:
2111             driver_bdm.refresh_connection_info(context, instance,
2112                                                self._volume_api, self)
2113 
2114     def volume_snapshot_create(self, context, instance, volume_id,
2115                                create_info):
2116         """Create snapshots of a Cinder volume via libvirt.
2117 
2118         :param instance: VM instance object reference
2119         :param volume_id: id of volume being snapshotted
2120         :param create_info: dict of information used to create snapshots
2121                      - snapshot_id : ID of snapshot
2122                      - type : qcow2 / <other>
2123                      - new_file : qcow2 file created by Cinder which
2124                      becomes the VM's active image after
2125                      the snapshot is complete
2126         """
2127 
2128         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2129                   {'c_info': create_info}, instance=instance)
2130 
2131         try:
2132             guest = self._host.get_guest(instance)
2133         except exception.InstanceNotFound:
2134             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2135 
2136         if create_info['type'] != 'qcow2':
2137             msg = _('Unknown type: %s') % create_info['type']
2138             raise exception.InternalError(msg)
2139 
2140         snapshot_id = create_info.get('snapshot_id', None)
2141         if snapshot_id is None:
2142             msg = _('snapshot_id required in create_info')
2143             raise exception.InternalError(msg)
2144 
2145         try:
2146             self._volume_snapshot_create(context, instance, guest,
2147                                          volume_id, create_info['new_file'])
2148         except Exception:
2149             with excutils.save_and_reraise_exception():
2150                 LOG.exception(_('Error occurred during '
2151                                 'volume_snapshot_create, '
2152                                 'sending error status to Cinder.'),
2153                               instance=instance)
2154                 self._volume_snapshot_update_status(
2155                     context, snapshot_id, 'error')
2156 
2157         self._volume_snapshot_update_status(
2158             context, snapshot_id, 'creating')
2159 
2160         def _wait_for_snapshot():
2161             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2162 
2163             if snapshot.get('status') != 'creating':
2164                 self._volume_refresh_connection_info(context, instance,
2165                                                      volume_id)
2166                 raise loopingcall.LoopingCallDone()
2167 
2168         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2169         timer.start(interval=0.5).wait()
2170 
2171     @staticmethod
2172     def _rebase_with_qemu_img(guest, device, active_disk_object,
2173                               rebase_base):
2174         """Rebase a device tied to a guest using qemu-img.
2175 
2176         :param guest:the Guest which owns the device being rebased
2177         :type guest: nova.virt.libvirt.guest.Guest
2178         :param device: the guest block device to rebase
2179         :type device: nova.virt.libvirt.guest.BlockDevice
2180         :param active_disk_object: the guest block device to rebase
2181         :type active_disk_object: nova.virt.libvirt.config.\
2182                                     LibvirtConfigGuestDisk
2183         :param rebase_base: the new parent in the backing chain
2184         :type rebase_base: None or string
2185         """
2186 
2187         # It's unsure how well qemu-img handles network disks for
2188         # every protocol. So let's be safe.
2189         active_protocol = active_disk_object.source_protocol
2190         if active_protocol is not None:
2191             msg = _("Something went wrong when deleting a volume snapshot: "
2192                     "rebasing a %(protocol)s network disk using qemu-img "
2193                     "has not been fully tested") % {'protocol':
2194                     active_protocol}
2195             LOG.error(msg)
2196             raise exception.InternalError(msg)
2197 
2198         if rebase_base is None:
2199             # If backing_file is specified as "" (the empty string), then
2200             # the image is rebased onto no backing file (i.e. it will exist
2201             # independently of any backing file).
2202             backing_file = ""
2203             qemu_img_extra_arg = []
2204         else:
2205             # If the rebased image is going to have a backing file then
2206             # explicitly set the backing file format to avoid any security
2207             # concerns related to file format auto detection.
2208             backing_file = rebase_base
2209             b_file_fmt = images.qemu_img_info(backing_file).file_format
2210             qemu_img_extra_arg = ['-F', b_file_fmt]
2211 
2212         qemu_img_extra_arg.append(active_disk_object.source_path)
2213         utils.execute("qemu-img", "rebase", "-b", backing_file,
2214                       *qemu_img_extra_arg)
2215 
2216     def _volume_snapshot_delete(self, context, instance, volume_id,
2217                                 snapshot_id, delete_info=None):
2218         """Note:
2219             if file being merged into == active image:
2220                 do a blockRebase (pull) operation
2221             else:
2222                 do a blockCommit operation
2223             Files must be adjacent in snap chain.
2224 
2225         :param instance: instance object reference
2226         :param volume_id: volume UUID
2227         :param snapshot_id: snapshot UUID (unused currently)
2228         :param delete_info: {
2229             'type':              'qcow2',
2230             'file_to_merge':     'a.img',
2231             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2232                                                   active image)
2233           }
2234         """
2235 
2236         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2237                   instance=instance)
2238 
2239         if delete_info['type'] != 'qcow2':
2240             msg = _('Unknown delete_info type %s') % delete_info['type']
2241             raise exception.InternalError(msg)
2242 
2243         try:
2244             guest = self._host.get_guest(instance)
2245         except exception.InstanceNotFound:
2246             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2247 
2248         # Find dev name
2249         my_dev = None
2250         active_disk = None
2251 
2252         xml = guest.get_xml_desc()
2253         xml_doc = etree.fromstring(xml)
2254 
2255         device_info = vconfig.LibvirtConfigGuest()
2256         device_info.parse_dom(xml_doc)
2257 
2258         active_disk_object = None
2259 
2260         for guest_disk in device_info.devices:
2261             if (guest_disk.root_name != 'disk'):
2262                 continue
2263 
2264             if (guest_disk.target_dev is None or guest_disk.serial is None):
2265                 continue
2266 
2267             if guest_disk.serial == volume_id:
2268                 my_dev = guest_disk.target_dev
2269 
2270                 active_disk = guest_disk.source_path
2271                 active_protocol = guest_disk.source_protocol
2272                 active_disk_object = guest_disk
2273                 break
2274 
2275         if my_dev is None or (active_disk is None and active_protocol is None):
2276             LOG.debug('Domain XML: %s', xml, instance=instance)
2277             msg = (_('Disk with id: %s not found attached to instance.')
2278                    % volume_id)
2279             raise exception.InternalError(msg)
2280 
2281         LOG.debug("found device at %s", my_dev, instance=instance)
2282 
2283         def _get_snap_dev(filename, backing_store):
2284             if filename is None:
2285                 msg = _('filename cannot be None')
2286                 raise exception.InternalError(msg)
2287 
2288             # libgfapi delete
2289             LOG.debug("XML: %s", xml)
2290 
2291             LOG.debug("active disk object: %s", active_disk_object)
2292 
2293             # determine reference within backing store for desired image
2294             filename_to_merge = filename
2295             matched_name = None
2296             b = backing_store
2297             index = None
2298 
2299             current_filename = active_disk_object.source_name.split('/')[1]
2300             if current_filename == filename_to_merge:
2301                 return my_dev + '[0]'
2302 
2303             while b is not None:
2304                 source_filename = b.source_name.split('/')[1]
2305                 if source_filename == filename_to_merge:
2306                     LOG.debug('found match: %s', b.source_name)
2307                     matched_name = b.source_name
2308                     index = b.index
2309                     break
2310 
2311                 b = b.backing_store
2312 
2313             if matched_name is None:
2314                 msg = _('no match found for %s') % (filename_to_merge)
2315                 raise exception.InternalError(msg)
2316 
2317             LOG.debug('index of match (%s) is %s', b.source_name, index)
2318 
2319             my_snap_dev = '%s[%s]' % (my_dev, index)
2320             return my_snap_dev
2321 
2322         if delete_info['merge_target_file'] is None:
2323             # pull via blockRebase()
2324 
2325             # Merge the most recent snapshot into the active image
2326 
2327             rebase_disk = my_dev
2328             rebase_base = delete_info['file_to_merge']  # often None
2329             if (active_protocol is not None) and (rebase_base is not None):
2330                 rebase_base = _get_snap_dev(rebase_base,
2331                                             active_disk_object.backing_store)
2332 
2333             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2334             # and when available this flag _must_ be used to ensure backing
2335             # paths are maintained relative by qemu.
2336             #
2337             # If _RELATIVE flag not found, continue with old behaviour
2338             # (relative backing path seems to work for this case)
2339             try:
2340                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2341                 relative = rebase_base is not None
2342             except AttributeError:
2343                 LOG.warning(
2344                     "Relative blockrebase support was not detected. "
2345                     "Continuing with old behaviour.")
2346                 relative = False
2347 
2348             LOG.debug(
2349                 'disk: %(disk)s, base: %(base)s, '
2350                 'bw: %(bw)s, relative: %(relative)s',
2351                 {'disk': rebase_disk,
2352                  'base': rebase_base,
2353                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2354                  'relative': str(relative)}, instance=instance)
2355 
2356             dev = guest.get_block_device(rebase_disk)
2357             if guest.is_active():
2358                 result = dev.rebase(rebase_base, relative=relative)
2359                 if result == 0:
2360                     LOG.debug('blockRebase started successfully',
2361                               instance=instance)
2362 
2363                 while not dev.is_job_complete():
2364                     LOG.debug('waiting for blockRebase job completion',
2365                               instance=instance)
2366                     time.sleep(0.5)
2367 
2368             # If the guest is not running libvirt won't do a blockRebase.
2369             # In that case, let's ask qemu-img to rebase the disk.
2370             else:
2371                 LOG.debug('Guest is not running so doing a block rebase '
2372                           'using "qemu-img rebase"', instance=instance)
2373                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2374                                            rebase_base)
2375 
2376         else:
2377             # commit with blockCommit()
2378             my_snap_base = None
2379             my_snap_top = None
2380             commit_disk = my_dev
2381 
2382             if active_protocol is not None:
2383                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2384                                              active_disk_object.backing_store)
2385                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2386                                             active_disk_object.backing_store)
2387 
2388             commit_base = my_snap_base or delete_info['merge_target_file']
2389             commit_top = my_snap_top or delete_info['file_to_merge']
2390 
2391             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2392                       'commit_base=%(commit_base)s '
2393                       'commit_top=%(commit_top)s ',
2394                       {'commit_disk': commit_disk,
2395                        'commit_base': commit_base,
2396                        'commit_top': commit_top}, instance=instance)
2397 
2398             dev = guest.get_block_device(commit_disk)
2399             result = dev.commit(commit_base, commit_top, relative=True)
2400 
2401             if result == 0:
2402                 LOG.debug('blockCommit started successfully',
2403                           instance=instance)
2404 
2405             while not dev.is_job_complete():
2406                 LOG.debug('waiting for blockCommit job completion',
2407                           instance=instance)
2408                 time.sleep(0.5)
2409 
2410     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2411                                delete_info):
2412         try:
2413             self._volume_snapshot_delete(context, instance, volume_id,
2414                                          snapshot_id, delete_info=delete_info)
2415         except Exception:
2416             with excutils.save_and_reraise_exception():
2417                 LOG.exception(_('Error occurred during '
2418                                 'volume_snapshot_delete, '
2419                                 'sending error status to Cinder.'),
2420                               instance=instance)
2421                 self._volume_snapshot_update_status(
2422                     context, snapshot_id, 'error_deleting')
2423 
2424         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2425         self._volume_refresh_connection_info(context, instance, volume_id)
2426 
2427     def reboot(self, context, instance, network_info, reboot_type,
2428                block_device_info=None, bad_volumes_callback=None):
2429         """Reboot a virtual machine, given an instance reference."""
2430         if reboot_type == 'SOFT':
2431             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2432             try:
2433                 soft_reboot_success = self._soft_reboot(instance)
2434             except libvirt.libvirtError as e:
2435                 LOG.debug("Instance soft reboot failed: %s",
2436                           encodeutils.exception_to_unicode(e),
2437                           instance=instance)
2438                 soft_reboot_success = False
2439 
2440             if soft_reboot_success:
2441                 LOG.info("Instance soft rebooted successfully.",
2442                          instance=instance)
2443                 return
2444             else:
2445                 LOG.warning("Failed to soft reboot instance. "
2446                             "Trying hard reboot.",
2447                             instance=instance)
2448         return self._hard_reboot(context, instance, network_info,
2449                                  block_device_info)
2450 
2451     def _soft_reboot(self, instance):
2452         """Attempt to shutdown and restart the instance gracefully.
2453 
2454         We use shutdown and create here so we can return if the guest
2455         responded and actually rebooted. Note that this method only
2456         succeeds if the guest responds to acpi. Therefore we return
2457         success or failure so we can fall back to a hard reboot if
2458         necessary.
2459 
2460         :returns: True if the reboot succeeded
2461         """
2462         guest = self._host.get_guest(instance)
2463 
2464         state = guest.get_power_state(self._host)
2465         old_domid = guest.id
2466         # NOTE(vish): This check allows us to reboot an instance that
2467         #             is already shutdown.
2468         if state == power_state.RUNNING:
2469             guest.shutdown()
2470         # NOTE(vish): This actually could take slightly longer than the
2471         #             FLAG defines depending on how long the get_info
2472         #             call takes to return.
2473         self._prepare_pci_devices_for_use(
2474             pci_manager.get_instance_pci_devs(instance, 'all'))
2475         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2476             guest = self._host.get_guest(instance)
2477 
2478             state = guest.get_power_state(self._host)
2479             new_domid = guest.id
2480 
2481             # NOTE(ivoks): By checking domain IDs, we make sure we are
2482             #              not recreating domain that's already running.
2483             if old_domid != new_domid:
2484                 if state in [power_state.SHUTDOWN,
2485                              power_state.CRASHED]:
2486                     LOG.info("Instance shutdown successfully.",
2487                              instance=instance)
2488                     self._create_domain(domain=guest._domain)
2489                     timer = loopingcall.FixedIntervalLoopingCall(
2490                         self._wait_for_running, instance)
2491                     timer.start(interval=0.5).wait()
2492                     return True
2493                 else:
2494                     LOG.info("Instance may have been rebooted during soft "
2495                              "reboot, so return now.", instance=instance)
2496                     return True
2497             greenthread.sleep(1)
2498         return False
2499 
2500     def _hard_reboot(self, context, instance, network_info,
2501                      block_device_info=None):
2502         """Reboot a virtual machine, given an instance reference.
2503 
2504         Performs a Libvirt reset (if supported) on the domain.
2505 
2506         If Libvirt reset is unavailable this method actually destroys and
2507         re-creates the domain to ensure the reboot happens, as the guest
2508         OS cannot ignore this action.
2509         """
2510         # NOTE(mdbooth): In addition to performing a hard reboot of the domain,
2511         # the hard reboot operation is relied upon by operators to be an
2512         # automated attempt to fix as many things as possible about a
2513         # non-functioning instance before resorting to manual intervention.
2514         # With this goal in mind, we tear down all the aspects of an instance
2515         # we can here without losing data. This allows us to re-initialise from
2516         # scratch, and hopefully fix, most aspects of a non-functioning guest.
2517         self.destroy(context, instance, network_info, destroy_disks=False,
2518                      block_device_info=block_device_info)
2519 
2520         # Convert the system metadata to image metadata
2521         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2522         #                https://bugs.launchpad.net/nova/+bug/1349978
2523         instance_dir = libvirt_utils.get_instance_path(instance)
2524         fileutils.ensure_tree(instance_dir)
2525 
2526         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2527                                             instance,
2528                                             instance.image_meta,
2529                                             block_device_info)
2530         # NOTE(vish): This could generate the wrong device_format if we are
2531         #             using the raw backend and the images don't exist yet.
2532         #             The create_images_and_backing below doesn't properly
2533         #             regenerate raw backend images, however, so when it
2534         #             does we need to (re)generate the xml after the images
2535         #             are in place.
2536         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2537                                   instance.image_meta,
2538                                   block_device_info=block_device_info)
2539 
2540         # NOTE(mdbooth): context.auth_token will not be set when we call
2541         #                _hard_reboot from resume_state_on_host_boot()
2542         if context.auth_token is not None:
2543             # NOTE (rmk): Re-populate any missing backing files.
2544             config = vconfig.LibvirtConfigGuest()
2545             config.parse_str(xml)
2546             backing_disk_info = self._get_instance_disk_info_from_config(
2547                 config, block_device_info)
2548             self._create_images_and_backing(context, instance, instance_dir,
2549                                             backing_disk_info)
2550 
2551         # Initialize all the necessary networking, block devices and
2552         # start the instance.
2553         self._create_domain_and_network(context, xml, instance, network_info,
2554                                         block_device_info=block_device_info)
2555         self._prepare_pci_devices_for_use(
2556             pci_manager.get_instance_pci_devs(instance, 'all'))
2557 
2558         def _wait_for_reboot():
2559             """Called at an interval until the VM is running again."""
2560             state = self.get_info(instance).state
2561 
2562             if state == power_state.RUNNING:
2563                 LOG.info("Instance rebooted successfully.",
2564                          instance=instance)
2565                 raise loopingcall.LoopingCallDone()
2566 
2567         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2568         timer.start(interval=0.5).wait()
2569 
2570     def pause(self, instance):
2571         """Pause VM instance."""
2572         self._host.get_guest(instance).pause()
2573 
2574     def unpause(self, instance):
2575         """Unpause paused VM instance."""
2576         guest = self._host.get_guest(instance)
2577         guest.resume()
2578         guest.sync_guest_time()
2579 
2580     def _clean_shutdown(self, instance, timeout, retry_interval):
2581         """Attempt to shutdown the instance gracefully.
2582 
2583         :param instance: The instance to be shutdown
2584         :param timeout: How long to wait in seconds for the instance to
2585                         shutdown
2586         :param retry_interval: How often in seconds to signal the instance
2587                                to shutdown while waiting
2588 
2589         :returns: True if the shutdown succeeded
2590         """
2591 
2592         # List of states that represent a shutdown instance
2593         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2594                            power_state.CRASHED]
2595 
2596         try:
2597             guest = self._host.get_guest(instance)
2598         except exception.InstanceNotFound:
2599             # If the instance has gone then we don't need to
2600             # wait for it to shutdown
2601             return True
2602 
2603         state = guest.get_power_state(self._host)
2604         if state in SHUTDOWN_STATES:
2605             LOG.info("Instance already shutdown.", instance=instance)
2606             return True
2607 
2608         LOG.debug("Shutting down instance from state %s", state,
2609                   instance=instance)
2610         guest.shutdown()
2611         retry_countdown = retry_interval
2612 
2613         for sec in range(timeout):
2614 
2615             guest = self._host.get_guest(instance)
2616             state = guest.get_power_state(self._host)
2617 
2618             if state in SHUTDOWN_STATES:
2619                 LOG.info("Instance shutdown successfully after %d seconds.",
2620                          sec, instance=instance)
2621                 return True
2622 
2623             # Note(PhilD): We can't assume that the Guest was able to process
2624             #              any previous shutdown signal (for example it may
2625             #              have still been startingup, so within the overall
2626             #              timeout we re-trigger the shutdown every
2627             #              retry_interval
2628             if retry_countdown == 0:
2629                 retry_countdown = retry_interval
2630                 # Instance could shutdown at any time, in which case we
2631                 # will get an exception when we call shutdown
2632                 try:
2633                     LOG.debug("Instance in state %s after %d seconds - "
2634                               "resending shutdown", state, sec,
2635                               instance=instance)
2636                     guest.shutdown()
2637                 except libvirt.libvirtError:
2638                     # Assume this is because its now shutdown, so loop
2639                     # one more time to clean up.
2640                     LOG.debug("Ignoring libvirt exception from shutdown "
2641                               "request.", instance=instance)
2642                     continue
2643             else:
2644                 retry_countdown -= 1
2645 
2646             time.sleep(1)
2647 
2648         LOG.info("Instance failed to shutdown in %d seconds.",
2649                  timeout, instance=instance)
2650         return False
2651 
2652     def power_off(self, instance, timeout=0, retry_interval=0):
2653         """Power off the specified instance."""
2654         if timeout:
2655             self._clean_shutdown(instance, timeout, retry_interval)
2656         self._destroy(instance)
2657 
2658     def power_on(self, context, instance, network_info,
2659                  block_device_info=None):
2660         """Power on the specified instance."""
2661         # We use _hard_reboot here to ensure that all backing files,
2662         # network, and block device connections, etc. are established
2663         # and available before we attempt to start the instance.
2664         self._hard_reboot(context, instance, network_info, block_device_info)
2665 
2666     def trigger_crash_dump(self, instance):
2667 
2668         """Trigger crash dump by injecting an NMI to the specified instance."""
2669         try:
2670             self._host.get_guest(instance).inject_nmi()
2671         except libvirt.libvirtError as ex:
2672             error_code = ex.get_error_code()
2673 
2674             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2675                 raise exception.TriggerCrashDumpNotSupported()
2676             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2677                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2678 
2679             LOG.exception(_('Error from libvirt while injecting an NMI to '
2680                             '%(instance_uuid)s: '
2681                             '[Error Code %(error_code)s] %(ex)s'),
2682                           {'instance_uuid': instance.uuid,
2683                            'error_code': error_code, 'ex': ex})
2684             raise
2685 
2686     def suspend(self, context, instance):
2687         """Suspend the specified instance."""
2688         guest = self._host.get_guest(instance)
2689 
2690         self._detach_pci_devices(guest,
2691             pci_manager.get_instance_pci_devs(instance))
2692         self._detach_direct_passthrough_ports(context, instance, guest)
2693         guest.save_memory_state()
2694 
2695     def resume(self, context, instance, network_info, block_device_info=None):
2696         """resume the specified instance."""
2697         xml = self._get_existing_domain_xml(instance, network_info,
2698                                             block_device_info)
2699         guest = self._create_domain_and_network(context, xml, instance,
2700                            network_info, block_device_info=block_device_info,
2701                            vifs_already_plugged=True)
2702         self._attach_pci_devices(guest,
2703             pci_manager.get_instance_pci_devs(instance))
2704         self._attach_direct_passthrough_ports(
2705             context, instance, guest, network_info)
2706         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2707                                                      instance)
2708         timer.start(interval=0.5).wait()
2709         guest.sync_guest_time()
2710 
2711     def resume_state_on_host_boot(self, context, instance, network_info,
2712                                   block_device_info=None):
2713         """resume guest state when a host is booted."""
2714         # Check if the instance is running already and avoid doing
2715         # anything if it is.
2716         try:
2717             guest = self._host.get_guest(instance)
2718             state = guest.get_power_state(self._host)
2719 
2720             ignored_states = (power_state.RUNNING,
2721                               power_state.SUSPENDED,
2722                               power_state.NOSTATE,
2723                               power_state.PAUSED)
2724 
2725             if state in ignored_states:
2726                 return
2727         except (exception.InternalError, exception.InstanceNotFound):
2728             pass
2729 
2730         # Instance is not up and could be in an unknown state.
2731         # Be as absolute as possible about getting it back into
2732         # a known and running state.
2733         self._hard_reboot(context, instance, network_info, block_device_info)
2734 
2735     def rescue(self, context, instance, network_info, image_meta,
2736                rescue_password):
2737         """Loads a VM using rescue images.
2738 
2739         A rescue is normally performed when something goes wrong with the
2740         primary images and data needs to be corrected/recovered. Rescuing
2741         should not edit or over-ride the original image, only allow for
2742         data recovery.
2743 
2744         """
2745         instance_dir = libvirt_utils.get_instance_path(instance)
2746         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2747         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2748         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2749 
2750         rescue_image_id = None
2751         if image_meta.obj_attr_is_set("id"):
2752             rescue_image_id = image_meta.id
2753 
2754         rescue_images = {
2755             'image_id': (rescue_image_id or
2756                         CONF.libvirt.rescue_image_id or instance.image_ref),
2757             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2758                           instance.kernel_id),
2759             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2760                            instance.ramdisk_id),
2761         }
2762         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2763                                             instance,
2764                                             image_meta,
2765                                             rescue=True)
2766         injection_info = InjectionInfo(network_info=network_info,
2767                                        admin_pass=rescue_password,
2768                                        files=None)
2769         gen_confdrive = functools.partial(self._create_configdrive,
2770                                           context, instance, injection_info,
2771                                           rescue=True)
2772         self._create_image(context, instance, disk_info['mapping'],
2773                            injection_info=injection_info, suffix='.rescue',
2774                            disk_images=rescue_images)
2775         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2776                                   image_meta, rescue=rescue_images)
2777         self._destroy(instance)
2778         self._create_domain(xml, post_xml_callback=gen_confdrive)
2779 
2780     def unrescue(self, instance, network_info):
2781         """Reboot the VM which is being rescued back into primary images.
2782         """
2783         instance_dir = libvirt_utils.get_instance_path(instance)
2784         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2785         xml = libvirt_utils.load_file(unrescue_xml_path)
2786         guest = self._host.get_guest(instance)
2787 
2788         # TODO(sahid): We are converting all calls from a
2789         # virDomain object to use nova.virt.libvirt.Guest.
2790         # We should be able to remove virt_dom at the end.
2791         virt_dom = guest._domain
2792         self._destroy(instance)
2793         self._create_domain(xml, virt_dom)
2794         os.unlink(unrescue_xml_path)
2795         rescue_files = os.path.join(instance_dir, "*.rescue")
2796         for rescue_file in glob.iglob(rescue_files):
2797             if os.path.isdir(rescue_file):
2798                 shutil.rmtree(rescue_file)
2799             else:
2800                 os.unlink(rescue_file)
2801         # cleanup rescue volume
2802         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2803                                 if lvmdisk.endswith('.rescue')])
2804         if CONF.libvirt.images_type == 'rbd':
2805             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
2806                                       disk.endswith('.rescue'))
2807             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
2808 
2809     def poll_rebooting_instances(self, timeout, instances):
2810         pass
2811 
2812     # NOTE(ilyaalekseyev): Implementation like in multinics
2813     # for xenapi(tr3buchet)
2814     def spawn(self, context, instance, image_meta, injected_files,
2815               admin_password, allocations, network_info=None,
2816               block_device_info=None):
2817         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2818                                             instance,
2819                                             image_meta,
2820                                             block_device_info)
2821         injection_info = InjectionInfo(network_info=network_info,
2822                                        files=injected_files,
2823                                        admin_pass=admin_password)
2824         gen_confdrive = functools.partial(self._create_configdrive,
2825                                           context, instance,
2826                                           injection_info)
2827         self._create_image(context, instance, disk_info['mapping'],
2828                            injection_info=injection_info,
2829                            block_device_info=block_device_info)
2830 
2831         # Required by Quobyte CI
2832         self._ensure_console_log_for_instance(instance)
2833 
2834         xml = self._get_guest_xml(context, instance, network_info,
2835                                   disk_info, image_meta,
2836                                   block_device_info=block_device_info)
2837         self._create_domain_and_network(
2838             context, xml, instance, network_info,
2839             block_device_info=block_device_info,
2840             post_xml_callback=gen_confdrive,
2841             destroy_disks_on_failure=True)
2842         LOG.debug("Instance is running", instance=instance)
2843 
2844         def _wait_for_boot():
2845             """Called at an interval until the VM is running."""
2846             state = self.get_info(instance).state
2847 
2848             if state == power_state.RUNNING:
2849                 LOG.info("Instance spawned successfully.", instance=instance)
2850                 raise loopingcall.LoopingCallDone()
2851 
2852         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
2853         timer.start(interval=0.5).wait()
2854 
2855     def _get_console_output_file(self, instance, console_log):
2856         bytes_to_read = MAX_CONSOLE_BYTES
2857         log_data = b""  # The last N read bytes
2858         i = 0  # in case there is a log rotation (like "virtlogd")
2859         path = console_log
2860 
2861         while bytes_to_read > 0 and os.path.exists(path):
2862             read_log_data, remaining = nova.privsep.path.last_bytes(
2863                                         path, bytes_to_read)
2864             # We need the log file content in chronological order,
2865             # that's why we *prepend* the log data.
2866             log_data = read_log_data + log_data
2867 
2868             # Prep to read the next file in the chain
2869             bytes_to_read -= len(read_log_data)
2870             path = console_log + "." + str(i)
2871             i += 1
2872 
2873             if remaining > 0:
2874                 LOG.info('Truncated console log returned, '
2875                          '%d bytes ignored', remaining, instance=instance)
2876         return log_data
2877 
2878     def get_console_output(self, context, instance):
2879         guest = self._host.get_guest(instance)
2880 
2881         xml = guest.get_xml_desc()
2882         tree = etree.fromstring(xml)
2883 
2884         # If the guest has a console logging to a file prefer to use that
2885         file_consoles = tree.findall("./devices/console[@type='file']")
2886         if file_consoles:
2887             for file_console in file_consoles:
2888                 source_node = file_console.find('./source')
2889                 if source_node is None:
2890                     continue
2891                 path = source_node.get("path")
2892                 if not path:
2893                     continue
2894 
2895                 if not os.path.exists(path):
2896                     LOG.info('Instance is configured with a file console, '
2897                              'but the backing file is not (yet?) present',
2898                              instance=instance)
2899                     return ""
2900 
2901                 return self._get_console_output_file(instance, path)
2902 
2903         # Try 'pty' types
2904         pty_consoles = tree.findall("./devices/console[@type='pty']")
2905         if pty_consoles:
2906             for pty_console in pty_consoles:
2907                 source_node = pty_console.find('./source')
2908                 if source_node is None:
2909                     continue
2910                 pty = source_node.get("path")
2911                 if not pty:
2912                     continue
2913                 break
2914             else:
2915                 raise exception.ConsoleNotAvailable()
2916         else:
2917             raise exception.ConsoleNotAvailable()
2918 
2919         console_log = self._get_console_log_path(instance)
2920         data = nova.privsep.libvirt.readpty(pty)
2921 
2922         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
2923         # which create a dedicated file device for the console logging.
2924         # Other virt_types like xen, lxc, uml, parallels depend on the
2925         # flush of that pty device into the "console.log" file to ensure
2926         # that a series of "get_console_output" calls return the complete
2927         # content even after rebooting a guest.
2928         nova.privsep.path.writefile(console_log, 'a+', data)
2929         return self._get_console_output_file(instance, console_log)
2930 
2931     def get_host_ip_addr(self):
2932         ips = compute_utils.get_machine_ips()
2933         if CONF.my_ip not in ips:
2934             LOG.warning('my_ip address (%(my_ip)s) was not found on '
2935                         'any of the interfaces: %(ifaces)s',
2936                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
2937         return CONF.my_ip
2938 
2939     def get_vnc_console(self, context, instance):
2940         def get_vnc_port_for_instance(instance_name):
2941             guest = self._host.get_guest(instance)
2942 
2943             xml = guest.get_xml_desc()
2944             xml_dom = etree.fromstring(xml)
2945 
2946             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
2947             if graphic is not None:
2948                 return graphic.get('port')
2949             # NOTE(rmk): We had VNC consoles enabled but the instance in
2950             # question is not actually listening for connections.
2951             raise exception.ConsoleTypeUnavailable(console_type='vnc')
2952 
2953         port = get_vnc_port_for_instance(instance.name)
2954         host = CONF.vnc.server_proxyclient_address
2955 
2956         return ctype.ConsoleVNC(host=host, port=port)
2957 
2958     def get_spice_console(self, context, instance):
2959         def get_spice_ports_for_instance(instance_name):
2960             guest = self._host.get_guest(instance)
2961 
2962             xml = guest.get_xml_desc()
2963             xml_dom = etree.fromstring(xml)
2964 
2965             graphic = xml_dom.find("./devices/graphics[@type='spice']")
2966             if graphic is not None:
2967                 return (graphic.get('port'), graphic.get('tlsPort'))
2968             # NOTE(rmk): We had Spice consoles enabled but the instance in
2969             # question is not actually listening for connections.
2970             raise exception.ConsoleTypeUnavailable(console_type='spice')
2971 
2972         ports = get_spice_ports_for_instance(instance.name)
2973         host = CONF.spice.server_proxyclient_address
2974 
2975         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
2976 
2977     def get_serial_console(self, context, instance):
2978         guest = self._host.get_guest(instance)
2979         for hostname, port in self._get_serial_ports_from_guest(
2980                 guest, mode='bind'):
2981             return ctype.ConsoleSerial(host=hostname, port=port)
2982         raise exception.ConsoleTypeUnavailable(console_type='serial')
2983 
2984     @staticmethod
2985     def _create_ephemeral(target, ephemeral_size,
2986                           fs_label, os_type, is_block_dev=False,
2987                           context=None, specified_fs=None,
2988                           vm_mode=None):
2989         if not is_block_dev:
2990             if (CONF.libvirt.virt_type == "parallels" and
2991                     vm_mode == fields.VMMode.EXE):
2992 
2993                 libvirt_utils.create_ploop_image('expanded', target,
2994                                                  '%dG' % ephemeral_size,
2995                                                  specified_fs)
2996                 return
2997             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
2998 
2999         # Run as root only for block devices.
3000         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3001                       specified_fs=specified_fs)
3002 
3003     @staticmethod
3004     def _create_swap(target, swap_mb, context=None):
3005         """Create a swap file of specified size."""
3006         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3007         utils.mkfs('swap', target)
3008 
3009     @staticmethod
3010     def _get_console_log_path(instance):
3011         return os.path.join(libvirt_utils.get_instance_path(instance),
3012                             'console.log')
3013 
3014     def _ensure_console_log_for_instance(self, instance):
3015         # NOTE(mdbooth): Although libvirt will create this file for us
3016         # automatically when it starts, it will initially create it with
3017         # root ownership and then chown it depending on the configuration of
3018         # the domain it is launching. Quobyte CI explicitly disables the
3019         # chown by setting dynamic_ownership=0 in libvirt's config.
3020         # Consequently when the domain starts it is unable to write to its
3021         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3022         #
3023         # To work around this, we create the file manually before starting
3024         # the domain so it has the same ownership as Nova. This works
3025         # for Quobyte CI because it is also configured to run qemu as the same
3026         # user as the Nova service. Installations which don't set
3027         # dynamic_ownership=0 are not affected because libvirt will always
3028         # correctly configure permissions regardless of initial ownership.
3029         #
3030         # Setting dynamic_ownership=0 is dubious and potentially broken in
3031         # more ways than console.log (see comment #22 on the above bug), so
3032         # Future Maintainer who finds this code problematic should check to see
3033         # if we still support it.
3034         console_file = self._get_console_log_path(instance)
3035         LOG.debug('Ensure instance console log exists: %s', console_file,
3036                   instance=instance)
3037         try:
3038             libvirt_utils.file_open(console_file, 'a').close()
3039         # NOTE(sfinucan): We can safely ignore permission issues here and
3040         # assume that it is libvirt that has taken ownership of this file.
3041         except IOError as ex:
3042             if ex.errno != errno.EACCES:
3043                 raise
3044             LOG.debug('Console file already exists: %s.', console_file)
3045 
3046     @staticmethod
3047     def _get_disk_config_image_type():
3048         # TODO(mikal): there is a bug here if images_type has
3049         # changed since creation of the instance, but I am pretty
3050         # sure that this bug already exists.
3051         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3052 
3053     @staticmethod
3054     def _is_booted_from_volume(block_device_info):
3055         """Determines whether the VM is booting from volume
3056 
3057         Determines whether the block device info indicates that the VM
3058         is booting from a volume.
3059         """
3060         block_device_mapping = driver.block_device_info_get_mapping(
3061             block_device_info)
3062         return bool(block_device.get_root_bdm(block_device_mapping))
3063 
3064     def _inject_data(self, disk, instance, injection_info):
3065         """Injects data in a disk image
3066 
3067         Helper used for injecting data in a disk image file system.
3068 
3069         :param disk: The disk we're injecting into (an Image object)
3070         :param instance: The instance we're injecting into
3071         :param injection_info: Injection info
3072         """
3073         # Handles the partition need to be used.
3074         LOG.debug('Checking root disk injection %s',
3075                   str(injection_info), instance=instance)
3076         target_partition = None
3077         if not instance.kernel_id:
3078             target_partition = CONF.libvirt.inject_partition
3079             if target_partition == 0:
3080                 target_partition = None
3081         if CONF.libvirt.virt_type == 'lxc':
3082             target_partition = None
3083 
3084         # Handles the key injection.
3085         if CONF.libvirt.inject_key and instance.get('key_data'):
3086             key = str(instance.key_data)
3087         else:
3088             key = None
3089 
3090         # Handles the admin password injection.
3091         if not CONF.libvirt.inject_password:
3092             admin_pass = None
3093         else:
3094             admin_pass = injection_info.admin_pass
3095 
3096         # Handles the network injection.
3097         net = netutils.get_injected_network_template(
3098             injection_info.network_info,
3099             libvirt_virt_type=CONF.libvirt.virt_type)
3100 
3101         # Handles the metadata injection
3102         metadata = instance.get('metadata')
3103 
3104         if any((key, net, metadata, admin_pass, injection_info.files)):
3105             LOG.debug('Injecting %s', str(injection_info),
3106                       instance=instance)
3107             img_id = instance.image_ref
3108             try:
3109                 disk_api.inject_data(disk.get_model(self._conn),
3110                                      key, net, metadata, admin_pass,
3111                                      injection_info.files,
3112                                      partition=target_partition,
3113                                      mandatory=('files',))
3114             except Exception as e:
3115                 with excutils.save_and_reraise_exception():
3116                     LOG.error('Error injecting data into image '
3117                               '%(img_id)s (%(e)s)',
3118                               {'img_id': img_id, 'e': e},
3119                               instance=instance)
3120 
3121     # NOTE(sileht): many callers of this method assume that this
3122     # method doesn't fail if an image already exists but instead
3123     # think that it will be reused (ie: (live)-migration/resize)
3124     def _create_image(self, context, instance,
3125                       disk_mapping, injection_info=None, suffix='',
3126                       disk_images=None, block_device_info=None,
3127                       fallback_from_host=None,
3128                       ignore_bdi_for_swap=False):
3129         booted_from_volume = self._is_booted_from_volume(block_device_info)
3130 
3131         def image(fname, image_type=CONF.libvirt.images_type):
3132             return self.image_backend.by_name(instance,
3133                                               fname + suffix, image_type)
3134 
3135         def raw(fname):
3136             return image(fname, image_type='raw')
3137 
3138         # ensure directories exist and are writable
3139         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3140 
3141         LOG.info('Creating image', instance=instance)
3142 
3143         inst_type = instance.get_flavor()
3144         swap_mb = 0
3145         if 'disk.swap' in disk_mapping:
3146             mapping = disk_mapping['disk.swap']
3147 
3148             if ignore_bdi_for_swap:
3149                 # This is a workaround to support legacy swap resizing,
3150                 # which does not touch swap size specified in bdm,
3151                 # but works with flavor specified size only.
3152                 # In this case we follow the legacy logic and ignore block
3153                 # device info completely.
3154                 # NOTE(ft): This workaround must be removed when a correct
3155                 # implementation of resize operation changing sizes in bdms is
3156                 # developed. Also at that stage we probably may get rid of
3157                 # the direct usage of flavor swap size here,
3158                 # leaving the work with bdm only.
3159                 swap_mb = inst_type['swap']
3160             else:
3161                 swap = driver.block_device_info_get_swap(block_device_info)
3162                 if driver.swap_is_usable(swap):
3163                     swap_mb = swap['swap_size']
3164                 elif (inst_type['swap'] > 0 and
3165                       not block_device.volume_in_mapping(
3166                         mapping['dev'], block_device_info)):
3167                     swap_mb = inst_type['swap']
3168 
3169             if swap_mb > 0:
3170                 if (CONF.libvirt.virt_type == "parallels" and
3171                         instance.vm_mode == fields.VMMode.EXE):
3172                     msg = _("Swap disk is not supported "
3173                             "for Virtuozzo container")
3174                     raise exception.Invalid(msg)
3175 
3176         if not disk_images:
3177             disk_images = {'image_id': instance.image_ref,
3178                            'kernel_id': instance.kernel_id,
3179                            'ramdisk_id': instance.ramdisk_id}
3180 
3181         if disk_images['kernel_id']:
3182             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3183             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3184                                 context=context,
3185                                 filename=fname,
3186                                 image_id=disk_images['kernel_id'])
3187             if disk_images['ramdisk_id']:
3188                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3189                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3190                                      context=context,
3191                                      filename=fname,
3192                                      image_id=disk_images['ramdisk_id'])
3193 
3194         if CONF.libvirt.virt_type == 'uml':
3195             # PONDERING(mikal): can I assume that root is UID zero in every
3196             # OS? Probably not.
3197             uid = pwd.getpwnam('root').pw_uid
3198             nova.privsep.path.chown(image('disk').path, uid=uid)
3199 
3200         self._create_and_inject_local_root(context, instance,
3201                                            booted_from_volume, suffix,
3202                                            disk_images, injection_info,
3203                                            fallback_from_host)
3204 
3205         # Lookup the filesystem type if required
3206         os_type_with_default = disk_api.get_fs_type_for_os_type(
3207             instance.os_type)
3208         # Generate a file extension based on the file system
3209         # type and the mkfs commands configured if any
3210         file_extension = disk_api.get_file_extension_for_os_type(
3211                                                           os_type_with_default)
3212 
3213         vm_mode = fields.VMMode.get_from_instance(instance)
3214         ephemeral_gb = instance.flavor.ephemeral_gb
3215         if 'disk.local' in disk_mapping:
3216             disk_image = image('disk.local')
3217             fn = functools.partial(self._create_ephemeral,
3218                                    fs_label='ephemeral0',
3219                                    os_type=instance.os_type,
3220                                    is_block_dev=disk_image.is_block_dev,
3221                                    vm_mode=vm_mode)
3222             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3223             size = ephemeral_gb * units.Gi
3224             disk_image.cache(fetch_func=fn,
3225                              context=context,
3226                              filename=fname,
3227                              size=size,
3228                              ephemeral_size=ephemeral_gb)
3229 
3230         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3231                 block_device_info)):
3232             disk_image = image(blockinfo.get_eph_disk(idx))
3233 
3234             specified_fs = eph.get('guest_format')
3235             if specified_fs and not self.is_supported_fs_format(specified_fs):
3236                 msg = _("%s format is not supported") % specified_fs
3237                 raise exception.InvalidBDMFormat(details=msg)
3238 
3239             fn = functools.partial(self._create_ephemeral,
3240                                    fs_label='ephemeral%d' % idx,
3241                                    os_type=instance.os_type,
3242                                    is_block_dev=disk_image.is_block_dev,
3243                                    vm_mode=vm_mode)
3244             size = eph['size'] * units.Gi
3245             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3246             disk_image.cache(fetch_func=fn,
3247                              context=context,
3248                              filename=fname,
3249                              size=size,
3250                              ephemeral_size=eph['size'],
3251                              specified_fs=specified_fs)
3252 
3253         if swap_mb > 0:
3254             size = swap_mb * units.Mi
3255             image('disk.swap').cache(fetch_func=self._create_swap,
3256                                      context=context,
3257                                      filename="swap_%s" % swap_mb,
3258                                      size=size,
3259                                      swap_mb=swap_mb)
3260 
3261     def _create_and_inject_local_root(self, context, instance,
3262                                       booted_from_volume, suffix, disk_images,
3263                                       injection_info, fallback_from_host):
3264         # File injection only if needed
3265         need_inject = (not configdrive.required_by(instance) and
3266                        injection_info is not None and
3267                        CONF.libvirt.inject_partition != -2)
3268 
3269         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3270         # currently happens only on rescue - we still don't want to
3271         # create a base image.
3272         if not booted_from_volume:
3273             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3274             size = instance.flavor.root_gb * units.Gi
3275 
3276             if size == 0 or suffix == '.rescue':
3277                 size = None
3278 
3279             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3280                                                  CONF.libvirt.images_type)
3281             if instance.task_state == task_states.RESIZE_FINISH:
3282                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3283             if backend.SUPPORTS_CLONE:
3284                 def clone_fallback_to_fetch(*args, **kwargs):
3285                     try:
3286                         backend.clone(context, disk_images['image_id'])
3287                     except exception.ImageUnacceptable:
3288                         libvirt_utils.fetch_image(*args, **kwargs)
3289                 fetch_func = clone_fallback_to_fetch
3290             else:
3291                 fetch_func = libvirt_utils.fetch_image
3292             self._try_fetch_image_cache(backend, fetch_func, context,
3293                                         root_fname, disk_images['image_id'],
3294                                         instance, size, fallback_from_host)
3295 
3296             if need_inject:
3297                 self._inject_data(backend, instance, injection_info)
3298 
3299         elif need_inject:
3300             LOG.warning('File injection into a boot from volume '
3301                         'instance is not supported', instance=instance)
3302 
3303     def _create_configdrive(self, context, instance, injection_info,
3304                             rescue=False):
3305         # As this method being called right after the definition of a
3306         # domain, but before its actual launch, device metadata will be built
3307         # and saved in the instance for it to be used by the config drive and
3308         # the metadata service.
3309         instance.device_metadata = self._build_device_metadata(context,
3310                                                                instance)
3311         if configdrive.required_by(instance):
3312             LOG.info('Using config drive', instance=instance)
3313 
3314             name = 'disk.config'
3315             if rescue:
3316                 name += '.rescue'
3317 
3318             config_disk = self.image_backend.by_name(
3319                 instance, name, self._get_disk_config_image_type())
3320 
3321             # Don't overwrite an existing config drive
3322             if not config_disk.exists():
3323                 extra_md = {}
3324                 if injection_info.admin_pass:
3325                     extra_md['admin_pass'] = injection_info.admin_pass
3326 
3327                 inst_md = instance_metadata.InstanceMetadata(
3328                     instance, content=injection_info.files, extra_md=extra_md,
3329                     network_info=injection_info.network_info,
3330                     request_context=context)
3331 
3332                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3333                 with cdb:
3334                     # NOTE(mdbooth): We're hardcoding here the path of the
3335                     # config disk when using the flat backend. This isn't
3336                     # good, but it's required because we need a local path we
3337                     # know we can write to in case we're subsequently
3338                     # importing into rbd. This will be cleaned up when we
3339                     # replace this with a call to create_from_func, but that
3340                     # can't happen until we've updated the backends and we
3341                     # teach them not to cache config disks. This isn't
3342                     # possible while we're still using cache() under the hood.
3343                     config_disk_local_path = os.path.join(
3344                         libvirt_utils.get_instance_path(instance), name)
3345                     LOG.info('Creating config drive at %(path)s',
3346                              {'path': config_disk_local_path},
3347                              instance=instance)
3348 
3349                     try:
3350                         cdb.make_drive(config_disk_local_path)
3351                     except processutils.ProcessExecutionError as e:
3352                         with excutils.save_and_reraise_exception():
3353                             LOG.error('Creating config drive failed with '
3354                                       'error: %s', e, instance=instance)
3355 
3356                 try:
3357                     config_disk.import_file(
3358                         instance, config_disk_local_path, name)
3359                 finally:
3360                     # NOTE(mikal): if the config drive was imported into RBD,
3361                     # then we no longer need the local copy
3362                     if CONF.libvirt.images_type == 'rbd':
3363                         LOG.info('Deleting local config drive %(path)s '
3364                                  'because it was imported into RBD.',
3365                                  {'path': config_disk_local_path},
3366                                  instance=instance)
3367                         os.unlink(config_disk_local_path)
3368 
3369     def _prepare_pci_devices_for_use(self, pci_devices):
3370         # kvm , qemu support managed mode
3371         # In managed mode, the configured device will be automatically
3372         # detached from the host OS drivers when the guest is started,
3373         # and then re-attached when the guest shuts down.
3374         if CONF.libvirt.virt_type != 'xen':
3375             # we do manual detach only for xen
3376             return
3377         try:
3378             for dev in pci_devices:
3379                 libvirt_dev_addr = dev['hypervisor_name']
3380                 libvirt_dev = \
3381                         self._host.device_lookup_by_name(libvirt_dev_addr)
3382                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3383                 # http://libvirt.org/html/libvirt-libvirt.html.
3384                 libvirt_dev.dettach()
3385 
3386             # Note(yjiang5): A reset of one PCI device may impact other
3387             # devices on the same bus, thus we need two separated loops
3388             # to detach and then reset it.
3389             for dev in pci_devices:
3390                 libvirt_dev_addr = dev['hypervisor_name']
3391                 libvirt_dev = \
3392                         self._host.device_lookup_by_name(libvirt_dev_addr)
3393                 libvirt_dev.reset()
3394 
3395         except libvirt.libvirtError as exc:
3396             raise exception.PciDevicePrepareFailed(id=dev['id'],
3397                                                    instance_uuid=
3398                                                    dev['instance_uuid'],
3399                                                    reason=six.text_type(exc))
3400 
3401     def _detach_pci_devices(self, guest, pci_devs):
3402         try:
3403             for dev in pci_devs:
3404                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3405                 # after detachDeviceFlags returned, we should check the dom to
3406                 # ensure the detaching is finished
3407                 xml = guest.get_xml_desc()
3408                 xml_doc = etree.fromstring(xml)
3409                 guest_config = vconfig.LibvirtConfigGuest()
3410                 guest_config.parse_dom(xml_doc)
3411 
3412                 for hdev in [d for d in guest_config.devices
3413                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3414                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3415                     dbsf = pci_utils.parse_address(dev.address)
3416                     if [int(x, 16) for x in hdbsf] ==\
3417                             [int(x, 16) for x in dbsf]:
3418                         raise exception.PciDeviceDetachFailed(reason=
3419                                                               "timeout",
3420                                                               dev=dev)
3421 
3422         except libvirt.libvirtError as ex:
3423             error_code = ex.get_error_code()
3424             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3425                 LOG.warning("Instance disappeared while detaching "
3426                             "a PCI device from it.")
3427             else:
3428                 raise
3429 
3430     def _attach_pci_devices(self, guest, pci_devs):
3431         try:
3432             for dev in pci_devs:
3433                 guest.attach_device(self._get_guest_pci_device(dev))
3434 
3435         except libvirt.libvirtError:
3436             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3437                       {'dev': pci_devs, 'dom': guest.id})
3438             raise
3439 
3440     @staticmethod
3441     def _has_direct_passthrough_port(network_info):
3442         for vif in network_info:
3443             if (vif['vnic_type'] in
3444                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3445                 return True
3446         return False
3447 
3448     def _attach_direct_passthrough_ports(
3449         self, context, instance, guest, network_info=None):
3450         if network_info is None:
3451             network_info = instance.info_cache.network_info
3452         if network_info is None:
3453             return
3454 
3455         if self._has_direct_passthrough_port(network_info):
3456             for vif in network_info:
3457                 if (vif['vnic_type'] in
3458                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3459                     cfg = self.vif_driver.get_config(instance,
3460                                                      vif,
3461                                                      instance.image_meta,
3462                                                      instance.flavor,
3463                                                      CONF.libvirt.virt_type,
3464                                                      self._host)
3465                     LOG.debug('Attaching direct passthrough port %(port)s '
3466                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3467                               instance=instance)
3468                     guest.attach_device(cfg)
3469 
3470     def _detach_direct_passthrough_ports(self, context, instance, guest):
3471         network_info = instance.info_cache.network_info
3472         if network_info is None:
3473             return
3474 
3475         if self._has_direct_passthrough_port(network_info):
3476             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3477             # pci request per direct passthrough port. Therefore we can trust
3478             # that pci_slot value in the vif is correct.
3479             direct_passthrough_pci_addresses = [
3480                 vif['profile']['pci_slot']
3481                 for vif in network_info
3482                 if (vif['vnic_type'] in
3483                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3484                     vif['profile'].get('pci_slot') is not None)
3485             ]
3486 
3487             # use detach_pci_devices to avoid failure in case of
3488             # multiple guest direct passthrough ports with the same MAC
3489             # (protection use-case, ports are on different physical
3490             # interfaces)
3491             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3492             direct_passthrough_pci_addresses = (
3493                 [pci_dev for pci_dev in pci_devs
3494                  if pci_dev.address in direct_passthrough_pci_addresses])
3495             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3496 
3497     def _set_host_enabled(self, enabled,
3498                           disable_reason=DISABLE_REASON_UNDEFINED):
3499         """Enables / Disables the compute service on this host.
3500 
3501            This doesn't override non-automatic disablement with an automatic
3502            setting; thereby permitting operators to keep otherwise
3503            healthy hosts out of rotation.
3504         """
3505 
3506         status_name = {True: 'disabled',
3507                        False: 'enabled'}
3508 
3509         disable_service = not enabled
3510 
3511         ctx = nova_context.get_admin_context()
3512         try:
3513             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3514 
3515             if service.disabled != disable_service:
3516                 # Note(jang): this is a quick fix to stop operator-
3517                 # disabled compute hosts from re-enabling themselves
3518                 # automatically. We prefix any automatic reason code
3519                 # with a fixed string. We only re-enable a host
3520                 # automatically if we find that string in place.
3521                 # This should probably be replaced with a separate flag.
3522                 if not service.disabled or (
3523                         service.disabled_reason and
3524                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3525                     service.disabled = disable_service
3526                     service.disabled_reason = (
3527                        DISABLE_PREFIX + disable_reason
3528                        if disable_service and disable_reason else
3529                            DISABLE_REASON_UNDEFINED)
3530                     service.save()
3531                     LOG.debug('Updating compute service status to %s',
3532                               status_name[disable_service])
3533                 else:
3534                     LOG.debug('Not overriding manual compute service '
3535                               'status with: %s',
3536                               status_name[disable_service])
3537         except exception.ComputeHostNotFound:
3538             LOG.warning('Cannot update service status on host "%s" '
3539                         'since it is not registered.', CONF.host)
3540         except Exception:
3541             LOG.warning('Cannot update service status on host "%s" '
3542                         'due to an unexpected exception.', CONF.host,
3543                         exc_info=True)
3544 
3545         if enabled:
3546             mount.get_manager().host_up(self._host)
3547         else:
3548             mount.get_manager().host_down()
3549 
3550     def _get_guest_cpu_model_config(self):
3551         mode = CONF.libvirt.cpu_mode
3552         model = CONF.libvirt.cpu_model
3553 
3554         if (CONF.libvirt.virt_type == "kvm" or
3555             CONF.libvirt.virt_type == "qemu"):
3556             if mode is None:
3557                 caps = self._host.get_capabilities()
3558                 # AArch64 lacks 'host-model' support because neither libvirt
3559                 # nor QEMU are able to tell what the host CPU model exactly is.
3560                 # And there is no CPU description code for ARM(64) at this
3561                 # point.
3562 
3563                 # Also worth noting: 'host-passthrough' mode will completely
3564                 # break live migration, *unless* all the Compute nodes (running
3565                 # libvirtd) have *identical* CPUs.
3566                 if caps.host.cpu.arch == fields.Architecture.AARCH64:
3567                     mode = "host-passthrough"
3568                     LOG.info('CPU mode "host-passthrough" was chosen. Live '
3569                              'migration can break unless all compute nodes '
3570                              'have identical cpus. AArch64 does not support '
3571                              'other modes.')
3572                 else:
3573                     mode = "host-model"
3574             if mode == "none":
3575                 return vconfig.LibvirtConfigGuestCPU()
3576         else:
3577             if mode is None or mode == "none":
3578                 return None
3579 
3580         if ((CONF.libvirt.virt_type != "kvm" and
3581              CONF.libvirt.virt_type != "qemu")):
3582             msg = _("Config requested an explicit CPU model, but "
3583                     "the current libvirt hypervisor '%s' does not "
3584                     "support selecting CPU models") % CONF.libvirt.virt_type
3585             raise exception.Invalid(msg)
3586 
3587         if mode == "custom" and model is None:
3588             msg = _("Config requested a custom CPU model, but no "
3589                     "model name was provided")
3590             raise exception.Invalid(msg)
3591         elif mode != "custom" and model is not None:
3592             msg = _("A CPU model name should not be set when a "
3593                     "host CPU model is requested")
3594             raise exception.Invalid(msg)
3595 
3596         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3597                   {'mode': mode, 'model': (model or "")})
3598 
3599         cpu = vconfig.LibvirtConfigGuestCPU()
3600         cpu.mode = mode
3601         cpu.model = model
3602 
3603         return cpu
3604 
3605     def _get_guest_cpu_config(self, flavor, image_meta,
3606                               guest_cpu_numa_config, instance_numa_topology):
3607         cpu = self._get_guest_cpu_model_config()
3608 
3609         if cpu is None:
3610             return None
3611 
3612         topology = hardware.get_best_cpu_topology(
3613                 flavor, image_meta, numa_topology=instance_numa_topology)
3614 
3615         cpu.sockets = topology.sockets
3616         cpu.cores = topology.cores
3617         cpu.threads = topology.threads
3618         cpu.numa = guest_cpu_numa_config
3619 
3620         return cpu
3621 
3622     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3623                                image_type=None):
3624         disk_unit = None
3625         disk = self.image_backend.by_name(instance, name, image_type)
3626         if (name == 'disk.config' and image_type == 'rbd' and
3627                 not disk.exists()):
3628             # This is likely an older config drive that has not been migrated
3629             # to rbd yet. Try to fall back on 'flat' image type.
3630             # TODO(melwitt): Add online migration of some sort so we can
3631             # remove this fall back once we know all config drives are in rbd.
3632             # NOTE(vladikr): make sure that the flat image exist, otherwise
3633             # the image will be created after the domain definition.
3634             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3635             if flat_disk.exists():
3636                 disk = flat_disk
3637                 LOG.debug('Config drive not found in RBD, falling back to the '
3638                           'instance directory', instance=instance)
3639         disk_info = disk_mapping[name]
3640         if 'unit' in disk_mapping:
3641             disk_unit = disk_mapping['unit']
3642             disk_mapping['unit'] += 1  # Increments for the next disk added
3643         conf = disk.libvirt_info(disk_info['bus'],
3644                                  disk_info['dev'],
3645                                  disk_info['type'],
3646                                  self.disk_cachemode,
3647                                  inst_type['extra_specs'],
3648                                  self._host.get_version(),
3649                                  disk_unit=disk_unit)
3650         return conf
3651 
3652     def _get_guest_fs_config(self, instance, name, image_type=None):
3653         disk = self.image_backend.by_name(instance, name, image_type)
3654         return disk.libvirt_fs_info("/", "ploop")
3655 
3656     def _get_guest_storage_config(self, instance, image_meta,
3657                                   disk_info,
3658                                   rescue, block_device_info,
3659                                   inst_type, os_type):
3660         devices = []
3661         disk_mapping = disk_info['mapping']
3662 
3663         block_device_mapping = driver.block_device_info_get_mapping(
3664             block_device_info)
3665         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3666         scsi_controller = self._get_scsi_controller(image_meta)
3667 
3668         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3669             # The virtio-scsi can handle up to 256 devices but the
3670             # optional element "address" must be defined to describe
3671             # where the device is placed on the controller (see:
3672             # LibvirtConfigGuestDeviceAddressDrive).
3673             #
3674             # Note about why it's added in disk_mapping: It's not
3675             # possible to pass an 'int' by reference in Python, so we
3676             # use disk_mapping as container to keep reference of the
3677             # unit added and be able to increment it for each disk
3678             # added.
3679             disk_mapping['unit'] = 0
3680 
3681         def _get_ephemeral_devices():
3682             eph_devices = []
3683             for idx, eph in enumerate(
3684                 driver.block_device_info_get_ephemerals(
3685                     block_device_info)):
3686                 diskeph = self._get_guest_disk_config(
3687                     instance,
3688                     blockinfo.get_eph_disk(idx),
3689                     disk_mapping, inst_type)
3690                 eph_devices.append(diskeph)
3691             return eph_devices
3692 
3693         if mount_rootfs:
3694             fs = vconfig.LibvirtConfigGuestFilesys()
3695             fs.source_type = "mount"
3696             fs.source_dir = os.path.join(
3697                 libvirt_utils.get_instance_path(instance), 'rootfs')
3698             devices.append(fs)
3699         elif (os_type == fields.VMMode.EXE and
3700               CONF.libvirt.virt_type == "parallels"):
3701             if rescue:
3702                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3703                 devices.append(fsrescue)
3704 
3705                 fsos = self._get_guest_fs_config(instance, "disk")
3706                 fsos.target_dir = "/mnt/rescue"
3707                 devices.append(fsos)
3708             else:
3709                 if 'disk' in disk_mapping:
3710                     fs = self._get_guest_fs_config(instance, "disk")
3711                     devices.append(fs)
3712                 devices = devices + _get_ephemeral_devices()
3713         else:
3714 
3715             if rescue:
3716                 diskrescue = self._get_guest_disk_config(instance,
3717                                                          'disk.rescue',
3718                                                          disk_mapping,
3719                                                          inst_type)
3720                 devices.append(diskrescue)
3721 
3722                 diskos = self._get_guest_disk_config(instance,
3723                                                      'disk',
3724                                                      disk_mapping,
3725                                                      inst_type)
3726                 devices.append(diskos)
3727             else:
3728                 if 'disk' in disk_mapping:
3729                     diskos = self._get_guest_disk_config(instance,
3730                                                          'disk',
3731                                                          disk_mapping,
3732                                                          inst_type)
3733                     devices.append(diskos)
3734 
3735                 if 'disk.local' in disk_mapping:
3736                     disklocal = self._get_guest_disk_config(instance,
3737                                                             'disk.local',
3738                                                             disk_mapping,
3739                                                             inst_type)
3740                     devices.append(disklocal)
3741                     instance.default_ephemeral_device = (
3742                         block_device.prepend_dev(disklocal.target_dev))
3743 
3744                 devices = devices + _get_ephemeral_devices()
3745 
3746                 if 'disk.swap' in disk_mapping:
3747                     diskswap = self._get_guest_disk_config(instance,
3748                                                            'disk.swap',
3749                                                            disk_mapping,
3750                                                            inst_type)
3751                     devices.append(diskswap)
3752                     instance.default_swap_device = (
3753                         block_device.prepend_dev(diskswap.target_dev))
3754 
3755             config_name = 'disk.config.rescue' if rescue else 'disk.config'
3756             if config_name in disk_mapping:
3757                 diskconfig = self._get_guest_disk_config(
3758                     instance, config_name, disk_mapping, inst_type,
3759                     self._get_disk_config_image_type())
3760                 devices.append(diskconfig)
3761 
3762         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3763                                                    mount_rootfs):
3764             connection_info = vol['connection_info']
3765             vol_dev = block_device.prepend_dev(vol['mount_device'])
3766             info = disk_mapping[vol_dev]
3767             self._connect_volume(connection_info, instance)
3768             if scsi_controller and scsi_controller.model == 'virtio-scsi':
3769                 info['unit'] = disk_mapping['unit']
3770                 disk_mapping['unit'] += 1
3771             cfg = self._get_volume_config(connection_info, info)
3772             devices.append(cfg)
3773             vol['connection_info'] = connection_info
3774             vol.save()
3775 
3776         for d in devices:
3777             self._set_cache_mode(d)
3778 
3779         if scsi_controller:
3780             devices.append(scsi_controller)
3781 
3782         return devices
3783 
3784     @staticmethod
3785     def _get_scsi_controller(image_meta):
3786         """Return scsi controller or None based on image meta"""
3787         # TODO(sahid): should raise an exception for an invalid controller
3788         if image_meta.properties.get('hw_scsi_model'):
3789             hw_scsi_model = image_meta.properties.hw_scsi_model
3790             scsi_controller = vconfig.LibvirtConfigGuestController()
3791             scsi_controller.type = 'scsi'
3792             scsi_controller.model = hw_scsi_model
3793             scsi_controller.index = 0
3794             return scsi_controller
3795 
3796     def _get_host_sysinfo_serial_hardware(self):
3797         """Get a UUID from the host hardware
3798 
3799         Get a UUID for the host hardware reported by libvirt.
3800         This is typically from the SMBIOS data, unless it has
3801         been overridden in /etc/libvirt/libvirtd.conf
3802         """
3803         caps = self._host.get_capabilities()
3804         return caps.host.uuid
3805 
3806     def _get_host_sysinfo_serial_os(self):
3807         """Get a UUID from the host operating system
3808 
3809         Get a UUID for the host operating system. Modern Linux
3810         distros based on systemd provide a /etc/machine-id
3811         file containing a UUID. This is also provided inside
3812         systemd based containers and can be provided by other
3813         init systems too, since it is just a plain text file.
3814         """
3815         if not os.path.exists("/etc/machine-id"):
3816             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
3817             raise exception.InternalError(msg)
3818 
3819         with open("/etc/machine-id") as f:
3820             # We want to have '-' in the right place
3821             # so we parse & reformat the value
3822             lines = f.read().split()
3823             if not lines:
3824                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
3825                 raise exception.InternalError(msg)
3826 
3827             return str(uuid.UUID(lines[0]))
3828 
3829     def _get_host_sysinfo_serial_auto(self):
3830         if os.path.exists("/etc/machine-id"):
3831             return self._get_host_sysinfo_serial_os()
3832         else:
3833             return self._get_host_sysinfo_serial_hardware()
3834 
3835     def _get_guest_config_sysinfo(self, instance):
3836         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
3837 
3838         sysinfo.system_manufacturer = version.vendor_string()
3839         sysinfo.system_product = version.product_string()
3840         sysinfo.system_version = version.version_string_with_package()
3841 
3842         sysinfo.system_serial = self._sysinfo_serial_func()
3843         sysinfo.system_uuid = instance.uuid
3844 
3845         sysinfo.system_family = "Virtual Machine"
3846 
3847         return sysinfo
3848 
3849     def _get_guest_pci_device(self, pci_device):
3850 
3851         dbsf = pci_utils.parse_address(pci_device.address)
3852         dev = vconfig.LibvirtConfigGuestHostdevPCI()
3853         dev.domain, dev.bus, dev.slot, dev.function = dbsf
3854 
3855         # only kvm support managed mode
3856         if CONF.libvirt.virt_type in ('xen', 'parallels',):
3857             dev.managed = 'no'
3858         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3859             dev.managed = 'yes'
3860 
3861         return dev
3862 
3863     def _get_guest_config_meta(self, instance):
3864         """Get metadata config for guest."""
3865 
3866         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
3867         meta.package = version.version_string_with_package()
3868         meta.name = instance.display_name
3869         meta.creationTime = time.time()
3870 
3871         if instance.image_ref not in ("", None):
3872             meta.roottype = "image"
3873             meta.rootid = instance.image_ref
3874 
3875         system_meta = instance.system_metadata
3876         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
3877         ometa.userid = instance.user_id
3878         ometa.username = system_meta.get('owner_user_name', 'N/A')
3879         ometa.projectid = instance.project_id
3880         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
3881         meta.owner = ometa
3882 
3883         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
3884         flavor = instance.flavor
3885         fmeta.name = flavor.name
3886         fmeta.memory = flavor.memory_mb
3887         fmeta.vcpus = flavor.vcpus
3888         fmeta.ephemeral = flavor.ephemeral_gb
3889         fmeta.disk = flavor.root_gb
3890         fmeta.swap = flavor.swap
3891 
3892         meta.flavor = fmeta
3893 
3894         return meta
3895 
3896     def _machine_type_mappings(self):
3897         mappings = {}
3898         for mapping in CONF.libvirt.hw_machine_type:
3899             host_arch, _, machine_type = mapping.partition('=')
3900             mappings[host_arch] = machine_type
3901         return mappings
3902 
3903     def _get_machine_type(self, image_meta, caps):
3904         # The underlying machine type can be set as an image attribute,
3905         # or otherwise based on some architecture specific defaults
3906 
3907         mach_type = None
3908 
3909         if image_meta.properties.get('hw_machine_type') is not None:
3910             mach_type = image_meta.properties.hw_machine_type
3911         else:
3912             # For ARM systems we will default to vexpress-a15 for armv7
3913             # and virt for aarch64
3914             if caps.host.cpu.arch == fields.Architecture.ARMV7:
3915                 mach_type = "vexpress-a15"
3916 
3917             if caps.host.cpu.arch == fields.Architecture.AARCH64:
3918                 mach_type = "virt"
3919 
3920             if caps.host.cpu.arch in (fields.Architecture.S390,
3921                                       fields.Architecture.S390X):
3922                 mach_type = 's390-ccw-virtio'
3923 
3924             # If set in the config, use that as the default.
3925             if CONF.libvirt.hw_machine_type:
3926                 mappings = self._machine_type_mappings()
3927                 mach_type = mappings.get(caps.host.cpu.arch)
3928 
3929         return mach_type
3930 
3931     @staticmethod
3932     def _create_idmaps(klass, map_strings):
3933         idmaps = []
3934         if len(map_strings) > 5:
3935             map_strings = map_strings[0:5]
3936             LOG.warning("Too many id maps, only included first five.")
3937         for map_string in map_strings:
3938             try:
3939                 idmap = klass()
3940                 values = [int(i) for i in map_string.split(":")]
3941                 idmap.start = values[0]
3942                 idmap.target = values[1]
3943                 idmap.count = values[2]
3944                 idmaps.append(idmap)
3945             except (ValueError, IndexError):
3946                 LOG.warning("Invalid value for id mapping %s", map_string)
3947         return idmaps
3948 
3949     def _get_guest_idmaps(self):
3950         id_maps = []
3951         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
3952             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
3953                                            CONF.libvirt.uid_maps)
3954             id_maps.extend(uid_maps)
3955         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
3956             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
3957                                            CONF.libvirt.gid_maps)
3958             id_maps.extend(gid_maps)
3959         return id_maps
3960 
3961     def _update_guest_cputune(self, guest, flavor, virt_type):
3962         is_able = self._host.is_cpu_control_policy_capable()
3963 
3964         cputuning = ['shares', 'period', 'quota']
3965         wants_cputune = any([k for k in cputuning
3966             if "quota:cpu_" + k in flavor.extra_specs.keys()])
3967 
3968         if wants_cputune and not is_able:
3969             raise exception.UnsupportedHostCPUControlPolicy()
3970 
3971         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
3972             return
3973 
3974         if guest.cputune is None:
3975             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
3976             # Setting the default cpu.shares value to be a value
3977             # dependent on the number of vcpus
3978         guest.cputune.shares = 1024 * guest.vcpus
3979 
3980         for name in cputuning:
3981             key = "quota:cpu_" + name
3982             if key in flavor.extra_specs:
3983                 setattr(guest.cputune, name,
3984                         int(flavor.extra_specs[key]))
3985 
3986     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
3987                                            wants_hugepages):
3988         if instance_numa_topology:
3989             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
3990             for instance_cell in instance_numa_topology.cells:
3991                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
3992                 guest_cell.id = instance_cell.id
3993                 guest_cell.cpus = instance_cell.cpuset
3994                 guest_cell.memory = instance_cell.memory * units.Ki
3995 
3996                 # The vhost-user network backend requires file backed
3997                 # guest memory (ie huge pages) to be marked as shared
3998                 # access, not private, so an external process can read
3999                 # and write the pages.
4000                 #
4001                 # You can't change the shared vs private flag for an
4002                 # already running guest, and since we can't predict what
4003                 # types of NIC may be hotplugged, we have no choice but
4004                 # to unconditionally turn on the shared flag. This has
4005                 # no real negative functional effect on the guest, so
4006                 # is a reasonable approach to take
4007                 if wants_hugepages:
4008                     guest_cell.memAccess = "shared"
4009                 guest_cpu_numa.cells.append(guest_cell)
4010             return guest_cpu_numa
4011 
4012     def _has_cpu_policy_support(self):
4013         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
4014             if self._host.has_version(ver):
4015                 ver_ = self._version_to_string(ver)
4016                 raise exception.CPUPinningNotSupported(reason=_(
4017                     'Invalid libvirt version %(version)s') % {'version': ver_})
4018         return True
4019 
4020     def _wants_hugepages(self, host_topology, instance_topology):
4021         """Determine if the guest / host topology implies the
4022            use of huge pages for guest RAM backing
4023         """
4024 
4025         if host_topology is None or instance_topology is None:
4026             return False
4027 
4028         avail_pagesize = [page.size_kb
4029                           for page in host_topology.cells[0].mempages]
4030         avail_pagesize.sort()
4031         # Remove smallest page size as that's not classed as a largepage
4032         avail_pagesize = avail_pagesize[1:]
4033 
4034         # See if we have page size set
4035         for cell in instance_topology.cells:
4036             if (cell.pagesize is not None and
4037                 cell.pagesize in avail_pagesize):
4038                 return True
4039 
4040         return False
4041 
4042     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4043                                allowed_cpus=None, image_meta=None):
4044         """Returns the config objects for the guest NUMA specs.
4045 
4046         Determines the CPUs that the guest can be pinned to if the guest
4047         specifies a cell topology and the host supports it. Constructs the
4048         libvirt XML config object representing the NUMA topology selected
4049         for the guest. Returns a tuple of:
4050 
4051             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4052 
4053         With the following caveats:
4054 
4055             a) If there is no specified guest NUMA topology, then
4056                all tuple elements except cpu_set shall be None. cpu_set
4057                will be populated with the chosen CPUs that the guest
4058                allowed CPUs fit within, which could be the supplied
4059                allowed_cpus value if the host doesn't support NUMA
4060                topologies.
4061 
4062             b) If there is a specified guest NUMA topology, then
4063                cpu_set will be None and guest_cpu_numa will be the
4064                LibvirtConfigGuestCPUNUMA object representing the guest's
4065                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4066                will contain a LibvirtConfigGuestCPUTune object representing
4067                the optimized chosen cells that match the host capabilities
4068                with the instance's requested topology. If the host does
4069                not support NUMA, then guest_cpu_tune and guest_numa_tune
4070                will be None.
4071         """
4072 
4073         if (not self._has_numa_support() and
4074                 instance_numa_topology is not None):
4075             # We should not get here, since we should have avoided
4076             # reporting NUMA topology from _get_host_numa_topology
4077             # in the first place. Just in case of a scheduler
4078             # mess up though, raise an exception
4079             raise exception.NUMATopologyUnsupported()
4080 
4081         topology = self._get_host_numa_topology()
4082 
4083         # We have instance NUMA so translate it to the config class
4084         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4085                 instance_numa_topology,
4086                 self._wants_hugepages(topology, instance_numa_topology))
4087 
4088         if not guest_cpu_numa_config:
4089             # No NUMA topology defined for instance - let the host kernel deal
4090             # with the NUMA effects.
4091             # TODO(ndipanov): Attempt to spread the instance
4092             # across NUMA nodes and expose the topology to the
4093             # instance as an optimisation
4094             return GuestNumaConfig(allowed_cpus, None, None, None)
4095         else:
4096             if topology:
4097                 # Now get the CpuTune configuration from the numa_topology
4098                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4099                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4100                 emupcpus = []
4101 
4102                 numa_mem = vconfig.LibvirtConfigGuestNUMATuneMemory()
4103                 numa_memnodes = [vconfig.LibvirtConfigGuestNUMATuneMemNode()
4104                                  for _ in guest_cpu_numa_config.cells]
4105 
4106                 emulator_threads_isolated = (
4107                     instance_numa_topology.emulator_threads_isolated)
4108 
4109                 vcpus_rt = set([])
4110                 wants_realtime = hardware.is_realtime_enabled(flavor)
4111                 if wants_realtime:
4112                     if not self._host.has_min_version(
4113                             MIN_LIBVIRT_REALTIME_VERSION):
4114                         raise exception.RealtimePolicyNotSupported()
4115                     # Prepare realtime config for libvirt
4116                     vcpus_rt = hardware.vcpus_realtime_topology(
4117                         flavor, image_meta)
4118                     vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4119                     vcpusched.vcpus = vcpus_rt
4120                     vcpusched.scheduler = "fifo"
4121                     vcpusched.priority = (
4122                         CONF.libvirt.realtime_scheduler_priority)
4123                     guest_cpu_tune.vcpusched.append(vcpusched)
4124 
4125                 # TODO(sahid): Defining domain topology should be
4126                 # refactored.
4127                 for host_cell in topology.cells:
4128                     for guest_node_id, guest_config_cell in enumerate(
4129                             guest_cpu_numa_config.cells):
4130                         if guest_config_cell.id == host_cell.id:
4131                             node = numa_memnodes[guest_node_id]
4132                             node.cellid = guest_node_id
4133                             node.nodeset = [host_cell.id]
4134                             node.mode = "strict"
4135 
4136                             numa_mem.nodeset.append(host_cell.id)
4137 
4138                             object_numa_cell = (
4139                                     instance_numa_topology.cells[guest_node_id]
4140                                 )
4141                             for cpu in guest_config_cell.cpus:
4142                                 pin_cpuset = (
4143                                     vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
4144                                 pin_cpuset.id = cpu
4145                                 # If there is pinning information in the cell
4146                                 # we pin to individual CPUs, otherwise we float
4147                                 # over the whole host NUMA node
4148 
4149                                 if (object_numa_cell.cpu_pinning and
4150                                         self._has_cpu_policy_support()):
4151                                     pcpu = object_numa_cell.cpu_pinning[cpu]
4152                                     pin_cpuset.cpuset = set([pcpu])
4153                                 else:
4154                                     pin_cpuset.cpuset = host_cell.cpuset
4155                                 if emulator_threads_isolated:
4156                                     emupcpus.extend(
4157                                         object_numa_cell.cpuset_reserved)
4158                                 elif not wants_realtime or cpu not in vcpus_rt:
4159                                     # - If realtime IS NOT enabled, the
4160                                     #   emulator threads are allowed to float
4161                                     #   across all the pCPUs associated with
4162                                     #   the guest vCPUs ("not wants_realtime"
4163                                     #   is true, so we add all pcpus)
4164                                     # - If realtime IS enabled, then at least
4165                                     #   1 vCPU is required to be set aside for
4166                                     #   non-realtime usage. The emulator
4167                                     #   threads are allowed to float acros the
4168                                     #   pCPUs that are associated with the
4169                                     #   non-realtime VCPUs (the "cpu not in
4170                                     #   vcpu_rt" check deals with this
4171                                     #   filtering)
4172                                     emupcpus.extend(pin_cpuset.cpuset)
4173                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4174 
4175                 # TODO(berrange) When the guest has >1 NUMA node, it will
4176                 # span multiple host NUMA nodes. By pinning emulator threads
4177                 # to the union of all nodes, we guarantee there will be
4178                 # cross-node memory access by the emulator threads when
4179                 # responding to guest I/O operations. The only way to avoid
4180                 # this would be to pin emulator threads to a single node and
4181                 # tell the guest OS to only do I/O from one of its virtual
4182                 # NUMA nodes. This is not even remotely practical.
4183                 #
4184                 # The long term solution is to make use of a new QEMU feature
4185                 # called "I/O Threads" which will let us configure an explicit
4186                 # I/O thread for each guest vCPU or guest NUMA node. It is
4187                 # still TBD how to make use of this feature though, especially
4188                 # how to associate IO threads with guest devices to eliminate
4189                 # cross NUMA node traffic. This is an area of investigation
4190                 # for QEMU community devs.
4191                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
4192                 emulatorpin.cpuset = set(emupcpus)
4193                 guest_cpu_tune.emulatorpin = emulatorpin
4194                 # Sort the vcpupin list per vCPU id for human-friendlier XML
4195                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4196 
4197                 guest_numa_tune.memory = numa_mem
4198                 guest_numa_tune.memnodes = numa_memnodes
4199 
4200                 # normalize cell.id
4201                 for i, (cell, memnode) in enumerate(
4202                                             zip(guest_cpu_numa_config.cells,
4203                                                 guest_numa_tune.memnodes)):
4204                     cell.id = i
4205                     memnode.cellid = i
4206 
4207                 return GuestNumaConfig(None, guest_cpu_tune,
4208                                        guest_cpu_numa_config,
4209                                        guest_numa_tune)
4210             else:
4211                 return GuestNumaConfig(allowed_cpus, None,
4212                                        guest_cpu_numa_config, None)
4213 
4214     def _get_guest_os_type(self, virt_type):
4215         """Returns the guest OS type based on virt type."""
4216         if virt_type == "lxc":
4217             ret = fields.VMMode.EXE
4218         elif virt_type == "uml":
4219             ret = fields.VMMode.UML
4220         elif virt_type == "xen":
4221             ret = fields.VMMode.XEN
4222         else:
4223             ret = fields.VMMode.HVM
4224         return ret
4225 
4226     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4227                               root_device_name):
4228         if rescue.get('kernel_id'):
4229             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4230             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4231             if virt_type == "qemu":
4232                 guest.os_cmdline += " no_timer_check"
4233         if rescue.get('ramdisk_id'):
4234             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4235 
4236     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4237                                 root_device_name, image_meta):
4238         guest.os_kernel = os.path.join(inst_path, "kernel")
4239         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4240         if virt_type == "qemu":
4241             guest.os_cmdline += " no_timer_check"
4242         if instance.ramdisk_id:
4243             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4244         # we only support os_command_line with images with an explicit
4245         # kernel set and don't want to break nova if there's an
4246         # os_command_line property without a specified kernel_id param
4247         if image_meta.properties.get("os_command_line"):
4248             guest.os_cmdline = image_meta.properties.os_command_line
4249 
4250     def _set_clock(self, guest, os_type, image_meta, virt_type):
4251         # NOTE(mikal): Microsoft Windows expects the clock to be in
4252         # "localtime". If the clock is set to UTC, then you can use a
4253         # registry key to let windows know, but Microsoft says this is
4254         # buggy in http://support.microsoft.com/kb/2687252
4255         clk = vconfig.LibvirtConfigGuestClock()
4256         if os_type == 'windows':
4257             LOG.info('Configuring timezone for windows instance to localtime')
4258             clk.offset = 'localtime'
4259         else:
4260             clk.offset = 'utc'
4261         guest.set_clock(clk)
4262 
4263         if virt_type == "kvm":
4264             self._set_kvm_timers(clk, os_type, image_meta)
4265 
4266     def _set_kvm_timers(self, clk, os_type, image_meta):
4267         # TODO(berrange) One day this should be per-guest
4268         # OS type configurable
4269         tmpit = vconfig.LibvirtConfigGuestTimer()
4270         tmpit.name = "pit"
4271         tmpit.tickpolicy = "delay"
4272 
4273         tmrtc = vconfig.LibvirtConfigGuestTimer()
4274         tmrtc.name = "rtc"
4275         tmrtc.tickpolicy = "catchup"
4276 
4277         clk.add_timer(tmpit)
4278         clk.add_timer(tmrtc)
4279 
4280         guestarch = libvirt_utils.get_arch(image_meta)
4281         if guestarch in (fields.Architecture.I686,
4282                          fields.Architecture.X86_64):
4283             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4284             # qemu -no-hpet is not supported on non-x86 targets.
4285             tmhpet = vconfig.LibvirtConfigGuestTimer()
4286             tmhpet.name = "hpet"
4287             tmhpet.present = False
4288             clk.add_timer(tmhpet)
4289 
4290         # Provide Windows guests with the paravirtualized hyperv timer source.
4291         # This is the windows equiv of kvm-clock, allowing Windows
4292         # guests to accurately keep time.
4293         if os_type == 'windows':
4294             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4295             tmhyperv.name = "hypervclock"
4296             tmhyperv.present = True
4297             clk.add_timer(tmhyperv)
4298 
4299     def _set_features(self, guest, os_type, caps, virt_type, image_meta):
4300         if virt_type == "xen":
4301             # PAE only makes sense in X86
4302             if caps.host.cpu.arch in (fields.Architecture.I686,
4303                                       fields.Architecture.X86_64):
4304                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4305 
4306         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4307                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4308             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4309             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4310 
4311         if (virt_type in ("qemu", "kvm") and
4312                 os_type == 'windows'):
4313             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4314             hv.relaxed = True
4315 
4316             hv.spinlocks = True
4317             # Increase spinlock retries - value recommended by
4318             # KVM maintainers who certify Windows guests
4319             # with Microsoft
4320             hv.spinlock_retries = 8191
4321             hv.vapic = True
4322             guest.features.append(hv)
4323 
4324         if (virt_type in ("qemu", "kvm") and
4325                 image_meta.properties.get('img_hide_hypervisor_id')):
4326             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4327 
4328     def _check_number_of_serial_console(self, num_ports):
4329         virt_type = CONF.libvirt.virt_type
4330         if (virt_type in ("kvm", "qemu") and
4331             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4332             raise exception.SerialPortNumberLimitExceeded(
4333                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4334 
4335     def _add_video_driver(self, guest, image_meta, flavor):
4336         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4337                                "xen", "qxl", "virtio")
4338         video = vconfig.LibvirtConfigGuestVideo()
4339         # NOTE(ldbragst): The following logic sets the video.type
4340         # depending on supported defaults given the architecture,
4341         # virtualization type, and features. The video.type attribute can
4342         # be overridden by the user with image_meta.properties, which
4343         # is carried out in the next if statement below this one.
4344         guestarch = libvirt_utils.get_arch(image_meta)
4345         if guest.os_type == fields.VMMode.XEN:
4346             video.type = 'xen'
4347         elif CONF.libvirt.virt_type == 'parallels':
4348             video.type = 'vga'
4349         elif guestarch in (fields.Architecture.PPC,
4350                            fields.Architecture.PPC64,
4351                            fields.Architecture.PPC64LE):
4352             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4353             # so use 'vga' instead when running on Power hardware.
4354             video.type = 'vga'
4355         elif guestarch in (fields.Architecture.AARCH64):
4356             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4357             # so use 'virtio' instead when running on AArch64 hardware.
4358             video.type = 'virtio'
4359         elif CONF.spice.enabled:
4360             video.type = 'qxl'
4361         if image_meta.properties.get('hw_video_model'):
4362             video.type = image_meta.properties.hw_video_model
4363             if (video.type not in VALID_VIDEO_DEVICES):
4364                 raise exception.InvalidVideoMode(model=video.type)
4365 
4366         # Set video memory, only if the flavor's limit is set
4367         video_ram = image_meta.properties.get('hw_video_ram', 0)
4368         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4369         if video_ram > max_vram:
4370             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4371                                                  max_vram=max_vram)
4372         if max_vram and video_ram:
4373             video.vram = video_ram * units.Mi / units.Ki
4374         guest.add_device(video)
4375 
4376     def _add_qga_device(self, guest, instance):
4377         qga = vconfig.LibvirtConfigGuestChannel()
4378         qga.type = "unix"
4379         qga.target_name = "org.qemu.guest_agent.0"
4380         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4381                           ("org.qemu.guest_agent.0", instance.name))
4382         guest.add_device(qga)
4383 
4384     def _add_rng_device(self, guest, flavor):
4385         rng_device = vconfig.LibvirtConfigGuestRng()
4386         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4387         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4388         if rate_bytes:
4389             rng_device.rate_bytes = int(rate_bytes)
4390             rng_device.rate_period = int(period)
4391         rng_path = CONF.libvirt.rng_dev_path
4392         if (rng_path and not os.path.exists(rng_path)):
4393             raise exception.RngDeviceNotExist(path=rng_path)
4394         rng_device.backend = rng_path
4395         guest.add_device(rng_device)
4396 
4397     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4398         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4399         if image_meta.properties.get('hw_qemu_guest_agent', False):
4400             LOG.debug("Qemu guest agent is enabled through image "
4401                       "metadata", instance=instance)
4402             self._add_qga_device(guest, instance)
4403         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4404         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4405         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4406         if rng_is_virtio and rng_allowed:
4407             self._add_rng_device(guest, flavor)
4408 
4409     def _get_guest_memory_backing_config(
4410             self, inst_topology, numatune, flavor):
4411         wantsmempages = False
4412         if inst_topology:
4413             for cell in inst_topology.cells:
4414                 if cell.pagesize:
4415                     wantsmempages = True
4416                     break
4417 
4418         wantsrealtime = hardware.is_realtime_enabled(flavor)
4419 
4420         membacking = None
4421         if wantsmempages:
4422             pages = self._get_memory_backing_hugepages_support(
4423                 inst_topology, numatune)
4424             if pages:
4425                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4426                 membacking.hugepages = pages
4427         if wantsrealtime:
4428             if not membacking:
4429                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4430             membacking.locked = True
4431             membacking.sharedpages = False
4432 
4433         return membacking
4434 
4435     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4436         if not self._has_numa_support():
4437             # We should not get here, since we should have avoided
4438             # reporting NUMA topology from _get_host_numa_topology
4439             # in the first place. Just in case of a scheduler
4440             # mess up though, raise an exception
4441             raise exception.MemoryPagesUnsupported()
4442 
4443         host_topology = self._get_host_numa_topology()
4444 
4445         if host_topology is None:
4446             # As above, we should not get here but just in case...
4447             raise exception.MemoryPagesUnsupported()
4448 
4449         # Currently libvirt does not support the smallest
4450         # pagesize set as a backend memory.
4451         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4452         avail_pagesize = [page.size_kb
4453                           for page in host_topology.cells[0].mempages]
4454         avail_pagesize.sort()
4455         smallest = avail_pagesize[0]
4456 
4457         pages = []
4458         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4459             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4460                 for memnode in numatune.memnodes:
4461                     if guest_cellid == memnode.cellid:
4462                         page = (
4463                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4464                         page.nodeset = [guest_cellid]
4465                         page.size_kb = inst_cell.pagesize
4466                         pages.append(page)
4467                         break  # Quit early...
4468         return pages
4469 
4470     def _get_flavor(self, ctxt, instance, flavor):
4471         if flavor is not None:
4472             return flavor
4473         return instance.flavor
4474 
4475     def _has_uefi_support(self):
4476         # This means that the host can support uefi booting for guests
4477         supported_archs = [fields.Architecture.X86_64,
4478                            fields.Architecture.AARCH64]
4479         caps = self._host.get_capabilities()
4480         return ((caps.host.cpu.arch in supported_archs) and
4481                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4482 
4483     def _get_supported_perf_events(self):
4484 
4485         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4486              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4487             return []
4488 
4489         supported_events = []
4490         host_cpu_info = self._get_cpu_info()
4491         for event in CONF.libvirt.enabled_perf_events:
4492             if self._supported_perf_event(event, host_cpu_info['features']):
4493                 supported_events.append(event)
4494         return supported_events
4495 
4496     def _supported_perf_event(self, event, cpu_features):
4497 
4498         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4499 
4500         if not hasattr(libvirt, libvirt_perf_event_name):
4501             LOG.warning("Libvirt doesn't support event type %s.", event)
4502             return False
4503 
4504         if (event in PERF_EVENTS_CPU_FLAG_MAPPING
4505             and PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features):
4506             LOG.warning("Host does not support event type %s.", event)
4507             return False
4508 
4509         return True
4510 
4511     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4512                                       image_meta, flavor, root_device_name):
4513         if virt_type == "xen":
4514             if guest.os_type == fields.VMMode.HVM:
4515                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4516             else:
4517                 guest.os_cmdline = CONSOLE
4518         elif virt_type in ("kvm", "qemu"):
4519             if caps.host.cpu.arch in (fields.Architecture.I686,
4520                                       fields.Architecture.X86_64):
4521                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4522                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4523             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4524             if caps.host.cpu.arch == fields.Architecture.AARCH64:
4525                 if not hw_firmware_type:
4526                     hw_firmware_type = fields.FirmwareType.UEFI
4527             if hw_firmware_type == fields.FirmwareType.UEFI:
4528                 if self._has_uefi_support():
4529                     global uefi_logged
4530                     if not uefi_logged:
4531                         LOG.warning("uefi support is without some kind of "
4532                                     "functional testing and therefore "
4533                                     "considered experimental.")
4534                         uefi_logged = True
4535                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4536                         caps.host.cpu.arch]
4537                     guest.os_loader_type = "pflash"
4538                 else:
4539                     raise exception.UEFINotSupported()
4540             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4541             if image_meta.properties.get('hw_boot_menu') is None:
4542                 guest.os_bootmenu = strutils.bool_from_string(
4543                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4544             else:
4545                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4546 
4547         elif virt_type == "lxc":
4548             guest.os_init_path = "/sbin/init"
4549             guest.os_cmdline = CONSOLE
4550         elif virt_type == "uml":
4551             guest.os_kernel = "/usr/bin/linux"
4552             guest.os_root = root_device_name
4553         elif virt_type == "parallels":
4554             if guest.os_type == fields.VMMode.EXE:
4555                 guest.os_init_path = "/sbin/init"
4556 
4557     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4558                     instance, inst_path, image_meta, disk_info):
4559         if rescue:
4560             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4561                                        root_device_name)
4562         elif instance.kernel_id:
4563             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4564                                             virt_type, root_device_name,
4565                                             image_meta)
4566         else:
4567             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4568 
4569     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4570                          image_meta):
4571         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4572         # easy to lose track. Use this chart to figure out your case:
4573         #
4574         # case | is serial | has       | is qemu | resulting
4575         #      | enabled?  | virtlogd? | or kvm? | devices
4576         # --------------------------------------------------
4577         #    1 |        no |        no |     no  | pty*
4578         #    2 |        no |        no |     yes | file + pty
4579         #    3 |        no |       yes |      no | see case 1
4580         #    4 |        no |       yes |     yes | pty with logd
4581         #    5 |       yes |        no |      no | see case 1
4582         #    6 |       yes |        no |     yes | tcp + pty
4583         #    7 |       yes |       yes |      no | see case 1
4584         #    8 |       yes |       yes |     yes | tcp with logd
4585         #    * exception: virt_type "parallels" doesn't create a device
4586         if virt_type == 'parallels':
4587             pass
4588         elif virt_type not in ("qemu", "kvm"):
4589             log_path = self._get_console_log_path(instance)
4590             self._create_pty_device(guest_cfg,
4591                                     vconfig.LibvirtConfigGuestConsole,
4592                                     log_path=log_path)
4593         elif (virt_type in ("qemu", "kvm") and
4594                   self._is_s390x_guest(image_meta)):
4595             self._create_consoles_s390x(guest_cfg, instance,
4596                                         flavor, image_meta)
4597         elif virt_type in ("qemu", "kvm"):
4598             self._create_consoles_qemu_kvm(guest_cfg, instance,
4599                                         flavor, image_meta)
4600 
4601     def _is_s390x_guest(self, image_meta):
4602         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4603         return libvirt_utils.get_arch(image_meta) in s390x_archs
4604 
4605     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4606                                   image_meta):
4607         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4608         log_path = self._get_console_log_path(instance)
4609         if CONF.serial_console.enabled:
4610             if not self._serial_ports_already_defined(instance):
4611                 num_ports = hardware.get_number_of_serial_ports(flavor,
4612                                                                 image_meta)
4613                 self._check_number_of_serial_console(num_ports)
4614                 self._create_serial_consoles(guest_cfg, num_ports,
4615                                              char_dev_cls, log_path)
4616         else:
4617             self._create_file_device(guest_cfg, instance, char_dev_cls)
4618         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4619 
4620     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4621         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4622         log_path = self._get_console_log_path(instance)
4623         if CONF.serial_console.enabled:
4624             if not self._serial_ports_already_defined(instance):
4625                 num_ports = hardware.get_number_of_serial_ports(flavor,
4626                                                                 image_meta)
4627                 self._create_serial_consoles(guest_cfg, num_ports,
4628                                              char_dev_cls, log_path)
4629         else:
4630             self._create_file_device(guest_cfg, instance, char_dev_cls,
4631                                      "sclplm")
4632         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4633 
4634     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4635                            log_path=None):
4636         def _create_base_dev():
4637             consolepty = char_dev_cls()
4638             consolepty.target_type = target_type
4639             consolepty.type = "pty"
4640             return consolepty
4641 
4642         def _create_logd_dev():
4643             consolepty = _create_base_dev()
4644             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4645             log.file = log_path
4646             consolepty.log = log
4647             return consolepty
4648 
4649         if CONF.serial_console.enabled:
4650             if self._is_virtlogd_available():
4651                 return
4652             else:
4653                 # NOTE(markus_z): You may wonder why this is necessary and
4654                 # so do I. I'm certain that this is *not* needed in any
4655                 # real use case. It is, however, useful if you want to
4656                 # pypass the Nova API and use "virsh console <guest>" on
4657                 # an hypervisor, as this CLI command doesn't work with TCP
4658                 # devices (like the serial console is).
4659                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4660                 # Pypassing the Nova API however is a thing we don't want.
4661                 # Future changes should remove this and fix the unit tests
4662                 # which ask for the existence.
4663                 guest_cfg.add_device(_create_base_dev())
4664         else:
4665             if self._is_virtlogd_available():
4666                 guest_cfg.add_device(_create_logd_dev())
4667             else:
4668                 guest_cfg.add_device(_create_base_dev())
4669 
4670     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4671                             target_type=None):
4672         if self._is_virtlogd_available():
4673             return
4674 
4675         consolelog = char_dev_cls()
4676         consolelog.target_type = target_type
4677         consolelog.type = "file"
4678         consolelog.source_path = self._get_console_log_path(instance)
4679         guest_cfg.add_device(consolelog)
4680 
4681     def _serial_ports_already_defined(self, instance):
4682         try:
4683             guest = self._host.get_guest(instance)
4684             if list(self._get_serial_ports_from_guest(guest)):
4685                 # Serial port are already configured for instance that
4686                 # means we are in a context of migration.
4687                 return True
4688         except exception.InstanceNotFound:
4689             LOG.debug(
4690                 "Instance does not exist yet on libvirt, we can "
4691                 "safely pass on looking for already defined serial "
4692                 "ports in its domain XML", instance=instance)
4693         return False
4694 
4695     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4696                                 log_path):
4697         for port in six.moves.range(num_ports):
4698             console = char_dev_cls()
4699             console.port = port
4700             console.type = "tcp"
4701             console.listen_host = CONF.serial_console.proxyclient_address
4702             listen_port = serial_console.acquire_port(console.listen_host)
4703             console.listen_port = listen_port
4704             # NOTE: only the first serial console gets the boot messages,
4705             # that's why we attach the logd subdevice only to that.
4706             if port == 0 and self._is_virtlogd_available():
4707                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
4708                 log.file = log_path
4709                 console.log = log
4710             guest_cfg.add_device(console)
4711 
4712     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
4713         """Update VirtCPUModel object according to libvirt CPU config.
4714 
4715         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
4716                            instance's virtual cpu configuration.
4717         :param:vcpu_model: VirtCPUModel object. A new object will be created
4718                            if None.
4719 
4720         :return: Updated VirtCPUModel object, or None if cpu_config is None
4721 
4722         """
4723 
4724         if not cpu_config:
4725             return
4726         if not vcpu_model:
4727             vcpu_model = objects.VirtCPUModel()
4728 
4729         vcpu_model.arch = cpu_config.arch
4730         vcpu_model.vendor = cpu_config.vendor
4731         vcpu_model.model = cpu_config.model
4732         vcpu_model.mode = cpu_config.mode
4733         vcpu_model.match = cpu_config.match
4734 
4735         if cpu_config.sockets:
4736             vcpu_model.topology = objects.VirtCPUTopology(
4737                 sockets=cpu_config.sockets,
4738                 cores=cpu_config.cores,
4739                 threads=cpu_config.threads)
4740         else:
4741             vcpu_model.topology = None
4742 
4743         features = [objects.VirtCPUFeature(
4744             name=f.name,
4745             policy=f.policy) for f in cpu_config.features]
4746         vcpu_model.features = features
4747 
4748         return vcpu_model
4749 
4750     def _vcpu_model_to_cpu_config(self, vcpu_model):
4751         """Create libvirt CPU config according to VirtCPUModel object.
4752 
4753         :param:vcpu_model: VirtCPUModel object.
4754 
4755         :return: vconfig.LibvirtConfigGuestCPU.
4756 
4757         """
4758 
4759         cpu_config = vconfig.LibvirtConfigGuestCPU()
4760         cpu_config.arch = vcpu_model.arch
4761         cpu_config.model = vcpu_model.model
4762         cpu_config.mode = vcpu_model.mode
4763         cpu_config.match = vcpu_model.match
4764         cpu_config.vendor = vcpu_model.vendor
4765         if vcpu_model.topology:
4766             cpu_config.sockets = vcpu_model.topology.sockets
4767             cpu_config.cores = vcpu_model.topology.cores
4768             cpu_config.threads = vcpu_model.topology.threads
4769         if vcpu_model.features:
4770             for f in vcpu_model.features:
4771                 xf = vconfig.LibvirtConfigGuestCPUFeature()
4772                 xf.name = f.name
4773                 xf.policy = f.policy
4774                 cpu_config.features.add(xf)
4775         return cpu_config
4776 
4777     def _get_guest_config(self, instance, network_info, image_meta,
4778                           disk_info, rescue=None, block_device_info=None,
4779                           context=None):
4780         """Get config data for parameters.
4781 
4782         :param rescue: optional dictionary that should contain the key
4783             'ramdisk_id' if a ramdisk is needed for the rescue image and
4784             'kernel_id' if a kernel is needed for the rescue image.
4785         """
4786         flavor = instance.flavor
4787         inst_path = libvirt_utils.get_instance_path(instance)
4788         disk_mapping = disk_info['mapping']
4789 
4790         virt_type = CONF.libvirt.virt_type
4791         guest = vconfig.LibvirtConfigGuest()
4792         guest.virt_type = virt_type
4793         guest.name = instance.name
4794         guest.uuid = instance.uuid
4795         # We are using default unit for memory: KiB
4796         guest.memory = flavor.memory_mb * units.Ki
4797         guest.vcpus = flavor.vcpus
4798         allowed_cpus = hardware.get_vcpu_pin_set()
4799 
4800         guest_numa_config = self._get_guest_numa_config(
4801             instance.numa_topology, flavor, allowed_cpus, image_meta)
4802 
4803         guest.cpuset = guest_numa_config.cpuset
4804         guest.cputune = guest_numa_config.cputune
4805         guest.numatune = guest_numa_config.numatune
4806 
4807         guest.membacking = self._get_guest_memory_backing_config(
4808             instance.numa_topology,
4809             guest_numa_config.numatune,
4810             flavor)
4811 
4812         guest.metadata.append(self._get_guest_config_meta(instance))
4813         guest.idmaps = self._get_guest_idmaps()
4814 
4815         for event in self._supported_perf_events:
4816             guest.add_perf_event(event)
4817 
4818         self._update_guest_cputune(guest, flavor, virt_type)
4819 
4820         guest.cpu = self._get_guest_cpu_config(
4821             flavor, image_meta, guest_numa_config.numaconfig,
4822             instance.numa_topology)
4823 
4824         # Notes(yjiang5): we always sync the instance's vcpu model with
4825         # the corresponding config file.
4826         instance.vcpu_model = self._cpu_config_to_vcpu_model(
4827             guest.cpu, instance.vcpu_model)
4828 
4829         if 'root' in disk_mapping:
4830             root_device_name = block_device.prepend_dev(
4831                 disk_mapping['root']['dev'])
4832         else:
4833             root_device_name = None
4834 
4835         if root_device_name:
4836             # NOTE(yamahata):
4837             # for nova.api.ec2.cloud.CloudController.get_metadata()
4838             instance.root_device_name = root_device_name
4839 
4840         guest.os_type = (fields.VMMode.get_from_instance(instance) or
4841                 self._get_guest_os_type(virt_type))
4842         caps = self._host.get_capabilities()
4843 
4844         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
4845                                            image_meta, flavor,
4846                                            root_device_name)
4847         if virt_type not in ('lxc', 'uml'):
4848             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
4849                     instance, inst_path, image_meta, disk_info)
4850 
4851         self._set_features(guest, instance.os_type, caps, virt_type,
4852                            image_meta)
4853         self._set_clock(guest, instance.os_type, image_meta, virt_type)
4854 
4855         storage_configs = self._get_guest_storage_config(
4856                 instance, image_meta, disk_info, rescue, block_device_info,
4857                 flavor, guest.os_type)
4858         for config in storage_configs:
4859             guest.add_device(config)
4860 
4861         for vif in network_info:
4862             config = self.vif_driver.get_config(
4863                 instance, vif, image_meta,
4864                 flavor, virt_type, self._host)
4865             guest.add_device(config)
4866 
4867         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
4868 
4869         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
4870         if pointer:
4871             guest.add_device(pointer)
4872 
4873         self._guest_add_spice_channel(guest)
4874 
4875         if self._guest_add_video_device(guest):
4876             self._add_video_driver(guest, image_meta, flavor)
4877 
4878         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
4879         if virt_type in ('qemu', 'kvm'):
4880             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
4881 
4882         self._guest_add_pci_devices(guest, instance)
4883 
4884         self._guest_add_watchdog_action(guest, flavor, image_meta)
4885 
4886         self._guest_add_memory_balloon(guest)
4887 
4888         return guest
4889 
4890     @staticmethod
4891     def _guest_add_spice_channel(guest):
4892         if (CONF.spice.enabled and CONF.spice.agent_enabled
4893                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
4894             channel = vconfig.LibvirtConfigGuestChannel()
4895             channel.type = 'spicevmc'
4896             channel.target_name = "com.redhat.spice.0"
4897             guest.add_device(channel)
4898 
4899     @staticmethod
4900     def _guest_add_memory_balloon(guest):
4901         virt_type = guest.virt_type
4902         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
4903         if (virt_type in ('xen', 'qemu', 'kvm') and
4904                     CONF.libvirt.mem_stats_period_seconds > 0):
4905             balloon = vconfig.LibvirtConfigMemoryBalloon()
4906             if virt_type in ('qemu', 'kvm'):
4907                 balloon.model = 'virtio'
4908             else:
4909                 balloon.model = 'xen'
4910             balloon.period = CONF.libvirt.mem_stats_period_seconds
4911             guest.add_device(balloon)
4912 
4913     @staticmethod
4914     def _guest_add_watchdog_action(guest, flavor, image_meta):
4915         # image meta takes precedence over flavor extra specs; disable the
4916         # watchdog action by default
4917         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
4918                            or 'disabled')
4919         watchdog_action = image_meta.properties.get('hw_watchdog_action',
4920                                                     watchdog_action)
4921         # NB(sross): currently only actually supported by KVM/QEmu
4922         if watchdog_action != 'disabled':
4923             if watchdog_action in fields.WatchdogAction.ALL:
4924                 bark = vconfig.LibvirtConfigGuestWatchdog()
4925                 bark.action = watchdog_action
4926                 guest.add_device(bark)
4927             else:
4928                 raise exception.InvalidWatchdogAction(action=watchdog_action)
4929 
4930     def _guest_add_pci_devices(self, guest, instance):
4931         virt_type = guest.virt_type
4932         if virt_type in ('xen', 'qemu', 'kvm'):
4933             # Get all generic PCI devices (non-SR-IOV).
4934             for pci_dev in pci_manager.get_instance_pci_devs(instance):
4935                 guest.add_device(self._get_guest_pci_device(pci_dev))
4936         else:
4937             # PCI devices is only supported for hypervisors
4938             #  'xen', 'qemu' and 'kvm'.
4939             if pci_manager.get_instance_pci_devs(instance, 'all'):
4940                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
4941 
4942     @staticmethod
4943     def _guest_add_video_device(guest):
4944         # NB some versions of libvirt support both SPICE and VNC
4945         # at the same time. We're not trying to second guess which
4946         # those versions are. We'll just let libvirt report the
4947         # errors appropriately if the user enables both.
4948         add_video_driver = False
4949         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
4950             graphics = vconfig.LibvirtConfigGuestGraphics()
4951             graphics.type = "vnc"
4952             if CONF.vnc.keymap:
4953                 # TODO(stephenfin): There are some issues here that may
4954                 # necessitate deprecating this option entirely in the future.
4955                 # Refer to bug #1682020 for more information.
4956                 graphics.keymap = CONF.vnc.keymap
4957             graphics.listen = CONF.vnc.server_listen
4958             guest.add_device(graphics)
4959             add_video_driver = True
4960         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
4961             graphics = vconfig.LibvirtConfigGuestGraphics()
4962             graphics.type = "spice"
4963             if CONF.spice.keymap:
4964                 # TODO(stephenfin): There are some issues here that may
4965                 # necessitate deprecating this option entirely in the future.
4966                 # Refer to bug #1682020 for more information.
4967                 graphics.keymap = CONF.spice.keymap
4968             graphics.listen = CONF.spice.server_listen
4969             guest.add_device(graphics)
4970             add_video_driver = True
4971         return add_video_driver
4972 
4973     def _get_guest_pointer_model(self, os_type, image_meta):
4974         pointer_model = image_meta.properties.get(
4975             'hw_pointer_model', CONF.pointer_model)
4976         if pointer_model is None and CONF.libvirt.use_usb_tablet:
4977             # TODO(sahid): We set pointer_model to keep compatibility
4978             # until the next release O*. It means operators can continue
4979             # to use the deprecated option "use_usb_tablet" or set a
4980             # specific device to use
4981             pointer_model = "usbtablet"
4982             LOG.warning('The option "use_usb_tablet" has been '
4983                         'deprecated for Newton in favor of the more '
4984                         'generic "pointer_model". Please update '
4985                         'nova.conf to address this change.')
4986 
4987         if pointer_model == "usbtablet":
4988             # We want a tablet if VNC is enabled, or SPICE is enabled and
4989             # the SPICE agent is disabled. If the SPICE agent is enabled
4990             # it provides a paravirt mouse which drastically reduces
4991             # overhead (by eliminating USB polling).
4992             if CONF.vnc.enabled or (
4993                     CONF.spice.enabled and not CONF.spice.agent_enabled):
4994                 return self._get_guest_usb_tablet(os_type)
4995             else:
4996                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
4997                     # For backward compatibility We don't want to break
4998                     # process of booting an instance if host is configured
4999                     # to use USB tablet without VNC or SPICE and SPICE
5000                     # agent disable.
5001                     LOG.warning('USB tablet requested for guests by host '
5002                                 'configuration. In order to accept this '
5003                                 'request VNC should be enabled or SPICE '
5004                                 'and SPICE agent disabled on host.')
5005                 else:
5006                     raise exception.UnsupportedPointerModelRequested(
5007                         model="usbtablet")
5008 
5009     def _get_guest_usb_tablet(self, os_type):
5010         tablet = None
5011         if os_type == fields.VMMode.HVM:
5012             tablet = vconfig.LibvirtConfigGuestInput()
5013             tablet.type = "tablet"
5014             tablet.bus = "usb"
5015         else:
5016             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5017                 # For backward compatibility We don't want to break
5018                 # process of booting an instance if virtual machine mode
5019                 # is not configured as HVM.
5020                 LOG.warning('USB tablet requested for guests by host '
5021                             'configuration. In order to accept this '
5022                             'request the machine mode should be '
5023                             'configured as HVM.')
5024             else:
5025                 raise exception.UnsupportedPointerModelRequested(
5026                     model="usbtablet")
5027         return tablet
5028 
5029     def _get_guest_xml(self, context, instance, network_info, disk_info,
5030                        image_meta, rescue=None,
5031                        block_device_info=None):
5032         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5033         # this ahead of time so that we don't acquire it while also
5034         # holding the logging lock.
5035         network_info_str = str(network_info)
5036         msg = ('Start _get_guest_xml '
5037                'network_info=%(network_info)s '
5038                'disk_info=%(disk_info)s '
5039                'image_meta=%(image_meta)s rescue=%(rescue)s '
5040                'block_device_info=%(block_device_info)s' %
5041                {'network_info': network_info_str, 'disk_info': disk_info,
5042                 'image_meta': image_meta, 'rescue': rescue,
5043                 'block_device_info': block_device_info})
5044         # NOTE(mriedem): block_device_info can contain auth_password so we
5045         # need to sanitize the password in the message.
5046         LOG.debug(strutils.mask_password(msg), instance=instance)
5047         conf = self._get_guest_config(instance, network_info, image_meta,
5048                                       disk_info, rescue, block_device_info,
5049                                       context)
5050         xml = conf.to_xml()
5051 
5052         LOG.debug('End _get_guest_xml xml=%(xml)s',
5053                   {'xml': xml}, instance=instance)
5054         return xml
5055 
5056     def get_info(self, instance):
5057         """Retrieve information from libvirt for a specific instance.
5058 
5059         If a libvirt error is encountered during lookup, we might raise a
5060         NotFound exception or Error exception depending on how severe the
5061         libvirt error is.
5062 
5063         :param instance: nova.objects.instance.Instance object
5064         :returns: An InstanceInfo object
5065         """
5066         guest = self._host.get_guest(instance)
5067         # Kind of ugly but we need to pass host to get_info as for a
5068         # workaround, see libvirt/compat.py
5069         return guest.get_info(self._host)
5070 
5071     def _create_domain_setup_lxc(self, instance, image_meta,
5072                                  block_device_info):
5073         inst_path = libvirt_utils.get_instance_path(instance)
5074         block_device_mapping = driver.block_device_info_get_mapping(
5075             block_device_info)
5076         root_disk = block_device.get_root_bdm(block_device_mapping)
5077         if root_disk:
5078             self._connect_volume(root_disk['connection_info'], instance)
5079             disk_path = root_disk['connection_info']['data']['device_path']
5080 
5081             # NOTE(apmelton) - Even though the instance is being booted from a
5082             # cinder volume, it is still presented as a local block device.
5083             # LocalBlockImage is used here to indicate that the instance's
5084             # disk is backed by a local block device.
5085             image_model = imgmodel.LocalBlockImage(disk_path)
5086         else:
5087             root_disk = self.image_backend.by_name(instance, 'disk')
5088             image_model = root_disk.get_model(self._conn)
5089 
5090         container_dir = os.path.join(inst_path, 'rootfs')
5091         fileutils.ensure_tree(container_dir)
5092         rootfs_dev = disk_api.setup_container(image_model,
5093                                               container_dir=container_dir)
5094 
5095         try:
5096             # Save rootfs device to disconnect it when deleting the instance
5097             if rootfs_dev:
5098                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5099             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5100                 id_maps = self._get_guest_idmaps()
5101                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5102         except Exception:
5103             with excutils.save_and_reraise_exception():
5104                 self._create_domain_cleanup_lxc(instance)
5105 
5106     def _create_domain_cleanup_lxc(self, instance):
5107         inst_path = libvirt_utils.get_instance_path(instance)
5108         container_dir = os.path.join(inst_path, 'rootfs')
5109 
5110         try:
5111             state = self.get_info(instance).state
5112         except exception.InstanceNotFound:
5113             # The domain may not be present if the instance failed to start
5114             state = None
5115 
5116         if state == power_state.RUNNING:
5117             # NOTE(uni): Now the container is running with its own private
5118             # mount namespace and so there is no need to keep the container
5119             # rootfs mounted in the host namespace
5120             LOG.debug('Attempting to unmount container filesystem: %s',
5121                       container_dir, instance=instance)
5122             disk_api.clean_lxc_namespace(container_dir=container_dir)
5123         else:
5124             disk_api.teardown_container(container_dir=container_dir)
5125 
5126     @contextlib.contextmanager
5127     def _lxc_disk_handler(self, instance, image_meta, block_device_info):
5128         """Context manager to handle the pre and post instance boot,
5129            LXC specific disk operations.
5130 
5131            An image or a volume path will be prepared and setup to be
5132            used by the container, prior to starting it.
5133            The disk will be disconnected and unmounted if a container has
5134            failed to start.
5135         """
5136 
5137         if CONF.libvirt.virt_type != 'lxc':
5138             yield
5139             return
5140 
5141         self._create_domain_setup_lxc(instance, image_meta, block_device_info)
5142 
5143         try:
5144             yield
5145         finally:
5146             self._create_domain_cleanup_lxc(instance)
5147 
5148     # TODO(sahid): Consider renaming this to _create_guest.
5149     def _create_domain(self, xml=None, domain=None,
5150                        power_on=True, pause=False, post_xml_callback=None):
5151         """Create a domain.
5152 
5153         Either domain or xml must be passed in. If both are passed, then
5154         the domain definition is overwritten from the xml.
5155 
5156         :returns guest.Guest: Guest just created
5157         """
5158         if xml:
5159             guest = libvirt_guest.Guest.create(xml, self._host)
5160             if post_xml_callback is not None:
5161                 post_xml_callback()
5162         else:
5163             guest = libvirt_guest.Guest(domain)
5164 
5165         if power_on or pause:
5166             guest.launch(pause=pause)
5167 
5168         if not utils.is_neutron():
5169             guest.enable_hairpin()
5170 
5171         return guest
5172 
5173     def _neutron_failed_callback(self, event_name, instance):
5174         LOG.error('Neutron Reported failure on event '
5175                   '%(event)s for instance %(uuid)s',
5176                   {'event': event_name, 'uuid': instance.uuid},
5177                   instance=instance)
5178         if CONF.vif_plugging_is_fatal:
5179             raise exception.VirtualInterfaceCreateException()
5180 
5181     def _get_neutron_events(self, network_info):
5182         # NOTE(danms): We need to collect any VIFs that are currently
5183         # down that we expect a down->up event for. Anything that is
5184         # already up will not undergo that transition, and for
5185         # anything that might be stale (cache-wise) assume it's
5186         # already up so we don't block on it.
5187         return [('network-vif-plugged', vif['id'])
5188                 for vif in network_info if vif.get('active', True) is False]
5189 
5190     def _get_neutron_events_for_live_migration(self, network_info):
5191         # Neutron should send to Nova events indicating that the VIFs
5192         # are well plugged on destination host.
5193 
5194         # TODO(sahid): Currenlty we only use the mechanism of waiting
5195         # neutron events during live-migration for linux-bridge.
5196         return [('network-vif-plugged', vif['id'])
5197                 for vif in network_info if (
5198                         vif.get('type') == network_model.VIF_TYPE_BRIDGE)]
5199 
5200     def _cleanup_failed_start(self, context, instance, network_info,
5201                               block_device_info, guest, destroy_disks):
5202         try:
5203             if guest and guest.is_active():
5204                 guest.poweroff()
5205         finally:
5206             self.cleanup(context, instance, network_info=network_info,
5207                          block_device_info=block_device_info,
5208                          destroy_disks=destroy_disks)
5209 
5210     def _create_domain_and_network(self, context, xml, instance, network_info,
5211                                    block_device_info=None, power_on=True,
5212                                    vifs_already_plugged=False,
5213                                    post_xml_callback=None,
5214                                    destroy_disks_on_failure=False):
5215 
5216         """Do required network setup and create domain."""
5217         block_device_mapping = driver.block_device_info_get_mapping(
5218             block_device_info)
5219 
5220         for vol in block_device_mapping:
5221             connection_info = vol['connection_info']
5222 
5223             if ('data' in connection_info and
5224                     'volume_id' in connection_info['data']):
5225                 volume_id = connection_info['data']['volume_id']
5226                 encryption = encryptors.get_encryption_metadata(
5227                     context, self._volume_api, volume_id, connection_info)
5228 
5229                 if encryption:
5230                     encryptor = self._get_volume_encryptor(connection_info,
5231                                                            encryption)
5232                     encryptor.attach_volume(context, **encryption)
5233 
5234         timeout = CONF.vif_plugging_timeout
5235         if (self._conn_supports_start_paused and
5236             utils.is_neutron() and not
5237             vifs_already_plugged and power_on and timeout):
5238             events = self._get_neutron_events(network_info)
5239         else:
5240             events = []
5241 
5242         pause = bool(events)
5243         guest = None
5244         try:
5245             with self.virtapi.wait_for_instance_event(
5246                     instance, events, deadline=timeout,
5247                     error_callback=self._neutron_failed_callback):
5248                 self.plug_vifs(instance, network_info)
5249                 self.firewall_driver.setup_basic_filtering(instance,
5250                                                            network_info)
5251                 self.firewall_driver.prepare_instance_filter(instance,
5252                                                              network_info)
5253                 with self._lxc_disk_handler(instance, instance.image_meta,
5254                                             block_device_info):
5255                     guest = self._create_domain(
5256                         xml, pause=pause, power_on=power_on,
5257                         post_xml_callback=post_xml_callback)
5258 
5259                 self.firewall_driver.apply_instance_filter(instance,
5260                                                            network_info)
5261         except exception.VirtualInterfaceCreateException:
5262             # Neutron reported failure and we didn't swallow it, so
5263             # bail here
5264             with excutils.save_and_reraise_exception():
5265                 self._cleanup_failed_start(context, instance, network_info,
5266                                            block_device_info, guest,
5267                                            destroy_disks_on_failure)
5268         except eventlet.timeout.Timeout:
5269             # We never heard from Neutron
5270             LOG.warning('Timeout waiting for vif plugging callback for '
5271                         'instance with vm_state %(vm_state)s and '
5272                         'task_state %(task_state)s.',
5273                         {'vm_state': instance.vm_state,
5274                          'task_state': instance.task_state},
5275                         instance=instance)
5276             if CONF.vif_plugging_is_fatal:
5277                 self._cleanup_failed_start(context, instance, network_info,
5278                                            block_device_info, guest,
5279                                            destroy_disks_on_failure)
5280                 raise exception.VirtualInterfaceCreateException()
5281         except Exception:
5282             # Any other error, be sure to clean up
5283             LOG.error('Failed to start libvirt guest', instance=instance)
5284             with excutils.save_and_reraise_exception():
5285                 self._cleanup_failed_start(context, instance, network_info,
5286                                            block_device_info, guest,
5287                                            destroy_disks_on_failure)
5288 
5289         # Resume only if domain has been paused
5290         if pause:
5291             guest.resume()
5292         return guest
5293 
5294     def _get_vcpu_total(self):
5295         """Get available vcpu number of physical computer.
5296 
5297         :returns: the number of cpu core instances can be used.
5298 
5299         """
5300         try:
5301             total_pcpus = self._host.get_cpu_count()
5302         except libvirt.libvirtError:
5303             LOG.warning("Cannot get the number of cpu, because this "
5304                         "function is not implemented for this platform. ")
5305             return 0
5306 
5307         if not CONF.vcpu_pin_set:
5308             return total_pcpus
5309 
5310         available_ids = hardware.get_vcpu_pin_set()
5311         # We get the list of online CPUs on the host and see if the requested
5312         # set falls under these. If not, we retain the old behavior.
5313         online_pcpus = None
5314         try:
5315             online_pcpus = self._host.get_online_cpus()
5316         except libvirt.libvirtError as ex:
5317             error_code = ex.get_error_code()
5318             err_msg = encodeutils.exception_to_unicode(ex)
5319             LOG.warning(
5320                 "Couldn't retrieve the online CPUs due to a Libvirt "
5321                 "error: %(error)s with error code: %(error_code)s",
5322                 {'error': err_msg, 'error_code': error_code})
5323         if online_pcpus:
5324             if not (available_ids <= online_pcpus):
5325                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5326                          "specified cpuset is not online. Online cpuset(s): "
5327                          "%(online)s, requested cpuset(s): %(req)s"),
5328                        {'online': sorted(online_pcpus),
5329                         'req': sorted(available_ids)})
5330                 raise exception.Invalid(msg)
5331         elif sorted(available_ids)[-1] >= total_pcpus:
5332             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5333                                       "out of hypervisor cpu range."))
5334         return len(available_ids)
5335 
5336     @staticmethod
5337     def _get_local_gb_info():
5338         """Get local storage info of the compute node in GB.
5339 
5340         :returns: A dict containing:
5341              :total: How big the overall usable filesystem is (in gigabytes)
5342              :free: How much space is free (in gigabytes)
5343              :used: How much space is used (in gigabytes)
5344         """
5345 
5346         if CONF.libvirt.images_type == 'lvm':
5347             info = lvm.get_volume_group_info(
5348                                CONF.libvirt.images_volume_group)
5349         elif CONF.libvirt.images_type == 'rbd':
5350             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5351         else:
5352             info = libvirt_utils.get_fs_info(CONF.instances_path)
5353 
5354         for (k, v) in info.items():
5355             info[k] = v / units.Gi
5356 
5357         return info
5358 
5359     def _get_vcpu_used(self):
5360         """Get vcpu usage number of physical computer.
5361 
5362         :returns: The total number of vcpu(s) that are currently being used.
5363 
5364         """
5365 
5366         total = 0
5367 
5368         # Not all libvirt drivers will support the get_vcpus_info()
5369         #
5370         # For example, LXC does not have a concept of vCPUs, while
5371         # QEMU (TCG) traditionally handles all vCPUs in a single
5372         # thread. So both will report an exception when the vcpus()
5373         # API call is made. In such a case we should report the
5374         # guest as having 1 vCPU, since that lets us still do
5375         # CPU over commit calculations that apply as the total
5376         # guest count scales.
5377         #
5378         # It is also possible that we might see an exception if
5379         # the guest is just in middle of shutting down. Technically
5380         # we should report 0 for vCPU usage in this case, but we
5381         # we can't reliably distinguish the vcpu not supported
5382         # case from the just shutting down case. Thus we don't know
5383         # whether to report 1 or 0 for vCPU count.
5384         #
5385         # Under-reporting vCPUs is bad because it could conceivably
5386         # let the scheduler place too many guests on the host. Over-
5387         # reporting vCPUs is not a problem as it'll auto-correct on
5388         # the next refresh of usage data.
5389         #
5390         # Thus when getting an exception we always report 1 as the
5391         # vCPU count, as the least worst value.
5392         for guest in self._host.list_guests():
5393             try:
5394                 vcpus = guest.get_vcpus_info()
5395                 total += len(list(vcpus))
5396             except libvirt.libvirtError:
5397                 total += 1
5398             # NOTE(gtt116): give other tasks a chance.
5399             greenthread.sleep(0)
5400         return total
5401 
5402     def _get_supported_vgpu_types(self):
5403         if not CONF.devices.enabled_vgpu_types:
5404             return []
5405         # TODO(sbauza): Move this check up to compute_manager.init_host
5406         if len(CONF.devices.enabled_vgpu_types) > 1:
5407             LOG.warning('libvirt only supports one GPU type per compute node,'
5408                         ' only first type will be used.')
5409         requested_types = CONF.devices.enabled_vgpu_types[:1]
5410         return requested_types
5411 
5412     def _get_vgpu_total(self):
5413         """Returns the number of total available vGPUs for any GPU type that is
5414         enabled with the enabled_vgpu_types CONF option.
5415         """
5416         requested_types = self._get_supported_vgpu_types()
5417         # Bail out early if operator doesn't care about providing vGPUs
5418         if not requested_types:
5419             return 0
5420         # Filter how many available mdevs we can create for all the supported
5421         # types.
5422         mdev_capable_devices = self._get_mdev_capable_devices(requested_types)
5423         vgpus = 0
5424         for dev in mdev_capable_devices:
5425             for _type in dev['types']:
5426                 vgpus += dev['types'][_type]['availableInstances']
5427         # Count the already created (but possibly not assigned to a guest)
5428         # mdevs for all the supported types
5429         mediated_devices = self._get_mediated_devices(requested_types)
5430         vgpus += len(mediated_devices)
5431         return vgpus
5432 
5433     def _get_instance_capabilities(self):
5434         """Get hypervisor instance capabilities
5435 
5436         Returns a list of tuples that describe instances the
5437         hypervisor is capable of hosting.  Each tuple consists
5438         of the triplet (arch, hypervisor_type, vm_mode).
5439 
5440         :returns: List of tuples describing instance capabilities
5441         """
5442         caps = self._host.get_capabilities()
5443         instance_caps = list()
5444         for g in caps.guests:
5445             for dt in g.domtype:
5446                 instance_cap = (
5447                     fields.Architecture.canonicalize(g.arch),
5448                     fields.HVType.canonicalize(dt),
5449                     fields.VMMode.canonicalize(g.ostype))
5450                 instance_caps.append(instance_cap)
5451 
5452         return instance_caps
5453 
5454     def _get_cpu_info(self):
5455         """Get cpuinfo information.
5456 
5457         Obtains cpu feature from virConnect.getCapabilities.
5458 
5459         :return: see above description
5460 
5461         """
5462 
5463         caps = self._host.get_capabilities()
5464         cpu_info = dict()
5465 
5466         cpu_info['arch'] = caps.host.cpu.arch
5467         cpu_info['model'] = caps.host.cpu.model
5468         cpu_info['vendor'] = caps.host.cpu.vendor
5469 
5470         topology = dict()
5471         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5472         topology['sockets'] = caps.host.cpu.sockets
5473         topology['cores'] = caps.host.cpu.cores
5474         topology['threads'] = caps.host.cpu.threads
5475         cpu_info['topology'] = topology
5476 
5477         features = set()
5478         for f in caps.host.cpu.features:
5479             features.add(f.name)
5480         cpu_info['features'] = features
5481         return cpu_info
5482 
5483     def _get_pcinet_info(self, vf_address):
5484         """Returns a dict of NET device."""
5485         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5486         if not devname:
5487             return
5488 
5489         virtdev = self._host.device_lookup_by_name(devname)
5490         xmlstr = virtdev.XMLDesc(0)
5491         cfgdev = vconfig.LibvirtConfigNodeDevice()
5492         cfgdev.parse_str(xmlstr)
5493         return {'name': cfgdev.name,
5494                 'capabilities': cfgdev.pci_capability.features}
5495 
5496     def _get_pcidev_info(self, devname):
5497         """Returns a dict of PCI device."""
5498 
5499         def _get_device_type(cfgdev, pci_address):
5500             """Get a PCI device's device type.
5501 
5502             An assignable PCI device can be a normal PCI device,
5503             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5504             Function (VF). Only normal PCI devices or SR-IOV VFs
5505             are assignable, while SR-IOV PFs are always owned by
5506             hypervisor.
5507             """
5508             for fun_cap in cfgdev.pci_capability.fun_capability:
5509                 if fun_cap.type == 'virt_functions':
5510                     return {
5511                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5512                     }
5513                 if (fun_cap.type == 'phys_function' and
5514                     len(fun_cap.device_addrs) != 0):
5515                     phys_address = "%04x:%02x:%02x.%01x" % (
5516                         fun_cap.device_addrs[0][0],
5517                         fun_cap.device_addrs[0][1],
5518                         fun_cap.device_addrs[0][2],
5519                         fun_cap.device_addrs[0][3])
5520                     return {
5521                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5522                         'parent_addr': phys_address,
5523                     }
5524 
5525             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5526             # only when VFs are enabled. The check below is a workaround
5527             # to get the correct report regardless of whether or not any
5528             # VFs are enabled for the device.
5529             if not self._host.has_min_version(
5530                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5531                 is_physical_function = pci_utils.is_physical_function(
5532                     *pci_utils.get_pci_address_fields(pci_address))
5533                 if is_physical_function:
5534                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5535 
5536             return {'dev_type': fields.PciDeviceType.STANDARD}
5537 
5538         def _get_device_capabilities(device, address):
5539             """Get PCI VF device's additional capabilities.
5540 
5541             If a PCI device is a virtual function, this function reads the PCI
5542             parent's network capabilities (must be always a NIC device) and
5543             appends this information to the device's dictionary.
5544             """
5545             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5546                 pcinet_info = self._get_pcinet_info(address)
5547                 if pcinet_info:
5548                     return {'capabilities':
5549                                 {'network': pcinet_info.get('capabilities')}}
5550             return {}
5551 
5552         virtdev = self._host.device_lookup_by_name(devname)
5553         xmlstr = virtdev.XMLDesc(0)
5554         cfgdev = vconfig.LibvirtConfigNodeDevice()
5555         cfgdev.parse_str(xmlstr)
5556 
5557         address = "%04x:%02x:%02x.%1x" % (
5558             cfgdev.pci_capability.domain,
5559             cfgdev.pci_capability.bus,
5560             cfgdev.pci_capability.slot,
5561             cfgdev.pci_capability.function)
5562 
5563         device = {
5564             "dev_id": cfgdev.name,
5565             "address": address,
5566             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5567             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5568             }
5569 
5570         device["numa_node"] = cfgdev.pci_capability.numa_node
5571 
5572         # requirement by DataBase Model
5573         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5574         device.update(_get_device_type(cfgdev, address))
5575         device.update(_get_device_capabilities(device, address))
5576         return device
5577 
5578     def _get_pci_passthrough_devices(self):
5579         """Get host PCI devices information.
5580 
5581         Obtains pci devices information from libvirt, and returns
5582         as a JSON string.
5583 
5584         Each device information is a dictionary, with mandatory keys
5585         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5586         'label' and other optional device specific information.
5587 
5588         Refer to the objects/pci_device.py for more idea of these keys.
5589 
5590         :returns: a JSON string containing a list of the assignable PCI
5591                   devices information
5592         """
5593         # Bail early if we know we can't support `listDevices` to avoid
5594         # repeated warnings within a periodic task
5595         if not getattr(self, '_list_devices_supported', True):
5596             return jsonutils.dumps([])
5597 
5598         try:
5599             dev_names = self._host.list_pci_devices() or []
5600         except libvirt.libvirtError as ex:
5601             error_code = ex.get_error_code()
5602             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5603                 self._list_devices_supported = False
5604                 LOG.warning("URI %(uri)s does not support "
5605                             "listDevices: %(error)s",
5606                             {'uri': self._uri(),
5607                              'error': encodeutils.exception_to_unicode(ex)})
5608                 return jsonutils.dumps([])
5609             else:
5610                 raise
5611 
5612         pci_info = []
5613         for name in dev_names:
5614             pci_info.append(self._get_pcidev_info(name))
5615 
5616         return jsonutils.dumps(pci_info)
5617 
5618     def _get_mdev_capabilities_for_dev(self, devname, types=None):
5619         """Returns a dict of MDEV capable device with the ID as first key
5620         and then a list of supported types, each of them being a dict.
5621 
5622         :param types: Only return those specific types.
5623         """
5624         virtdev = self._host.device_lookup_by_name(devname)
5625         xmlstr = virtdev.XMLDesc(0)
5626         cfgdev = vconfig.LibvirtConfigNodeDevice()
5627         cfgdev.parse_str(xmlstr)
5628 
5629         device = {
5630             "dev_id": cfgdev.name,
5631             "types": {},
5632         }
5633         for mdev_cap in cfgdev.pci_capability.mdev_capability:
5634             for cap in mdev_cap.mdev_types:
5635                 if not types or cap['type'] in types:
5636                     device["types"].update({cap['type']: {
5637                         'availableInstances': cap['availableInstances'],
5638                         'name': cap['name'],
5639                         'deviceAPI': cap['deviceAPI']}})
5640         return device
5641 
5642     def _get_mdev_capable_devices(self, types=None):
5643         """Get host devices supporting mdev types.
5644 
5645         Obtain devices information from libvirt and returns a list of
5646         dictionaries.
5647 
5648         :param types: Filter only devices supporting those types.
5649         """
5650         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
5651             return []
5652         dev_names = self._host.list_mdev_capable_devices() or []
5653         mdev_capable_devices = []
5654         for name in dev_names:
5655             device = self._get_mdev_capabilities_for_dev(name, types)
5656             if not device["types"]:
5657                 continue
5658             mdev_capable_devices.append(device)
5659         return mdev_capable_devices
5660 
5661     def _get_mediated_device_information(self, devname):
5662         """Returns a dict of a mediated device."""
5663         virtdev = self._host.device_lookup_by_name(devname)
5664         xmlstr = virtdev.XMLDesc(0)
5665         cfgdev = vconfig.LibvirtConfigNodeDevice()
5666         cfgdev.parse_str(xmlstr)
5667 
5668         device = {
5669             "dev_id": cfgdev.name,
5670             "type": cfgdev.mdev_information.type,
5671             "iommu_group": cfgdev.mdev_information.iommu_group,
5672         }
5673         return device
5674 
5675     def _get_mediated_devices(self, types=None):
5676         """Get host mediated devices.
5677 
5678         Obtain devices information from libvirt and returns a list of
5679         dictionaries.
5680 
5681         :param types: Filter only devices supporting those types.
5682         """
5683         if not self._host.has_min_version(MIN_LIBVIRT_MDEV_SUPPORT):
5684             return []
5685         dev_names = self._host.list_mediated_devices() or []
5686         mediated_devices = []
5687         for name in dev_names:
5688             device = self._get_mediated_device_information(name)
5689             if not types or device["type"] in types:
5690                 mediated_devices.append(device)
5691         return mediated_devices
5692 
5693     def _has_numa_support(self):
5694         # This means that the host can support LibvirtConfigGuestNUMATune
5695         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
5696         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
5697             if self._host.has_version(ver):
5698                 if not getattr(self, '_bad_libvirt_numa_version_warn', False):
5699                     LOG.warning('You are running with libvirt version %s '
5700                                 'which is known to have broken NUMA support. '
5701                                 'Consider patching or updating libvirt on '
5702                                 'this host if you need NUMA support.',
5703                                 self._version_to_string(ver))
5704                     self._bad_libvirt_numa_version_warn = True
5705                 return False
5706 
5707         caps = self._host.get_capabilities()
5708 
5709         if (caps.host.cpu.arch in (fields.Architecture.I686,
5710                                    fields.Architecture.X86_64,
5711                                    fields.Architecture.AARCH64) and
5712                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
5713             return True
5714         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
5715                                      fields.Architecture.PPC64LE) and
5716                 self._host.has_min_version(MIN_LIBVIRT_NUMA_VERSION_PPC,
5717                                            hv_type=host.HV_DRIVER_QEMU)):
5718             return True
5719 
5720         return False
5721 
5722     def _get_host_numa_topology(self):
5723         if not self._has_numa_support():
5724             return
5725 
5726         caps = self._host.get_capabilities()
5727         topology = caps.host.topology
5728 
5729         if topology is None or not topology.cells:
5730             return
5731 
5732         cells = []
5733         allowed_cpus = hardware.get_vcpu_pin_set()
5734         online_cpus = self._host.get_online_cpus()
5735         if allowed_cpus:
5736             allowed_cpus &= online_cpus
5737         else:
5738             allowed_cpus = online_cpus
5739 
5740         def _get_reserved_memory_for_cell(self, cell_id, page_size):
5741             cell = self._reserved_hugepages.get(cell_id, {})
5742             return cell.get(page_size, 0)
5743 
5744         for cell in topology.cells:
5745             cpuset = set(cpu.id for cpu in cell.cpus)
5746             siblings = sorted(map(set,
5747                                   set(tuple(cpu.siblings)
5748                                         if cpu.siblings else ()
5749                                       for cpu in cell.cpus)
5750                                   ))
5751             cpuset &= allowed_cpus
5752             siblings = [sib & allowed_cpus for sib in siblings]
5753             # Filter out singles and empty sibling sets that may be left
5754             siblings = [sib for sib in siblings if len(sib) > 1]
5755 
5756             mempages = [
5757                 objects.NUMAPagesTopology(
5758                     size_kb=pages.size,
5759                     total=pages.total,
5760                     used=0,
5761                     reserved=_get_reserved_memory_for_cell(
5762                         self, cell.id, pages.size))
5763                 for pages in cell.mempages]
5764 
5765             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
5766                                     memory=cell.memory / units.Ki,
5767                                     cpu_usage=0, memory_usage=0,
5768                                     siblings=siblings,
5769                                     pinned_cpus=set([]),
5770                                     mempages=mempages)
5771             cells.append(cell)
5772 
5773         return objects.NUMATopology(cells=cells)
5774 
5775     def get_all_volume_usage(self, context, compute_host_bdms):
5776         """Return usage info for volumes attached to vms on
5777            a given host.
5778         """
5779         vol_usage = []
5780 
5781         for instance_bdms in compute_host_bdms:
5782             instance = instance_bdms['instance']
5783 
5784             for bdm in instance_bdms['instance_bdms']:
5785                 mountpoint = bdm['device_name']
5786                 if mountpoint.startswith('/dev/'):
5787                     mountpoint = mountpoint[5:]
5788                 volume_id = bdm['volume_id']
5789 
5790                 LOG.debug("Trying to get stats for the volume %s",
5791                           volume_id, instance=instance)
5792                 vol_stats = self.block_stats(instance, mountpoint)
5793 
5794                 if vol_stats:
5795                     stats = dict(volume=volume_id,
5796                                  instance=instance,
5797                                  rd_req=vol_stats[0],
5798                                  rd_bytes=vol_stats[1],
5799                                  wr_req=vol_stats[2],
5800                                  wr_bytes=vol_stats[3])
5801                     LOG.debug(
5802                         "Got volume usage stats for the volume=%(volume)s,"
5803                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
5804                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
5805                         stats, instance=instance)
5806                     vol_usage.append(stats)
5807 
5808         return vol_usage
5809 
5810     def block_stats(self, instance, disk_id):
5811         """Note that this function takes an instance name."""
5812         try:
5813             guest = self._host.get_guest(instance)
5814 
5815             # TODO(sahid): We are converting all calls from a
5816             # virDomain object to use nova.virt.libvirt.Guest.
5817             # We should be able to remove domain at the end.
5818             domain = guest._domain
5819             return domain.blockStats(disk_id)
5820         except libvirt.libvirtError as e:
5821             errcode = e.get_error_code()
5822             LOG.info('Getting block stats failed, device might have '
5823                      'been detached. Instance=%(instance_name)s '
5824                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
5825                      {'instance_name': instance.name, 'disk': disk_id,
5826                       'errcode': errcode, 'e': e},
5827                      instance=instance)
5828         except exception.InstanceNotFound:
5829             LOG.info('Could not find domain in libvirt for instance %s. '
5830                      'Cannot get block stats for device', instance.name,
5831                      instance=instance)
5832 
5833     def get_console_pool_info(self, console_type):
5834         # TODO(mdragon): console proxy should be implemented for libvirt,
5835         #                in case someone wants to use it with kvm or
5836         #                such. For now return fake data.
5837         return {'address': '127.0.0.1',
5838                 'username': 'fakeuser',
5839                 'password': 'fakepassword'}
5840 
5841     def refresh_security_group_rules(self, security_group_id):
5842         self.firewall_driver.refresh_security_group_rules(security_group_id)
5843 
5844     def refresh_instance_security_rules(self, instance):
5845         self.firewall_driver.refresh_instance_security_rules(instance)
5846 
5847     def get_inventory(self, nodename):
5848         """Return a dict, keyed by resource class, of inventory information for
5849         the supplied node.
5850         """
5851         disk_gb = int(self._get_local_gb_info()['total'])
5852         memory_mb = int(self._host.get_memory_mb_total())
5853         vcpus = self._get_vcpu_total()
5854 
5855         # NOTE(sbauza): For the moment, the libvirt driver only supports
5856         # providing the total number of virtual GPUs for a single GPU type. If
5857         # you have multiple physical GPUs, each of them providing multiple GPU
5858         # types, libvirt will return the total sum of virtual GPUs
5859         # corresponding to the single type passed in enabled_vgpu_types
5860         # configuration option. Eg. if you have 2 pGPUs supporting 'nvidia-35',
5861         # each of them having 16 available instances, the total here will be
5862         # 32.
5863         # If one of the 2 pGPUs doesn't support 'nvidia-35', it won't be used.
5864         # TODO(sbauza): Use ProviderTree and traits to make a better world.
5865         vgpus = self._get_vgpu_total()
5866 
5867         # NOTE(jaypipes): We leave some fields like allocation_ratio and
5868         # reserved out of the returned dicts here because, for now at least,
5869         # the RT injects those values into the inventory dict based on the
5870         # compute_nodes record values.
5871         result = {
5872             fields.ResourceClass.VCPU: {
5873                 'total': vcpus,
5874                 'min_unit': 1,
5875                 'max_unit': vcpus,
5876                 'step_size': 1,
5877             },
5878             fields.ResourceClass.MEMORY_MB: {
5879                 'total': memory_mb,
5880                 'min_unit': 1,
5881                 'max_unit': memory_mb,
5882                 'step_size': 1,
5883             },
5884             fields.ResourceClass.DISK_GB: {
5885                 'total': disk_gb,
5886                 'min_unit': 1,
5887                 'max_unit': disk_gb,
5888                 'step_size': 1,
5889             },
5890         }
5891 
5892         if vgpus > 0:
5893             # Only provide VGPU resource classes if the driver supports it.
5894             result[fields.ResourceClass.VGPU] = {
5895                 'total': vgpus,
5896                 'min_unit': 1,
5897                 'max_unit': vgpus,
5898                 'step_size': 1,
5899                 }
5900 
5901         return result
5902 
5903     def get_available_resource(self, nodename):
5904         """Retrieve resource information.
5905 
5906         This method is called when nova-compute launches, and
5907         as part of a periodic task that records the results in the DB.
5908 
5909         :param nodename: unused in this driver
5910         :returns: dictionary containing resource info
5911         """
5912 
5913         disk_info_dict = self._get_local_gb_info()
5914         data = {}
5915 
5916         # NOTE(dprince): calling capabilities before getVersion works around
5917         # an initialization issue with some versions of Libvirt (1.0.5.5).
5918         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
5919         # See: https://bugs.launchpad.net/nova/+bug/1215593
5920         data["supported_instances"] = self._get_instance_capabilities()
5921 
5922         data["vcpus"] = self._get_vcpu_total()
5923         data["memory_mb"] = self._host.get_memory_mb_total()
5924         data["local_gb"] = disk_info_dict['total']
5925         data["vcpus_used"] = self._get_vcpu_used()
5926         data["memory_mb_used"] = self._host.get_memory_mb_used()
5927         data["local_gb_used"] = disk_info_dict['used']
5928         data["hypervisor_type"] = self._host.get_driver_type()
5929         data["hypervisor_version"] = self._host.get_version()
5930         data["hypervisor_hostname"] = self._host.get_hostname()
5931         # TODO(berrange): why do we bother converting the
5932         # libvirt capabilities XML into a special JSON format ?
5933         # The data format is different across all the drivers
5934         # so we could just return the raw capabilities XML
5935         # which 'compare_cpu' could use directly
5936         #
5937         # That said, arch_filter.py now seems to rely on
5938         # the libvirt drivers format which suggests this
5939         # data format needs to be standardized across drivers
5940         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
5941 
5942         disk_free_gb = disk_info_dict['free']
5943         disk_over_committed = self._get_disk_over_committed_size_total()
5944         available_least = disk_free_gb * units.Gi - disk_over_committed
5945         data['disk_available_least'] = available_least / units.Gi
5946 
5947         data['pci_passthrough_devices'] = \
5948             self._get_pci_passthrough_devices()
5949 
5950         numa_topology = self._get_host_numa_topology()
5951         if numa_topology:
5952             data['numa_topology'] = numa_topology._to_json()
5953         else:
5954             data['numa_topology'] = None
5955 
5956         return data
5957 
5958     def check_instance_shared_storage_local(self, context, instance):
5959         """Check if instance files located on shared storage.
5960 
5961         This runs check on the destination host, and then calls
5962         back to the source host to check the results.
5963 
5964         :param context: security context
5965         :param instance: nova.objects.instance.Instance object
5966         :returns:
5967          - tempfile: A dict containing the tempfile info on the destination
5968                      host
5969          - None:
5970 
5971             1. If the instance path is not existing.
5972             2. If the image backend is shared block storage type.
5973         """
5974         if self.image_backend.backend().is_shared_block_storage():
5975             return None
5976 
5977         dirpath = libvirt_utils.get_instance_path(instance)
5978 
5979         if not os.path.exists(dirpath):
5980             return None
5981 
5982         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
5983         LOG.debug("Creating tmpfile %s to verify with other "
5984                   "compute node that the instance is on "
5985                   "the same shared storage.",
5986                   tmp_file, instance=instance)
5987         os.close(fd)
5988         return {"filename": tmp_file}
5989 
5990     def check_instance_shared_storage_remote(self, context, data):
5991         return os.path.exists(data['filename'])
5992 
5993     def check_instance_shared_storage_cleanup(self, context, data):
5994         fileutils.delete_if_exists(data["filename"])
5995 
5996     def check_can_live_migrate_destination(self, context, instance,
5997                                            src_compute_info, dst_compute_info,
5998                                            block_migration=False,
5999                                            disk_over_commit=False):
6000         """Check if it is possible to execute live migration.
6001 
6002         This runs checks on the destination host, and then calls
6003         back to the source host to check the results.
6004 
6005         :param context: security context
6006         :param instance: nova.db.sqlalchemy.models.Instance
6007         :param block_migration: if true, prepare for block migration
6008         :param disk_over_commit: if true, allow disk over commit
6009         :returns: a LibvirtLiveMigrateData object
6010         """
6011         if disk_over_commit:
6012             disk_available_gb = dst_compute_info['local_gb']
6013         else:
6014             disk_available_gb = dst_compute_info['disk_available_least']
6015         disk_available_mb = (
6016             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
6017 
6018         # Compare CPU
6019         if not instance.vcpu_model or not instance.vcpu_model.model:
6020             source_cpu_info = src_compute_info['cpu_info']
6021             self._compare_cpu(None, source_cpu_info, instance)
6022         else:
6023             self._compare_cpu(instance.vcpu_model, None, instance)
6024 
6025         # Create file on storage, to be checked on source host
6026         filename = self._create_shared_storage_test_file(instance)
6027 
6028         data = objects.LibvirtLiveMigrateData()
6029         data.filename = filename
6030         data.image_type = CONF.libvirt.images_type
6031         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
6032         data.graphics_listen_addr_spice = CONF.spice.server_listen
6033         if CONF.serial_console.enabled:
6034             data.serial_listen_addr = CONF.serial_console.proxyclient_address
6035         else:
6036             data.serial_listen_addr = None
6037         # Notes(eliqiao): block_migration and disk_over_commit are not
6038         # nullable, so just don't set them if they are None
6039         if block_migration is not None:
6040             data.block_migration = block_migration
6041         if disk_over_commit is not None:
6042             data.disk_over_commit = disk_over_commit
6043         data.disk_available_mb = disk_available_mb
6044         return data
6045 
6046     def cleanup_live_migration_destination_check(self, context,
6047                                                  dest_check_data):
6048         """Do required cleanup on dest host after check_can_live_migrate calls
6049 
6050         :param context: security context
6051         """
6052         filename = dest_check_data.filename
6053         self._cleanup_shared_storage_test_file(filename)
6054 
6055     def check_can_live_migrate_source(self, context, instance,
6056                                       dest_check_data,
6057                                       block_device_info=None):
6058         """Check if it is possible to execute live migration.
6059 
6060         This checks if the live migration can succeed, based on the
6061         results from check_can_live_migrate_destination.
6062 
6063         :param context: security context
6064         :param instance: nova.db.sqlalchemy.models.Instance
6065         :param dest_check_data: result of check_can_live_migrate_destination
6066         :param block_device_info: result of _get_instance_block_device_info
6067         :returns: a LibvirtLiveMigrateData object
6068         """
6069         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
6070             md_obj = objects.LibvirtLiveMigrateData()
6071             md_obj.from_legacy_dict(dest_check_data)
6072             dest_check_data = md_obj
6073 
6074         # Checking shared storage connectivity
6075         # if block migration, instances_path should not be on shared storage.
6076         source = CONF.host
6077 
6078         dest_check_data.is_shared_instance_path = (
6079             self._check_shared_storage_test_file(
6080                 dest_check_data.filename, instance))
6081 
6082         dest_check_data.is_shared_block_storage = (
6083             self._is_shared_block_storage(instance, dest_check_data,
6084                                           block_device_info))
6085 
6086         if 'block_migration' not in dest_check_data:
6087             dest_check_data.block_migration = (
6088                 not dest_check_data.is_on_shared_storage())
6089 
6090         if dest_check_data.block_migration:
6091             # TODO(eliqiao): Once block_migration flag is removed from the API
6092             # we can safely remove the if condition
6093             if dest_check_data.is_on_shared_storage():
6094                 reason = _("Block migration can not be used "
6095                            "with shared storage.")
6096                 raise exception.InvalidLocalStorage(reason=reason, path=source)
6097             if 'disk_over_commit' in dest_check_data:
6098                 self._assert_dest_node_has_enough_disk(context, instance,
6099                                         dest_check_data.disk_available_mb,
6100                                         dest_check_data.disk_over_commit,
6101                                         block_device_info)
6102             if block_device_info:
6103                 bdm = block_device_info.get('block_device_mapping')
6104                 # NOTE(pkoniszewski): libvirt from version 1.2.17 upwards
6105                 # supports selective block device migration. It means that it
6106                 # is possible to define subset of block devices to be copied
6107                 # during migration. If they are not specified - block devices
6108                 # won't be migrated. However, it does not work when live
6109                 # migration is tunnelled through libvirt.
6110                 if bdm and not self._host.has_min_version(
6111                         MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6112                     # NOTE(stpierre): if this instance has mapped volumes,
6113                     # we can't do a block migration, since that will result
6114                     # in volumes being copied from themselves to themselves,
6115                     # which is a recipe for disaster.
6116                     ver = ".".join([str(x) for x in
6117                                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION])
6118                     msg = (_('Cannot block migrate instance %(uuid)s with'
6119                              ' mapped volumes. Selective block device'
6120                              ' migration feature requires libvirt version'
6121                              ' %(libvirt_ver)s') %
6122                            {'uuid': instance.uuid, 'libvirt_ver': ver})
6123                     LOG.error(msg, instance=instance)
6124                     raise exception.MigrationPreCheckError(reason=msg)
6125                 # NOTE(eliqiao): Selective disk migrations are not supported
6126                 # with tunnelled block migrations so we can block them early.
6127                 if (bdm and
6128                     (self._block_migration_flags &
6129                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
6130                     msg = (_('Cannot block migrate instance %(uuid)s with'
6131                              ' mapped volumes. Selective block device'
6132                              ' migration is not supported with tunnelled'
6133                              ' block migrations.') % {'uuid': instance.uuid})
6134                     LOG.error(msg, instance=instance)
6135                     raise exception.MigrationPreCheckError(reason=msg)
6136         elif not (dest_check_data.is_shared_block_storage or
6137                   dest_check_data.is_shared_instance_path):
6138             reason = _("Shared storage live-migration requires either shared "
6139                        "storage or boot-from-volume with no local disks.")
6140             raise exception.InvalidSharedStorage(reason=reason, path=source)
6141 
6142         # NOTE(mikal): include the instance directory name here because it
6143         # doesn't yet exist on the destination but we want to force that
6144         # same name to be used
6145         instance_path = libvirt_utils.get_instance_path(instance,
6146                                                         relative=True)
6147         dest_check_data.instance_relative_path = instance_path
6148 
6149         return dest_check_data
6150 
6151     def _is_shared_block_storage(self, instance, dest_check_data,
6152                                  block_device_info=None):
6153         """Check if all block storage of an instance can be shared
6154         between source and destination of a live migration.
6155 
6156         Returns true if the instance is volume backed and has no local disks,
6157         or if the image backend is the same on source and destination and the
6158         backend shares block storage between compute nodes.
6159 
6160         :param instance: nova.objects.instance.Instance object
6161         :param dest_check_data: dict with boolean fields image_type,
6162                                 is_shared_instance_path, and is_volume_backed
6163         """
6164         if (dest_check_data.obj_attr_is_set('image_type') and
6165                 CONF.libvirt.images_type == dest_check_data.image_type and
6166                 self.image_backend.backend().is_shared_block_storage()):
6167             # NOTE(dgenin): currently true only for RBD image backend
6168             return True
6169 
6170         if (dest_check_data.is_shared_instance_path and
6171                 self.image_backend.backend().is_file_in_instance_path()):
6172             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6173             # place block device files under the instance path
6174             return True
6175 
6176         if (dest_check_data.is_volume_backed and
6177                 not bool(self._get_instance_disk_info(instance,
6178                                                       block_device_info))):
6179             return True
6180 
6181         return False
6182 
6183     def _assert_dest_node_has_enough_disk(self, context, instance,
6184                                              available_mb, disk_over_commit,
6185                                              block_device_info):
6186         """Checks if destination has enough disk for block migration."""
6187         # Libvirt supports qcow2 disk format,which is usually compressed
6188         # on compute nodes.
6189         # Real disk image (compressed) may enlarged to "virtual disk size",
6190         # that is specified as the maximum disk size.
6191         # (See qemu-img -f path-to-disk)
6192         # Scheduler recognizes destination host still has enough disk space
6193         # if real disk size < available disk size
6194         # if disk_over_commit is True,
6195         #  otherwise virtual disk size < available disk size.
6196 
6197         available = 0
6198         if available_mb:
6199             available = available_mb * units.Mi
6200 
6201         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6202 
6203         necessary = 0
6204         if disk_over_commit:
6205             for info in disk_infos:
6206                 necessary += int(info['disk_size'])
6207         else:
6208             for info in disk_infos:
6209                 necessary += int(info['virt_disk_size'])
6210 
6211         # Check that available disk > necessary disk
6212         if (available - necessary) < 0:
6213             reason = (_('Unable to migrate %(instance_uuid)s: '
6214                         'Disk of instance is too large(available'
6215                         ' on destination host:%(available)s '
6216                         '< need:%(necessary)s)') %
6217                       {'instance_uuid': instance.uuid,
6218                        'available': available,
6219                        'necessary': necessary})
6220             raise exception.MigrationPreCheckError(reason=reason)
6221 
6222     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6223         """Check the host is compatible with the requested CPU
6224 
6225         :param guest_cpu: nova.objects.VirtCPUModel or None
6226         :param host_cpu_str: JSON from _get_cpu_info() method
6227 
6228         If the 'guest_cpu' parameter is not None, this will be
6229         validated for migration compatibility with the host.
6230         Otherwise the 'host_cpu_str' JSON string will be used for
6231         validation.
6232 
6233         :returns:
6234             None. if given cpu info is not compatible to this server,
6235             raise exception.
6236         """
6237 
6238         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6239         # guests (<domain type='qemu'>) should not matter -- in this
6240         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6241         # hardware acceleration, like KVM, is involved. So, skip the CPU
6242         # compatibility check for the QEMU domain type, and retain it for
6243         # KVM guests.
6244         if CONF.libvirt.virt_type not in ['kvm']:
6245             return
6246 
6247         if guest_cpu is None:
6248             info = jsonutils.loads(host_cpu_str)
6249             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6250             cpu = vconfig.LibvirtConfigCPU()
6251             cpu.arch = info['arch']
6252             cpu.model = info['model']
6253             cpu.vendor = info['vendor']
6254             cpu.sockets = info['topology']['sockets']
6255             cpu.cores = info['topology']['cores']
6256             cpu.threads = info['topology']['threads']
6257             for f in info['features']:
6258                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6259         else:
6260             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6261 
6262         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6263              "virCPUCompareResult")
6264         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6265         # unknown character exists in xml, then libvirt complains
6266         try:
6267             cpu_xml = cpu.to_xml()
6268             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6269             ret = self._host.compare_cpu(cpu_xml)
6270         except libvirt.libvirtError as e:
6271             error_code = e.get_error_code()
6272             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6273                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6274                           "It will be proceeded though. Error: %(error)s",
6275                           {'uri': self._uri(), 'error': e})
6276                 return
6277             else:
6278                 LOG.error(m, {'ret': e, 'u': u})
6279                 raise exception.MigrationPreCheckError(
6280                     reason=m % {'ret': e, 'u': u})
6281 
6282         if ret <= 0:
6283             LOG.error(m, {'ret': ret, 'u': u})
6284             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6285 
6286     def _create_shared_storage_test_file(self, instance):
6287         """Makes tmpfile under CONF.instances_path."""
6288         dirpath = CONF.instances_path
6289         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6290         LOG.debug("Creating tmpfile %s to notify to other "
6291                   "compute nodes that they should mount "
6292                   "the same storage.", tmp_file, instance=instance)
6293         os.close(fd)
6294         return os.path.basename(tmp_file)
6295 
6296     def _check_shared_storage_test_file(self, filename, instance):
6297         """Confirms existence of the tmpfile under CONF.instances_path.
6298 
6299         Cannot confirm tmpfile return False.
6300         """
6301         # NOTE(tpatzig): if instances_path is a shared volume that is
6302         # under heavy IO (many instances on many compute nodes),
6303         # then checking the existence of the testfile fails,
6304         # just because it takes longer until the client refreshes and new
6305         # content gets visible.
6306         # os.utime (like touch) on the directory forces the client to refresh.
6307         os.utime(CONF.instances_path, None)
6308 
6309         tmp_file = os.path.join(CONF.instances_path, filename)
6310         if not os.path.exists(tmp_file):
6311             exists = False
6312         else:
6313             exists = True
6314         LOG.debug('Check if temp file %s exists to indicate shared storage '
6315                   'is being used for migration. Exists? %s', tmp_file, exists,
6316                   instance=instance)
6317         return exists
6318 
6319     def _cleanup_shared_storage_test_file(self, filename):
6320         """Removes existence of the tmpfile under CONF.instances_path."""
6321         tmp_file = os.path.join(CONF.instances_path, filename)
6322         os.remove(tmp_file)
6323 
6324     def ensure_filtering_rules_for_instance(self, instance, network_info):
6325         """Ensure that an instance's filtering rules are enabled.
6326 
6327         When migrating an instance, we need the filtering rules to
6328         be configured on the destination host before starting the
6329         migration.
6330 
6331         Also, when restarting the compute service, we need to ensure
6332         that filtering rules exist for all running services.
6333         """
6334 
6335         self.firewall_driver.setup_basic_filtering(instance, network_info)
6336         self.firewall_driver.prepare_instance_filter(instance,
6337                 network_info)
6338 
6339         # nwfilters may be defined in a separate thread in the case
6340         # of libvirt non-blocking mode, so we wait for completion
6341         timeout_count = list(range(CONF.live_migration_retry_count))
6342         while timeout_count:
6343             if self.firewall_driver.instance_filter_exists(instance,
6344                                                            network_info):
6345                 break
6346             timeout_count.pop()
6347             if len(timeout_count) == 0:
6348                 msg = _('The firewall filter for %s does not exist')
6349                 raise exception.InternalError(msg % instance.name)
6350             greenthread.sleep(1)
6351 
6352     def filter_defer_apply_on(self):
6353         self.firewall_driver.filter_defer_apply_on()
6354 
6355     def filter_defer_apply_off(self):
6356         self.firewall_driver.filter_defer_apply_off()
6357 
6358     def live_migration(self, context, instance, dest,
6359                        post_method, recover_method, block_migration=False,
6360                        migrate_data=None):
6361         """Spawning live_migration operation for distributing high-load.
6362 
6363         :param context: security context
6364         :param instance:
6365             nova.db.sqlalchemy.models.Instance object
6366             instance object that is migrated.
6367         :param dest: destination host
6368         :param post_method:
6369             post operation method.
6370             expected nova.compute.manager._post_live_migration.
6371         :param recover_method:
6372             recovery method when any exception occurs.
6373             expected nova.compute.manager._rollback_live_migration.
6374         :param block_migration: if true, do block migration.
6375         :param migrate_data: a LibvirtLiveMigrateData object
6376 
6377         """
6378 
6379         # 'dest' will be substituted into 'migration_uri' so ensure
6380         # it does't contain any characters that could be used to
6381         # exploit the URI accepted by libivrt
6382         if not libvirt_utils.is_valid_hostname(dest):
6383             raise exception.InvalidHostname(hostname=dest)
6384 
6385         self._live_migration(context, instance, dest,
6386                              post_method, recover_method, block_migration,
6387                              migrate_data)
6388 
6389     def live_migration_abort(self, instance):
6390         """Aborting a running live-migration.
6391 
6392         :param instance: instance object that is in migration
6393 
6394         """
6395 
6396         guest = self._host.get_guest(instance)
6397         dom = guest._domain
6398 
6399         try:
6400             dom.abortJob()
6401         except libvirt.libvirtError as e:
6402             LOG.error("Failed to cancel migration %s",
6403                     encodeutils.exception_to_unicode(e), instance=instance)
6404             raise
6405 
6406     def _verify_serial_console_is_disabled(self):
6407         if CONF.serial_console.enabled:
6408 
6409             msg = _('Your destination node does not support'
6410                     ' retrieving listen addresses.  In order'
6411                     ' for live migration to work properly you'
6412                     ' must disable serial console.')
6413             raise exception.MigrationError(reason=msg)
6414 
6415     def _live_migration_operation(self, context, instance, dest,
6416                                   block_migration, migrate_data, guest,
6417                                   device_names):
6418         """Invoke the live migration operation
6419 
6420         :param context: security context
6421         :param instance:
6422             nova.db.sqlalchemy.models.Instance object
6423             instance object that is migrated.
6424         :param dest: destination host
6425         :param block_migration: if true, do block migration.
6426         :param migrate_data: a LibvirtLiveMigrateData object
6427         :param guest: the guest domain object
6428         :param device_names: list of device names that are being migrated with
6429             instance
6430 
6431         This method is intended to be run in a background thread and will
6432         block that thread until the migration is finished or failed.
6433         """
6434         try:
6435             if migrate_data.block_migration:
6436                 migration_flags = self._block_migration_flags
6437             else:
6438                 migration_flags = self._live_migration_flags
6439 
6440             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6441                 migrate_data)
6442             if not serial_listen_addr:
6443                 # In this context we want to ensure that serial console is
6444                 # disabled on source node. This is because nova couldn't
6445                 # retrieve serial listen address from destination node, so we
6446                 # consider that destination node might have serial console
6447                 # disabled as well.
6448                 self._verify_serial_console_is_disabled()
6449 
6450             # NOTE(aplanas) migrate_uri will have a value only in the
6451             # case that `live_migration_inbound_addr` parameter is
6452             # set, and we propose a non tunneled migration.
6453             migrate_uri = None
6454             if ('target_connect_addr' in migrate_data and
6455                     migrate_data.target_connect_addr is not None):
6456                 dest = migrate_data.target_connect_addr
6457                 if (migration_flags &
6458                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6459                     migrate_uri = self._migrate_uri(dest)
6460 
6461             params = None
6462             new_xml_str = None
6463             if CONF.libvirt.virt_type != "parallels":
6464                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
6465                     # TODO(sahid): It's not a really good idea to pass
6466                     # the method _get_volume_config and we should to find
6467                     # a way to avoid this in future.
6468                     guest, migrate_data, self._get_volume_config)
6469             if self._host.has_min_version(
6470                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6471                 params = {
6472                     'destination_xml': new_xml_str,
6473                     'migrate_disks': device_names,
6474                 }
6475                 # NOTE(pkoniszewski): Because of precheck which blocks
6476                 # tunnelled block live migration with mapped volumes we
6477                 # can safely remove migrate_disks when tunnelling is on.
6478                 # Otherwise we will block all tunnelled block migrations,
6479                 # even when an instance does not have volumes mapped.
6480                 # This is because selective disk migration is not
6481                 # supported in tunnelled block live migration. Also we
6482                 # cannot fallback to migrateToURI2 in this case because of
6483                 # bug #1398999
6484                 if (migration_flags &
6485                     libvirt.VIR_MIGRATE_TUNNELLED != 0):
6486                     params.pop('migrate_disks')
6487 
6488             # TODO(sahid): This should be in
6489             # post_live_migration_at_source but no way to retrieve
6490             # ports acquired on the host for the guest at this
6491             # step. Since the domain is going to be removed from
6492             # libvird on source host after migration, we backup the
6493             # serial ports to release them if all went well.
6494             serial_ports = []
6495             if CONF.serial_console.enabled:
6496                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6497 
6498             guest.migrate(self._live_migration_uri(dest),
6499                           migrate_uri=migrate_uri,
6500                           flags=migration_flags,
6501                           params=params,
6502                           domain_xml=new_xml_str,
6503                           bandwidth=CONF.libvirt.live_migration_bandwidth)
6504 
6505             for hostname, port in serial_ports:
6506                 serial_console.release_port(host=hostname, port=port)
6507         except Exception as e:
6508             with excutils.save_and_reraise_exception():
6509                 LOG.error("Live Migration failure: %s", e, instance=instance)
6510 
6511         # If 'migrateToURI' fails we don't know what state the
6512         # VM instances on each host are in. Possibilities include
6513         #
6514         #  1. src==running, dst==none
6515         #
6516         #     Migration failed & rolled back, or never started
6517         #
6518         #  2. src==running, dst==paused
6519         #
6520         #     Migration started but is still ongoing
6521         #
6522         #  3. src==paused,  dst==paused
6523         #
6524         #     Migration data transfer completed, but switchover
6525         #     is still ongoing, or failed
6526         #
6527         #  4. src==paused,  dst==running
6528         #
6529         #     Migration data transfer completed, switchover
6530         #     happened but cleanup on source failed
6531         #
6532         #  5. src==none,    dst==running
6533         #
6534         #     Migration fully succeeded.
6535         #
6536         # Libvirt will aim to complete any migration operation
6537         # or roll it back. So even if the migrateToURI call has
6538         # returned an error, if the migration was not finished
6539         # libvirt should clean up.
6540         #
6541         # So we take the error raise here with a pinch of salt
6542         # and rely on the domain job info status to figure out
6543         # what really happened to the VM, which is a much more
6544         # reliable indicator.
6545         #
6546         # In particular we need to try very hard to ensure that
6547         # Nova does not "forget" about the guest. ie leaving it
6548         # running on a different host to the one recorded in
6549         # the database, as that would be a serious resource leak
6550 
6551         LOG.debug("Migration operation thread has finished",
6552                   instance=instance)
6553 
6554     def _live_migration_copy_disk_paths(self, context, instance, guest):
6555         '''Get list of disks to copy during migration
6556 
6557         :param context: security context
6558         :param instance: the instance being migrated
6559         :param guest: the Guest instance being migrated
6560 
6561         Get the list of disks to copy during migration.
6562 
6563         :returns: a list of local source paths and a list of device names to
6564             copy
6565         '''
6566 
6567         disk_paths = []
6568         device_names = []
6569         block_devices = []
6570 
6571         # TODO(pkoniszewski): Remove version check when we bump min libvirt
6572         # version to >= 1.2.17.
6573         if (self._block_migration_flags &
6574                 libvirt.VIR_MIGRATE_TUNNELLED == 0 and
6575                 self._host.has_min_version(
6576                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION)):
6577             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
6578                 context, instance.uuid)
6579             block_device_info = driver.get_block_device_info(instance,
6580                                                              bdm_list)
6581 
6582             block_device_mappings = driver.block_device_info_get_mapping(
6583                 block_device_info)
6584             for bdm in block_device_mappings:
6585                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
6586                 block_devices.append(device_name)
6587 
6588         for dev in guest.get_all_disks():
6589             if dev.readonly or dev.shareable:
6590                 continue
6591             if dev.source_type not in ["file", "block"]:
6592                 continue
6593             if dev.target_dev in block_devices:
6594                 continue
6595             disk_paths.append(dev.source_path)
6596             device_names.append(dev.target_dev)
6597         return (disk_paths, device_names)
6598 
6599     def _live_migration_data_gb(self, instance, disk_paths):
6600         '''Calculate total amount of data to be transferred
6601 
6602         :param instance: the nova.objects.Instance being migrated
6603         :param disk_paths: list of disk paths that are being migrated
6604         with instance
6605 
6606         Calculates the total amount of data that needs to be
6607         transferred during the live migration. The actual
6608         amount copied will be larger than this, due to the
6609         guest OS continuing to dirty RAM while the migration
6610         is taking place. So this value represents the minimal
6611         data size possible.
6612 
6613         :returns: data size to be copied in GB
6614         '''
6615 
6616         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
6617         if ram_gb < 2:
6618             ram_gb = 2
6619 
6620         disk_gb = 0
6621         for path in disk_paths:
6622             try:
6623                 size = os.stat(path).st_size
6624                 size_gb = (size / units.Gi)
6625                 if size_gb < 2:
6626                     size_gb = 2
6627                 disk_gb += size_gb
6628             except OSError as e:
6629                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
6630                             {'disk': path, 'ex': e})
6631                 # Ignore error since we don't want to break
6632                 # the migration monitoring thread operation
6633 
6634         return ram_gb + disk_gb
6635 
6636     def _get_migration_flags(self, is_block_migration):
6637         if is_block_migration:
6638             return self._block_migration_flags
6639         return self._live_migration_flags
6640 
6641     def _live_migration_monitor(self, context, instance, guest,
6642                                 dest, post_method,
6643                                 recover_method, block_migration,
6644                                 migrate_data, finish_event,
6645                                 disk_paths):
6646         on_migration_failure = deque()
6647         data_gb = self._live_migration_data_gb(instance, disk_paths)
6648         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
6649         migration = migrate_data.migration
6650         curdowntime = None
6651 
6652         migration_flags = self._get_migration_flags(
6653                                   migrate_data.block_migration)
6654 
6655         n = 0
6656         start = time.time()
6657         progress_time = start
6658         progress_watermark = None
6659         previous_data_remaining = -1
6660         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
6661         while True:
6662             info = guest.get_job_info()
6663 
6664             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6665                 # Either still running, or failed or completed,
6666                 # lets untangle the mess
6667                 if not finish_event.ready():
6668                     LOG.debug("Operation thread is still running",
6669                               instance=instance)
6670                 else:
6671                     info.type = libvirt_migrate.find_job_type(guest, instance)
6672                     LOG.debug("Fixed incorrect job type to be %d",
6673                               info.type, instance=instance)
6674 
6675             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6676                 # Migration is not yet started
6677                 LOG.debug("Migration not running yet",
6678                           instance=instance)
6679             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
6680                 # Migration is still running
6681                 #
6682                 # This is where we wire up calls to change live
6683                 # migration status. eg change max downtime, cancel
6684                 # the operation, change max bandwidth
6685                 libvirt_migrate.run_tasks(guest, instance,
6686                                           self.active_migrations,
6687                                           on_migration_failure,
6688                                           migration,
6689                                           is_post_copy_enabled)
6690 
6691                 now = time.time()
6692                 elapsed = now - start
6693 
6694                 if ((progress_watermark is None) or
6695                     (progress_watermark == 0) or
6696                     (progress_watermark > info.data_remaining)):
6697                     progress_watermark = info.data_remaining
6698                     progress_time = now
6699 
6700                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
6701                 completion_timeout = int(
6702                     CONF.libvirt.live_migration_completion_timeout * data_gb)
6703                 if libvirt_migrate.should_abort(instance, now, progress_time,
6704                                                 progress_timeout, elapsed,
6705                                                 completion_timeout,
6706                                                 migration.status):
6707                     try:
6708                         guest.abort_job()
6709                     except libvirt.libvirtError as e:
6710                         LOG.warning("Failed to abort migration %s",
6711                                 encodeutils.exception_to_unicode(e),
6712                                 instance=instance)
6713                         self._clear_empty_migration(instance)
6714                         raise
6715 
6716                 if (is_post_copy_enabled and
6717                     libvirt_migrate.should_switch_to_postcopy(
6718                     info.memory_iteration, info.data_remaining,
6719                     previous_data_remaining, migration.status)):
6720                     libvirt_migrate.trigger_postcopy_switch(guest,
6721                                                             instance,
6722                                                             migration)
6723                 previous_data_remaining = info.data_remaining
6724 
6725                 curdowntime = libvirt_migrate.update_downtime(
6726                     guest, instance, curdowntime,
6727                     downtime_steps, elapsed)
6728 
6729                 # We loop every 500ms, so don't log on every
6730                 # iteration to avoid spamming logs for long
6731                 # running migrations. Just once every 5 secs
6732                 # is sufficient for developers to debug problems.
6733                 # We log once every 30 seconds at info to help
6734                 # admins see slow running migration operations
6735                 # when debug logs are off.
6736                 if (n % 10) == 0:
6737                     # Ignoring memory_processed, as due to repeated
6738                     # dirtying of data, this can be way larger than
6739                     # memory_total. Best to just look at what's
6740                     # remaining to copy and ignore what's done already
6741                     #
6742                     # TODO(berrange) perhaps we could include disk
6743                     # transfer stats in the progress too, but it
6744                     # might make memory info more obscure as large
6745                     # disk sizes might dwarf memory size
6746                     remaining = 100
6747                     if info.memory_total != 0:
6748                         remaining = round(info.memory_remaining *
6749                                           100 / info.memory_total)
6750 
6751                     libvirt_migrate.save_stats(instance, migration,
6752                                                info, remaining)
6753 
6754                     lg = LOG.debug
6755                     if (n % 60) == 0:
6756                         lg = LOG.info
6757 
6758                     lg("Migration running for %(secs)d secs, "
6759                        "memory %(remaining)d%% remaining; "
6760                        "(bytes processed=%(processed_memory)d, "
6761                        "remaining=%(remaining_memory)d, "
6762                        "total=%(total_memory)d)",
6763                        {"secs": n / 2, "remaining": remaining,
6764                         "processed_memory": info.memory_processed,
6765                         "remaining_memory": info.memory_remaining,
6766                         "total_memory": info.memory_total}, instance=instance)
6767                     if info.data_remaining > progress_watermark:
6768                         lg("Data remaining %(remaining)d bytes, "
6769                            "low watermark %(watermark)d bytes "
6770                            "%(last)d seconds ago",
6771                            {"remaining": info.data_remaining,
6772                             "watermark": progress_watermark,
6773                             "last": (now - progress_time)}, instance=instance)
6774 
6775                 n = n + 1
6776             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
6777                 # Migration is all done
6778                 LOG.info("Migration operation has completed",
6779                          instance=instance)
6780                 post_method(context, instance, dest, block_migration,
6781                             migrate_data)
6782                 break
6783             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
6784                 # Migration did not succeed
6785                 LOG.error("Migration operation has aborted", instance=instance)
6786                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6787                                                   on_migration_failure)
6788                 recover_method(context, instance, dest, migrate_data)
6789                 break
6790             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
6791                 # Migration was stopped by admin
6792                 LOG.warning("Migration operation was cancelled",
6793                             instance=instance)
6794                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6795                                                   on_migration_failure)
6796                 recover_method(context, instance, dest, migrate_data,
6797                                migration_status='cancelled')
6798                 break
6799             else:
6800                 LOG.warning("Unexpected migration job type: %d",
6801                             info.type, instance=instance)
6802 
6803             time.sleep(0.5)
6804         self._clear_empty_migration(instance)
6805 
6806     def _clear_empty_migration(self, instance):
6807         try:
6808             del self.active_migrations[instance.uuid]
6809         except KeyError:
6810             LOG.warning("There are no records in active migrations "
6811                         "for instance", instance=instance)
6812 
6813     def _live_migration(self, context, instance, dest, post_method,
6814                         recover_method, block_migration,
6815                         migrate_data):
6816         """Do live migration.
6817 
6818         :param context: security context
6819         :param instance:
6820             nova.db.sqlalchemy.models.Instance object
6821             instance object that is migrated.
6822         :param dest: destination host
6823         :param post_method:
6824             post operation method.
6825             expected nova.compute.manager._post_live_migration.
6826         :param recover_method:
6827             recovery method when any exception occurs.
6828             expected nova.compute.manager._rollback_live_migration.
6829         :param block_migration: if true, do block migration.
6830         :param migrate_data: a LibvirtLiveMigrateData object
6831 
6832         This fires off a new thread to run the blocking migration
6833         operation, and then this thread monitors the progress of
6834         migration and controls its operation
6835         """
6836 
6837         guest = self._host.get_guest(instance)
6838 
6839         disk_paths = []
6840         device_names = []
6841         if (migrate_data.block_migration and
6842                 CONF.libvirt.virt_type != "parallels"):
6843             disk_paths, device_names = self._live_migration_copy_disk_paths(
6844                 context, instance, guest)
6845 
6846         if utils.is_neutron():
6847             # Before to continue process of live migration we have to
6848             # ensure the VIFs on destination node are ready.
6849             LOG.debug("Slow the migration process until Nova "
6850                       "receives network events indicating everything is "
6851                       "setup. (default: %d)",
6852                       CONF.libvirt.live_migration_bandwidth)
6853             guest.migrate_configure_max_speed(1)  # 1 MiB/s
6854 
6855             # In case of Linux Bridge, the agent is waiting for new TAPs
6856             # devices on destination node. They are going to be created by
6857             # libvirt at the very beginning of the live-migration
6858             # process. Then receiving the events from Neutron will ensure
6859             # that everything is configured correclty.
6860             events = self._get_neutron_events_for_live_migration(
6861                 instance.get_network_info())
6862         else:
6863             # TODO(sahid): This condition should be removed when
6864             # nova-network will be erased from the tree (Rocky).
6865             events = []
6866         try:
6867             with self.virtapi.wait_for_instance_event(
6868                     instance, events, deadline=CONF.vif_plugging_timeout):
6869                 opthread = utils.spawn(self._live_migration_operation,
6870                                        context, instance, dest,
6871                                        block_migration,
6872                                        migrate_data, guest,
6873                                        device_names)
6874         except eventlet.timeout.Timeout:
6875             LOG.warning('Timeout waiting for VIFs plugging events, '
6876                         'continuing anyway but network could have '
6877                         'connectivity issues', instance=instance)
6878         else:
6879             if utils.is_neutron():
6880                 LOG.debug('VIFs events received, continuing migration, '
6881                           'bandwidth: %d',
6882                           CONF.libvirt.live_migration_bandwidth,
6883                           instance=instance)
6884         finally:
6885             if utils.is_neutron():
6886                 # Even if quit 'wait_for_instance_event' with a
6887                 # timeout or events received we should reconfigure
6888                 # QEMU to use the maximum bandwidth.
6889                 guest.migrate_configure_max_speed(
6890                     CONF.libvirt.live_migration_bandwidth)
6891 
6892         finish_event = eventlet.event.Event()
6893         self.active_migrations[instance.uuid] = deque()
6894 
6895         def thread_finished(thread, event):
6896             LOG.debug("Migration operation thread notification",
6897                       instance=instance)
6898             event.send()
6899         opthread.link(thread_finished, finish_event)
6900 
6901         # Let eventlet schedule the new thread right away
6902         time.sleep(0)
6903 
6904         try:
6905             LOG.debug("Starting monitoring of live migration",
6906                       instance=instance)
6907             self._live_migration_monitor(context, instance, guest, dest,
6908                                          post_method, recover_method,
6909                                          block_migration, migrate_data,
6910                                          finish_event, disk_paths)
6911         except Exception as ex:
6912             LOG.warning("Error monitoring migration: %(ex)s",
6913                         {"ex": ex}, instance=instance, exc_info=True)
6914             raise
6915         finally:
6916             LOG.debug("Live migration monitoring is all done",
6917                       instance=instance)
6918 
6919     def _is_post_copy_enabled(self, migration_flags):
6920         if self._is_post_copy_available():
6921             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
6922                 return True
6923         return False
6924 
6925     def live_migration_force_complete(self, instance):
6926         try:
6927             self.active_migrations[instance.uuid].append('force-complete')
6928         except KeyError:
6929             raise exception.NoActiveMigrationForInstance(
6930                 instance_id=instance.uuid)
6931 
6932     def _try_fetch_image(self, context, path, image_id, instance,
6933                          fallback_from_host=None):
6934         try:
6935             libvirt_utils.fetch_image(context, path, image_id)
6936         except exception.ImageNotFound:
6937             if not fallback_from_host:
6938                 raise
6939             LOG.debug("Image %(image_id)s doesn't exist anymore on "
6940                       "image service, attempting to copy image "
6941                       "from %(host)s",
6942                       {'image_id': image_id, 'host': fallback_from_host})
6943             libvirt_utils.copy_image(src=path, dest=path,
6944                                      host=fallback_from_host,
6945                                      receive=True)
6946 
6947     def _fetch_instance_kernel_ramdisk(self, context, instance,
6948                                        fallback_from_host=None):
6949         """Download kernel and ramdisk for instance in instance directory."""
6950         instance_dir = libvirt_utils.get_instance_path(instance)
6951         if instance.kernel_id:
6952             kernel_path = os.path.join(instance_dir, 'kernel')
6953             # NOTE(dsanders): only fetch image if it's not available at
6954             # kernel_path. This also avoids ImageNotFound exception if
6955             # the image has been deleted from glance
6956             if not os.path.exists(kernel_path):
6957                 self._try_fetch_image(context,
6958                                       kernel_path,
6959                                       instance.kernel_id,
6960                                       instance, fallback_from_host)
6961             if instance.ramdisk_id:
6962                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
6963                 # NOTE(dsanders): only fetch image if it's not available at
6964                 # ramdisk_path. This also avoids ImageNotFound exception if
6965                 # the image has been deleted from glance
6966                 if not os.path.exists(ramdisk_path):
6967                     self._try_fetch_image(context,
6968                                           ramdisk_path,
6969                                           instance.ramdisk_id,
6970                                           instance, fallback_from_host)
6971 
6972     def rollback_live_migration_at_destination(self, context, instance,
6973                                                network_info,
6974                                                block_device_info,
6975                                                destroy_disks=True,
6976                                                migrate_data=None):
6977         """Clean up destination node after a failed live migration."""
6978         try:
6979             self.destroy(context, instance, network_info, block_device_info,
6980                          destroy_disks)
6981         finally:
6982             # NOTE(gcb): Failed block live migration may leave instance
6983             # directory at destination node, ensure it is always deleted.
6984             is_shared_instance_path = True
6985             if migrate_data:
6986                 is_shared_instance_path = migrate_data.is_shared_instance_path
6987                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
6988                     and migrate_data.serial_listen_ports):
6989                     # Releases serial ports reserved.
6990                     for port in migrate_data.serial_listen_ports:
6991                         serial_console.release_port(
6992                             host=migrate_data.serial_listen_addr, port=port)
6993 
6994             if not is_shared_instance_path:
6995                 instance_dir = libvirt_utils.get_instance_path_at_destination(
6996                     instance, migrate_data)
6997                 if os.path.exists(instance_dir):
6998                     shutil.rmtree(instance_dir)
6999 
7000     def pre_live_migration(self, context, instance, block_device_info,
7001                            network_info, disk_info, migrate_data):
7002         """Preparation live migration."""
7003         if disk_info is not None:
7004             disk_info = jsonutils.loads(disk_info)
7005 
7006         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
7007                   instance=instance)
7008         is_shared_block_storage = migrate_data.is_shared_block_storage
7009         is_shared_instance_path = migrate_data.is_shared_instance_path
7010         is_block_migration = migrate_data.block_migration
7011 
7012         if not is_shared_instance_path:
7013             instance_dir = libvirt_utils.get_instance_path_at_destination(
7014                             instance, migrate_data)
7015 
7016             if os.path.exists(instance_dir):
7017                 raise exception.DestinationDiskExists(path=instance_dir)
7018 
7019             LOG.debug('Creating instance directory: %s', instance_dir,
7020                       instance=instance)
7021             os.mkdir(instance_dir)
7022 
7023             # Recreate the disk.info file and in doing so stop the
7024             # imagebackend from recreating it incorrectly by inspecting the
7025             # contents of each file when using the Raw backend.
7026             if disk_info:
7027                 image_disk_info = {}
7028                 for info in disk_info:
7029                     image_file = os.path.basename(info['path'])
7030                     image_path = os.path.join(instance_dir, image_file)
7031                     image_disk_info[image_path] = info['type']
7032 
7033                 LOG.debug('Creating disk.info with the contents: %s',
7034                           image_disk_info, instance=instance)
7035 
7036                 image_disk_info_path = os.path.join(instance_dir,
7037                                                     'disk.info')
7038                 libvirt_utils.write_to_file(image_disk_info_path,
7039                                             jsonutils.dumps(image_disk_info))
7040 
7041             if not is_shared_block_storage:
7042                 # Ensure images and backing files are present.
7043                 LOG.debug('Checking to make sure images and backing files are '
7044                           'present before live migration.', instance=instance)
7045                 self._create_images_and_backing(
7046                     context, instance, instance_dir, disk_info,
7047                     fallback_from_host=instance.host)
7048                 if (configdrive.required_by(instance) and
7049                         CONF.config_drive_format == 'iso9660'):
7050                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
7051                     # drive needs to be copied to destination prior to
7052                     # migration when instance path is not shared and block
7053                     # storage is not shared. Files that are already present
7054                     # on destination are excluded from a list of files that
7055                     # need to be copied to destination. If we don't do that
7056                     # live migration will fail on copying iso config drive to
7057                     # destination and writing to read-only device.
7058                     # Please see bug/1246201 for more details.
7059                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
7060                     self._remotefs.copy_file(src, instance_dir)
7061 
7062             if not is_block_migration:
7063                 # NOTE(angdraug): when block storage is shared between source
7064                 # and destination and instance path isn't (e.g. volume backed
7065                 # or rbd backed instance), instance path on destination has to
7066                 # be prepared
7067 
7068                 # Required by Quobyte CI
7069                 self._ensure_console_log_for_instance(instance)
7070 
7071                 # if image has kernel and ramdisk, just download
7072                 # following normal way.
7073                 self._fetch_instance_kernel_ramdisk(context, instance)
7074 
7075         # Establishing connection to volume server.
7076         block_device_mapping = driver.block_device_info_get_mapping(
7077             block_device_info)
7078 
7079         if len(block_device_mapping):
7080             LOG.debug('Connecting volumes before live migration.',
7081                       instance=instance)
7082 
7083         for bdm in block_device_mapping:
7084             connection_info = bdm['connection_info']
7085             self._connect_volume(connection_info, instance)
7086 
7087         # We call plug_vifs before the compute manager calls
7088         # ensure_filtering_rules_for_instance, to ensure bridge is set up
7089         # Retry operation is necessary because continuously request comes,
7090         # concurrent request occurs to iptables, then it complains.
7091         LOG.debug('Plugging VIFs before live migration.', instance=instance)
7092         max_retry = CONF.live_migration_retry_count
7093         for cnt in range(max_retry):
7094             try:
7095                 self.plug_vifs(instance, network_info)
7096                 break
7097             except processutils.ProcessExecutionError:
7098                 if cnt == max_retry - 1:
7099                     raise
7100                 else:
7101                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
7102                                 '%(max_retry)d.',
7103                                 {'cnt': cnt, 'max_retry': max_retry},
7104                                 instance=instance)
7105                     greenthread.sleep(1)
7106 
7107         # Store server_listen and latest disk device info
7108         if not migrate_data:
7109             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
7110         else:
7111             migrate_data.bdms = []
7112         # Store live_migration_inbound_addr
7113         migrate_data.target_connect_addr = \
7114             CONF.libvirt.live_migration_inbound_addr
7115         migrate_data.supported_perf_events = self._supported_perf_events
7116 
7117         migrate_data.serial_listen_ports = []
7118         if CONF.serial_console.enabled:
7119             num_ports = hardware.get_number_of_serial_ports(
7120                 instance.flavor, instance.image_meta)
7121             for port in six.moves.range(num_ports):
7122                 migrate_data.serial_listen_ports.append(
7123                     serial_console.acquire_port(
7124                         migrate_data.serial_listen_addr))
7125 
7126         for vol in block_device_mapping:
7127             connection_info = vol['connection_info']
7128             if connection_info.get('serial'):
7129                 disk_info = blockinfo.get_info_from_bdm(
7130                     instance, CONF.libvirt.virt_type,
7131                     instance.image_meta, vol)
7132 
7133                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
7134                 bdmi.serial = connection_info['serial']
7135                 bdmi.connection_info = connection_info
7136                 bdmi.bus = disk_info['bus']
7137                 bdmi.dev = disk_info['dev']
7138                 bdmi.type = disk_info['type']
7139                 bdmi.format = disk_info.get('format')
7140                 bdmi.boot_index = disk_info.get('boot_index')
7141                 migrate_data.bdms.append(bdmi)
7142 
7143         return migrate_data
7144 
7145     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
7146                                image_id, instance, size,
7147                                fallback_from_host=None):
7148         try:
7149             image.cache(fetch_func=fetch_func,
7150                         context=context,
7151                         filename=filename,
7152                         image_id=image_id,
7153                         size=size)
7154         except exception.ImageNotFound:
7155             if not fallback_from_host:
7156                 raise
7157             LOG.debug("Image %(image_id)s doesn't exist anymore "
7158                       "on image service, attempting to copy "
7159                       "image from %(host)s",
7160                       {'image_id': image_id, 'host': fallback_from_host},
7161                       instance=instance)
7162 
7163             def copy_from_host(target):
7164                 libvirt_utils.copy_image(src=target,
7165                                          dest=target,
7166                                          host=fallback_from_host,
7167                                          receive=True)
7168             image.cache(fetch_func=copy_from_host,
7169                         filename=filename)
7170 
7171     def _create_images_and_backing(self, context, instance, instance_dir,
7172                                    disk_info, fallback_from_host=None):
7173         """:param context: security context
7174            :param instance:
7175                nova.db.sqlalchemy.models.Instance object
7176                instance object that is migrated.
7177            :param instance_dir:
7178                instance path to use, calculated externally to handle block
7179                migrating an instance with an old style instance path
7180            :param disk_info:
7181                disk info specified in _get_instance_disk_info_from_config
7182                (list of dicts)
7183            :param fallback_from_host:
7184                host where we can retrieve images if the glance images are
7185                not available.
7186 
7187         """
7188 
7189         # Virtuozzo containers don't use backing file
7190         if (CONF.libvirt.virt_type == "parallels" and
7191                 instance.vm_mode == fields.VMMode.EXE):
7192             return
7193 
7194         if not disk_info:
7195             disk_info = []
7196 
7197         for info in disk_info:
7198             base = os.path.basename(info['path'])
7199             # Get image type and create empty disk image, and
7200             # create backing file in case of qcow2.
7201             instance_disk = os.path.join(instance_dir, base)
7202             if not info['backing_file'] and not os.path.exists(instance_disk):
7203                 libvirt_utils.create_image(info['type'], instance_disk,
7204                                            info['virt_disk_size'])
7205             elif info['backing_file']:
7206                 # Creating backing file follows same way as spawning instances.
7207                 cache_name = os.path.basename(info['backing_file'])
7208 
7209                 disk = self.image_backend.by_name(instance, instance_disk,
7210                                                   CONF.libvirt.images_type)
7211                 if cache_name.startswith('ephemeral'):
7212                     # The argument 'size' is used by image.cache to
7213                     # validate disk size retrieved from cache against
7214                     # the instance disk size (should always return OK)
7215                     # and ephemeral_size is used by _create_ephemeral
7216                     # to build the image if the disk is not already
7217                     # cached.
7218                     disk.cache(
7219                         fetch_func=self._create_ephemeral,
7220                         fs_label=cache_name,
7221                         os_type=instance.os_type,
7222                         filename=cache_name,
7223                         size=info['virt_disk_size'],
7224                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7225                 elif cache_name.startswith('swap'):
7226                     inst_type = instance.get_flavor()
7227                     swap_mb = inst_type.swap
7228                     disk.cache(fetch_func=self._create_swap,
7229                                 filename="swap_%s" % swap_mb,
7230                                 size=swap_mb * units.Mi,
7231                                 swap_mb=swap_mb)
7232                 else:
7233                     self._try_fetch_image_cache(disk,
7234                                                 libvirt_utils.fetch_image,
7235                                                 context, cache_name,
7236                                                 instance.image_ref,
7237                                                 instance,
7238                                                 info['virt_disk_size'],
7239                                                 fallback_from_host)
7240 
7241         # if disk has kernel and ramdisk, just download
7242         # following normal way.
7243         self._fetch_instance_kernel_ramdisk(
7244             context, instance, fallback_from_host=fallback_from_host)
7245 
7246     def post_live_migration(self, context, instance, block_device_info,
7247                             migrate_data=None):
7248         # Disconnect from volume server
7249         block_device_mapping = driver.block_device_info_get_mapping(
7250                 block_device_info)
7251         volume_api = self._volume_api
7252         for vol in block_device_mapping:
7253             volume_id = vol['connection_info']['serial']
7254             if vol['attachment_id'] is None:
7255                 # Cinder v2 api flow: Retrieve connection info from Cinder's
7256                 # initialize_connection API. The info returned will be
7257                 # accurate for the source server.
7258                 connector = self.get_volume_connector(instance)
7259                 connection_info = volume_api.initialize_connection(
7260                     context, volume_id, connector)
7261             else:
7262                 # cinder v3.44 api flow: Retrieve the connection_info for
7263                 # the old attachment from cinder.
7264                 old_attachment_id = \
7265                     migrate_data.old_vol_attachment_ids[volume_id]
7266                 old_attachment = volume_api.attachment_get(
7267                     context, old_attachment_id)
7268                 connection_info = old_attachment['connection_info']
7269 
7270             # TODO(leeantho) The following multipath_id logic is temporary
7271             # and will be removed in the future once os-brick is updated
7272             # to handle multipath for drivers in a more efficient way.
7273             # For now this logic is needed to ensure the connection info
7274             # data is correct.
7275 
7276             # Pull out multipath_id from the bdm information. The
7277             # multipath_id can be placed into the connection info
7278             # because it is based off of the volume and will be the
7279             # same on the source and destination hosts.
7280             if 'multipath_id' in vol['connection_info']['data']:
7281                 multipath_id = vol['connection_info']['data']['multipath_id']
7282                 connection_info['data']['multipath_id'] = multipath_id
7283 
7284             self._disconnect_volume(connection_info, instance)
7285 
7286     def post_live_migration_at_source(self, context, instance, network_info):
7287         """Unplug VIFs from networks at source.
7288 
7289         :param context: security context
7290         :param instance: instance object reference
7291         :param network_info: instance network information
7292         """
7293         self.unplug_vifs(instance, network_info)
7294 
7295     def post_live_migration_at_destination(self, context,
7296                                            instance,
7297                                            network_info,
7298                                            block_migration=False,
7299                                            block_device_info=None):
7300         """Post operation of live migration at destination host.
7301 
7302         :param context: security context
7303         :param instance:
7304             nova.db.sqlalchemy.models.Instance object
7305             instance object that is migrated.
7306         :param network_info: instance network information
7307         :param block_migration: if true, post operation of block_migration.
7308         """
7309         # The source node set the VIR_MIGRATE_PERSIST_DEST flag when live
7310         # migrating so the guest xml should already be persisted on the
7311         # destination host, so just perform a sanity check to make sure it
7312         # made it as expected.
7313         self._host.get_guest(instance)
7314 
7315     def _get_instance_disk_info_from_config(self, guest_config,
7316                                             block_device_info):
7317         """Get the non-volume disk information from the domain xml
7318 
7319         :param LibvirtConfigGuest guest_config: the libvirt domain config
7320                                                 for the instance
7321         :param dict block_device_info: block device info for BDMs
7322         :returns disk_info: list of dicts with keys:
7323 
7324           * 'type': the disk type (str)
7325           * 'path': the disk path (str)
7326           * 'virt_disk_size': the virtual disk size (int)
7327           * 'backing_file': backing file of a disk image (str)
7328           * 'disk_size': physical disk size (int)
7329           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7330         """
7331         block_device_mapping = driver.block_device_info_get_mapping(
7332             block_device_info)
7333 
7334         volume_devices = set()
7335         for vol in block_device_mapping:
7336             disk_dev = vol['mount_device'].rpartition("/")[2]
7337             volume_devices.add(disk_dev)
7338 
7339         disk_info = []
7340 
7341         if (guest_config.virt_type == 'parallels' and
7342                 guest_config.os_type == fields.VMMode.EXE):
7343             node_type = 'filesystem'
7344         else:
7345             node_type = 'disk'
7346 
7347         for device in guest_config.devices:
7348             if device.root_name != node_type:
7349                 continue
7350             disk_type = device.source_type
7351             if device.root_name == 'filesystem':
7352                 target = device.target_dir
7353                 if device.source_type == 'file':
7354                     path = device.source_file
7355                 elif device.source_type == 'block':
7356                     path = device.source_dev
7357                 else:
7358                     path = None
7359             else:
7360                 target = device.target_dev
7361                 path = device.source_path
7362 
7363             if not path:
7364                 LOG.debug('skipping disk for %s as it does not have a path',
7365                           guest_config.name)
7366                 continue
7367 
7368             if disk_type not in ['file', 'block']:
7369                 LOG.debug('skipping disk because it looks like a volume', path)
7370                 continue
7371 
7372             if target in volume_devices:
7373                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
7374                           'volume', {'path': path, 'target': target})
7375                 continue
7376 
7377             if device.root_name == 'filesystem':
7378                 driver_type = device.driver_type
7379             else:
7380                 driver_type = device.driver_format
7381             # get the real disk size or
7382             # raise a localized error if image is unavailable
7383             if disk_type == 'file':
7384                 if driver_type == 'ploop':
7385                     dk_size = 0
7386                     for dirpath, dirnames, filenames in os.walk(path):
7387                         for f in filenames:
7388                             fp = os.path.join(dirpath, f)
7389                             dk_size += os.path.getsize(fp)
7390                 else:
7391                     dk_size = int(os.path.getsize(path))
7392             elif disk_type == 'block' and block_device_info:
7393                 dk_size = lvm.get_volume_size(path)
7394             else:
7395                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7396                           'determine if volume',
7397                           {'path': path, 'target': target})
7398                 continue
7399 
7400             if driver_type in ("qcow2", "ploop"):
7401                 backing_file = libvirt_utils.get_disk_backing_file(path)
7402                 virt_size = disk_api.get_disk_size(path)
7403                 over_commit_size = int(virt_size) - dk_size
7404             else:
7405                 backing_file = ""
7406                 virt_size = dk_size
7407                 over_commit_size = 0
7408 
7409             disk_info.append({'type': driver_type,
7410                               'path': path,
7411                               'virt_disk_size': virt_size,
7412                               'backing_file': backing_file,
7413                               'disk_size': dk_size,
7414                               'over_committed_disk_size': over_commit_size})
7415         return disk_info
7416 
7417     def _get_instance_disk_info(self, instance, block_device_info):
7418         try:
7419             guest = self._host.get_guest(instance)
7420             config = guest.get_config()
7421         except libvirt.libvirtError as ex:
7422             error_code = ex.get_error_code()
7423             LOG.warning('Error from libvirt while getting description of '
7424                         '%(instance_name)s: [Error Code %(error_code)s] '
7425                         '%(ex)s',
7426                         {'instance_name': instance.name,
7427                          'error_code': error_code,
7428                          'ex': encodeutils.exception_to_unicode(ex)},
7429                         instance=instance)
7430             raise exception.InstanceNotFound(instance_id=instance.uuid)
7431 
7432         return self._get_instance_disk_info_from_config(config,
7433                                                         block_device_info)
7434 
7435     def get_instance_disk_info(self, instance,
7436                                block_device_info=None):
7437         return jsonutils.dumps(
7438             self._get_instance_disk_info(instance, block_device_info))
7439 
7440     def _get_disk_over_committed_size_total(self):
7441         """Return total over committed disk size for all instances."""
7442         # Disk size that all instance uses : virtual_size - disk_size
7443         disk_over_committed_size = 0
7444         instance_domains = self._host.list_instance_domains(only_running=False)
7445         if not instance_domains:
7446             return disk_over_committed_size
7447 
7448         # Get all instance uuids
7449         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7450         ctx = nova_context.get_admin_context()
7451         # Get instance object list by uuid filter
7452         filters = {'uuid': instance_uuids}
7453         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7454         # getting called twice one is here and another in the
7455         # _update_available_resource method of resource_tracker. Since
7456         # _update_available_resource method is synchronized, there is a
7457         # possibility the instances list retrieved here to calculate
7458         # disk_over_committed_size would differ to the list you would get
7459         # in _update_available_resource method for calculating usages based
7460         # on instance utilization.
7461         local_instance_list = objects.InstanceList.get_by_filters(
7462             ctx, filters, use_slave=True)
7463         # Convert instance list to dictionary with instance uuid as key.
7464         local_instances = {inst.uuid: inst for inst in local_instance_list}
7465 
7466         # Get bdms by instance uuids
7467         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7468             ctx, instance_uuids)
7469 
7470         for dom in instance_domains:
7471             try:
7472                 guest = libvirt_guest.Guest(dom)
7473                 config = guest.get_config()
7474 
7475                 block_device_info = None
7476                 if guest.uuid in local_instances \
7477                         and (bdms and guest.uuid in bdms):
7478                     # Get block device info for instance
7479                     block_device_info = driver.get_block_device_info(
7480                         local_instances[guest.uuid], bdms[guest.uuid])
7481 
7482                 disk_infos = self._get_instance_disk_info_from_config(
7483                     config, block_device_info)
7484                 if not disk_infos:
7485                     continue
7486 
7487                 for info in disk_infos:
7488                     disk_over_committed_size += int(
7489                         info['over_committed_disk_size'])
7490             except libvirt.libvirtError as ex:
7491                 error_code = ex.get_error_code()
7492                 LOG.warning(
7493                     'Error from libvirt while getting description of '
7494                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
7495                     {'instance_name': guest.name,
7496                      'error_code': error_code,
7497                      'ex': encodeutils.exception_to_unicode(ex)})
7498             except OSError as e:
7499                 if e.errno in (errno.ENOENT, errno.ESTALE):
7500                     LOG.warning('Periodic task is updating the host stat, '
7501                                 'it is trying to get disk %(i_name)s, '
7502                                 'but disk file was removed by concurrent '
7503                                 'operations such as resize.',
7504                                 {'i_name': guest.name})
7505                 elif e.errno == errno.EACCES:
7506                     LOG.warning('Periodic task is updating the host stat, '
7507                                 'it is trying to get disk %(i_name)s, '
7508                                 'but access is denied. It is most likely '
7509                                 'due to a VM that exists on the compute '
7510                                 'node but is not managed by Nova.',
7511                                 {'i_name': guest.name})
7512                 else:
7513                     raise
7514             except exception.VolumeBDMPathNotFound as e:
7515                 LOG.warning('Periodic task is updating the host stats, '
7516                             'it is trying to get disk info for %(i_name)s, '
7517                             'but the backing volume block device was removed '
7518                             'by concurrent operations such as resize. '
7519                             'Error: %(error)s',
7520                             {'i_name': guest.name, 'error': e})
7521             # NOTE(gtt116): give other tasks a chance.
7522             greenthread.sleep(0)
7523         return disk_over_committed_size
7524 
7525     def unfilter_instance(self, instance, network_info):
7526         """See comments of same method in firewall_driver."""
7527         self.firewall_driver.unfilter_instance(instance,
7528                                                network_info=network_info)
7529 
7530     def get_available_nodes(self, refresh=False):
7531         return [self._host.get_hostname()]
7532 
7533     def get_host_cpu_stats(self):
7534         """Return the current CPU state of the host."""
7535         return self._host.get_cpu_stats()
7536 
7537     def get_host_uptime(self):
7538         """Returns the result of calling "uptime"."""
7539         out, err = utils.execute('env', 'LANG=C', 'uptime')
7540         return out
7541 
7542     def manage_image_cache(self, context, all_instances):
7543         """Manage the local cache of images."""
7544         self.image_cache_manager.update(context, all_instances)
7545 
7546     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
7547                                   shared_storage=False):
7548         """Used only for cleanup in case migrate_disk_and_power_off fails."""
7549         try:
7550             if os.path.exists(inst_base_resize):
7551                 utils.execute('rm', '-rf', inst_base)
7552                 utils.execute('mv', inst_base_resize, inst_base)
7553                 if not shared_storage:
7554                     self._remotefs.remove_dir(dest, inst_base)
7555         except Exception:
7556             pass
7557 
7558     def _is_storage_shared_with(self, dest, inst_base):
7559         # NOTE (rmk): There are two methods of determining whether we are
7560         #             on the same filesystem: the source and dest IP are the
7561         #             same, or we create a file on the dest system via SSH
7562         #             and check whether the source system can also see it.
7563         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
7564         #                it will always be shared storage
7565         if CONF.libvirt.images_type == 'rbd':
7566             return True
7567         shared_storage = (dest == self.get_host_ip_addr())
7568         if not shared_storage:
7569             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
7570             tmp_path = os.path.join(inst_base, tmp_file)
7571 
7572             try:
7573                 self._remotefs.create_file(dest, tmp_path)
7574                 if os.path.exists(tmp_path):
7575                     shared_storage = True
7576                     os.unlink(tmp_path)
7577                 else:
7578                     self._remotefs.remove_file(dest, tmp_path)
7579             except Exception:
7580                 pass
7581         return shared_storage
7582 
7583     def migrate_disk_and_power_off(self, context, instance, dest,
7584                                    flavor, network_info,
7585                                    block_device_info=None,
7586                                    timeout=0, retry_interval=0):
7587         LOG.debug("Starting migrate_disk_and_power_off",
7588                    instance=instance)
7589 
7590         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
7591 
7592         # get_bdm_ephemeral_disk_size() will return 0 if the new
7593         # instance's requested block device mapping contain no
7594         # ephemeral devices. However, we still want to check if
7595         # the original instance's ephemeral_gb property was set and
7596         # ensure that the new requested flavor ephemeral size is greater
7597         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
7598                     instance.flavor.ephemeral_gb)
7599 
7600         # Checks if the migration needs a disk resize down.
7601         root_down = flavor.root_gb < instance.flavor.root_gb
7602         ephemeral_down = flavor.ephemeral_gb < eph_size
7603         booted_from_volume = self._is_booted_from_volume(block_device_info)
7604 
7605         if (root_down and not booted_from_volume) or ephemeral_down:
7606             reason = _("Unable to resize disk down.")
7607             raise exception.InstanceFaultRollback(
7608                 exception.ResizeError(reason=reason))
7609 
7610         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
7611         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
7612             reason = _("Migration is not supported for LVM backed instances")
7613             raise exception.InstanceFaultRollback(
7614                 exception.MigrationPreCheckError(reason=reason))
7615 
7616         # copy disks to destination
7617         # rename instance dir to +_resize at first for using
7618         # shared storage for instance dir (eg. NFS).
7619         inst_base = libvirt_utils.get_instance_path(instance)
7620         inst_base_resize = inst_base + "_resize"
7621         shared_storage = self._is_storage_shared_with(dest, inst_base)
7622 
7623         # try to create the directory on the remote compute node
7624         # if this fails we pass the exception up the stack so we can catch
7625         # failures here earlier
7626         if not shared_storage:
7627             try:
7628                 self._remotefs.create_dir(dest, inst_base)
7629             except processutils.ProcessExecutionError as e:
7630                 reason = _("not able to execute ssh command: %s") % e
7631                 raise exception.InstanceFaultRollback(
7632                     exception.ResizeError(reason=reason))
7633 
7634         self.power_off(instance, timeout, retry_interval)
7635 
7636         block_device_mapping = driver.block_device_info_get_mapping(
7637             block_device_info)
7638         for vol in block_device_mapping:
7639             connection_info = vol['connection_info']
7640             self._disconnect_volume(connection_info, instance)
7641 
7642         disk_info = self._get_instance_disk_info(instance, block_device_info)
7643 
7644         try:
7645             utils.execute('mv', inst_base, inst_base_resize)
7646             # if we are migrating the instance with shared storage then
7647             # create the directory.  If it is a remote node the directory
7648             # has already been created
7649             if shared_storage:
7650                 dest = None
7651                 fileutils.ensure_tree(inst_base)
7652 
7653             on_execute = lambda process: \
7654                 self.job_tracker.add_job(instance, process.pid)
7655             on_completion = lambda process: \
7656                 self.job_tracker.remove_job(instance, process.pid)
7657 
7658             for info in disk_info:
7659                 # assume inst_base == dirname(info['path'])
7660                 img_path = info['path']
7661                 fname = os.path.basename(img_path)
7662                 from_path = os.path.join(inst_base_resize, fname)
7663 
7664                 # We will not copy over the swap disk here, and rely on
7665                 # finish_migration to re-create it for us. This is ok because
7666                 # the OS is shut down, and as recreating a swap disk is very
7667                 # cheap it is more efficient than copying either locally or
7668                 # over the network. This also means we don't have to resize it.
7669                 if fname == 'disk.swap':
7670                     continue
7671 
7672                 compression = info['type'] not in NO_COMPRESSION_TYPES
7673                 libvirt_utils.copy_image(from_path, img_path, host=dest,
7674                                          on_execute=on_execute,
7675                                          on_completion=on_completion,
7676                                          compression=compression)
7677 
7678             # Ensure disk.info is written to the new path to avoid disks being
7679             # reinspected and potentially changing format.
7680             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
7681             if os.path.exists(src_disk_info_path):
7682                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
7683                 libvirt_utils.copy_image(src_disk_info_path,
7684                                          dst_disk_info_path,
7685                                          host=dest, on_execute=on_execute,
7686                                          on_completion=on_completion)
7687         except Exception:
7688             with excutils.save_and_reraise_exception():
7689                 self._cleanup_remote_migration(dest, inst_base,
7690                                                inst_base_resize,
7691                                                shared_storage)
7692 
7693         return jsonutils.dumps(disk_info)
7694 
7695     def _wait_for_running(self, instance):
7696         state = self.get_info(instance).state
7697 
7698         if state == power_state.RUNNING:
7699             LOG.info("Instance running successfully.", instance=instance)
7700             raise loopingcall.LoopingCallDone()
7701 
7702     @staticmethod
7703     def _disk_raw_to_qcow2(path):
7704         """Converts a raw disk to qcow2."""
7705         path_qcow = path + '_qcow'
7706         utils.execute('qemu-img', 'convert', '-f', 'raw',
7707                       '-O', 'qcow2', path, path_qcow)
7708         utils.execute('mv', path_qcow, path)
7709 
7710     @staticmethod
7711     def _disk_qcow2_to_raw(path):
7712         """Converts a qcow2 disk to raw."""
7713         path_raw = path + '_raw'
7714         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
7715                       '-O', 'raw', path, path_raw)
7716         utils.execute('mv', path_raw, path)
7717 
7718     def finish_migration(self, context, migration, instance, disk_info,
7719                          network_info, image_meta, resize_instance,
7720                          block_device_info=None, power_on=True):
7721         LOG.debug("Starting finish_migration", instance=instance)
7722 
7723         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7724                                                   instance,
7725                                                   image_meta,
7726                                                   block_device_info)
7727         # assume _create_image does nothing if a target file exists.
7728         # NOTE: This has the intended side-effect of fetching a missing
7729         # backing file.
7730         self._create_image(context, instance, block_disk_info['mapping'],
7731                            block_device_info=block_device_info,
7732                            ignore_bdi_for_swap=True,
7733                            fallback_from_host=migration.source_compute)
7734 
7735         # Required by Quobyte CI
7736         self._ensure_console_log_for_instance(instance)
7737 
7738         gen_confdrive = functools.partial(
7739             self._create_configdrive, context, instance,
7740             InjectionInfo(admin_pass=None, network_info=network_info,
7741                           files=None))
7742 
7743         # Convert raw disks to qcow2 if migrating to host which uses
7744         # qcow2 from host which uses raw.
7745         disk_info = jsonutils.loads(disk_info)
7746         for info in disk_info:
7747             path = info['path']
7748             disk_name = os.path.basename(path)
7749 
7750             # NOTE(mdbooth): The code below looks wrong, but is actually
7751             # required to prevent a security hole when migrating from a host
7752             # with use_cow_images=False to one with use_cow_images=True.
7753             # Imagebackend uses use_cow_images to select between the
7754             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
7755             # writes to disk.info, but does not read it as it assumes qcow2.
7756             # Therefore if we don't convert raw to qcow2 here, a raw disk will
7757             # be incorrectly assumed to be qcow2, which is a severe security
7758             # flaw. The reverse is not true, because the atrociously-named-Raw
7759             # backend supports both qcow2 and raw disks, and will choose
7760             # appropriately between them as long as disk.info exists and is
7761             # correctly populated, which it is because Qcow2 writes to
7762             # disk.info.
7763             #
7764             # In general, we do not yet support format conversion during
7765             # migration. For example:
7766             #   * Converting from use_cow_images=True to use_cow_images=False
7767             #     isn't handled. This isn't a security bug, but is almost
7768             #     certainly buggy in other cases, as the 'Raw' backend doesn't
7769             #     expect a backing file.
7770             #   * Converting to/from lvm and rbd backends is not supported.
7771             #
7772             # This behaviour is inconsistent, and therefore undesirable for
7773             # users. It is tightly-coupled to implementation quirks of 2
7774             # out of 5 backends in imagebackend and defends against a severe
7775             # security flaw which is not at all obvious without deep analysis,
7776             # and is therefore undesirable to developers. We should aim to
7777             # remove it. This will not be possible, though, until we can
7778             # represent the storage layout of a specific instance
7779             # independent of the default configuration of the local compute
7780             # host.
7781 
7782             # Config disks are hard-coded to be raw even when
7783             # use_cow_images=True (see _get_disk_config_image_type),so don't
7784             # need to be converted.
7785             if (disk_name != 'disk.config' and
7786                         info['type'] == 'raw' and CONF.use_cow_images):
7787                 self._disk_raw_to_qcow2(info['path'])
7788 
7789         xml = self._get_guest_xml(context, instance, network_info,
7790                                   block_disk_info, image_meta,
7791                                   block_device_info=block_device_info)
7792         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
7793         # or not we've migrated to another host, because we unplug VIFs locally
7794         # and the status change in the port might go undetected by the neutron
7795         # L2 agent (or neutron server) so neutron may not know that the VIF was
7796         # unplugged in the first place and never send an event.
7797         guest = self._create_domain_and_network(context, xml, instance,
7798                                         network_info,
7799                                         block_device_info=block_device_info,
7800                                         power_on=power_on,
7801                                         vifs_already_plugged=True,
7802                                         post_xml_callback=gen_confdrive)
7803         if power_on:
7804             timer = loopingcall.FixedIntervalLoopingCall(
7805                                                     self._wait_for_running,
7806                                                     instance)
7807             timer.start(interval=0.5).wait()
7808 
7809             # Sync guest time after migration.
7810             guest.sync_guest_time()
7811 
7812         LOG.debug("finish_migration finished successfully.", instance=instance)
7813 
7814     def _cleanup_failed_migration(self, inst_base):
7815         """Make sure that a failed migrate doesn't prevent us from rolling
7816         back in a revert.
7817         """
7818         try:
7819             shutil.rmtree(inst_base)
7820         except OSError as e:
7821             if e.errno != errno.ENOENT:
7822                 raise
7823 
7824     def finish_revert_migration(self, context, instance, network_info,
7825                                 block_device_info=None, power_on=True):
7826         LOG.debug("Starting finish_revert_migration",
7827                   instance=instance)
7828 
7829         inst_base = libvirt_utils.get_instance_path(instance)
7830         inst_base_resize = inst_base + "_resize"
7831 
7832         # NOTE(danms): if we're recovering from a failed migration,
7833         # make sure we don't have a left-over same-host base directory
7834         # that would conflict. Also, don't fail on the rename if the
7835         # failure happened early.
7836         if os.path.exists(inst_base_resize):
7837             self._cleanup_failed_migration(inst_base)
7838             utils.execute('mv', inst_base_resize, inst_base)
7839 
7840         root_disk = self.image_backend.by_name(instance, 'disk')
7841         # Once we rollback, the snapshot is no longer needed, so remove it
7842         # TODO(nic): Remove the try/except/finally in a future release
7843         # To avoid any upgrade issues surrounding instances being in pending
7844         # resize state when the software is updated, this portion of the
7845         # method logs exceptions rather than failing on them.  Once it can be
7846         # reasonably assumed that no such instances exist in the wild
7847         # anymore, the try/except/finally should be removed,
7848         # and ignore_errors should be set back to False (the default) so
7849         # that problems throw errors, like they should.
7850         if root_disk.exists():
7851             try:
7852                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
7853             except exception.SnapshotNotFound:
7854                 LOG.warning("Failed to rollback snapshot (%s)",
7855                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
7856             finally:
7857                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
7858                                       ignore_errors=True)
7859 
7860         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7861                                             instance,
7862                                             instance.image_meta,
7863                                             block_device_info)
7864         xml = self._get_guest_xml(context, instance, network_info, disk_info,
7865                                   instance.image_meta,
7866                                   block_device_info=block_device_info)
7867         self._create_domain_and_network(context, xml, instance, network_info,
7868                                         block_device_info=block_device_info,
7869                                         power_on=power_on,
7870                                         vifs_already_plugged=True)
7871 
7872         if power_on:
7873             timer = loopingcall.FixedIntervalLoopingCall(
7874                                                     self._wait_for_running,
7875                                                     instance)
7876             timer.start(interval=0.5).wait()
7877 
7878         LOG.debug("finish_revert_migration finished successfully.",
7879                   instance=instance)
7880 
7881     def confirm_migration(self, context, migration, instance, network_info):
7882         """Confirms a resize, destroying the source VM."""
7883         self._cleanup_resize(context, instance, network_info)
7884 
7885     @staticmethod
7886     def _get_io_devices(xml_doc):
7887         """get the list of io devices from the xml document."""
7888         result = {"volumes": [], "ifaces": []}
7889         try:
7890             doc = etree.fromstring(xml_doc)
7891         except Exception:
7892             return result
7893         blocks = [('./devices/disk', 'volumes'),
7894             ('./devices/interface', 'ifaces')]
7895         for block, key in blocks:
7896             section = doc.findall(block)
7897             for node in section:
7898                 for child in node.getchildren():
7899                     if child.tag == 'target' and child.get('dev'):
7900                         result[key].append(child.get('dev'))
7901         return result
7902 
7903     def get_diagnostics(self, instance):
7904         guest = self._host.get_guest(instance)
7905 
7906         # TODO(sahid): We are converting all calls from a
7907         # virDomain object to use nova.virt.libvirt.Guest.
7908         # We should be able to remove domain at the end.
7909         domain = guest._domain
7910         output = {}
7911         # get cpu time, might launch an exception if the method
7912         # is not supported by the underlying hypervisor being
7913         # used by libvirt
7914         try:
7915             for vcpu in guest.get_vcpus_info():
7916                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
7917         except libvirt.libvirtError:
7918             pass
7919         # get io status
7920         xml = guest.get_xml_desc()
7921         dom_io = LibvirtDriver._get_io_devices(xml)
7922         for guest_disk in dom_io["volumes"]:
7923             try:
7924                 # blockStats might launch an exception if the method
7925                 # is not supported by the underlying hypervisor being
7926                 # used by libvirt
7927                 stats = domain.blockStats(guest_disk)
7928                 output[guest_disk + "_read_req"] = stats[0]
7929                 output[guest_disk + "_read"] = stats[1]
7930                 output[guest_disk + "_write_req"] = stats[2]
7931                 output[guest_disk + "_write"] = stats[3]
7932                 output[guest_disk + "_errors"] = stats[4]
7933             except libvirt.libvirtError:
7934                 pass
7935         for interface in dom_io["ifaces"]:
7936             try:
7937                 # interfaceStats might launch an exception if the method
7938                 # is not supported by the underlying hypervisor being
7939                 # used by libvirt
7940                 stats = domain.interfaceStats(interface)
7941                 output[interface + "_rx"] = stats[0]
7942                 output[interface + "_rx_packets"] = stats[1]
7943                 output[interface + "_rx_errors"] = stats[2]
7944                 output[interface + "_rx_drop"] = stats[3]
7945                 output[interface + "_tx"] = stats[4]
7946                 output[interface + "_tx_packets"] = stats[5]
7947                 output[interface + "_tx_errors"] = stats[6]
7948                 output[interface + "_tx_drop"] = stats[7]
7949             except libvirt.libvirtError:
7950                 pass
7951         output["memory"] = domain.maxMemory()
7952         # memoryStats might launch an exception if the method
7953         # is not supported by the underlying hypervisor being
7954         # used by libvirt
7955         try:
7956             mem = domain.memoryStats()
7957             for key in mem.keys():
7958                 output["memory-" + key] = mem[key]
7959         except (libvirt.libvirtError, AttributeError):
7960             pass
7961         return output
7962 
7963     def get_instance_diagnostics(self, instance):
7964         guest = self._host.get_guest(instance)
7965 
7966         # TODO(sahid): We are converting all calls from a
7967         # virDomain object to use nova.virt.libvirt.Guest.
7968         # We should be able to remove domain at the end.
7969         domain = guest._domain
7970 
7971         xml = guest.get_xml_desc()
7972         xml_doc = etree.fromstring(xml)
7973 
7974         # TODO(sahid): Needs to use get_info but more changes have to
7975         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
7976         # needed.
7977         (state, max_mem, mem, num_cpu, cpu_time) = \
7978             guest._get_domain_info(self._host)
7979         config_drive = configdrive.required_by(instance)
7980         launched_at = timeutils.normalize_time(instance.launched_at)
7981         uptime = timeutils.delta_seconds(launched_at,
7982                                          timeutils.utcnow())
7983         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
7984                                         driver='libvirt',
7985                                         config_drive=config_drive,
7986                                         hypervisor=CONF.libvirt.virt_type,
7987                                         hypervisor_os='linux',
7988                                         uptime=uptime)
7989         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
7990             maximum=max_mem / units.Mi,
7991             used=mem / units.Mi)
7992 
7993         # get cpu time, might launch an exception if the method
7994         # is not supported by the underlying hypervisor being
7995         # used by libvirt
7996         try:
7997             for vcpu in guest.get_vcpus_info():
7998                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
7999         except libvirt.libvirtError:
8000             pass
8001         # get io status
8002         dom_io = LibvirtDriver._get_io_devices(xml)
8003         for guest_disk in dom_io["volumes"]:
8004             try:
8005                 # blockStats might launch an exception if the method
8006                 # is not supported by the underlying hypervisor being
8007                 # used by libvirt
8008                 stats = domain.blockStats(guest_disk)
8009                 diags.add_disk(read_bytes=stats[1],
8010                                read_requests=stats[0],
8011                                write_bytes=stats[3],
8012                                write_requests=stats[2],
8013                                errors_count=stats[4])
8014             except libvirt.libvirtError:
8015                 pass
8016         for interface in dom_io["ifaces"]:
8017             try:
8018                 # interfaceStats might launch an exception if the method
8019                 # is not supported by the underlying hypervisor being
8020                 # used by libvirt
8021                 stats = domain.interfaceStats(interface)
8022                 diags.add_nic(rx_octets=stats[0],
8023                               rx_errors=stats[2],
8024                               rx_drop=stats[3],
8025                               rx_packets=stats[1],
8026                               tx_octets=stats[4],
8027                               tx_errors=stats[6],
8028                               tx_drop=stats[7],
8029                               tx_packets=stats[5])
8030             except libvirt.libvirtError:
8031                 pass
8032 
8033         # Update mac addresses of interface if stats have been reported
8034         if diags.nic_details:
8035             nodes = xml_doc.findall('./devices/interface/mac')
8036             for index, node in enumerate(nodes):
8037                 diags.nic_details[index].mac_address = node.get('address')
8038         return diags
8039 
8040     @staticmethod
8041     def _prepare_device_bus(dev):
8042         """Determines the device bus and its hypervisor assigned address
8043         """
8044         bus = None
8045         address = (dev.device_addr.format_address() if
8046                    dev.device_addr else None)
8047         if isinstance(dev.device_addr,
8048                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
8049             bus = objects.PCIDeviceBus()
8050         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8051             if dev.target_bus == 'scsi':
8052                 bus = objects.SCSIDeviceBus()
8053             elif dev.target_bus == 'ide':
8054                 bus = objects.IDEDeviceBus()
8055             elif dev.target_bus == 'usb':
8056                 bus = objects.USBDeviceBus()
8057         if address is not None and bus is not None:
8058             bus.address = address
8059         return bus
8060 
8061     def _build_device_metadata(self, context, instance):
8062         """Builds a metadata object for instance devices, that maps the user
8063            provided tag to the hypervisor assigned device address.
8064         """
8065         def _get_device_name(bdm):
8066             return block_device.strip_dev(bdm.device_name)
8067 
8068         network_info = instance.info_cache.network_info
8069         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
8070         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
8071                                                                  instance.uuid)
8072         vifs_to_expose = {vif.address: vif for vif in vifs
8073                           if ('tag' in vif and vif.tag) or
8074                              vlans_by_mac.get(vif.address)}
8075         # TODO(mriedem): We should be able to avoid the DB query here by using
8076         # block_device_info['block_device_mapping'] which is passed into most
8077         # methods that call this function.
8078         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8079             context, instance.uuid)
8080         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
8081 
8082         devices = []
8083         guest = self._host.get_guest(instance)
8084         xml = guest.get_xml_desc()
8085         xml_dom = etree.fromstring(xml)
8086         guest_config = vconfig.LibvirtConfigGuest()
8087         guest_config.parse_dom(xml_dom)
8088 
8089         for dev in guest_config.devices:
8090             # Build network interfaces related metadata
8091             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
8092                 vif = vifs_to_expose.get(dev.mac_addr)
8093                 if not vif:
8094                     continue
8095                 bus = self._prepare_device_bus(dev)
8096                 device = objects.NetworkInterfaceMetadata(mac=vif.address)
8097                 if 'tag' in vif and vif.tag:
8098                     device.tags = [vif.tag]
8099                 if bus:
8100                     device.bus = bus
8101                 vlan = vlans_by_mac.get(vif.address)
8102                 if vlan:
8103                     device.vlan = int(vlan)
8104                 devices.append(device)
8105 
8106             # Build disks related metadata
8107             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
8108                 bdm = tagged_bdms.get(dev.target_dev)
8109                 if not bdm:
8110                     continue
8111                 bus = self._prepare_device_bus(dev)
8112                 device = objects.DiskMetadata(tags=[bdm.tag])
8113                 # NOTE(artom) Setting the serial (which corresponds to
8114                 # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
8115                 # find the disks's BlockDeviceMapping object when we detach the
8116                 # volume and want to clean up its metadata.
8117                 device.serial = bdm.volume_id
8118                 if bus:
8119                     device.bus = bus
8120                 devices.append(device)
8121         if devices:
8122             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
8123             return dev_meta
8124 
8125     def instance_on_disk(self, instance):
8126         # ensure directories exist and are writable
8127         instance_path = libvirt_utils.get_instance_path(instance)
8128         LOG.debug('Checking instance files accessibility %s', instance_path,
8129                   instance=instance)
8130         shared_instance_path = os.access(instance_path, os.W_OK)
8131         # NOTE(flwang): For shared block storage scenario, the file system is
8132         # not really shared by the two hosts, but the volume of evacuated
8133         # instance is reachable.
8134         shared_block_storage = (self.image_backend.backend().
8135                                 is_shared_block_storage())
8136         return shared_instance_path or shared_block_storage
8137 
8138     def inject_network_info(self, instance, nw_info):
8139         self.firewall_driver.setup_basic_filtering(instance, nw_info)
8140 
8141     def delete_instance_files(self, instance):
8142         target = libvirt_utils.get_instance_path(instance)
8143         # A resize may be in progress
8144         target_resize = target + '_resize'
8145         # Other threads may attempt to rename the path, so renaming the path
8146         # to target + '_del' (because it is atomic) and iterating through
8147         # twice in the unlikely event that a concurrent rename occurs between
8148         # the two rename attempts in this method. In general this method
8149         # should be fairly thread-safe without these additional checks, since
8150         # other operations involving renames are not permitted when the task
8151         # state is not None and the task state should be set to something
8152         # other than None by the time this method is invoked.
8153         target_del = target + '_del'
8154         for i in range(2):
8155             try:
8156                 utils.execute('mv', target, target_del)
8157                 break
8158             except Exception:
8159                 pass
8160             try:
8161                 utils.execute('mv', target_resize, target_del)
8162                 break
8163             except Exception:
8164                 pass
8165         # Either the target or target_resize path may still exist if all
8166         # rename attempts failed.
8167         remaining_path = None
8168         for p in (target, target_resize):
8169             if os.path.exists(p):
8170                 remaining_path = p
8171                 break
8172 
8173         # A previous delete attempt may have been interrupted, so target_del
8174         # may exist even if all rename attempts during the present method
8175         # invocation failed due to the absence of both target and
8176         # target_resize.
8177         if not remaining_path and os.path.exists(target_del):
8178             self.job_tracker.terminate_jobs(instance)
8179 
8180             LOG.info('Deleting instance files %s', target_del,
8181                      instance=instance)
8182             remaining_path = target_del
8183             try:
8184                 shutil.rmtree(target_del)
8185             except OSError as e:
8186                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8187                           {'target': target_del, 'e': e}, instance=instance)
8188 
8189         # It is possible that the delete failed, if so don't mark the instance
8190         # as cleaned.
8191         if remaining_path and os.path.exists(remaining_path):
8192             LOG.info('Deletion of %s failed', remaining_path,
8193                      instance=instance)
8194             return False
8195 
8196         LOG.info('Deletion of %s complete', target_del, instance=instance)
8197         return True
8198 
8199     @property
8200     def need_legacy_block_device_info(self):
8201         return False
8202 
8203     def default_root_device_name(self, instance, image_meta, root_bdm):
8204         disk_bus = blockinfo.get_disk_bus_for_device_type(
8205             instance, CONF.libvirt.virt_type, image_meta, "disk")
8206         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8207             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8208         root_info = blockinfo.get_root_info(
8209             instance, CONF.libvirt.virt_type, image_meta,
8210             root_bdm, disk_bus, cdrom_bus)
8211         return block_device.prepend_dev(root_info['dev'])
8212 
8213     def default_device_names_for_instance(self, instance, root_device_name,
8214                                           *block_device_lists):
8215         block_device_mapping = list(itertools.chain(*block_device_lists))
8216         # NOTE(ndipanov): Null out the device names so that blockinfo code
8217         #                 will assign them
8218         for bdm in block_device_mapping:
8219             if bdm.device_name is not None:
8220                 LOG.warning(
8221                     "Ignoring supplied device name: %(device_name)s. "
8222                     "Libvirt can't honour user-supplied dev names",
8223                     {'device_name': bdm.device_name}, instance=instance)
8224                 bdm.device_name = None
8225         block_device_info = driver.get_block_device_info(instance,
8226                                                          block_device_mapping)
8227 
8228         blockinfo.default_device_names(CONF.libvirt.virt_type,
8229                                        nova_context.get_admin_context(),
8230                                        instance,
8231                                        block_device_info,
8232                                        instance.image_meta)
8233 
8234     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8235         block_device_info = driver.get_block_device_info(instance, bdms)
8236         instance_info = blockinfo.get_disk_info(
8237                 CONF.libvirt.virt_type, instance,
8238                 instance.image_meta, block_device_info=block_device_info)
8239 
8240         suggested_dev_name = block_device_obj.device_name
8241         if suggested_dev_name is not None:
8242             LOG.warning(
8243                 'Ignoring supplied device name: %(suggested_dev)s',
8244                 {'suggested_dev': suggested_dev_name}, instance=instance)
8245 
8246         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8247         #                 only when it's actually not set on the bd object
8248         block_device_obj.device_name = None
8249         disk_info = blockinfo.get_info_from_bdm(
8250             instance, CONF.libvirt.virt_type, instance.image_meta,
8251             block_device_obj, mapping=instance_info['mapping'])
8252         return block_device.prepend_dev(disk_info['dev'])
8253 
8254     def is_supported_fs_format(self, fs_type):
8255         return fs_type in [disk_api.FS_FORMAT_EXT2, disk_api.FS_FORMAT_EXT3,
8256                            disk_api.FS_FORMAT_EXT4, disk_api.FS_FORMAT_XFS]
