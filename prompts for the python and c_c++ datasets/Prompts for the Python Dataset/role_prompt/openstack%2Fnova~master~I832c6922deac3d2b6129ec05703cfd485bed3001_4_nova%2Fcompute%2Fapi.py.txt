I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import base64
23 import collections
24 import copy
25 import functools
26 import re
27 import string
28 
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import jsonutils
32 from oslo_utils import excutils
33 from oslo_utils import strutils
34 from oslo_utils import timeutils
35 from oslo_utils import units
36 from oslo_utils import uuidutils
37 import six
38 from six.moves import range
39 
40 from nova import availability_zones
41 from nova import block_device
42 from nova.cells import opts as cells_opts
43 from nova.compute import flavors
44 from nova.compute import instance_actions
45 from nova.compute import power_state
46 from nova.compute import rpcapi as compute_rpcapi
47 from nova.compute import task_states
48 from nova.compute import utils as compute_utils
49 from nova.compute import vm_states
50 from nova import conductor
51 import nova.conf
52 from nova.consoleauth import rpcapi as consoleauth_rpcapi
53 from nova import context as nova_context
54 from nova import crypto
55 from nova.db import base
56 from nova import exception
57 from nova import exception_wrapper
58 from nova import hooks
59 from nova.i18n import _
60 from nova.i18n import _LE
61 from nova.i18n import _LI
62 from nova.i18n import _LW
63 from nova import image
64 from nova import keymgr
65 from nova import network
66 from nova.network import model as network_model
67 from nova.network.security_group import openstack_driver
68 from nova.network.security_group import security_group_base
69 from nova import notifications
70 from nova import objects
71 from nova.objects import base as obj_base
72 from nova.objects import block_device as block_device_obj
73 from nova.objects import fields as fields_obj
74 from nova.objects import keypair as keypair_obj
75 from nova.objects import quotas as quotas_obj
76 from nova.pci import request as pci_request
77 import nova.policy
78 from nova import rpc
79 from nova.scheduler import client as scheduler_client
80 from nova.scheduler import utils as scheduler_utils
81 from nova import servicegroup
82 from nova import utils
83 from nova.virt import hardware
84 from nova.volume import cinder
85 
86 LOG = logging.getLogger(__name__)
87 
88 get_notifier = functools.partial(rpc.get_notifier, service='compute')
89 # NOTE(gibi): legacy notification used compute as a service but these
90 # calls still run on the client side of the compute service which is
91 # nova-api. By setting the binary to nova-api below, we can make sure
92 # that the new versioned notifications has the right publisher_id but the
93 # legacy notifications does not change.
94 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
95                                    get_notifier=get_notifier,
96                                    binary='nova-api')
97 CONF = nova.conf.CONF
98 
99 MAX_USERDATA_SIZE = 65535
100 RO_SECURITY_GROUPS = ['default']
101 VIDEO_RAM = 'hw_video:ram_max_mb'
102 
103 AGGREGATE_ACTION_UPDATE = 'Update'
104 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
105 AGGREGATE_ACTION_DELETE = 'Delete'
106 AGGREGATE_ACTION_ADD = 'Add'
107 
108 
109 def check_instance_state(vm_state=None, task_state=(None,),
110                          must_have_launched=True):
111     """Decorator to check VM and/or task state before entry to API functions.
112 
113     If the instance is in the wrong state, or has not been successfully
114     started at least once the wrapper will raise an exception.
115     """
116 
117     if vm_state is not None and not isinstance(vm_state, set):
118         vm_state = set(vm_state)
119     if task_state is not None and not isinstance(task_state, set):
120         task_state = set(task_state)
121 
122     def outer(f):
123         @six.wraps(f)
124         def inner(self, context, instance, *args, **kw):
125             if vm_state is not None and instance.vm_state not in vm_state:
126                 raise exception.InstanceInvalidState(
127                     attr='vm_state',
128                     instance_uuid=instance.uuid,
129                     state=instance.vm_state,
130                     method=f.__name__)
131             if (task_state is not None and
132                     instance.task_state not in task_state):
133                 raise exception.InstanceInvalidState(
134                     attr='task_state',
135                     instance_uuid=instance.uuid,
136                     state=instance.task_state,
137                     method=f.__name__)
138             if must_have_launched and not instance.launched_at:
139                 raise exception.InstanceInvalidState(
140                     attr='launched_at',
141                     instance_uuid=instance.uuid,
142                     state=instance.launched_at,
143                     method=f.__name__)
144 
145             return f(self, context, instance, *args, **kw)
146         return inner
147     return outer
148 
149 
150 def check_instance_host(function):
151     @six.wraps(function)
152     def wrapped(self, context, instance, *args, **kwargs):
153         if not instance.host:
154             raise exception.InstanceNotReady(instance_id=instance.uuid)
155         return function(self, context, instance, *args, **kwargs)
156     return wrapped
157 
158 
159 def check_instance_lock(function):
160     @six.wraps(function)
161     def inner(self, context, instance, *args, **kwargs):
162         if instance.locked and not context.is_admin:
163             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
164         return function(self, context, instance, *args, **kwargs)
165     return inner
166 
167 
168 def check_instance_cell(fn):
169     @six.wraps(fn)
170     def _wrapped(self, context, instance, *args, **kwargs):
171         self._validate_cell(instance)
172         return fn(self, context, instance, *args, **kwargs)
173     return _wrapped
174 
175 
176 def _diff_dict(orig, new):
177     """Return a dict describing how to change orig to new.  The keys
178     correspond to values that have changed; the value will be a list
179     of one or two elements.  The first element of the list will be
180     either '+' or '-', indicating whether the key was updated or
181     deleted; if the key was updated, the list will contain a second
182     element, giving the updated value.
183     """
184     # Figure out what keys went away
185     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
186     # Compute the updates
187     for key, value in new.items():
188         if key not in orig or value != orig[key]:
189             result[key] = ['+', value]
190     return result
191 
192 
193 class API(base.Base):
194     """API for interacting with the compute manager."""
195 
196     def __init__(self, image_api=None, network_api=None, volume_api=None,
197                  security_group_api=None, **kwargs):
198         self.image_api = image_api or image.API()
199         self.network_api = network_api or network.API()
200         self.volume_api = volume_api or cinder.API()
201         self.security_group_api = (security_group_api or
202             openstack_driver.get_openstack_security_group_driver())
203         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
204         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
205         self.compute_task_api = conductor.ComputeTaskAPI()
206         self.servicegroup_api = servicegroup.API()
207         self.notifier = rpc.get_notifier('compute', CONF.host)
208         if CONF.ephemeral_storage_encryption.enabled:
209             self.key_manager = keymgr.API()
210 
211         super(API, self).__init__(**kwargs)
212 
213     @property
214     def cell_type(self):
215         try:
216             return getattr(self, '_cell_type')
217         except AttributeError:
218             self._cell_type = cells_opts.get_cell_type()
219             return self._cell_type
220 
221     def _validate_cell(self, instance):
222         if self.cell_type != 'api':
223             return
224         cell_name = instance.cell_name
225         if not cell_name:
226             raise exception.InstanceUnknownCell(
227                     instance_uuid=instance.uuid)
228 
229     def _record_action_start(self, context, instance, action):
230         objects.InstanceAction.action_start(context, instance.uuid,
231                                             action, want_result=False)
232 
233     def _check_injected_file_quota(self, context, injected_files):
234         """Enforce quota limits on injected files.
235 
236         Raises a QuotaError if any limit is exceeded.
237         """
238         if injected_files is None:
239             return
240 
241         # Check number of files first
242         try:
243             objects.Quotas.limit_check(context,
244                                        injected_files=len(injected_files))
245         except exception.OverQuota:
246             raise exception.OnsetFileLimitExceeded()
247 
248         # OK, now count path and content lengths; we're looking for
249         # the max...
250         max_path = 0
251         max_content = 0
252         for path, content in injected_files:
253             max_path = max(max_path, len(path))
254             max_content = max(max_content, len(content))
255 
256         try:
257             objects.Quotas.limit_check(context,
258                                        injected_file_path_bytes=max_path,
259                                        injected_file_content_bytes=max_content)
260         except exception.OverQuota as exc:
261             # Favor path limit over content limit for reporting
262             # purposes
263             if 'injected_file_path_bytes' in exc.kwargs['overs']:
264                 raise exception.OnsetFilePathLimitExceeded()
265             else:
266                 raise exception.OnsetFileContentLimitExceeded()
267 
268     def _get_headroom(self, quotas, usages, deltas):
269         headroom = {res: quotas[res] -
270                          (usages[res]['in_use'] + usages[res]['reserved'])
271                     for res in quotas.keys()}
272         # If quota_cores is unlimited [-1]:
273         # - set cores headroom based on instances headroom:
274         if quotas.get('cores') == -1:
275             if deltas.get('cores'):
276                 hc = headroom.get('instances', 1) * deltas['cores']
277                 headroom['cores'] = hc / deltas.get('instances', 1)
278             else:
279                 headroom['cores'] = headroom.get('instances', 1)
280 
281         # If quota_ram is unlimited [-1]:
282         # - set ram headroom based on instances headroom:
283         if quotas.get('ram') == -1:
284             if deltas.get('ram'):
285                 hr = headroom.get('instances', 1) * deltas['ram']
286                 headroom['ram'] = hr / deltas.get('instances', 1)
287             else:
288                 headroom['ram'] = headroom.get('instances', 1)
289 
290         return headroom
291 
292     def _check_num_instances_quota(self, context, instance_type, min_count,
293                                    max_count, project_id=None, user_id=None):
294         """Enforce quota limits on number of instances created."""
295 
296         # Determine requested cores and ram
297         req_cores = max_count * instance_type['vcpus']
298         vram_mb = int(instance_type.get('extra_specs', {}).get(VIDEO_RAM, 0))
299         req_ram = max_count * (instance_type['memory_mb'] + vram_mb)
300 
301         # Check the quota
302         try:
303             quotas = objects.Quotas(context=context)
304             quotas.reserve(instances=max_count,
305                            cores=req_cores, ram=req_ram,
306                            project_id=project_id, user_id=user_id)
307         except exception.OverQuota as exc:
308             # OK, we exceeded quota; let's figure out why...
309             quotas = exc.kwargs['quotas']
310             overs = exc.kwargs['overs']
311             usages = exc.kwargs['usages']
312             deltas = {'instances': max_count,
313                       'cores': req_cores, 'ram': req_ram}
314             headroom = self._get_headroom(quotas, usages, deltas)
315 
316             allowed = headroom.get('instances', 1)
317             # Reduce 'allowed' instances in line with the cores & ram headroom
318             if instance_type['vcpus']:
319                 allowed = min(allowed,
320                               headroom['cores'] // instance_type['vcpus'])
321             if instance_type['memory_mb']:
322                 allowed = min(allowed,
323                               headroom['ram'] // (instance_type['memory_mb'] +
324                                                   vram_mb))
325 
326             # Convert to the appropriate exception message
327             if allowed <= 0:
328                 msg = _("Cannot run any more instances of this type.")
329             elif min_count <= allowed <= max_count:
330                 # We're actually OK, but still need reservations
331                 return self._check_num_instances_quota(context, instance_type,
332                                                        min_count, allowed)
333             else:
334                 msg = (_("Can only run %s more instances of this type.") %
335                        allowed)
336 
337             num_instances = (str(min_count) if min_count == max_count else
338                 "%s-%s" % (min_count, max_count))
339             requested = dict(instances=num_instances, cores=req_cores,
340                              ram=req_ram)
341             (overs, reqs, total_alloweds, useds) = self._get_over_quota_detail(
342                 headroom, overs, quotas, requested)
343             params = {'overs': overs, 'pid': context.project_id,
344                       'min_count': min_count, 'max_count': max_count,
345                       'msg': msg}
346 
347             if min_count == max_count:
348                 LOG.debug(("%(overs)s quota exceeded for %(pid)s,"
349                            " tried to run %(min_count)d instances. "
350                            "%(msg)s"), params)
351             else:
352                 LOG.debug(("%(overs)s quota exceeded for %(pid)s,"
353                            " tried to run between %(min_count)d and"
354                            " %(max_count)d instances. %(msg)s"),
355                           params)
356             raise exception.TooManyInstances(overs=overs,
357                                              req=reqs,
358                                              used=useds,
359                                              allowed=total_alloweds)
360 
361         return max_count, quotas
362 
363     def _get_over_quota_detail(self, headroom, overs, quotas, requested):
364         reqs = []
365         useds = []
366         total_alloweds = []
367         for resource in overs:
368             reqs.append(str(requested[resource]))
369             useds.append(str(quotas[resource] - headroom[resource]))
370             total_alloweds.append(str(quotas[resource]))
371         (overs, reqs, useds, total_alloweds) = map(', '.join, (
372             overs, reqs, useds, total_alloweds))
373         return overs, reqs, total_alloweds, useds
374 
375     def _check_metadata_properties_quota(self, context, metadata=None):
376         """Enforce quota limits on metadata properties."""
377         if not metadata:
378             metadata = {}
379         if not isinstance(metadata, dict):
380             msg = (_("Metadata type should be dict."))
381             raise exception.InvalidMetadata(reason=msg)
382         num_metadata = len(metadata)
383         try:
384             objects.Quotas.limit_check(context, metadata_items=num_metadata)
385         except exception.OverQuota as exc:
386             quota_metadata = exc.kwargs['quotas']['metadata_items']
387             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
388 
389         # Because metadata is stored in the DB, we hard-code the size limits
390         # In future, we may support more variable length strings, so we act
391         #  as if this is quota-controlled for forwards compatibility.
392         # Those are only used in V2 API, from V2.1 API, those checks are
393         # validated at API layer schema validation.
394         for k, v in six.iteritems(metadata):
395             try:
396                 utils.check_string_length(v)
397                 utils.check_string_length(k, min_length=1)
398             except exception.InvalidInput as e:
399                 raise exception.InvalidMetadata(reason=e.format_message())
400 
401             if len(k) > 255:
402                 msg = _("Metadata property key greater than 255 characters")
403                 raise exception.InvalidMetadataSize(reason=msg)
404             if len(v) > 255:
405                 msg = _("Metadata property value greater than 255 characters")
406                 raise exception.InvalidMetadataSize(reason=msg)
407 
408     def _check_requested_secgroups(self, context, secgroups):
409         """Check if the security group requested exists and belongs to
410         the project.
411 
412         :param context: The nova request context.
413         :type context: nova.context.RequestContext
414         :param secgroups: list of requested security group names, or uuids in
415             the case of Neutron.
416         :type secgroups: list
417         :returns: list of requested security group names unmodified if using
418             nova-network. If using Neutron, the list returned is all uuids.
419             Note that 'default' is a special case and will be unmodified if
420             it's requested.
421         """
422         security_groups = []
423         for secgroup in secgroups:
424             # NOTE(sdague): default is handled special
425             if secgroup == "default":
426                 security_groups.append(secgroup)
427                 continue
428             secgroup_dict = self.security_group_api.get(context, secgroup)
429             if not secgroup_dict:
430                 raise exception.SecurityGroupNotFoundForProject(
431                     project_id=context.project_id, security_group_id=secgroup)
432 
433             # Check to see if it's a nova-network or neutron type.
434             if isinstance(secgroup_dict['id'], int):
435                 # This is nova-network so just return the requested name.
436                 security_groups.append(secgroup)
437             else:
438                 # The id for neutron is a uuid, so we return the id (uuid).
439                 security_groups.append(secgroup_dict['id'])
440 
441         return security_groups
442 
443     def _check_requested_networks(self, context, requested_networks,
444                                   max_count):
445         """Check if the networks requested belongs to the project
446         and the fixed IP address for each network provided is within
447         same the network block
448         """
449         if requested_networks is not None:
450             if requested_networks.no_allocate:
451                 # If the network request was specifically 'none' meaning don't
452                 # allocate any networks, we just return the number of requested
453                 # instances since quotas don't change at all.
454                 return max_count
455 
456             # NOTE(danms): Temporary transition
457             requested_networks = requested_networks.as_tuples()
458 
459         return self.network_api.validate_networks(context, requested_networks,
460                                                   max_count)
461 
462     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
463                                    image):
464         """Choose kernel and ramdisk appropriate for the instance.
465 
466         The kernel and ramdisk can be chosen in one of three ways:
467 
468             1. Passed in with create-instance request.
469 
470             2. Inherited from image.
471 
472             3. Forced to None by using `null_kernel` FLAG.
473         """
474         # Inherit from image if not specified
475         image_properties = image.get('properties', {})
476 
477         if kernel_id is None:
478             kernel_id = image_properties.get('kernel_id')
479 
480         if ramdisk_id is None:
481             ramdisk_id = image_properties.get('ramdisk_id')
482 
483         # Force to None if using null_kernel
484         if kernel_id == str(CONF.null_kernel):
485             kernel_id = None
486             ramdisk_id = None
487 
488         # Verify kernel and ramdisk exist (fail-fast)
489         if kernel_id is not None:
490             kernel_image = self.image_api.get(context, kernel_id)
491             # kernel_id could have been a URI, not a UUID, so to keep behaviour
492             # from before, which leaked that implementation detail out to the
493             # caller, we return the image UUID of the kernel image and ramdisk
494             # image (below) and not any image URIs that might have been
495             # supplied.
496             # TODO(jaypipes): Get rid of this silliness once we move to a real
497             # Image object and hide all of that stuff within nova.image.api.
498             kernel_id = kernel_image['id']
499 
500         if ramdisk_id is not None:
501             ramdisk_image = self.image_api.get(context, ramdisk_id)
502             ramdisk_id = ramdisk_image['id']
503 
504         return kernel_id, ramdisk_id
505 
506     @staticmethod
507     def parse_availability_zone(context, availability_zone):
508         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
509         #             via az using az:host:node. It might be nice to expose an
510         #             api to specify specific hosts to force onto, but for
511         #             now it just supports this legacy hack.
512         # NOTE(deva): It is also possible to specify az::node, in which case
513         #             the host manager will determine the correct host.
514         forced_host = None
515         forced_node = None
516         if availability_zone and ':' in availability_zone:
517             c = availability_zone.count(':')
518             if c == 1:
519                 availability_zone, forced_host = availability_zone.split(':')
520             elif c == 2:
521                 if '::' in availability_zone:
522                     availability_zone, forced_node = \
523                             availability_zone.split('::')
524                 else:
525                     availability_zone, forced_host, forced_node = \
526                             availability_zone.split(':')
527             else:
528                 raise exception.InvalidInput(
529                         reason="Unable to parse availability_zone")
530 
531         if not availability_zone:
532             availability_zone = CONF.default_schedule_zone
533 
534         return availability_zone, forced_host, forced_node
535 
536     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
537                                           auto_disk_config, image):
538         auto_disk_config_disabled = \
539                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
540         if auto_disk_config_disabled and auto_disk_config:
541             raise exception.AutoDiskConfigDisabledByImage(image=image)
542 
543     def _inherit_properties_from_image(self, image, auto_disk_config):
544         image_properties = image.get('properties', {})
545         auto_disk_config_img = \
546                 utils.get_auto_disk_config_from_image_props(image_properties)
547         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
548                                                auto_disk_config,
549                                                image.get("id"))
550         if auto_disk_config is None:
551             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
552 
553         return {
554             'os_type': image_properties.get('os_type'),
555             'architecture': image_properties.get('architecture'),
556             'vm_mode': image_properties.get('vm_mode'),
557             'auto_disk_config': auto_disk_config
558         }
559 
560     def _new_instance_name_from_template(self, uuid, display_name, index):
561         params = {
562             'uuid': uuid,
563             'name': display_name,
564             'count': index + 1,
565         }
566         try:
567             new_name = (CONF.multi_instance_display_name_template %
568                         params)
569         except (KeyError, TypeError):
570             LOG.exception(_LE('Failed to set instance name using '
571                               'multi_instance_display_name_template.'))
572             new_name = display_name
573         return new_name
574 
575     def _apply_instance_name_template(self, context, instance, index):
576         original_name = instance.display_name
577         new_name = self._new_instance_name_from_template(instance.uuid,
578                 instance.display_name, index)
579         instance.display_name = new_name
580         if not instance.get('hostname', None):
581             if utils.sanitize_hostname(original_name) == "":
582                 instance.hostname = self._default_host_name(instance.uuid)
583             else:
584                 instance.hostname = utils.sanitize_hostname(new_name)
585         return instance
586 
587     def _check_config_drive(self, config_drive):
588         if config_drive:
589             try:
590                 bool_val = strutils.bool_from_string(config_drive,
591                                                      strict=True)
592             except ValueError:
593                 raise exception.ConfigDriveInvalidValue(option=config_drive)
594         else:
595             bool_val = False
596         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
597         # but this is because the config drive column is a String.  False
598         # is represented by using an empty string.  And for whatever
599         # reason, we rely on the DB to cast True to a String.
600         return True if bool_val else ''
601 
602     def _check_requested_image(self, context, image_id, image,
603                                instance_type, root_bdm):
604         if not image:
605             return
606 
607         if image['status'] != 'active':
608             raise exception.ImageNotActive(image_id=image_id)
609 
610         image_properties = image.get('properties', {})
611         config_drive_option = image_properties.get(
612             'img_config_drive', 'optional')
613         if config_drive_option not in ['optional', 'mandatory']:
614             raise exception.InvalidImageConfigDrive(
615                 config_drive=config_drive_option)
616 
617         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
618             raise exception.FlavorMemoryTooSmall()
619 
620         # Image min_disk is in gb, size is in bytes. For sanity, have them both
621         # in bytes.
622         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
623         image_size = int(image.get('size') or 0)
624 
625         # Target disk is a volume. Don't check flavor disk size because it
626         # doesn't make sense, and check min_disk against the volume size.
627         if (root_bdm is not None and root_bdm.is_volume):
628             # There are 2 possibilities here: either the target volume already
629             # exists, or it doesn't, in which case the bdm will contain the
630             # intended volume size.
631             #
632             # Cinder does its own check against min_disk, so if the target
633             # volume already exists this has already been done and we don't
634             # need to check it again here. In this case, volume_size may not be
635             # set on the bdm.
636             #
637             # If we're going to create the volume, the bdm will contain
638             # volume_size. Therefore we should check it if it exists. This will
639             # still be checked again by cinder when the volume is created, but
640             # that will not happen until the request reaches a host. By
641             # checking it here, the user gets an immediate and useful failure
642             # indication.
643             #
644             # The third possibility is that we have failed to consider
645             # something, and there are actually more than 2 possibilities. In
646             # this case cinder will still do the check at volume creation time.
647             # The behaviour will still be correct, but the user will not get an
648             # immediate failure from the api, and will instead have to
649             # determine why the instance is in an error state with a task of
650             # block_device_mapping.
651             #
652             # We could reasonably refactor this check into _validate_bdm at
653             # some future date, as the various size logic is already split out
654             # in there.
655             dest_size = root_bdm.volume_size
656             if dest_size is not None:
657                 dest_size *= units.Gi
658 
659                 if image_min_disk > dest_size:
660                     raise exception.VolumeSmallerThanMinDisk(
661                         volume_size=dest_size, image_min_disk=image_min_disk)
662 
663         # Target disk is a local disk whose size is taken from the flavor
664         else:
665             dest_size = instance_type['root_gb'] * units.Gi
666 
667             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
668             # since libvirt interpreted the value differently than other
669             # drivers. A value of 0 means don't check size.
670             if dest_size != 0:
671                 if image_size > dest_size:
672                     raise exception.FlavorDiskSmallerThanImage(
673                         flavor_size=dest_size, image_size=image_size)
674 
675                 if image_min_disk > dest_size:
676                     raise exception.FlavorDiskSmallerThanMinDisk(
677                         flavor_size=dest_size, image_min_disk=image_min_disk)
678 
679     def _get_image_defined_bdms(self, instance_type, image_meta,
680                                 root_device_name):
681         image_properties = image_meta.get('properties', {})
682 
683         # Get the block device mappings defined by the image.
684         image_defined_bdms = image_properties.get('block_device_mapping', [])
685         legacy_image_defined = not image_properties.get('bdm_v2', False)
686 
687         image_mapping = image_properties.get('mappings', [])
688 
689         if legacy_image_defined:
690             image_defined_bdms = block_device.from_legacy_mapping(
691                 image_defined_bdms, None, root_device_name)
692         else:
693             image_defined_bdms = list(map(block_device.BlockDeviceDict,
694                                           image_defined_bdms))
695 
696         if image_mapping:
697             image_mapping = self._prepare_image_mapping(instance_type,
698                                                         image_mapping)
699             image_defined_bdms = self._merge_bdms_lists(
700                 image_mapping, image_defined_bdms)
701 
702         return image_defined_bdms
703 
704     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
705         flavor_defined_bdms = []
706 
707         have_ephemeral_bdms = any(filter(
708             block_device.new_format_is_ephemeral, block_device_mapping))
709         have_swap_bdms = any(filter(
710             block_device.new_format_is_swap, block_device_mapping))
711 
712         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
713             flavor_defined_bdms.append(
714                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
715         if instance_type.get('swap') and not have_swap_bdms:
716             flavor_defined_bdms.append(
717                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
718 
719         return flavor_defined_bdms
720 
721     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
722         """Override any block devices from the first list by device name
723 
724         :param overridable_mappings: list which items are overridden
725         :param overrider_mappings: list which items override
726 
727         :returns: A merged list of bdms
728         """
729         device_names = set(bdm['device_name'] for bdm in overrider_mappings
730                            if bdm['device_name'])
731         return (overrider_mappings +
732                 [bdm for bdm in overridable_mappings
733                  if bdm['device_name'] not in device_names])
734 
735     def _check_and_transform_bdm(self, context, base_options, instance_type,
736                                  image_meta, min_count, max_count,
737                                  block_device_mapping, legacy_bdm):
738         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
739         #                  It's needed for legacy conversion to work.
740         root_device_name = (base_options.get('root_device_name') or 'vda')
741         image_ref = base_options.get('image_ref', '')
742         # If the instance is booted by image and has a volume attached,
743         # the volume cannot have the same device name as root_device_name
744         if image_ref:
745             for bdm in block_device_mapping:
746                 if (bdm.get('destination_type') == 'volume' and
747                     block_device.strip_dev(bdm.get(
748                     'device_name')) == root_device_name):
749                     msg = _('The volume cannot be assigned the same device'
750                             ' name as the root device %s') % root_device_name
751                     raise exception.InvalidRequest(msg)
752 
753         image_defined_bdms = self._get_image_defined_bdms(
754             instance_type, image_meta, root_device_name)
755         root_in_image_bdms = (
756             block_device.get_root_bdm(image_defined_bdms) is not None)
757 
758         if legacy_bdm:
759             block_device_mapping = block_device.from_legacy_mapping(
760                 block_device_mapping, image_ref, root_device_name,
761                 no_root=root_in_image_bdms)
762         elif root_in_image_bdms:
763             # NOTE (ndipanov): client will insert an image mapping into the v2
764             # block_device_mapping, but if there is a bootable device in image
765             # mappings - we need to get rid of the inserted image
766             # NOTE (gibi): another case is when a server is booted with an
767             # image to bdm mapping where the image only contains a bdm to a
768             # snapshot. In this case the other image to bdm mapping
769             # contains an unnecessary device with boot_index == 0.
770             # Also in this case the image_ref is None as we are booting from
771             # an image to volume bdm.
772             def not_image_and_root_bdm(bdm):
773                 return not (bdm.get('boot_index') == 0 and
774                             bdm.get('source_type') == 'image')
775 
776             block_device_mapping = list(
777                 filter(not_image_and_root_bdm, block_device_mapping))
778 
779         block_device_mapping = self._merge_bdms_lists(
780             image_defined_bdms, block_device_mapping)
781 
782         if min_count > 1 or max_count > 1:
783             if any(map(lambda bdm: bdm['source_type'] == 'volume',
784                        block_device_mapping)):
785                 msg = _('Cannot attach one or more volumes to multiple'
786                         ' instances')
787                 raise exception.InvalidRequest(msg)
788 
789         block_device_mapping += self._get_flavor_defined_bdms(
790             instance_type, block_device_mapping)
791 
792         return block_device_obj.block_device_make_list_from_dicts(
793                 context, block_device_mapping)
794 
795     def _get_image(self, context, image_href):
796         if not image_href:
797             return None, {}
798 
799         image = self.image_api.get(context, image_href)
800         return image['id'], image
801 
802     def _checks_for_create_and_rebuild(self, context, image_id, image,
803                                        instance_type, metadata,
804                                        files_to_inject, root_bdm):
805         self._check_metadata_properties_quota(context, metadata)
806         self._check_injected_file_quota(context, files_to_inject)
807         self._check_requested_image(context, image_id, image,
808                                     instance_type, root_bdm)
809 
810     def _validate_and_build_base_options(self, context, instance_type,
811                                          boot_meta, image_href, image_id,
812                                          kernel_id, ramdisk_id, display_name,
813                                          display_description, key_name,
814                                          key_data, security_groups,
815                                          availability_zone, user_data,
816                                          metadata, access_ip_v4, access_ip_v6,
817                                          requested_networks, config_drive,
818                                          auto_disk_config, reservation_id,
819                                          max_count):
820         """Verify all the input parameters regardless of the provisioning
821         strategy being performed.
822         """
823         if instance_type['disabled']:
824             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
825 
826         if user_data:
827             l = len(user_data)
828             if l > MAX_USERDATA_SIZE:
829                 # NOTE(mikal): user_data is stored in a text column, and
830                 # the database might silently truncate if its over length.
831                 raise exception.InstanceUserDataTooLarge(
832                     length=l, maxsize=MAX_USERDATA_SIZE)
833 
834             try:
835                 base64.decodestring(user_data)
836             except base64.binascii.Error:
837                 raise exception.InstanceUserDataMalformed()
838 
839         # When using Neutron, _check_requested_secgroups will translate and
840         # return any requested security group names to uuids.
841         security_groups = (
842             self._check_requested_secgroups(context, security_groups))
843 
844         # Note:  max_count is the number of instances requested by the user,
845         # max_network_count is the maximum number of instances taking into
846         # account any network quotas
847         max_network_count = self._check_requested_networks(context,
848                                      requested_networks, max_count)
849 
850         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
851                 context, kernel_id, ramdisk_id, boot_meta)
852 
853         config_drive = self._check_config_drive(config_drive)
854 
855         if key_data is None and key_name is not None:
856             key_pair = objects.KeyPair.get_by_name(context,
857                                                    context.user_id,
858                                                    key_name)
859             key_data = key_pair.public_key
860         else:
861             key_pair = None
862 
863         root_device_name = block_device.prepend_dev(
864                 block_device.properties_root_device_name(
865                     boot_meta.get('properties', {})))
866 
867         try:
868             image_meta = objects.ImageMeta.from_dict(boot_meta)
869         except ValueError as e:
870             # there must be invalid values in the image meta properties so
871             # consider this an invalid request
872             msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
873             raise exception.InvalidRequest(msg)
874         numa_topology = hardware.numa_get_constraints(
875                 instance_type, image_meta)
876 
877         system_metadata = {}
878 
879         # PCI requests come from two sources: instance flavor and
880         # requested_networks. The first call in below returns an
881         # InstancePCIRequests object which is a list of InstancePCIRequest
882         # objects. The second call in below creates an InstancePCIRequest
883         # object for each SR-IOV port, and append it to the list in the
884         # InstancePCIRequests object
885         pci_request_info = pci_request.get_pci_requests_from_flavor(
886             instance_type)
887         self.network_api.create_pci_requests_for_sriov_ports(context,
888             pci_request_info, requested_networks)
889 
890         base_options = {
891             'reservation_id': reservation_id,
892             'image_ref': image_href,
893             'kernel_id': kernel_id or '',
894             'ramdisk_id': ramdisk_id or '',
895             'power_state': power_state.NOSTATE,
896             'vm_state': vm_states.BUILDING,
897             'config_drive': config_drive,
898             'user_id': context.user_id,
899             'project_id': context.project_id,
900             'instance_type_id': instance_type['id'],
901             'memory_mb': instance_type['memory_mb'],
902             'vcpus': instance_type['vcpus'],
903             'root_gb': instance_type['root_gb'],
904             'ephemeral_gb': instance_type['ephemeral_gb'],
905             'display_name': display_name,
906             'display_description': display_description,
907             'user_data': user_data,
908             'key_name': key_name,
909             'key_data': key_data,
910             'locked': False,
911             'metadata': metadata or {},
912             'access_ip_v4': access_ip_v4,
913             'access_ip_v6': access_ip_v6,
914             'availability_zone': availability_zone,
915             'root_device_name': root_device_name,
916             'progress': 0,
917             'pci_requests': pci_request_info,
918             'numa_topology': numa_topology,
919             'system_metadata': system_metadata}
920 
921         options_from_image = self._inherit_properties_from_image(
922                 boot_meta, auto_disk_config)
923 
924         base_options.update(options_from_image)
925 
926         # return the validated options and maximum number of instances allowed
927         # by the network quotas
928         return base_options, max_network_count, key_pair, security_groups
929 
930     def _provision_instances(self, context, instance_type, min_count,
931             max_count, base_options, boot_meta, security_groups,
932             block_device_mapping, shutdown_terminate,
933             instance_group, check_server_group_quota, filter_properties,
934             key_pair):
935         # Reserve quotas
936         num_instances, quotas = self._check_num_instances_quota(
937                 context, instance_type, min_count, max_count)
938         security_groups = self.security_group_api.populate_security_groups(
939                 security_groups)
940         self.security_group_api.ensure_default(context)
941         LOG.debug("Going to run %s instances...", num_instances)
942         instances_to_build = []
943         try:
944             for i in range(num_instances):
945                 # Create a uuid for the instance so we can store the
946                 # RequestSpec before the instance is created.
947                 instance_uuid = uuidutils.generate_uuid()
948                 # Store the RequestSpec that will be used for scheduling.
949                 req_spec = objects.RequestSpec.from_components(context,
950                         instance_uuid, boot_meta, instance_type,
951                         base_options['numa_topology'],
952                         base_options['pci_requests'], filter_properties,
953                         instance_group, base_options['availability_zone'],
954                         security_groups=security_groups)
955                 req_spec.create()
956 
957                 # Create an instance object, but do not store in db yet.
958                 instance = objects.Instance(context=context)
959                 instance.uuid = instance_uuid
960                 instance.update(base_options)
961                 instance.keypairs = objects.KeyPairList(objects=[])
962                 if key_pair:
963                     instance.keypairs.objects.append(key_pair)
964                 instance = self.create_db_entry_for_new_instance(context,
965                         instance_type, boot_meta, instance, security_groups,
966                         block_device_mapping, num_instances, i,
967                         shutdown_terminate, create_instance=False)
968                 block_device_mapping = (
969                     self._bdm_validate_set_size_and_instance(context,
970                         instance, instance_type, block_device_mapping))
971 
972                 # NOTE(danms): BDMs are still not created, so we need to pass
973                 # a clone and then reset them on our object after create so
974                 # that they're still dirty for later in this process
975                 build_request = objects.BuildRequest(context,
976                         instance=instance, instance_uuid=instance.uuid,
977                         project_id=instance.project_id,
978                         block_device_mappings=block_device_mapping.obj_clone())
979                 build_request.create()
980                 build_request.block_device_mappings = block_device_mapping
981 
982                 # Create an instance_mapping.  The null cell_mapping indicates
983                 # that the instance doesn't yet exist in a cell, and lookups
984                 # for it need to instead look for the RequestSpec.
985                 # cell_mapping will be populated after scheduling, with a
986                 # scheduling failure using the cell_mapping for the special
987                 # cell0.
988                 inst_mapping = objects.InstanceMapping(context=context)
989                 inst_mapping.instance_uuid = instance_uuid
990                 inst_mapping.project_id = context.project_id
991                 inst_mapping.cell_mapping = None
992                 inst_mapping.create()
993 
994                 instances_to_build.append(
995                     (req_spec, build_request, inst_mapping))
996 
997                 if instance_group:
998                     if check_server_group_quota:
999                         count = objects.Quotas.count(context,
1000                                              'server_group_members',
1001                                              instance_group,
1002                                              context.user_id)
1003                         try:
1004                             objects.Quotas.limit_check(context,
1005                                                server_group_members=count + 1)
1006                         except exception.OverQuota:
1007                             msg = _("Quota exceeded, too many servers in "
1008                                     "group")
1009                             raise exception.QuotaError(msg)
1010 
1011                     members = objects.InstanceGroup.add_members(
1012                         context, instance_group.uuid, [instance.uuid])
1013                     # list of members added to servers group in this iteration
1014                     # is needed to check quota of server group during add next
1015                     # instance
1016                     instance_group.members.extend(members)
1017 
1018         # In the case of any exceptions, attempt DB cleanup and rollback the
1019         # quota reservations.
1020         except Exception:
1021             with excutils.save_and_reraise_exception():
1022                 try:
1023                     for rs, br, im in instances_to_build:
1024                         try:
1025                             rs.destroy()
1026                         except exception.RequestSpecNotFound:
1027                             pass
1028                         try:
1029                             im.destroy()
1030                         except exception.InstanceMappingNotFound:
1031                             pass
1032                         try:
1033                             br.destroy()
1034                         except exception.BuildRequestNotFound:
1035                             pass
1036                 finally:
1037                     quotas.rollback()
1038 
1039         # Commit the reservations
1040         quotas.commit()
1041 
1042         return instances_to_build
1043 
1044     def _get_bdm_image_metadata(self, context, block_device_mapping,
1045                                 legacy_bdm=True):
1046         """If we are booting from a volume, we need to get the
1047         volume details from Cinder and make sure we pass the
1048         metadata back accordingly.
1049         """
1050         if not block_device_mapping:
1051             return {}
1052 
1053         for bdm in block_device_mapping:
1054             if (legacy_bdm and
1055                     block_device.get_device_letter(
1056                        bdm.get('device_name', '')) != 'a'):
1057                 continue
1058             elif not legacy_bdm and bdm.get('boot_index') != 0:
1059                 continue
1060 
1061             volume_id = bdm.get('volume_id')
1062             snapshot_id = bdm.get('snapshot_id')
1063             if snapshot_id:
1064                 # NOTE(alaski): A volume snapshot inherits metadata from the
1065                 # originating volume, but the API does not expose metadata
1066                 # on the snapshot itself.  So we query the volume for it below.
1067                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
1068                 volume_id = snapshot['volume_id']
1069 
1070             if bdm.get('image_id'):
1071                 try:
1072                     image_id = bdm['image_id']
1073                     image_meta = self.image_api.get(context, image_id)
1074                     return image_meta
1075                 except Exception:
1076                     raise exception.InvalidBDMImage(id=image_id)
1077             elif volume_id:
1078                 try:
1079                     volume = self.volume_api.get(context, volume_id)
1080                 except exception.CinderConnectionFailed:
1081                     raise
1082                 except Exception:
1083                     raise exception.InvalidBDMVolume(id=volume_id)
1084 
1085                 if not volume.get('bootable', True):
1086                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1087 
1088                 return utils.get_image_metadata_from_volume(volume)
1089         return {}
1090 
1091     @staticmethod
1092     def _get_requested_instance_group(context, filter_properties):
1093         if (not filter_properties or
1094                 not filter_properties.get('scheduler_hints')):
1095             return
1096 
1097         group_hint = filter_properties.get('scheduler_hints').get('group')
1098         if not group_hint:
1099             return
1100 
1101         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1102 
1103     def _safe_destroy_instance_residue(self, instances, instances_to_build):
1104         """Delete residue left over from a failed instance build with
1105            reckless abandon.
1106 
1107         :param instances: List of Instance objects to destroy
1108         :param instances_to_build: List of tuples, output from
1109             _provision_instances, which is:
1110              request_spec, build_request, instance_mapping
1111         """
1112         for instance in instances:
1113             try:
1114                 instance.destroy()
1115             except Exception as e:
1116                 LOG.debug('Failed to destroy instance residue: %s', e,
1117                           instance=instance)
1118         for to_destroy in instances_to_build:
1119             for thing in to_destroy:
1120                 try:
1121                     thing.destroy()
1122                 except Exception as e:
1123                     LOG.debug(
1124                         'Failed to destroy %s during residue cleanup: %s',
1125                         thing, e)
1126 
1127     def _create_instance(self, context, instance_type,
1128                image_href, kernel_id, ramdisk_id,
1129                min_count, max_count,
1130                display_name, display_description,
1131                key_name, key_data, security_groups,
1132                availability_zone, user_data, metadata, injected_files,
1133                admin_password, access_ip_v4, access_ip_v6,
1134                requested_networks, config_drive,
1135                block_device_mapping, auto_disk_config, filter_properties,
1136                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1137                check_server_group_quota=False):
1138         """Verify all the input parameters regardless of the provisioning
1139         strategy being performed and schedule the instance(s) for
1140         creation.
1141         """
1142 
1143         # Normalize and setup some parameters
1144         if reservation_id is None:
1145             reservation_id = utils.generate_uid('r')
1146         security_groups = security_groups or ['default']
1147         min_count = min_count or 1
1148         max_count = max_count or min_count
1149         block_device_mapping = block_device_mapping or []
1150 
1151         if image_href:
1152             image_id, boot_meta = self._get_image(context, image_href)
1153         else:
1154             image_id = None
1155             boot_meta = self._get_bdm_image_metadata(
1156                 context, block_device_mapping, legacy_bdm)
1157 
1158         self._check_auto_disk_config(image=boot_meta,
1159                                      auto_disk_config=auto_disk_config)
1160 
1161         base_options, max_net_count, key_pair, security_groups = \
1162                 self._validate_and_build_base_options(
1163                     context, instance_type, boot_meta, image_href, image_id,
1164                     kernel_id, ramdisk_id, display_name, display_description,
1165                     key_name, key_data, security_groups, availability_zone,
1166                     user_data, metadata, access_ip_v4, access_ip_v6,
1167                     requested_networks, config_drive, auto_disk_config,
1168                     reservation_id, max_count)
1169 
1170         # max_net_count is the maximum number of instances requested by the
1171         # user adjusted for any network quota constraints, including
1172         # consideration of connections to each requested network
1173         if max_net_count < min_count:
1174             raise exception.PortLimitExceeded()
1175         elif max_net_count < max_count:
1176             LOG.info(_LI("max count reduced from %(max_count)d to "
1177                          "%(max_net_count)d due to network port quota"),
1178                         {'max_count': max_count,
1179                          'max_net_count': max_net_count})
1180             max_count = max_net_count
1181 
1182         block_device_mapping = self._check_and_transform_bdm(context,
1183             base_options, instance_type, boot_meta, min_count, max_count,
1184             block_device_mapping, legacy_bdm)
1185 
1186         # We can't do this check earlier because we need bdms from all sources
1187         # to have been merged in order to get the root bdm.
1188         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1189                 instance_type, metadata, injected_files,
1190                 block_device_mapping.root_bdm())
1191 
1192         instance_group = self._get_requested_instance_group(context,
1193                                    filter_properties)
1194 
1195         instances_to_build = self._provision_instances(context, instance_type,
1196                 min_count, max_count, base_options, boot_meta, security_groups,
1197                 block_device_mapping, shutdown_terminate,
1198                 instance_group, check_server_group_quota, filter_properties,
1199                 key_pair)
1200 
1201         instances = []
1202         build_requests = []
1203         # TODO(alaski): Cast to conductor here which will call the
1204         # scheduler and defer instance creation until the scheduler
1205         # has picked a cell/host. Set the instance_mapping to the cell
1206         # that the instance is scheduled to.
1207         # NOTE(alaski): Instance and block device creation are going
1208         # to move to the conductor.
1209         try:
1210             for rs, build_request, im in instances_to_build:
1211                 build_requests.append(build_request)
1212                 instance = build_request.get_new_instance(context)
1213                 instance.create()
1214                 instances.append(instance)
1215                 self._create_block_device_mapping(
1216                     build_request.block_device_mappings)
1217                 # send a state update notification for the initial create to
1218                 # show it going from non-existent to BUILDING
1219                 notifications.send_update_with_states(context, instance, None,
1220                         vm_states.BUILDING, None, None, service="api")
1221         except Exception:
1222             with excutils.save_and_reraise_exception():
1223                 self._safe_destroy_instance_residue(instances,
1224                                                     instances_to_build)
1225 
1226         for instance in instances:
1227             self._record_action_start(context, instance,
1228                                       instance_actions.CREATE)
1229 
1230         self.compute_task_api.build_instances(context,
1231                 instances=instances, image=boot_meta,
1232                 filter_properties=filter_properties,
1233                 admin_password=admin_password,
1234                 injected_files=injected_files,
1235                 requested_networks=requested_networks,
1236                 security_groups=security_groups,
1237                 block_device_mapping=block_device_mapping,
1238                 legacy_bdm=False)
1239 
1240         return (instances, reservation_id)
1241 
1242     @staticmethod
1243     def _volume_size(instance_type, bdm):
1244         size = bdm.get('volume_size')
1245         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1246         if (size is None and bdm.get('source_type') == 'blank' and
1247                 bdm.get('destination_type') == 'local'):
1248             if bdm.get('guest_format') == 'swap':
1249                 size = instance_type.get('swap', 0)
1250             else:
1251                 size = instance_type.get('ephemeral_gb', 0)
1252         return size
1253 
1254     def _prepare_image_mapping(self, instance_type, mappings):
1255         """Extract and format blank devices from image mappings."""
1256 
1257         prepared_mappings = []
1258 
1259         for bdm in block_device.mappings_prepend_dev(mappings):
1260             LOG.debug("Image bdm %s", bdm)
1261 
1262             virtual_name = bdm['virtual']
1263             if virtual_name == 'ami' or virtual_name == 'root':
1264                 continue
1265 
1266             if not block_device.is_swap_or_ephemeral(virtual_name):
1267                 continue
1268 
1269             guest_format = bdm.get('guest_format')
1270             if virtual_name == 'swap':
1271                 guest_format = 'swap'
1272             if not guest_format:
1273                 guest_format = CONF.default_ephemeral_format
1274 
1275             values = block_device.BlockDeviceDict({
1276                 'device_name': bdm['device'],
1277                 'source_type': 'blank',
1278                 'destination_type': 'local',
1279                 'device_type': 'disk',
1280                 'guest_format': guest_format,
1281                 'delete_on_termination': True,
1282                 'boot_index': -1})
1283 
1284             values['volume_size'] = self._volume_size(
1285                 instance_type, values)
1286             if values['volume_size'] == 0:
1287                 continue
1288 
1289             prepared_mappings.append(values)
1290 
1291         return prepared_mappings
1292 
1293     def _bdm_validate_set_size_and_instance(self, context, instance,
1294                                             instance_type,
1295                                             block_device_mapping):
1296         """Ensure the bdms are valid, then set size and associate with instance
1297 
1298         Because this method can be called multiple times when more than one
1299         instance is booted in a single request it makes a copy of the bdm list.
1300         """
1301         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1302                   instance_uuid=instance.uuid)
1303         self._validate_bdm(
1304             context, instance, instance_type, block_device_mapping)
1305         instance_block_device_mapping = block_device_mapping.obj_clone()
1306         for bdm in instance_block_device_mapping:
1307             bdm.volume_size = self._volume_size(instance_type, bdm)
1308             bdm.instance_uuid = instance.uuid
1309         return instance_block_device_mapping
1310 
1311     def _create_block_device_mapping(self, block_device_mapping):
1312         # Copy the block_device_mapping because this method can be called
1313         # multiple times when more than one instance is booted in a single
1314         # request. This avoids 'id' being set and triggering the object dupe
1315         # detection
1316         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1317         # Create the BlockDeviceMapping objects in the db.
1318         for bdm in db_block_device_mapping:
1319             # TODO(alaski): Why is this done?
1320             if bdm.volume_size == 0:
1321                 continue
1322 
1323             bdm.update_or_create()
1324 
1325     def _validate_bdm(self, context, instance, instance_type,
1326                       block_device_mappings):
1327         def _subsequent_list(l):
1328             # Each device which is capable of being used as boot device should
1329             # be given a unique boot index, starting from 0 in ascending order.
1330             return all(el + 1 == l[i + 1] for i, el in enumerate(l[:-1]))
1331 
1332         # Make sure that the boot indexes make sense.
1333         # Setting a negative value or None indicates that the device should not
1334         # be used for booting.
1335         boot_indexes = sorted([bdm.boot_index
1336                                for bdm in block_device_mappings
1337                                if bdm.boot_index is not None
1338                                and bdm.boot_index >= 0])
1339 
1340         if 0 not in boot_indexes or not _subsequent_list(boot_indexes):
1341             # Convert the BlockDeviceMappingList to a list for repr details.
1342             LOG.debug('Invalid block device mapping boot sequence for '
1343                       'instance: %s', list(block_device_mappings),
1344                       instance=instance)
1345             raise exception.InvalidBDMBootSequence()
1346 
1347         for bdm in block_device_mappings:
1348             # NOTE(vish): For now, just make sure the volumes are accessible.
1349             # Additionally, check that the volume can be attached to this
1350             # instance.
1351             snapshot_id = bdm.snapshot_id
1352             volume_id = bdm.volume_id
1353             image_id = bdm.image_id
1354             if (image_id is not None and
1355                     image_id != instance.get('image_ref')):
1356                 try:
1357                     self._get_image(context, image_id)
1358                 except Exception:
1359                     raise exception.InvalidBDMImage(id=image_id)
1360                 if (bdm.source_type == 'image' and
1361                         bdm.destination_type == 'volume' and
1362                         not bdm.volume_size):
1363                     raise exception.InvalidBDM(message=_("Images with "
1364                         "destination_type 'volume' need to have a non-zero "
1365                         "size specified"))
1366             elif volume_id is not None:
1367                 try:
1368                     volume = self.volume_api.get(context, volume_id)
1369                     self.volume_api.check_attach(context,
1370                                                  volume,
1371                                                  instance=instance)
1372                     bdm.volume_size = volume.get('size')
1373                 except (exception.CinderConnectionFailed,
1374                         exception.InvalidVolume):
1375                     raise
1376                 except Exception:
1377                     raise exception.InvalidBDMVolume(id=volume_id)
1378             elif snapshot_id is not None:
1379                 try:
1380                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1381                     bdm.volume_size = bdm.volume_size or snap.get('size')
1382                 except exception.CinderConnectionFailed:
1383                     raise
1384                 except Exception:
1385                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1386             elif (bdm.source_type == 'blank' and
1387                     bdm.destination_type == 'volume' and
1388                     not bdm.volume_size):
1389                 raise exception.InvalidBDM(message=_("Blank volumes "
1390                     "(source: 'blank', dest: 'volume') need to have non-zero "
1391                     "size"))
1392 
1393         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1394                 for bdm in block_device_mappings
1395                 if block_device.new_format_is_ephemeral(bdm))
1396         if ephemeral_size > instance_type['ephemeral_gb']:
1397             raise exception.InvalidBDMEphemeralSize()
1398 
1399         # There should be only one swap
1400         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1401         if len(swap_list) > 1:
1402             msg = _("More than one swap drive requested.")
1403             raise exception.InvalidBDMFormat(details=msg)
1404 
1405         if swap_list:
1406             swap_size = swap_list[0].volume_size or 0
1407             if swap_size > instance_type['swap']:
1408                 raise exception.InvalidBDMSwapSize()
1409 
1410         max_local = CONF.max_local_block_devices
1411         if max_local >= 0:
1412             num_local = len([bdm for bdm in block_device_mappings
1413                              if bdm.destination_type == 'local'])
1414             if num_local > max_local:
1415                 raise exception.InvalidBDMLocalsLimit()
1416 
1417     def _populate_instance_names(self, instance, num_instances):
1418         """Populate instance display_name and hostname."""
1419         display_name = instance.get('display_name')
1420         if instance.obj_attr_is_set('hostname'):
1421             hostname = instance.get('hostname')
1422         else:
1423             hostname = None
1424 
1425         # NOTE(mriedem): This is only here for test simplicity since a server
1426         # name is required in the REST API.
1427         if display_name is None:
1428             display_name = self._default_display_name(instance.uuid)
1429             instance.display_name = display_name
1430 
1431         if hostname is None and num_instances == 1:
1432             # NOTE(russellb) In the multi-instance case, we're going to
1433             # overwrite the display_name using the
1434             # multi_instance_display_name_template.  We need the default
1435             # display_name set so that it can be used in the template, though.
1436             # Only set the hostname here if we're only creating one instance.
1437             # Otherwise, it will be built after the template based
1438             # display_name.
1439             hostname = display_name
1440             default_hostname = self._default_host_name(instance.uuid)
1441             instance.hostname = utils.sanitize_hostname(hostname,
1442                                                         default_hostname)
1443 
1444     def _default_display_name(self, instance_uuid):
1445         return "Server %s" % instance_uuid
1446 
1447     def _default_host_name(self, instance_uuid):
1448         return "Server-%s" % instance_uuid
1449 
1450     def _populate_instance_for_create(self, context, instance, image,
1451                                       index, security_groups, instance_type,
1452                                       num_instances, shutdown_terminate):
1453         """Build the beginning of a new instance."""
1454 
1455         instance.launch_index = index
1456         instance.vm_state = vm_states.BUILDING
1457         instance.task_state = task_states.SCHEDULING
1458         info_cache = objects.InstanceInfoCache()
1459         info_cache.instance_uuid = instance.uuid
1460         info_cache.network_info = network_model.NetworkInfo()
1461         instance.info_cache = info_cache
1462         instance.flavor = instance_type
1463         instance.old_flavor = None
1464         instance.new_flavor = None
1465         if CONF.ephemeral_storage_encryption.enabled:
1466             instance.ephemeral_key_uuid = self.key_manager.create_key(
1467                 context,
1468                 length=CONF.ephemeral_storage_encryption.key_size)
1469         else:
1470             instance.ephemeral_key_uuid = None
1471 
1472         # Store image properties so we can use them later
1473         # (for notifications, etc).  Only store what we can.
1474         if not instance.obj_attr_is_set('system_metadata'):
1475             instance.system_metadata = {}
1476         # Make sure we have the dict form that we need for instance_update.
1477         instance.system_metadata = utils.instance_sys_meta(instance)
1478 
1479         system_meta = utils.get_system_metadata_from_image(
1480             image, instance_type)
1481 
1482         # In case we couldn't find any suitable base_image
1483         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1484 
1485         instance.system_metadata.update(system_meta)
1486 
1487         if CONF.use_neutron:
1488             # For Neutron we don't actually store anything in the database, we
1489             # proxy the security groups on the instance from the ports
1490             # attached to the instance.
1491             instance.security_groups = objects.SecurityGroupList()
1492         else:
1493             instance.security_groups = security_groups
1494 
1495         self._populate_instance_names(instance, num_instances)
1496         instance.shutdown_terminate = shutdown_terminate
1497         if num_instances > 1 and self.cell_type != 'api':
1498             instance = self._apply_instance_name_template(context, instance,
1499                                                           index)
1500 
1501         return instance
1502 
1503     # This method remains because cellsv1 uses it in the scheduler
1504     def create_db_entry_for_new_instance(self, context, instance_type, image,
1505             instance, security_group, block_device_mapping, num_instances,
1506             index, shutdown_terminate=False, create_instance=True):
1507         """Create an entry in the DB for this new instance,
1508         including any related table updates (such as security group,
1509         etc).
1510 
1511         This is called by the scheduler after a location for the
1512         instance has been determined.
1513 
1514         :param create_instance: Determines if the instance is created here or
1515             just populated for later creation. This is done so that this code
1516             can be shared with cellsv1 which needs the instance creation to
1517             happen here. It should be removed and this method cleaned up when
1518             cellsv1 is a distant memory.
1519         """
1520         self._populate_instance_for_create(context, instance, image, index,
1521                                            security_group, instance_type,
1522                                            num_instances, shutdown_terminate)
1523 
1524         if create_instance:
1525             instance.create()
1526 
1527         return instance
1528 
1529     def _check_multiple_instances_with_neutron_ports(self,
1530                                                      requested_networks):
1531         """Check whether multiple instances are created from port id(s)."""
1532         for requested_net in requested_networks:
1533             if requested_net.port_id:
1534                 msg = _("Unable to launch multiple instances with"
1535                         " a single configured port ID. Please launch your"
1536                         " instance one by one with different ports.")
1537                 raise exception.MultiplePortsNotApplicable(reason=msg)
1538 
1539     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1540         """Check whether multiple instances are created with specified ip."""
1541 
1542         for requested_net in requested_networks:
1543             if requested_net.network_id and requested_net.address:
1544                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1545                         "is specified.")
1546                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1547 
1548     @hooks.add_hook("create_instance")
1549     def create(self, context, instance_type,
1550                image_href, kernel_id=None, ramdisk_id=None,
1551                min_count=None, max_count=None,
1552                display_name=None, display_description=None,
1553                key_name=None, key_data=None, security_groups=None,
1554                availability_zone=None, forced_host=None, forced_node=None,
1555                user_data=None, metadata=None, injected_files=None,
1556                admin_password=None, block_device_mapping=None,
1557                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1558                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1559                legacy_bdm=True, shutdown_terminate=False,
1560                check_server_group_quota=False):
1561         """Provision instances, sending instance information to the
1562         scheduler.  The scheduler will determine where the instance(s)
1563         go and will handle creating the DB entries.
1564 
1565         Returns a tuple of (instances, reservation_id)
1566         """
1567         if requested_networks and max_count is not None and max_count > 1:
1568             self._check_multiple_instances_with_specified_ip(
1569                 requested_networks)
1570             if utils.is_neutron():
1571                 self._check_multiple_instances_with_neutron_ports(
1572                     requested_networks)
1573 
1574         if availability_zone:
1575             available_zones = availability_zones.\
1576                 get_availability_zones(context.elevated(), True)
1577             if forced_host is None and availability_zone not in \
1578                     available_zones:
1579                 msg = _('The requested availability zone is not available')
1580                 raise exception.InvalidRequest(msg)
1581 
1582         filter_properties = scheduler_utils.build_filter_properties(
1583                 scheduler_hints, forced_host, forced_node, instance_type)
1584 
1585         return self._create_instance(
1586                        context, instance_type,
1587                        image_href, kernel_id, ramdisk_id,
1588                        min_count, max_count,
1589                        display_name, display_description,
1590                        key_name, key_data, security_groups,
1591                        availability_zone, user_data, metadata,
1592                        injected_files, admin_password,
1593                        access_ip_v4, access_ip_v6,
1594                        requested_networks, config_drive,
1595                        block_device_mapping, auto_disk_config,
1596                        filter_properties=filter_properties,
1597                        legacy_bdm=legacy_bdm,
1598                        shutdown_terminate=shutdown_terminate,
1599                        check_server_group_quota=check_server_group_quota)
1600 
1601     def _check_auto_disk_config(self, instance=None, image=None,
1602                                 **extra_instance_updates):
1603         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1604         if auto_disk_config is None:
1605             return
1606         if not image and not instance:
1607             return
1608 
1609         if image:
1610             image_props = image.get("properties", {})
1611             auto_disk_config_img = \
1612                 utils.get_auto_disk_config_from_image_props(image_props)
1613             image_ref = image.get("id")
1614         else:
1615             sys_meta = utils.instance_sys_meta(instance)
1616             image_ref = sys_meta.get('image_base_image_ref')
1617             auto_disk_config_img = \
1618                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1619 
1620         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1621                                                auto_disk_config,
1622                                                image_ref)
1623 
1624     def _lookup_instance(self, context, uuid):
1625         '''Helper method for pulling an instance object from a database.
1626 
1627         During the transition to cellsv2 there is some complexity around
1628         retrieving an instance from the database which this method hides. If
1629         there is an instance mapping then query the cell for the instance, if
1630         no mapping exists then query the configured nova database.
1631 
1632         Once we are past the point that all deployments can be assumed to be
1633         migrated to cellsv2 this method can go away.
1634         '''
1635         inst_map = None
1636         try:
1637             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1638                 context, uuid)
1639         except exception.InstanceMappingNotFound:
1640             # TODO(alaski): This exception block can be removed once we're
1641             # guaranteed everyone is using cellsv2.
1642             pass
1643 
1644         if inst_map is None or inst_map.cell_mapping is None:
1645             # If inst_map is None then the deployment has not migrated to
1646             # cellsv2 yet.
1647             # If inst_map.cell_mapping is None then the instance is not in a
1648             # cell yet. Until instance creation moves to the conductor the
1649             # instance can be found in the configured database, so attempt
1650             # to look it up.
1651             try:
1652                 instance = objects.Instance.get_by_uuid(context, uuid)
1653             except exception.InstanceNotFound:
1654                 # If we get here then the conductor is in charge of writing the
1655                 # instance to the database and hasn't done that yet. It's up to
1656                 # the caller of this method to determine what to do with that
1657                 # information.
1658                 return
1659         else:
1660             with nova_context.target_cell(context, inst_map.cell_mapping):
1661                 try:
1662                     instance = objects.Instance.get_by_uuid(context,
1663                                                             uuid)
1664                 except exception.InstanceNotFound:
1665                     # Since the cell_mapping exists we know the instance is in
1666                     # the cell, however InstanceNotFound means it's already
1667                     # deleted.
1668                     return
1669         return instance
1670 
1671     def _delete_while_booting(self, context, instance):
1672         """Handle deletion if the instance has not reached a cell yet
1673 
1674         Deletion before an instance reaches a cell needs to be handled
1675         differently. What we're attempting to do is delete the BuildRequest
1676         before the api level conductor does.  If we succeed here then the boot
1677         request stops before reaching a cell.  If not then the instance will
1678         need to be looked up in a cell db and the normal delete path taken.
1679         """
1680         deleted = self._attempt_delete_of_buildrequest(context, instance)
1681 
1682         # After service version 15 deletion of the BuildRequest will halt the
1683         # build process in the conductor. In that case run the rest of this
1684         # method and consider the instance deleted. If we have not yet reached
1685         # service version 15 then just return False so the rest of the delete
1686         # process will proceed usually.
1687         service_version = objects.Service.get_minimum_version(
1688             context, 'nova-osapi_compute')
1689         if service_version < 15:
1690             return False
1691 
1692         if deleted:
1693             # If we've reached this block the successful deletion of the
1694             # buildrequest indicates that the build process should be halted by
1695             # the conductor.
1696 
1697             # Since conductor has halted the build process no cleanup of the
1698             # instance is necessary, but quotas must still be decremented.
1699             project_id, user_id = quotas_obj.ids_from_instance(
1700                 context, instance)
1701             # This is confusing but actually decrements quota.
1702             quotas = self._create_reservations(context,
1703                                                instance,
1704                                                instance.task_state,
1705                                                project_id, user_id)
1706             try:
1707                 quotas.commit()
1708 
1709                 # NOTE(alaski): Though the conductor halts the build process it
1710                 # does not currently delete the instance record. This is
1711                 # because in the near future the instance record will not be
1712                 # created if the buildrequest has been deleted here. For now we
1713                 # ensure the instance has been set to deleted at this point.
1714                 # Yes this directly contradicts the comment earlier in this
1715                 # method, but this is a temporary measure.
1716                 # Look up the instance because the current instance object was
1717                 # stashed on the buildrequest and therefore not complete enough
1718                 # to run .destroy().
1719                 instance = self._lookup_instance(context, instance.uuid)
1720                 if instance is not None:
1721                     # If instance is None it has already been deleted.
1722                     instance.destroy()
1723             except exception.InstanceNotFound:
1724                 quotas.rollback()
1725 
1726             return True
1727         return False
1728 
1729     def _attempt_delete_of_buildrequest(self, context, instance):
1730         # If there is a BuildRequest then the instance may not have been
1731         # written to a cell db yet. Delete the BuildRequest here, which
1732         # will indicate that the Instance build should not proceed.
1733         try:
1734             build_req = objects.BuildRequest.get_by_instance_uuid(
1735                 context, instance.uuid)
1736             build_req.destroy()
1737         except exception.BuildRequestNotFound:
1738             # This means that conductor has deleted the BuildRequest so the
1739             # instance is now in a cell and the delete needs to proceed
1740             # normally.
1741             return False
1742         return True
1743 
1744     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1745         if instance.disable_terminate:
1746             LOG.info(_LI('instance termination disabled'),
1747                      instance=instance)
1748             return
1749 
1750         if instance.task_state == task_states.FORCE_DELETING:
1751             raise exception.InstanceInvalidState(
1752                 attr='task_state',
1753                 instance_uuid=instance.uuid,
1754                 state=instance.task_state,
1755                 method=delete_type)
1756 
1757         # If there is an instance.host the instance has been scheduled and
1758         # sent to a cell/compute which means it was pulled from the cell db.
1759         # Normal delete should be attempted.
1760         if not instance.host:
1761             if self._delete_while_booting(context, instance):
1762                 return
1763             # If instance.host was not set it's possible that the Instance
1764             # object here was pulled from a BuildRequest object and is not
1765             # fully populated. Notably it will be missing an 'id' field which
1766             # will prevent instance.destroy from functioning properly. A lookup
1767             # is attempted which will either return a full Instance or None if
1768             # not found. If not found then it's acceptable to skip the rest of
1769             # the delete processing.
1770             instance = self._lookup_instance(context, instance.uuid)
1771             if not instance:
1772                 # Instance is already deleted.
1773                 return
1774 
1775         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1776                 context, instance.uuid)
1777 
1778         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
1779 
1780         # At these states an instance has a snapshot associate.
1781         if instance.vm_state in (vm_states.SHELVED,
1782                                  vm_states.SHELVED_OFFLOADED):
1783             snapshot_id = instance.system_metadata.get('shelved_image_id')
1784             LOG.info(_LI("Working on deleting snapshot %s "
1785                          "from shelved instance..."),
1786                      snapshot_id, instance=instance)
1787             try:
1788                 self.image_api.delete(context, snapshot_id)
1789             except (exception.ImageNotFound,
1790                     exception.ImageNotAuthorized) as exc:
1791                 LOG.warning(_LW("Failed to delete snapshot "
1792                                 "from shelved instance (%s)."),
1793                             exc.format_message(), instance=instance)
1794             except Exception:
1795                 LOG.exception(_LE("Something wrong happened when trying to "
1796                                   "delete snapshot from shelved instance."),
1797                               instance=instance)
1798 
1799         original_task_state = instance.task_state
1800         quotas = None
1801         try:
1802             # NOTE(maoy): no expected_task_state needs to be set
1803             instance.update(instance_attrs)
1804             instance.progress = 0
1805             instance.save()
1806 
1807             # NOTE(comstud): If we delete the instance locally, we'll
1808             # commit the reservations here.  Otherwise, the manager side
1809             # will commit or rollback the reservations based on success.
1810             quotas = self._create_reservations(context,
1811                                                instance,
1812                                                original_task_state,
1813                                                project_id, user_id)
1814 
1815             if self.cell_type == 'api':
1816                 # NOTE(comstud): If we're in the API cell, we need to
1817                 # skip all remaining logic and just call the callback,
1818                 # which will cause a cast to the child cell.  Also,
1819                 # commit reservations here early until we have a better
1820                 # way to deal with quotas with cells.
1821                 cb(context, instance, bdms, reservations=None)
1822                 quotas.commit()
1823                 return
1824             shelved_offloaded = (instance.vm_state
1825                                  == vm_states.SHELVED_OFFLOADED)
1826             if not instance.host and not shelved_offloaded:
1827                 try:
1828                     compute_utils.notify_about_instance_usage(
1829                             self.notifier, context, instance,
1830                             "%s.start" % delete_type)
1831                     instance.destroy()
1832                     compute_utils.notify_about_instance_usage(
1833                             self.notifier, context, instance,
1834                             "%s.end" % delete_type,
1835                             system_metadata=instance.system_metadata)
1836                     quotas.commit()
1837                     LOG.info(_LI('Instance deleted and does not have host '
1838                                  'field, its vm_state is %(state)s.'),
1839                                  {'state': instance.vm_state},
1840                                  instance=instance)
1841                     return
1842                 except exception.ObjectActionError:
1843                     instance.refresh()
1844 
1845             if instance.vm_state == vm_states.RESIZED:
1846                 self._confirm_resize_on_deleting(context, instance)
1847 
1848             is_local_delete = True
1849             try:
1850                 if not shelved_offloaded:
1851                     service = objects.Service.get_by_compute_host(
1852                         context.elevated(), instance.host)
1853                     is_local_delete = not self.servicegroup_api.service_is_up(
1854                         service)
1855                 if not is_local_delete:
1856                     if (delete_type != 'force_delete'
1857                         and original_task_state in (
1858                             task_states.DELETING, task_states.SOFT_DELETING)):
1859                         LOG.info(_LI('Instance is already in deleting state, '
1860                                      'ignoring this request'),
1861                                  instance=instance)
1862                         quotas.rollback()
1863                         return
1864                     self._record_action_start(context, instance,
1865                                               instance_actions.DELETE)
1866 
1867                     # NOTE(snikitin): If instance's vm_state is 'soft-delete',
1868                     # we should not count reservations here, because instance
1869                     # in soft-delete vm_state have already had quotas
1870                     # decremented. More details:
1871                     # https://bugs.launchpad.net/nova/+bug/1333145
1872                     if instance.vm_state == vm_states.SOFT_DELETED:
1873                         quotas.rollback()
1874 
1875                     cb(context, instance, bdms,
1876                        reservations=quotas.reservations)
1877             except exception.ComputeHostNotFound:
1878                 pass
1879 
1880             if is_local_delete:
1881                 # If instance is in shelved_offloaded state or compute node
1882                 # isn't up, delete instance from db and clean bdms info and
1883                 # network info
1884                 self._local_delete(context, instance, bdms, delete_type, cb)
1885                 quotas.commit()
1886 
1887         except exception.InstanceNotFound:
1888             # NOTE(comstud): Race condition. Instance already gone.
1889             if quotas:
1890                 quotas.rollback()
1891         except Exception:
1892             with excutils.save_and_reraise_exception():
1893                 if quotas:
1894                     quotas.rollback()
1895 
1896     def _confirm_resize_on_deleting(self, context, instance):
1897         # If in the middle of a resize, use confirm_resize to
1898         # ensure the original instance is cleaned up too
1899         migration = None
1900         for status in ('finished', 'confirming'):
1901             try:
1902                 migration = objects.Migration.get_by_instance_and_status(
1903                         context.elevated(), instance.uuid, status)
1904                 LOG.info(_LI('Found an unconfirmed migration during delete, '
1905                              'id: %(id)s, status: %(status)s'),
1906                          {'id': migration.id,
1907                           'status': migration.status},
1908                          instance=instance)
1909                 break
1910             except exception.MigrationNotFoundByStatus:
1911                 pass
1912 
1913         if not migration:
1914             LOG.info(_LI('Instance may have been confirmed during delete'),
1915                      instance=instance)
1916             return
1917 
1918         src_host = migration.source_compute
1919         # Call since this can race with the terminate_instance.
1920         # The resize is done but awaiting confirmation/reversion,
1921         # so there are two cases:
1922         # 1. up-resize: here -instance['vcpus'/'memory_mb'] match
1923         #    the quota usages accounted for this instance,
1924         #    so no further quota adjustment is needed
1925         # 2. down-resize: here -instance['vcpus'/'memory_mb'] are
1926         #    shy by delta(old, new) from the quota usages accounted
1927         #    for this instance, so we must adjust
1928         try:
1929             deltas = compute_utils.downsize_quota_delta(context, instance)
1930         except KeyError:
1931             LOG.info(_LI('Migration %s may have been confirmed during '
1932                          'delete'), migration.id, instance=instance)
1933             return
1934         quotas = compute_utils.reserve_quota_delta(context, deltas, instance)
1935 
1936         self._record_action_start(context, instance,
1937                                   instance_actions.CONFIRM_RESIZE)
1938 
1939         self.compute_rpcapi.confirm_resize(context,
1940                 instance, migration,
1941                 src_host, quotas.reservations,
1942                 cast=False)
1943 
1944     def _create_reservations(self, context, instance, original_task_state,
1945                              project_id, user_id):
1946         # NOTE(wangpan): if the instance is resizing, and the resources
1947         #                are updated to new instance type, we should use
1948         #                the old instance type to create reservation.
1949         # see https://bugs.launchpad.net/nova/+bug/1099729 for more details
1950         if original_task_state in (task_states.RESIZE_MIGRATED,
1951                                    task_states.RESIZE_FINISH):
1952             old_flavor = instance.old_flavor
1953             instance_vcpus = old_flavor.vcpus
1954             vram_mb = old_flavor.extra_specs.get('hw_video:ram_max_mb', 0)
1955             instance_memory_mb = old_flavor.memory_mb + vram_mb
1956         else:
1957             instance_vcpus = instance.flavor.vcpus
1958             instance_memory_mb = instance.flavor.memory_mb
1959 
1960         quotas = objects.Quotas(context=context)
1961         quotas.reserve(project_id=project_id,
1962                        user_id=user_id,
1963                        instances=-1,
1964                        cores=-instance_vcpus,
1965                        ram=-instance_memory_mb)
1966         return quotas
1967 
1968     def _get_stashed_volume_connector(self, bdm, instance):
1969         """Lookup a connector dict from the bdm.connection_info if set
1970 
1971         Gets the stashed connector dict out of the bdm.connection_info if set
1972         and the connector host matches the instance host.
1973 
1974         :param bdm: nova.objects.block_device.BlockDeviceMapping
1975         :param instance: nova.objects.instance.Instance
1976         :returns: volume connector dict or None
1977         """
1978         if 'connection_info' in bdm and bdm.connection_info is not None:
1979             # NOTE(mriedem): We didn't start stashing the connector in the
1980             # bdm.connection_info until Mitaka so it might not be there on old
1981             # attachments. Also, if the volume was attached when the instance
1982             # was in shelved_offloaded state and it hasn't been unshelved yet
1983             # we don't have the attachment/connection information either.
1984             connector = jsonutils.loads(bdm.connection_info).get('connector')
1985             if connector:
1986                 if connector.get('host') == instance.host:
1987                     return connector
1988                 LOG.debug('Found stashed volume connector for instance but '
1989                           'connector host %(connector_host)s does not match '
1990                           'the instance host %(instance_host)s.',
1991                           {'connector_host': connector.get('host'),
1992                            'instance_host': instance.host}, instance=instance)
1993 
1994     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
1995         """The method deletes the bdm records and, if a bdm is a volume, call
1996         the terminate connection and the detach volume via the Volume API.
1997         """
1998         elevated = context.elevated()
1999         for bdm in bdms:
2000             if bdm.is_volume:
2001                 try:
2002                     connector = self._get_stashed_volume_connector(
2003                         bdm, instance)
2004                     if connector:
2005                         self.volume_api.terminate_connection(context,
2006                                                              bdm.volume_id,
2007                                                              connector)
2008                     else:
2009                         LOG.debug('Unable to find connector for volume %s, '
2010                                   'not attempting terminate_connection.',
2011                                   bdm.volume_id, instance=instance)
2012                     # Attempt to detach the volume. If there was no connection
2013                     # made in the first place this is just cleaning up the
2014                     # volume state in the Cinder database.
2015                     self.volume_api.detach(elevated, bdm.volume_id,
2016                                            instance.uuid)
2017                     if bdm.delete_on_termination:
2018                         self.volume_api.delete(context, bdm.volume_id)
2019                 except Exception as exc:
2020                     err_str = _LW("Ignoring volume cleanup failure due to %s")
2021                     LOG.warning(err_str, exc, instance=instance)
2022             bdm.destroy()
2023 
2024     def _local_delete(self, context, instance, bdms, delete_type, cb):
2025         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2026             LOG.info(_LI("instance is in SHELVED_OFFLOADED state, cleanup"
2027                          " the instance's info from database."),
2028                      instance=instance)
2029         else:
2030             LOG.warning(_LW("instance's host %s is down, deleting from "
2031                             "database"), instance.host, instance=instance)
2032         compute_utils.notify_about_instance_usage(
2033             self.notifier, context, instance, "%s.start" % delete_type)
2034 
2035         elevated = context.elevated()
2036         if self.cell_type != 'api':
2037             # NOTE(liusheng): In nova-network multi_host scenario,deleting
2038             # network info of the instance may need instance['host'] as
2039             # destination host of RPC call. If instance in SHELVED_OFFLOADED
2040             # state, instance['host'] is None, here, use shelved_host as host
2041             # to deallocate network info and reset instance['host'] after that.
2042             # Here we shouldn't use instance.save(), because this will mislead
2043             # user who may think the instance's host has been changed, and
2044             # actually, the instance.host is always None.
2045             orig_host = instance.host
2046             try:
2047                 if instance.vm_state == vm_states.SHELVED_OFFLOADED:
2048                     sysmeta = getattr(instance,
2049                                       obj_base.get_attrname('system_metadata'))
2050                     instance.host = sysmeta.get('shelved_host')
2051                 self.network_api.deallocate_for_instance(elevated,
2052                                                          instance)
2053             finally:
2054                 instance.host = orig_host
2055 
2056         # cleanup volumes
2057         self._local_cleanup_bdm_volumes(bdms, instance, context)
2058         cb(context, instance, bdms, local=True)
2059         sys_meta = instance.system_metadata
2060         instance.destroy()
2061         compute_utils.notify_about_instance_usage(
2062             self.notifier, context, instance, "%s.end" % delete_type,
2063             system_metadata=sys_meta)
2064 
2065     def _do_delete(self, context, instance, bdms, reservations=None,
2066                    local=False):
2067         if local:
2068             instance.vm_state = vm_states.DELETED
2069             instance.task_state = None
2070             instance.terminated_at = timeutils.utcnow()
2071             instance.save()
2072         else:
2073             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2074                                                    reservations=reservations,
2075                                                    delete_type='delete')
2076 
2077     def _do_force_delete(self, context, instance, bdms, reservations=None,
2078                          local=False):
2079         if local:
2080             instance.vm_state = vm_states.DELETED
2081             instance.task_state = None
2082             instance.terminated_at = timeutils.utcnow()
2083             instance.save()
2084         else:
2085             self.compute_rpcapi.terminate_instance(context, instance, bdms,
2086                                                    reservations=reservations,
2087                                                    delete_type='force_delete')
2088 
2089     def _do_soft_delete(self, context, instance, bdms, reservations=None,
2090                         local=False):
2091         if local:
2092             instance.vm_state = vm_states.SOFT_DELETED
2093             instance.task_state = None
2094             instance.terminated_at = timeutils.utcnow()
2095             instance.save()
2096         else:
2097             self.compute_rpcapi.soft_delete_instance(context, instance,
2098                                                      reservations=reservations)
2099 
2100     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
2101     @check_instance_lock
2102     @check_instance_cell
2103     @check_instance_state(vm_state=None, task_state=None,
2104                           must_have_launched=True)
2105     def soft_delete(self, context, instance):
2106         """Terminate an instance."""
2107         LOG.debug('Going to try to soft delete instance',
2108                   instance=instance)
2109 
2110         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
2111                      task_state=task_states.SOFT_DELETING,
2112                      deleted_at=timeutils.utcnow())
2113 
2114     def _delete_instance(self, context, instance):
2115         self._delete(context, instance, 'delete', self._do_delete,
2116                      task_state=task_states.DELETING)
2117 
2118     @check_instance_lock
2119     @check_instance_cell
2120     @check_instance_state(vm_state=None, task_state=None,
2121                           must_have_launched=False)
2122     def delete(self, context, instance):
2123         """Terminate an instance."""
2124         LOG.debug("Going to try to terminate instance", instance=instance)
2125         self._delete_instance(context, instance)
2126 
2127     @check_instance_lock
2128     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
2129     def restore(self, context, instance):
2130         """Restore a previously deleted (but not reclaimed) instance."""
2131         # Reserve quotas
2132         flavor = instance.get_flavor()
2133         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
2134         num_instances, quotas = self._check_num_instances_quota(
2135                 context, flavor, 1, 1,
2136                 project_id=project_id, user_id=user_id)
2137 
2138         self._record_action_start(context, instance, instance_actions.RESTORE)
2139 
2140         try:
2141             if instance.host:
2142                 instance.task_state = task_states.RESTORING
2143                 instance.deleted_at = None
2144                 instance.save(expected_task_state=[None])
2145                 self.compute_rpcapi.restore_instance(context, instance)
2146             else:
2147                 instance.vm_state = vm_states.ACTIVE
2148                 instance.task_state = None
2149                 instance.deleted_at = None
2150                 instance.save(expected_task_state=[None])
2151 
2152             quotas.commit()
2153         except Exception:
2154             with excutils.save_and_reraise_exception():
2155                 quotas.rollback()
2156 
2157     @check_instance_lock
2158     @check_instance_state(task_state=None, must_have_launched=False)
2159     def force_delete(self, context, instance):
2160         """Force delete an instance in any vm_state/task_state."""
2161         self._delete(context, instance, 'force_delete', self._do_force_delete,
2162                      task_state=task_states.FORCE_DELETING)
2163 
2164     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2165         LOG.debug("Going to try to stop instance", instance=instance)
2166 
2167         instance.task_state = task_states.POWERING_OFF
2168         instance.progress = 0
2169         instance.save(expected_task_state=[None])
2170 
2171         self._record_action_start(context, instance, instance_actions.STOP)
2172 
2173         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2174                                           clean_shutdown=clean_shutdown)
2175 
2176     @check_instance_lock
2177     @check_instance_host
2178     @check_instance_cell
2179     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2180     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2181         """Stop an instance."""
2182         self.force_stop(context, instance, do_cast, clean_shutdown)
2183 
2184     @check_instance_lock
2185     @check_instance_host
2186     @check_instance_cell
2187     @check_instance_state(vm_state=[vm_states.STOPPED])
2188     def start(self, context, instance):
2189         """Start an instance."""
2190         LOG.debug("Going to try to start instance", instance=instance)
2191 
2192         instance.task_state = task_states.POWERING_ON
2193         instance.save(expected_task_state=[None])
2194 
2195         self._record_action_start(context, instance, instance_actions.START)
2196         # TODO(yamahata): injected_files isn't supported right now.
2197         #                 It is used only for osapi. not for ec2 api.
2198         #                 availability_zone isn't used by run_instance.
2199         self.compute_rpcapi.start_instance(context, instance)
2200 
2201     @check_instance_lock
2202     @check_instance_host
2203     @check_instance_cell
2204     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2205     def trigger_crash_dump(self, context, instance):
2206         """Trigger crash dump in an instance."""
2207         LOG.debug("Try to trigger crash dump", instance=instance)
2208 
2209         self._record_action_start(context, instance,
2210                                   instance_actions.TRIGGER_CRASH_DUMP)
2211 
2212         self.compute_rpcapi.trigger_crash_dump(context, instance)
2213 
2214     def _get_instance_map_or_none(self, context, instance_uuid):
2215         try:
2216             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2217                     context, instance_uuid)
2218         except exception.InstanceMappingNotFound:
2219             # InstanceMapping should always be found generally. This exception
2220             # may be raised if a deployment has partially migrated the nova-api
2221             # services.
2222             inst_map = None
2223         return inst_map
2224 
2225     def _get_instance(self, context, instance_uuid, expected_attrs):
2226         # Before service version 15 the BuildRequest is not cleaned up during
2227         # a delete request so there is no reason to look it up here as we can't
2228         # trust that it's not referencing a deleted instance. Also even if
2229         # there is an instance mapping we don't need to honor it for older
2230         # service versions.
2231         service_version = objects.Service.get_minimum_version(
2232             context, 'nova-osapi_compute')
2233         if service_version < 15:
2234             return objects.Instance.get_by_uuid(context, instance_uuid,
2235                                                 expected_attrs=expected_attrs)
2236         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2237         if inst_map and (inst_map.cell_mapping is not None):
2238             with nova_context.target_cell(context, inst_map.cell_mapping):
2239                 instance = objects.Instance.get_by_uuid(
2240                     context, instance_uuid, expected_attrs=expected_attrs)
2241         elif inst_map and (inst_map.cell_mapping is None):
2242             # This means the instance has not been scheduled and put in
2243             # a cell yet. For now it also may mean that the deployer
2244             # has not created their cell(s) yet.
2245             try:
2246                 build_req = objects.BuildRequest.get_by_instance_uuid(
2247                         context, instance_uuid)
2248                 instance = build_req.instance
2249             except exception.BuildRequestNotFound:
2250                 # Instance was mapped and the BuildRequest was deleted
2251                 # while fetching. Try again.
2252                 inst_map = self._get_instance_map_or_none(context,
2253                                                           instance_uuid)
2254                 if inst_map and (inst_map.cell_mapping is not None):
2255                     with nova_context.target_cell(context,
2256                                                   inst_map.cell_mapping):
2257                         instance = objects.Instance.get_by_uuid(
2258                             context, instance_uuid,
2259                             expected_attrs=expected_attrs)
2260                 else:
2261                     # If BuildRequest is not found but inst_map.cell_mapping
2262                     # does not point at a cell then cell migration has not
2263                     # happened yet. This will be a failure case later.
2264                     # TODO(alaski): Make this a failure case after we put in
2265                     # a block that requires migrating to cellsv2.
2266                     instance = objects.Instance.get_by_uuid(
2267                         context, instance_uuid, expected_attrs=expected_attrs)
2268         else:
2269             # This should not happen once a deployment has migrated to cellsv2.
2270             # If it happens after that point we handle it gracefully for now
2271             # but this will become an exception in the future.
2272             # TODO(alaski): Once devstack is setting up cellsv2 by default add
2273             # a warning log message that this will become an exception in the
2274             # future. The warning message will be conditional upon the
2275             # migration having happened, which means a db lookup to check that.
2276             instance = objects.Instance.get_by_uuid(
2277                 context, instance_uuid, expected_attrs=expected_attrs)
2278 
2279         return instance
2280 
2281     def get(self, context, instance_id, expected_attrs=None):
2282         """Get a single instance with the given instance_id."""
2283         if not expected_attrs:
2284             expected_attrs = []
2285         expected_attrs.extend(['metadata', 'system_metadata',
2286                                'security_groups', 'info_cache'])
2287         # NOTE(ameade): we still need to support integer ids for ec2
2288         try:
2289             if uuidutils.is_uuid_like(instance_id):
2290                 LOG.debug("Fetching instance by UUID",
2291                            instance_uuid=instance_id)
2292 
2293                 instance = self._get_instance(context, instance_id,
2294                                               expected_attrs)
2295             else:
2296                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2297                 raise exception.InstanceNotFound(instance_id=instance_id)
2298         except exception.InvalidID:
2299             LOG.debug("Invalid instance id %s", instance_id)
2300             raise exception.InstanceNotFound(instance_id=instance_id)
2301 
2302         return instance
2303 
2304     def get_all(self, context, search_opts=None, limit=None, marker=None,
2305                 expected_attrs=None, sort_keys=None, sort_dirs=None):
2306         """Get all instances filtered by one of the given parameters.
2307 
2308         If there is no filter and the context is an admin, it will retrieve
2309         all instances in the system.
2310 
2311         Deleted instances will be returned by default, unless there is a
2312         search option that says otherwise.
2313 
2314         The results will be sorted based on the list of sort keys in the
2315         'sort_keys' parameter (first value is primary sort key, second value is
2316         secondary sort ket, etc.). For each sort key, the associated sort
2317         direction is based on the list of sort directions in the 'sort_dirs'
2318         parameter.
2319         """
2320         if search_opts is None:
2321             search_opts = {}
2322 
2323         LOG.debug("Searching by: %s", str(search_opts))
2324 
2325         # Fixups for the DB call
2326         filters = {}
2327 
2328         def _remap_flavor_filter(flavor_id):
2329             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2330             filters['instance_type_id'] = flavor.id
2331 
2332         def _remap_fixed_ip_filter(fixed_ip):
2333             # Turn fixed_ip into a regexp match. Since '.' matches
2334             # any character, we need to use regexp escaping for it.
2335             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2336 
2337         def _remap_metadata_filter(metadata):
2338             filters['metadata'] = jsonutils.loads(metadata)
2339 
2340         def _remap_system_metadata_filter(metadata):
2341             filters['system_metadata'] = jsonutils.loads(metadata)
2342 
2343         # search_option to filter_name mapping.
2344         filter_mapping = {
2345                 'image': 'image_ref',
2346                 'name': 'display_name',
2347                 'tenant_id': 'project_id',
2348                 'flavor': _remap_flavor_filter,
2349                 'fixed_ip': _remap_fixed_ip_filter,
2350                 'metadata': _remap_metadata_filter,
2351                 'system_metadata': _remap_system_metadata_filter}
2352 
2353         # copy from search_opts, doing various remappings as necessary
2354         for opt, value in six.iteritems(search_opts):
2355             # Do remappings.
2356             # Values not in the filter_mapping table are copied as-is.
2357             # If remapping is None, option is not copied
2358             # If the remapping is a string, it is the filter_name to use
2359             try:
2360                 remap_object = filter_mapping[opt]
2361             except KeyError:
2362                 filters[opt] = value
2363             else:
2364                 # Remaps are strings to translate to, or functions to call
2365                 # to do the translating as defined by the table above.
2366                 if isinstance(remap_object, six.string_types):
2367                     filters[remap_object] = value
2368                 else:
2369                     try:
2370                         remap_object(value)
2371 
2372                     # We already know we can't match the filter, so
2373                     # return an empty list
2374                     except ValueError:
2375                         return objects.InstanceList()
2376 
2377         # IP address filtering cannot be applied at the DB layer, remove any DB
2378         # limit so that it can be applied after the IP filter.
2379         filter_ip = 'ip6' in filters or 'ip' in filters
2380         orig_limit = limit
2381         if filter_ip and limit:
2382             LOG.debug('Removing limit for DB query due to IP filter')
2383             limit = None
2384 
2385         # The ordering of instances will be
2386         # [sorted instances with no host] + [sorted instances with host].
2387         # This means BuildRequest and cell0 instances first, then cell
2388         # instances
2389         build_requests = objects.BuildRequestList.get_by_filters(
2390             context, filters, limit=limit, marker=marker, sort_keys=sort_keys,
2391             sort_dirs=sort_dirs)
2392         build_req_instances = objects.InstanceList(
2393             objects=[build_req.instance for build_req in build_requests])
2394         # Only subtract from limit if it is not None
2395         limit = (limit - len(build_req_instances)) if limit else limit
2396 
2397         try:
2398             cell0_mapping = objects.CellMapping.get_by_uuid(context,
2399                 objects.CellMapping.CELL0_UUID)
2400         except exception.CellMappingNotFound:
2401             cell0_instances = objects.InstanceList(objects=[])
2402         else:
2403             with nova_context.target_cell(context, cell0_mapping):
2404                 cell0_instances = self._get_instances_by_filters(
2405                     context, filters, limit=limit, marker=marker,
2406                     expected_attrs=expected_attrs, sort_keys=sort_keys,
2407                     sort_dirs=sort_dirs)
2408         # Only subtract from limit if it is not None
2409         limit = (limit - len(cell0_instances)) if limit else limit
2410 
2411         # There is only planned support for a single cell here. Multiple cell
2412         # instance lists should be proxied to project Searchlight, or a similar
2413         # alternative.
2414         if limit is None or limit > 0:
2415             cell_instances = self._get_instances_by_filters(context, filters,
2416                     limit=limit, marker=marker, expected_attrs=expected_attrs,
2417                     sort_keys=sort_keys, sort_dirs=sort_dirs)
2418         else:
2419             cell_instances = objects.InstanceList(objects=[])
2420 
2421         def _get_unique_filter_method():
2422             seen_uuids = set()
2423 
2424             def _filter(instance):
2425                 if instance.uuid in seen_uuids:
2426                     return False
2427                 seen_uuids.add(instance.uuid)
2428                 return True
2429 
2430             return _filter
2431 
2432         filter_method = _get_unique_filter_method()
2433         # TODO(alaski): Clean up the objects concatenation when List objects
2434         # support it natively.
2435         instances = objects.InstanceList(
2436             objects=list(filter(filter_method,
2437                            build_req_instances.objects +
2438                            cell0_instances.objects +
2439                            cell_instances.objects)))
2440 
2441         if filter_ip:
2442             instances = self._ip_filter(instances, filters, orig_limit)
2443 
2444         return instances
2445 
2446     @staticmethod
2447     def _ip_filter(inst_models, filters, limit):
2448         ipv4_f = re.compile(str(filters.get('ip')))
2449         ipv6_f = re.compile(str(filters.get('ip6')))
2450 
2451         def _match_instance(instance):
2452             nw_info = compute_utils.get_nw_info_for_instance(instance)
2453             for vif in nw_info:
2454                 for fixed_ip in vif.fixed_ips():
2455                     address = fixed_ip.get('address')
2456                     if not address:
2457                         continue
2458                     version = fixed_ip.get('version')
2459                     if ((version == 4 and ipv4_f.match(address)) or
2460                         (version == 6 and ipv6_f.match(address))):
2461                         return True
2462             return False
2463 
2464         result_objs = []
2465         for instance in inst_models:
2466             if _match_instance(instance):
2467                 result_objs.append(instance)
2468                 if limit and len(result_objs) == limit:
2469                     break
2470         return objects.InstanceList(objects=result_objs)
2471 
2472     def _get_instances_by_filters(self, context, filters,
2473                                   limit=None, marker=None, expected_attrs=None,
2474                                   sort_keys=None, sort_dirs=None):
2475         fields = ['metadata', 'system_metadata', 'info_cache',
2476                   'security_groups']
2477         if expected_attrs:
2478             fields.extend(expected_attrs)
2479         return objects.InstanceList.get_by_filters(
2480             context, filters=filters, limit=limit, marker=marker,
2481             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2482 
2483     def update_instance(self, context, instance, updates):
2484         """Updates a single Instance object with some updates dict.
2485 
2486         Returns the updated instance.
2487         """
2488 
2489         # NOTE(sbauza): Given we only persist the Instance object after we
2490         # create the BuildRequest, we are sure that if the Instance object
2491         # has an ID field set, then it was persisted in the right Cell DB.
2492         if instance.obj_attr_is_set('id'):
2493             instance.update(updates)
2494             # Instance has been scheduled and the BuildRequest has been deleted
2495             # we can directly write the update down to the right cell.
2496             inst_map = self._get_instance_map_or_none(context, instance.uuid)
2497             if inst_map and (inst_map.cell_mapping is not None):
2498                 with nova_context.target_cell(context, inst_map.cell_mapping):
2499                     instance.save()
2500             else:
2501                 # If inst_map.cell_mapping does not point at a cell then cell
2502                 # migration has not happened yet.
2503                 # TODO(alaski): Make this a failure case after we put in
2504                 # a block that requires migrating to cellsv2.
2505                 instance.save()
2506         else:
2507             # Instance is not yet mapped to a cell, so we need to update
2508             # BuildRequest instead
2509             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2510             # could be deleted because of either a concurrent instance delete
2511             # or because the scheduler just returned a destination right
2512             # after we called the instance in the API.
2513             try:
2514                 build_req = objects.BuildRequest.get_by_instance_uuid(
2515                     context, instance.uuid)
2516                 instance = build_req.instance
2517                 instance.update(updates)
2518                 # FIXME(sbauza): Here we are updating the current
2519                 # thread-related BuildRequest object. Given that another worker
2520                 # could have looking up at that BuildRequest in the API, it
2521                 # means that it could pass it down to the conductor without
2522                 # making sure that it's not updated, we could have some race
2523                 # condition where it would missing the updated fields, but
2524                 # that's something we could discuss once the instance record
2525                 # is persisted by the conductor.
2526                 build_req.save()
2527             except exception.BuildRequestNotFound:
2528                 # Instance was mapped and the BuildRequest was deleted
2529                 # while fetching (and possibly the instance could have been
2530                 # deleted as well). We need to lookup again the Instance object
2531                 # in order to correctly update it.
2532                 # TODO(sbauza): Figure out a good way to know the expected
2533                 # attributes by checking which fields are set or not.
2534                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2535                                   'tags', 'metadata', 'system_metadata',
2536                                   'security_groups', 'info_cache']
2537                 inst_map = self._get_instance_map_or_none(context,
2538                                                           instance.uuid)
2539                 if inst_map and (inst_map.cell_mapping is not None):
2540                     with nova_context.target_cell(context,
2541                                                   inst_map.cell_mapping):
2542                         instance = objects.Instance.get_by_uuid(
2543                             context, instance.uuid,
2544                             expected_attrs=expected_attrs)
2545                         instance.update(updates)
2546                         instance.save()
2547                 else:
2548                     # If inst_map.cell_mapping does not point at a cell then
2549                     # cell migration has not happened yet.
2550                     # TODO(alaski): Make this a failure case after we put in
2551                     # a block that requires migrating to cellsv2.
2552                     instance = objects.Instance.get_by_uuid(
2553                         context, instance.uuid, expected_attrs=expected_attrs)
2554                     instance.update(updates)
2555                     instance.save()
2556         return instance
2557 
2558     # NOTE(melwitt): We don't check instance lock for backup because lock is
2559     #                intended to prevent accidental change/delete of instances
2560     @check_instance_cell
2561     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2562                                     vm_states.PAUSED, vm_states.SUSPENDED])
2563     def backup(self, context, instance, name, backup_type, rotation,
2564                extra_properties=None):
2565         """Backup the given instance
2566 
2567         :param instance: nova.objects.instance.Instance object
2568         :param name: name of the backup
2569         :param backup_type: 'daily' or 'weekly'
2570         :param rotation: int representing how many backups to keep around;
2571             None if rotation shouldn't be used (as in the case of snapshots)
2572         :param extra_properties: dict of extra image properties to include
2573                                  when creating the image.
2574         :returns: A dict containing image metadata
2575         """
2576         props_copy = dict(extra_properties, backup_type=backup_type)
2577 
2578         if compute_utils.is_volume_backed_instance(context, instance):
2579             LOG.info(_LI("It's not supported to backup volume backed "
2580                          "instance."), instance=instance)
2581             raise exception.InvalidRequest()
2582         else:
2583             image_meta = self._create_image(context, instance,
2584                                             name, 'backup',
2585                                             extra_properties=props_copy)
2586 
2587         # NOTE(comstud): Any changes to this method should also be made
2588         # to the backup_instance() method in nova/cells/messaging.py
2589 
2590         instance.task_state = task_states.IMAGE_BACKUP
2591         instance.save(expected_task_state=[None])
2592 
2593         self.compute_rpcapi.backup_instance(context, instance,
2594                                             image_meta['id'],
2595                                             backup_type,
2596                                             rotation)
2597         return image_meta
2598 
2599     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2600     #                intended to prevent accidental change/delete of instances
2601     @check_instance_cell
2602     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2603                                     vm_states.PAUSED, vm_states.SUSPENDED])
2604     def snapshot(self, context, instance, name, extra_properties=None):
2605         """Snapshot the given instance.
2606 
2607         :param instance: nova.objects.instance.Instance object
2608         :param name: name of the snapshot
2609         :param extra_properties: dict of extra image properties to include
2610                                  when creating the image.
2611         :returns: A dict containing image metadata
2612         """
2613         image_meta = self._create_image(context, instance, name,
2614                                         'snapshot',
2615                                         extra_properties=extra_properties)
2616 
2617         # NOTE(comstud): Any changes to this method should also be made
2618         # to the snapshot_instance() method in nova/cells/messaging.py
2619         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2620         try:
2621             instance.save(expected_task_state=[None])
2622         except (exception.InstanceNotFound,
2623                 exception.UnexpectedDeletingTaskStateError) as ex:
2624             # Changing the instance task state to use in raising the
2625             # InstanceInvalidException below
2626             LOG.debug('Instance disappeared during snapshot.',
2627                       instance=instance)
2628             try:
2629                 image_id = image_meta['id']
2630                 self.image_api.delete(context, image_id)
2631                 LOG.info(_LI('Image %s deleted because instance '
2632                              'deleted before snapshot started.'),
2633                          image_id, instance=instance)
2634             except exception.ImageNotFound:
2635                 pass
2636             except Exception as exc:
2637                 msg = _LW("Error while trying to clean up image %(img_id)s: "
2638                           "%(error_msg)s")
2639                 LOG.warning(msg, {"img_id": image_meta['id'],
2640                                   "error_msg": six.text_type(exc)})
2641             attr = 'task_state'
2642             state = task_states.DELETING
2643             if type(ex) == exception.InstanceNotFound:
2644                 attr = 'vm_state'
2645                 state = vm_states.DELETED
2646             raise exception.InstanceInvalidState(attr=attr,
2647                                            instance_uuid=instance.uuid,
2648                                            state=state,
2649                                            method='snapshot')
2650 
2651         self.compute_rpcapi.snapshot_instance(context, instance,
2652                                               image_meta['id'])
2653 
2654         return image_meta
2655 
2656     def _create_image(self, context, instance, name, image_type,
2657                       extra_properties=None):
2658         """Create new image entry in the image service.  This new image
2659         will be reserved for the compute manager to upload a snapshot
2660         or backup.
2661 
2662         :param context: security context
2663         :param instance: nova.objects.instance.Instance object
2664         :param name: string for name of the snapshot
2665         :param image_type: snapshot | backup
2666         :param extra_properties: dict of extra image properties to include
2667 
2668         """
2669         properties = {
2670             'instance_uuid': instance.uuid,
2671             'user_id': str(context.user_id),
2672             'image_type': image_type,
2673         }
2674         properties.update(extra_properties or {})
2675 
2676         image_meta = self._initialize_instance_snapshot_metadata(
2677             instance, name, properties)
2678         # if we're making a snapshot, omit the disk and container formats,
2679         # since the image may have been converted to another format, and the
2680         # original values won't be accurate.  The driver will populate these
2681         # with the correct values later, on image upload.
2682         if image_type == 'snapshot':
2683             image_meta.pop('disk_format', None)
2684             image_meta.pop('container_format', None)
2685         return self.image_api.create(context, image_meta)
2686 
2687     def _initialize_instance_snapshot_metadata(self, instance, name,
2688                                                extra_properties=None):
2689         """Initialize new metadata for a snapshot of the given instance.
2690 
2691         :param instance: nova.objects.instance.Instance object
2692         :param name: string for name of the snapshot
2693         :param extra_properties: dict of extra metadata properties to include
2694 
2695         :returns: the new instance snapshot metadata
2696         """
2697         image_meta = utils.get_image_from_system_metadata(
2698             instance.system_metadata)
2699         image_meta.update({'name': name,
2700                            'is_public': False})
2701 
2702         # Delete properties that are non-inheritable
2703         properties = image_meta['properties']
2704         for key in CONF.non_inheritable_image_properties:
2705             properties.pop(key, None)
2706 
2707         # The properties in extra_properties have precedence
2708         properties.update(extra_properties or {})
2709 
2710         return image_meta
2711 
2712     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2713     #                intended to prevent accidental change/delete of instances
2714     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2715                                     vm_states.SUSPENDED])
2716     def snapshot_volume_backed(self, context, instance, name,
2717                                extra_properties=None):
2718         """Snapshot the given volume-backed instance.
2719 
2720         :param instance: nova.objects.instance.Instance object
2721         :param name: name of the backup or snapshot
2722         :param extra_properties: dict of extra image properties to include
2723 
2724         :returns: the new image metadata
2725         """
2726         image_meta = self._initialize_instance_snapshot_metadata(
2727             instance, name, extra_properties)
2728         # the new image is simply a bucket of properties (particularly the
2729         # block device mapping, kernel and ramdisk IDs) with no image data,
2730         # hence the zero size
2731         image_meta['size'] = 0
2732         for attr in ('container_format', 'disk_format'):
2733             image_meta.pop(attr, None)
2734         properties = image_meta['properties']
2735         # clean properties before filling
2736         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
2737             properties.pop(key, None)
2738         if instance.root_device_name:
2739             properties['root_device_name'] = instance.root_device_name
2740 
2741         quiesced = False
2742         if instance.vm_state == vm_states.ACTIVE:
2743             try:
2744                 self.compute_rpcapi.quiesce_instance(context, instance)
2745                 quiesced = True
2746             except (exception.InstanceQuiesceNotSupported,
2747                     exception.QemuGuestAgentNotEnabled,
2748                     exception.NovaException, NotImplementedError) as err:
2749                 if strutils.bool_from_string(instance.system_metadata.get(
2750                         'image_os_require_quiesce')):
2751                     raise
2752                 else:
2753                     LOG.info(_LI('Skipping quiescing instance: '
2754                                  '%(reason)s.'), {'reason': err},
2755                              instance=instance)
2756 
2757         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2758                 context, instance.uuid)
2759 
2760         mapping = []
2761         for bdm in bdms:
2762             if bdm.no_device:
2763                 continue
2764 
2765             if bdm.is_volume:
2766                 # create snapshot based on volume_id
2767                 volume = self.volume_api.get(context, bdm.volume_id)
2768                 # NOTE(yamahata): Should we wait for snapshot creation?
2769                 #                 Linux LVM snapshot creation completes in
2770                 #                 short time, it doesn't matter for now.
2771                 name = _('snapshot for %s') % image_meta['name']
2772                 LOG.debug('Creating snapshot from volume %s.', volume['id'],
2773                           instance=instance)
2774                 snapshot = self.volume_api.create_snapshot_force(
2775                     context, volume['id'], name, volume['display_description'])
2776                 mapping_dict = block_device.snapshot_from_bdm(snapshot['id'],
2777                                                               bdm)
2778                 mapping_dict = mapping_dict.get_image_mapping()
2779             else:
2780                 mapping_dict = bdm.get_image_mapping()
2781 
2782             mapping.append(mapping_dict)
2783 
2784         if quiesced:
2785             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
2786 
2787         if mapping:
2788             properties['block_device_mapping'] = mapping
2789             properties['bdm_v2'] = True
2790 
2791         return self.image_api.create(context, image_meta)
2792 
2793     @check_instance_lock
2794     def reboot(self, context, instance, reboot_type):
2795         """Reboot the given instance."""
2796         if reboot_type == 'SOFT':
2797             self._soft_reboot(context, instance)
2798         else:
2799             self._hard_reboot(context, instance)
2800 
2801     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
2802                           task_state=[None])
2803     def _soft_reboot(self, context, instance):
2804         expected_task_state = [None]
2805         instance.task_state = task_states.REBOOTING
2806         instance.save(expected_task_state=expected_task_state)
2807 
2808         self._record_action_start(context, instance, instance_actions.REBOOT)
2809 
2810         self.compute_rpcapi.reboot_instance(context, instance=instance,
2811                                             block_device_info=None,
2812                                             reboot_type='SOFT')
2813 
2814     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
2815                           task_state=task_states.ALLOW_REBOOT)
2816     def _hard_reboot(self, context, instance):
2817         instance.task_state = task_states.REBOOTING_HARD
2818         expected_task_state = [None,
2819                                task_states.REBOOTING,
2820                                task_states.REBOOT_PENDING,
2821                                task_states.REBOOT_STARTED,
2822                                task_states.REBOOTING_HARD,
2823                                task_states.RESUMING,
2824                                task_states.UNPAUSING,
2825                                task_states.SUSPENDING]
2826         instance.save(expected_task_state = expected_task_state)
2827 
2828         self._record_action_start(context, instance, instance_actions.REBOOT)
2829 
2830         self.compute_rpcapi.reboot_instance(context, instance=instance,
2831                                             block_device_info=None,
2832                                             reboot_type='HARD')
2833 
2834     @check_instance_lock
2835     @check_instance_cell
2836     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2837                                     vm_states.ERROR])
2838     def rebuild(self, context, instance, image_href, admin_password,
2839                 files_to_inject=None, **kwargs):
2840         """Rebuild the given instance with the provided attributes."""
2841         orig_image_ref = instance.image_ref or ''
2842         files_to_inject = files_to_inject or []
2843         metadata = kwargs.get('metadata', {})
2844         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
2845         auto_disk_config = kwargs.get('auto_disk_config')
2846 
2847         image_id, image = self._get_image(context, image_href)
2848         self._check_auto_disk_config(image=image, **kwargs)
2849 
2850         flavor = instance.get_flavor()
2851         root_bdm = compute_utils.get_root_bdm(context, instance)
2852         self._checks_for_create_and_rebuild(context, image_id, image,
2853                 flavor, metadata, files_to_inject, root_bdm)
2854 
2855         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
2856                 context, None, None, image)
2857 
2858         def _reset_image_metadata():
2859             """Remove old image properties that we're storing as instance
2860             system metadata.  These properties start with 'image_'.
2861             Then add the properties for the new image.
2862             """
2863             # FIXME(comstud): There's a race condition here in that if
2864             # the system_metadata for this instance is updated after
2865             # we do the previous save() and before we update.. those
2866             # other updates will be lost. Since this problem exists in
2867             # a lot of other places, I think it should be addressed in
2868             # a DB layer overhaul.
2869 
2870             orig_sys_metadata = dict(instance.system_metadata)
2871             # Remove the old keys
2872             for key in list(instance.system_metadata.keys()):
2873                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
2874                     del instance.system_metadata[key]
2875 
2876             # Add the new ones
2877             new_sys_metadata = utils.get_system_metadata_from_image(
2878                 image, flavor)
2879 
2880             instance.system_metadata.update(new_sys_metadata)
2881             instance.save()
2882             return orig_sys_metadata
2883 
2884         # Since image might have changed, we may have new values for
2885         # os_type, vm_mode, etc
2886         options_from_image = self._inherit_properties_from_image(
2887                 image, auto_disk_config)
2888         instance.update(options_from_image)
2889 
2890         instance.task_state = task_states.REBUILDING
2891         instance.image_ref = image_href
2892         instance.kernel_id = kernel_id or ""
2893         instance.ramdisk_id = ramdisk_id or ""
2894         instance.progress = 0
2895         instance.update(kwargs)
2896         instance.save(expected_task_state=[None])
2897 
2898         # On a rebuild, since we're potentially changing images, we need to
2899         # wipe out the old image properties that we're storing as instance
2900         # system metadata... and copy in the properties for the new image.
2901         orig_sys_metadata = _reset_image_metadata()
2902 
2903         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2904                 context, instance.uuid)
2905 
2906         self._record_action_start(context, instance, instance_actions.REBUILD)
2907 
2908         # NOTE(sbauza): The migration script we provided in Newton should make
2909         # sure that all our instances are currently migrated to have an
2910         # attached RequestSpec object but let's consider that the operator only
2911         # half migrated all their instances in the meantime.
2912         try:
2913             request_spec = objects.RequestSpec.get_by_instance_uuid(
2914                 context, instance.uuid)
2915         except exception.RequestSpecNotFound:
2916             # Some old instances can still have no RequestSpec object attached
2917             # to them, we need to support the old way
2918             request_spec = None
2919 
2920         self.compute_task_api.rebuild_instance(context, instance=instance,
2921                 new_pass=admin_password, injected_files=files_to_inject,
2922                 image_ref=image_href, orig_image_ref=orig_image_ref,
2923                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
2924                 preserve_ephemeral=preserve_ephemeral, host=instance.host,
2925                 request_spec=request_spec,
2926                 kwargs=kwargs)
2927 
2928     @check_instance_lock
2929     @check_instance_cell
2930     @check_instance_state(vm_state=[vm_states.RESIZED])
2931     def revert_resize(self, context, instance):
2932         """Reverts a resize, deleting the 'new' instance in the process."""
2933         elevated = context.elevated()
2934         migration = objects.Migration.get_by_instance_and_status(
2935             elevated, instance.uuid, 'finished')
2936 
2937         # reverse quota reservation for increased resource usage
2938         deltas = compute_utils.reverse_upsize_quota_delta(context, instance)
2939         quotas = compute_utils.reserve_quota_delta(context, deltas, instance)
2940 
2941         instance.task_state = task_states.RESIZE_REVERTING
2942         try:
2943             instance.save(expected_task_state=[None])
2944         except Exception:
2945             with excutils.save_and_reraise_exception():
2946                 quotas.rollback()
2947 
2948         migration.status = 'reverting'
2949         migration.save()
2950         # With cells, the best we can do right now is commit the reservations
2951         # immediately...
2952         if CONF.cells.enable:
2953             quotas.commit()
2954 
2955         self._record_action_start(context, instance,
2956                                   instance_actions.REVERT_RESIZE)
2957 
2958         self.compute_rpcapi.revert_resize(context, instance,
2959                                           migration,
2960                                           migration.dest_compute,
2961                                           quotas.reservations or [])
2962 
2963     @check_instance_lock
2964     @check_instance_cell
2965     @check_instance_state(vm_state=[vm_states.RESIZED])
2966     def confirm_resize(self, context, instance, migration=None):
2967         """Confirms a migration/resize and deletes the 'old' instance."""
2968         elevated = context.elevated()
2969         if migration is None:
2970             migration = objects.Migration.get_by_instance_and_status(
2971                 elevated, instance.uuid, 'finished')
2972 
2973         # reserve quota only for any decrease in resource usage
2974         deltas = compute_utils.downsize_quota_delta(context, instance)
2975         quotas = compute_utils.reserve_quota_delta(context, deltas, instance)
2976 
2977         migration.status = 'confirming'
2978         migration.save()
2979         # With cells, the best we can do right now is commit the reservations
2980         # immediately...
2981         if CONF.cells.enable:
2982             quotas.commit()
2983 
2984         self._record_action_start(context, instance,
2985                                   instance_actions.CONFIRM_RESIZE)
2986 
2987         self.compute_rpcapi.confirm_resize(context,
2988                                            instance,
2989                                            migration,
2990                                            migration.source_compute,
2991                                            quotas.reservations or [])
2992 
2993     @staticmethod
2994     def _resize_cells_support(context, quotas, instance,
2995                               current_instance_type, new_instance_type):
2996         """Special API cell logic for resize."""
2997         # With cells, the best we can do right now is commit the
2998         # reservations immediately...
2999         quotas.commit()
3000         # NOTE(johannes/comstud): The API cell needs a local migration
3001         # record for later resize_confirm and resize_reverts to deal
3002         # with quotas.  We don't need source and/or destination
3003         # information, just the old and new flavors. Status is set to
3004         # 'finished' since nothing else will update the status along
3005         # the way.
3006         mig = objects.Migration(context=context.elevated())
3007         mig.instance_uuid = instance.uuid
3008         mig.old_instance_type_id = current_instance_type['id']
3009         mig.new_instance_type_id = new_instance_type['id']
3010         mig.status = 'finished'
3011         mig.migration_type = (
3012             mig.old_instance_type_id != mig.new_instance_type_id and
3013             'resize' or 'migration')
3014         mig.create()
3015 
3016     @check_instance_lock
3017     @check_instance_cell
3018     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
3019     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
3020                **extra_instance_updates):
3021         """Resize (ie, migrate) a running instance.
3022 
3023         If flavor_id is None, the process is considered a migration, keeping
3024         the original flavor_id. If flavor_id is not None, the instance should
3025         be migrated to a new host and resized to the new flavor_id.
3026         """
3027         self._check_auto_disk_config(instance, **extra_instance_updates)
3028 
3029         current_instance_type = instance.get_flavor()
3030 
3031         # If flavor_id is not provided, only migrate the instance.
3032         if not flavor_id:
3033             LOG.debug("flavor_id is None. Assuming migration.",
3034                       instance=instance)
3035             new_instance_type = current_instance_type
3036         else:
3037             new_instance_type = flavors.get_flavor_by_flavor_id(
3038                     flavor_id, read_deleted="no")
3039             if (new_instance_type.get('root_gb') == 0 and
3040                 current_instance_type.get('root_gb') != 0 and
3041                 not compute_utils.is_volume_backed_instance(context,
3042                     instance)):
3043                 reason = _('Resize to zero disk flavor is not allowed.')
3044                 raise exception.CannotResizeDisk(reason=reason)
3045 
3046         if not new_instance_type:
3047             raise exception.FlavorNotFound(flavor_id=flavor_id)
3048 
3049         current_instance_type_name = current_instance_type['name']
3050         new_instance_type_name = new_instance_type['name']
3051         LOG.debug("Old instance type %(current_instance_type_name)s, "
3052                   "new instance type %(new_instance_type_name)s",
3053                   {'current_instance_type_name': current_instance_type_name,
3054                    'new_instance_type_name': new_instance_type_name},
3055                   instance=instance)
3056 
3057         same_instance_type = (current_instance_type['id'] ==
3058                               new_instance_type['id'])
3059 
3060         # NOTE(sirp): We don't want to force a customer to change their flavor
3061         # when Ops is migrating off of a failed host.
3062         if not same_instance_type and new_instance_type.get('disabled'):
3063             raise exception.FlavorNotFound(flavor_id=flavor_id)
3064 
3065         if same_instance_type and flavor_id and self.cell_type != 'compute':
3066             raise exception.CannotResizeToSameFlavor()
3067 
3068         # ensure there is sufficient headroom for upsizes
3069         if flavor_id:
3070             deltas = compute_utils.upsize_quota_delta(context,
3071                                                       new_instance_type,
3072                                                       current_instance_type)
3073             try:
3074                 quotas = compute_utils.reserve_quota_delta(context, deltas,
3075                                                            instance)
3076             except exception.OverQuota as exc:
3077                 quotas = exc.kwargs['quotas']
3078                 overs = exc.kwargs['overs']
3079                 usages = exc.kwargs['usages']
3080                 headroom = self._get_headroom(quotas, usages, deltas)
3081                 (overs, reqs, total_alloweds,
3082                  useds) = self._get_over_quota_detail(headroom, overs, quotas,
3083                                                       deltas)
3084                 LOG.warning(_LW("%(overs)s quota exceeded for %(pid)s,"
3085                                 " tried to resize instance."),
3086                             {'overs': overs, 'pid': context.project_id})
3087                 raise exception.TooManyInstances(overs=overs,
3088                                                  req=reqs,
3089                                                  used=useds,
3090                                                  allowed=total_alloweds)
3091         else:
3092             quotas = objects.Quotas(context=context)
3093 
3094         instance.task_state = task_states.RESIZE_PREP
3095         instance.progress = 0
3096         instance.update(extra_instance_updates)
3097         instance.save(expected_task_state=[None])
3098 
3099         filter_properties = {'ignore_hosts': []}
3100 
3101         if not CONF.allow_resize_to_same_host:
3102             filter_properties['ignore_hosts'].append(instance.host)
3103 
3104         if self.cell_type == 'api':
3105             # Commit reservations early and create migration record.
3106             self._resize_cells_support(context, quotas, instance,
3107                                        current_instance_type,
3108                                        new_instance_type)
3109 
3110         if not flavor_id:
3111             self._record_action_start(context, instance,
3112                                       instance_actions.MIGRATE)
3113         else:
3114             self._record_action_start(context, instance,
3115                                       instance_actions.RESIZE)
3116 
3117         # NOTE(sbauza): The migration script we provided in Newton should make
3118         # sure that all our instances are currently migrated to have an
3119         # attached RequestSpec object but let's consider that the operator only
3120         # half migrated all their instances in the meantime.
3121         try:
3122             request_spec = objects.RequestSpec.get_by_instance_uuid(
3123                 context, instance.uuid)
3124             request_spec.ignore_hosts = filter_properties['ignore_hosts']
3125         except exception.RequestSpecNotFound:
3126             # Some old instances can still have no RequestSpec object attached
3127             # to them, we need to support the old way
3128             request_spec = None
3129 
3130         scheduler_hint = {'filter_properties': filter_properties}
3131         self.compute_task_api.resize_instance(context, instance,
3132                 extra_instance_updates, scheduler_hint=scheduler_hint,
3133                 flavor=new_instance_type,
3134                 reservations=quotas.reservations or [],
3135                 clean_shutdown=clean_shutdown,
3136                 request_spec=request_spec)
3137 
3138     @check_instance_lock
3139     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3140                                     vm_states.PAUSED, vm_states.SUSPENDED])
3141     def shelve(self, context, instance, clean_shutdown=True):
3142         """Shelve an instance.
3143 
3144         Shuts down an instance and frees it up to be removed from the
3145         hypervisor.
3146         """
3147         instance.task_state = task_states.SHELVING
3148         instance.save(expected_task_state=[None])
3149 
3150         self._record_action_start(context, instance, instance_actions.SHELVE)
3151 
3152         if not compute_utils.is_volume_backed_instance(context, instance):
3153             name = '%s-shelved' % instance.display_name
3154             image_meta = self._create_image(context, instance, name,
3155                     'snapshot')
3156             image_id = image_meta['id']
3157             self.compute_rpcapi.shelve_instance(context, instance=instance,
3158                     image_id=image_id, clean_shutdown=clean_shutdown)
3159         else:
3160             self.compute_rpcapi.shelve_offload_instance(context,
3161                     instance=instance, clean_shutdown=clean_shutdown)
3162 
3163     @check_instance_lock
3164     @check_instance_state(vm_state=[vm_states.SHELVED])
3165     def shelve_offload(self, context, instance, clean_shutdown=True):
3166         """Remove a shelved instance from the hypervisor."""
3167         instance.task_state = task_states.SHELVING_OFFLOADING
3168         instance.save(expected_task_state=[None])
3169 
3170         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3171             clean_shutdown=clean_shutdown)
3172 
3173     @check_instance_lock
3174     @check_instance_state(vm_state=[vm_states.SHELVED,
3175         vm_states.SHELVED_OFFLOADED])
3176     def unshelve(self, context, instance):
3177         """Restore a shelved instance."""
3178         instance.task_state = task_states.UNSHELVING
3179         instance.save(expected_task_state=[None])
3180 
3181         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3182 
3183         try:
3184             request_spec = objects.RequestSpec.get_by_instance_uuid(
3185                 context, instance.uuid)
3186         except exception.RequestSpecNotFound:
3187             # Some old instances can still have no RequestSpec object attached
3188             # to them, we need to support the old way
3189             request_spec = None
3190         self.compute_task_api.unshelve_instance(context, instance,
3191                                                 request_spec)
3192 
3193     @check_instance_lock
3194     def add_fixed_ip(self, context, instance, network_id):
3195         """Add fixed_ip from specified network to given instance."""
3196         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3197                 instance=instance, network_id=network_id)
3198 
3199     @check_instance_lock
3200     def remove_fixed_ip(self, context, instance, address):
3201         """Remove fixed_ip from specified network to given instance."""
3202         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3203                 instance=instance, address=address)
3204 
3205     @check_instance_lock
3206     @check_instance_cell
3207     @check_instance_state(vm_state=[vm_states.ACTIVE])
3208     def pause(self, context, instance):
3209         """Pause the given instance."""
3210         instance.task_state = task_states.PAUSING
3211         instance.save(expected_task_state=[None])
3212         self._record_action_start(context, instance, instance_actions.PAUSE)
3213         self.compute_rpcapi.pause_instance(context, instance)
3214 
3215     @check_instance_lock
3216     @check_instance_cell
3217     @check_instance_state(vm_state=[vm_states.PAUSED])
3218     def unpause(self, context, instance):
3219         """Unpause the given instance."""
3220         instance.task_state = task_states.UNPAUSING
3221         instance.save(expected_task_state=[None])
3222         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3223         self.compute_rpcapi.unpause_instance(context, instance)
3224 
3225     @check_instance_host
3226     def get_diagnostics(self, context, instance):
3227         """Retrieve diagnostics for the given instance."""
3228         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3229 
3230     @check_instance_host
3231     def get_instance_diagnostics(self, context, instance):
3232         """Retrieve diagnostics for the given instance."""
3233         return self.compute_rpcapi.get_instance_diagnostics(context,
3234                                                             instance=instance)
3235 
3236     @check_instance_lock
3237     @check_instance_cell
3238     @check_instance_state(vm_state=[vm_states.ACTIVE])
3239     def suspend(self, context, instance):
3240         """Suspend the given instance."""
3241         instance.task_state = task_states.SUSPENDING
3242         instance.save(expected_task_state=[None])
3243         self._record_action_start(context, instance, instance_actions.SUSPEND)
3244         self.compute_rpcapi.suspend_instance(context, instance)
3245 
3246     @check_instance_lock
3247     @check_instance_cell
3248     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3249     def resume(self, context, instance):
3250         """Resume the given instance."""
3251         instance.task_state = task_states.RESUMING
3252         instance.save(expected_task_state=[None])
3253         self._record_action_start(context, instance, instance_actions.RESUME)
3254         self.compute_rpcapi.resume_instance(context, instance)
3255 
3256     @check_instance_lock
3257     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3258                                     vm_states.ERROR])
3259     def rescue(self, context, instance, rescue_password=None,
3260                rescue_image_ref=None, clean_shutdown=True):
3261         """Rescue the given instance."""
3262 
3263         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3264                     context, instance.uuid)
3265         for bdm in bdms:
3266             if bdm.volume_id:
3267                 vol = self.volume_api.get(context, bdm.volume_id)
3268                 self.volume_api.check_attached(context, vol)
3269         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3270             reason = _("Cannot rescue a volume-backed instance")
3271             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3272                                                  reason=reason)
3273 
3274         instance.task_state = task_states.RESCUING
3275         instance.save(expected_task_state=[None])
3276 
3277         self._record_action_start(context, instance, instance_actions.RESCUE)
3278 
3279         self.compute_rpcapi.rescue_instance(context, instance=instance,
3280             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3281             clean_shutdown=clean_shutdown)
3282 
3283     @check_instance_lock
3284     @check_instance_state(vm_state=[vm_states.RESCUED])
3285     def unrescue(self, context, instance):
3286         """Unrescue the given instance."""
3287         instance.task_state = task_states.UNRESCUING
3288         instance.save(expected_task_state=[None])
3289 
3290         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3291 
3292         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3293 
3294     @check_instance_lock
3295     @check_instance_cell
3296     @check_instance_state(vm_state=[vm_states.ACTIVE])
3297     def set_admin_password(self, context, instance, password=None):
3298         """Set the root/admin password for the given instance.
3299 
3300         @param context: Nova auth context.
3301         @param instance: Nova instance object.
3302         @param password: The admin password for the instance.
3303         """
3304         instance.task_state = task_states.UPDATING_PASSWORD
3305         instance.save(expected_task_state=[None])
3306 
3307         self._record_action_start(context, instance,
3308                                   instance_actions.CHANGE_PASSWORD)
3309 
3310         self.compute_rpcapi.set_admin_password(context,
3311                                                instance=instance,
3312                                                new_pass=password)
3313 
3314     @check_instance_host
3315     def get_vnc_console(self, context, instance, console_type):
3316         """Get a url to an instance Console."""
3317         connect_info = self.compute_rpcapi.get_vnc_console(context,
3318                 instance=instance, console_type=console_type)
3319 
3320         self.consoleauth_rpcapi.authorize_console(context,
3321                 connect_info['token'], console_type,
3322                 connect_info['host'], connect_info['port'],
3323                 connect_info['internal_access_path'], instance.uuid,
3324                 access_url=connect_info['access_url'])
3325 
3326         return {'url': connect_info['access_url']}
3327 
3328     @check_instance_host
3329     def get_vnc_connect_info(self, context, instance, console_type):
3330         """Used in a child cell to get console info."""
3331         connect_info = self.compute_rpcapi.get_vnc_console(context,
3332                 instance=instance, console_type=console_type)
3333         return connect_info
3334 
3335     @check_instance_host
3336     def get_spice_console(self, context, instance, console_type):
3337         """Get a url to an instance Console."""
3338         connect_info = self.compute_rpcapi.get_spice_console(context,
3339                 instance=instance, console_type=console_type)
3340         self.consoleauth_rpcapi.authorize_console(context,
3341                 connect_info['token'], console_type,
3342                 connect_info['host'], connect_info['port'],
3343                 connect_info['internal_access_path'], instance.uuid,
3344                 access_url=connect_info['access_url'])
3345 
3346         return {'url': connect_info['access_url']}
3347 
3348     @check_instance_host
3349     def get_spice_connect_info(self, context, instance, console_type):
3350         """Used in a child cell to get console info."""
3351         connect_info = self.compute_rpcapi.get_spice_console(context,
3352                 instance=instance, console_type=console_type)
3353         return connect_info
3354 
3355     @check_instance_host
3356     def get_rdp_console(self, context, instance, console_type):
3357         """Get a url to an instance Console."""
3358         connect_info = self.compute_rpcapi.get_rdp_console(context,
3359                 instance=instance, console_type=console_type)
3360         self.consoleauth_rpcapi.authorize_console(context,
3361                 connect_info['token'], console_type,
3362                 connect_info['host'], connect_info['port'],
3363                 connect_info['internal_access_path'], instance.uuid,
3364                 access_url=connect_info['access_url'])
3365 
3366         return {'url': connect_info['access_url']}
3367 
3368     @check_instance_host
3369     def get_rdp_connect_info(self, context, instance, console_type):
3370         """Used in a child cell to get console info."""
3371         connect_info = self.compute_rpcapi.get_rdp_console(context,
3372                 instance=instance, console_type=console_type)
3373         return connect_info
3374 
3375     @check_instance_host
3376     def get_serial_console(self, context, instance, console_type):
3377         """Get a url to a serial console."""
3378         connect_info = self.compute_rpcapi.get_serial_console(context,
3379                 instance=instance, console_type=console_type)
3380 
3381         self.consoleauth_rpcapi.authorize_console(context,
3382                 connect_info['token'], console_type,
3383                 connect_info['host'], connect_info['port'],
3384                 connect_info['internal_access_path'], instance.uuid,
3385                 access_url=connect_info['access_url'])
3386         return {'url': connect_info['access_url']}
3387 
3388     @check_instance_host
3389     def get_serial_console_connect_info(self, context, instance, console_type):
3390         """Used in a child cell to get serial console."""
3391         connect_info = self.compute_rpcapi.get_serial_console(context,
3392                 instance=instance, console_type=console_type)
3393         return connect_info
3394 
3395     @check_instance_host
3396     def get_mks_console(self, context, instance, console_type):
3397         """Get a url to a MKS console."""
3398         connect_info = self.compute_rpcapi.get_mks_console(context,
3399                 instance=instance, console_type=console_type)
3400         self.consoleauth_rpcapi.authorize_console(context,
3401                 connect_info['token'], console_type,
3402                 connect_info['host'], connect_info['port'],
3403                 connect_info['internal_access_path'], instance.uuid,
3404                 access_url=connect_info['access_url'])
3405         return {'url': connect_info['access_url']}
3406 
3407     @check_instance_host
3408     def get_console_output(self, context, instance, tail_length=None):
3409         """Get console output for an instance."""
3410         return self.compute_rpcapi.get_console_output(context,
3411                 instance=instance, tail_length=tail_length)
3412 
3413     def lock(self, context, instance):
3414         """Lock the given instance."""
3415         # Only update the lock if we are an admin (non-owner)
3416         is_owner = instance.project_id == context.project_id
3417         if instance.locked and is_owner:
3418             return
3419 
3420         context = context.elevated()
3421         LOG.debug('Locking', instance=instance)
3422         instance.locked = True
3423         instance.locked_by = 'owner' if is_owner else 'admin'
3424         instance.save()
3425 
3426     def is_expected_locked_by(self, context, instance):
3427         is_owner = instance.project_id == context.project_id
3428         expect_locked_by = 'owner' if is_owner else 'admin'
3429         locked_by = instance.locked_by
3430         if locked_by and locked_by != expect_locked_by:
3431             return False
3432         return True
3433 
3434     def unlock(self, context, instance):
3435         """Unlock the given instance."""
3436         context = context.elevated()
3437         LOG.debug('Unlocking', instance=instance)
3438         instance.locked = False
3439         instance.locked_by = None
3440         instance.save()
3441 
3442     @check_instance_lock
3443     @check_instance_cell
3444     def reset_network(self, context, instance):
3445         """Reset networking on the instance."""
3446         self.compute_rpcapi.reset_network(context, instance=instance)
3447 
3448     @check_instance_lock
3449     @check_instance_cell
3450     def inject_network_info(self, context, instance):
3451         """Inject network info for the instance."""
3452         self.compute_rpcapi.inject_network_info(context, instance=instance)
3453 
3454     def _create_volume_bdm(self, context, instance, device, volume_id,
3455                            disk_bus, device_type, is_local_creation=False):
3456         if is_local_creation:
3457             # when the creation is done locally we can't specify the device
3458             # name as we do not have a way to check that the name specified is
3459             # a valid one.
3460             # We leave the setting of that value when the actual attach
3461             # happens on the compute manager
3462             volume_bdm = objects.BlockDeviceMapping(
3463                 context=context,
3464                 source_type='volume', destination_type='volume',
3465                 instance_uuid=instance.uuid, boot_index=None,
3466                 volume_id=volume_id,
3467                 device_name=None, guest_format=None,
3468                 disk_bus=disk_bus, device_type=device_type)
3469             volume_bdm.create()
3470         else:
3471             # NOTE(vish): This is done on the compute host because we want
3472             #             to avoid a race where two devices are requested at
3473             #             the same time. When db access is removed from
3474             #             compute, the bdm will be created here and we will
3475             #             have to make sure that they are assigned atomically.
3476             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3477                 context, instance, device, volume_id, disk_bus=disk_bus,
3478                 device_type=device_type)
3479         return volume_bdm
3480 
3481     def _check_attach_and_reserve_volume(self, context, volume_id, instance):
3482         volume = self.volume_api.get(context, volume_id)
3483         self.volume_api.check_availability_zone(context, volume,
3484                                                 instance=instance)
3485         self.volume_api.reserve_volume(context, volume_id)
3486 
3487     def _attach_volume(self, context, instance, volume_id, device,
3488                        disk_bus, device_type):
3489         """Attach an existing volume to an existing instance.
3490 
3491         This method is separated to make it possible for cells version
3492         to override it.
3493         """
3494         volume_bdm = self._create_volume_bdm(
3495             context, instance, device, volume_id, disk_bus=disk_bus,
3496             device_type=device_type)
3497         try:
3498             self._check_attach_and_reserve_volume(context, volume_id, instance)
3499             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
3500         except Exception:
3501             with excutils.save_and_reraise_exception():
3502                 volume_bdm.destroy()
3503 
3504         return volume_bdm.device_name
3505 
3506     def _attach_volume_shelved_offloaded(self, context, instance, volume_id,
3507                                          device, disk_bus, device_type):
3508         """Attach an existing volume to an instance in shelved offloaded state.
3509 
3510         Attaching a volume for an instance in shelved offloaded state requires
3511         to perform the regular check to see if we can attach and reserve the
3512         volume then we need to call the attach method on the volume API
3513         to mark the volume as 'in-use'.
3514         The instance at this stage is not managed by a compute manager
3515         therefore the actual attachment will be performed once the
3516         instance will be unshelved.
3517         """
3518 
3519         volume_bdm = self._create_volume_bdm(
3520             context, instance, device, volume_id, disk_bus=disk_bus,
3521             device_type=device_type, is_local_creation=True)
3522         try:
3523             self._check_attach_and_reserve_volume(context, volume_id, instance)
3524             self.volume_api.attach(context,
3525                                    volume_id,
3526                                    instance.uuid,
3527                                    device)
3528         except Exception:
3529             with excutils.save_and_reraise_exception():
3530                 volume_bdm.destroy()
3531 
3532         return volume_bdm.device_name
3533 
3534     @check_instance_lock
3535     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3536                                     vm_states.STOPPED, vm_states.RESIZED,
3537                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3538                                     vm_states.SHELVED_OFFLOADED])
3539     def attach_volume(self, context, instance, volume_id, device=None,
3540                        disk_bus=None, device_type=None):
3541         """Attach an existing volume to an existing instance."""
3542         # NOTE(vish): Fail fast if the device is not going to pass. This
3543         #             will need to be removed along with the test if we
3544         #             change the logic in the manager for what constitutes
3545         #             a valid device.
3546         if device and not block_device.match_device(device):
3547             raise exception.InvalidDevicePath(path=device)
3548 
3549         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
3550         if is_shelved_offloaded:
3551             return self._attach_volume_shelved_offloaded(context,
3552                                                          instance,
3553                                                          volume_id,
3554                                                          device,
3555                                                          disk_bus,
3556                                                          device_type)
3557 
3558         return self._attach_volume(context, instance, volume_id, device,
3559                                    disk_bus, device_type)
3560 
3561     def _check_and_begin_detach(self, context, volume, instance):
3562         self.volume_api.check_detach(context, volume, instance=instance)
3563         self.volume_api.begin_detaching(context, volume['id'])
3564 
3565     def _detach_volume(self, context, instance, volume):
3566         """Detach volume from instance.
3567 
3568         This method is separated to make it easier for cells version
3569         to override.
3570         """
3571         self._check_and_begin_detach(context, volume, instance)
3572         attachments = volume.get('attachments', {})
3573         attachment_id = None
3574         if attachments and instance.uuid in attachments:
3575             attachment_id = attachments[instance.uuid]['attachment_id']
3576         self.compute_rpcapi.detach_volume(context, instance=instance,
3577                 volume_id=volume['id'], attachment_id=attachment_id)
3578 
3579     def _detach_volume_shelved_offloaded(self, context, instance, volume):
3580         """Detach a volume from an instance in shelved offloaded state.
3581 
3582         If the instance is shelved offloaded we just need to cleanup volume
3583         calling the volume api detach, the volume api terminate_connection
3584         and delete the bdm record.
3585         If the volume has delete_on_termination option set then we call the
3586         volume api delete as well.
3587         """
3588         self._check_and_begin_detach(context, volume, instance)
3589         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
3590                 context, volume['id'], instance.uuid)]
3591         self._local_cleanup_bdm_volumes(bdms, instance, context)
3592 
3593     @check_instance_lock
3594     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3595                                     vm_states.STOPPED, vm_states.RESIZED,
3596                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3597                                     vm_states.SHELVED_OFFLOADED])
3598     def detach_volume(self, context, instance, volume):
3599         """Detach a volume from an instance."""
3600         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
3601             self._detach_volume_shelved_offloaded(context, instance, volume)
3602         else:
3603             self._detach_volume(context, instance, volume)
3604 
3605     @check_instance_lock
3606     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3607                                     vm_states.SUSPENDED, vm_states.STOPPED,
3608                                     vm_states.RESIZED, vm_states.SOFT_DELETED])
3609     def swap_volume(self, context, instance, old_volume, new_volume):
3610         """Swap volume attached to an instance."""
3611         if old_volume['attach_status'] == 'detached':
3612             raise exception.VolumeUnattached(volume_id=old_volume['id'])
3613         # The caller likely got the instance from volume['attachments']
3614         # in the first place, but let's sanity check.
3615         if not old_volume.get('attachments', {}).get(instance.uuid):
3616             msg = _("Old volume is attached to a different instance.")
3617             raise exception.InvalidVolume(reason=msg)
3618         if new_volume['attach_status'] == 'attached':
3619             msg = _("New volume must be detached in order to swap.")
3620             raise exception.InvalidVolume(reason=msg)
3621         if int(new_volume['size']) < int(old_volume['size']):
3622             msg = _("New volume must be the same size or larger.")
3623             raise exception.InvalidVolume(reason=msg)
3624         self.volume_api.check_detach(context, old_volume)
3625         self.volume_api.check_attach(context, new_volume, instance=instance)
3626         self.volume_api.begin_detaching(context, old_volume['id'])
3627         self.volume_api.reserve_volume(context, new_volume['id'])
3628         try:
3629             self.compute_rpcapi.swap_volume(
3630                     context, instance=instance,
3631                     old_volume_id=old_volume['id'],
3632                     new_volume_id=new_volume['id'])
3633         except Exception:
3634             with excutils.save_and_reraise_exception():
3635                 self.volume_api.roll_detaching(context, old_volume['id'])
3636                 self.volume_api.unreserve_volume(context, new_volume['id'])
3637 
3638     @check_instance_lock
3639     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3640                                     vm_states.STOPPED],
3641                           task_state=[None])
3642     def attach_interface(self, context, instance, network_id, port_id,
3643                          requested_ip):
3644         """Use hotplug to add an network adapter to an instance."""
3645         return self.compute_rpcapi.attach_interface(context,
3646             instance=instance, network_id=network_id, port_id=port_id,
3647             requested_ip=requested_ip)
3648 
3649     @check_instance_lock
3650     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3651                                     vm_states.STOPPED],
3652                           task_state=[None])
3653     def detach_interface(self, context, instance, port_id):
3654         """Detach an network adapter from an instance."""
3655         self.compute_rpcapi.detach_interface(context, instance=instance,
3656             port_id=port_id)
3657 
3658     def get_instance_metadata(self, context, instance):
3659         """Get all metadata associated with an instance."""
3660         return self.db.instance_metadata_get(context, instance.uuid)
3661 
3662     def get_all_instance_metadata(self, context, search_filts):
3663         return self._get_all_instance_metadata(
3664             context, search_filts, metadata_type='metadata')
3665 
3666     def get_all_system_metadata(self, context, search_filts):
3667         return self._get_all_instance_metadata(
3668             context, search_filts, metadata_type='system_metadata')
3669 
3670     def _get_all_instance_metadata(self, context, search_filts, metadata_type):
3671         """Get all metadata."""
3672         instances = self._get_instances_by_filters(context, filters={},
3673                                                    sort_keys=['created_at'],
3674                                                    sort_dirs=['desc'])
3675         return utils.filter_and_format_resource_metadata('instance', instances,
3676                 search_filts, metadata_type)
3677 
3678     @check_instance_lock
3679     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3680                                     vm_states.SUSPENDED, vm_states.STOPPED],
3681                           task_state=None)
3682     def delete_instance_metadata(self, context, instance, key):
3683         """Delete the given metadata item from an instance."""
3684         instance.delete_metadata_key(key)
3685         self.compute_rpcapi.change_instance_metadata(context,
3686                                                      instance=instance,
3687                                                      diff={key: ['-']})
3688 
3689     @check_instance_lock
3690     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3691                                     vm_states.SUSPENDED, vm_states.STOPPED],
3692                           task_state=None)
3693     def update_instance_metadata(self, context, instance,
3694                                  metadata, delete=False):
3695         """Updates or creates instance metadata.
3696 
3697         If delete is True, metadata items that are not specified in the
3698         `metadata` argument will be deleted.
3699 
3700         """
3701         orig = dict(instance.metadata)
3702         if delete:
3703             _metadata = metadata
3704         else:
3705             _metadata = dict(instance.metadata)
3706             _metadata.update(metadata)
3707 
3708         self._check_metadata_properties_quota(context, _metadata)
3709         instance.metadata = _metadata
3710         instance.save()
3711         diff = _diff_dict(orig, instance.metadata)
3712         self.compute_rpcapi.change_instance_metadata(context,
3713                                                      instance=instance,
3714                                                      diff=diff)
3715         return _metadata
3716 
3717     @check_instance_lock
3718     @check_instance_cell
3719     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
3720     def live_migrate(self, context, instance, block_migration,
3721                      disk_over_commit, host_name, force=None, async=False):
3722         """Migrate a server lively to a new host."""
3723         LOG.debug("Going to try to live migrate instance to %s",
3724                   host_name or "another host", instance=instance)
3725 
3726         instance.task_state = task_states.MIGRATING
3727         instance.save(expected_task_state=[None])
3728 
3729         self._record_action_start(context, instance,
3730                                   instance_actions.LIVE_MIGRATION)
3731         try:
3732             request_spec = objects.RequestSpec.get_by_instance_uuid(
3733                 context, instance.uuid)
3734         except exception.RequestSpecNotFound:
3735             # Some old instances can still have no RequestSpec object attached
3736             # to them, we need to support the old way
3737             request_spec = None
3738 
3739         # NOTE(sbauza): Force is a boolean by the new related API version
3740         if force is False and host_name:
3741             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
3742             # NOTE(sbauza): Unset the host to make sure we call the scheduler
3743             host_name = None
3744             # FIXME(sbauza): Since only Ironic driver uses more than one
3745             # compute per service but doesn't support evacuations,
3746             # let's provide the first one.
3747             target = nodes[0]
3748             if request_spec:
3749                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
3750                 # having a request spec attached to them (particularly true for
3751                 # cells v1). For the moment, let's keep the same behaviour for
3752                 # all the instances but provide the destination only if a spec
3753                 # is found.
3754                 destination = objects.Destination(
3755                     host=target.host,
3756                     node=target.hypervisor_hostname
3757                 )
3758                 request_spec.requested_destination = destination
3759 
3760         try:
3761             self.compute_task_api.live_migrate_instance(context, instance,
3762                 host_name, block_migration=block_migration,
3763                 disk_over_commit=disk_over_commit,
3764                 request_spec=request_spec, async=async)
3765         except oslo_exceptions.MessagingTimeout as messaging_timeout:
3766             with excutils.save_and_reraise_exception():
3767                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
3768                 # occurs, but LM will still be in progress, so write
3769                 # instance fault to database
3770                 compute_utils.add_instance_fault_from_exc(context,
3771                                                           instance,
3772                                                           messaging_timeout)
3773 
3774     @check_instance_lock
3775     @check_instance_cell
3776     @check_instance_state(vm_state=[vm_states.ACTIVE],
3777                           task_state=[task_states.MIGRATING])
3778     def live_migrate_force_complete(self, context, instance, migration_id):
3779         """Force live migration to complete.
3780 
3781         :param context: Security context
3782         :param instance: The instance that is being migrated
3783         :param migration_id: ID of ongoing migration
3784 
3785         """
3786         LOG.debug("Going to try to force live migration to complete",
3787                   instance=instance)
3788 
3789         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
3790         # live migration for particular instance. Also pass migration id to
3791         # compute to double check and avoid possible race condition.
3792         migration = objects.Migration.get_by_id_and_instance(
3793             context, migration_id, instance.uuid)
3794         if migration.status != 'running':
3795             raise exception.InvalidMigrationState(migration_id=migration_id,
3796                                                   instance_uuid=instance.uuid,
3797                                                   state=migration.status,
3798                                                   method='force complete')
3799 
3800         self._record_action_start(
3801             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
3802 
3803         self.compute_rpcapi.live_migration_force_complete(
3804             context, instance, migration)
3805 
3806     @check_instance_lock
3807     @check_instance_cell
3808     @check_instance_state(task_state=[task_states.MIGRATING])
3809     def live_migrate_abort(self, context, instance, migration_id):
3810         """Abort an in-progress live migration.
3811 
3812         :param context: Security context
3813         :param instance: The instance that is being migrated
3814         :param migration_id: ID of in-progress live migration
3815 
3816         """
3817         migration = objects.Migration.get_by_id_and_instance(context,
3818                     migration_id, instance.uuid)
3819         LOG.debug("Going to cancel live migration %s",
3820                   migration.id, instance=instance)
3821 
3822         if migration.status != 'running':
3823             raise exception.InvalidMigrationState(migration_id=migration_id,
3824                     instance_uuid=instance.uuid,
3825                     state=migration.status,
3826                     method='abort live migration')
3827         self._record_action_start(context, instance,
3828                                   instance_actions.LIVE_MIGRATION_CANCEL)
3829 
3830         self.compute_rpcapi.live_migration_abort(context,
3831                 instance, migration.id)
3832 
3833     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3834                                     vm_states.ERROR])
3835     def evacuate(self, context, instance, host, on_shared_storage,
3836                  admin_password=None, force=None):
3837         """Running evacuate to target host.
3838 
3839         Checking vm compute host state, if the host not in expected_state,
3840         raising an exception.
3841 
3842         :param instance: The instance to evacuate
3843         :param host: Target host. if not set, the scheduler will pick up one
3844         :param on_shared_storage: True if instance files on shared storage
3845         :param admin_password: password to set on rebuilt instance
3846         :param force: Force the evacuation to the specific host target
3847 
3848         """
3849         LOG.debug('vm evacuation scheduled', instance=instance)
3850         inst_host = instance.host
3851         service = objects.Service.get_by_compute_host(context, inst_host)
3852         if self.servicegroup_api.service_is_up(service):
3853             LOG.error(_LE('Instance compute service state on %s '
3854                           'expected to be down, but it was up.'), inst_host)
3855             raise exception.ComputeServiceInUse(host=inst_host)
3856 
3857         instance.task_state = task_states.REBUILDING
3858         instance.save(expected_task_state=[None])
3859         self._record_action_start(context, instance, instance_actions.EVACUATE)
3860 
3861         # NOTE(danms): Create this as a tombstone for the source compute
3862         # to find and cleanup. No need to pass it anywhere else.
3863         migration = objects.Migration(context,
3864                                       source_compute=instance.host,
3865                                       source_node=instance.node,
3866                                       instance_uuid=instance.uuid,
3867                                       status='accepted',
3868                                       migration_type='evacuation')
3869         if host:
3870             migration.dest_compute = host
3871         migration.create()
3872 
3873         compute_utils.notify_about_instance_usage(
3874             self.notifier, context, instance, "evacuate")
3875 
3876         try:
3877             request_spec = objects.RequestSpec.get_by_instance_uuid(
3878                 context, instance.uuid)
3879         except exception.RequestSpecNotFound:
3880             # Some old instances can still have no RequestSpec object attached
3881             # to them, we need to support the old way
3882             request_spec = None
3883 
3884         # NOTE(sbauza): Force is a boolean by the new related API version
3885         if force is False and host:
3886             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
3887             # NOTE(sbauza): Unset the host to make sure we call the scheduler
3888             host = None
3889             # FIXME(sbauza): Since only Ironic driver uses more than one
3890             # compute per service but doesn't support evacuations,
3891             # let's provide the first one.
3892             target = nodes[0]
3893             if request_spec:
3894                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
3895                 # having a request spec attached to them (particularly true for
3896                 # cells v1). For the moment, let's keep the same behaviour for
3897                 # all the instances but provide the destination only if a spec
3898                 # is found.
3899                 destination = objects.Destination(
3900                     host=target.host,
3901                     node=target.hypervisor_hostname
3902                 )
3903                 request_spec.requested_destination = destination
3904 
3905         return self.compute_task_api.rebuild_instance(context,
3906                        instance=instance,
3907                        new_pass=admin_password,
3908                        injected_files=None,
3909                        image_ref=None,
3910                        orig_image_ref=None,
3911                        orig_sys_metadata=None,
3912                        bdms=None,
3913                        recreate=True,
3914                        on_shared_storage=on_shared_storage,
3915                        host=host,
3916                        request_spec=request_spec,
3917                        )
3918 
3919     def get_migrations(self, context, filters):
3920         """Get all migrations for the given filters."""
3921         return objects.MigrationList.get_by_filters(context, filters)
3922 
3923     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
3924                                                migration_type=None):
3925         """Get all migrations of an instance in progress."""
3926         return objects.MigrationList.get_in_progress_by_instance(
3927                 context, instance_uuid, migration_type)
3928 
3929     def get_migration_by_id_and_instance(self, context,
3930                                          migration_id, instance_uuid):
3931         """Get the migration of an instance by id."""
3932         return objects.Migration.get_by_id_and_instance(
3933                 context, migration_id, instance_uuid)
3934 
3935     def volume_snapshot_create(self, context, volume_id, create_info):
3936         bdm = objects.BlockDeviceMapping.get_by_volume(
3937                 context, volume_id, expected_attrs=['instance'])
3938         self.compute_rpcapi.volume_snapshot_create(context, bdm.instance,
3939                 volume_id, create_info)
3940         snapshot = {
3941             'snapshot': {
3942                 'id': create_info.get('id'),
3943                 'volumeId': volume_id
3944             }
3945         }
3946         return snapshot
3947 
3948     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
3949                                delete_info):
3950         bdm = objects.BlockDeviceMapping.get_by_volume(
3951                 context, volume_id, expected_attrs=['instance'])
3952         self.compute_rpcapi.volume_snapshot_delete(context, bdm.instance,
3953                 volume_id, snapshot_id, delete_info)
3954 
3955     def external_instance_event(self, context, instances, events):
3956         # NOTE(danms): The external API consumer just provides events,
3957         # but doesn't know where they go. We need to collate lists
3958         # by the host the affected instance is on and dispatch them
3959         # according to host
3960         instances_by_host = collections.defaultdict(list)
3961         events_by_host = collections.defaultdict(list)
3962         hosts_by_instance = collections.defaultdict(list)
3963         for instance in instances:
3964             for host in self._get_relevant_hosts(context, instance):
3965                 instances_by_host[host].append(instance)
3966                 hosts_by_instance[instance.uuid].append(host)
3967 
3968         for event in events:
3969             for host in hosts_by_instance[event.instance_uuid]:
3970                 events_by_host[host].append(event)
3971 
3972         for host in instances_by_host:
3973             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
3974             # in order to ensure that a failure in processing events on a host
3975             # will not prevent processing events on other hosts
3976             self.compute_rpcapi.external_instance_event(
3977                 context, instances_by_host[host], events_by_host[host],
3978                 host=host)
3979 
3980     def _get_relevant_hosts(self, context, instance):
3981         hosts = set()
3982         hosts.add(instance.host)
3983         if instance.migration_context is not None:
3984             migration_id = instance.migration_context.migration_id
3985             migration = objects.Migration.get_by_id(context, migration_id)
3986             hosts.add(migration.dest_compute)
3987             hosts.add(migration.source_compute)
3988             LOG.debug('Instance %(instance)s is migrating, '
3989                       'copying events to all relevant hosts: '
3990                       '%(hosts)s', {'instance': instance.uuid,
3991                                     'hosts': hosts})
3992         return hosts
3993 
3994     def get_instance_host_status(self, instance):
3995         if instance.host:
3996             try:
3997                 service = [service for service in instance.services if
3998                            service.binary == 'nova-compute'][0]
3999                 if service.forced_down:
4000                     host_status = fields_obj.HostStatus.DOWN
4001                 elif service.disabled:
4002                     host_status = fields_obj.HostStatus.MAINTENANCE
4003                 else:
4004                     alive = self.servicegroup_api.service_is_up(service)
4005                     host_status = ((alive and fields_obj.HostStatus.UP) or
4006                                    fields_obj.HostStatus.UNKNOWN)
4007             except IndexError:
4008                 host_status = fields_obj.HostStatus.NONE
4009         else:
4010             host_status = fields_obj.HostStatus.NONE
4011         return host_status
4012 
4013     def get_instances_host_statuses(self, instance_list):
4014         host_status_dict = dict()
4015         host_statuses = dict()
4016         for instance in instance_list:
4017             if instance.host:
4018                 if instance.host not in host_status_dict:
4019                     host_status = self.get_instance_host_status(instance)
4020                     host_status_dict[instance.host] = host_status
4021                 else:
4022                     host_status = host_status_dict[instance.host]
4023             else:
4024                 host_status = fields_obj.HostStatus.NONE
4025             host_statuses[instance.uuid] = host_status
4026         return host_statuses
4027 
4028 
4029 class HostAPI(base.Base):
4030     """Sub-set of the Compute Manager API for managing host operations."""
4031 
4032     def __init__(self, rpcapi=None):
4033         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
4034         self.servicegroup_api = servicegroup.API()
4035         super(HostAPI, self).__init__()
4036 
4037     def _assert_host_exists(self, context, host_name, must_be_up=False):
4038         """Raise HostNotFound if compute host doesn't exist."""
4039         service = objects.Service.get_by_compute_host(context, host_name)
4040         if not service:
4041             raise exception.HostNotFound(host=host_name)
4042         if must_be_up and not self.servicegroup_api.service_is_up(service):
4043             raise exception.ComputeServiceUnavailable(host=host_name)
4044         return service['host']
4045 
4046     @wrap_exception()
4047     def set_host_enabled(self, context, host_name, enabled):
4048         """Sets the specified host's ability to accept new instances."""
4049         host_name = self._assert_host_exists(context, host_name)
4050         payload = {'host_name': host_name, 'enabled': enabled}
4051         compute_utils.notify_about_host_update(context,
4052                                                'set_enabled.start',
4053                                                payload)
4054         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
4055                 host=host_name)
4056         compute_utils.notify_about_host_update(context,
4057                                                'set_enabled.end',
4058                                                payload)
4059         return result
4060 
4061     def get_host_uptime(self, context, host_name):
4062         """Returns the result of calling "uptime" on the target host."""
4063         host_name = self._assert_host_exists(context, host_name,
4064                          must_be_up=True)
4065         return self.rpcapi.get_host_uptime(context, host=host_name)
4066 
4067     @wrap_exception()
4068     def host_power_action(self, context, host_name, action):
4069         """Reboots, shuts down or powers up the host."""
4070         host_name = self._assert_host_exists(context, host_name)
4071         payload = {'host_name': host_name, 'action': action}
4072         compute_utils.notify_about_host_update(context,
4073                                                'power_action.start',
4074                                                payload)
4075         result = self.rpcapi.host_power_action(context, action=action,
4076                 host=host_name)
4077         compute_utils.notify_about_host_update(context,
4078                                                'power_action.end',
4079                                                payload)
4080         return result
4081 
4082     @wrap_exception()
4083     def set_host_maintenance(self, context, host_name, mode):
4084         """Start/Stop host maintenance window. On start, it triggers
4085         guest VMs evacuation.
4086         """
4087         host_name = self._assert_host_exists(context, host_name)
4088         payload = {'host_name': host_name, 'mode': mode}
4089         compute_utils.notify_about_host_update(context,
4090                                                'set_maintenance.start',
4091                                                payload)
4092         result = self.rpcapi.host_maintenance_mode(context,
4093                 host_param=host_name, mode=mode, host=host_name)
4094         compute_utils.notify_about_host_update(context,
4095                                                'set_maintenance.end',
4096                                                payload)
4097         return result
4098 
4099     def service_get_all(self, context, filters=None, set_zones=False):
4100         """Returns a list of services, optionally filtering the results.
4101 
4102         If specified, 'filters' should be a dictionary containing services
4103         attributes and matching values.  Ie, to get a list of services for
4104         the 'compute' topic, use filters={'topic': 'compute'}.
4105         """
4106         if filters is None:
4107             filters = {}
4108         disabled = filters.pop('disabled', None)
4109         if 'availability_zone' in filters:
4110             set_zones = True
4111         services = objects.ServiceList.get_all(context, disabled,
4112                                                set_zones=set_zones)
4113         ret_services = []
4114         for service in services:
4115             for key, val in six.iteritems(filters):
4116                 if service[key] != val:
4117                     break
4118             else:
4119                 # All filters matched.
4120                 ret_services.append(service)
4121         return ret_services
4122 
4123     def service_get_by_id(self, context, service_id):
4124         """Get service entry for the given service id."""
4125         return objects.Service.get_by_id(context, service_id)
4126 
4127     def service_get_by_compute_host(self, context, host_name):
4128         """Get service entry for the given compute hostname."""
4129         return objects.Service.get_by_compute_host(context, host_name)
4130 
4131     def _service_update(self, context, host_name, binary, params_to_update):
4132         """Performs the actual service update operation."""
4133         service = objects.Service.get_by_args(context, host_name, binary)
4134         service.update(params_to_update)
4135         service.save()
4136         return service
4137 
4138     def service_update(self, context, host_name, binary, params_to_update):
4139         """Enable / Disable a service.
4140 
4141         For compute services, this stops new builds and migrations going to
4142         the host.
4143         """
4144         return self._service_update(context, host_name, binary,
4145                                     params_to_update)
4146 
4147     def _service_delete(self, context, service_id):
4148         """Performs the actual Service deletion operation."""
4149         objects.Service.get_by_id(context, service_id).destroy()
4150 
4151     def service_delete(self, context, service_id):
4152         """Deletes the specified service."""
4153         self._service_delete(context, service_id)
4154 
4155     def instance_get_all_by_host(self, context, host_name):
4156         """Return all instances on the given host."""
4157         return objects.InstanceList.get_by_host(context, host_name)
4158 
4159     def task_log_get_all(self, context, task_name, period_beginning,
4160                          period_ending, host=None, state=None):
4161         """Return the task logs within a given range, optionally
4162         filtering by host and/or state.
4163         """
4164         return self.db.task_log_get_all(context, task_name,
4165                                         period_beginning,
4166                                         period_ending,
4167                                         host=host,
4168                                         state=state)
4169 
4170     def compute_node_get(self, context, compute_id):
4171         """Return compute node entry for particular integer ID."""
4172         return objects.ComputeNode.get_by_id(context, int(compute_id))
4173 
4174     def compute_node_get_all(self, context, limit=None, marker=None):
4175         return objects.ComputeNodeList.get_by_pagination(
4176             context, limit=limit, marker=marker)
4177 
4178     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
4179         return objects.ComputeNodeList.get_by_hypervisor(context,
4180                                                          hypervisor_match)
4181 
4182     def compute_node_statistics(self, context):
4183         return self.db.compute_node_statistics(context)
4184 
4185 
4186 class InstanceActionAPI(base.Base):
4187     """Sub-set of the Compute Manager API for managing instance actions."""
4188 
4189     def actions_get(self, context, instance):
4190         return objects.InstanceActionList.get_by_instance_uuid(
4191             context, instance.uuid)
4192 
4193     def action_get_by_request_id(self, context, instance, request_id):
4194         return objects.InstanceAction.get_by_request_id(
4195             context, instance.uuid, request_id)
4196 
4197     def action_events_get(self, context, instance, action_id):
4198         return objects.InstanceActionEventList.get_by_action(
4199             context, action_id)
4200 
4201 
4202 class AggregateAPI(base.Base):
4203     """Sub-set of the Compute Manager API for managing host aggregates."""
4204     def __init__(self, **kwargs):
4205         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4206         self.scheduler_client = scheduler_client.SchedulerClient()
4207         super(AggregateAPI, self).__init__(**kwargs)
4208 
4209     @wrap_exception()
4210     def create_aggregate(self, context, aggregate_name, availability_zone):
4211         """Creates the model for the aggregate."""
4212 
4213         aggregate = objects.Aggregate(context=context)
4214         aggregate.name = aggregate_name
4215         if availability_zone:
4216             aggregate.metadata = {'availability_zone': availability_zone}
4217         aggregate.create()
4218         self.scheduler_client.update_aggregates(context, [aggregate])
4219         return aggregate
4220 
4221     def get_aggregate(self, context, aggregate_id):
4222         """Get an aggregate by id."""
4223         return objects.Aggregate.get_by_id(context, aggregate_id)
4224 
4225     def get_aggregate_list(self, context):
4226         """Get all the aggregates."""
4227         return objects.AggregateList.get_all(context)
4228 
4229     def get_aggregates_by_host(self, context, compute_host):
4230         """Get all the aggregates where the given host is presented."""
4231         return objects.AggregateList.get_by_host(context, compute_host)
4232 
4233     @wrap_exception()
4234     def update_aggregate(self, context, aggregate_id, values):
4235         """Update the properties of an aggregate."""
4236         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4237         if 'name' in values:
4238             aggregate.name = values.pop('name')
4239             aggregate.save()
4240         self.is_safe_to_update_az(context, values, aggregate=aggregate,
4241                                   action_name=AGGREGATE_ACTION_UPDATE)
4242         if values:
4243             aggregate.update_metadata(values)
4244             aggregate.updated_at = timeutils.utcnow()
4245         self.scheduler_client.update_aggregates(context, [aggregate])
4246         # If updated values include availability_zones, then the cache
4247         # which stored availability_zones and host need to be reset
4248         if values.get('availability_zone'):
4249             availability_zones.reset_cache()
4250         return aggregate
4251 
4252     @wrap_exception()
4253     def update_aggregate_metadata(self, context, aggregate_id, metadata):
4254         """Updates the aggregate metadata."""
4255         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4256         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
4257                                   action_name=AGGREGATE_ACTION_UPDATE_META)
4258         aggregate.update_metadata(metadata)
4259         self.scheduler_client.update_aggregates(context, [aggregate])
4260         # If updated metadata include availability_zones, then the cache
4261         # which stored availability_zones and host need to be reset
4262         if metadata and metadata.get('availability_zone'):
4263             availability_zones.reset_cache()
4264         aggregate.updated_at = timeutils.utcnow()
4265         return aggregate
4266 
4267     @wrap_exception()
4268     def delete_aggregate(self, context, aggregate_id):
4269         """Deletes the aggregate."""
4270         aggregate_payload = {'aggregate_id': aggregate_id}
4271         compute_utils.notify_about_aggregate_update(context,
4272                                                     "delete.start",
4273                                                     aggregate_payload)
4274         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4275         if len(aggregate.hosts) > 0:
4276             msg = _("Host aggregate is not empty")
4277             raise exception.InvalidAggregateActionDelete(
4278                 aggregate_id=aggregate_id, reason=msg)
4279         aggregate.destroy()
4280         self.scheduler_client.delete_aggregate(context, aggregate)
4281         compute_utils.notify_about_aggregate_update(context,
4282                                                     "delete.end",
4283                                                     aggregate_payload)
4284 
4285     def is_safe_to_update_az(self, context, metadata, aggregate,
4286                              hosts=None,
4287                              action_name=AGGREGATE_ACTION_ADD):
4288         """Determine if updates alter an aggregate's availability zone.
4289 
4290             :param context: local context
4291             :param metadata: Target metadata for updating aggregate
4292             :param aggregate: Aggregate to update
4293             :param hosts: Hosts to check. If None, aggregate.hosts is used
4294             :type hosts: list
4295             :action_name: Calling method for logging purposes
4296 
4297         """
4298         if 'availability_zone' in metadata:
4299             if not metadata['availability_zone']:
4300                 msg = _("Aggregate %s does not support empty named "
4301                         "availability zone") % aggregate.name
4302                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4303                                                   msg)
4304             _hosts = hosts or aggregate.hosts
4305             host_aggregates = objects.AggregateList.get_by_metadata_key(
4306                 context, 'availability_zone', hosts=_hosts)
4307             conflicting_azs = [
4308                 agg.availability_zone for agg in host_aggregates
4309                 if agg.availability_zone != metadata['availability_zone']
4310                 and agg.id != aggregate.id]
4311             if conflicting_azs:
4312                 msg = _("One or more hosts already in availability zone(s) "
4313                         "%s") % conflicting_azs
4314                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4315                                                   msg)
4316 
4317     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
4318         if action_name == AGGREGATE_ACTION_ADD:
4319             raise exception.InvalidAggregateActionAdd(
4320                 aggregate_id=aggregate_id, reason=reason)
4321         elif action_name == AGGREGATE_ACTION_UPDATE:
4322             raise exception.InvalidAggregateActionUpdate(
4323                 aggregate_id=aggregate_id, reason=reason)
4324         elif action_name == AGGREGATE_ACTION_UPDATE_META:
4325             raise exception.InvalidAggregateActionUpdateMeta(
4326                 aggregate_id=aggregate_id, reason=reason)
4327         elif action_name == AGGREGATE_ACTION_DELETE:
4328             raise exception.InvalidAggregateActionDelete(
4329                 aggregate_id=aggregate_id, reason=reason)
4330 
4331         raise exception.NovaException(
4332             _("Unexpected aggregate action %s") % action_name)
4333 
4334     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
4335         # Update the availability_zone cache to avoid getting wrong
4336         # availability_zone in cache retention time when add/remove
4337         # host to/from aggregate.
4338         if aggregate_meta and aggregate_meta.get('availability_zone'):
4339             availability_zones.update_host_availability_zone_cache(context,
4340                                                                    host_name)
4341 
4342     @wrap_exception()
4343     def add_host_to_aggregate(self, context, aggregate_id, host_name):
4344         """Adds the host to an aggregate."""
4345         aggregate_payload = {'aggregate_id': aggregate_id,
4346                              'host_name': host_name}
4347         compute_utils.notify_about_aggregate_update(context,
4348                                                     "addhost.start",
4349                                                     aggregate_payload)
4350         # validates the host; ComputeHostNotFound is raised if invalid
4351         objects.Service.get_by_compute_host(context, host_name)
4352 
4353         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4354         self.is_safe_to_update_az(context, aggregate.metadata,
4355                                   hosts=[host_name], aggregate=aggregate)
4356 
4357         aggregate.add_host(host_name)
4358         self.scheduler_client.update_aggregates(context, [aggregate])
4359         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
4360         # NOTE(jogo): Send message to host to support resource pools
4361         self.compute_rpcapi.add_aggregate_host(context,
4362                 aggregate=aggregate, host_param=host_name, host=host_name)
4363         aggregate_payload.update({'name': aggregate.name})
4364         compute_utils.notify_about_aggregate_update(context,
4365                                                     "addhost.end",
4366                                                     aggregate_payload)
4367         return aggregate
4368 
4369     @wrap_exception()
4370     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
4371         """Removes host from the aggregate."""
4372         aggregate_payload = {'aggregate_id': aggregate_id,
4373                              'host_name': host_name}
4374         compute_utils.notify_about_aggregate_update(context,
4375                                                     "removehost.start",
4376                                                     aggregate_payload)
4377         # validates the host; ComputeHostNotFound is raised if invalid
4378         objects.Service.get_by_compute_host(context, host_name)
4379         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4380         aggregate.delete_host(host_name)
4381         self.scheduler_client.update_aggregates(context, [aggregate])
4382         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
4383         self.compute_rpcapi.remove_aggregate_host(context,
4384                 aggregate=aggregate, host_param=host_name, host=host_name)
4385         compute_utils.notify_about_aggregate_update(context,
4386                                                     "removehost.end",
4387                                                     aggregate_payload)
4388         return aggregate
4389 
4390 
4391 class KeypairAPI(base.Base):
4392     """Subset of the Compute Manager API for managing key pairs."""
4393 
4394     get_notifier = functools.partial(rpc.get_notifier, service='api')
4395     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
4396                                        get_notifier=get_notifier,
4397                                        binary='nova-api')
4398 
4399     def _notify(self, context, event_suffix, keypair_name):
4400         payload = {
4401             'tenant_id': context.project_id,
4402             'user_id': context.user_id,
4403             'key_name': keypair_name,
4404         }
4405         notify = self.get_notifier()
4406         notify.info(context, 'keypair.%s' % event_suffix, payload)
4407 
4408     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
4409         safe_chars = "_- " + string.digits + string.ascii_letters
4410         clean_value = "".join(x for x in key_name if x in safe_chars)
4411         if clean_value != key_name:
4412             raise exception.InvalidKeypair(
4413                 reason=_("Keypair name contains unsafe characters"))
4414 
4415         try:
4416             utils.check_string_length(key_name, min_length=1, max_length=255)
4417         except exception.InvalidInput:
4418             raise exception.InvalidKeypair(
4419                 reason=_('Keypair name must be string and between '
4420                          '1 and 255 characters long'))
4421 
4422         count = objects.Quotas.count(context, 'key_pairs', user_id)
4423 
4424         try:
4425             objects.Quotas.limit_check(context, key_pairs=count + 1)
4426         except exception.OverQuota:
4427             raise exception.KeypairLimitExceeded()
4428 
4429     @wrap_exception()
4430     def import_key_pair(self, context, user_id, key_name, public_key,
4431                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
4432         """Import a key pair using an existing public key."""
4433         self._validate_new_key_pair(context, user_id, key_name, key_type)
4434 
4435         self._notify(context, 'import.start', key_name)
4436 
4437         fingerprint = self._generate_fingerprint(public_key, key_type)
4438 
4439         keypair = objects.KeyPair(context)
4440         keypair.user_id = user_id
4441         keypair.name = key_name
4442         keypair.type = key_type
4443         keypair.fingerprint = fingerprint
4444         keypair.public_key = public_key
4445         keypair.create()
4446 
4447         self._notify(context, 'import.end', key_name)
4448 
4449         return keypair
4450 
4451     @wrap_exception()
4452     def create_key_pair(self, context, user_id, key_name,
4453                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
4454         """Create a new key pair."""
4455         self._validate_new_key_pair(context, user_id, key_name, key_type)
4456 
4457         self._notify(context, 'create.start', key_name)
4458 
4459         private_key, public_key, fingerprint = self._generate_key_pair(
4460             user_id, key_type)
4461 
4462         keypair = objects.KeyPair(context)
4463         keypair.user_id = user_id
4464         keypair.name = key_name
4465         keypair.type = key_type
4466         keypair.fingerprint = fingerprint
4467         keypair.public_key = public_key
4468         keypair.create()
4469 
4470         self._notify(context, 'create.end', key_name)
4471 
4472         return keypair, private_key
4473 
4474     def _generate_fingerprint(self, public_key, key_type):
4475         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4476             return crypto.generate_fingerprint(public_key)
4477         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4478             return crypto.generate_x509_fingerprint(public_key)
4479 
4480     def _generate_key_pair(self, user_id, key_type):
4481         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4482             return crypto.generate_key_pair()
4483         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4484             return crypto.generate_winrm_x509_cert(user_id)
4485 
4486     @wrap_exception()
4487     def delete_key_pair(self, context, user_id, key_name):
4488         """Delete a keypair by name."""
4489         self._notify(context, 'delete.start', key_name)
4490         objects.KeyPair.destroy_by_name(context, user_id, key_name)
4491         self._notify(context, 'delete.end', key_name)
4492 
4493     def get_key_pairs(self, context, user_id, limit=None, marker=None):
4494         """List key pairs."""
4495         return objects.KeyPairList.get_by_user(
4496             context, user_id, limit=limit, marker=marker)
4497 
4498     def get_key_pair(self, context, user_id, key_name):
4499         """Get a keypair by name."""
4500         return objects.KeyPair.get_by_name(context, user_id, key_name)
4501 
4502 
4503 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
4504     """Sub-set of the Compute API related to managing security groups
4505     and security group rules
4506     """
4507 
4508     # The nova security group api does not use a uuid for the id.
4509     id_is_uuid = False
4510 
4511     def __init__(self, **kwargs):
4512         super(SecurityGroupAPI, self).__init__(**kwargs)
4513         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4514 
4515     def validate_property(self, value, property, allowed):
4516         """Validate given security group property.
4517 
4518         :param value:          the value to validate, as a string or unicode
4519         :param property:       the property, either 'name' or 'description'
4520         :param allowed:        the range of characters allowed
4521         """
4522 
4523         try:
4524             val = value.strip()
4525         except AttributeError:
4526             msg = _("Security group %s is not a string or unicode") % property
4527             self.raise_invalid_property(msg)
4528         utils.check_string_length(val, name=property, min_length=1,
4529                                   max_length=255)
4530 
4531         if allowed and not re.match(allowed, val):
4532             # Some validation to ensure that values match API spec.
4533             # - Alphanumeric characters, spaces, dashes, and underscores.
4534             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
4535             #  probably create a param validator that can be used elsewhere.
4536             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
4537                      "invalid. Content limited to '%(allowed)s'.") %
4538                    {'value': value, 'allowed': allowed,
4539                     'property': property.capitalize()})
4540             self.raise_invalid_property(msg)
4541 
4542     def ensure_default(self, context):
4543         """Ensure that a context has a security group.
4544 
4545         Creates a security group for the security context if it does not
4546         already exist.
4547 
4548         :param context: the security context
4549         """
4550         self.db.security_group_ensure_default(context)
4551 
4552     def create_security_group(self, context, name, description):
4553         quotas = objects.Quotas(context=context)
4554         try:
4555             quotas.reserve(security_groups=1)
4556         except exception.OverQuota:
4557             msg = _("Quota exceeded, too many security groups.")
4558             self.raise_over_quota(msg)
4559 
4560         LOG.info(_LI("Create Security Group %s"), name)
4561 
4562         try:
4563             self.ensure_default(context)
4564 
4565             group = {'user_id': context.user_id,
4566                      'project_id': context.project_id,
4567                      'name': name,
4568                      'description': description}
4569             try:
4570                 group_ref = self.db.security_group_create(context, group)
4571             except exception.SecurityGroupExists:
4572                 msg = _('Security group %s already exists') % name
4573                 self.raise_group_already_exists(msg)
4574             # Commit the reservation
4575             quotas.commit()
4576         except Exception:
4577             with excutils.save_and_reraise_exception():
4578                 quotas.rollback()
4579 
4580         return group_ref
4581 
4582     def update_security_group(self, context, security_group,
4583                                 name, description):
4584         if security_group['name'] in RO_SECURITY_GROUPS:
4585             msg = (_("Unable to update system group '%s'") %
4586                     security_group['name'])
4587             self.raise_invalid_group(msg)
4588 
4589         group = {'name': name,
4590                  'description': description}
4591 
4592         columns_to_join = ['rules.grantee_group']
4593         group_ref = self.db.security_group_update(context,
4594                 security_group['id'],
4595                 group,
4596                 columns_to_join=columns_to_join)
4597         return group_ref
4598 
4599     def get(self, context, name=None, id=None, map_exception=False):
4600         self.ensure_default(context)
4601         cols = ['rules']
4602         try:
4603             if name:
4604                 return self.db.security_group_get_by_name(context,
4605                                                           context.project_id,
4606                                                           name,
4607                                                           columns_to_join=cols)
4608             elif id:
4609                 return self.db.security_group_get(context, id,
4610                                                   columns_to_join=cols)
4611         except exception.NotFound as exp:
4612             if map_exception:
4613                 msg = exp.format_message()
4614                 self.raise_not_found(msg)
4615             else:
4616                 raise
4617 
4618     def list(self, context, names=None, ids=None, project=None,
4619              search_opts=None):
4620         self.ensure_default(context)
4621 
4622         groups = []
4623         if names or ids:
4624             if names:
4625                 for name in names:
4626                     groups.append(self.db.security_group_get_by_name(context,
4627                                                                      project,
4628                                                                      name))
4629             if ids:
4630                 for id in ids:
4631                     groups.append(self.db.security_group_get(context, id))
4632 
4633         elif context.is_admin:
4634             # TODO(eglynn): support a wider set of search options than just
4635             # all_tenants, at least include the standard filters defined for
4636             # the EC2 DescribeSecurityGroups API for the non-admin case also
4637             if (search_opts and 'all_tenants' in search_opts):
4638                 groups = self.db.security_group_get_all(context)
4639             else:
4640                 groups = self.db.security_group_get_by_project(context,
4641                                                                project)
4642 
4643         elif project:
4644             groups = self.db.security_group_get_by_project(context, project)
4645 
4646         return groups
4647 
4648     def destroy(self, context, security_group):
4649         if security_group['name'] in RO_SECURITY_GROUPS:
4650             msg = _("Unable to delete system group '%s'") % \
4651                     security_group['name']
4652             self.raise_invalid_group(msg)
4653 
4654         if self.db.security_group_in_use(context, security_group['id']):
4655             msg = _("Security group is still in use")
4656             self.raise_invalid_group(msg)
4657 
4658         quotas = objects.Quotas(context=context)
4659         quota_project, quota_user = quotas_obj.ids_from_security_group(
4660                                 context, security_group)
4661         try:
4662             quotas.reserve(project_id=quota_project,
4663                            user_id=quota_user, security_groups=-1)
4664         except Exception:
4665             LOG.exception(_LE("Failed to update usages deallocating "
4666                               "security group"))
4667 
4668         LOG.info(_LI("Delete security group %s"), security_group['name'])
4669         self.db.security_group_destroy(context, security_group['id'])
4670 
4671         # Commit the reservations
4672         quotas.commit()
4673 
4674     def is_associated_with_server(self, security_group, instance_uuid):
4675         """Check if the security group is already associated
4676            with the instance. If Yes, return True.
4677         """
4678 
4679         if not security_group:
4680             return False
4681 
4682         instances = security_group.get('instances')
4683         if not instances:
4684             return False
4685 
4686         for inst in instances:
4687             if (instance_uuid == inst['uuid']):
4688                 return True
4689 
4690         return False
4691 
4692     def add_to_instance(self, context, instance, security_group_name):
4693         """Add security group to the instance."""
4694         security_group = self.db.security_group_get_by_name(context,
4695                 context.project_id,
4696                 security_group_name)
4697 
4698         instance_uuid = instance.uuid
4699 
4700         # check if the security group is associated with the server
4701         if self.is_associated_with_server(security_group, instance_uuid):
4702             raise exception.SecurityGroupExistsForInstance(
4703                                         security_group_id=security_group['id'],
4704                                         instance_id=instance_uuid)
4705 
4706         self.db.instance_add_security_group(context.elevated(),
4707                                             instance_uuid,
4708                                             security_group['id'])
4709         if instance.host:
4710             self.compute_rpcapi.refresh_instance_security_rules(
4711                     context, instance, instance.host)
4712 
4713     def remove_from_instance(self, context, instance, security_group_name):
4714         """Remove the security group associated with the instance."""
4715         security_group = self.db.security_group_get_by_name(context,
4716                 context.project_id,
4717                 security_group_name)
4718 
4719         instance_uuid = instance.uuid
4720 
4721         # check if the security group is associated with the server
4722         if not self.is_associated_with_server(security_group, instance_uuid):
4723             raise exception.SecurityGroupNotExistsForInstance(
4724                                     security_group_id=security_group['id'],
4725                                     instance_id=instance_uuid)
4726 
4727         self.db.instance_remove_security_group(context.elevated(),
4728                                                instance_uuid,
4729                                                security_group['id'])
4730         if instance.host:
4731             self.compute_rpcapi.refresh_instance_security_rules(
4732                     context, instance, instance.host)
4733 
4734     def get_rule(self, context, id):
4735         self.ensure_default(context)
4736         try:
4737             return self.db.security_group_rule_get(context, id)
4738         except exception.NotFound:
4739             msg = _("Rule (%s) not found") % id
4740             self.raise_not_found(msg)
4741 
4742     def add_rules(self, context, id, name, vals):
4743         """Add security group rule(s) to security group.
4744 
4745         Note: the Nova security group API doesn't support adding multiple
4746         security group rules at once but the EC2 one does. Therefore,
4747         this function is written to support both.
4748         """
4749 
4750         count = objects.Quotas.count(context, 'security_group_rules', id)
4751         try:
4752             projected = count + len(vals)
4753             objects.Quotas.limit_check(context, security_group_rules=projected)
4754         except exception.OverQuota:
4755             msg = _("Quota exceeded, too many security group rules.")
4756             self.raise_over_quota(msg)
4757 
4758         msg = _LI("Security group %(name)s added %(protocol)s ingress "
4759                   "(%(from_port)s:%(to_port)s)")
4760         rules = []
4761         for v in vals:
4762             rule = self.db.security_group_rule_create(context, v)
4763             rules.append(rule)
4764             LOG.info(msg, {'name': name,
4765                            'protocol': rule.protocol,
4766                            'from_port': rule.from_port,
4767                            'to_port': rule.to_port})
4768 
4769         self.trigger_rules_refresh(context, id=id)
4770         return rules
4771 
4772     def remove_rules(self, context, security_group, rule_ids):
4773         msg = _LI("Security group %(name)s removed %(protocol)s ingress "
4774                   "(%(from_port)s:%(to_port)s)")
4775         for rule_id in rule_ids:
4776             rule = self.get_rule(context, rule_id)
4777             LOG.info(msg, {'name': security_group['name'],
4778                            'protocol': rule.protocol,
4779                            'from_port': rule.from_port,
4780                            'to_port': rule.to_port})
4781 
4782             self.db.security_group_rule_destroy(context, rule_id)
4783 
4784         # NOTE(vish): we removed some rules, so refresh
4785         self.trigger_rules_refresh(context, id=security_group['id'])
4786 
4787     def remove_default_rules(self, context, rule_ids):
4788         for rule_id in rule_ids:
4789             self.db.security_group_default_rule_destroy(context, rule_id)
4790 
4791     def add_default_rules(self, context, vals):
4792         rules = [self.db.security_group_default_rule_create(context, v)
4793                  for v in vals]
4794         return rules
4795 
4796     def default_rule_exists(self, context, values):
4797         """Indicates whether the specified rule values are already
4798            defined in the default security group rules.
4799         """
4800         for rule in self.db.security_group_default_rule_list(context):
4801             keys = ('cidr', 'from_port', 'to_port', 'protocol')
4802             for key in keys:
4803                 if rule.get(key) != values.get(key):
4804                     break
4805             else:
4806                 return rule.get('id') or True
4807         return False
4808 
4809     def get_all_default_rules(self, context):
4810         try:
4811             rules = self.db.security_group_default_rule_list(context)
4812         except Exception:
4813             msg = 'cannot get default security group rules'
4814             raise exception.SecurityGroupDefaultRuleNotFound(msg)
4815 
4816         return rules
4817 
4818     def get_default_rule(self, context, id):
4819         return self.db.security_group_default_rule_get(context, id)
4820 
4821     def validate_id(self, id):
4822         try:
4823             return int(id)
4824         except ValueError:
4825             msg = _("Security group id should be integer")
4826             self.raise_invalid_property(msg)
4827 
4828     def _refresh_instance_security_rules(self, context, instances):
4829         for instance in instances:
4830             if instance.host is not None:
4831                 self.compute_rpcapi.refresh_instance_security_rules(
4832                         context, instance, instance.host)
4833 
4834     def trigger_rules_refresh(self, context, id):
4835         """Called when a rule is added to or removed from a security_group."""
4836         instances = objects.InstanceList.get_by_security_group_id(context, id)
4837         self._refresh_instance_security_rules(context, instances)
4838 
4839     def trigger_members_refresh(self, context, group_ids):
4840         """Called when a security group gains a new or loses a member.
4841 
4842         Sends an update request to each compute node for each instance for
4843         which this is relevant.
4844         """
4845         instances = objects.InstanceList.get_by_grantee_security_group_ids(
4846             context, group_ids)
4847         self._refresh_instance_security_rules(context, instances)
4848 
4849     def get_instance_security_groups(self, context, instance, detailed=False):
4850         if detailed:
4851             return self.db.security_group_get_by_instance(context,
4852                                                           instance.uuid)
4853         return [{'name': group.name} for group in instance.security_groups]
