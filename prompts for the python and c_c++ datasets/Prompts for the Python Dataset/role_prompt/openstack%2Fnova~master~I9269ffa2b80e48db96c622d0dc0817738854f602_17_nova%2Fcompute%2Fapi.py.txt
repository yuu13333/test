I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Piston Cloud Computing, Inc.
4 # Copyright 2012-2013 Red Hat, Inc.
5 # All Rights Reserved.
6 #
7 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
8 #    not use this file except in compliance with the License. You may obtain
9 #    a copy of the License at
10 #
11 #         http://www.apache.org/licenses/LICENSE-2.0
12 #
13 #    Unless required by applicable law or agreed to in writing, software
14 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
15 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
16 #    License for the specific language governing permissions and limitations
17 #    under the License.
18 
19 """Handles all requests relating to compute resources (e.g. guest VMs,
20 networking and storage of VMs, and compute hosts on which they run)."""
21 
22 import base64
23 import collections
24 import copy
25 import functools
26 import re
27 import string
28 
29 from oslo_log import log as logging
30 from oslo_messaging import exceptions as oslo_exceptions
31 from oslo_serialization import base64 as base64utils
32 from oslo_serialization import jsonutils
33 from oslo_utils import excutils
34 from oslo_utils import strutils
35 from oslo_utils import timeutils
36 from oslo_utils import units
37 from oslo_utils import uuidutils
38 import six
39 from six.moves import range
40 
41 from nova import availability_zones
42 from nova import block_device
43 from nova.cells import opts as cells_opts
44 from nova.compute import flavors
45 from nova.compute import instance_actions
46 from nova.compute import power_state
47 from nova.compute import rpcapi as compute_rpcapi
48 from nova.compute import task_states
49 from nova.compute import utils as compute_utils
50 from nova.compute import vm_states
51 from nova import conductor
52 import nova.conf
53 from nova.consoleauth import rpcapi as consoleauth_rpcapi
54 from nova import context as nova_context
55 from nova import crypto
56 from nova.db import base
57 from nova import exception
58 from nova import exception_wrapper
59 from nova import hooks
60 from nova.i18n import _
61 from nova.i18n import _LE
62 from nova.i18n import _LI
63 from nova.i18n import _LW
64 from nova import image
65 from nova import keymgr
66 from nova import network
67 from nova.network import model as network_model
68 from nova.network.security_group import openstack_driver
69 from nova.network.security_group import security_group_base
70 from nova import objects
71 from nova.objects import base as obj_base
72 from nova.objects import block_device as block_device_obj
73 from nova.objects import fields as fields_obj
74 from nova.objects import keypair as keypair_obj
75 from nova.objects import quotas as quotas_obj
76 from nova.pci import request as pci_request
77 import nova.policy
78 from nova import profiler
79 from nova import rpc
80 from nova.scheduler import client as scheduler_client
81 from nova.scheduler import utils as scheduler_utils
82 from nova import servicegroup
83 from nova import utils
84 from nova.virt import hardware
85 from nova.volume import cinder
86 
87 LOG = logging.getLogger(__name__)
88 
89 get_notifier = functools.partial(rpc.get_notifier, service='compute')
90 # NOTE(gibi): legacy notification used compute as a service but these
91 # calls still run on the client side of the compute service which is
92 # nova-api. By setting the binary to nova-api below, we can make sure
93 # that the new versioned notifications has the right publisher_id but the
94 # legacy notifications does not change.
95 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
96                                    get_notifier=get_notifier,
97                                    binary='nova-api')
98 CONF = nova.conf.CONF
99 
100 MAX_USERDATA_SIZE = 65535
101 RO_SECURITY_GROUPS = ['default']
102 
103 AGGREGATE_ACTION_UPDATE = 'Update'
104 AGGREGATE_ACTION_UPDATE_META = 'UpdateMeta'
105 AGGREGATE_ACTION_DELETE = 'Delete'
106 AGGREGATE_ACTION_ADD = 'Add'
107 BFV_RESERVE_MIN_COMPUTE_VERSION = 17
108 
109 # FIXME(danms): Keep a global cache of the cells we find the
110 # first time we look. This needs to be refreshed on a timer or
111 # trigger.
112 CELLS = []
113 
114 
115 def check_instance_state(vm_state=None, task_state=(None,),
116                          must_have_launched=True):
117     """Decorator to check VM and/or task state before entry to API functions.
118 
119     If the instance is in the wrong state, or has not been successfully
120     started at least once the wrapper will raise an exception.
121     """
122 
123     if vm_state is not None and not isinstance(vm_state, set):
124         vm_state = set(vm_state)
125     if task_state is not None and not isinstance(task_state, set):
126         task_state = set(task_state)
127 
128     def outer(f):
129         @six.wraps(f)
130         def inner(self, context, instance, *args, **kw):
131             if vm_state is not None and instance.vm_state not in vm_state:
132                 raise exception.InstanceInvalidState(
133                     attr='vm_state',
134                     instance_uuid=instance.uuid,
135                     state=instance.vm_state,
136                     method=f.__name__)
137             if (task_state is not None and
138                     instance.task_state not in task_state):
139                 raise exception.InstanceInvalidState(
140                     attr='task_state',
141                     instance_uuid=instance.uuid,
142                     state=instance.task_state,
143                     method=f.__name__)
144             if must_have_launched and not instance.launched_at:
145                 raise exception.InstanceInvalidState(
146                     attr='launched_at',
147                     instance_uuid=instance.uuid,
148                     state=instance.launched_at,
149                     method=f.__name__)
150 
151             return f(self, context, instance, *args, **kw)
152         return inner
153     return outer
154 
155 
156 def check_instance_host(function):
157     @six.wraps(function)
158     def wrapped(self, context, instance, *args, **kwargs):
159         if not instance.host:
160             raise exception.InstanceNotReady(instance_id=instance.uuid)
161         return function(self, context, instance, *args, **kwargs)
162     return wrapped
163 
164 
165 def check_instance_lock(function):
166     @six.wraps(function)
167     def inner(self, context, instance, *args, **kwargs):
168         if instance.locked and not context.is_admin:
169             raise exception.InstanceIsLocked(instance_uuid=instance.uuid)
170         return function(self, context, instance, *args, **kwargs)
171     return inner
172 
173 
174 def check_instance_cell(fn):
175     @six.wraps(fn)
176     def _wrapped(self, context, instance, *args, **kwargs):
177         self._validate_cell(instance)
178         return fn(self, context, instance, *args, **kwargs)
179     return _wrapped
180 
181 
182 def _diff_dict(orig, new):
183     """Return a dict describing how to change orig to new.  The keys
184     correspond to values that have changed; the value will be a list
185     of one or two elements.  The first element of the list will be
186     either '+' or '-', indicating whether the key was updated or
187     deleted; if the key was updated, the list will contain a second
188     element, giving the updated value.
189     """
190     # Figure out what keys went away
191     result = {k: ['-'] for k in set(orig.keys()) - set(new.keys())}
192     # Compute the updates
193     for key, value in new.items():
194         if key not in orig or value != orig[key]:
195             result[key] = ['+', value]
196     return result
197 
198 
199 @profiler.trace_cls("compute_api")
200 class API(base.Base):
201     """API for interacting with the compute manager."""
202 
203     def __init__(self, image_api=None, network_api=None, volume_api=None,
204                  security_group_api=None, **kwargs):
205         self.image_api = image_api or image.API()
206         self.network_api = network_api or network.API()
207         self.volume_api = volume_api or cinder.API()
208         self.security_group_api = (security_group_api or
209             openstack_driver.get_openstack_security_group_driver())
210         self.consoleauth_rpcapi = consoleauth_rpcapi.ConsoleAuthAPI()
211         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
212         self.compute_task_api = conductor.ComputeTaskAPI()
213         self.servicegroup_api = servicegroup.API()
214         self.notifier = rpc.get_notifier('compute', CONF.host)
215         if CONF.ephemeral_storage_encryption.enabled:
216             self.key_manager = keymgr.API()
217 
218         super(API, self).__init__(**kwargs)
219 
220     @property
221     def cell_type(self):
222         try:
223             return getattr(self, '_cell_type')
224         except AttributeError:
225             self._cell_type = cells_opts.get_cell_type()
226             return self._cell_type
227 
228     def _validate_cell(self, instance):
229         if self.cell_type != 'api':
230             return
231         cell_name = instance.cell_name
232         if not cell_name:
233             raise exception.InstanceUnknownCell(
234                     instance_uuid=instance.uuid)
235 
236     def _record_action_start(self, context, instance, action):
237         objects.InstanceAction.action_start(context, instance.uuid,
238                                             action, want_result=False)
239 
240     def _check_injected_file_quota(self, context, injected_files):
241         """Enforce quota limits on injected files.
242 
243         Raises a QuotaError if any limit is exceeded.
244         """
245         if injected_files is None:
246             return
247 
248         # Check number of files first
249         try:
250             objects.Quotas.limit_check(context,
251                                        injected_files=len(injected_files))
252         except exception.OverQuota:
253             raise exception.OnsetFileLimitExceeded()
254 
255         # OK, now count path and content lengths; we're looking for
256         # the max...
257         max_path = 0
258         max_content = 0
259         for path, content in injected_files:
260             max_path = max(max_path, len(path))
261             max_content = max(max_content, len(content))
262 
263         try:
264             objects.Quotas.limit_check(context,
265                                        injected_file_path_bytes=max_path,
266                                        injected_file_content_bytes=max_content)
267         except exception.OverQuota as exc:
268             # Favor path limit over content limit for reporting
269             # purposes
270             if 'injected_file_path_bytes' in exc.kwargs['overs']:
271                 raise exception.OnsetFilePathLimitExceeded()
272             else:
273                 raise exception.OnsetFileContentLimitExceeded()
274 
275     def _check_metadata_properties_quota(self, context, metadata=None):
276         """Enforce quota limits on metadata properties."""
277         if not metadata:
278             metadata = {}
279         if not isinstance(metadata, dict):
280             msg = (_("Metadata type should be dict."))
281             raise exception.InvalidMetadata(reason=msg)
282         num_metadata = len(metadata)
283         try:
284             objects.Quotas.limit_check(context, metadata_items=num_metadata)
285         except exception.OverQuota as exc:
286             quota_metadata = exc.kwargs['quotas']['metadata_items']
287             raise exception.MetadataLimitExceeded(allowed=quota_metadata)
288 
289         # Because metadata is stored in the DB, we hard-code the size limits
290         # In future, we may support more variable length strings, so we act
291         #  as if this is quota-controlled for forwards compatibility.
292         # Those are only used in V2 API, from V2.1 API, those checks are
293         # validated at API layer schema validation.
294         for k, v in metadata.items():
295             try:
296                 utils.check_string_length(v)
297                 utils.check_string_length(k, min_length=1)
298             except exception.InvalidInput as e:
299                 raise exception.InvalidMetadata(reason=e.format_message())
300 
301             if len(k) > 255:
302                 msg = _("Metadata property key greater than 255 characters")
303                 raise exception.InvalidMetadataSize(reason=msg)
304             if len(v) > 255:
305                 msg = _("Metadata property value greater than 255 characters")
306                 raise exception.InvalidMetadataSize(reason=msg)
307 
308     def _check_requested_secgroups(self, context, secgroups):
309         """Check if the security group requested exists and belongs to
310         the project.
311 
312         :param context: The nova request context.
313         :type context: nova.context.RequestContext
314         :param secgroups: list of requested security group names, or uuids in
315             the case of Neutron.
316         :type secgroups: list
317         :returns: list of requested security group names unmodified if using
318             nova-network. If using Neutron, the list returned is all uuids.
319             Note that 'default' is a special case and will be unmodified if
320             it's requested.
321         """
322         security_groups = []
323         for secgroup in secgroups:
324             # NOTE(sdague): default is handled special
325             if secgroup == "default":
326                 security_groups.append(secgroup)
327                 continue
328             secgroup_dict = self.security_group_api.get(context, secgroup)
329             if not secgroup_dict:
330                 raise exception.SecurityGroupNotFoundForProject(
331                     project_id=context.project_id, security_group_id=secgroup)
332 
333             # Check to see if it's a nova-network or neutron type.
334             if isinstance(secgroup_dict['id'], int):
335                 # This is nova-network so just return the requested name.
336                 security_groups.append(secgroup)
337             else:
338                 # The id for neutron is a uuid, so we return the id (uuid).
339                 security_groups.append(secgroup_dict['id'])
340 
341         return security_groups
342 
343     def _check_requested_networks(self, context, requested_networks,
344                                   max_count):
345         """Check if the networks requested belongs to the project
346         and the fixed IP address for each network provided is within
347         same the network block
348         """
349         if requested_networks is not None:
350             if requested_networks.no_allocate:
351                 # If the network request was specifically 'none' meaning don't
352                 # allocate any networks, we just return the number of requested
353                 # instances since quotas don't change at all.
354                 return max_count
355 
356             # NOTE(danms): Temporary transition
357             requested_networks = requested_networks.as_tuples()
358 
359         return self.network_api.validate_networks(context, requested_networks,
360                                                   max_count)
361 
362     def _handle_kernel_and_ramdisk(self, context, kernel_id, ramdisk_id,
363                                    image):
364         """Choose kernel and ramdisk appropriate for the instance.
365 
366         The kernel and ramdisk can be chosen in one of three ways:
367 
368             1. Passed in with create-instance request.
369 
370             2. Inherited from image.
371 
372             3. Forced to None by using `null_kernel` FLAG.
373         """
374         # Inherit from image if not specified
375         image_properties = image.get('properties', {})
376 
377         if kernel_id is None:
378             kernel_id = image_properties.get('kernel_id')
379 
380         if ramdisk_id is None:
381             ramdisk_id = image_properties.get('ramdisk_id')
382 
383         # Force to None if using null_kernel
384         if kernel_id == str(CONF.null_kernel):
385             kernel_id = None
386             ramdisk_id = None
387 
388         # Verify kernel and ramdisk exist (fail-fast)
389         if kernel_id is not None:
390             kernel_image = self.image_api.get(context, kernel_id)
391             # kernel_id could have been a URI, not a UUID, so to keep behaviour
392             # from before, which leaked that implementation detail out to the
393             # caller, we return the image UUID of the kernel image and ramdisk
394             # image (below) and not any image URIs that might have been
395             # supplied.
396             # TODO(jaypipes): Get rid of this silliness once we move to a real
397             # Image object and hide all of that stuff within nova.image.api.
398             kernel_id = kernel_image['id']
399 
400         if ramdisk_id is not None:
401             ramdisk_image = self.image_api.get(context, ramdisk_id)
402             ramdisk_id = ramdisk_image['id']
403 
404         return kernel_id, ramdisk_id
405 
406     @staticmethod
407     def parse_availability_zone(context, availability_zone):
408         # NOTE(vish): We have a legacy hack to allow admins to specify hosts
409         #             via az using az:host:node. It might be nice to expose an
410         #             api to specify specific hosts to force onto, but for
411         #             now it just supports this legacy hack.
412         # NOTE(deva): It is also possible to specify az::node, in which case
413         #             the host manager will determine the correct host.
414         forced_host = None
415         forced_node = None
416         if availability_zone and ':' in availability_zone:
417             c = availability_zone.count(':')
418             if c == 1:
419                 availability_zone, forced_host = availability_zone.split(':')
420             elif c == 2:
421                 if '::' in availability_zone:
422                     availability_zone, forced_node = \
423                             availability_zone.split('::')
424                 else:
425                     availability_zone, forced_host, forced_node = \
426                             availability_zone.split(':')
427             else:
428                 raise exception.InvalidInput(
429                         reason="Unable to parse availability_zone")
430 
431         if not availability_zone:
432             availability_zone = CONF.default_schedule_zone
433 
434         return availability_zone, forced_host, forced_node
435 
436     def _ensure_auto_disk_config_is_valid(self, auto_disk_config_img,
437                                           auto_disk_config, image):
438         auto_disk_config_disabled = \
439                 utils.is_auto_disk_config_disabled(auto_disk_config_img)
440         if auto_disk_config_disabled and auto_disk_config:
441             raise exception.AutoDiskConfigDisabledByImage(image=image)
442 
443     def _inherit_properties_from_image(self, image, auto_disk_config):
444         image_properties = image.get('properties', {})
445         auto_disk_config_img = \
446                 utils.get_auto_disk_config_from_image_props(image_properties)
447         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
448                                                auto_disk_config,
449                                                image.get("id"))
450         if auto_disk_config is None:
451             auto_disk_config = strutils.bool_from_string(auto_disk_config_img)
452 
453         return {
454             'os_type': image_properties.get('os_type'),
455             'architecture': image_properties.get('architecture'),
456             'vm_mode': image_properties.get('vm_mode'),
457             'auto_disk_config': auto_disk_config
458         }
459 
460     def _new_instance_name_from_template(self, uuid, display_name, index):
461         params = {
462             'uuid': uuid,
463             'name': display_name,
464             'count': index + 1,
465         }
466         try:
467             new_name = (CONF.multi_instance_display_name_template %
468                         params)
469         except (KeyError, TypeError):
470             LOG.exception(_LE('Failed to set instance name using '
471                               'multi_instance_display_name_template.'))
472             new_name = display_name
473         return new_name
474 
475     def _apply_instance_name_template(self, context, instance, index):
476         original_name = instance.display_name
477         new_name = self._new_instance_name_from_template(instance.uuid,
478                 instance.display_name, index)
479         instance.display_name = new_name
480         if not instance.get('hostname', None):
481             if utils.sanitize_hostname(original_name) == "":
482                 instance.hostname = self._default_host_name(instance.uuid)
483             else:
484                 instance.hostname = utils.sanitize_hostname(new_name)
485         return instance
486 
487     def _check_config_drive(self, config_drive):
488         if config_drive:
489             try:
490                 bool_val = strutils.bool_from_string(config_drive,
491                                                      strict=True)
492             except ValueError:
493                 raise exception.ConfigDriveInvalidValue(option=config_drive)
494         else:
495             bool_val = False
496         # FIXME(comstud):  Bug ID 1193438 filed for this. This looks silly,
497         # but this is because the config drive column is a String.  False
498         # is represented by using an empty string.  And for whatever
499         # reason, we rely on the DB to cast True to a String.
500         return True if bool_val else ''
501 
502     def _check_requested_image(self, context, image_id, image,
503                                instance_type, root_bdm):
504         if not image:
505             return
506 
507         if image['status'] != 'active':
508             raise exception.ImageNotActive(image_id=image_id)
509 
510         image_properties = image.get('properties', {})
511         config_drive_option = image_properties.get(
512             'img_config_drive', 'optional')
513         if config_drive_option not in ['optional', 'mandatory']:
514             raise exception.InvalidImageConfigDrive(
515                 config_drive=config_drive_option)
516 
517         if instance_type['memory_mb'] < int(image.get('min_ram') or 0):
518             raise exception.FlavorMemoryTooSmall()
519 
520         # Image min_disk is in gb, size is in bytes. For sanity, have them both
521         # in bytes.
522         image_min_disk = int(image.get('min_disk') or 0) * units.Gi
523         image_size = int(image.get('size') or 0)
524 
525         # Target disk is a volume. Don't check flavor disk size because it
526         # doesn't make sense, and check min_disk against the volume size.
527         if (root_bdm is not None and root_bdm.is_volume):
528             # There are 2 possibilities here: either the target volume already
529             # exists, or it doesn't, in which case the bdm will contain the
530             # intended volume size.
531             #
532             # Cinder does its own check against min_disk, so if the target
533             # volume already exists this has already been done and we don't
534             # need to check it again here. In this case, volume_size may not be
535             # set on the bdm.
536             #
537             # If we're going to create the volume, the bdm will contain
538             # volume_size. Therefore we should check it if it exists. This will
539             # still be checked again by cinder when the volume is created, but
540             # that will not happen until the request reaches a host. By
541             # checking it here, the user gets an immediate and useful failure
542             # indication.
543             #
544             # The third possibility is that we have failed to consider
545             # something, and there are actually more than 2 possibilities. In
546             # this case cinder will still do the check at volume creation time.
547             # The behaviour will still be correct, but the user will not get an
548             # immediate failure from the api, and will instead have to
549             # determine why the instance is in an error state with a task of
550             # block_device_mapping.
551             #
552             # We could reasonably refactor this check into _validate_bdm at
553             # some future date, as the various size logic is already split out
554             # in there.
555             dest_size = root_bdm.volume_size
556             if dest_size is not None:
557                 dest_size *= units.Gi
558 
559                 if image_min_disk > dest_size:
560                     raise exception.VolumeSmallerThanMinDisk(
561                         volume_size=dest_size, image_min_disk=image_min_disk)
562 
563         # Target disk is a local disk whose size is taken from the flavor
564         else:
565             dest_size = instance_type['root_gb'] * units.Gi
566 
567             # NOTE(johannes): root_gb is allowed to be 0 for legacy reasons
568             # since libvirt interpreted the value differently than other
569             # drivers. A value of 0 means don't check size.
570             if dest_size != 0:
571                 if image_size > dest_size:
572                     raise exception.FlavorDiskSmallerThanImage(
573                         flavor_size=dest_size, image_size=image_size)
574 
575                 if image_min_disk > dest_size:
576                     raise exception.FlavorDiskSmallerThanMinDisk(
577                         flavor_size=dest_size, image_min_disk=image_min_disk)
578 
579     def _get_image_defined_bdms(self, instance_type, image_meta,
580                                 root_device_name):
581         image_properties = image_meta.get('properties', {})
582 
583         # Get the block device mappings defined by the image.
584         image_defined_bdms = image_properties.get('block_device_mapping', [])
585         legacy_image_defined = not image_properties.get('bdm_v2', False)
586 
587         image_mapping = image_properties.get('mappings', [])
588 
589         if legacy_image_defined:
590             image_defined_bdms = block_device.from_legacy_mapping(
591                 image_defined_bdms, None, root_device_name)
592         else:
593             image_defined_bdms = list(map(block_device.BlockDeviceDict,
594                                           image_defined_bdms))
595 
596         if image_mapping:
597             image_mapping = self._prepare_image_mapping(instance_type,
598                                                         image_mapping)
599             image_defined_bdms = self._merge_bdms_lists(
600                 image_mapping, image_defined_bdms)
601 
602         return image_defined_bdms
603 
604     def _get_flavor_defined_bdms(self, instance_type, block_device_mapping):
605         flavor_defined_bdms = []
606 
607         have_ephemeral_bdms = any(filter(
608             block_device.new_format_is_ephemeral, block_device_mapping))
609         have_swap_bdms = any(filter(
610             block_device.new_format_is_swap, block_device_mapping))
611 
612         if instance_type.get('ephemeral_gb') and not have_ephemeral_bdms:
613             flavor_defined_bdms.append(
614                 block_device.create_blank_bdm(instance_type['ephemeral_gb']))
615         if instance_type.get('swap') and not have_swap_bdms:
616             flavor_defined_bdms.append(
617                 block_device.create_blank_bdm(instance_type['swap'], 'swap'))
618 
619         return flavor_defined_bdms
620 
621     def _merge_bdms_lists(self, overridable_mappings, overrider_mappings):
622         """Override any block devices from the first list by device name
623 
624         :param overridable_mappings: list which items are overridden
625         :param overrider_mappings: list which items override
626 
627         :returns: A merged list of bdms
628         """
629         device_names = set(bdm['device_name'] for bdm in overrider_mappings
630                            if bdm['device_name'])
631         return (overrider_mappings +
632                 [bdm for bdm in overridable_mappings
633                  if bdm['device_name'] not in device_names])
634 
635     def _check_and_transform_bdm(self, context, base_options, instance_type,
636                                  image_meta, min_count, max_count,
637                                  block_device_mapping, legacy_bdm):
638         # NOTE (ndipanov): Assume root dev name is 'vda' if not supplied.
639         #                  It's needed for legacy conversion to work.
640         root_device_name = (base_options.get('root_device_name') or 'vda')
641         image_ref = base_options.get('image_ref', '')
642         # If the instance is booted by image and has a volume attached,
643         # the volume cannot have the same device name as root_device_name
644         if image_ref:
645             for bdm in block_device_mapping:
646                 if (bdm.get('destination_type') == 'volume' and
647                     block_device.strip_dev(bdm.get(
648                     'device_name')) == root_device_name):
649                     msg = _('The volume cannot be assigned the same device'
650                             ' name as the root device %s') % root_device_name
651                     raise exception.InvalidRequest(msg)
652 
653         image_defined_bdms = self._get_image_defined_bdms(
654             instance_type, image_meta, root_device_name)
655         root_in_image_bdms = (
656             block_device.get_root_bdm(image_defined_bdms) is not None)
657 
658         if legacy_bdm:
659             block_device_mapping = block_device.from_legacy_mapping(
660                 block_device_mapping, image_ref, root_device_name,
661                 no_root=root_in_image_bdms)
662         elif root_in_image_bdms:
663             # NOTE (ndipanov): client will insert an image mapping into the v2
664             # block_device_mapping, but if there is a bootable device in image
665             # mappings - we need to get rid of the inserted image
666             # NOTE (gibi): another case is when a server is booted with an
667             # image to bdm mapping where the image only contains a bdm to a
668             # snapshot. In this case the other image to bdm mapping
669             # contains an unnecessary device with boot_index == 0.
670             # Also in this case the image_ref is None as we are booting from
671             # an image to volume bdm.
672             def not_image_and_root_bdm(bdm):
673                 return not (bdm.get('boot_index') == 0 and
674                             bdm.get('source_type') == 'image')
675 
676             block_device_mapping = list(
677                 filter(not_image_and_root_bdm, block_device_mapping))
678 
679         block_device_mapping = self._merge_bdms_lists(
680             image_defined_bdms, block_device_mapping)
681 
682         if min_count > 1 or max_count > 1:
683             if any(map(lambda bdm: bdm['source_type'] == 'volume',
684                        block_device_mapping)):
685                 msg = _('Cannot attach one or more volumes to multiple'
686                         ' instances')
687                 raise exception.InvalidRequest(msg)
688 
689         block_device_mapping += self._get_flavor_defined_bdms(
690             instance_type, block_device_mapping)
691 
692         return block_device_obj.block_device_make_list_from_dicts(
693                 context, block_device_mapping)
694 
695     def _get_image(self, context, image_href):
696         if not image_href:
697             return None, {}
698 
699         image = self.image_api.get(context, image_href)
700         return image['id'], image
701 
702     def _checks_for_create_and_rebuild(self, context, image_id, image,
703                                        instance_type, metadata,
704                                        files_to_inject, root_bdm):
705         self._check_metadata_properties_quota(context, metadata)
706         self._check_injected_file_quota(context, files_to_inject)
707         self._check_requested_image(context, image_id, image,
708                                     instance_type, root_bdm)
709 
710     def _validate_and_build_base_options(self, context, instance_type,
711                                          boot_meta, image_href, image_id,
712                                          kernel_id, ramdisk_id, display_name,
713                                          display_description, key_name,
714                                          key_data, security_groups,
715                                          availability_zone, user_data,
716                                          metadata, access_ip_v4, access_ip_v6,
717                                          requested_networks, config_drive,
718                                          auto_disk_config, reservation_id,
719                                          max_count):
720         """Verify all the input parameters regardless of the provisioning
721         strategy being performed.
722         """
723         if instance_type['disabled']:
724             raise exception.FlavorNotFound(flavor_id=instance_type['id'])
725 
726         if user_data:
727             l = len(user_data)
728             if l > MAX_USERDATA_SIZE:
729                 # NOTE(mikal): user_data is stored in a text column, and
730                 # the database might silently truncate if its over length.
731                 raise exception.InstanceUserDataTooLarge(
732                     length=l, maxsize=MAX_USERDATA_SIZE)
733 
734             try:
735                 base64utils.decode_as_bytes(user_data)
736             except (base64.binascii.Error, TypeError):
737                 # TODO(harlowja): reduce the above exceptions caught to
738                 # only type error once we get a new oslo.serialization
739                 # release that captures and makes only one be output.
740                 #
741                 # We can eliminate the capture of `binascii.Error` when:
742                 #
743                 # https://review.openstack.org/#/c/418066/ is released.
744                 raise exception.InstanceUserDataMalformed()
745 
746         # When using Neutron, _check_requested_secgroups will translate and
747         # return any requested security group names to uuids.
748         security_groups = (
749             self._check_requested_secgroups(context, security_groups))
750 
751         # Note:  max_count is the number of instances requested by the user,
752         # max_network_count is the maximum number of instances taking into
753         # account any network quotas
754         max_network_count = self._check_requested_networks(context,
755                                      requested_networks, max_count)
756 
757         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
758                 context, kernel_id, ramdisk_id, boot_meta)
759 
760         config_drive = self._check_config_drive(config_drive)
761 
762         if key_data is None and key_name is not None:
763             key_pair = objects.KeyPair.get_by_name(context,
764                                                    context.user_id,
765                                                    key_name)
766             key_data = key_pair.public_key
767         else:
768             key_pair = None
769 
770         root_device_name = block_device.prepend_dev(
771                 block_device.properties_root_device_name(
772                     boot_meta.get('properties', {})))
773 
774         try:
775             image_meta = objects.ImageMeta.from_dict(boot_meta)
776         except ValueError as e:
777             # there must be invalid values in the image meta properties so
778             # consider this an invalid request
779             msg = _('Invalid image metadata. Error: %s') % six.text_type(e)
780             raise exception.InvalidRequest(msg)
781         numa_topology = hardware.numa_get_constraints(
782                 instance_type, image_meta)
783 
784         system_metadata = {}
785 
786         # PCI requests come from two sources: instance flavor and
787         # requested_networks. The first call in below returns an
788         # InstancePCIRequests object which is a list of InstancePCIRequest
789         # objects. The second call in below creates an InstancePCIRequest
790         # object for each SR-IOV port, and append it to the list in the
791         # InstancePCIRequests object
792         pci_request_info = pci_request.get_pci_requests_from_flavor(
793             instance_type)
794         self.network_api.create_pci_requests_for_sriov_ports(context,
795             pci_request_info, requested_networks)
796 
797         base_options = {
798             'reservation_id': reservation_id,
799             'image_ref': image_href,
800             'kernel_id': kernel_id or '',
801             'ramdisk_id': ramdisk_id or '',
802             'power_state': power_state.NOSTATE,
803             'vm_state': vm_states.BUILDING,
804             'config_drive': config_drive,
805             'user_id': context.user_id,
806             'project_id': context.project_id,
807             'instance_type_id': instance_type['id'],
808             'memory_mb': instance_type['memory_mb'],
809             'vcpus': instance_type['vcpus'],
810             'root_gb': instance_type['root_gb'],
811             'ephemeral_gb': instance_type['ephemeral_gb'],
812             'display_name': display_name,
813             'display_description': display_description,
814             'user_data': user_data,
815             'key_name': key_name,
816             'key_data': key_data,
817             'locked': False,
818             'metadata': metadata or {},
819             'access_ip_v4': access_ip_v4,
820             'access_ip_v6': access_ip_v6,
821             'availability_zone': availability_zone,
822             'root_device_name': root_device_name,
823             'progress': 0,
824             'pci_requests': pci_request_info,
825             'numa_topology': numa_topology,
826             'system_metadata': system_metadata}
827 
828         options_from_image = self._inherit_properties_from_image(
829                 boot_meta, auto_disk_config)
830 
831         base_options.update(options_from_image)
832 
833         # return the validated options and maximum number of instances allowed
834         # by the network quotas
835         return base_options, max_network_count, key_pair, security_groups
836 
837     def _provision_instances(self, context, instance_type, min_count,
838             max_count, base_options, boot_meta, security_groups,
839             block_device_mapping, shutdown_terminate,
840             instance_group, check_server_group_quota, filter_properties,
841             key_pair):
842         # Check quotas
843         num_instances = compute_utils.check_num_instances_quota(
844                 context, instance_type, min_count, max_count,
845                 project_id=context.project_id, user_id=context.user_id)
846         security_groups = self.security_group_api.populate_security_groups(
847                 security_groups)
848         self.security_group_api.ensure_default(context)
849         LOG.debug("Going to run %s instances...", num_instances)
850         instances_to_build = []
851         try:
852             for i in range(num_instances):
853                 # Create a uuid for the instance so we can store the
854                 # RequestSpec before the instance is created.
855                 instance_uuid = uuidutils.generate_uuid()
856                 # Store the RequestSpec that will be used for scheduling.
857                 req_spec = objects.RequestSpec.from_components(context,
858                         instance_uuid, boot_meta, instance_type,
859                         base_options['numa_topology'],
860                         base_options['pci_requests'], filter_properties,
861                         instance_group, base_options['availability_zone'],
862                         security_groups=security_groups)
863                 # NOTE(danms): We need to record num_instances on the request
864                 # spec as this is how the conductor knows how many were in this
865                 # batch.
866                 req_spec.num_instances = num_instances
867                 req_spec.create()
868 
869                 # Create an instance object, but do not store in db yet.
870                 instance = objects.Instance(context=context)
871                 instance.uuid = instance_uuid
872                 instance.update(base_options)
873                 instance.keypairs = objects.KeyPairList(objects=[])
874                 if key_pair:
875                     instance.keypairs.objects.append(key_pair)
876                 instance = self.create_db_entry_for_new_instance(context,
877                         instance_type, boot_meta, instance, security_groups,
878                         block_device_mapping, num_instances, i,
879                         shutdown_terminate, create_instance=False)
880                 block_device_mapping = (
881                     self._bdm_validate_set_size_and_instance(context,
882                         instance, instance_type, block_device_mapping))
883 
884                 # NOTE(danms): BDMs are still not created, so we need to pass
885                 # a clone and then reset them on our object after create so
886                 # that they're still dirty for later in this process
887                 build_request = objects.BuildRequest(context,
888                         instance=instance, instance_uuid=instance.uuid,
889                         project_id=instance.project_id,
890                         block_device_mappings=block_device_mapping.obj_clone())
891                 build_request.create()
892                 build_request.block_device_mappings = block_device_mapping
893 
894                 # Create an instance_mapping.  The null cell_mapping indicates
895                 # that the instance doesn't yet exist in a cell, and lookups
896                 # for it need to instead look for the RequestSpec.
897                 # cell_mapping will be populated after scheduling, with a
898                 # scheduling failure using the cell_mapping for the special
899                 # cell0.
900                 inst_mapping = objects.InstanceMapping(context=context)
901                 inst_mapping.instance_uuid = instance_uuid
902                 inst_mapping.project_id = context.project_id
903                 inst_mapping.cell_mapping = None
904                 inst_mapping.create()
905 
906                 instances_to_build.append(
907                     (req_spec, build_request, inst_mapping))
908 
909                 if instance_group:
910                     if check_server_group_quota:
911                         try:
912                             objects.Quotas.check_deltas(
913                                 context, 'server_group_members',
914                                 {'server_group_members': 1}, instance_group,
915                                 context.user_id)
916                         except exception.OverQuota:
917                             msg = _("Quota exceeded, too many servers in "
918                                     "group")
919                             raise exception.QuotaError(msg)
920 
921                     members = objects.InstanceGroup.add_members(
922                         context, instance_group.uuid, [instance.uuid])
923 
924                     # NOTE(melwitt): We recheck the quota after creating the
925                     # object to handle races. Delete the object if we're over
926                     # quota after creating the object.
927                     if check_server_group_quota:
928                         try:
929                             objects.Quotas.check_deltas(
930                                 context, 'server_group_members',
931                                 {'server_group_members': 0}, instance_group,
932                                 context.user_id)
933                         except exception.OverQuota:
934                             objects.Quotas._remove_members_in_db(context,
935                                 instance_group.id, [instance.uuid])
936                             msg = _("Quota exceeded, too many servers in "
937                                     "group")
938                             raise exception.QuotaError(msg)
939                     # list of members added to servers group in this iteration
940                     # is needed to check quota of server group during add next
941                     # instance
942                     instance_group.members.extend(members)
943 
944         # In the case of any exceptions, attempt DB cleanup
945         except Exception:
946             with excutils.save_and_reraise_exception():
947                 for rs, br, im in instances_to_build:
948                     try:
949                         rs.destroy()
950                     except exception.RequestSpecNotFound:
951                         pass
952                     try:
953                         im.destroy()
954                     except exception.InstanceMappingNotFound:
955                         pass
956                     try:
957                         br.destroy()
958                     except exception.BuildRequestNotFound:
959                         pass
960 
961         return instances_to_build
962 
963     def _get_bdm_image_metadata(self, context, block_device_mapping,
964                                 legacy_bdm=True):
965         """If we are booting from a volume, we need to get the
966         volume details from Cinder and make sure we pass the
967         metadata back accordingly.
968         """
969         if not block_device_mapping:
970             return {}
971 
972         for bdm in block_device_mapping:
973             if (legacy_bdm and
974                     block_device.get_device_letter(
975                        bdm.get('device_name', '')) != 'a'):
976                 continue
977             elif not legacy_bdm and bdm.get('boot_index') != 0:
978                 continue
979 
980             volume_id = bdm.get('volume_id')
981             snapshot_id = bdm.get('snapshot_id')
982             if snapshot_id:
983                 # NOTE(alaski): A volume snapshot inherits metadata from the
984                 # originating volume, but the API does not expose metadata
985                 # on the snapshot itself.  So we query the volume for it below.
986                 snapshot = self.volume_api.get_snapshot(context, snapshot_id)
987                 volume_id = snapshot['volume_id']
988 
989             if bdm.get('image_id'):
990                 try:
991                     image_id = bdm['image_id']
992                     image_meta = self.image_api.get(context, image_id)
993                     return image_meta
994                 except Exception:
995                     raise exception.InvalidBDMImage(id=image_id)
996             elif volume_id:
997                 try:
998                     volume = self.volume_api.get(context, volume_id)
999                 except exception.CinderConnectionFailed:
1000                     raise
1001                 except Exception:
1002                     raise exception.InvalidBDMVolume(id=volume_id)
1003 
1004                 if not volume.get('bootable', True):
1005                     raise exception.InvalidBDMVolumeNotBootable(id=volume_id)
1006 
1007                 return utils.get_image_metadata_from_volume(volume)
1008         return {}
1009 
1010     @staticmethod
1011     def _get_requested_instance_group(context, filter_properties):
1012         if (not filter_properties or
1013                 not filter_properties.get('scheduler_hints')):
1014             return
1015 
1016         group_hint = filter_properties.get('scheduler_hints').get('group')
1017         if not group_hint:
1018             return
1019 
1020         return objects.InstanceGroup.get_by_uuid(context, group_hint)
1021 
1022     def _create_instance(self, context, instance_type,
1023                image_href, kernel_id, ramdisk_id,
1024                min_count, max_count,
1025                display_name, display_description,
1026                key_name, key_data, security_groups,
1027                availability_zone, user_data, metadata, injected_files,
1028                admin_password, access_ip_v4, access_ip_v6,
1029                requested_networks, config_drive,
1030                block_device_mapping, auto_disk_config, filter_properties,
1031                reservation_id=None, legacy_bdm=True, shutdown_terminate=False,
1032                check_server_group_quota=False):
1033         """Verify all the input parameters regardless of the provisioning
1034         strategy being performed and schedule the instance(s) for
1035         creation.
1036         """
1037 
1038         # Normalize and setup some parameters
1039         if reservation_id is None:
1040             reservation_id = utils.generate_uid('r')
1041         security_groups = security_groups or ['default']
1042         min_count = min_count or 1
1043         max_count = max_count or min_count
1044         block_device_mapping = block_device_mapping or []
1045 
1046         if image_href:
1047             image_id, boot_meta = self._get_image(context, image_href)
1048         else:
1049             image_id = None
1050             boot_meta = self._get_bdm_image_metadata(
1051                 context, block_device_mapping, legacy_bdm)
1052 
1053         self._check_auto_disk_config(image=boot_meta,
1054                                      auto_disk_config=auto_disk_config)
1055 
1056         base_options, max_net_count, key_pair, security_groups = \
1057                 self._validate_and_build_base_options(
1058                     context, instance_type, boot_meta, image_href, image_id,
1059                     kernel_id, ramdisk_id, display_name, display_description,
1060                     key_name, key_data, security_groups, availability_zone,
1061                     user_data, metadata, access_ip_v4, access_ip_v6,
1062                     requested_networks, config_drive, auto_disk_config,
1063                     reservation_id, max_count)
1064 
1065         # max_net_count is the maximum number of instances requested by the
1066         # user adjusted for any network quota constraints, including
1067         # consideration of connections to each requested network
1068         if max_net_count < min_count:
1069             raise exception.PortLimitExceeded()
1070         elif max_net_count < max_count:
1071             LOG.info(_LI("max count reduced from %(max_count)d to "
1072                          "%(max_net_count)d due to network port quota"),
1073                         {'max_count': max_count,
1074                          'max_net_count': max_net_count})
1075             max_count = max_net_count
1076 
1077         block_device_mapping = self._check_and_transform_bdm(context,
1078             base_options, instance_type, boot_meta, min_count, max_count,
1079             block_device_mapping, legacy_bdm)
1080 
1081         # We can't do this check earlier because we need bdms from all sources
1082         # to have been merged in order to get the root bdm.
1083         self._checks_for_create_and_rebuild(context, image_id, boot_meta,
1084                 instance_type, metadata, injected_files,
1085                 block_device_mapping.root_bdm())
1086 
1087         instance_group = self._get_requested_instance_group(context,
1088                                    filter_properties)
1089 
1090         instances_to_build = self._provision_instances(context, instance_type,
1091                 min_count, max_count, base_options, boot_meta, security_groups,
1092                 block_device_mapping, shutdown_terminate,
1093                 instance_group, check_server_group_quota, filter_properties,
1094                 key_pair)
1095 
1096         instances = []
1097         request_specs = []
1098         build_requests = []
1099         for rs, build_request, im in instances_to_build:
1100             build_requests.append(build_request)
1101             instance = build_request.get_new_instance(context)
1102             instances.append(instance)
1103             request_specs.append(rs)
1104 
1105         if CONF.cells.enable:
1106             # NOTE(danms): CellsV1 can't do the new thing, so we
1107             # do the old thing here. We can remove this path once
1108             # we stop supporting v1.
1109             for instance in instances:
1110                 instance.create()
1111                 # NOTE(melwitt): We recheck the quota after creating the
1112                 # object to handle races. Delete the object if we're over
1113                 # quota after creating the object.
1114                 try:
1115                     compute_utils.check_num_instances_quota(
1116                         context, instance_type, 0, 0,
1117                         project_id=context.project_id, user_id=context.user_id)
1118                 except exception.TooManyInstances:
1119                     with excutils.save_and_reraise_exception():
1120                         instance.destroy()
1121             self.compute_task_api.build_instances(context,
1122                 instances=instances, image=boot_meta,
1123                 filter_properties=filter_properties,
1124                 admin_password=admin_password,
1125                 injected_files=injected_files,
1126                 requested_networks=requested_networks,
1127                 security_groups=security_groups,
1128                 block_device_mapping=block_device_mapping,
1129                 legacy_bdm=False)
1130         else:
1131             self.compute_task_api.schedule_and_build_instances(
1132                 context,
1133                 build_requests=build_requests,
1134                 request_spec=request_specs,
1135                 image=boot_meta,
1136                 admin_password=admin_password,
1137                 injected_files=injected_files,
1138                 requested_networks=requested_networks,
1139                 block_device_mapping=block_device_mapping)
1140 
1141         return (instances, reservation_id)
1142 
1143     @staticmethod
1144     def _volume_size(instance_type, bdm):
1145         size = bdm.get('volume_size')
1146         # NOTE (ndipanov): inherit flavor size only for swap and ephemeral
1147         if (size is None and bdm.get('source_type') == 'blank' and
1148                 bdm.get('destination_type') == 'local'):
1149             if bdm.get('guest_format') == 'swap':
1150                 size = instance_type.get('swap', 0)
1151             else:
1152                 size = instance_type.get('ephemeral_gb', 0)
1153         return size
1154 
1155     def _prepare_image_mapping(self, instance_type, mappings):
1156         """Extract and format blank devices from image mappings."""
1157 
1158         prepared_mappings = []
1159 
1160         for bdm in block_device.mappings_prepend_dev(mappings):
1161             LOG.debug("Image bdm %s", bdm)
1162 
1163             virtual_name = bdm['virtual']
1164             if virtual_name == 'ami' or virtual_name == 'root':
1165                 continue
1166 
1167             if not block_device.is_swap_or_ephemeral(virtual_name):
1168                 continue
1169 
1170             guest_format = bdm.get('guest_format')
1171             if virtual_name == 'swap':
1172                 guest_format = 'swap'
1173             if not guest_format:
1174                 guest_format = CONF.default_ephemeral_format
1175 
1176             values = block_device.BlockDeviceDict({
1177                 'device_name': bdm['device'],
1178                 'source_type': 'blank',
1179                 'destination_type': 'local',
1180                 'device_type': 'disk',
1181                 'guest_format': guest_format,
1182                 'delete_on_termination': True,
1183                 'boot_index': -1})
1184 
1185             values['volume_size'] = self._volume_size(
1186                 instance_type, values)
1187             if values['volume_size'] == 0:
1188                 continue
1189 
1190             prepared_mappings.append(values)
1191 
1192         return prepared_mappings
1193 
1194     def _bdm_validate_set_size_and_instance(self, context, instance,
1195                                             instance_type,
1196                                             block_device_mapping):
1197         """Ensure the bdms are valid, then set size and associate with instance
1198 
1199         Because this method can be called multiple times when more than one
1200         instance is booted in a single request it makes a copy of the bdm list.
1201         """
1202         LOG.debug("block_device_mapping %s", list(block_device_mapping),
1203                   instance_uuid=instance.uuid)
1204         self._validate_bdm(
1205             context, instance, instance_type, block_device_mapping)
1206         instance_block_device_mapping = block_device_mapping.obj_clone()
1207         for bdm in instance_block_device_mapping:
1208             bdm.volume_size = self._volume_size(instance_type, bdm)
1209             bdm.instance_uuid = instance.uuid
1210         return instance_block_device_mapping
1211 
1212     def _create_block_device_mapping(self, block_device_mapping):
1213         # Copy the block_device_mapping because this method can be called
1214         # multiple times when more than one instance is booted in a single
1215         # request. This avoids 'id' being set and triggering the object dupe
1216         # detection
1217         db_block_device_mapping = copy.deepcopy(block_device_mapping)
1218         # Create the BlockDeviceMapping objects in the db.
1219         for bdm in db_block_device_mapping:
1220             # TODO(alaski): Why is this done?
1221             if bdm.volume_size == 0:
1222                 continue
1223 
1224             bdm.update_or_create()
1225 
1226     def _validate_bdm(self, context, instance, instance_type,
1227                       block_device_mappings):
1228         def _subsequent_list(l):
1229             # Each device which is capable of being used as boot device should
1230             # be given a unique boot index, starting from 0 in ascending order.
1231             return all(el + 1 == l[i + 1] for i, el in enumerate(l[:-1]))
1232 
1233         # Make sure that the boot indexes make sense.
1234         # Setting a negative value or None indicates that the device should not
1235         # be used for booting.
1236         boot_indexes = sorted([bdm.boot_index
1237                                for bdm in block_device_mappings
1238                                if bdm.boot_index is not None
1239                                and bdm.boot_index >= 0])
1240 
1241         if 0 not in boot_indexes or not _subsequent_list(boot_indexes):
1242             # Convert the BlockDeviceMappingList to a list for repr details.
1243             LOG.debug('Invalid block device mapping boot sequence for '
1244                       'instance: %s', list(block_device_mappings),
1245                       instance=instance)
1246             raise exception.InvalidBDMBootSequence()
1247 
1248         for bdm in block_device_mappings:
1249             # NOTE(vish): For now, just make sure the volumes are accessible.
1250             # Additionally, check that the volume can be attached to this
1251             # instance.
1252             snapshot_id = bdm.snapshot_id
1253             volume_id = bdm.volume_id
1254             image_id = bdm.image_id
1255             if (image_id is not None and
1256                     image_id != instance.get('image_ref')):
1257                 try:
1258                     self._get_image(context, image_id)
1259                 except Exception:
1260                     raise exception.InvalidBDMImage(id=image_id)
1261                 if (bdm.source_type == 'image' and
1262                         bdm.destination_type == 'volume' and
1263                         not bdm.volume_size):
1264                     raise exception.InvalidBDM(message=_("Images with "
1265                         "destination_type 'volume' need to have a non-zero "
1266                         "size specified"))
1267             elif volume_id is not None:
1268                 min_compute_version = objects.Service.get_minimum_version(
1269                     context, 'nova-compute')
1270                 try:
1271                     # NOTE(ildikov): The boot from volume operation did not
1272                     # reserve the volume before Pike and as the older computes
1273                     # are running 'check_attach' which will fail if the volume
1274                     # is in 'attaching' state; if the compute service version
1275                     # is not high enough we will just perform the old check as
1276                     # opposed to reserving the volume here.
1277                     if (min_compute_version >=
1278                         BFV_RESERVE_MIN_COMPUTE_VERSION):
1279                         volume = self._check_attach_and_reserve_volume(
1280                             context, volume_id, instance)
1281                     else:
1282                         # NOTE(ildikov): This call is here only for backward
1283                         # compatibility can be removed after Ocata EOL.
1284                         volume = self._check_attach(context, volume_id,
1285                                                     instance)
1286                     bdm.volume_size = volume.get('size')
1287                 except (exception.CinderConnectionFailed,
1288                         exception.InvalidVolume):
1289                     raise
1290                 except exception.InvalidInput as exc:
1291                     raise exception.InvalidVolume(reason=exc.format_message())
1292                 except Exception:
1293                     raise exception.InvalidBDMVolume(id=volume_id)
1294             elif snapshot_id is not None:
1295                 try:
1296                     snap = self.volume_api.get_snapshot(context, snapshot_id)
1297                     bdm.volume_size = bdm.volume_size or snap.get('size')
1298                 except exception.CinderConnectionFailed:
1299                     raise
1300                 except Exception:
1301                     raise exception.InvalidBDMSnapshot(id=snapshot_id)
1302             elif (bdm.source_type == 'blank' and
1303                     bdm.destination_type == 'volume' and
1304                     not bdm.volume_size):
1305                 raise exception.InvalidBDM(message=_("Blank volumes "
1306                     "(source: 'blank', dest: 'volume') need to have non-zero "
1307                     "size"))
1308 
1309         ephemeral_size = sum(bdm.volume_size or instance_type['ephemeral_gb']
1310                 for bdm in block_device_mappings
1311                 if block_device.new_format_is_ephemeral(bdm))
1312         if ephemeral_size > instance_type['ephemeral_gb']:
1313             raise exception.InvalidBDMEphemeralSize()
1314 
1315         # There should be only one swap
1316         swap_list = block_device.get_bdm_swap_list(block_device_mappings)
1317         if len(swap_list) > 1:
1318             msg = _("More than one swap drive requested.")
1319             raise exception.InvalidBDMFormat(details=msg)
1320 
1321         if swap_list:
1322             swap_size = swap_list[0].volume_size or 0
1323             if swap_size > instance_type['swap']:
1324                 raise exception.InvalidBDMSwapSize()
1325 
1326         max_local = CONF.max_local_block_devices
1327         if max_local >= 0:
1328             num_local = len([bdm for bdm in block_device_mappings
1329                              if bdm.destination_type == 'local'])
1330             if num_local > max_local:
1331                 raise exception.InvalidBDMLocalsLimit()
1332 
1333     def _check_attach(self, context, volume_id, instance):
1334         # TODO(ildikov): This check_attach code is kept only for backward
1335         # compatibility and should be removed after Ocata EOL.
1336         volume = self.volume_api.get(context, volume_id)
1337         if volume['status'] != 'available':
1338             msg = _("volume '%(vol)s' status must be 'available'. Currently "
1339                     "in '%(status)s'") % {'vol': volume['id'],
1340                                           'status': volume['status']}
1341             raise exception.InvalidVolume(reason=msg)
1342         if volume['attach_status'] == 'attached':
1343             msg = _("volume %s already attached") % volume['id']
1344             raise exception.InvalidVolume(reason=msg)
1345         self.volume_api.check_availability_zone(context, volume,
1346                                                 instance=instance)
1347 
1348         return volume
1349 
1350     def _populate_instance_names(self, instance, num_instances):
1351         """Populate instance display_name and hostname."""
1352         display_name = instance.get('display_name')
1353         if instance.obj_attr_is_set('hostname'):
1354             hostname = instance.get('hostname')
1355         else:
1356             hostname = None
1357 
1358         # NOTE(mriedem): This is only here for test simplicity since a server
1359         # name is required in the REST API.
1360         if display_name is None:
1361             display_name = self._default_display_name(instance.uuid)
1362             instance.display_name = display_name
1363 
1364         if hostname is None and num_instances == 1:
1365             # NOTE(russellb) In the multi-instance case, we're going to
1366             # overwrite the display_name using the
1367             # multi_instance_display_name_template.  We need the default
1368             # display_name set so that it can be used in the template, though.
1369             # Only set the hostname here if we're only creating one instance.
1370             # Otherwise, it will be built after the template based
1371             # display_name.
1372             hostname = display_name
1373             default_hostname = self._default_host_name(instance.uuid)
1374             instance.hostname = utils.sanitize_hostname(hostname,
1375                                                         default_hostname)
1376 
1377     def _default_display_name(self, instance_uuid):
1378         return "Server %s" % instance_uuid
1379 
1380     def _default_host_name(self, instance_uuid):
1381         return "Server-%s" % instance_uuid
1382 
1383     def _populate_instance_for_create(self, context, instance, image,
1384                                       index, security_groups, instance_type,
1385                                       num_instances, shutdown_terminate):
1386         """Build the beginning of a new instance."""
1387 
1388         instance.launch_index = index
1389         instance.vm_state = vm_states.BUILDING
1390         instance.task_state = task_states.SCHEDULING
1391         info_cache = objects.InstanceInfoCache()
1392         info_cache.instance_uuid = instance.uuid
1393         info_cache.network_info = network_model.NetworkInfo()
1394         instance.info_cache = info_cache
1395         instance.flavor = instance_type
1396         instance.old_flavor = None
1397         instance.new_flavor = None
1398         if CONF.ephemeral_storage_encryption.enabled:
1399             instance.ephemeral_key_uuid = self.key_manager.create_key(
1400                 context,
1401                 length=CONF.ephemeral_storage_encryption.key_size)
1402         else:
1403             instance.ephemeral_key_uuid = None
1404 
1405         # Store image properties so we can use them later
1406         # (for notifications, etc).  Only store what we can.
1407         if not instance.obj_attr_is_set('system_metadata'):
1408             instance.system_metadata = {}
1409         # Make sure we have the dict form that we need for instance_update.
1410         instance.system_metadata = utils.instance_sys_meta(instance)
1411 
1412         system_meta = utils.get_system_metadata_from_image(
1413             image, instance_type)
1414 
1415         # In case we couldn't find any suitable base_image
1416         system_meta.setdefault('image_base_image_ref', instance.image_ref)
1417 
1418         instance.system_metadata.update(system_meta)
1419 
1420         if CONF.use_neutron:
1421             # For Neutron we don't actually store anything in the database, we
1422             # proxy the security groups on the instance from the ports
1423             # attached to the instance.
1424             instance.security_groups = objects.SecurityGroupList()
1425         else:
1426             instance.security_groups = security_groups
1427 
1428         self._populate_instance_names(instance, num_instances)
1429         instance.shutdown_terminate = shutdown_terminate
1430         if num_instances > 1 and self.cell_type != 'api':
1431             instance = self._apply_instance_name_template(context, instance,
1432                                                           index)
1433 
1434         return instance
1435 
1436     # This method remains because cellsv1 uses it in the scheduler
1437     def create_db_entry_for_new_instance(self, context, instance_type, image,
1438             instance, security_group, block_device_mapping, num_instances,
1439             index, shutdown_terminate=False, create_instance=True):
1440         """Create an entry in the DB for this new instance,
1441         including any related table updates (such as security group,
1442         etc).
1443 
1444         This is called by the scheduler after a location for the
1445         instance has been determined.
1446 
1447         :param create_instance: Determines if the instance is created here or
1448             just populated for later creation. This is done so that this code
1449             can be shared with cellsv1 which needs the instance creation to
1450             happen here. It should be removed and this method cleaned up when
1451             cellsv1 is a distant memory.
1452         """
1453         self._populate_instance_for_create(context, instance, image, index,
1454                                            security_group, instance_type,
1455                                            num_instances, shutdown_terminate)
1456 
1457         if create_instance:
1458             instance.create()
1459 
1460         return instance
1461 
1462     def _check_multiple_instances_with_neutron_ports(self,
1463                                                      requested_networks):
1464         """Check whether multiple instances are created from port id(s)."""
1465         for requested_net in requested_networks:
1466             if requested_net.port_id:
1467                 msg = _("Unable to launch multiple instances with"
1468                         " a single configured port ID. Please launch your"
1469                         " instance one by one with different ports.")
1470                 raise exception.MultiplePortsNotApplicable(reason=msg)
1471 
1472     def _check_multiple_instances_with_specified_ip(self, requested_networks):
1473         """Check whether multiple instances are created with specified ip."""
1474 
1475         for requested_net in requested_networks:
1476             if requested_net.network_id and requested_net.address:
1477                 msg = _("max_count cannot be greater than 1 if an fixed_ip "
1478                         "is specified.")
1479                 raise exception.InvalidFixedIpAndMaxCountRequest(reason=msg)
1480 
1481     @hooks.add_hook("create_instance")
1482     def create(self, context, instance_type,
1483                image_href, kernel_id=None, ramdisk_id=None,
1484                min_count=None, max_count=None,
1485                display_name=None, display_description=None,
1486                key_name=None, key_data=None, security_groups=None,
1487                availability_zone=None, forced_host=None, forced_node=None,
1488                user_data=None, metadata=None, injected_files=None,
1489                admin_password=None, block_device_mapping=None,
1490                access_ip_v4=None, access_ip_v6=None, requested_networks=None,
1491                config_drive=None, auto_disk_config=None, scheduler_hints=None,
1492                legacy_bdm=True, shutdown_terminate=False,
1493                check_server_group_quota=False):
1494         """Provision instances, sending instance information to the
1495         scheduler.  The scheduler will determine where the instance(s)
1496         go and will handle creating the DB entries.
1497 
1498         Returns a tuple of (instances, reservation_id)
1499         """
1500         if requested_networks and max_count is not None and max_count > 1:
1501             self._check_multiple_instances_with_specified_ip(
1502                 requested_networks)
1503             if utils.is_neutron():
1504                 self._check_multiple_instances_with_neutron_ports(
1505                     requested_networks)
1506 
1507         if availability_zone:
1508             available_zones = availability_zones.\
1509                 get_availability_zones(context.elevated(), True)
1510             if forced_host is None and availability_zone not in \
1511                     available_zones:
1512                 msg = _('The requested availability zone is not available')
1513                 raise exception.InvalidRequest(msg)
1514 
1515         filter_properties = scheduler_utils.build_filter_properties(
1516                 scheduler_hints, forced_host, forced_node, instance_type)
1517 
1518         return self._create_instance(
1519                        context, instance_type,
1520                        image_href, kernel_id, ramdisk_id,
1521                        min_count, max_count,
1522                        display_name, display_description,
1523                        key_name, key_data, security_groups,
1524                        availability_zone, user_data, metadata,
1525                        injected_files, admin_password,
1526                        access_ip_v4, access_ip_v6,
1527                        requested_networks, config_drive,
1528                        block_device_mapping, auto_disk_config,
1529                        filter_properties=filter_properties,
1530                        legacy_bdm=legacy_bdm,
1531                        shutdown_terminate=shutdown_terminate,
1532                        check_server_group_quota=check_server_group_quota)
1533 
1534     def _check_auto_disk_config(self, instance=None, image=None,
1535                                 **extra_instance_updates):
1536         auto_disk_config = extra_instance_updates.get("auto_disk_config")
1537         if auto_disk_config is None:
1538             return
1539         if not image and not instance:
1540             return
1541 
1542         if image:
1543             image_props = image.get("properties", {})
1544             auto_disk_config_img = \
1545                 utils.get_auto_disk_config_from_image_props(image_props)
1546             image_ref = image.get("id")
1547         else:
1548             sys_meta = utils.instance_sys_meta(instance)
1549             image_ref = sys_meta.get('image_base_image_ref')
1550             auto_disk_config_img = \
1551                 utils.get_auto_disk_config_from_instance(sys_meta=sys_meta)
1552 
1553         self._ensure_auto_disk_config_is_valid(auto_disk_config_img,
1554                                                auto_disk_config,
1555                                                image_ref)
1556 
1557     def _lookup_instance(self, context, uuid):
1558         '''Helper method for pulling an instance object from a database.
1559 
1560         During the transition to cellsv2 there is some complexity around
1561         retrieving an instance from the database which this method hides. If
1562         there is an instance mapping then query the cell for the instance, if
1563         no mapping exists then query the configured nova database.
1564 
1565         Once we are past the point that all deployments can be assumed to be
1566         migrated to cellsv2 this method can go away.
1567         '''
1568         inst_map = None
1569         try:
1570             inst_map = objects.InstanceMapping.get_by_instance_uuid(
1571                 context, uuid)
1572         except exception.InstanceMappingNotFound:
1573             # TODO(alaski): This exception block can be removed once we're
1574             # guaranteed everyone is using cellsv2.
1575             pass
1576 
1577         if (inst_map is None or inst_map.cell_mapping is None or
1578                 CONF.cells.enable):
1579             # If inst_map is None then the deployment has not migrated to
1580             # cellsv2 yet.
1581             # If inst_map.cell_mapping is None then the instance is not in a
1582             # cell yet. Until instance creation moves to the conductor the
1583             # instance can be found in the configured database, so attempt
1584             # to look it up.
1585             # If we're on cellsv1, we can't yet short-circuit the cells
1586             # messaging path
1587             cell = None
1588             try:
1589                 instance = objects.Instance.get_by_uuid(context, uuid)
1590             except exception.InstanceNotFound:
1591                 # If we get here then the conductor is in charge of writing the
1592                 # instance to the database and hasn't done that yet. It's up to
1593                 # the caller of this method to determine what to do with that
1594                 # information.
1595                 return None, None
1596         else:
1597             cell = inst_map.cell_mapping
1598             with nova_context.target_cell(context, cell):
1599                 try:
1600                     instance = objects.Instance.get_by_uuid(context,
1601                                                             uuid)
1602                 except exception.InstanceNotFound:
1603                     # Since the cell_mapping exists we know the instance is in
1604                     # the cell, however InstanceNotFound means it's already
1605                     # deleted.
1606                     return None, None
1607         return cell, instance
1608 
1609     def _delete_while_booting(self, context, instance):
1610         """Handle deletion if the instance has not reached a cell yet
1611 
1612         Deletion before an instance reaches a cell needs to be handled
1613         differently. What we're attempting to do is delete the BuildRequest
1614         before the api level conductor does.  If we succeed here then the boot
1615         request stops before reaching a cell.  If not then the instance will
1616         need to be looked up in a cell db and the normal delete path taken.
1617         """
1618         deleted = self._attempt_delete_of_buildrequest(context, instance)
1619 
1620         # After service version 15 deletion of the BuildRequest will halt the
1621         # build process in the conductor. In that case run the rest of this
1622         # method and consider the instance deleted. If we have not yet reached
1623         # service version 15 then just return False so the rest of the delete
1624         # process will proceed usually.
1625         service_version = objects.Service.get_minimum_version(
1626             context, 'nova-osapi_compute')
1627         if service_version < 15:
1628             return False
1629 
1630         if deleted:
1631             # If we've reached this block the successful deletion of the
1632             # buildrequest indicates that the build process should be halted by
1633             # the conductor.
1634 
1635             # NOTE(alaski): Though the conductor halts the build process it
1636             # does not currently delete the instance record. This is
1637             # because in the near future the instance record will not be
1638             # created if the buildrequest has been deleted here. For now we
1639             # ensure the instance has been set to deleted at this point.
1640             # Yes this directly contradicts the comment earlier in this
1641             # method, but this is a temporary measure.
1642             # Look up the instance because the current instance object was
1643             # stashed on the buildrequest and therefore not complete enough
1644             # to run .destroy().
1645             try:
1646                 cell, instance = self._lookup_instance(context, instance.uuid)
1647                 if instance is not None:
1648                     # If instance is None it has already been deleted.
1649                     if cell:
1650                         with nova_context.target_cell(context, cell):
1651                             instance.destroy()
1652                     else:
1653                         instance.destroy()
1654             except exception.InstanceNotFound:
1655                 pass
1656 
1657             return True
1658         return False
1659 
1660     def _attempt_delete_of_buildrequest(self, context, instance):
1661         # If there is a BuildRequest then the instance may not have been
1662         # written to a cell db yet. Delete the BuildRequest here, which
1663         # will indicate that the Instance build should not proceed.
1664         try:
1665             build_req = objects.BuildRequest.get_by_instance_uuid(
1666                 context, instance.uuid)
1667             build_req.destroy()
1668         except exception.BuildRequestNotFound:
1669             # This means that conductor has deleted the BuildRequest so the
1670             # instance is now in a cell and the delete needs to proceed
1671             # normally.
1672             return False
1673         return True
1674 
1675     def _delete(self, context, instance, delete_type, cb, **instance_attrs):
1676         if instance.disable_terminate:
1677             LOG.info(_LI('instance termination disabled'),
1678                      instance=instance)
1679             return
1680 
1681         # If there is an instance.host the instance has been scheduled and
1682         # sent to a cell/compute which means it was pulled from the cell db.
1683         # Normal delete should be attempted.
1684         if not instance.host:
1685             try:
1686                 if self._delete_while_booting(context, instance):
1687                     return
1688                 # If instance.host was not set it's possible that the Instance
1689                 # object here was pulled from a BuildRequest object and is not
1690                 # fully populated. Notably it will be missing an 'id' field
1691                 # which will prevent instance.destroy from functioning
1692                 # properly. A lookup is attempted which will either return a
1693                 # full Instance or None if not found. If not found then it's
1694                 # acceptable to skip the rest of the delete processing.
1695                 cell, instance = self._lookup_instance(context, instance.uuid)
1696                 if cell and instance:
1697                     with nova_context.target_cell(context, cell):
1698                         instance.destroy()
1699                         return
1700                 if not instance:
1701                     # Instance is already deleted.
1702                     return
1703             except exception.ObjectActionError:
1704                 # NOTE(melwitt): This means the instance.host changed
1705                 # under us indicating the instance became scheduled
1706                 # during the destroy(). Refresh the instance from the DB and
1707                 # continue on with the delete logic for a scheduled instance.
1708                 # NOTE(danms): If instance.host is set, we should be able to
1709                 # do the following lookup. If not, there's not much we can
1710                 # do to recover.
1711                 cell, instance = self._lookup_instance(context, instance.uuid)
1712                 if not instance:
1713                     # Instance is already deleted
1714                     return
1715 
1716         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1717                 context, instance.uuid)
1718 
1719         # At these states an instance has a snapshot associate.
1720         if instance.vm_state in (vm_states.SHELVED,
1721                                  vm_states.SHELVED_OFFLOADED):
1722             snapshot_id = instance.system_metadata.get('shelved_image_id')
1723             LOG.info(_LI("Working on deleting snapshot %s "
1724                          "from shelved instance..."),
1725                      snapshot_id, instance=instance)
1726             try:
1727                 self.image_api.delete(context, snapshot_id)
1728             except (exception.ImageNotFound,
1729                     exception.ImageNotAuthorized) as exc:
1730                 LOG.warning(_LW("Failed to delete snapshot "
1731                                 "from shelved instance (%s)."),
1732                             exc.format_message(), instance=instance)
1733             except Exception:
1734                 LOG.exception(_LE("Something wrong happened when trying to "
1735                                   "delete snapshot from shelved instance."),
1736                               instance=instance)
1737 
1738         original_task_state = instance.task_state
1739         # NOTE(maoy): no expected_task_state needs to be set
1740         instance.update(instance_attrs)
1741         instance.progress = 0
1742         instance.save()
1743 
1744         if self.cell_type == 'api':
1745             # NOTE(comstud): If we're in the API cell, we need to
1746             # skip all remaining logic and just call the callback,
1747             # which will cause a cast to the child cell.
1748             cb(context, instance, bdms)
1749             return
1750         shelved_offloaded = (instance.vm_state
1751                              == vm_states.SHELVED_OFFLOADED)
1752         if not instance.host and not shelved_offloaded:
1753             try:
1754                 compute_utils.notify_about_instance_usage(
1755                         self.notifier, context, instance,
1756                         "%s.start" % delete_type)
1757                 instance.destroy()
1758                 compute_utils.notify_about_instance_usage(
1759                         self.notifier, context, instance,
1760                         "%s.end" % delete_type,
1761                         system_metadata=instance.system_metadata)
1762                 LOG.info(_LI('Instance deleted and does not have host '
1763                              'field, its vm_state is %(state)s.'),
1764                              {'state': instance.vm_state},
1765                              instance=instance)
1766                 return
1767             except exception.ObjectActionError:
1768                 instance.refresh()
1769 
1770         if instance.vm_state == vm_states.RESIZED:
1771             self._confirm_resize_on_deleting(context, instance)
1772 
1773         is_local_delete = True
1774         try:
1775             if not shelved_offloaded:
1776                 service = objects.Service.get_by_compute_host(
1777                     context.elevated(), instance.host)
1778                 is_local_delete = not self.servicegroup_api.service_is_up(
1779                     service)
1780             if not is_local_delete:
1781                 if original_task_state in (task_states.DELETING,
1782                                               task_states.SOFT_DELETING):
1783                     LOG.info(_LI('Instance is already in deleting state, '
1784                                  'ignoring this request'),
1785                              instance=instance)
1786                     return
1787                 self._record_action_start(context, instance,
1788                                           instance_actions.DELETE)
1789 
1790                 cb(context, instance, bdms)
1791         except exception.ComputeHostNotFound:
1792             pass
1793 
1794         if is_local_delete:
1795             # If instance is in shelved_offloaded state or compute node
1796             # isn't up, delete instance from db and clean bdms info and
1797             # network info
1798             self._local_delete(context, instance, bdms, delete_type, cb)
1799 
1800     def _confirm_resize_on_deleting(self, context, instance):
1801         # If in the middle of a resize, use confirm_resize to
1802         # ensure the original instance is cleaned up too
1803         migration = None
1804         for status in ('finished', 'confirming'):
1805             try:
1806                 migration = objects.Migration.get_by_instance_and_status(
1807                         context.elevated(), instance.uuid, status)
1808                 LOG.info(_LI('Found an unconfirmed migration during delete, '
1809                              'id: %(id)s, status: %(status)s'),
1810                          {'id': migration.id,
1811                           'status': migration.status},
1812                          instance=instance)
1813                 break
1814             except exception.MigrationNotFoundByStatus:
1815                 pass
1816 
1817         if not migration:
1818             LOG.info(_LI('Instance may have been confirmed during delete'),
1819                      instance=instance)
1820             return
1821 
1822         src_host = migration.source_compute
1823 
1824         self._record_action_start(context, instance,
1825                                   instance_actions.CONFIRM_RESIZE)
1826 
1827         # cells/rpcapi requires that we pass reservations
1828         self.compute_rpcapi.confirm_resize(context,
1829                 instance, migration,
1830                 src_host, [], cast=False)
1831 
1832     def _get_stashed_volume_connector(self, bdm, instance):
1833         """Lookup a connector dict from the bdm.connection_info if set
1834 
1835         Gets the stashed connector dict out of the bdm.connection_info if set
1836         and the connector host matches the instance host.
1837 
1838         :param bdm: nova.objects.block_device.BlockDeviceMapping
1839         :param instance: nova.objects.instance.Instance
1840         :returns: volume connector dict or None
1841         """
1842         if 'connection_info' in bdm and bdm.connection_info is not None:
1843             # NOTE(mriedem): We didn't start stashing the connector in the
1844             # bdm.connection_info until Mitaka so it might not be there on old
1845             # attachments. Also, if the volume was attached when the instance
1846             # was in shelved_offloaded state and it hasn't been unshelved yet
1847             # we don't have the attachment/connection information either.
1848             connector = jsonutils.loads(bdm.connection_info).get('connector')
1849             if connector:
1850                 if connector.get('host') == instance.host:
1851                     return connector
1852                 LOG.debug('Found stashed volume connector for instance but '
1853                           'connector host %(connector_host)s does not match '
1854                           'the instance host %(instance_host)s.',
1855                           {'connector_host': connector.get('host'),
1856                            'instance_host': instance.host}, instance=instance)
1857 
1858     def _local_cleanup_bdm_volumes(self, bdms, instance, context):
1859         """The method deletes the bdm records and, if a bdm is a volume, call
1860         the terminate connection and the detach volume via the Volume API.
1861         """
1862         elevated = context.elevated()
1863         for bdm in bdms:
1864             if bdm.is_volume:
1865                 try:
1866                     connector = self._get_stashed_volume_connector(
1867                         bdm, instance)
1868                     if connector:
1869                         self.volume_api.terminate_connection(context,
1870                                                              bdm.volume_id,
1871                                                              connector)
1872                     else:
1873                         LOG.debug('Unable to find connector for volume %s, '
1874                                   'not attempting terminate_connection.',
1875                                   bdm.volume_id, instance=instance)
1876                     # Attempt to detach the volume. If there was no connection
1877                     # made in the first place this is just cleaning up the
1878                     # volume state in the Cinder database.
1879                     self.volume_api.detach(elevated, bdm.volume_id,
1880                                            instance.uuid)
1881                     if bdm.delete_on_termination:
1882                         self.volume_api.delete(context, bdm.volume_id)
1883                 except Exception as exc:
1884                     err_str = _LW("Ignoring volume cleanup failure due to %s")
1885                     LOG.warning(err_str, exc, instance=instance)
1886             bdm.destroy()
1887 
1888     def _local_delete(self, context, instance, bdms, delete_type, cb):
1889         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
1890             LOG.info(_LI("instance is in SHELVED_OFFLOADED state, cleanup"
1891                          " the instance's info from database."),
1892                      instance=instance)
1893         else:
1894             LOG.warning(_LW("instance's host %s is down, deleting from "
1895                             "database"), instance.host, instance=instance)
1896         compute_utils.notify_about_instance_usage(
1897             self.notifier, context, instance, "%s.start" % delete_type)
1898 
1899         elevated = context.elevated()
1900         if self.cell_type != 'api':
1901             # NOTE(liusheng): In nova-network multi_host scenario,deleting
1902             # network info of the instance may need instance['host'] as
1903             # destination host of RPC call. If instance in SHELVED_OFFLOADED
1904             # state, instance['host'] is None, here, use shelved_host as host
1905             # to deallocate network info and reset instance['host'] after that.
1906             # Here we shouldn't use instance.save(), because this will mislead
1907             # user who may think the instance's host has been changed, and
1908             # actually, the instance.host is always None.
1909             orig_host = instance.host
1910             try:
1911                 if instance.vm_state == vm_states.SHELVED_OFFLOADED:
1912                     sysmeta = getattr(instance,
1913                                       obj_base.get_attrname('system_metadata'))
1914                     instance.host = sysmeta.get('shelved_host')
1915                 self.network_api.deallocate_for_instance(elevated,
1916                                                          instance)
1917             finally:
1918                 instance.host = orig_host
1919 
1920         # cleanup volumes
1921         self._local_cleanup_bdm_volumes(bdms, instance, context)
1922         cb(context, instance, bdms, local=True)
1923         sys_meta = instance.system_metadata
1924         instance.destroy()
1925         compute_utils.notify_about_instance_usage(
1926             self.notifier, context, instance, "%s.end" % delete_type,
1927             system_metadata=sys_meta)
1928 
1929     def _do_delete(self, context, instance, bdms, local=False):
1930         if local:
1931             instance.vm_state = vm_states.DELETED
1932             instance.task_state = None
1933             instance.terminated_at = timeutils.utcnow()
1934             instance.save()
1935         else:
1936             self.compute_rpcapi.terminate_instance(context, instance, bdms,
1937                                                    delete_type='delete')
1938 
1939     def _do_force_delete(self, context, instance, bdms, local=False):
1940         if local:
1941             instance.vm_state = vm_states.DELETED
1942             instance.task_state = None
1943             instance.terminated_at = timeutils.utcnow()
1944             instance.save()
1945         else:
1946             self.compute_rpcapi.terminate_instance(context, instance, bdms,
1947                                                    delete_type='force_delete')
1948 
1949     def _do_soft_delete(self, context, instance, bdms, local=False):
1950         if local:
1951             instance.vm_state = vm_states.SOFT_DELETED
1952             instance.task_state = None
1953             instance.terminated_at = timeutils.utcnow()
1954             instance.save()
1955         else:
1956             self.compute_rpcapi.soft_delete_instance(context, instance)
1957 
1958     # NOTE(maoy): we allow delete to be called no matter what vm_state says.
1959     @check_instance_lock
1960     @check_instance_cell
1961     @check_instance_state(vm_state=None, task_state=None,
1962                           must_have_launched=True)
1963     def soft_delete(self, context, instance):
1964         """Terminate an instance."""
1965         LOG.debug('Going to try to soft delete instance',
1966                   instance=instance)
1967 
1968         self._delete(context, instance, 'soft_delete', self._do_soft_delete,
1969                      task_state=task_states.SOFT_DELETING,
1970                      deleted_at=timeutils.utcnow())
1971 
1972     def _delete_instance(self, context, instance):
1973         self._delete(context, instance, 'delete', self._do_delete,
1974                      task_state=task_states.DELETING)
1975 
1976     @check_instance_lock
1977     @check_instance_cell
1978     @check_instance_state(vm_state=None, task_state=None,
1979                           must_have_launched=False)
1980     def delete(self, context, instance):
1981         """Terminate an instance."""
1982         LOG.debug("Going to try to terminate instance", instance=instance)
1983         self._delete_instance(context, instance)
1984 
1985     @check_instance_lock
1986     @check_instance_state(vm_state=[vm_states.SOFT_DELETED])
1987     def restore(self, context, instance):
1988         """Restore a previously deleted (but not reclaimed) instance."""
1989         # Check quotas
1990         flavor = instance.get_flavor()
1991         project_id, user_id = quotas_obj.ids_from_instance(context, instance)
1992         compute_utils.check_num_instances_quota(context, flavor, 1, 1,
1993                 project_id=project_id, user_id=user_id)
1994 
1995         self._record_action_start(context, instance, instance_actions.RESTORE)
1996 
1997         if instance.host:
1998             instance.task_state = task_states.RESTORING
1999             instance.deleted_at = None
2000             instance.save(expected_task_state=[None])
2001             self.compute_rpcapi.restore_instance(context, instance)
2002         else:
2003             instance.vm_state = vm_states.ACTIVE
2004             instance.task_state = None
2005             instance.deleted_at = None
2006             instance.save(expected_task_state=[None])
2007 
2008     @check_instance_lock
2009     @check_instance_state(must_have_launched=False)
2010     def force_delete(self, context, instance):
2011         """Force delete an instance in any vm_state/task_state."""
2012         self._delete(context, instance, 'force_delete', self._do_force_delete,
2013                      task_state=task_states.DELETING)
2014 
2015     def force_stop(self, context, instance, do_cast=True, clean_shutdown=True):
2016         LOG.debug("Going to try to stop instance", instance=instance)
2017 
2018         instance.task_state = task_states.POWERING_OFF
2019         instance.progress = 0
2020         instance.save(expected_task_state=[None])
2021 
2022         self._record_action_start(context, instance, instance_actions.STOP)
2023 
2024         self.compute_rpcapi.stop_instance(context, instance, do_cast=do_cast,
2025                                           clean_shutdown=clean_shutdown)
2026 
2027     @check_instance_lock
2028     @check_instance_host
2029     @check_instance_cell
2030     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.ERROR])
2031     def stop(self, context, instance, do_cast=True, clean_shutdown=True):
2032         """Stop an instance."""
2033         self.force_stop(context, instance, do_cast, clean_shutdown)
2034 
2035     @check_instance_lock
2036     @check_instance_host
2037     @check_instance_cell
2038     @check_instance_state(vm_state=[vm_states.STOPPED])
2039     def start(self, context, instance):
2040         """Start an instance."""
2041         LOG.debug("Going to try to start instance", instance=instance)
2042 
2043         instance.task_state = task_states.POWERING_ON
2044         instance.save(expected_task_state=[None])
2045 
2046         self._record_action_start(context, instance, instance_actions.START)
2047         # TODO(yamahata): injected_files isn't supported right now.
2048         #                 It is used only for osapi. not for ec2 api.
2049         #                 availability_zone isn't used by run_instance.
2050         self.compute_rpcapi.start_instance(context, instance)
2051 
2052     @check_instance_lock
2053     @check_instance_host
2054     @check_instance_cell
2055     @check_instance_state(vm_state=vm_states.ALLOW_TRIGGER_CRASH_DUMP)
2056     def trigger_crash_dump(self, context, instance):
2057         """Trigger crash dump in an instance."""
2058         LOG.debug("Try to trigger crash dump", instance=instance)
2059 
2060         self._record_action_start(context, instance,
2061                                   instance_actions.TRIGGER_CRASH_DUMP)
2062 
2063         self.compute_rpcapi.trigger_crash_dump(context, instance)
2064 
2065     def _get_instance_map_or_none(self, context, instance_uuid):
2066         try:
2067             inst_map = objects.InstanceMapping.get_by_instance_uuid(
2068                     context, instance_uuid)
2069         except exception.InstanceMappingNotFound:
2070             # InstanceMapping should always be found generally. This exception
2071             # may be raised if a deployment has partially migrated the nova-api
2072             # services.
2073             inst_map = None
2074         return inst_map
2075 
2076     def _get_instance(self, context, instance_uuid, expected_attrs):
2077         # Before service version 15 the BuildRequest is not cleaned up during
2078         # a delete request so there is no reason to look it up here as we can't
2079         # trust that it's not referencing a deleted instance. Also even if
2080         # there is an instance mapping we don't need to honor it for older
2081         # service versions.
2082         service_version = objects.Service.get_minimum_version(
2083             context, 'nova-osapi_compute')
2084         # If we're on cellsv1, we also need to consult the top-level
2085         # merged replica instead of the cell directly, so fall through
2086         # here in that case as well.
2087         if service_version < 15 or CONF.cells.enable:
2088             return objects.Instance.get_by_uuid(context, instance_uuid,
2089                                                 expected_attrs=expected_attrs)
2090         inst_map = self._get_instance_map_or_none(context, instance_uuid)
2091         if inst_map and (inst_map.cell_mapping is not None):
2092             nova_context.set_target_cell(context, inst_map.cell_mapping)
2093             instance = objects.Instance.get_by_uuid(
2094                 context, instance_uuid, expected_attrs=expected_attrs)
2095         elif inst_map and (inst_map.cell_mapping is None):
2096             # This means the instance has not been scheduled and put in
2097             # a cell yet. For now it also may mean that the deployer
2098             # has not created their cell(s) yet.
2099             try:
2100                 build_req = objects.BuildRequest.get_by_instance_uuid(
2101                         context, instance_uuid)
2102                 instance = build_req.instance
2103             except exception.BuildRequestNotFound:
2104                 # Instance was mapped and the BuildRequest was deleted
2105                 # while fetching. Try again.
2106                 inst_map = self._get_instance_map_or_none(context,
2107                                                           instance_uuid)
2108                 if inst_map and (inst_map.cell_mapping is not None):
2109                     nova_context.set_target_cell(context,
2110                                                  inst_map.cell_mapping)
2111                     instance = objects.Instance.get_by_uuid(
2112                         context, instance_uuid,
2113                         expected_attrs=expected_attrs)
2114                 else:
2115                     raise exception.InstanceNotFound(instance_id=instance_uuid)
2116         else:
2117             raise exception.InstanceNotFound(instance_id=instance_uuid)
2118 
2119         return instance
2120 
2121     def get(self, context, instance_id, expected_attrs=None):
2122         """Get a single instance with the given instance_id."""
2123         if not expected_attrs:
2124             expected_attrs = []
2125         expected_attrs.extend(['metadata', 'system_metadata',
2126                                'security_groups', 'info_cache'])
2127         # NOTE(ameade): we still need to support integer ids for ec2
2128         try:
2129             if uuidutils.is_uuid_like(instance_id):
2130                 LOG.debug("Fetching instance by UUID",
2131                            instance_uuid=instance_id)
2132 
2133                 instance = self._get_instance(context, instance_id,
2134                                               expected_attrs)
2135             else:
2136                 LOG.debug("Failed to fetch instance by id %s", instance_id)
2137                 raise exception.InstanceNotFound(instance_id=instance_id)
2138         except exception.InvalidID:
2139             LOG.debug("Invalid instance id %s", instance_id)
2140             raise exception.InstanceNotFound(instance_id=instance_id)
2141 
2142         return instance
2143 
2144     def get_all(self, context, search_opts=None, limit=None, marker=None,
2145                 expected_attrs=None, sort_keys=None, sort_dirs=None):
2146         """Get all instances filtered by one of the given parameters.
2147 
2148         If there is no filter and the context is an admin, it will retrieve
2149         all instances in the system.
2150 
2151         Deleted instances will be returned by default, unless there is a
2152         search option that says otherwise.
2153 
2154         The results will be sorted based on the list of sort keys in the
2155         'sort_keys' parameter (first value is primary sort key, second value is
2156         secondary sort ket, etc.). For each sort key, the associated sort
2157         direction is based on the list of sort directions in the 'sort_dirs'
2158         parameter.
2159         """
2160         if search_opts is None:
2161             search_opts = {}
2162 
2163         LOG.debug("Searching by: %s", str(search_opts))
2164 
2165         # Fixups for the DB call
2166         filters = {}
2167 
2168         def _remap_flavor_filter(flavor_id):
2169             flavor = objects.Flavor.get_by_flavor_id(context, flavor_id)
2170             filters['instance_type_id'] = flavor.id
2171 
2172         def _remap_fixed_ip_filter(fixed_ip):
2173             # Turn fixed_ip into a regexp match. Since '.' matches
2174             # any character, we need to use regexp escaping for it.
2175             filters['ip'] = '^%s$' % fixed_ip.replace('.', '\\.')
2176 
2177         def _remap_metadata_filter(metadata):
2178             filters['metadata'] = jsonutils.loads(metadata)
2179 
2180         def _remap_system_metadata_filter(metadata):
2181             filters['system_metadata'] = jsonutils.loads(metadata)
2182 
2183         # search_option to filter_name mapping.
2184         filter_mapping = {
2185                 'image': 'image_ref',
2186                 'name': 'display_name',
2187                 'tenant_id': 'project_id',
2188                 'flavor': _remap_flavor_filter,
2189                 'fixed_ip': _remap_fixed_ip_filter,
2190                 'metadata': _remap_metadata_filter,
2191                 'system_metadata': _remap_system_metadata_filter}
2192 
2193         # copy from search_opts, doing various remappings as necessary
2194         for opt, value in search_opts.items():
2195             # Do remappings.
2196             # Values not in the filter_mapping table are copied as-is.
2197             # If remapping is None, option is not copied
2198             # If the remapping is a string, it is the filter_name to use
2199             try:
2200                 remap_object = filter_mapping[opt]
2201             except KeyError:
2202                 filters[opt] = value
2203             else:
2204                 # Remaps are strings to translate to, or functions to call
2205                 # to do the translating as defined by the table above.
2206                 if isinstance(remap_object, six.string_types):
2207                     filters[remap_object] = value
2208                 else:
2209                     try:
2210                         remap_object(value)
2211 
2212                     # We already know we can't match the filter, so
2213                     # return an empty list
2214                     except ValueError:
2215                         return objects.InstanceList()
2216 
2217         # IP address filtering cannot be applied at the DB layer, remove any DB
2218         # limit so that it can be applied after the IP filter.
2219         filter_ip = 'ip6' in filters or 'ip' in filters
2220         orig_limit = limit
2221         if filter_ip and limit:
2222             LOG.debug('Removing limit for DB query due to IP filter')
2223             limit = None
2224 
2225         # The ordering of instances will be
2226         # [sorted instances with no host] + [sorted instances with host].
2227         # This means BuildRequest and cell0 instances first, then cell
2228         # instances
2229         build_requests = objects.BuildRequestList.get_by_filters(
2230             context, filters, limit=limit, marker=marker, sort_keys=sort_keys,
2231             sort_dirs=sort_dirs)
2232         build_req_instances = objects.InstanceList(
2233             objects=[build_req.instance for build_req in build_requests])
2234         # Only subtract from limit if it is not None
2235         limit = (limit - len(build_req_instances)) if limit else limit
2236 
2237         try:
2238             cell0_mapping = objects.CellMapping.get_by_uuid(context,
2239                 objects.CellMapping.CELL0_UUID)
2240         except exception.CellMappingNotFound:
2241             cell0_instances = objects.InstanceList(objects=[])
2242         else:
2243             with nova_context.target_cell(context, cell0_mapping):
2244                 try:
2245                     cell0_instances = self._get_instances_by_filters(
2246                         context, filters, limit=limit, marker=marker,
2247                         expected_attrs=expected_attrs, sort_keys=sort_keys,
2248                         sort_dirs=sort_dirs)
2249                 except exception.MarkerNotFound:
2250                     # We can ignore this since we need to look in the cell DB
2251                     cell0_instances = objects.InstanceList(objects=[])
2252         # Only subtract from limit if it is not None
2253         limit = (limit - len(cell0_instances)) if limit else limit
2254 
2255         # There is only planned support for a single cell here. Multiple cell
2256         # instance lists should be proxied to project Searchlight, or a similar
2257         # alternative.
2258         if limit is None or limit > 0:
2259             if not CONF.cells.enable:
2260                 cell_instances = self._get_instances_by_filters_all_cells(
2261                         context, filters,
2262                         limit=limit, marker=marker,
2263                         expected_attrs=expected_attrs, sort_keys=sort_keys,
2264                         sort_dirs=sort_dirs)
2265             else:
2266                 # NOTE(melwitt): If we're on cells v1, we need to read
2267                 # instances from the top-level database because reading from
2268                 # cells results in changed behavior, because of the syncing.
2269                 # We can remove this path once we stop supporting cells v1.
2270                 cell_instances = self._get_instances_by_filters(
2271                     context, filters, limit=limit, marker=marker,
2272                     expected_attrs=expected_attrs, sort_keys=sort_keys,
2273                     sort_dirs=sort_dirs)
2274         else:
2275             LOG.debug('Limit excludes any results from real cells')
2276             cell_instances = objects.InstanceList(objects=[])
2277 
2278         def _get_unique_filter_method():
2279             seen_uuids = set()
2280 
2281             def _filter(instance):
2282                 if instance.uuid in seen_uuids:
2283                     return False
2284                 seen_uuids.add(instance.uuid)
2285                 return True
2286 
2287             return _filter
2288 
2289         filter_method = _get_unique_filter_method()
2290         # Only subtract from limit if it is not None
2291         limit = (limit - len(cell_instances)) if limit else limit
2292         # TODO(alaski): Clean up the objects concatenation when List objects
2293         # support it natively.
2294         instances = objects.InstanceList(
2295             objects=list(filter(filter_method,
2296                            build_req_instances.objects +
2297                            cell0_instances.objects +
2298                            cell_instances.objects)))
2299 
2300         if filter_ip:
2301             instances = self._ip_filter(instances, filters, orig_limit)
2302 
2303         return instances
2304 
2305     @staticmethod
2306     def _ip_filter(inst_models, filters, limit):
2307         ipv4_f = re.compile(str(filters.get('ip')))
2308         ipv6_f = re.compile(str(filters.get('ip6')))
2309 
2310         def _match_instance(instance):
2311             nw_info = compute_utils.get_nw_info_for_instance(instance)
2312             for vif in nw_info:
2313                 for fixed_ip in vif.fixed_ips():
2314                     address = fixed_ip.get('address')
2315                     if not address:
2316                         continue
2317                     version = fixed_ip.get('version')
2318                     if ((version == 4 and ipv4_f.match(address)) or
2319                         (version == 6 and ipv6_f.match(address))):
2320                         return True
2321             return False
2322 
2323         result_objs = []
2324         for instance in inst_models:
2325             if _match_instance(instance):
2326                 result_objs.append(instance)
2327                 if limit and len(result_objs) == limit:
2328                     break
2329         return objects.InstanceList(objects=result_objs)
2330 
2331     def _get_instances_by_filters_all_cells(self, context, *args, **kwargs):
2332         """This is just a wrapper that iterates (non-zero) cells."""
2333 
2334         global CELLS
2335         if not CELLS:
2336             CELLS = objects.CellMappingList.get_all(context)
2337             LOG.debug('Found %(count)i cells: %(cells)s',
2338                       dict(count=len(CELLS), cells=CELLS))
2339 
2340         if not CELLS:
2341             LOG.error(_LE('No cells are configured, unable to list instances'))
2342 
2343         limit = kwargs.pop('limit', None)
2344 
2345         instances = []
2346         for cell in CELLS:
2347             if cell.uuid == objects.CellMapping.CELL0_UUID:
2348                 LOG.debug('Skipping already-collected cell0 list')
2349                 continue
2350             LOG.debug('Listing %s instances in cell %s',
2351                       limit or 'all', cell.name)
2352             with nova_context.target_cell(context, cell) as ccontext:
2353                 try:
2354                     cell_insts = self._get_instances_by_filters(ccontext,
2355                                                                 *args,
2356                                                                 limit=limit,
2357                                                                 **kwargs)
2358                 except exception.MarkerNotFound:
2359                     # NOTE(danms): We need to keep looking through the
2360                     # later cells to find the marker
2361                     continue
2362                 instances.extend(cell_insts)
2363                 # NOTE(danms): We must have found a marker if we had one,
2364                 # so make sure we don't require a marker in the next cell
2365                 kwargs['marker'] = None
2366                 if limit:
2367                     limit -= len(cell_insts)
2368                     if limit <= 0:
2369                         break
2370 
2371         marker = kwargs.get('marker')
2372         if marker is not None and len(instances) == 0:
2373             # NOTE(danms): If we did not find the marker in any cell,
2374             # mimic the db_api behavior here.
2375             raise exception.MarkerNotFound(marker=marker)
2376 
2377         return objects.InstanceList(objects=instances)
2378 
2379     def _get_instances_by_filters(self, context, filters,
2380                                   limit=None, marker=None, expected_attrs=None,
2381                                   sort_keys=None, sort_dirs=None):
2382         fields = ['metadata', 'system_metadata', 'info_cache',
2383                   'security_groups']
2384         if expected_attrs:
2385             fields.extend(expected_attrs)
2386         return objects.InstanceList.get_by_filters(
2387             context, filters=filters, limit=limit, marker=marker,
2388             expected_attrs=fields, sort_keys=sort_keys, sort_dirs=sort_dirs)
2389 
2390     def update_instance(self, context, instance, updates):
2391         """Updates a single Instance object with some updates dict.
2392 
2393         Returns the updated instance.
2394         """
2395 
2396         # NOTE(sbauza): Given we only persist the Instance object after we
2397         # create the BuildRequest, we are sure that if the Instance object
2398         # has an ID field set, then it was persisted in the right Cell DB.
2399         if instance.obj_attr_is_set('id'):
2400             instance.update(updates)
2401             # Instance has been scheduled and the BuildRequest has been deleted
2402             # we can directly write the update down to the right cell.
2403             inst_map = self._get_instance_map_or_none(context, instance.uuid)
2404             # If we have a cell_mapping and we're not on cells v1, then
2405             # look up the instance in the cell database
2406             if inst_map and (inst_map.cell_mapping is not None) and (
2407                     not CONF.cells.enable):
2408                 with nova_context.target_cell(context, inst_map.cell_mapping):
2409                     instance.save()
2410             else:
2411                 # If inst_map.cell_mapping does not point at a cell then cell
2412                 # migration has not happened yet.
2413                 # TODO(alaski): Make this a failure case after we put in
2414                 # a block that requires migrating to cellsv2.
2415                 instance.save()
2416         else:
2417             # Instance is not yet mapped to a cell, so we need to update
2418             # BuildRequest instead
2419             # TODO(sbauza): Fix the possible race conditions where BuildRequest
2420             # could be deleted because of either a concurrent instance delete
2421             # or because the scheduler just returned a destination right
2422             # after we called the instance in the API.
2423             try:
2424                 build_req = objects.BuildRequest.get_by_instance_uuid(
2425                     context, instance.uuid)
2426                 instance = build_req.instance
2427                 instance.update(updates)
2428                 # FIXME(sbauza): Here we are updating the current
2429                 # thread-related BuildRequest object. Given that another worker
2430                 # could have looking up at that BuildRequest in the API, it
2431                 # means that it could pass it down to the conductor without
2432                 # making sure that it's not updated, we could have some race
2433                 # condition where it would missing the updated fields, but
2434                 # that's something we could discuss once the instance record
2435                 # is persisted by the conductor.
2436                 build_req.save()
2437             except exception.BuildRequestNotFound:
2438                 # Instance was mapped and the BuildRequest was deleted
2439                 # while fetching (and possibly the instance could have been
2440                 # deleted as well). We need to lookup again the Instance object
2441                 # in order to correctly update it.
2442                 # TODO(sbauza): Figure out a good way to know the expected
2443                 # attributes by checking which fields are set or not.
2444                 expected_attrs = ['flavor', 'pci_devices', 'numa_topology',
2445                                   'tags', 'metadata', 'system_metadata',
2446                                   'security_groups', 'info_cache']
2447                 inst_map = self._get_instance_map_or_none(context,
2448                                                           instance.uuid)
2449                 if inst_map and (inst_map.cell_mapping is not None):
2450                     with nova_context.target_cell(context,
2451                                                   inst_map.cell_mapping):
2452                         instance = objects.Instance.get_by_uuid(
2453                             context, instance.uuid,
2454                             expected_attrs=expected_attrs)
2455                         instance.update(updates)
2456                         instance.save()
2457                 else:
2458                     # If inst_map.cell_mapping does not point at a cell then
2459                     # cell migration has not happened yet.
2460                     # TODO(alaski): Make this a failure case after we put in
2461                     # a block that requires migrating to cellsv2.
2462                     instance = objects.Instance.get_by_uuid(
2463                         context, instance.uuid, expected_attrs=expected_attrs)
2464                     instance.update(updates)
2465                     instance.save()
2466         return instance
2467 
2468     # NOTE(melwitt): We don't check instance lock for backup because lock is
2469     #                intended to prevent accidental change/delete of instances
2470     @check_instance_cell
2471     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2472                                     vm_states.PAUSED, vm_states.SUSPENDED])
2473     def backup(self, context, instance, name, backup_type, rotation,
2474                extra_properties=None):
2475         """Backup the given instance
2476 
2477         :param instance: nova.objects.instance.Instance object
2478         :param name: name of the backup
2479         :param backup_type: 'daily' or 'weekly'
2480         :param rotation: int representing how many backups to keep around;
2481             None if rotation shouldn't be used (as in the case of snapshots)
2482         :param extra_properties: dict of extra image properties to include
2483                                  when creating the image.
2484         :returns: A dict containing image metadata
2485         """
2486         props_copy = dict(extra_properties, backup_type=backup_type)
2487 
2488         if compute_utils.is_volume_backed_instance(context, instance):
2489             LOG.info(_LI("It's not supported to backup volume backed "
2490                          "instance."), instance=instance)
2491             raise exception.InvalidRequest()
2492         else:
2493             image_meta = self._create_image(context, instance,
2494                                             name, 'backup',
2495                                             extra_properties=props_copy)
2496 
2497         # NOTE(comstud): Any changes to this method should also be made
2498         # to the backup_instance() method in nova/cells/messaging.py
2499 
2500         instance.task_state = task_states.IMAGE_BACKUP
2501         instance.save(expected_task_state=[None])
2502 
2503         self.compute_rpcapi.backup_instance(context, instance,
2504                                             image_meta['id'],
2505                                             backup_type,
2506                                             rotation)
2507         return image_meta
2508 
2509     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2510     #                intended to prevent accidental change/delete of instances
2511     @check_instance_cell
2512     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2513                                     vm_states.PAUSED, vm_states.SUSPENDED])
2514     def snapshot(self, context, instance, name, extra_properties=None):
2515         """Snapshot the given instance.
2516 
2517         :param instance: nova.objects.instance.Instance object
2518         :param name: name of the snapshot
2519         :param extra_properties: dict of extra image properties to include
2520                                  when creating the image.
2521         :returns: A dict containing image metadata
2522         """
2523         image_meta = self._create_image(context, instance, name,
2524                                         'snapshot',
2525                                         extra_properties=extra_properties)
2526 
2527         # NOTE(comstud): Any changes to this method should also be made
2528         # to the snapshot_instance() method in nova/cells/messaging.py
2529         instance.task_state = task_states.IMAGE_SNAPSHOT_PENDING
2530         try:
2531             instance.save(expected_task_state=[None])
2532         except (exception.InstanceNotFound,
2533                 exception.UnexpectedDeletingTaskStateError) as ex:
2534             # Changing the instance task state to use in raising the
2535             # InstanceInvalidException below
2536             LOG.debug('Instance disappeared during snapshot.',
2537                       instance=instance)
2538             try:
2539                 image_id = image_meta['id']
2540                 self.image_api.delete(context, image_id)
2541                 LOG.info(_LI('Image %s deleted because instance '
2542                              'deleted before snapshot started.'),
2543                          image_id, instance=instance)
2544             except exception.ImageNotFound:
2545                 pass
2546             except Exception as exc:
2547                 msg = _LW("Error while trying to clean up image %(img_id)s: "
2548                           "%(error_msg)s")
2549                 LOG.warning(msg, {"img_id": image_meta['id'],
2550                                   "error_msg": six.text_type(exc)})
2551             attr = 'task_state'
2552             state = task_states.DELETING
2553             if type(ex) == exception.InstanceNotFound:
2554                 attr = 'vm_state'
2555                 state = vm_states.DELETED
2556             raise exception.InstanceInvalidState(attr=attr,
2557                                            instance_uuid=instance.uuid,
2558                                            state=state,
2559                                            method='snapshot')
2560 
2561         self.compute_rpcapi.snapshot_instance(context, instance,
2562                                               image_meta['id'])
2563 
2564         return image_meta
2565 
2566     def _create_image(self, context, instance, name, image_type,
2567                       extra_properties=None):
2568         """Create new image entry in the image service.  This new image
2569         will be reserved for the compute manager to upload a snapshot
2570         or backup.
2571 
2572         :param context: security context
2573         :param instance: nova.objects.instance.Instance object
2574         :param name: string for name of the snapshot
2575         :param image_type: snapshot | backup
2576         :param extra_properties: dict of extra image properties to include
2577 
2578         """
2579         properties = {
2580             'instance_uuid': instance.uuid,
2581             'user_id': str(context.user_id),
2582             'image_type': image_type,
2583         }
2584         properties.update(extra_properties or {})
2585 
2586         image_meta = self._initialize_instance_snapshot_metadata(
2587             instance, name, properties)
2588         # if we're making a snapshot, omit the disk and container formats,
2589         # since the image may have been converted to another format, and the
2590         # original values won't be accurate.  The driver will populate these
2591         # with the correct values later, on image upload.
2592         if image_type == 'snapshot':
2593             image_meta.pop('disk_format', None)
2594             image_meta.pop('container_format', None)
2595         return self.image_api.create(context, image_meta)
2596 
2597     def _initialize_instance_snapshot_metadata(self, instance, name,
2598                                                extra_properties=None):
2599         """Initialize new metadata for a snapshot of the given instance.
2600 
2601         :param instance: nova.objects.instance.Instance object
2602         :param name: string for name of the snapshot
2603         :param extra_properties: dict of extra metadata properties to include
2604 
2605         :returns: the new instance snapshot metadata
2606         """
2607         image_meta = utils.get_image_from_system_metadata(
2608             instance.system_metadata)
2609         image_meta.update({'name': name,
2610                            'is_public': False})
2611 
2612         # Delete properties that are non-inheritable
2613         properties = image_meta['properties']
2614         for key in CONF.non_inheritable_image_properties:
2615             properties.pop(key, None)
2616 
2617         # The properties in extra_properties have precedence
2618         properties.update(extra_properties or {})
2619 
2620         return image_meta
2621 
2622     # NOTE(melwitt): We don't check instance lock for snapshot because lock is
2623     #                intended to prevent accidental change/delete of instances
2624     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2625                                     vm_states.SUSPENDED])
2626     def snapshot_volume_backed(self, context, instance, name,
2627                                extra_properties=None):
2628         """Snapshot the given volume-backed instance.
2629 
2630         :param instance: nova.objects.instance.Instance object
2631         :param name: name of the backup or snapshot
2632         :param extra_properties: dict of extra image properties to include
2633 
2634         :returns: the new image metadata
2635         """
2636         image_meta = self._initialize_instance_snapshot_metadata(
2637             instance, name, extra_properties)
2638         # the new image is simply a bucket of properties (particularly the
2639         # block device mapping, kernel and ramdisk IDs) with no image data,
2640         # hence the zero size
2641         image_meta['size'] = 0
2642         for attr in ('container_format', 'disk_format'):
2643             image_meta.pop(attr, None)
2644         properties = image_meta['properties']
2645         # clean properties before filling
2646         for key in ('block_device_mapping', 'bdm_v2', 'root_device_name'):
2647             properties.pop(key, None)
2648         if instance.root_device_name:
2649             properties['root_device_name'] = instance.root_device_name
2650 
2651         quiesced = False
2652         if instance.vm_state == vm_states.ACTIVE:
2653             try:
2654                 self.compute_rpcapi.quiesce_instance(context, instance)
2655                 quiesced = True
2656             except (exception.InstanceQuiesceNotSupported,
2657                     exception.QemuGuestAgentNotEnabled,
2658                     exception.NovaException, NotImplementedError) as err:
2659                 if strutils.bool_from_string(instance.system_metadata.get(
2660                         'image_os_require_quiesce')):
2661                     raise
2662                 else:
2663                     LOG.info(_LI('Skipping quiescing instance: '
2664                                  '%(reason)s.'), {'reason': err},
2665                              instance=instance)
2666 
2667         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2668                 context, instance.uuid)
2669 
2670         mapping = []
2671         for bdm in bdms:
2672             if bdm.no_device:
2673                 continue
2674 
2675             if bdm.is_volume:
2676                 # create snapshot based on volume_id
2677                 volume = self.volume_api.get(context, bdm.volume_id)
2678                 # NOTE(yamahata): Should we wait for snapshot creation?
2679                 #                 Linux LVM snapshot creation completes in
2680                 #                 short time, it doesn't matter for now.
2681                 name = _('snapshot for %s') % image_meta['name']
2682                 LOG.debug('Creating snapshot from volume %s.', volume['id'],
2683                           instance=instance)
2684                 snapshot = self.volume_api.create_snapshot_force(
2685                     context, volume['id'], name, volume['display_description'])
2686                 mapping_dict = block_device.snapshot_from_bdm(snapshot['id'],
2687                                                               bdm)
2688                 mapping_dict = mapping_dict.get_image_mapping()
2689             else:
2690                 mapping_dict = bdm.get_image_mapping()
2691 
2692             mapping.append(mapping_dict)
2693 
2694         if quiesced:
2695             self.compute_rpcapi.unquiesce_instance(context, instance, mapping)
2696 
2697         if mapping:
2698             properties['block_device_mapping'] = mapping
2699             properties['bdm_v2'] = True
2700 
2701         return self.image_api.create(context, image_meta)
2702 
2703     @check_instance_lock
2704     def reboot(self, context, instance, reboot_type):
2705         """Reboot the given instance."""
2706         if reboot_type == 'SOFT':
2707             self._soft_reboot(context, instance)
2708         else:
2709             self._hard_reboot(context, instance)
2710 
2711     @check_instance_state(vm_state=set(vm_states.ALLOW_SOFT_REBOOT),
2712                           task_state=[None])
2713     def _soft_reboot(self, context, instance):
2714         expected_task_state = [None]
2715         instance.task_state = task_states.REBOOTING
2716         instance.save(expected_task_state=expected_task_state)
2717 
2718         self._record_action_start(context, instance, instance_actions.REBOOT)
2719 
2720         self.compute_rpcapi.reboot_instance(context, instance=instance,
2721                                             block_device_info=None,
2722                                             reboot_type='SOFT')
2723 
2724     @check_instance_state(vm_state=set(vm_states.ALLOW_HARD_REBOOT),
2725                           task_state=task_states.ALLOW_REBOOT)
2726     def _hard_reboot(self, context, instance):
2727         instance.task_state = task_states.REBOOTING_HARD
2728         expected_task_state = [None,
2729                                task_states.REBOOTING,
2730                                task_states.REBOOT_PENDING,
2731                                task_states.REBOOT_STARTED,
2732                                task_states.REBOOTING_HARD,
2733                                task_states.RESUMING,
2734                                task_states.UNPAUSING,
2735                                task_states.SUSPENDING]
2736         instance.save(expected_task_state = expected_task_state)
2737 
2738         self._record_action_start(context, instance, instance_actions.REBOOT)
2739 
2740         self.compute_rpcapi.reboot_instance(context, instance=instance,
2741                                             block_device_info=None,
2742                                             reboot_type='HARD')
2743 
2744     @check_instance_lock
2745     @check_instance_cell
2746     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
2747                                     vm_states.ERROR])
2748     def rebuild(self, context, instance, image_href, admin_password,
2749                 files_to_inject=None, **kwargs):
2750         """Rebuild the given instance with the provided attributes."""
2751         orig_image_ref = instance.image_ref or ''
2752         files_to_inject = files_to_inject or []
2753         metadata = kwargs.get('metadata', {})
2754         preserve_ephemeral = kwargs.get('preserve_ephemeral', False)
2755         auto_disk_config = kwargs.get('auto_disk_config')
2756 
2757         image_id, image = self._get_image(context, image_href)
2758         self._check_auto_disk_config(image=image, **kwargs)
2759 
2760         flavor = instance.get_flavor()
2761         root_bdm = compute_utils.get_root_bdm(context, instance)
2762         self._checks_for_create_and_rebuild(context, image_id, image,
2763                 flavor, metadata, files_to_inject, root_bdm)
2764 
2765         kernel_id, ramdisk_id = self._handle_kernel_and_ramdisk(
2766                 context, None, None, image)
2767 
2768         def _reset_image_metadata():
2769             """Remove old image properties that we're storing as instance
2770             system metadata.  These properties start with 'image_'.
2771             Then add the properties for the new image.
2772             """
2773             # FIXME(comstud): There's a race condition here in that if
2774             # the system_metadata for this instance is updated after
2775             # we do the previous save() and before we update.. those
2776             # other updates will be lost. Since this problem exists in
2777             # a lot of other places, I think it should be addressed in
2778             # a DB layer overhaul.
2779 
2780             orig_sys_metadata = dict(instance.system_metadata)
2781             # Remove the old keys
2782             for key in list(instance.system_metadata.keys()):
2783                 if key.startswith(utils.SM_IMAGE_PROP_PREFIX):
2784                     del instance.system_metadata[key]
2785 
2786             # Add the new ones
2787             new_sys_metadata = utils.get_system_metadata_from_image(
2788                 image, flavor)
2789 
2790             instance.system_metadata.update(new_sys_metadata)
2791             instance.save()
2792             return orig_sys_metadata
2793 
2794         # Since image might have changed, we may have new values for
2795         # os_type, vm_mode, etc
2796         options_from_image = self._inherit_properties_from_image(
2797                 image, auto_disk_config)
2798         instance.update(options_from_image)
2799 
2800         instance.task_state = task_states.REBUILDING
2801         instance.image_ref = image_href
2802         instance.kernel_id = kernel_id or ""
2803         instance.ramdisk_id = ramdisk_id or ""
2804         instance.progress = 0
2805         instance.update(kwargs)
2806         instance.save(expected_task_state=[None])
2807 
2808         # On a rebuild, since we're potentially changing images, we need to
2809         # wipe out the old image properties that we're storing as instance
2810         # system metadata... and copy in the properties for the new image.
2811         orig_sys_metadata = _reset_image_metadata()
2812 
2813         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2814                 context, instance.uuid)
2815 
2816         self._record_action_start(context, instance, instance_actions.REBUILD)
2817 
2818         # NOTE(sbauza): The migration script we provided in Newton should make
2819         # sure that all our instances are currently migrated to have an
2820         # attached RequestSpec object but let's consider that the operator only
2821         # half migrated all their instances in the meantime.
2822         try:
2823             request_spec = objects.RequestSpec.get_by_instance_uuid(
2824                 context, instance.uuid)
2825         except exception.RequestSpecNotFound:
2826             # Some old instances can still have no RequestSpec object attached
2827             # to them, we need to support the old way
2828             request_spec = None
2829 
2830         self.compute_task_api.rebuild_instance(context, instance=instance,
2831                 new_pass=admin_password, injected_files=files_to_inject,
2832                 image_ref=image_href, orig_image_ref=orig_image_ref,
2833                 orig_sys_metadata=orig_sys_metadata, bdms=bdms,
2834                 preserve_ephemeral=preserve_ephemeral, host=instance.host,
2835                 request_spec=request_spec,
2836                 kwargs=kwargs)
2837 
2838     @check_instance_lock
2839     @check_instance_cell
2840     @check_instance_state(vm_state=[vm_states.RESIZED])
2841     def revert_resize(self, context, instance):
2842         """Reverts a resize, deleting the 'new' instance in the process."""
2843         elevated = context.elevated()
2844         migration = objects.Migration.get_by_instance_and_status(
2845             elevated, instance.uuid, 'finished')
2846 
2847         instance.task_state = task_states.RESIZE_REVERTING
2848         instance.save(expected_task_state=[None])
2849 
2850         migration.status = 'reverting'
2851         migration.save()
2852 
2853         self._record_action_start(context, instance,
2854                                   instance_actions.REVERT_RESIZE)
2855 
2856         # cells/rpcapi requires that we pass reservations
2857         self.compute_rpcapi.revert_resize(context, instance,
2858                                           migration,
2859                                           migration.dest_compute,
2860                                           [])
2861 
2862     @check_instance_lock
2863     @check_instance_cell
2864     @check_instance_state(vm_state=[vm_states.RESIZED])
2865     def confirm_resize(self, context, instance, migration=None):
2866         """Confirms a migration/resize and deletes the 'old' instance."""
2867         elevated = context.elevated()
2868         if migration is None:
2869             migration = objects.Migration.get_by_instance_and_status(
2870                 elevated, instance.uuid, 'finished')
2871 
2872         migration.status = 'confirming'
2873         migration.save()
2874 
2875         self._record_action_start(context, instance,
2876                                   instance_actions.CONFIRM_RESIZE)
2877 
2878         # cells/rpcapi requires that we pass reservations
2879         self.compute_rpcapi.confirm_resize(context,
2880                                            instance,
2881                                            migration,
2882                                            migration.source_compute,
2883                                            [])
2884 
2885     @staticmethod
2886     def _resize_cells_support(context, instance,
2887                               current_instance_type, new_instance_type):
2888         """Special API cell logic for resize."""
2889         # NOTE(johannes/comstud): The API cell needs a local migration
2890         # record for later resize_confirm and resize_reverts.
2891         # We don't need source and/or destination
2892         # information, just the old and new flavors. Status is set to
2893         # 'finished' since nothing else will update the status along
2894         # the way.
2895         mig = objects.Migration(context=context.elevated())
2896         mig.instance_uuid = instance.uuid
2897         mig.old_instance_type_id = current_instance_type['id']
2898         mig.new_instance_type_id = new_instance_type['id']
2899         mig.status = 'finished'
2900         mig.migration_type = (
2901             mig.old_instance_type_id != mig.new_instance_type_id and
2902             'resize' or 'migration')
2903         mig.create()
2904 
2905     @check_instance_lock
2906     @check_instance_cell
2907     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED])
2908     def resize(self, context, instance, flavor_id=None, clean_shutdown=True,
2909                **extra_instance_updates):
2910         """Resize (ie, migrate) a running instance.
2911 
2912         If flavor_id is None, the process is considered a migration, keeping
2913         the original flavor_id. If flavor_id is not None, the instance should
2914         be migrated to a new host and resized to the new flavor_id.
2915         """
2916         self._check_auto_disk_config(instance, **extra_instance_updates)
2917 
2918         current_instance_type = instance.get_flavor()
2919 
2920         # If flavor_id is not provided, only migrate the instance.
2921         if not flavor_id:
2922             LOG.debug("flavor_id is None. Assuming migration.",
2923                       instance=instance)
2924             new_instance_type = current_instance_type
2925         else:
2926             new_instance_type = flavors.get_flavor_by_flavor_id(
2927                     flavor_id, read_deleted="no")
2928             if (new_instance_type.get('root_gb') == 0 and
2929                 current_instance_type.get('root_gb') != 0 and
2930                 not compute_utils.is_volume_backed_instance(context,
2931                     instance)):
2932                 reason = _('Resize to zero disk flavor is not allowed.')
2933                 raise exception.CannotResizeDisk(reason=reason)
2934 
2935         if not new_instance_type:
2936             raise exception.FlavorNotFound(flavor_id=flavor_id)
2937 
2938         current_instance_type_name = current_instance_type['name']
2939         new_instance_type_name = new_instance_type['name']
2940         LOG.debug("Old instance type %(current_instance_type_name)s, "
2941                   "new instance type %(new_instance_type_name)s",
2942                   {'current_instance_type_name': current_instance_type_name,
2943                    'new_instance_type_name': new_instance_type_name},
2944                   instance=instance)
2945 
2946         same_instance_type = (current_instance_type['id'] ==
2947                               new_instance_type['id'])
2948 
2949         # NOTE(sirp): We don't want to force a customer to change their flavor
2950         # when Ops is migrating off of a failed host.
2951         if not same_instance_type and new_instance_type.get('disabled'):
2952             raise exception.FlavorNotFound(flavor_id=flavor_id)
2953 
2954         if same_instance_type and flavor_id and self.cell_type != 'compute':
2955             raise exception.CannotResizeToSameFlavor()
2956 
2957         # ensure there is sufficient headroom for upsizes
2958         if flavor_id:
2959             project_id, user_id = quotas_obj.ids_from_instance(context,
2960                                                                instance)
2961             deltas = compute_utils.upsize_quota_delta(context,
2962                                                       new_instance_type,
2963                                                       current_instance_type)
2964             if deltas:
2965                 try:
2966                     res_deltas = {'cores': deltas.get('cores', 0),
2967                                   'ram': deltas.get('ram', 0)}
2968                     objects.Quotas.check_deltas(context, 'instances',
2969                                                 res_deltas,
2970                                                 project_id, user_id=user_id,
2971                                                 check_project_id=project_id,
2972                                                 check_user_id=user_id)
2973                 except exception.OverQuota as exc:
2974                     quotas = exc.kwargs['quotas']
2975                     overs = exc.kwargs['overs']
2976                     usages = exc.kwargs['usages']
2977                     headroom = compute_utils.get_headroom(quotas, usages,
2978                                                           deltas)
2979                     (overs, reqs, total_alloweds,
2980                      useds) = compute_utils.get_over_quota_detail(headroom,
2981                                                                   overs,
2982                                                                   quotas,
2983                                                                   deltas)
2984                     LOG.warning(_LW("%(overs)s quota exceeded for %(pid)s,"
2985                                     " tried to resize instance."),
2986                                 {'overs': overs, 'pid': context.project_id})
2987                     raise exception.TooManyInstances(overs=overs,
2988                                                      req=reqs,
2989                                                      used=useds,
2990                                                      allowed=total_alloweds)
2991 
2992         instance.task_state = task_states.RESIZE_PREP
2993         instance.progress = 0
2994         instance.update(extra_instance_updates)
2995         instance.save(expected_task_state=[None])
2996 
2997         filter_properties = {'ignore_hosts': []}
2998 
2999         if not CONF.allow_resize_to_same_host:
3000             filter_properties['ignore_hosts'].append(instance.host)
3001 
3002         if self.cell_type == 'api':
3003             # Create migration record.
3004             self._resize_cells_support(context, instance,
3005                                        current_instance_type,
3006                                        new_instance_type)
3007 
3008         if not flavor_id:
3009             self._record_action_start(context, instance,
3010                                       instance_actions.MIGRATE)
3011         else:
3012             self._record_action_start(context, instance,
3013                                       instance_actions.RESIZE)
3014 
3015         # NOTE(sbauza): The migration script we provided in Newton should make
3016         # sure that all our instances are currently migrated to have an
3017         # attached RequestSpec object but let's consider that the operator only
3018         # half migrated all their instances in the meantime.
3019         try:
3020             request_spec = objects.RequestSpec.get_by_instance_uuid(
3021                 context, instance.uuid)
3022             request_spec.ignore_hosts = filter_properties['ignore_hosts']
3023         except exception.RequestSpecNotFound:
3024             # Some old instances can still have no RequestSpec object attached
3025             # to them, we need to support the old way
3026             request_spec = None
3027 
3028         scheduler_hint = {'filter_properties': filter_properties}
3029         self.compute_task_api.resize_instance(context, instance,
3030                 extra_instance_updates, scheduler_hint=scheduler_hint,
3031                 flavor=new_instance_type,
3032                 reservations=[],
3033                 clean_shutdown=clean_shutdown,
3034                 request_spec=request_spec)
3035 
3036     @check_instance_lock
3037     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3038                                     vm_states.PAUSED, vm_states.SUSPENDED])
3039     def shelve(self, context, instance, clean_shutdown=True):
3040         """Shelve an instance.
3041 
3042         Shuts down an instance and frees it up to be removed from the
3043         hypervisor.
3044         """
3045         instance.task_state = task_states.SHELVING
3046         instance.save(expected_task_state=[None])
3047 
3048         self._record_action_start(context, instance, instance_actions.SHELVE)
3049 
3050         if not compute_utils.is_volume_backed_instance(context, instance):
3051             name = '%s-shelved' % instance.display_name
3052             image_meta = self._create_image(context, instance, name,
3053                     'snapshot')
3054             image_id = image_meta['id']
3055             self.compute_rpcapi.shelve_instance(context, instance=instance,
3056                     image_id=image_id, clean_shutdown=clean_shutdown)
3057         else:
3058             self.compute_rpcapi.shelve_offload_instance(context,
3059                     instance=instance, clean_shutdown=clean_shutdown)
3060 
3061     @check_instance_lock
3062     @check_instance_state(vm_state=[vm_states.SHELVED])
3063     def shelve_offload(self, context, instance, clean_shutdown=True):
3064         """Remove a shelved instance from the hypervisor."""
3065         instance.task_state = task_states.SHELVING_OFFLOADING
3066         instance.save(expected_task_state=[None])
3067 
3068         self.compute_rpcapi.shelve_offload_instance(context, instance=instance,
3069             clean_shutdown=clean_shutdown)
3070 
3071     @check_instance_lock
3072     @check_instance_state(vm_state=[vm_states.SHELVED,
3073         vm_states.SHELVED_OFFLOADED])
3074     def unshelve(self, context, instance):
3075         """Restore a shelved instance."""
3076         instance.task_state = task_states.UNSHELVING
3077         instance.save(expected_task_state=[None])
3078 
3079         self._record_action_start(context, instance, instance_actions.UNSHELVE)
3080 
3081         try:
3082             request_spec = objects.RequestSpec.get_by_instance_uuid(
3083                 context, instance.uuid)
3084         except exception.RequestSpecNotFound:
3085             # Some old instances can still have no RequestSpec object attached
3086             # to them, we need to support the old way
3087             request_spec = None
3088         self.compute_task_api.unshelve_instance(context, instance,
3089                                                 request_spec)
3090 
3091     @check_instance_lock
3092     def add_fixed_ip(self, context, instance, network_id):
3093         """Add fixed_ip from specified network to given instance."""
3094         self.compute_rpcapi.add_fixed_ip_to_instance(context,
3095                 instance=instance, network_id=network_id)
3096 
3097     @check_instance_lock
3098     def remove_fixed_ip(self, context, instance, address):
3099         """Remove fixed_ip from specified network to given instance."""
3100         self.compute_rpcapi.remove_fixed_ip_from_instance(context,
3101                 instance=instance, address=address)
3102 
3103     @check_instance_lock
3104     @check_instance_cell
3105     @check_instance_state(vm_state=[vm_states.ACTIVE])
3106     def pause(self, context, instance):
3107         """Pause the given instance."""
3108         instance.task_state = task_states.PAUSING
3109         instance.save(expected_task_state=[None])
3110         self._record_action_start(context, instance, instance_actions.PAUSE)
3111         self.compute_rpcapi.pause_instance(context, instance)
3112 
3113     @check_instance_lock
3114     @check_instance_cell
3115     @check_instance_state(vm_state=[vm_states.PAUSED])
3116     def unpause(self, context, instance):
3117         """Unpause the given instance."""
3118         instance.task_state = task_states.UNPAUSING
3119         instance.save(expected_task_state=[None])
3120         self._record_action_start(context, instance, instance_actions.UNPAUSE)
3121         self.compute_rpcapi.unpause_instance(context, instance)
3122 
3123     @check_instance_host
3124     def get_diagnostics(self, context, instance):
3125         """Retrieve diagnostics for the given instance."""
3126         return self.compute_rpcapi.get_diagnostics(context, instance=instance)
3127 
3128     @check_instance_host
3129     def get_instance_diagnostics(self, context, instance):
3130         """Retrieve diagnostics for the given instance."""
3131         return self.compute_rpcapi.get_instance_diagnostics(context,
3132                                                             instance=instance)
3133 
3134     @check_instance_lock
3135     @check_instance_cell
3136     @check_instance_state(vm_state=[vm_states.ACTIVE])
3137     def suspend(self, context, instance):
3138         """Suspend the given instance."""
3139         instance.task_state = task_states.SUSPENDING
3140         instance.save(expected_task_state=[None])
3141         self._record_action_start(context, instance, instance_actions.SUSPEND)
3142         self.compute_rpcapi.suspend_instance(context, instance)
3143 
3144     @check_instance_lock
3145     @check_instance_cell
3146     @check_instance_state(vm_state=[vm_states.SUSPENDED])
3147     def resume(self, context, instance):
3148         """Resume the given instance."""
3149         instance.task_state = task_states.RESUMING
3150         instance.save(expected_task_state=[None])
3151         self._record_action_start(context, instance, instance_actions.RESUME)
3152         self.compute_rpcapi.resume_instance(context, instance)
3153 
3154     @check_instance_lock
3155     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3156                                     vm_states.ERROR])
3157     def rescue(self, context, instance, rescue_password=None,
3158                rescue_image_ref=None, clean_shutdown=True):
3159         """Rescue the given instance."""
3160 
3161         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3162                     context, instance.uuid)
3163         for bdm in bdms:
3164             if bdm.volume_id:
3165                 vol = self.volume_api.get(context, bdm.volume_id)
3166                 self.volume_api.check_attached(context, vol)
3167         if compute_utils.is_volume_backed_instance(context, instance, bdms):
3168             reason = _("Cannot rescue a volume-backed instance")
3169             raise exception.InstanceNotRescuable(instance_id=instance.uuid,
3170                                                  reason=reason)
3171 
3172         instance.task_state = task_states.RESCUING
3173         instance.save(expected_task_state=[None])
3174 
3175         self._record_action_start(context, instance, instance_actions.RESCUE)
3176 
3177         self.compute_rpcapi.rescue_instance(context, instance=instance,
3178             rescue_password=rescue_password, rescue_image_ref=rescue_image_ref,
3179             clean_shutdown=clean_shutdown)
3180 
3181     @check_instance_lock
3182     @check_instance_state(vm_state=[vm_states.RESCUED])
3183     def unrescue(self, context, instance):
3184         """Unrescue the given instance."""
3185         instance.task_state = task_states.UNRESCUING
3186         instance.save(expected_task_state=[None])
3187 
3188         self._record_action_start(context, instance, instance_actions.UNRESCUE)
3189 
3190         self.compute_rpcapi.unrescue_instance(context, instance=instance)
3191 
3192     @check_instance_lock
3193     @check_instance_cell
3194     @check_instance_state(vm_state=[vm_states.ACTIVE])
3195     def set_admin_password(self, context, instance, password=None):
3196         """Set the root/admin password for the given instance.
3197 
3198         @param context: Nova auth context.
3199         @param instance: Nova instance object.
3200         @param password: The admin password for the instance.
3201         """
3202         instance.task_state = task_states.UPDATING_PASSWORD
3203         instance.save(expected_task_state=[None])
3204 
3205         self._record_action_start(context, instance,
3206                                   instance_actions.CHANGE_PASSWORD)
3207 
3208         self.compute_rpcapi.set_admin_password(context,
3209                                                instance=instance,
3210                                                new_pass=password)
3211 
3212     @check_instance_host
3213     def get_vnc_console(self, context, instance, console_type):
3214         """Get a url to an instance Console."""
3215         connect_info = self.compute_rpcapi.get_vnc_console(context,
3216                 instance=instance, console_type=console_type)
3217 
3218         self.consoleauth_rpcapi.authorize_console(context,
3219                 connect_info['token'], console_type,
3220                 connect_info['host'], connect_info['port'],
3221                 connect_info['internal_access_path'], instance.uuid,
3222                 access_url=connect_info['access_url'])
3223 
3224         return {'url': connect_info['access_url']}
3225 
3226     @check_instance_host
3227     def get_vnc_connect_info(self, context, instance, console_type):
3228         """Used in a child cell to get console info."""
3229         connect_info = self.compute_rpcapi.get_vnc_console(context,
3230                 instance=instance, console_type=console_type)
3231         return connect_info
3232 
3233     @check_instance_host
3234     def get_spice_console(self, context, instance, console_type):
3235         """Get a url to an instance Console."""
3236         connect_info = self.compute_rpcapi.get_spice_console(context,
3237                 instance=instance, console_type=console_type)
3238         self.consoleauth_rpcapi.authorize_console(context,
3239                 connect_info['token'], console_type,
3240                 connect_info['host'], connect_info['port'],
3241                 connect_info['internal_access_path'], instance.uuid,
3242                 access_url=connect_info['access_url'])
3243 
3244         return {'url': connect_info['access_url']}
3245 
3246     @check_instance_host
3247     def get_spice_connect_info(self, context, instance, console_type):
3248         """Used in a child cell to get console info."""
3249         connect_info = self.compute_rpcapi.get_spice_console(context,
3250                 instance=instance, console_type=console_type)
3251         return connect_info
3252 
3253     @check_instance_host
3254     def get_rdp_console(self, context, instance, console_type):
3255         """Get a url to an instance Console."""
3256         connect_info = self.compute_rpcapi.get_rdp_console(context,
3257                 instance=instance, console_type=console_type)
3258         self.consoleauth_rpcapi.authorize_console(context,
3259                 connect_info['token'], console_type,
3260                 connect_info['host'], connect_info['port'],
3261                 connect_info['internal_access_path'], instance.uuid,
3262                 access_url=connect_info['access_url'])
3263 
3264         return {'url': connect_info['access_url']}
3265 
3266     @check_instance_host
3267     def get_rdp_connect_info(self, context, instance, console_type):
3268         """Used in a child cell to get console info."""
3269         connect_info = self.compute_rpcapi.get_rdp_console(context,
3270                 instance=instance, console_type=console_type)
3271         return connect_info
3272 
3273     @check_instance_host
3274     def get_serial_console(self, context, instance, console_type):
3275         """Get a url to a serial console."""
3276         connect_info = self.compute_rpcapi.get_serial_console(context,
3277                 instance=instance, console_type=console_type)
3278 
3279         self.consoleauth_rpcapi.authorize_console(context,
3280                 connect_info['token'], console_type,
3281                 connect_info['host'], connect_info['port'],
3282                 connect_info['internal_access_path'], instance.uuid,
3283                 access_url=connect_info['access_url'])
3284         return {'url': connect_info['access_url']}
3285 
3286     @check_instance_host
3287     def get_serial_console_connect_info(self, context, instance, console_type):
3288         """Used in a child cell to get serial console."""
3289         connect_info = self.compute_rpcapi.get_serial_console(context,
3290                 instance=instance, console_type=console_type)
3291         return connect_info
3292 
3293     @check_instance_host
3294     def get_mks_console(self, context, instance, console_type):
3295         """Get a url to a MKS console."""
3296         connect_info = self.compute_rpcapi.get_mks_console(context,
3297                 instance=instance, console_type=console_type)
3298         self.consoleauth_rpcapi.authorize_console(context,
3299                 connect_info['token'], console_type,
3300                 connect_info['host'], connect_info['port'],
3301                 connect_info['internal_access_path'], instance.uuid,
3302                 access_url=connect_info['access_url'])
3303         return {'url': connect_info['access_url']}
3304 
3305     @check_instance_host
3306     def get_console_output(self, context, instance, tail_length=None):
3307         """Get console output for an instance."""
3308         return self.compute_rpcapi.get_console_output(context,
3309                 instance=instance, tail_length=tail_length)
3310 
3311     def lock(self, context, instance):
3312         """Lock the given instance."""
3313         # Only update the lock if we are an admin (non-owner)
3314         is_owner = instance.project_id == context.project_id
3315         if instance.locked and is_owner:
3316             return
3317 
3318         context = context.elevated()
3319         LOG.debug('Locking', instance=instance)
3320         instance.locked = True
3321         instance.locked_by = 'owner' if is_owner else 'admin'
3322         instance.save()
3323 
3324     def is_expected_locked_by(self, context, instance):
3325         is_owner = instance.project_id == context.project_id
3326         expect_locked_by = 'owner' if is_owner else 'admin'
3327         locked_by = instance.locked_by
3328         if locked_by and locked_by != expect_locked_by:
3329             return False
3330         return True
3331 
3332     def unlock(self, context, instance):
3333         """Unlock the given instance."""
3334         context = context.elevated()
3335         LOG.debug('Unlocking', instance=instance)
3336         instance.locked = False
3337         instance.locked_by = None
3338         instance.save()
3339 
3340     @check_instance_lock
3341     @check_instance_cell
3342     def reset_network(self, context, instance):
3343         """Reset networking on the instance."""
3344         self.compute_rpcapi.reset_network(context, instance=instance)
3345 
3346     @check_instance_lock
3347     @check_instance_cell
3348     def inject_network_info(self, context, instance):
3349         """Inject network info for the instance."""
3350         self.compute_rpcapi.inject_network_info(context, instance=instance)
3351 
3352     def _create_volume_bdm(self, context, instance, device, volume_id,
3353                            disk_bus, device_type, is_local_creation=False):
3354         if is_local_creation:
3355             # when the creation is done locally we can't specify the device
3356             # name as we do not have a way to check that the name specified is
3357             # a valid one.
3358             # We leave the setting of that value when the actual attach
3359             # happens on the compute manager
3360             volume_bdm = objects.BlockDeviceMapping(
3361                 context=context,
3362                 source_type='volume', destination_type='volume',
3363                 instance_uuid=instance.uuid, boot_index=None,
3364                 volume_id=volume_id,
3365                 device_name=None, guest_format=None,
3366                 disk_bus=disk_bus, device_type=device_type)
3367             volume_bdm.create()
3368         else:
3369             # NOTE(vish): This is done on the compute host because we want
3370             #             to avoid a race where two devices are requested at
3371             #             the same time. When db access is removed from
3372             #             compute, the bdm will be created here and we will
3373             #             have to make sure that they are assigned atomically.
3374             volume_bdm = self.compute_rpcapi.reserve_block_device_name(
3375                 context, instance, device, volume_id, disk_bus=disk_bus,
3376                 device_type=device_type)
3377         return volume_bdm
3378 
3379     def _check_attach_and_reserve_volume(self, context, volume_id, instance):
3380         volume = self.volume_api.get(context, volume_id)
3381         self.volume_api.check_availability_zone(context, volume,
3382                                                 instance=instance)
3383         self.volume_api.reserve_volume(context, volume_id)
3384 
3385         return volume
3386 
3387     def _attach_volume(self, context, instance, volume_id, device,
3388                        disk_bus, device_type):
3389         """Attach an existing volume to an existing instance.
3390 
3391         This method is separated to make it possible for cells version
3392         to override it.
3393         """
3394         volume_bdm = self._create_volume_bdm(
3395             context, instance, device, volume_id, disk_bus=disk_bus,
3396             device_type=device_type)
3397         try:
3398             self._check_attach_and_reserve_volume(context, volume_id, instance)
3399             self.compute_rpcapi.attach_volume(context, instance, volume_bdm)
3400         except Exception:
3401             with excutils.save_and_reraise_exception():
3402                 volume_bdm.destroy()
3403 
3404         return volume_bdm.device_name
3405 
3406     def _attach_volume_shelved_offloaded(self, context, instance, volume_id,
3407                                          device, disk_bus, device_type):
3408         """Attach an existing volume to an instance in shelved offloaded state.
3409 
3410         Attaching a volume for an instance in shelved offloaded state requires
3411         to perform the regular check to see if we can attach and reserve the
3412         volume then we need to call the attach method on the volume API
3413         to mark the volume as 'in-use'.
3414         The instance at this stage is not managed by a compute manager
3415         therefore the actual attachment will be performed once the
3416         instance will be unshelved.
3417         """
3418 
3419         volume_bdm = self._create_volume_bdm(
3420             context, instance, device, volume_id, disk_bus=disk_bus,
3421             device_type=device_type, is_local_creation=True)
3422         try:
3423             self._check_attach_and_reserve_volume(context, volume_id, instance)
3424             self.volume_api.attach(context,
3425                                    volume_id,
3426                                    instance.uuid,
3427                                    device)
3428         except Exception:
3429             with excutils.save_and_reraise_exception():
3430                 volume_bdm.destroy()
3431 
3432         return volume_bdm.device_name
3433 
3434     @check_instance_lock
3435     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3436                                     vm_states.STOPPED, vm_states.RESIZED,
3437                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3438                                     vm_states.SHELVED_OFFLOADED])
3439     def attach_volume(self, context, instance, volume_id, device=None,
3440                        disk_bus=None, device_type=None):
3441         """Attach an existing volume to an existing instance."""
3442         # NOTE(vish): Fail fast if the device is not going to pass. This
3443         #             will need to be removed along with the test if we
3444         #             change the logic in the manager for what constitutes
3445         #             a valid device.
3446         if device and not block_device.match_device(device):
3447             raise exception.InvalidDevicePath(path=device)
3448 
3449         is_shelved_offloaded = instance.vm_state == vm_states.SHELVED_OFFLOADED
3450         if is_shelved_offloaded:
3451             return self._attach_volume_shelved_offloaded(context,
3452                                                          instance,
3453                                                          volume_id,
3454                                                          device,
3455                                                          disk_bus,
3456                                                          device_type)
3457 
3458         return self._attach_volume(context, instance, volume_id, device,
3459                                    disk_bus, device_type)
3460 
3461     def _check_and_begin_detach(self, context, volume, instance):
3462         self.volume_api.check_detach(context, volume, instance=instance)
3463         self.volume_api.begin_detaching(context, volume['id'])
3464 
3465     def _detach_volume(self, context, instance, volume):
3466         """Detach volume from instance.
3467 
3468         This method is separated to make it easier for cells version
3469         to override.
3470         """
3471         self._check_and_begin_detach(context, volume, instance)
3472         attachments = volume.get('attachments', {})
3473         attachment_id = None
3474         if attachments and instance.uuid in attachments:
3475             attachment_id = attachments[instance.uuid]['attachment_id']
3476         self.compute_rpcapi.detach_volume(context, instance=instance,
3477                 volume_id=volume['id'], attachment_id=attachment_id)
3478 
3479     def _detach_volume_shelved_offloaded(self, context, instance, volume):
3480         """Detach a volume from an instance in shelved offloaded state.
3481 
3482         If the instance is shelved offloaded we just need to cleanup volume
3483         calling the volume api detach, the volume api terminate_connection
3484         and delete the bdm record.
3485         If the volume has delete_on_termination option set then we call the
3486         volume api delete as well.
3487         """
3488         self._check_and_begin_detach(context, volume, instance)
3489         bdms = [objects.BlockDeviceMapping.get_by_volume_id(
3490                 context, volume['id'], instance.uuid)]
3491         self._local_cleanup_bdm_volumes(bdms, instance, context)
3492 
3493     @check_instance_lock
3494     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3495                                     vm_states.STOPPED, vm_states.RESIZED,
3496                                     vm_states.SOFT_DELETED, vm_states.SHELVED,
3497                                     vm_states.SHELVED_OFFLOADED])
3498     def detach_volume(self, context, instance, volume):
3499         """Detach a volume from an instance."""
3500         if instance.vm_state == vm_states.SHELVED_OFFLOADED:
3501             self._detach_volume_shelved_offloaded(context, instance, volume)
3502         else:
3503             self._detach_volume(context, instance, volume)
3504 
3505     @check_instance_lock
3506     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3507                                     vm_states.SUSPENDED, vm_states.STOPPED,
3508                                     vm_states.RESIZED, vm_states.SOFT_DELETED])
3509     def swap_volume(self, context, instance, old_volume, new_volume):
3510         """Swap volume attached to an instance."""
3511         if old_volume['attach_status'] == 'detached':
3512             raise exception.VolumeUnattached(volume_id=old_volume['id'])
3513         # The caller likely got the instance from volume['attachments']
3514         # in the first place, but let's sanity check.
3515         if not old_volume.get('attachments', {}).get(instance.uuid):
3516             msg = _("Old volume is attached to a different instance.")
3517             raise exception.InvalidVolume(reason=msg)
3518         if new_volume['attach_status'] == 'attached':
3519             msg = _("New volume must be detached in order to swap.")
3520             raise exception.InvalidVolume(reason=msg)
3521         if int(new_volume['size']) < int(old_volume['size']):
3522             msg = _("New volume must be the same size or larger.")
3523             raise exception.InvalidVolume(reason=msg)
3524         self.volume_api.check_detach(context, old_volume)
3525         self.volume_api.check_availability_zone(context, new_volume,
3526                                                 instance=instance)
3527         self.volume_api.begin_detaching(context, old_volume['id'])
3528         self.volume_api.reserve_volume(context, new_volume['id'])
3529         try:
3530             self.compute_rpcapi.swap_volume(
3531                     context, instance=instance,
3532                     old_volume_id=old_volume['id'],
3533                     new_volume_id=new_volume['id'])
3534         except Exception:
3535             with excutils.save_and_reraise_exception():
3536                 self.volume_api.roll_detaching(context, old_volume['id'])
3537                 self.volume_api.unreserve_volume(context, new_volume['id'])
3538 
3539     @check_instance_lock
3540     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3541                                     vm_states.STOPPED],
3542                           task_state=[None])
3543     def attach_interface(self, context, instance, network_id, port_id,
3544                          requested_ip):
3545         """Use hotplug to add an network adapter to an instance."""
3546         return self.compute_rpcapi.attach_interface(context,
3547             instance=instance, network_id=network_id, port_id=port_id,
3548             requested_ip=requested_ip)
3549 
3550     @check_instance_lock
3551     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3552                                     vm_states.STOPPED],
3553                           task_state=[None])
3554     def detach_interface(self, context, instance, port_id):
3555         """Detach an network adapter from an instance."""
3556         self.compute_rpcapi.detach_interface(context, instance=instance,
3557             port_id=port_id)
3558 
3559     def get_instance_metadata(self, context, instance):
3560         """Get all metadata associated with an instance."""
3561         return self.db.instance_metadata_get(context, instance.uuid)
3562 
3563     def get_all_instance_metadata(self, context, search_filts):
3564         return self._get_all_instance_metadata(
3565             context, search_filts, metadata_type='metadata')
3566 
3567     def get_all_system_metadata(self, context, search_filts):
3568         return self._get_all_instance_metadata(
3569             context, search_filts, metadata_type='system_metadata')
3570 
3571     def _get_all_instance_metadata(self, context, search_filts, metadata_type):
3572         """Get all metadata."""
3573         instances = self._get_instances_by_filters(context, filters={},
3574                                                    sort_keys=['created_at'],
3575                                                    sort_dirs=['desc'])
3576         return utils.filter_and_format_resource_metadata('instance', instances,
3577                 search_filts, metadata_type)
3578 
3579     @check_instance_lock
3580     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3581                                     vm_states.SUSPENDED, vm_states.STOPPED],
3582                           task_state=None)
3583     def delete_instance_metadata(self, context, instance, key):
3584         """Delete the given metadata item from an instance."""
3585         instance.delete_metadata_key(key)
3586         self.compute_rpcapi.change_instance_metadata(context,
3587                                                      instance=instance,
3588                                                      diff={key: ['-']})
3589 
3590     @check_instance_lock
3591     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED,
3592                                     vm_states.SUSPENDED, vm_states.STOPPED],
3593                           task_state=None)
3594     def update_instance_metadata(self, context, instance,
3595                                  metadata, delete=False):
3596         """Updates or creates instance metadata.
3597 
3598         If delete is True, metadata items that are not specified in the
3599         `metadata` argument will be deleted.
3600 
3601         """
3602         orig = dict(instance.metadata)
3603         if delete:
3604             _metadata = metadata
3605         else:
3606             _metadata = dict(instance.metadata)
3607             _metadata.update(metadata)
3608 
3609         self._check_metadata_properties_quota(context, _metadata)
3610         instance.metadata = _metadata
3611         instance.save()
3612         diff = _diff_dict(orig, instance.metadata)
3613         self.compute_rpcapi.change_instance_metadata(context,
3614                                                      instance=instance,
3615                                                      diff=diff)
3616         return _metadata
3617 
3618     @check_instance_lock
3619     @check_instance_cell
3620     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.PAUSED])
3621     def live_migrate(self, context, instance, block_migration,
3622                      disk_over_commit, host_name, force=None, async=False):
3623         """Migrate a server lively to a new host."""
3624         LOG.debug("Going to try to live migrate instance to %s",
3625                   host_name or "another host", instance=instance)
3626 
3627         instance.task_state = task_states.MIGRATING
3628         instance.save(expected_task_state=[None])
3629 
3630         self._record_action_start(context, instance,
3631                                   instance_actions.LIVE_MIGRATION)
3632         try:
3633             request_spec = objects.RequestSpec.get_by_instance_uuid(
3634                 context, instance.uuid)
3635         except exception.RequestSpecNotFound:
3636             # Some old instances can still have no RequestSpec object attached
3637             # to them, we need to support the old way
3638             request_spec = None
3639 
3640         # NOTE(sbauza): Force is a boolean by the new related API version
3641         if force is False and host_name:
3642             nodes = objects.ComputeNodeList.get_all_by_host(context, host_name)
3643             # NOTE(sbauza): Unset the host to make sure we call the scheduler
3644             host_name = None
3645             # FIXME(sbauza): Since only Ironic driver uses more than one
3646             # compute per service but doesn't support evacuations,
3647             # let's provide the first one.
3648             target = nodes[0]
3649             if request_spec:
3650                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
3651                 # having a request spec attached to them (particularly true for
3652                 # cells v1). For the moment, let's keep the same behaviour for
3653                 # all the instances but provide the destination only if a spec
3654                 # is found.
3655                 destination = objects.Destination(
3656                     host=target.host,
3657                     node=target.hypervisor_hostname
3658                 )
3659                 request_spec.requested_destination = destination
3660 
3661         try:
3662             self.compute_task_api.live_migrate_instance(context, instance,
3663                 host_name, block_migration=block_migration,
3664                 disk_over_commit=disk_over_commit,
3665                 request_spec=request_spec, async=async)
3666         except oslo_exceptions.MessagingTimeout as messaging_timeout:
3667             with excutils.save_and_reraise_exception():
3668                 # NOTE(pkoniszewski): It is possible that MessagingTimeout
3669                 # occurs, but LM will still be in progress, so write
3670                 # instance fault to database
3671                 compute_utils.add_instance_fault_from_exc(context,
3672                                                           instance,
3673                                                           messaging_timeout)
3674 
3675     @check_instance_lock
3676     @check_instance_cell
3677     @check_instance_state(vm_state=[vm_states.ACTIVE],
3678                           task_state=[task_states.MIGRATING])
3679     def live_migrate_force_complete(self, context, instance, migration_id):
3680         """Force live migration to complete.
3681 
3682         :param context: Security context
3683         :param instance: The instance that is being migrated
3684         :param migration_id: ID of ongoing migration
3685 
3686         """
3687         LOG.debug("Going to try to force live migration to complete",
3688                   instance=instance)
3689 
3690         # NOTE(pkoniszewski): Get migration object to check if there is ongoing
3691         # live migration for particular instance. Also pass migration id to
3692         # compute to double check and avoid possible race condition.
3693         migration = objects.Migration.get_by_id_and_instance(
3694             context, migration_id, instance.uuid)
3695         if migration.status != 'running':
3696             raise exception.InvalidMigrationState(migration_id=migration_id,
3697                                                   instance_uuid=instance.uuid,
3698                                                   state=migration.status,
3699                                                   method='force complete')
3700 
3701         self._record_action_start(
3702             context, instance, instance_actions.LIVE_MIGRATION_FORCE_COMPLETE)
3703 
3704         self.compute_rpcapi.live_migration_force_complete(
3705             context, instance, migration)
3706 
3707     @check_instance_lock
3708     @check_instance_cell
3709     @check_instance_state(task_state=[task_states.MIGRATING])
3710     def live_migrate_abort(self, context, instance, migration_id):
3711         """Abort an in-progress live migration.
3712 
3713         :param context: Security context
3714         :param instance: The instance that is being migrated
3715         :param migration_id: ID of in-progress live migration
3716 
3717         """
3718         migration = objects.Migration.get_by_id_and_instance(context,
3719                     migration_id, instance.uuid)
3720         LOG.debug("Going to cancel live migration %s",
3721                   migration.id, instance=instance)
3722 
3723         if migration.status != 'running':
3724             raise exception.InvalidMigrationState(migration_id=migration_id,
3725                     instance_uuid=instance.uuid,
3726                     state=migration.status,
3727                     method='abort live migration')
3728         self._record_action_start(context, instance,
3729                                   instance_actions.LIVE_MIGRATION_CANCEL)
3730 
3731         self.compute_rpcapi.live_migration_abort(context,
3732                 instance, migration.id)
3733 
3734     @check_instance_state(vm_state=[vm_states.ACTIVE, vm_states.STOPPED,
3735                                     vm_states.ERROR])
3736     def evacuate(self, context, instance, host, on_shared_storage,
3737                  admin_password=None, force=None):
3738         """Running evacuate to target host.
3739 
3740         Checking vm compute host state, if the host not in expected_state,
3741         raising an exception.
3742 
3743         :param instance: The instance to evacuate
3744         :param host: Target host. if not set, the scheduler will pick up one
3745         :param on_shared_storage: True if instance files on shared storage
3746         :param admin_password: password to set on rebuilt instance
3747         :param force: Force the evacuation to the specific host target
3748 
3749         """
3750         LOG.debug('vm evacuation scheduled', instance=instance)
3751         inst_host = instance.host
3752         service = objects.Service.get_by_compute_host(context, inst_host)
3753         if self.servicegroup_api.service_is_up(service):
3754             LOG.error(_LE('Instance compute service state on %s '
3755                           'expected to be down, but it was up.'), inst_host)
3756             raise exception.ComputeServiceInUse(host=inst_host)
3757 
3758         instance.task_state = task_states.REBUILDING
3759         instance.save(expected_task_state=[None])
3760         self._record_action_start(context, instance, instance_actions.EVACUATE)
3761 
3762         # NOTE(danms): Create this as a tombstone for the source compute
3763         # to find and cleanup. No need to pass it anywhere else.
3764         migration = objects.Migration(context,
3765                                       source_compute=instance.host,
3766                                       source_node=instance.node,
3767                                       instance_uuid=instance.uuid,
3768                                       status='accepted',
3769                                       migration_type='evacuation')
3770         if host:
3771             migration.dest_compute = host
3772         migration.create()
3773 
3774         compute_utils.notify_about_instance_usage(
3775             self.notifier, context, instance, "evacuate")
3776 
3777         try:
3778             request_spec = objects.RequestSpec.get_by_instance_uuid(
3779                 context, instance.uuid)
3780         except exception.RequestSpecNotFound:
3781             # Some old instances can still have no RequestSpec object attached
3782             # to them, we need to support the old way
3783             request_spec = None
3784 
3785         # NOTE(sbauza): Force is a boolean by the new related API version
3786         if force is False and host:
3787             nodes = objects.ComputeNodeList.get_all_by_host(context, host)
3788             # NOTE(sbauza): Unset the host to make sure we call the scheduler
3789             host = None
3790             # FIXME(sbauza): Since only Ironic driver uses more than one
3791             # compute per service but doesn't support evacuations,
3792             # let's provide the first one.
3793             target = nodes[0]
3794             if request_spec:
3795                 # TODO(sbauza): Hydrate a fake spec for old instances not yet
3796                 # having a request spec attached to them (particularly true for
3797                 # cells v1). For the moment, let's keep the same behaviour for
3798                 # all the instances but provide the destination only if a spec
3799                 # is found.
3800                 destination = objects.Destination(
3801                     host=target.host,
3802                     node=target.hypervisor_hostname
3803                 )
3804                 request_spec.requested_destination = destination
3805 
3806         return self.compute_task_api.rebuild_instance(context,
3807                        instance=instance,
3808                        new_pass=admin_password,
3809                        injected_files=None,
3810                        image_ref=None,
3811                        orig_image_ref=None,
3812                        orig_sys_metadata=None,
3813                        bdms=None,
3814                        recreate=True,
3815                        on_shared_storage=on_shared_storage,
3816                        host=host,
3817                        request_spec=request_spec,
3818                        )
3819 
3820     def get_migrations(self, context, filters):
3821         """Get all migrations for the given filters."""
3822         return objects.MigrationList.get_by_filters(context, filters)
3823 
3824     def get_migrations_in_progress_by_instance(self, context, instance_uuid,
3825                                                migration_type=None):
3826         """Get all migrations of an instance in progress."""
3827         return objects.MigrationList.get_in_progress_by_instance(
3828                 context, instance_uuid, migration_type)
3829 
3830     def get_migration_by_id_and_instance(self, context,
3831                                          migration_id, instance_uuid):
3832         """Get the migration of an instance by id."""
3833         return objects.Migration.get_by_id_and_instance(
3834                 context, migration_id, instance_uuid)
3835 
3836     def volume_snapshot_create(self, context, volume_id, create_info):
3837         bdm = objects.BlockDeviceMapping.get_by_volume(
3838                 context, volume_id, expected_attrs=['instance'])
3839         self.compute_rpcapi.volume_snapshot_create(context, bdm.instance,
3840                 volume_id, create_info)
3841         snapshot = {
3842             'snapshot': {
3843                 'id': create_info.get('id'),
3844                 'volumeId': volume_id
3845             }
3846         }
3847         return snapshot
3848 
3849     def volume_snapshot_delete(self, context, volume_id, snapshot_id,
3850                                delete_info):
3851         bdm = objects.BlockDeviceMapping.get_by_volume(
3852                 context, volume_id, expected_attrs=['instance'])
3853         self.compute_rpcapi.volume_snapshot_delete(context, bdm.instance,
3854                 volume_id, snapshot_id, delete_info)
3855 
3856     def external_instance_event(self, context, instances, mappings, events):
3857         # NOTE(danms): The external API consumer just provides events,
3858         # but doesn't know where they go. We need to collate lists
3859         # by the host the affected instance is on and dispatch them
3860         # according to host
3861         instances_by_host = collections.defaultdict(list)
3862         events_by_host = collections.defaultdict(list)
3863         hosts_by_instance = collections.defaultdict(list)
3864         for instance in instances:
3865             for host in self._get_relevant_hosts(context, instance):
3866                 instances_by_host[host].append(instance)
3867                 hosts_by_instance[instance.uuid].append(host)
3868 
3869         for event in events:
3870             for host in hosts_by_instance[event.instance_uuid]:
3871                 events_by_host[host].append(event)
3872 
3873         for host in instances_by_host:
3874             # NOTE(danms): All instances on a host must have the same
3875             # mapping, so just use that
3876             cell_mapping = mappings[instances_by_host[host][0].uuid]
3877 
3878             # TODO(salv-orlando): Handle exceptions raised by the rpc api layer
3879             # in order to ensure that a failure in processing events on a host
3880             # will not prevent processing events on other hosts
3881             with nova_context.target_cell(context, cell_mapping):
3882                 self.compute_rpcapi.external_instance_event(
3883                     context, instances_by_host[host], events_by_host[host],
3884                     host=host)
3885 
3886     def _get_relevant_hosts(self, context, instance):
3887         hosts = set()
3888         hosts.add(instance.host)
3889         if instance.migration_context is not None:
3890             migration_id = instance.migration_context.migration_id
3891             migration = objects.Migration.get_by_id(context, migration_id)
3892             hosts.add(migration.dest_compute)
3893             hosts.add(migration.source_compute)
3894             LOG.debug('Instance %(instance)s is migrating, '
3895                       'copying events to all relevant hosts: '
3896                       '%(hosts)s', {'instance': instance.uuid,
3897                                     'hosts': hosts})
3898         return hosts
3899 
3900     def get_instance_host_status(self, instance):
3901         if instance.host:
3902             try:
3903                 service = [service for service in instance.services if
3904                            service.binary == 'nova-compute'][0]
3905                 if service.forced_down:
3906                     host_status = fields_obj.HostStatus.DOWN
3907                 elif service.disabled:
3908                     host_status = fields_obj.HostStatus.MAINTENANCE
3909                 else:
3910                     alive = self.servicegroup_api.service_is_up(service)
3911                     host_status = ((alive and fields_obj.HostStatus.UP) or
3912                                    fields_obj.HostStatus.UNKNOWN)
3913             except IndexError:
3914                 host_status = fields_obj.HostStatus.NONE
3915         else:
3916             host_status = fields_obj.HostStatus.NONE
3917         return host_status
3918 
3919     def get_instances_host_statuses(self, instance_list):
3920         host_status_dict = dict()
3921         host_statuses = dict()
3922         for instance in instance_list:
3923             if instance.host:
3924                 if instance.host not in host_status_dict:
3925                     host_status = self.get_instance_host_status(instance)
3926                     host_status_dict[instance.host] = host_status
3927                 else:
3928                     host_status = host_status_dict[instance.host]
3929             else:
3930                 host_status = fields_obj.HostStatus.NONE
3931             host_statuses[instance.uuid] = host_status
3932         return host_statuses
3933 
3934 
3935 class HostAPI(base.Base):
3936     """Sub-set of the Compute Manager API for managing host operations."""
3937 
3938     def __init__(self, rpcapi=None):
3939         self.rpcapi = rpcapi or compute_rpcapi.ComputeAPI()
3940         self.servicegroup_api = servicegroup.API()
3941         super(HostAPI, self).__init__()
3942 
3943     def _assert_host_exists(self, context, host_name, must_be_up=False):
3944         """Raise HostNotFound if compute host doesn't exist."""
3945         service = objects.Service.get_by_compute_host(context, host_name)
3946         if not service:
3947             raise exception.HostNotFound(host=host_name)
3948         if must_be_up and not self.servicegroup_api.service_is_up(service):
3949             raise exception.ComputeServiceUnavailable(host=host_name)
3950         return service['host']
3951 
3952     @wrap_exception()
3953     def set_host_enabled(self, context, host_name, enabled):
3954         """Sets the specified host's ability to accept new instances."""
3955         host_name = self._assert_host_exists(context, host_name)
3956         payload = {'host_name': host_name, 'enabled': enabled}
3957         compute_utils.notify_about_host_update(context,
3958                                                'set_enabled.start',
3959                                                payload)
3960         result = self.rpcapi.set_host_enabled(context, enabled=enabled,
3961                 host=host_name)
3962         compute_utils.notify_about_host_update(context,
3963                                                'set_enabled.end',
3964                                                payload)
3965         return result
3966 
3967     def get_host_uptime(self, context, host_name):
3968         """Returns the result of calling "uptime" on the target host."""
3969         host_name = self._assert_host_exists(context, host_name,
3970                          must_be_up=True)
3971         return self.rpcapi.get_host_uptime(context, host=host_name)
3972 
3973     @wrap_exception()
3974     def host_power_action(self, context, host_name, action):
3975         """Reboots, shuts down or powers up the host."""
3976         host_name = self._assert_host_exists(context, host_name)
3977         payload = {'host_name': host_name, 'action': action}
3978         compute_utils.notify_about_host_update(context,
3979                                                'power_action.start',
3980                                                payload)
3981         result = self.rpcapi.host_power_action(context, action=action,
3982                 host=host_name)
3983         compute_utils.notify_about_host_update(context,
3984                                                'power_action.end',
3985                                                payload)
3986         return result
3987 
3988     @wrap_exception()
3989     def set_host_maintenance(self, context, host_name, mode):
3990         """Start/Stop host maintenance window. On start, it triggers
3991         guest VMs evacuation.
3992         """
3993         host_name = self._assert_host_exists(context, host_name)
3994         payload = {'host_name': host_name, 'mode': mode}
3995         compute_utils.notify_about_host_update(context,
3996                                                'set_maintenance.start',
3997                                                payload)
3998         result = self.rpcapi.host_maintenance_mode(context,
3999                 host_param=host_name, mode=mode, host=host_name)
4000         compute_utils.notify_about_host_update(context,
4001                                                'set_maintenance.end',
4002                                                payload)
4003         return result
4004 
4005     def service_get_all(self, context, filters=None, set_zones=False):
4006         """Returns a list of services, optionally filtering the results.
4007 
4008         If specified, 'filters' should be a dictionary containing services
4009         attributes and matching values.  Ie, to get a list of services for
4010         the 'compute' topic, use filters={'topic': 'compute'}.
4011         """
4012         if filters is None:
4013             filters = {}
4014         disabled = filters.pop('disabled', None)
4015         if 'availability_zone' in filters:
4016             set_zones = True
4017         services = objects.ServiceList.get_all(context, disabled,
4018                                                set_zones=set_zones)
4019         ret_services = []
4020         for service in services:
4021             for key, val in filters.items():
4022                 if service[key] != val:
4023                     break
4024             else:
4025                 # All filters matched.
4026                 ret_services.append(service)
4027         return ret_services
4028 
4029     def service_get_by_id(self, context, service_id):
4030         """Get service entry for the given service id."""
4031         return objects.Service.get_by_id(context, service_id)
4032 
4033     def service_get_by_compute_host(self, context, host_name):
4034         """Get service entry for the given compute hostname."""
4035         return objects.Service.get_by_compute_host(context, host_name)
4036 
4037     def _service_update(self, context, host_name, binary, params_to_update):
4038         """Performs the actual service update operation."""
4039         service = objects.Service.get_by_args(context, host_name, binary)
4040         service.update(params_to_update)
4041         service.save()
4042         return service
4043 
4044     def service_update(self, context, host_name, binary, params_to_update):
4045         """Enable / Disable a service.
4046 
4047         For compute services, this stops new builds and migrations going to
4048         the host.
4049         """
4050         return self._service_update(context, host_name, binary,
4051                                     params_to_update)
4052 
4053     def _service_delete(self, context, service_id):
4054         """Performs the actual Service deletion operation."""
4055         objects.Service.get_by_id(context, service_id).destroy()
4056 
4057     def service_delete(self, context, service_id):
4058         """Deletes the specified service."""
4059         self._service_delete(context, service_id)
4060 
4061     def instance_get_all_by_host(self, context, host_name):
4062         """Return all instances on the given host."""
4063         return objects.InstanceList.get_by_host(context, host_name)
4064 
4065     def task_log_get_all(self, context, task_name, period_beginning,
4066                          period_ending, host=None, state=None):
4067         """Return the task logs within a given range, optionally
4068         filtering by host and/or state.
4069         """
4070         return self.db.task_log_get_all(context, task_name,
4071                                         period_beginning,
4072                                         period_ending,
4073                                         host=host,
4074                                         state=state)
4075 
4076     def compute_node_get(self, context, compute_id):
4077         """Return compute node entry for particular integer ID."""
4078         return objects.ComputeNode.get_by_id(context, int(compute_id))
4079 
4080     def compute_node_get_all(self, context, limit=None, marker=None):
4081         return objects.ComputeNodeList.get_by_pagination(
4082             context, limit=limit, marker=marker)
4083 
4084     def compute_node_search_by_hypervisor(self, context, hypervisor_match):
4085         return objects.ComputeNodeList.get_by_hypervisor(context,
4086                                                          hypervisor_match)
4087 
4088     def compute_node_statistics(self, context):
4089         return self.db.compute_node_statistics(context)
4090 
4091 
4092 class InstanceActionAPI(base.Base):
4093     """Sub-set of the Compute Manager API for managing instance actions."""
4094 
4095     def actions_get(self, context, instance):
4096         return objects.InstanceActionList.get_by_instance_uuid(
4097             context, instance.uuid)
4098 
4099     def action_get_by_request_id(self, context, instance, request_id):
4100         return objects.InstanceAction.get_by_request_id(
4101             context, instance.uuid, request_id)
4102 
4103     def action_events_get(self, context, instance, action_id):
4104         return objects.InstanceActionEventList.get_by_action(
4105             context, action_id)
4106 
4107 
4108 class AggregateAPI(base.Base):
4109     """Sub-set of the Compute Manager API for managing host aggregates."""
4110     def __init__(self, **kwargs):
4111         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4112         self.scheduler_client = scheduler_client.SchedulerClient()
4113         super(AggregateAPI, self).__init__(**kwargs)
4114 
4115     @wrap_exception()
4116     def create_aggregate(self, context, aggregate_name, availability_zone):
4117         """Creates the model for the aggregate."""
4118 
4119         aggregate = objects.Aggregate(context=context)
4120         aggregate.name = aggregate_name
4121         if availability_zone:
4122             aggregate.metadata = {'availability_zone': availability_zone}
4123         aggregate.create()
4124         self.scheduler_client.update_aggregates(context, [aggregate])
4125         return aggregate
4126 
4127     def get_aggregate(self, context, aggregate_id):
4128         """Get an aggregate by id."""
4129         return objects.Aggregate.get_by_id(context, aggregate_id)
4130 
4131     def get_aggregate_list(self, context):
4132         """Get all the aggregates."""
4133         return objects.AggregateList.get_all(context)
4134 
4135     def get_aggregates_by_host(self, context, compute_host):
4136         """Get all the aggregates where the given host is presented."""
4137         return objects.AggregateList.get_by_host(context, compute_host)
4138 
4139     @wrap_exception()
4140     def update_aggregate(self, context, aggregate_id, values):
4141         """Update the properties of an aggregate."""
4142         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4143         if 'name' in values:
4144             aggregate.name = values.pop('name')
4145             aggregate.save()
4146         self.is_safe_to_update_az(context, values, aggregate=aggregate,
4147                                   action_name=AGGREGATE_ACTION_UPDATE)
4148         if values:
4149             aggregate.update_metadata(values)
4150             aggregate.updated_at = timeutils.utcnow()
4151         self.scheduler_client.update_aggregates(context, [aggregate])
4152         # If updated values include availability_zones, then the cache
4153         # which stored availability_zones and host need to be reset
4154         if values.get('availability_zone'):
4155             availability_zones.reset_cache()
4156         return aggregate
4157 
4158     @wrap_exception()
4159     def update_aggregate_metadata(self, context, aggregate_id, metadata):
4160         """Updates the aggregate metadata."""
4161         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4162         self.is_safe_to_update_az(context, metadata, aggregate=aggregate,
4163                                   action_name=AGGREGATE_ACTION_UPDATE_META)
4164         aggregate.update_metadata(metadata)
4165         self.scheduler_client.update_aggregates(context, [aggregate])
4166         # If updated metadata include availability_zones, then the cache
4167         # which stored availability_zones and host need to be reset
4168         if metadata and metadata.get('availability_zone'):
4169             availability_zones.reset_cache()
4170         aggregate.updated_at = timeutils.utcnow()
4171         return aggregate
4172 
4173     @wrap_exception()
4174     def delete_aggregate(self, context, aggregate_id):
4175         """Deletes the aggregate."""
4176         aggregate_payload = {'aggregate_id': aggregate_id}
4177         compute_utils.notify_about_aggregate_update(context,
4178                                                     "delete.start",
4179                                                     aggregate_payload)
4180         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4181 
4182         compute_utils.notify_about_aggregate_action(
4183             context=context,
4184             aggregate=aggregate,
4185             action=fields_obj.NotificationAction.DELETE,
4186             phase=fields_obj.NotificationPhase.START)
4187 
4188         if len(aggregate.hosts) > 0:
4189             msg = _("Host aggregate is not empty")
4190             raise exception.InvalidAggregateActionDelete(
4191                 aggregate_id=aggregate_id, reason=msg)
4192         aggregate.destroy()
4193         self.scheduler_client.delete_aggregate(context, aggregate)
4194         compute_utils.notify_about_aggregate_update(context,
4195                                                     "delete.end",
4196                                                     aggregate_payload)
4197         compute_utils.notify_about_aggregate_action(
4198             context=context,
4199             aggregate=aggregate,
4200             action=fields_obj.NotificationAction.DELETE,
4201             phase=fields_obj.NotificationPhase.END)
4202 
4203     def is_safe_to_update_az(self, context, metadata, aggregate,
4204                              hosts=None,
4205                              action_name=AGGREGATE_ACTION_ADD):
4206         """Determine if updates alter an aggregate's availability zone.
4207 
4208             :param context: local context
4209             :param metadata: Target metadata for updating aggregate
4210             :param aggregate: Aggregate to update
4211             :param hosts: Hosts to check. If None, aggregate.hosts is used
4212             :type hosts: list
4213             :action_name: Calling method for logging purposes
4214 
4215         """
4216         if 'availability_zone' in metadata:
4217             if not metadata['availability_zone']:
4218                 msg = _("Aggregate %s does not support empty named "
4219                         "availability zone") % aggregate.name
4220                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4221                                                   msg)
4222             _hosts = hosts or aggregate.hosts
4223             host_aggregates = objects.AggregateList.get_by_metadata_key(
4224                 context, 'availability_zone', hosts=_hosts)
4225             conflicting_azs = [
4226                 agg.availability_zone for agg in host_aggregates
4227                 if agg.availability_zone != metadata['availability_zone']
4228                 and agg.id != aggregate.id]
4229             if conflicting_azs:
4230                 msg = _("One or more hosts already in availability zone(s) "
4231                         "%s") % conflicting_azs
4232                 self._raise_invalid_aggregate_exc(action_name, aggregate.id,
4233                                                   msg)
4234 
4235     def _raise_invalid_aggregate_exc(self, action_name, aggregate_id, reason):
4236         if action_name == AGGREGATE_ACTION_ADD:
4237             raise exception.InvalidAggregateActionAdd(
4238                 aggregate_id=aggregate_id, reason=reason)
4239         elif action_name == AGGREGATE_ACTION_UPDATE:
4240             raise exception.InvalidAggregateActionUpdate(
4241                 aggregate_id=aggregate_id, reason=reason)
4242         elif action_name == AGGREGATE_ACTION_UPDATE_META:
4243             raise exception.InvalidAggregateActionUpdateMeta(
4244                 aggregate_id=aggregate_id, reason=reason)
4245         elif action_name == AGGREGATE_ACTION_DELETE:
4246             raise exception.InvalidAggregateActionDelete(
4247                 aggregate_id=aggregate_id, reason=reason)
4248 
4249         raise exception.NovaException(
4250             _("Unexpected aggregate action %s") % action_name)
4251 
4252     def _update_az_cache_for_host(self, context, host_name, aggregate_meta):
4253         # Update the availability_zone cache to avoid getting wrong
4254         # availability_zone in cache retention time when add/remove
4255         # host to/from aggregate.
4256         if aggregate_meta and aggregate_meta.get('availability_zone'):
4257             availability_zones.update_host_availability_zone_cache(context,
4258                                                                    host_name)
4259 
4260     @wrap_exception()
4261     def add_host_to_aggregate(self, context, aggregate_id, host_name):
4262         """Adds the host to an aggregate."""
4263         aggregate_payload = {'aggregate_id': aggregate_id,
4264                              'host_name': host_name}
4265         compute_utils.notify_about_aggregate_update(context,
4266                                                     "addhost.start",
4267                                                     aggregate_payload)
4268         # validates the host; ComputeHostNotFound is raised if invalid
4269         objects.Service.get_by_compute_host(context, host_name)
4270 
4271         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4272         self.is_safe_to_update_az(context, aggregate.metadata,
4273                                   hosts=[host_name], aggregate=aggregate)
4274 
4275         aggregate.add_host(host_name)
4276         self.scheduler_client.update_aggregates(context, [aggregate])
4277         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
4278         # NOTE(jogo): Send message to host to support resource pools
4279         self.compute_rpcapi.add_aggregate_host(context,
4280                 aggregate=aggregate, host_param=host_name, host=host_name)
4281         aggregate_payload.update({'name': aggregate.name})
4282         compute_utils.notify_about_aggregate_update(context,
4283                                                     "addhost.end",
4284                                                     aggregate_payload)
4285         return aggregate
4286 
4287     @wrap_exception()
4288     def remove_host_from_aggregate(self, context, aggregate_id, host_name):
4289         """Removes host from the aggregate."""
4290         aggregate_payload = {'aggregate_id': aggregate_id,
4291                              'host_name': host_name}
4292         compute_utils.notify_about_aggregate_update(context,
4293                                                     "removehost.start",
4294                                                     aggregate_payload)
4295         # validates the host; ComputeHostNotFound is raised if invalid
4296         objects.Service.get_by_compute_host(context, host_name)
4297         aggregate = objects.Aggregate.get_by_id(context, aggregate_id)
4298         aggregate.delete_host(host_name)
4299         self.scheduler_client.update_aggregates(context, [aggregate])
4300         self._update_az_cache_for_host(context, host_name, aggregate.metadata)
4301         self.compute_rpcapi.remove_aggregate_host(context,
4302                 aggregate=aggregate, host_param=host_name, host=host_name)
4303         compute_utils.notify_about_aggregate_update(context,
4304                                                     "removehost.end",
4305                                                     aggregate_payload)
4306         return aggregate
4307 
4308 
4309 class KeypairAPI(base.Base):
4310     """Subset of the Compute Manager API for managing key pairs."""
4311 
4312     get_notifier = functools.partial(rpc.get_notifier, service='api')
4313     wrap_exception = functools.partial(exception_wrapper.wrap_exception,
4314                                        get_notifier=get_notifier,
4315                                        binary='nova-api')
4316 
4317     def _notify(self, context, event_suffix, keypair_name):
4318         payload = {
4319             'tenant_id': context.project_id,
4320             'user_id': context.user_id,
4321             'key_name': keypair_name,
4322         }
4323         notify = self.get_notifier()
4324         notify.info(context, 'keypair.%s' % event_suffix, payload)
4325 
4326     def _validate_new_key_pair(self, context, user_id, key_name, key_type):
4327         safe_chars = "_- " + string.digits + string.ascii_letters
4328         clean_value = "".join(x for x in key_name if x in safe_chars)
4329         if clean_value != key_name:
4330             raise exception.InvalidKeypair(
4331                 reason=_("Keypair name contains unsafe characters"))
4332 
4333         try:
4334             utils.check_string_length(key_name, min_length=1, max_length=255)
4335         except exception.InvalidInput:
4336             raise exception.InvalidKeypair(
4337                 reason=_('Keypair name must be string and between '
4338                          '1 and 255 characters long'))
4339         try:
4340             objects.Quotas.check_deltas(context, 'key_pairs', {'key_pairs': 1},
4341                                         user_id)
4342         except exception.OverQuota:
4343             raise exception.KeypairLimitExceeded()
4344 
4345     @wrap_exception()
4346     def import_key_pair(self, context, user_id, key_name, public_key,
4347                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
4348         """Import a key pair using an existing public key."""
4349         self._validate_new_key_pair(context, user_id, key_name, key_type)
4350 
4351         self._notify(context, 'import.start', key_name)
4352 
4353         fingerprint = self._generate_fingerprint(public_key, key_type)
4354 
4355         keypair = objects.KeyPair(context)
4356         keypair.user_id = user_id
4357         keypair.name = key_name
4358         keypair.type = key_type
4359         keypair.fingerprint = fingerprint
4360         keypair.public_key = public_key
4361         keypair.create()
4362 
4363         self._notify(context, 'import.end', key_name)
4364 
4365         return keypair
4366 
4367     @wrap_exception()
4368     def create_key_pair(self, context, user_id, key_name,
4369                         key_type=keypair_obj.KEYPAIR_TYPE_SSH):
4370         """Create a new key pair."""
4371         self._validate_new_key_pair(context, user_id, key_name, key_type)
4372 
4373         self._notify(context, 'create.start', key_name)
4374 
4375         private_key, public_key, fingerprint = self._generate_key_pair(
4376             user_id, key_type)
4377 
4378         keypair = objects.KeyPair(context)
4379         keypair.user_id = user_id
4380         keypair.name = key_name
4381         keypair.type = key_type
4382         keypair.fingerprint = fingerprint
4383         keypair.public_key = public_key
4384         keypair.create()
4385         # NOTE(melwitt): We recheck the quota after creating the
4386         # object to handle races. Delete the object if we're over
4387         # quota after creating the object.
4388         try:
4389             objects.Quotas.check_deltas(context, 'key_pairs', {'key_pairs': 0},
4390                                         user_id)
4391         except exception.OverQuota:
4392             keypair.destroy()
4393             raise exception.KeypairLimitExceeded()
4394 
4395         self._notify(context, 'create.end', key_name)
4396 
4397         return keypair, private_key
4398 
4399     def _generate_fingerprint(self, public_key, key_type):
4400         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4401             return crypto.generate_fingerprint(public_key)
4402         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4403             return crypto.generate_x509_fingerprint(public_key)
4404 
4405     def _generate_key_pair(self, user_id, key_type):
4406         if key_type == keypair_obj.KEYPAIR_TYPE_SSH:
4407             return crypto.generate_key_pair()
4408         elif key_type == keypair_obj.KEYPAIR_TYPE_X509:
4409             return crypto.generate_winrm_x509_cert(user_id)
4410 
4411     @wrap_exception()
4412     def delete_key_pair(self, context, user_id, key_name):
4413         """Delete a keypair by name."""
4414         self._notify(context, 'delete.start', key_name)
4415         objects.KeyPair.destroy_by_name(context, user_id, key_name)
4416         self._notify(context, 'delete.end', key_name)
4417 
4418     def get_key_pairs(self, context, user_id, limit=None, marker=None):
4419         """List key pairs."""
4420         return objects.KeyPairList.get_by_user(
4421             context, user_id, limit=limit, marker=marker)
4422 
4423     def get_key_pair(self, context, user_id, key_name):
4424         """Get a keypair by name."""
4425         return objects.KeyPair.get_by_name(context, user_id, key_name)
4426 
4427 
4428 class SecurityGroupAPI(base.Base, security_group_base.SecurityGroupBase):
4429     """Sub-set of the Compute API related to managing security groups
4430     and security group rules
4431     """
4432 
4433     # The nova security group api does not use a uuid for the id.
4434     id_is_uuid = False
4435 
4436     def __init__(self, **kwargs):
4437         super(SecurityGroupAPI, self).__init__(**kwargs)
4438         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
4439 
4440     def validate_property(self, value, property, allowed):
4441         """Validate given security group property.
4442 
4443         :param value:          the value to validate, as a string or unicode
4444         :param property:       the property, either 'name' or 'description'
4445         :param allowed:        the range of characters allowed
4446         """
4447 
4448         try:
4449             val = value.strip()
4450         except AttributeError:
4451             msg = _("Security group %s is not a string or unicode") % property
4452             self.raise_invalid_property(msg)
4453         utils.check_string_length(val, name=property, min_length=1,
4454                                   max_length=255)
4455 
4456         if allowed and not re.match(allowed, val):
4457             # Some validation to ensure that values match API spec.
4458             # - Alphanumeric characters, spaces, dashes, and underscores.
4459             # TODO(Daviey): LP: #813685 extend beyond group_name checking, and
4460             #  probably create a param validator that can be used elsewhere.
4461             msg = (_("Value (%(value)s) for parameter Group%(property)s is "
4462                      "invalid. Content limited to '%(allowed)s'.") %
4463                    {'value': value, 'allowed': allowed,
4464                     'property': property.capitalize()})
4465             self.raise_invalid_property(msg)
4466 
4467     def ensure_default(self, context):
4468         """Ensure that a context has a security group.
4469 
4470         Creates a security group for the security context if it does not
4471         already exist.
4472 
4473         :param context: the security context
4474         """
4475         self.db.security_group_ensure_default(context)
4476 
4477     def create_security_group(self, context, name, description):
4478         try:
4479             objects.Quotas.check_deltas(context, 'security_groups',
4480                                         {'security_groups': 1},
4481                                         context.project_id,
4482                                         user_id=context.user_id)
4483         except exception.OverQuota:
4484             msg = _("Quota exceeded, too many security groups.")
4485             self.raise_over_quota(msg)
4486 
4487         LOG.info(_LI("Create Security Group %s"), name)
4488 
4489         self.ensure_default(context)
4490 
4491         group = {'user_id': context.user_id,
4492                  'project_id': context.project_id,
4493                  'name': name,
4494                  'description': description}
4495         try:
4496             group_ref = self.db.security_group_create(context, group)
4497         except exception.SecurityGroupExists:
4498             msg = _('Security group %s already exists') % name
4499             self.raise_group_already_exists(msg)
4500 
4501         # NOTE(melwitt): We recheck the quota after creating the
4502         # object to handle races. Delete the object if we're over
4503         # quota after creating the object.
4504         try:
4505             objects.Quotas.check_deltas(context, 'security_groups',
4506                                         {'security_groups': 0},
4507                                         context.project_id,
4508                                         user_id=context.user_id)
4509         except exception.OverQuota:
4510             self.db.security_group_destroy(context, group_ref['id'])
4511             msg = _("Quota exceeded, too many security groups.")
4512             self.raise_over_quota(msg)
4513 
4514         return group_ref
4515 
4516     def update_security_group(self, context, security_group,
4517                                 name, description):
4518         if security_group['name'] in RO_SECURITY_GROUPS:
4519             msg = (_("Unable to update system group '%s'") %
4520                     security_group['name'])
4521             self.raise_invalid_group(msg)
4522 
4523         group = {'name': name,
4524                  'description': description}
4525 
4526         columns_to_join = ['rules.grantee_group']
4527         group_ref = self.db.security_group_update(context,
4528                 security_group['id'],
4529                 group,
4530                 columns_to_join=columns_to_join)
4531         return group_ref
4532 
4533     def get(self, context, name=None, id=None, map_exception=False):
4534         self.ensure_default(context)
4535         cols = ['rules']
4536         try:
4537             if name:
4538                 return self.db.security_group_get_by_name(context,
4539                                                           context.project_id,
4540                                                           name,
4541                                                           columns_to_join=cols)
4542             elif id:
4543                 return self.db.security_group_get(context, id,
4544                                                   columns_to_join=cols)
4545         except exception.NotFound as exp:
4546             if map_exception:
4547                 msg = exp.format_message()
4548                 self.raise_not_found(msg)
4549             else:
4550                 raise
4551 
4552     def list(self, context, names=None, ids=None, project=None,
4553              search_opts=None):
4554         self.ensure_default(context)
4555 
4556         groups = []
4557         if names or ids:
4558             if names:
4559                 for name in names:
4560                     groups.append(self.db.security_group_get_by_name(context,
4561                                                                      project,
4562                                                                      name))
4563             if ids:
4564                 for id in ids:
4565                     groups.append(self.db.security_group_get(context, id))
4566 
4567         elif context.is_admin:
4568             # TODO(eglynn): support a wider set of search options than just
4569             # all_tenants, at least include the standard filters defined for
4570             # the EC2 DescribeSecurityGroups API for the non-admin case also
4571             if (search_opts and 'all_tenants' in search_opts):
4572                 groups = self.db.security_group_get_all(context)
4573             else:
4574                 groups = self.db.security_group_get_by_project(context,
4575                                                                project)
4576 
4577         elif project:
4578             groups = self.db.security_group_get_by_project(context, project)
4579 
4580         return groups
4581 
4582     def destroy(self, context, security_group):
4583         if security_group['name'] in RO_SECURITY_GROUPS:
4584             msg = _("Unable to delete system group '%s'") % \
4585                     security_group['name']
4586             self.raise_invalid_group(msg)
4587 
4588         if self.db.security_group_in_use(context, security_group['id']):
4589             msg = _("Security group is still in use")
4590             self.raise_invalid_group(msg)
4591 
4592         LOG.info(_LI("Delete security group %s"), security_group['name'])
4593         self.db.security_group_destroy(context, security_group['id'])
4594 
4595     def is_associated_with_server(self, security_group, instance_uuid):
4596         """Check if the security group is already associated
4597            with the instance. If Yes, return True.
4598         """
4599 
4600         if not security_group:
4601             return False
4602 
4603         instances = security_group.get('instances')
4604         if not instances:
4605             return False
4606 
4607         for inst in instances:
4608             if (instance_uuid == inst['uuid']):
4609                 return True
4610 
4611         return False
4612 
4613     def add_to_instance(self, context, instance, security_group_name):
4614         """Add security group to the instance."""
4615         security_group = self.db.security_group_get_by_name(context,
4616                 context.project_id,
4617                 security_group_name)
4618 
4619         instance_uuid = instance.uuid
4620 
4621         # check if the security group is associated with the server
4622         if self.is_associated_with_server(security_group, instance_uuid):
4623             raise exception.SecurityGroupExistsForInstance(
4624                                         security_group_id=security_group['id'],
4625                                         instance_id=instance_uuid)
4626 
4627         self.db.instance_add_security_group(context.elevated(),
4628                                             instance_uuid,
4629                                             security_group['id'])
4630         if instance.host:
4631             self.compute_rpcapi.refresh_instance_security_rules(
4632                     context, instance, instance.host)
4633 
4634     def remove_from_instance(self, context, instance, security_group_name):
4635         """Remove the security group associated with the instance."""
4636         security_group = self.db.security_group_get_by_name(context,
4637                 context.project_id,
4638                 security_group_name)
4639 
4640         instance_uuid = instance.uuid
4641 
4642         # check if the security group is associated with the server
4643         if not self.is_associated_with_server(security_group, instance_uuid):
4644             raise exception.SecurityGroupNotExistsForInstance(
4645                                     security_group_id=security_group['id'],
4646                                     instance_id=instance_uuid)
4647 
4648         self.db.instance_remove_security_group(context.elevated(),
4649                                                instance_uuid,
4650                                                security_group['id'])
4651         if instance.host:
4652             self.compute_rpcapi.refresh_instance_security_rules(
4653                     context, instance, instance.host)
4654 
4655     def get_rule(self, context, id):
4656         self.ensure_default(context)
4657         try:
4658             return self.db.security_group_rule_get(context, id)
4659         except exception.NotFound:
4660             msg = _("Rule (%s) not found") % id
4661             self.raise_not_found(msg)
4662 
4663     def add_rules(self, context, id, name, vals):
4664         """Add security group rule(s) to security group.
4665 
4666         Note: the Nova security group API doesn't support adding multiple
4667         security group rules at once but the EC2 one does. Therefore,
4668         this function is written to support both.
4669         """
4670 
4671         try:
4672             objects.Quotas.check_deltas(context, 'security_group_rules',
4673                                         {'security_group_rules': len(vals)},
4674                                         id)
4675         except exception.OverQuota:
4676             msg = _("Quota exceeded, too many security group rules.")
4677             self.raise_over_quota(msg)
4678 
4679         msg = _LI("Security group %(name)s added %(protocol)s ingress "
4680                   "(%(from_port)s:%(to_port)s)")
4681         rules = []
4682         for v in vals:
4683             rule = self.db.security_group_rule_create(context, v)
4684 
4685             # NOTE(melwitt): We recheck the quota after creating the
4686             # object to handle races. Delete the object if we're over
4687             # quota after creating the object.
4688             try:
4689                 objects.Quotas.check_deltas(context, 'security_group_rules',
4690                                             {'security_group_rules': 0}, id)
4691             except exception.OverQuota:
4692                 self.db.security_group_rule_destroy(context, rule['id'])
4693                 msg = _("Quota exceeded, too many security group rules.")
4694                 self.raise_over_quota(msg)
4695 
4696             rules.append(rule)
4697             LOG.info(msg, {'name': name,
4698                            'protocol': rule.protocol,
4699                            'from_port': rule.from_port,
4700                            'to_port': rule.to_port})
4701 
4702         self.trigger_rules_refresh(context, id=id)
4703         return rules
4704 
4705     def remove_rules(self, context, security_group, rule_ids):
4706         msg = _LI("Security group %(name)s removed %(protocol)s ingress "
4707                   "(%(from_port)s:%(to_port)s)")
4708         for rule_id in rule_ids:
4709             rule = self.get_rule(context, rule_id)
4710             LOG.info(msg, {'name': security_group['name'],
4711                            'protocol': rule.protocol,
4712                            'from_port': rule.from_port,
4713                            'to_port': rule.to_port})
4714 
4715             self.db.security_group_rule_destroy(context, rule_id)
4716 
4717         # NOTE(vish): we removed some rules, so refresh
4718         self.trigger_rules_refresh(context, id=security_group['id'])
4719 
4720     def remove_default_rules(self, context, rule_ids):
4721         for rule_id in rule_ids:
4722             self.db.security_group_default_rule_destroy(context, rule_id)
4723 
4724     def add_default_rules(self, context, vals):
4725         rules = [self.db.security_group_default_rule_create(context, v)
4726                  for v in vals]
4727         return rules
4728 
4729     def default_rule_exists(self, context, values):
4730         """Indicates whether the specified rule values are already
4731            defined in the default security group rules.
4732         """
4733         for rule in self.db.security_group_default_rule_list(context):
4734             keys = ('cidr', 'from_port', 'to_port', 'protocol')
4735             for key in keys:
4736                 if rule.get(key) != values.get(key):
4737                     break
4738             else:
4739                 return rule.get('id') or True
4740         return False
4741 
4742     def get_all_default_rules(self, context):
4743         try:
4744             rules = self.db.security_group_default_rule_list(context)
4745         except Exception:
4746             msg = 'cannot get default security group rules'
4747             raise exception.SecurityGroupDefaultRuleNotFound(msg)
4748 
4749         return rules
4750 
4751     def get_default_rule(self, context, id):
4752         return self.db.security_group_default_rule_get(context, id)
4753 
4754     def validate_id(self, id):
4755         try:
4756             return int(id)
4757         except ValueError:
4758             msg = _("Security group id should be integer")
4759             self.raise_invalid_property(msg)
4760 
4761     def _refresh_instance_security_rules(self, context, instances):
4762         for instance in instances:
4763             if instance.host is not None:
4764                 self.compute_rpcapi.refresh_instance_security_rules(
4765                         context, instance, instance.host)
4766 
4767     def trigger_rules_refresh(self, context, id):
4768         """Called when a rule is added to or removed from a security_group."""
4769         instances = objects.InstanceList.get_by_security_group_id(context, id)
4770         self._refresh_instance_security_rules(context, instances)
4771 
4772     def trigger_members_refresh(self, context, group_ids):
4773         """Called when a security group gains a new or loses a member.
4774 
4775         Sends an update request to each compute node for each instance for
4776         which this is relevant.
4777         """
4778         instances = objects.InstanceList.get_by_grantee_security_group_ids(
4779             context, group_ids)
4780         self._refresh_instance_security_rules(context, instances)
4781 
4782     def get_instance_security_groups(self, context, instance, detailed=False):
4783         if detailed:
4784             return self.db.security_group_get_by_instance(context,
4785                                                           instance.uuid)
4786         return [{'name': group.name} for group in instance.security_groups]
