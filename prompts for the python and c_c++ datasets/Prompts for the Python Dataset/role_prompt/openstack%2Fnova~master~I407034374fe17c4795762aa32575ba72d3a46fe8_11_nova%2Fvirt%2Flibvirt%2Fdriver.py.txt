I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # All Rights Reserved.
4 # Copyright (c) 2010 Citrix Systems, Inc.
5 # Copyright (c) 2011 Piston Cloud Computing, Inc
6 # Copyright (c) 2012 University Of Minho
7 # (c) Copyright 2013 Hewlett-Packard Development Company, L.P.
8 #
9 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
10 #    not use this file except in compliance with the License. You may obtain
11 #    a copy of the License at
12 #
13 #         http://www.apache.org/licenses/LICENSE-2.0
14 #
15 #    Unless required by applicable law or agreed to in writing, software
16 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
17 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
18 #    License for the specific language governing permissions and limitations
19 #    under the License.
20 
21 """
22 A connection to a hypervisor through libvirt.
23 
24 Supports KVM, LXC, QEMU, UML, XEN and Parallels.
25 
26 """
27 
28 import collections
29 from collections import deque
30 import contextlib
31 import errno
32 import functools
33 import glob
34 import itertools
35 import mmap
36 import operator
37 import os
38 import pwd
39 import shutil
40 import tempfile
41 import time
42 import uuid
43 
44 from castellan import key_manager
45 import eventlet
46 from eventlet import greenthread
47 from eventlet import tpool
48 from lxml import etree
49 from os_brick import encryptors
50 from os_brick import exception as brick_exception
51 from os_brick.initiator import connector
52 from oslo_concurrency import processutils
53 from oslo_log import log as logging
54 from oslo_serialization import jsonutils
55 from oslo_service import loopingcall
56 from oslo_utils import excutils
57 from oslo_utils import fileutils
58 from oslo_utils import importutils
59 from oslo_utils import strutils
60 from oslo_utils import timeutils
61 from oslo_utils import units
62 from oslo_utils import uuidutils
63 import six
64 from six.moves import range
65 
66 from nova.api.metadata import base as instance_metadata
67 from nova import block_device
68 from nova.compute import power_state
69 from nova.compute import task_states
70 from nova.compute import utils as compute_utils
71 import nova.conf
72 from nova.console import serial as serial_console
73 from nova.console import type as ctype
74 from nova import context as nova_context
75 from nova import exception
76 from nova.i18n import _
77 from nova import image
78 from nova.network import model as network_model
79 from nova import objects
80 from nova.objects import diagnostics as diagnostics_obj
81 from nova.objects import fields
82 from nova.objects import migrate_data as migrate_data_obj
83 from nova.pci import manager as pci_manager
84 from nova.pci import utils as pci_utils
85 import nova.privsep.libvirt
86 import nova.privsep.path
87 from nova import utils
88 from nova import version
89 from nova.virt import block_device as driver_block_device
90 from nova.virt import configdrive
91 from nova.virt.disk import api as disk_api
92 from nova.virt.disk.vfs import guestfs
93 from nova.virt import driver
94 from nova.virt import firewall
95 from nova.virt import hardware
96 from nova.virt.image import model as imgmodel
97 from nova.virt import images
98 from nova.virt.libvirt import blockinfo
99 from nova.virt.libvirt import config as vconfig
100 from nova.virt.libvirt import firewall as libvirt_firewall
101 from nova.virt.libvirt import guest as libvirt_guest
102 from nova.virt.libvirt import host
103 from nova.virt.libvirt import imagebackend
104 from nova.virt.libvirt import imagecache
105 from nova.virt.libvirt import instancejobtracker
106 from nova.virt.libvirt import migration as libvirt_migrate
107 from nova.virt.libvirt.storage import dmcrypt
108 from nova.virt.libvirt.storage import lvm
109 from nova.virt.libvirt.storage import rbd_utils
110 from nova.virt.libvirt import utils as libvirt_utils
111 from nova.virt.libvirt import vif as libvirt_vif
112 from nova.virt.libvirt.volume import mount
113 from nova.virt.libvirt.volume import remotefs
114 from nova.virt import netutils
115 from nova.volume import cinder
116 
117 libvirt = None
118 
119 uefi_logged = False
120 
121 LOG = logging.getLogger(__name__)
122 
123 CONF = nova.conf.CONF
124 
125 DEFAULT_FIREWALL_DRIVER = "%s.%s" % (
126     libvirt_firewall.__name__,
127     libvirt_firewall.IptablesFirewallDriver.__name__)
128 
129 DEFAULT_UEFI_LOADER_PATH = {
130     "x86_64": "/usr/share/OVMF/OVMF_CODE.fd",
131     "aarch64": "/usr/share/AAVMF/AAVMF_CODE.fd"
132 }
133 
134 MAX_CONSOLE_BYTES = 100 * units.Ki
135 
136 # The libvirt driver will prefix any disable reason codes with this string.
137 DISABLE_PREFIX = 'AUTO: '
138 # Disable reason for the service which was enabled or disabled without reason
139 DISABLE_REASON_UNDEFINED = None
140 
141 # Guest config console string
142 CONSOLE = "console=tty0 console=ttyS0 console=hvc0"
143 
144 GuestNumaConfig = collections.namedtuple(
145     'GuestNumaConfig', ['cpuset', 'cputune', 'numaconfig', 'numatune'])
146 
147 InjectionInfo = collections.namedtuple(
148     'InjectionInfo', ['network_info', 'files', 'admin_pass'])
149 
150 libvirt_volume_drivers = [
151     'iscsi=nova.virt.libvirt.volume.iscsi.LibvirtISCSIVolumeDriver',
152     'iser=nova.virt.libvirt.volume.iser.LibvirtISERVolumeDriver',
153     'local=nova.virt.libvirt.volume.volume.LibvirtVolumeDriver',
154     'drbd=nova.virt.libvirt.volume.drbd.LibvirtDRBDVolumeDriver',
155     'fake=nova.virt.libvirt.volume.volume.LibvirtFakeVolumeDriver',
156     'rbd=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
157     'sheepdog=nova.virt.libvirt.volume.net.LibvirtNetVolumeDriver',
158     'nfs=nova.virt.libvirt.volume.nfs.LibvirtNFSVolumeDriver',
159     'smbfs=nova.virt.libvirt.volume.smbfs.LibvirtSMBFSVolumeDriver',
160     'aoe=nova.virt.libvirt.volume.aoe.LibvirtAOEVolumeDriver',
161     'fibre_channel='
162         'nova.virt.libvirt.volume.fibrechannel.'
163         'LibvirtFibreChannelVolumeDriver',
164     'gpfs=nova.virt.libvirt.volume.gpfs.LibvirtGPFSVolumeDriver',
165     'quobyte=nova.virt.libvirt.volume.quobyte.LibvirtQuobyteVolumeDriver',
166     'hgst=nova.virt.libvirt.volume.hgst.LibvirtHGSTVolumeDriver',
167     'scaleio=nova.virt.libvirt.volume.scaleio.LibvirtScaleIOVolumeDriver',
168     'disco=nova.virt.libvirt.volume.disco.LibvirtDISCOVolumeDriver',
169     'vzstorage='
170         'nova.virt.libvirt.volume.vzstorage.LibvirtVZStorageVolumeDriver',
171     'veritas_hyperscale='
172         'nova.virt.libvirt.volume.vrtshyperscale.'
173         'LibvirtHyperScaleVolumeDriver',
174 ]
175 
176 
177 def patch_tpool_proxy():
178     """eventlet.tpool.Proxy doesn't work with old-style class in __str__()
179     or __repr__() calls. See bug #962840 for details.
180     We perform a monkey patch to replace those two instance methods.
181     """
182     def str_method(self):
183         return str(self._obj)
184 
185     def repr_method(self):
186         return repr(self._obj)
187 
188     tpool.Proxy.__str__ = str_method
189     tpool.Proxy.__repr__ = repr_method
190 
191 
192 patch_tpool_proxy()
193 
194 # For information about when MIN_LIBVIRT_VERSION and
195 # NEXT_MIN_LIBVIRT_VERSION can be changed, consult
196 #
197 #   https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
198 #
199 # Currently this is effectively the min version for i686/x86_64
200 # + KVM/QEMU, as other architectures/hypervisors require newer
201 # versions. Over time, this will become a common min version
202 # for all architectures/hypervisors, as this value rises to
203 # meet them.
204 MIN_LIBVIRT_VERSION = (1, 2, 9)
205 MIN_QEMU_VERSION = (2, 1, 0)
206 # TODO(berrange): Re-evaluate this at start of each release cycle
207 # to decide if we want to plan a future min version bump.
208 # MIN_LIBVIRT_VERSION can be updated to match this after
209 # NEXT_MIN_LIBVIRT_VERSION  has been at a higher value for
210 # one cycle
211 NEXT_MIN_LIBVIRT_VERSION = (1, 3, 1)
212 NEXT_MIN_QEMU_VERSION = (2, 5, 0)
213 
214 # When the above version matches/exceeds this version
215 # delete it & corresponding code using it
216 # Libvirt version 1.2.17 is required for successful block live migration
217 # of vm booted from image with attached devices
218 MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION = (1, 2, 17)
219 # PowerPC based hosts that support NUMA using libvirt
220 MIN_LIBVIRT_NUMA_VERSION_PPC = (1, 2, 19)
221 # Versions of libvirt with known NUMA topology issues
222 # See bug #1449028
223 BAD_LIBVIRT_NUMA_VERSIONS = [(1, 2, 9, 2)]
224 # Versions of libvirt with broken cpu pinning support. This excludes
225 # versions of libvirt with broken NUMA support since pinning needs
226 # NUMA
227 # See bug #1438226
228 BAD_LIBVIRT_CPU_POLICY_VERSIONS = [(1, 2, 10)]
229 
230 # Virtuozzo driver support
231 MIN_VIRTUOZZO_VERSION = (7, 0, 0)
232 MIN_LIBVIRT_VIRTUOZZO_VERSION = (1, 2, 12)
233 
234 # Ability to set the user guest password with Qemu
235 MIN_LIBVIRT_SET_ADMIN_PASSWD = (1, 2, 16)
236 
237 # Ability to set the user guest password with parallels
238 MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD = (2, 0, 0)
239 
240 # s/390 & s/390x architectures with KVM
241 MIN_LIBVIRT_KVM_S390_VERSION = (1, 2, 13)
242 MIN_QEMU_S390_VERSION = (2, 3, 0)
243 
244 # libvirt < 1.3 reported virt_functions capability
245 # only when VFs are enabled.
246 # libvirt 1.3 fix f391889f4e942e22b9ef8ecca492de05106ce41e
247 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION = (1, 3, 0)
248 
249 # Use the "logd" backend for handling stdout/stderr from QEMU processes.
250 MIN_LIBVIRT_VIRTLOGD = (1, 3, 3)
251 MIN_QEMU_VIRTLOGD = (2, 7, 0)
252 
253 # ppc64/ppc64le architectures with KVM
254 # NOTE(rfolco): Same levels for Libvirt/Qemu on Big Endian and Little
255 # Endian giving the nuance around guest vs host architectures
256 MIN_LIBVIRT_KVM_PPC64_VERSION = (1, 2, 12)
257 
258 # Names of the types that do not get compressed during migration
259 NO_COMPRESSION_TYPES = ('qcow2',)
260 
261 
262 # number of serial console limit
263 QEMU_MAX_SERIAL_PORTS = 4
264 # Qemu supports 4 serial consoles, we remove 1 because of the PTY one defined
265 ALLOWED_QEMU_SERIAL_PORTS = QEMU_MAX_SERIAL_PORTS - 1
266 
267 # realtime support
268 MIN_LIBVIRT_REALTIME_VERSION = (1, 2, 13)
269 
270 # libvirt postcopy support
271 MIN_LIBVIRT_POSTCOPY_VERSION = (1, 3, 3)
272 
273 # qemu postcopy support
274 MIN_QEMU_POSTCOPY_VERSION = (2, 5, 0)
275 
276 MIN_LIBVIRT_OTHER_ARCH = {
277     fields.Architecture.S390: MIN_LIBVIRT_KVM_S390_VERSION,
278     fields.Architecture.S390X: MIN_LIBVIRT_KVM_S390_VERSION,
279     fields.Architecture.PPC: MIN_LIBVIRT_KVM_PPC64_VERSION,
280     fields.Architecture.PPC64: MIN_LIBVIRT_KVM_PPC64_VERSION,
281     fields.Architecture.PPC64LE: MIN_LIBVIRT_KVM_PPC64_VERSION,
282 }
283 
284 MIN_QEMU_OTHER_ARCH = {
285     fields.Architecture.S390: MIN_QEMU_S390_VERSION,
286     fields.Architecture.S390X: MIN_QEMU_S390_VERSION,
287 }
288 
289 # perf events support
290 MIN_LIBVIRT_PERF_VERSION = (2, 0, 0)
291 LIBVIRT_PERF_EVENT_PREFIX = 'VIR_PERF_PARAM_'
292 
293 PERF_EVENTS_CPU_FLAG_MAPPING = {'cmt': 'cmt',
294                                 'mbml': 'mbm_local',
295                                 'mbmt': 'mbm_total',
296                                }
297 
298 
299 class LibvirtDriver(driver.ComputeDriver):
300     capabilities = {
301         "has_imagecache": True,
302         "supports_recreate": True,
303         "supports_migrate_to_same_host": False,
304         "supports_attach_interface": True,
305         "supports_device_tagging": True,
306         "supports_tagged_attach_interface": True,
307         "supports_tagged_attach_volume": True,
308         "supports_extend_volume": True,
309     }
310 
311     def __init__(self, virtapi, read_only=False):
312         super(LibvirtDriver, self).__init__(virtapi)
313 
314         global libvirt
315         if libvirt is None:
316             libvirt = importutils.import_module('libvirt')
317             libvirt_migrate.libvirt = libvirt
318 
319         self._host = host.Host(self._uri(), read_only,
320                                lifecycle_event_handler=self.emit_event,
321                                conn_event_handler=self._handle_conn_event)
322         self._initiator = None
323         self._fc_wwnns = None
324         self._fc_wwpns = None
325         self._caps = None
326         self._supported_perf_events = []
327         self.firewall_driver = firewall.load_driver(
328             DEFAULT_FIREWALL_DRIVER,
329             host=self._host)
330 
331         self.vif_driver = libvirt_vif.LibvirtGenericVIFDriver()
332 
333         # TODO(mriedem): Long-term we should load up the volume drivers on
334         # demand as needed rather than doing this on startup, as there might
335         # be unsupported volume drivers in this list based on the underlying
336         # platform.
337         self.volume_drivers = self._get_volume_drivers()
338 
339         self._disk_cachemode = None
340         self.image_cache_manager = imagecache.ImageCacheManager()
341         self.image_backend = imagebackend.Backend(CONF.use_cow_images)
342 
343         self.disk_cachemodes = {}
344 
345         self.valid_cachemodes = ["default",
346                                  "none",
347                                  "writethrough",
348                                  "writeback",
349                                  "directsync",
350                                  "unsafe",
351                                 ]
352         self._conn_supports_start_paused = CONF.libvirt.virt_type in ('kvm',
353                                                                       'qemu')
354 
355         for mode_str in CONF.libvirt.disk_cachemodes:
356             disk_type, sep, cache_mode = mode_str.partition('=')
357             if cache_mode not in self.valid_cachemodes:
358                 LOG.warning('Invalid cachemode %(cache_mode)s specified '
359                             'for disk type %(disk_type)s.',
360                             {'cache_mode': cache_mode, 'disk_type': disk_type})
361                 continue
362             self.disk_cachemodes[disk_type] = cache_mode
363 
364         self._volume_api = cinder.API()
365         self._image_api = image.API()
366 
367         sysinfo_serial_funcs = {
368             'none': lambda: None,
369             'hardware': self._get_host_sysinfo_serial_hardware,
370             'os': self._get_host_sysinfo_serial_os,
371             'auto': self._get_host_sysinfo_serial_auto,
372         }
373 
374         self._sysinfo_serial_func = sysinfo_serial_funcs.get(
375             CONF.libvirt.sysinfo_serial)
376 
377         self.job_tracker = instancejobtracker.InstanceJobTracker()
378         self._remotefs = remotefs.RemoteFilesystem()
379 
380         self._live_migration_flags = self._block_migration_flags = 0
381         self.active_migrations = {}
382 
383         # Compute reserved hugepages from conf file at the very
384         # beginning to ensure any syntax error will be reported and
385         # avoid any re-calculation when computing resources.
386         self._reserved_hugepages = hardware.numa_get_reserved_huge_pages()
387 
388     def _get_volume_drivers(self):
389         driver_registry = dict()
390 
391         for driver_str in libvirt_volume_drivers:
392             driver_type, _sep, driver = driver_str.partition('=')
393             driver_class = importutils.import_class(driver)
394             try:
395                 driver_registry[driver_type] = driver_class(self._host)
396             except brick_exception.InvalidConnectorProtocol:
397                 LOG.debug('Unable to load volume driver %s. It is not '
398                           'supported on this host.', driver)
399 
400         return driver_registry
401 
402     @property
403     def disk_cachemode(self):
404         if self._disk_cachemode is None:
405             # We prefer 'none' for consistent performance, host crash
406             # safety & migration correctness by avoiding host page cache.
407             # Some filesystems don't support O_DIRECT though. For those we
408             # fallback to 'writethrough' which gives host crash safety, and
409             # is safe for migration provided the filesystem is cache coherent
410             # (cluster filesystems typically are, but things like NFS are not).
411             self._disk_cachemode = "none"
412             if not self._supports_direct_io(CONF.instances_path):
413                 self._disk_cachemode = "writethrough"
414         return self._disk_cachemode
415 
416     def _set_cache_mode(self, conf):
417         """Set cache mode on LibvirtConfigGuestDisk object."""
418         try:
419             source_type = conf.source_type
420             driver_cache = conf.driver_cache
421         except AttributeError:
422             return
423 
424         cache_mode = self.disk_cachemodes.get(source_type,
425                                               driver_cache)
426         conf.driver_cache = cache_mode
427 
428     def _do_quality_warnings(self):
429         """Warn about untested driver configurations.
430 
431         This will log a warning message about untested driver or host arch
432         configurations to indicate to administrators that the quality is
433         unknown. Currently, only qemu or kvm on intel 32- or 64-bit systems
434         is tested upstream.
435         """
436         caps = self._host.get_capabilities()
437         hostarch = caps.host.cpu.arch
438         if (CONF.libvirt.virt_type not in ('qemu', 'kvm') or
439             hostarch not in (fields.Architecture.I686,
440                              fields.Architecture.X86_64)):
441             LOG.warning('The libvirt driver is not tested on '
442                         '%(type)s/%(arch)s by the OpenStack project and '
443                         'thus its quality can not be ensured. For more '
444                         'information, see: https://docs.openstack.org/'
445                         'nova/latest/user/support-matrix.html',
446                         {'type': CONF.libvirt.virt_type, 'arch': hostarch})
447 
448     def _handle_conn_event(self, enabled, reason):
449         LOG.info("Connection event '%(enabled)d' reason '%(reason)s'",
450                  {'enabled': enabled, 'reason': reason})
451         self._set_host_enabled(enabled, reason)
452 
453     def _version_to_string(self, version):
454         return '.'.join([str(x) for x in version])
455 
456     def init_host(self, host):
457         self._host.initialize()
458 
459         self._do_quality_warnings()
460 
461         self._parse_migration_flags()
462 
463         self._supported_perf_events = self._get_supported_perf_events()
464 
465         if (CONF.libvirt.virt_type == 'lxc' and
466                 not (CONF.libvirt.uid_maps and CONF.libvirt.gid_maps)):
467             LOG.warning("Running libvirt-lxc without user namespaces is "
468                         "dangerous. Containers spawned by Nova will be run "
469                         "as the host's root user. It is highly suggested "
470                         "that user namespaces be used in a public or "
471                         "multi-tenant environment.")
472 
473         # Stop libguestfs using KVM unless we're also configured
474         # to use this. This solves problem where people need to
475         # stop Nova use of KVM because nested-virt is broken
476         if CONF.libvirt.virt_type != "kvm":
477             guestfs.force_tcg()
478 
479         if not self._host.has_min_version(MIN_LIBVIRT_VERSION):
480             raise exception.InternalError(
481                 _('Nova requires libvirt version %s or greater.') %
482                 self._version_to_string(MIN_LIBVIRT_VERSION))
483 
484         if CONF.libvirt.virt_type in ("qemu", "kvm"):
485             if self._host.has_min_version(hv_ver=MIN_QEMU_VERSION):
486                 # "qemu-img info" calls are version dependent, so we need to
487                 # store the version in the images module.
488                 images.QEMU_VERSION = self._host.get_connection().getVersion()
489             else:
490                 raise exception.InternalError(
491                     _('Nova requires QEMU version %s or greater.') %
492                     self._version_to_string(MIN_QEMU_VERSION))
493 
494         if CONF.libvirt.virt_type == 'parallels':
495             if not self._host.has_min_version(hv_ver=MIN_VIRTUOZZO_VERSION):
496                 raise exception.InternalError(
497                     _('Nova requires Virtuozzo version %s or greater.') %
498                     self._version_to_string(MIN_VIRTUOZZO_VERSION))
499             if not self._host.has_min_version(MIN_LIBVIRT_VIRTUOZZO_VERSION):
500                 raise exception.InternalError(
501                     _('Running Nova with parallels virt_type requires '
502                       'libvirt version %s') %
503                     self._version_to_string(MIN_LIBVIRT_VIRTUOZZO_VERSION))
504 
505         # Give the cloud admin a heads up if we are intending to
506         # change the MIN_LIBVIRT_VERSION in the next release.
507         if not self._host.has_min_version(NEXT_MIN_LIBVIRT_VERSION):
508             LOG.warning('Running Nova with a libvirt version less than '
509                         '%(version)s is deprecated. The required minimum '
510                         'version of libvirt will be raised to %(version)s '
511                         'in the next release.',
512                         {'version': self._version_to_string(
513                             NEXT_MIN_LIBVIRT_VERSION)})
514         if (CONF.libvirt.virt_type in ("qemu", "kvm") and
515             not self._host.has_min_version(hv_ver=NEXT_MIN_QEMU_VERSION)):
516             LOG.warning('Running Nova with a QEMU version less than '
517                         '%(version)s is deprecated. The required minimum '
518                         'version of QEMU will be raised to %(version)s '
519                         'in the next release.',
520                         {'version': self._version_to_string(
521                             NEXT_MIN_QEMU_VERSION)})
522 
523         kvm_arch = fields.Architecture.from_host()
524         if (CONF.libvirt.virt_type in ('kvm', 'qemu') and
525             kvm_arch in MIN_LIBVIRT_OTHER_ARCH and
526                 not self._host.has_min_version(
527                                         MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch),
528                                         MIN_QEMU_OTHER_ARCH.get(kvm_arch))):
529             if MIN_QEMU_OTHER_ARCH.get(kvm_arch):
530                 raise exception.InternalError(
531                     _('Running Nova with qemu/kvm virt_type on %(arch)s '
532                       'requires libvirt version %(libvirt_ver)s and '
533                       'qemu version %(qemu_ver)s, or greater') %
534                     {'arch': kvm_arch,
535                      'libvirt_ver': self._version_to_string(
536                          MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch)),
537                      'qemu_ver': self._version_to_string(
538                          MIN_QEMU_OTHER_ARCH.get(kvm_arch))})
539             # no qemu version in the error message
540             raise exception.InternalError(
541                 _('Running Nova with qemu/kvm virt_type on %(arch)s '
542                   'requires libvirt version %(libvirt_ver)s or greater') %
543                 {'arch': kvm_arch,
544                  'libvirt_ver': self._version_to_string(
545                      MIN_LIBVIRT_OTHER_ARCH.get(kvm_arch))})
546 
547     def _prepare_migration_flags(self):
548         migration_flags = 0
549 
550         migration_flags |= libvirt.VIR_MIGRATE_LIVE
551 
552         # Adding p2p flag only if xen is not in use, because xen does not
553         # support p2p migrations
554         if CONF.libvirt.virt_type != 'xen':
555             migration_flags |= libvirt.VIR_MIGRATE_PEER2PEER
556 
557         # Adding VIR_MIGRATE_UNDEFINE_SOURCE because, without it, migrated
558         # instance will remain defined on the source host
559         migration_flags |= libvirt.VIR_MIGRATE_UNDEFINE_SOURCE
560 
561         # Adding VIR_MIGRATE_PERSIST_DEST to persist the VM on the
562         # destination host
563         migration_flags |= libvirt.VIR_MIGRATE_PERSIST_DEST
564 
565         live_migration_flags = block_migration_flags = migration_flags
566 
567         # Adding VIR_MIGRATE_NON_SHARED_INC, otherwise all block-migrations
568         # will be live-migrations instead
569         block_migration_flags |= libvirt.VIR_MIGRATE_NON_SHARED_INC
570 
571         return (live_migration_flags, block_migration_flags)
572 
573     def _handle_live_migration_tunnelled(self, migration_flags):
574         if (CONF.libvirt.live_migration_tunnelled is None or
575                 CONF.libvirt.live_migration_tunnelled):
576             migration_flags |= libvirt.VIR_MIGRATE_TUNNELLED
577         return migration_flags
578 
579     def _is_post_copy_available(self):
580         if self._host.has_min_version(lv_ver=MIN_LIBVIRT_POSTCOPY_VERSION,
581                                       hv_ver=MIN_QEMU_POSTCOPY_VERSION):
582             return True
583         return False
584 
585     def _is_virtlogd_available(self):
586         return self._host.has_min_version(MIN_LIBVIRT_VIRTLOGD,
587                                           MIN_QEMU_VIRTLOGD)
588 
589     def _handle_live_migration_post_copy(self, migration_flags):
590         if CONF.libvirt.live_migration_permit_post_copy:
591             if self._is_post_copy_available():
592                 migration_flags |= libvirt.VIR_MIGRATE_POSTCOPY
593             else:
594                 LOG.info('The live_migration_permit_post_copy is set '
595                          'to True, but it is not supported.')
596         return migration_flags
597 
598     def _handle_live_migration_auto_converge(self, migration_flags):
599         if (self._is_post_copy_available() and
600                 (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0):
601             LOG.info('The live_migration_permit_post_copy is set to '
602                      'True and post copy live migration is available '
603                      'so auto-converge will not be in use.')
604         elif CONF.libvirt.live_migration_permit_auto_converge:
605             migration_flags |= libvirt.VIR_MIGRATE_AUTO_CONVERGE
606         return migration_flags
607 
608     def _parse_migration_flags(self):
609         (live_migration_flags,
610             block_migration_flags) = self._prepare_migration_flags()
611 
612         live_migration_flags = self._handle_live_migration_tunnelled(
613             live_migration_flags)
614         block_migration_flags = self._handle_live_migration_tunnelled(
615             block_migration_flags)
616 
617         live_migration_flags = self._handle_live_migration_post_copy(
618             live_migration_flags)
619         block_migration_flags = self._handle_live_migration_post_copy(
620             block_migration_flags)
621 
622         live_migration_flags = self._handle_live_migration_auto_converge(
623             live_migration_flags)
624         block_migration_flags = self._handle_live_migration_auto_converge(
625             block_migration_flags)
626 
627         self._live_migration_flags = live_migration_flags
628         self._block_migration_flags = block_migration_flags
629 
630     # TODO(sahid): This method is targeted for removal when the tests
631     # have been updated to avoid its use
632     #
633     # All libvirt API calls on the libvirt.Connect object should be
634     # encapsulated by methods on the nova.virt.libvirt.host.Host
635     # object, rather than directly invoking the libvirt APIs. The goal
636     # is to avoid a direct dependency on the libvirt API from the
637     # driver.py file.
638     def _get_connection(self):
639         return self._host.get_connection()
640 
641     _conn = property(_get_connection)
642 
643     @staticmethod
644     def _uri():
645         if CONF.libvirt.virt_type == 'uml':
646             uri = CONF.libvirt.connection_uri or 'uml:///system'
647         elif CONF.libvirt.virt_type == 'xen':
648             uri = CONF.libvirt.connection_uri or 'xen:///'
649         elif CONF.libvirt.virt_type == 'lxc':
650             uri = CONF.libvirt.connection_uri or 'lxc:///'
651         elif CONF.libvirt.virt_type == 'parallels':
652             uri = CONF.libvirt.connection_uri or 'parallels:///system'
653         else:
654             uri = CONF.libvirt.connection_uri or 'qemu:///system'
655         return uri
656 
657     @staticmethod
658     def _live_migration_uri(dest):
659         uris = {
660             'kvm': 'qemu+%s://%s/system',
661             'qemu': 'qemu+%s://%s/system',
662             'xen': 'xenmigr://%s/system',
663             'parallels': 'parallels+tcp://%s/system',
664         }
665         virt_type = CONF.libvirt.virt_type
666         # TODO(pkoniszewski): Remove fetching live_migration_uri in Pike
667         uri = CONF.libvirt.live_migration_uri
668         if uri:
669             return uri % dest
670 
671         uri = uris.get(virt_type)
672         if uri is None:
673             raise exception.LiveMigrationURINotAvailable(virt_type=virt_type)
674 
675         str_format = (dest,)
676         if virt_type in ('kvm', 'qemu'):
677             scheme = CONF.libvirt.live_migration_scheme or 'tcp'
678             str_format = (scheme, dest)
679         return uris.get(virt_type) % str_format
680 
681     @staticmethod
682     def _migrate_uri(dest):
683         uri = None
684         # Only QEMU live migrations supports migrate-uri parameter
685         virt_type = CONF.libvirt.virt_type
686         if virt_type in ('qemu', 'kvm'):
687             # QEMU accept two schemes: tcp and rdma.  By default
688             # libvirt build the URI using the remote hostname and the
689             # tcp schema.
690             uri = 'tcp://%s' % dest
691         # Because dest might be of type unicode, here we might return value of
692         # type unicode as well which is not acceptable by libvirt python
693         # binding when Python 2.7 is in use, so let's convert it explicitly
694         # back to string. When Python 3.x is in use, libvirt python binding
695         # accepts unicode type so it is completely fine to do a no-op str(uri)
696         # conversion which will return value of type unicode.
697         return uri and str(uri)
698 
699     def instance_exists(self, instance):
700         """Efficient override of base instance_exists method."""
701         try:
702             self._host.get_guest(instance)
703             return True
704         except (exception.InternalError, exception.InstanceNotFound):
705             return False
706 
707     def estimate_instance_overhead(self, instance_info):
708         overhead = super(LibvirtDriver, self).estimate_instance_overhead(
709             instance_info)
710         if isinstance(instance_info, objects.Flavor):
711             # A flavor object is passed during case of migrate
712             # TODO(sahid): We do not have any way to retrieve the
713             # image meta related to the instance so if the cpu_policy
714             # has been set in image_meta we will get an
715             # exception. Until we fix it we specifically set the
716             # cpu_policy in dedicated in an ImageMeta object so if the
717             # emulator threads has been requested nothing is going to
718             # fail.
719             image_meta = objects.ImageMeta.from_dict({"properties": {
720                 "hw_cpu_policy": fields.CPUAllocationPolicy.DEDICATED,
721             }})
722             if (hardware.get_emulator_threads_constraint(
723                     instance_info, image_meta)
724                 == fields.CPUEmulatorThreadsPolicy.ISOLATE):
725                 overhead['vcpus'] += 1
726         else:
727             # An instance object is passed during case of spawing or a
728             # dict is passed when computing resource for an instance
729             numa_topology = hardware.instance_topology_from_instance(
730                 instance_info)
731             if numa_topology and numa_topology.emulator_threads_isolated:
732                 overhead['vcpus'] += 1
733         return overhead
734 
735     def list_instances(self):
736         names = []
737         for guest in self._host.list_guests(only_running=False):
738             names.append(guest.name)
739 
740         return names
741 
742     def list_instance_uuids(self):
743         uuids = []
744         for guest in self._host.list_guests(only_running=False):
745             uuids.append(guest.uuid)
746 
747         return uuids
748 
749     def plug_vifs(self, instance, network_info):
750         """Plug VIFs into networks."""
751         for vif in network_info:
752             self.vif_driver.plug(instance, vif)
753 
754     def _unplug_vifs(self, instance, network_info, ignore_errors):
755         """Unplug VIFs from networks."""
756         for vif in network_info:
757             try:
758                 self.vif_driver.unplug(instance, vif)
759             except exception.NovaException:
760                 if not ignore_errors:
761                     raise
762 
763     def unplug_vifs(self, instance, network_info):
764         self._unplug_vifs(instance, network_info, False)
765 
766     def _teardown_container(self, instance):
767         inst_path = libvirt_utils.get_instance_path(instance)
768         container_dir = os.path.join(inst_path, 'rootfs')
769         rootfs_dev = instance.system_metadata.get('rootfs_device_name')
770         LOG.debug('Attempting to teardown container at path %(dir)s with '
771                   'root device: %(rootfs_dev)s',
772                   {'dir': container_dir, 'rootfs_dev': rootfs_dev},
773                   instance=instance)
774         disk_api.teardown_container(container_dir, rootfs_dev)
775 
776     def _destroy(self, instance, attempt=1):
777         try:
778             guest = self._host.get_guest(instance)
779             if CONF.serial_console.enabled:
780                 # This method is called for several events: destroy,
781                 # rebuild, hard-reboot, power-off - For all of these
782                 # events we want to release the serial ports acquired
783                 # for the guest before destroying it.
784                 serials = self._get_serial_ports_from_guest(guest)
785                 for hostname, port in serials:
786                     serial_console.release_port(host=hostname, port=port)
787         except exception.InstanceNotFound:
788             guest = None
789 
790         # If the instance is already terminated, we're still happy
791         # Otherwise, destroy it
792         old_domid = -1
793         if guest is not None:
794             try:
795                 old_domid = guest.id
796                 guest.poweroff()
797 
798             except libvirt.libvirtError as e:
799                 is_okay = False
800                 errcode = e.get_error_code()
801                 if errcode == libvirt.VIR_ERR_NO_DOMAIN:
802                     # Domain already gone. This can safely be ignored.
803                     is_okay = True
804                 elif errcode == libvirt.VIR_ERR_OPERATION_INVALID:
805                     # If the instance is already shut off, we get this:
806                     # Code=55 Error=Requested operation is not valid:
807                     # domain is not running
808 
809                     state = guest.get_power_state(self._host)
810                     if state == power_state.SHUTDOWN:
811                         is_okay = True
812                 elif errcode == libvirt.VIR_ERR_INTERNAL_ERROR:
813                     errmsg = e.get_error_message()
814                     if (CONF.libvirt.virt_type == 'lxc' and
815                         errmsg == 'internal error: '
816                                   'Some processes refused to die'):
817                         # Some processes in the container didn't die
818                         # fast enough for libvirt. The container will
819                         # eventually die. For now, move on and let
820                         # the wait_for_destroy logic take over.
821                         is_okay = True
822                 elif errcode == libvirt.VIR_ERR_OPERATION_TIMEOUT:
823                     LOG.warning("Cannot destroy instance, operation time out",
824                                 instance=instance)
825                     reason = _("operation time out")
826                     raise exception.InstancePowerOffFailure(reason=reason)
827                 elif errcode == libvirt.VIR_ERR_SYSTEM_ERROR:
828                     if e.get_int1() == errno.EBUSY:
829                         # NOTE(danpb): When libvirt kills a process it sends it
830                         # SIGTERM first and waits 10 seconds. If it hasn't gone
831                         # it sends SIGKILL and waits another 5 seconds. If it
832                         # still hasn't gone then you get this EBUSY error.
833                         # Usually when a QEMU process fails to go away upon
834                         # SIGKILL it is because it is stuck in an
835                         # uninterruptible kernel sleep waiting on I/O from
836                         # some non-responsive server.
837                         # Given the CPU load of the gate tests though, it is
838                         # conceivable that the 15 second timeout is too short,
839                         # particularly if the VM running tempest has a high
840                         # steal time from the cloud host. ie 15 wallclock
841                         # seconds may have passed, but the VM might have only
842                         # have a few seconds of scheduled run time.
843                         LOG.warning('Error from libvirt during destroy. '
844                                     'Code=%(errcode)s Error=%(e)s; '
845                                     'attempt %(attempt)d of 3',
846                                     {'errcode': errcode, 'e': e,
847                                      'attempt': attempt},
848                                     instance=instance)
849                         with excutils.save_and_reraise_exception() as ctxt:
850                             # Try up to 3 times before giving up.
851                             if attempt < 3:
852                                 ctxt.reraise = False
853                                 self._destroy(instance, attempt + 1)
854                                 return
855 
856                 if not is_okay:
857                     with excutils.save_and_reraise_exception():
858                         LOG.error('Error from libvirt during destroy. '
859                                   'Code=%(errcode)s Error=%(e)s',
860                                   {'errcode': errcode, 'e': e},
861                                   instance=instance)
862 
863         def _wait_for_destroy(expected_domid):
864             """Called at an interval until the VM is gone."""
865             # NOTE(vish): If the instance disappears during the destroy
866             #             we ignore it so the cleanup can still be
867             #             attempted because we would prefer destroy to
868             #             never fail.
869             try:
870                 dom_info = self.get_info(instance)
871                 state = dom_info.state
872                 new_domid = dom_info.internal_id
873             except exception.InstanceNotFound:
874                 LOG.debug("During wait destroy, instance disappeared.",
875                           instance=instance)
876                 state = power_state.SHUTDOWN
877 
878             if state == power_state.SHUTDOWN:
879                 LOG.info("Instance destroyed successfully.", instance=instance)
880                 raise loopingcall.LoopingCallDone()
881 
882             # NOTE(wangpan): If the instance was booted again after destroy,
883             #                this may be an endless loop, so check the id of
884             #                domain here, if it changed and the instance is
885             #                still running, we should destroy it again.
886             # see https://bugs.launchpad.net/nova/+bug/1111213 for more details
887             if new_domid != expected_domid:
888                 LOG.info("Instance may be started again.", instance=instance)
889                 kwargs['is_running'] = True
890                 raise loopingcall.LoopingCallDone()
891 
892         kwargs = {'is_running': False}
893         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_destroy,
894                                                      old_domid)
895         timer.start(interval=0.5).wait()
896         if kwargs['is_running']:
897             LOG.info("Going to destroy instance again.", instance=instance)
898             self._destroy(instance)
899         else:
900             # NOTE(GuanQiang): teardown container to avoid resource leak
901             if CONF.libvirt.virt_type == 'lxc':
902                 self._teardown_container(instance)
903 
904     def destroy(self, context, instance, network_info, block_device_info=None,
905                 destroy_disks=True):
906         self._destroy(instance)
907         self.cleanup(context, instance, network_info, block_device_info,
908                      destroy_disks)
909 
910     def _undefine_domain(self, instance):
911         try:
912             guest = self._host.get_guest(instance)
913             try:
914                 support_uefi = self._has_uefi_support()
915                 guest.delete_configuration(support_uefi)
916             except libvirt.libvirtError as e:
917                 with excutils.save_and_reraise_exception() as ctxt:
918                     errcode = e.get_error_code()
919                     if errcode == libvirt.VIR_ERR_NO_DOMAIN:
920                         LOG.debug("Called undefine, but domain already gone.",
921                                   instance=instance)
922                         ctxt.reraise = False
923                     else:
924                         LOG.error('Error from libvirt during undefine. '
925                                   'Code=%(errcode)s Error=%(e)s',
926                                   {'errcode': errcode, 'e': e},
927                                   instance=instance)
928         except exception.InstanceNotFound:
929             pass
930 
931     def cleanup(self, context, instance, network_info, block_device_info=None,
932                 destroy_disks=True, migrate_data=None, destroy_vifs=True):
933         if destroy_vifs:
934             self._unplug_vifs(instance, network_info, True)
935 
936         retry = True
937         while retry:
938             try:
939                 self.unfilter_instance(instance, network_info)
940             except libvirt.libvirtError as e:
941                 try:
942                     state = self.get_info(instance).state
943                 except exception.InstanceNotFound:
944                     state = power_state.SHUTDOWN
945 
946                 if state != power_state.SHUTDOWN:
947                     LOG.warning("Instance may be still running, destroy "
948                                 "it again.", instance=instance)
949                     self._destroy(instance)
950                 else:
951                     retry = False
952                     errcode = e.get_error_code()
953                     LOG.exception(_('Error from libvirt during unfilter. '
954                                     'Code=%(errcode)s Error=%(e)s'),
955                                   {'errcode': errcode, 'e': e},
956                                   instance=instance)
957                     reason = _("Error unfiltering instance.")
958                     raise exception.InstanceTerminationFailure(reason=reason)
959             except Exception:
960                 retry = False
961                 raise
962             else:
963                 retry = False
964 
965         # FIXME(wangpan): if the instance is booted again here, such as the
966         #                 soft reboot operation boot it here, it will become
967         #                 "running deleted", should we check and destroy it
968         #                 at the end of this method?
969 
970         # NOTE(vish): we disconnect from volumes regardless
971         block_device_mapping = driver.block_device_info_get_mapping(
972             block_device_info)
973         for vol in block_device_mapping:
974             connection_info = vol['connection_info']
975             disk_dev = vol['mount_device']
976             if disk_dev is not None:
977                 disk_dev = disk_dev.rpartition("/")[2]
978 
979             if ('data' in connection_info and
980                     'volume_id' in connection_info['data']):
981                 volume_id = connection_info['data']['volume_id']
982                 encryption = encryptors.get_encryption_metadata(
983                     context, self._volume_api, volume_id, connection_info)
984 
985                 if encryption:
986                     # The volume must be detached from the VM before
987                     # disconnecting it from its encryptor. Otherwise, the
988                     # encryptor may report that the volume is still in use.
989                     encryptor = self._get_volume_encryptor(connection_info,
990                                                            encryption)
991                     encryptor.detach_volume(**encryption)
992 
993             try:
994                 self._disconnect_volume(connection_info, disk_dev, instance)
995             except Exception as exc:
996                 with excutils.save_and_reraise_exception() as ctxt:
997                     if destroy_disks:
998                         # Don't block on Volume errors if we're trying to
999                         # delete the instance as we may be partially created
1000                         # or deleted
1001                         ctxt.reraise = False
1002                         LOG.warning(
1003                             "Ignoring Volume Error on vol %(vol_id)s "
1004                             "during delete %(exc)s",
1005                             {'vol_id': vol.get('volume_id'), 'exc': exc},
1006                             instance=instance)
1007 
1008         if destroy_disks:
1009             # NOTE(haomai): destroy volumes if needed
1010             if CONF.libvirt.images_type == 'lvm':
1011                 self._cleanup_lvm(instance, block_device_info)
1012             if CONF.libvirt.images_type == 'rbd':
1013                 self._cleanup_rbd(instance)
1014 
1015         is_shared_block_storage = False
1016         if migrate_data and 'is_shared_block_storage' in migrate_data:
1017             is_shared_block_storage = migrate_data.is_shared_block_storage
1018         if destroy_disks or is_shared_block_storage:
1019             attempts = int(instance.system_metadata.get('clean_attempts',
1020                                                         '0'))
1021             success = self.delete_instance_files(instance)
1022             # NOTE(mriedem): This is used in the _run_pending_deletes periodic
1023             # task in the compute manager. The tight coupling is not great...
1024             instance.system_metadata['clean_attempts'] = str(attempts + 1)
1025             if success:
1026                 instance.cleaned = True
1027             instance.save()
1028 
1029         self._undefine_domain(instance)
1030 
1031     def _detach_encrypted_volumes(self, instance, block_device_info):
1032         """Detaches encrypted volumes attached to instance."""
1033         disks = self._get_instance_disk_info(instance, block_device_info)
1034         encrypted_volumes = filter(dmcrypt.is_encrypted,
1035                                    [disk['path'] for disk in disks])
1036         for path in encrypted_volumes:
1037             dmcrypt.delete_volume(path)
1038 
1039     def _get_serial_ports_from_guest(self, guest, mode=None):
1040         """Returns an iterator over serial port(s) configured on guest.
1041 
1042         :param mode: Should be a value in (None, bind, connect)
1043         """
1044         xml = guest.get_xml_desc()
1045         tree = etree.fromstring(xml)
1046 
1047         # The 'serial' device is the base for x86 platforms. Other platforms
1048         # (e.g. kvm on system z = S390X) can only use 'console' devices.
1049         xpath_mode = "[@mode='%s']" % mode if mode else ""
1050         serial_tcp = "./devices/serial[@type='tcp']/source" + xpath_mode
1051         console_tcp = "./devices/console[@type='tcp']/source" + xpath_mode
1052 
1053         tcp_devices = tree.findall(serial_tcp)
1054         if len(tcp_devices) == 0:
1055             tcp_devices = tree.findall(console_tcp)
1056         for source in tcp_devices:
1057             yield (source.get("host"), int(source.get("service")))
1058 
1059     def _get_scsi_controller_max_unit(self, guest):
1060         """Returns the max disk unit used by scsi controller"""
1061         xml = guest.get_xml_desc()
1062         tree = etree.fromstring(xml)
1063         addrs = "./devices/disk[@device='disk']/address[@type='drive']"
1064 
1065         ret = []
1066         for obj in tree.findall(addrs):
1067             ret.append(int(obj.get('unit', 0)))
1068         return max(ret)
1069 
1070     @staticmethod
1071     def _get_rbd_driver():
1072         return rbd_utils.RBDDriver(
1073                 pool=CONF.libvirt.images_rbd_pool,
1074                 ceph_conf=CONF.libvirt.images_rbd_ceph_conf,
1075                 rbd_user=CONF.libvirt.rbd_user)
1076 
1077     def _cleanup_rbd(self, instance):
1078         # NOTE(nic): On revert_resize, the cleanup steps for the root
1079         # volume are handled with an "rbd snap rollback" command,
1080         # and none of this is needed (and is, in fact, harmful) so
1081         # filter out non-ephemerals from the list
1082         if instance.task_state == task_states.RESIZE_REVERTING:
1083             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
1084                                       disk.endswith('disk.local'))
1085         else:
1086             filter_fn = lambda disk: disk.startswith(instance.uuid)
1087         LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
1088 
1089     def _cleanup_lvm(self, instance, block_device_info):
1090         """Delete all LVM disks for given instance object."""
1091         if instance.get('ephemeral_key_uuid') is not None:
1092             self._detach_encrypted_volumes(instance, block_device_info)
1093 
1094         disks = self._lvm_disks(instance)
1095         if disks:
1096             lvm.remove_volumes(disks)
1097 
1098     def _lvm_disks(self, instance):
1099         """Returns all LVM disks for given instance object."""
1100         if CONF.libvirt.images_volume_group:
1101             vg = os.path.join('/dev', CONF.libvirt.images_volume_group)
1102             if not os.path.exists(vg):
1103                 return []
1104             pattern = '%s_' % instance.uuid
1105 
1106             def belongs_to_instance(disk):
1107                 return disk.startswith(pattern)
1108 
1109             def fullpath(name):
1110                 return os.path.join(vg, name)
1111 
1112             logical_volumes = lvm.list_volumes(vg)
1113 
1114             disks = [fullpath(disk) for disk in logical_volumes
1115                      if belongs_to_instance(disk)]
1116             return disks
1117         return []
1118 
1119     def get_volume_connector(self, instance):
1120         root_helper = utils.get_root_helper()
1121         return connector.get_connector_properties(
1122             root_helper, CONF.my_block_storage_ip,
1123             CONF.libvirt.volume_use_multipath,
1124             enforce_multipath=True,
1125             host=CONF.host)
1126 
1127     def _cleanup_resize(self, instance, network_info):
1128         inst_base = libvirt_utils.get_instance_path(instance)
1129         target = inst_base + '_resize'
1130 
1131         if os.path.exists(target):
1132             # Deletion can fail over NFS, so retry the deletion as required.
1133             # Set maximum attempt as 5, most test can remove the directory
1134             # for the second time.
1135             utils.execute('rm', '-rf', target, delay_on_retry=True,
1136                           attempts=5)
1137 
1138         root_disk = self.image_backend.by_name(instance, 'disk')
1139         # TODO(nic): Set ignore_errors=False in a future release.
1140         # It is set to True here to avoid any upgrade issues surrounding
1141         # instances being in pending resize state when the software is updated;
1142         # in that case there will be no snapshot to remove.  Once it can be
1143         # reasonably assumed that no such instances exist in the wild
1144         # anymore, it should be set back to False (the default) so it will
1145         # throw errors, like it should.
1146         if root_disk.exists():
1147             root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
1148                                   ignore_errors=True)
1149 
1150         # NOTE(mjozefcz):
1151         # self.image_backend.image for some backends recreates instance
1152         # directory and image disk.info - remove it here if exists
1153         if os.path.exists(inst_base) and not root_disk.exists():
1154             try:
1155                 shutil.rmtree(inst_base)
1156             except OSError as e:
1157                 if e.errno != errno.ENOENT:
1158                     raise
1159 
1160         if instance.host != CONF.host:
1161             self._undefine_domain(instance)
1162             self.unplug_vifs(instance, network_info)
1163             self.unfilter_instance(instance, network_info)
1164 
1165     def _get_volume_driver(self, connection_info):
1166         driver_type = connection_info.get('driver_volume_type')
1167         if driver_type not in self.volume_drivers:
1168             raise exception.VolumeDriverNotFound(driver_type=driver_type)
1169         return self.volume_drivers[driver_type]
1170 
1171     def _connect_volume(self, connection_info, disk_info, instance):
1172         vol_driver = self._get_volume_driver(connection_info)
1173         vol_driver.connect_volume(connection_info, disk_info, instance)
1174 
1175     def _disconnect_volume(self, connection_info, disk_dev, instance):
1176         vol_driver = self._get_volume_driver(connection_info)
1177         vol_driver.disconnect_volume(connection_info, disk_dev, instance)
1178 
1179     def _extend_volume(self, connection_info, instance):
1180         vol_driver = self._get_volume_driver(connection_info)
1181         return vol_driver.extend_volume(connection_info, instance)
1182 
1183     def _get_volume_config(self, connection_info, disk_info):
1184         vol_driver = self._get_volume_driver(connection_info)
1185         conf = vol_driver.get_config(connection_info, disk_info)
1186         self._set_cache_mode(conf)
1187         return conf
1188 
1189     def _get_volume_encryptor(self, connection_info, encryption):
1190         root_helper = utils.get_root_helper()
1191         return encryptors.get_volume_encryptor(root_helper=root_helper,
1192                                                keymgr=key_manager.API(CONF),
1193                                                connection_info=connection_info,
1194                                                **encryption)
1195 
1196     def _check_discard_for_attach_volume(self, conf, instance):
1197         """Perform some checks for volumes configured for discard support.
1198 
1199         If discard is configured for the volume, and the guest is using a
1200         configuration known to not work, we will log a message explaining
1201         the reason why.
1202         """
1203         if conf.driver_discard == 'unmap' and conf.target_bus == 'virtio':
1204             LOG.debug('Attempting to attach volume %(id)s with discard '
1205                       'support enabled to an instance using an '
1206                       'unsupported configuration. target_bus = '
1207                       '%(bus)s. Trim commands will not be issued to '
1208                       'the storage device.',
1209                       {'bus': conf.target_bus,
1210                        'id': conf.serial},
1211                       instance=instance)
1212 
1213     def attach_volume(self, context, connection_info, instance, mountpoint,
1214                       disk_bus=None, device_type=None, encryption=None):
1215         guest = self._host.get_guest(instance)
1216 
1217         disk_dev = mountpoint.rpartition("/")[2]
1218         bdm = {
1219             'device_name': disk_dev,
1220             'disk_bus': disk_bus,
1221             'device_type': device_type}
1222 
1223         # Note(cfb): If the volume has a custom block size, check that
1224         #            that we are using QEMU/KVM and libvirt >= 0.10.2. The
1225         #            presence of a block size is considered mandatory by
1226         #            cinder so we fail if we can't honor the request.
1227         data = {}
1228         if ('data' in connection_info):
1229             data = connection_info['data']
1230         if ('logical_block_size' in data or 'physical_block_size' in data):
1231             if ((CONF.libvirt.virt_type != "kvm" and
1232                  CONF.libvirt.virt_type != "qemu")):
1233                 msg = _("Volume sets block size, but the current "
1234                         "libvirt hypervisor '%s' does not support custom "
1235                         "block size") % CONF.libvirt.virt_type
1236                 raise exception.InvalidHypervisorType(msg)
1237 
1238         disk_info = blockinfo.get_info_from_bdm(
1239             instance, CONF.libvirt.virt_type, instance.image_meta, bdm)
1240         self._connect_volume(connection_info, disk_info, instance)
1241         if disk_info['bus'] == 'scsi':
1242             disk_info['unit'] = self._get_scsi_controller_max_unit(guest) + 1
1243 
1244         conf = self._get_volume_config(connection_info, disk_info)
1245 
1246         self._check_discard_for_attach_volume(conf, instance)
1247 
1248         try:
1249             state = guest.get_power_state(self._host)
1250             live = state in (power_state.RUNNING, power_state.PAUSED)
1251 
1252             if encryption:
1253                 encryptor = self._get_volume_encryptor(connection_info,
1254                                                        encryption)
1255                 encryptor.attach_volume(context, **encryption)
1256 
1257             guest.attach_device(conf, persistent=True, live=live)
1258             # NOTE(artom) If we're attaching with a device role tag, we need to
1259             # rebuild device_metadata. If we're attaching without a role
1260             # tag, we're rebuilding it here needlessly anyways. This isn't a
1261             # massive deal, and it helps reduce code complexity by not having
1262             # to indicate to the virt driver that the attach is tagged. The
1263             # really important optimization of not calling the database unless
1264             # device_metadata has actually changed is done for us by
1265             # instance.save().
1266             instance.device_metadata = self._build_device_metadata(
1267                 context, instance)
1268             instance.save()
1269         except Exception:
1270             LOG.exception(_('Failed to attach volume at mountpoint: %s'),
1271                           mountpoint, instance=instance)
1272             with excutils.save_and_reraise_exception():
1273                 self._disconnect_volume(connection_info, disk_dev, instance)
1274 
1275     def _swap_volume(self, guest, disk_path, conf, resize_to):
1276         """Swap existing disk with a new block device."""
1277         dev = guest.get_block_device(disk_path)
1278 
1279         # Save a copy of the domain's persistent XML file
1280         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1281 
1282         # Abort is an idempotent operation, so make sure any block
1283         # jobs which may have failed are ended.
1284         try:
1285             dev.abort_job()
1286         except Exception:
1287             pass
1288 
1289         try:
1290             # NOTE (rmk): blockRebase cannot be executed on persistent
1291             #             domains, so we need to temporarily undefine it.
1292             #             If any part of this block fails, the domain is
1293             #             re-defined regardless.
1294             if guest.has_persistent_configuration():
1295                 support_uefi = self._has_uefi_support()
1296                 guest.delete_configuration(support_uefi)
1297 
1298             try:
1299                 # Start copy with VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT flag to
1300                 # allow writing to existing external volume file. Use
1301                 # VIR_DOMAIN_BLOCK_REBASE_COPY_DEV if it's a block device to
1302                 # make sure XML is generated correctly (bug 1691195)
1303                 copy_dev = conf.source_type == 'block'
1304                 dev.rebase(conf.source_path, copy=True, reuse_ext=True,
1305                            copy_dev=copy_dev)
1306                 while not dev.is_job_complete():
1307                     time.sleep(0.5)
1308 
1309                 dev.abort_job(pivot=True)
1310 
1311             except Exception as exc:
1312                 LOG.exception("Failure rebasing volume %(new_path)s on "
1313                     "%(old_path)s.", {'new_path': conf.source_path,
1314                                       'old_path': disk_path})
1315                 raise exception.VolumeRebaseFailed(reason=six.text_type(exc))
1316 
1317             if resize_to:
1318                 dev.resize(resize_to * units.Gi / units.Ki)
1319         finally:
1320             self._host.write_instance_config(xml)
1321 
1322     def swap_volume(self, old_connection_info,
1323                     new_connection_info, instance, mountpoint, resize_to):
1324 
1325         guest = self._host.get_guest(instance)
1326 
1327         disk_dev = mountpoint.rpartition("/")[2]
1328         if not guest.get_disk(disk_dev):
1329             raise exception.DiskNotFound(location=disk_dev)
1330         disk_info = {
1331             'dev': disk_dev,
1332             'bus': blockinfo.get_disk_bus_for_disk_dev(
1333                 CONF.libvirt.virt_type, disk_dev),
1334             'type': 'disk',
1335             }
1336         # NOTE (lyarwood): new_connection_info will be modified by the
1337         # following _connect_volume call down into the volume drivers. The
1338         # majority of the volume drivers will add a device_path that is in turn
1339         # used by _get_volume_config to set the source_path of the
1340         # LibvirtConfigGuestDisk object it returns. We do not explicitly save
1341         # this to the BDM here as the upper compute swap_volume method will
1342         # eventually do this for us.
1343         self._connect_volume(new_connection_info, disk_info, instance)
1344         conf = self._get_volume_config(new_connection_info, disk_info)
1345         if not conf.source_path:
1346             self._disconnect_volume(new_connection_info, disk_dev, instance)
1347             raise NotImplementedError(_("Swap only supports host devices"))
1348 
1349         try:
1350             self._swap_volume(guest, disk_dev, conf, resize_to)
1351         except exception.VolumeRebaseFailed:
1352             with excutils.save_and_reraise_exception():
1353                 self._disconnect_volume(new_connection_info, disk_dev,
1354                                         instance)
1355 
1356         self._disconnect_volume(old_connection_info, disk_dev, instance)
1357 
1358     def _get_existing_domain_xml(self, instance, network_info,
1359                                  block_device_info=None):
1360         try:
1361             guest = self._host.get_guest(instance)
1362             xml = guest.get_xml_desc()
1363         except exception.InstanceNotFound:
1364             disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
1365                                                 instance,
1366                                                 instance.image_meta,
1367                                                 block_device_info)
1368             xml = self._get_guest_xml(nova_context.get_admin_context(),
1369                                       instance, network_info, disk_info,
1370                                       instance.image_meta,
1371                                       block_device_info=block_device_info)
1372         return xml
1373 
1374     def detach_volume(self, connection_info, instance, mountpoint,
1375                       encryption=None):
1376         disk_dev = mountpoint.rpartition("/")[2]
1377         try:
1378             guest = self._host.get_guest(instance)
1379 
1380             state = guest.get_power_state(self._host)
1381             live = state in (power_state.RUNNING, power_state.PAUSED)
1382 
1383             # The volume must be detached from the VM before disconnecting it
1384             # from its encryptor. Otherwise, the encryptor may report that the
1385             # volume is still in use.
1386             wait_for_detach = guest.detach_device_with_retry(guest.get_disk,
1387                                                              disk_dev,
1388                                                              live=live)
1389             wait_for_detach()
1390 
1391             if encryption:
1392                 encryptor = self._get_volume_encryptor(connection_info,
1393                                                        encryption)
1394                 encryptor.detach_volume(**encryption)
1395 
1396         except exception.InstanceNotFound:
1397             # NOTE(zhaoqin): If the instance does not exist, _lookup_by_name()
1398             #                will throw InstanceNotFound exception. Need to
1399             #                disconnect volume under this circumstance.
1400             LOG.warning("During detach_volume, instance disappeared.",
1401                         instance=instance)
1402         except exception.DeviceNotFound:
1403             raise exception.DiskNotFound(location=disk_dev)
1404         except libvirt.libvirtError as ex:
1405             # NOTE(vish): This is called to cleanup volumes after live
1406             #             migration, so we should still disconnect even if
1407             #             the instance doesn't exist here anymore.
1408             error_code = ex.get_error_code()
1409             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1410                 # NOTE(vish):
1411                 LOG.warning("During detach_volume, instance disappeared.",
1412                             instance=instance)
1413             else:
1414                 raise
1415 
1416         self._disconnect_volume(connection_info, disk_dev, instance)
1417 
1418     def extend_volume(self, connection_info, instance):
1419         try:
1420             new_size = self._extend_volume(connection_info, instance)
1421         except NotImplementedError:
1422             raise exception.ExtendVolumeNotSupported()
1423 
1424         # Resize the device in QEMU so its size is updated and
1425         # detected by the instance without rebooting.
1426         try:
1427             guest = self._host.get_guest(instance)
1428             state = guest.get_power_state(self._host)
1429             active_state = state in (power_state.RUNNING, power_state.PAUSED)
1430             if active_state:
1431                 disk_path = connection_info['data']['device_path']
1432                 LOG.debug('resizing block device %(dev)s to %(size)u kb',
1433                           {'dev': disk_path, 'size': new_size})
1434                 dev = guest.get_block_device(disk_path)
1435                 dev.resize(new_size // units.Ki)
1436             else:
1437                 LOG.debug('Skipping block device resize, guest is not running',
1438                           instance=instance)
1439         except exception.InstanceNotFound:
1440             with excutils.save_and_reraise_exception():
1441                 LOG.warning('During extend_volume, instance disappeared.',
1442                             instance=instance)
1443         except libvirt.libvirtError:
1444             with excutils.save_and_reraise_exception():
1445                 LOG.exception('resizing block device failed.',
1446                               instance=instance)
1447 
1448     def attach_interface(self, context, instance, image_meta, vif):
1449         guest = self._host.get_guest(instance)
1450 
1451         self.vif_driver.plug(instance, vif)
1452         self.firewall_driver.setup_basic_filtering(instance, [vif])
1453         cfg = self.vif_driver.get_config(instance, vif, image_meta,
1454                                          instance.flavor,
1455                                          CONF.libvirt.virt_type,
1456                                          self._host)
1457         try:
1458             state = guest.get_power_state(self._host)
1459             live = state in (power_state.RUNNING, power_state.PAUSED)
1460             guest.attach_device(cfg, persistent=True, live=live)
1461         except libvirt.libvirtError:
1462             LOG.error('attaching network adapter failed.',
1463                       instance=instance, exc_info=True)
1464             self.vif_driver.unplug(instance, vif)
1465             raise exception.InterfaceAttachFailed(
1466                     instance_uuid=instance.uuid)
1467         try:
1468             # NOTE(artom) If we're attaching with a device role tag, we need to
1469             # rebuild device_metadata. If we're attaching without a role
1470             # tag, we're rebuilding it here needlessly anyways. This isn't a
1471             # massive deal, and it helps reduce code complexity by not having
1472             # to indicate to the virt driver that the attach is tagged. The
1473             # really important optimization of not calling the database unless
1474             # device_metadata has actually changed is done for us by
1475             # instance.save().
1476             instance.device_metadata = self._build_device_metadata(
1477                 context, instance)
1478             instance.save()
1479         except Exception:
1480             # NOTE(artom) If we fail here it means the interface attached
1481             # successfully but building and/or saving the device metadata
1482             # failed. Just unplugging the vif is therefore not enough cleanup,
1483             # we need to detach the interface.
1484             with excutils.save_and_reraise_exception(reraise=False):
1485                 LOG.error('Interface attached successfully but building '
1486                           'and/or saving device metadata failed.',
1487                           instance=instance, exc_info=True)
1488                 self.detach_interface(context, instance, vif)
1489                 raise exception.InterfaceAttachFailed(
1490                     instance_uuid=instance.uuid)
1491 
1492     def detach_interface(self, context, instance, vif):
1493         guest = self._host.get_guest(instance)
1494         cfg = self.vif_driver.get_config(instance, vif,
1495                                          instance.image_meta,
1496                                          instance.flavor,
1497                                          CONF.libvirt.virt_type, self._host)
1498         interface = guest.get_interface_by_cfg(cfg)
1499         try:
1500             self.vif_driver.unplug(instance, vif)
1501             # NOTE(mriedem): When deleting an instance and using Neutron,
1502             # we can be racing against Neutron deleting the port and
1503             # sending the vif-deleted event which then triggers a call to
1504             # detach the interface, so if the interface is not found then
1505             # we can just log it as a warning.
1506             if not interface:
1507                 mac = vif.get('address')
1508                 # The interface is gone so just log it as a warning.
1509                 LOG.warning('Detaching interface %(mac)s failed because '
1510                             'the device is no longer found on the guest.',
1511                             {'mac': mac}, instance=instance)
1512                 return
1513 
1514             state = guest.get_power_state(self._host)
1515             live = state in (power_state.RUNNING, power_state.PAUSED)
1516             # Now we are going to loop until the interface is detached or we
1517             # timeout.
1518             wait_for_detach = guest.detach_device_with_retry(
1519                 guest.get_interface_by_cfg, cfg, live=live,
1520                 alternative_device_name=self.vif_driver.get_vif_devname(vif))
1521             wait_for_detach()
1522         except exception.DeviceDetachFailed:
1523             # We failed to detach the device even with the retry loop, so let's
1524             # dump some debug information to the logs before raising back up.
1525             with excutils.save_and_reraise_exception():
1526                 devname = self.vif_driver.get_vif_devname(vif)
1527                 interface = guest.get_interface_by_cfg(cfg)
1528                 if interface:
1529                     LOG.warning(
1530                         'Failed to detach interface %(devname)s after '
1531                         'repeated attempts. Final interface xml:\n'
1532                         '%(interface_xml)s\nFinal guest xml:\n%(guest_xml)s',
1533                         {'devname': devname,
1534                          'interface_xml': interface.to_xml(),
1535                          'guest_xml': guest.get_xml_desc()},
1536                         instance=instance)
1537         except exception.DeviceNotFound:
1538             # The interface is gone so just log it as a warning.
1539             LOG.warning('Detaching interface %(mac)s failed because '
1540                         'the device is no longer found on the guest.',
1541                         {'mac': vif.get('address')}, instance=instance)
1542         except libvirt.libvirtError as ex:
1543             error_code = ex.get_error_code()
1544             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
1545                 LOG.warning("During detach_interface, instance disappeared.",
1546                             instance=instance)
1547             else:
1548                 # NOTE(mriedem): When deleting an instance and using Neutron,
1549                 # we can be racing against Neutron deleting the port and
1550                 # sending the vif-deleted event which then triggers a call to
1551                 # detach the interface, so we might have failed because the
1552                 # network device no longer exists. Libvirt will fail with
1553                 # "operation failed: no matching network device was found"
1554                 # which unfortunately does not have a unique error code so we
1555                 # need to look up the interface by config and if it's not found
1556                 # then we can just log it as a warning rather than tracing an
1557                 # error.
1558                 mac = vif.get('address')
1559                 interface = guest.get_interface_by_cfg(cfg)
1560                 if interface:
1561                     LOG.error('detaching network adapter failed.',
1562                               instance=instance, exc_info=True)
1563                     raise exception.InterfaceDetachFailed(
1564                             instance_uuid=instance.uuid)
1565 
1566                 # The interface is gone so just log it as a warning.
1567                 LOG.warning('Detaching interface %(mac)s failed because '
1568                             'the device is no longer found on the guest.',
1569                             {'mac': mac}, instance=instance)
1570 
1571     def _create_snapshot_metadata(self, image_meta, instance,
1572                                   img_fmt, snp_name):
1573         metadata = {'is_public': False,
1574                     'status': 'active',
1575                     'name': snp_name,
1576                     'properties': {
1577                                    'kernel_id': instance.kernel_id,
1578                                    'image_location': 'snapshot',
1579                                    'image_state': 'available',
1580                                    'owner_id': instance.project_id,
1581                                    'ramdisk_id': instance.ramdisk_id,
1582                                    }
1583                     }
1584         if instance.os_type:
1585             metadata['properties']['os_type'] = instance.os_type
1586 
1587         # NOTE(vish): glance forces ami disk format to be ami
1588         if image_meta.disk_format == 'ami':
1589             metadata['disk_format'] = 'ami'
1590         else:
1591             metadata['disk_format'] = img_fmt
1592 
1593         if image_meta.obj_attr_is_set("container_format"):
1594             metadata['container_format'] = image_meta.container_format
1595         else:
1596             metadata['container_format'] = "bare"
1597 
1598         return metadata
1599 
1600     def snapshot(self, context, instance, image_id, update_task_state):
1601         """Create snapshot from a running VM instance.
1602 
1603         This command only works with qemu 0.14+
1604         """
1605         try:
1606             guest = self._host.get_guest(instance)
1607 
1608             # TODO(sahid): We are converting all calls from a
1609             # virDomain object to use nova.virt.libvirt.Guest.
1610             # We should be able to remove virt_dom at the end.
1611             virt_dom = guest._domain
1612         except exception.InstanceNotFound:
1613             raise exception.InstanceNotRunning(instance_id=instance.uuid)
1614 
1615         snapshot = self._image_api.get(context, image_id)
1616 
1617         # source_format is an on-disk format
1618         # source_type is a backend type
1619         disk_path, source_format = libvirt_utils.find_disk(guest)
1620         source_type = libvirt_utils.get_disk_type_from_path(disk_path)
1621 
1622         # We won't have source_type for raw or qcow2 disks, because we can't
1623         # determine that from the path. We should have it from the libvirt
1624         # xml, though.
1625         if source_type is None:
1626             source_type = source_format
1627         # For lxc instances we won't have it either from libvirt xml
1628         # (because we just gave libvirt the mounted filesystem), or the path,
1629         # so source_type is still going to be None. In this case,
1630         # root_disk is going to default to CONF.libvirt.images_type
1631         # below, which is still safe.
1632 
1633         image_format = CONF.libvirt.snapshot_image_format or source_type
1634 
1635         # NOTE(bfilippov): save lvm and rbd as raw
1636         if image_format == 'lvm' or image_format == 'rbd':
1637             image_format = 'raw'
1638 
1639         metadata = self._create_snapshot_metadata(instance.image_meta,
1640                                                   instance,
1641                                                   image_format,
1642                                                   snapshot['name'])
1643 
1644         snapshot_name = uuidutils.generate_uuid(dashed=False)
1645 
1646         state = guest.get_power_state(self._host)
1647 
1648         # NOTE(dgenin): Instances with LVM encrypted ephemeral storage require
1649         #               cold snapshots. Currently, checking for encryption is
1650         #               redundant because LVM supports only cold snapshots.
1651         #               It is necessary in case this situation changes in the
1652         #               future.
1653         if (self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)
1654              and source_type not in ('lvm')
1655              and not CONF.ephemeral_storage_encryption.enabled
1656              and not CONF.workarounds.disable_libvirt_livesnapshot):
1657             live_snapshot = True
1658             # Abort is an idempotent operation, so make sure any block
1659             # jobs which may have failed are ended. This operation also
1660             # confirms the running instance, as opposed to the system as a
1661             # whole, has a new enough version of the hypervisor (bug 1193146).
1662             try:
1663                 guest.get_block_device(disk_path).abort_job()
1664             except libvirt.libvirtError as ex:
1665                 error_code = ex.get_error_code()
1666                 if error_code == libvirt.VIR_ERR_CONFIG_UNSUPPORTED:
1667                     live_snapshot = False
1668                 else:
1669                     pass
1670         else:
1671             live_snapshot = False
1672 
1673         # NOTE(rmk): We cannot perform live snapshots when a managedSave
1674         #            file is present, so we will use the cold/legacy method
1675         #            for instances which are shutdown.
1676         if state == power_state.SHUTDOWN:
1677             live_snapshot = False
1678 
1679         self._prepare_domain_for_snapshot(context, live_snapshot, state,
1680                                           instance)
1681 
1682         root_disk = self.image_backend.by_libvirt_path(
1683             instance, disk_path, image_type=source_type)
1684 
1685         if live_snapshot:
1686             LOG.info("Beginning live snapshot process", instance=instance)
1687         else:
1688             LOG.info("Beginning cold snapshot process", instance=instance)
1689 
1690         update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD)
1691 
1692         try:
1693             update_task_state(task_state=task_states.IMAGE_UPLOADING,
1694                               expected_state=task_states.IMAGE_PENDING_UPLOAD)
1695             metadata['location'] = root_disk.direct_snapshot(
1696                 context, snapshot_name, image_format, image_id,
1697                 instance.image_ref)
1698             self._snapshot_domain(context, live_snapshot, virt_dom, state,
1699                                   instance)
1700             self._image_api.update(context, image_id, metadata,
1701                                    purge_props=False)
1702         except (NotImplementedError, exception.ImageUnacceptable,
1703                 exception.Forbidden) as e:
1704             if type(e) != NotImplementedError:
1705                 LOG.warning('Performing standard snapshot because direct '
1706                             'snapshot failed: %(error)s', {'error': e})
1707             failed_snap = metadata.pop('location', None)
1708             if failed_snap:
1709                 failed_snap = {'url': str(failed_snap)}
1710             root_disk.cleanup_direct_snapshot(failed_snap,
1711                                                   also_destroy_volume=True,
1712                                                   ignore_errors=True)
1713             update_task_state(task_state=task_states.IMAGE_PENDING_UPLOAD,
1714                               expected_state=task_states.IMAGE_UPLOADING)
1715 
1716             # TODO(nic): possibly abstract this out to the root_disk
1717             if source_type == 'rbd' and live_snapshot:
1718                 # Standard snapshot uses qemu-img convert from RBD which is
1719                 # not safe to run with live_snapshot.
1720                 live_snapshot = False
1721                 # Suspend the guest, so this is no longer a live snapshot
1722                 self._prepare_domain_for_snapshot(context, live_snapshot,
1723                                                   state, instance)
1724 
1725             snapshot_directory = CONF.libvirt.snapshots_directory
1726             fileutils.ensure_tree(snapshot_directory)
1727             with utils.tempdir(dir=snapshot_directory) as tmpdir:
1728                 try:
1729                     out_path = os.path.join(tmpdir, snapshot_name)
1730                     if live_snapshot:
1731                         # NOTE(xqueralt): libvirt needs o+x in the tempdir
1732                         os.chmod(tmpdir, 0o701)
1733                         self._live_snapshot(context, instance, guest,
1734                                             disk_path, out_path, source_format,
1735                                             image_format, instance.image_meta)
1736                     else:
1737                         root_disk.snapshot_extract(out_path, image_format)
1738                 finally:
1739                     self._snapshot_domain(context, live_snapshot, virt_dom,
1740                                           state, instance)
1741                     LOG.info("Snapshot extracted, beginning image upload",
1742                              instance=instance)
1743 
1744                 # Upload that image to the image service
1745                 update_task_state(task_state=task_states.IMAGE_UPLOADING,
1746                         expected_state=task_states.IMAGE_PENDING_UPLOAD)
1747                 with libvirt_utils.file_open(out_path, 'rb') as image_file:
1748                     self._image_api.update(context,
1749                                            image_id,
1750                                            metadata,
1751                                            image_file)
1752         except Exception:
1753             with excutils.save_and_reraise_exception():
1754                 LOG.exception(_("Failed to snapshot image"))
1755                 failed_snap = metadata.pop('location', None)
1756                 if failed_snap:
1757                     failed_snap = {'url': str(failed_snap)}
1758                 root_disk.cleanup_direct_snapshot(
1759                         failed_snap, also_destroy_volume=True,
1760                         ignore_errors=True)
1761 
1762         LOG.info("Snapshot image upload complete", instance=instance)
1763 
1764     def _prepare_domain_for_snapshot(self, context, live_snapshot, state,
1765                                      instance):
1766         # NOTE(dkang): managedSave does not work for LXC
1767         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1768             if state == power_state.RUNNING or state == power_state.PAUSED:
1769                 self.suspend(context, instance)
1770 
1771     def _snapshot_domain(self, context, live_snapshot, virt_dom, state,
1772                          instance):
1773         guest = None
1774         # NOTE(dkang): because previous managedSave is not called
1775         #              for LXC, _create_domain must not be called.
1776         if CONF.libvirt.virt_type != 'lxc' and not live_snapshot:
1777             if state == power_state.RUNNING:
1778                 guest = self._create_domain(domain=virt_dom)
1779             elif state == power_state.PAUSED:
1780                 guest = self._create_domain(domain=virt_dom, pause=True)
1781 
1782             if guest is not None:
1783                 self._attach_pci_devices(
1784                     guest, pci_manager.get_instance_pci_devs(instance))
1785                 self._attach_direct_passthrough_ports(
1786                     context, instance, guest)
1787 
1788     def _can_set_admin_password(self, image_meta):
1789 
1790         if CONF.libvirt.virt_type == 'parallels':
1791             if not self._host.has_min_version(
1792                    MIN_LIBVIRT_PARALLELS_SET_ADMIN_PASSWD):
1793                 raise exception.SetAdminPasswdNotSupported()
1794         elif CONF.libvirt.virt_type in ('kvm', 'qemu'):
1795             if not self._host.has_min_version(
1796                    MIN_LIBVIRT_SET_ADMIN_PASSWD):
1797                 raise exception.SetAdminPasswdNotSupported()
1798             if not image_meta.properties.get('hw_qemu_guest_agent', False):
1799                 raise exception.QemuGuestAgentNotEnabled()
1800         else:
1801             raise exception.SetAdminPasswdNotSupported()
1802 
1803     def set_admin_password(self, instance, new_pass):
1804         self._can_set_admin_password(instance.image_meta)
1805 
1806         guest = self._host.get_guest(instance)
1807         user = instance.image_meta.properties.get("os_admin_user")
1808         if not user:
1809             if instance.os_type == "windows":
1810                 user = "Administrator"
1811             else:
1812                 user = "root"
1813         try:
1814             guest.set_user_password(user, new_pass)
1815         except libvirt.libvirtError as ex:
1816             error_code = ex.get_error_code()
1817             msg = (_('Error from libvirt while set password for username '
1818                      '"%(user)s": [Error Code %(error_code)s] %(ex)s')
1819                    % {'user': user, 'error_code': error_code, 'ex': ex})
1820             raise exception.InternalError(msg)
1821 
1822     def _can_quiesce(self, instance, image_meta):
1823         if CONF.libvirt.virt_type not in ('kvm', 'qemu'):
1824             raise exception.InstanceQuiesceNotSupported(
1825                 instance_id=instance.uuid)
1826 
1827         if not image_meta.properties.get('hw_qemu_guest_agent', False):
1828             raise exception.QemuGuestAgentNotEnabled()
1829 
1830     def _requires_quiesce(self, image_meta):
1831         return image_meta.properties.get('os_require_quiesce', False)
1832 
1833     def _set_quiesced(self, context, instance, image_meta, quiesced):
1834         self._can_quiesce(instance, image_meta)
1835         try:
1836             guest = self._host.get_guest(instance)
1837             if quiesced:
1838                 guest.freeze_filesystems()
1839             else:
1840                 guest.thaw_filesystems()
1841         except libvirt.libvirtError as ex:
1842             error_code = ex.get_error_code()
1843             msg = (_('Error from libvirt while quiescing %(instance_name)s: '
1844                      '[Error Code %(error_code)s] %(ex)s')
1845                    % {'instance_name': instance.name,
1846                       'error_code': error_code, 'ex': ex})
1847             raise exception.InternalError(msg)
1848 
1849     def quiesce(self, context, instance, image_meta):
1850         """Freeze the guest filesystems to prepare for snapshot.
1851 
1852         The qemu-guest-agent must be setup to execute fsfreeze.
1853         """
1854         self._set_quiesced(context, instance, image_meta, True)
1855 
1856     def unquiesce(self, context, instance, image_meta):
1857         """Thaw the guest filesystems after snapshot."""
1858         self._set_quiesced(context, instance, image_meta, False)
1859 
1860     def _live_snapshot(self, context, instance, guest, disk_path, out_path,
1861                        source_format, image_format, image_meta):
1862         """Snapshot an instance without downtime."""
1863         dev = guest.get_block_device(disk_path)
1864 
1865         # Save a copy of the domain's persistent XML file
1866         xml = guest.get_xml_desc(dump_inactive=True, dump_sensitive=True)
1867 
1868         # Abort is an idempotent operation, so make sure any block
1869         # jobs which may have failed are ended.
1870         try:
1871             dev.abort_job()
1872         except Exception:
1873             pass
1874 
1875         # NOTE (rmk): We are using shallow rebases as a workaround to a bug
1876         #             in QEMU 1.3. In order to do this, we need to create
1877         #             a destination image with the original backing file
1878         #             and matching size of the instance root disk.
1879         src_disk_size = libvirt_utils.get_disk_size(disk_path,
1880                                                     format=source_format)
1881         src_back_path = libvirt_utils.get_disk_backing_file(disk_path,
1882                                                         format=source_format,
1883                                                         basename=False)
1884         disk_delta = out_path + '.delta'
1885         libvirt_utils.create_cow_image(src_back_path, disk_delta,
1886                                        src_disk_size)
1887 
1888         quiesced = False
1889         try:
1890             self._set_quiesced(context, instance, image_meta, True)
1891             quiesced = True
1892         except exception.NovaException as err:
1893             if self._requires_quiesce(image_meta):
1894                 raise
1895             LOG.info('Skipping quiescing instance: %(reason)s.',
1896                      {'reason': err}, instance=instance)
1897 
1898         try:
1899             # NOTE (rmk): blockRebase cannot be executed on persistent
1900             #             domains, so we need to temporarily undefine it.
1901             #             If any part of this block fails, the domain is
1902             #             re-defined regardless.
1903             if guest.has_persistent_configuration():
1904                 support_uefi = self._has_uefi_support()
1905                 guest.delete_configuration(support_uefi)
1906 
1907             # NOTE (rmk): Establish a temporary mirror of our root disk and
1908             #             issue an abort once we have a complete copy.
1909             dev.rebase(disk_delta, copy=True, reuse_ext=True, shallow=True)
1910 
1911             while not dev.is_job_complete():
1912                 time.sleep(0.5)
1913 
1914             dev.abort_job()
1915             nova.privsep.path.chown(disk_delta, uid=os.getuid())
1916         finally:
1917             self._host.write_instance_config(xml)
1918             if quiesced:
1919                 self._set_quiesced(context, instance, image_meta, False)
1920 
1921         # Convert the delta (CoW) image with a backing file to a flat
1922         # image with no backing file.
1923         libvirt_utils.extract_snapshot(disk_delta, 'qcow2',
1924                                        out_path, image_format)
1925 
1926     def _volume_snapshot_update_status(self, context, snapshot_id, status):
1927         """Send a snapshot status update to Cinder.
1928 
1929         This method captures and logs exceptions that occur
1930         since callers cannot do anything useful with these exceptions.
1931 
1932         Operations on the Cinder side waiting for this will time out if
1933         a failure occurs sending the update.
1934 
1935         :param context: security context
1936         :param snapshot_id: id of snapshot being updated
1937         :param status: new status value
1938 
1939         """
1940 
1941         try:
1942             self._volume_api.update_snapshot_status(context,
1943                                                     snapshot_id,
1944                                                     status)
1945         except Exception:
1946             LOG.exception(_('Failed to send updated snapshot status '
1947                             'to volume service.'))
1948 
1949     def _volume_snapshot_create(self, context, instance, guest,
1950                                 volume_id, new_file):
1951         """Perform volume snapshot.
1952 
1953            :param guest: VM that volume is attached to
1954            :param volume_id: volume UUID to snapshot
1955            :param new_file: relative path to new qcow2 file present on share
1956 
1957         """
1958         xml = guest.get_xml_desc()
1959         xml_doc = etree.fromstring(xml)
1960 
1961         device_info = vconfig.LibvirtConfigGuest()
1962         device_info.parse_dom(xml_doc)
1963 
1964         disks_to_snap = []          # to be snapshotted by libvirt
1965         network_disks_to_snap = []  # network disks (netfs, etc.)
1966         disks_to_skip = []          # local disks not snapshotted
1967 
1968         for guest_disk in device_info.devices:
1969             if (guest_disk.root_name != 'disk'):
1970                 continue
1971 
1972             if (guest_disk.target_dev is None):
1973                 continue
1974 
1975             if (guest_disk.serial is None or guest_disk.serial != volume_id):
1976                 disks_to_skip.append(guest_disk.target_dev)
1977                 continue
1978 
1979             # disk is a Cinder volume with the correct volume_id
1980 
1981             disk_info = {
1982                 'dev': guest_disk.target_dev,
1983                 'serial': guest_disk.serial,
1984                 'current_file': guest_disk.source_path,
1985                 'source_protocol': guest_disk.source_protocol,
1986                 'source_name': guest_disk.source_name,
1987                 'source_hosts': guest_disk.source_hosts,
1988                 'source_ports': guest_disk.source_ports
1989             }
1990 
1991             # Determine path for new_file based on current path
1992             if disk_info['current_file'] is not None:
1993                 current_file = disk_info['current_file']
1994                 new_file_path = os.path.join(os.path.dirname(current_file),
1995                                              new_file)
1996                 disks_to_snap.append((current_file, new_file_path))
1997             # NOTE(mriedem): This used to include a check for gluster in
1998             # addition to netfs since they were added together. Support for
1999             # gluster was removed in the 16.0.0 Pike release. It is unclear,
2000             # however, if other volume drivers rely on the netfs disk source
2001             # protocol.
2002             elif disk_info['source_protocol'] == 'netfs':
2003                 network_disks_to_snap.append((disk_info, new_file))
2004 
2005         if not disks_to_snap and not network_disks_to_snap:
2006             msg = _('Found no disk to snapshot.')
2007             raise exception.InternalError(msg)
2008 
2009         snapshot = vconfig.LibvirtConfigGuestSnapshot()
2010 
2011         for current_name, new_filename in disks_to_snap:
2012             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2013             snap_disk.name = current_name
2014             snap_disk.source_path = new_filename
2015             snap_disk.source_type = 'file'
2016             snap_disk.snapshot = 'external'
2017             snap_disk.driver_name = 'qcow2'
2018 
2019             snapshot.add_disk(snap_disk)
2020 
2021         for disk_info, new_filename in network_disks_to_snap:
2022             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2023             snap_disk.name = disk_info['dev']
2024             snap_disk.source_type = 'network'
2025             snap_disk.source_protocol = disk_info['source_protocol']
2026             snap_disk.snapshot = 'external'
2027             snap_disk.source_path = new_filename
2028             old_dir = disk_info['source_name'].split('/')[0]
2029             snap_disk.source_name = '%s/%s' % (old_dir, new_filename)
2030             snap_disk.source_hosts = disk_info['source_hosts']
2031             snap_disk.source_ports = disk_info['source_ports']
2032 
2033             snapshot.add_disk(snap_disk)
2034 
2035         for dev in disks_to_skip:
2036             snap_disk = vconfig.LibvirtConfigGuestSnapshotDisk()
2037             snap_disk.name = dev
2038             snap_disk.snapshot = 'no'
2039 
2040             snapshot.add_disk(snap_disk)
2041 
2042         snapshot_xml = snapshot.to_xml()
2043         LOG.debug("snap xml: %s", snapshot_xml, instance=instance)
2044 
2045         image_meta = instance.image_meta
2046         try:
2047             # Check to see if we can quiesce the guest before taking the
2048             # snapshot.
2049             self._can_quiesce(instance, image_meta)
2050             try:
2051                 guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2052                                reuse_ext=True, quiesce=True)
2053                 return
2054             except libvirt.libvirtError:
2055                 # If the image says that quiesce is required then we fail.
2056                 if self._requires_quiesce(image_meta):
2057                     raise
2058                 LOG.exception(_('Unable to create quiesced VM snapshot, '
2059                                 'attempting again with quiescing disabled.'),
2060                               instance=instance)
2061         except (exception.InstanceQuiesceNotSupported,
2062                 exception.QemuGuestAgentNotEnabled) as err:
2063             # If the image says that quiesce is required then we need to fail.
2064             if self._requires_quiesce(image_meta):
2065                 raise
2066             LOG.info('Skipping quiescing instance: %(reason)s.',
2067                      {'reason': err}, instance=instance)
2068 
2069         try:
2070             guest.snapshot(snapshot, no_metadata=True, disk_only=True,
2071                            reuse_ext=True, quiesce=False)
2072         except libvirt.libvirtError:
2073             LOG.exception(_('Unable to create VM snapshot, '
2074                             'failing volume_snapshot operation.'),
2075                           instance=instance)
2076 
2077             raise
2078 
2079     def _volume_refresh_connection_info(self, context, instance, volume_id):
2080         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
2081                   context, volume_id, instance.uuid)
2082 
2083         driver_bdm = driver_block_device.convert_volume(bdm)
2084         if driver_bdm:
2085             driver_bdm.refresh_connection_info(context, instance,
2086                                                self._volume_api, self)
2087 
2088     def volume_snapshot_create(self, context, instance, volume_id,
2089                                create_info):
2090         """Create snapshots of a Cinder volume via libvirt.
2091 
2092         :param instance: VM instance object reference
2093         :param volume_id: id of volume being snapshotted
2094         :param create_info: dict of information used to create snapshots
2095                      - snapshot_id : ID of snapshot
2096                      - type : qcow2 / <other>
2097                      - new_file : qcow2 file created by Cinder which
2098                      becomes the VM's active image after
2099                      the snapshot is complete
2100         """
2101 
2102         LOG.debug("volume_snapshot_create: create_info: %(c_info)s",
2103                   {'c_info': create_info}, instance=instance)
2104 
2105         try:
2106             guest = self._host.get_guest(instance)
2107         except exception.InstanceNotFound:
2108             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2109 
2110         if create_info['type'] != 'qcow2':
2111             msg = _('Unknown type: %s') % create_info['type']
2112             raise exception.InternalError(msg)
2113 
2114         snapshot_id = create_info.get('snapshot_id', None)
2115         if snapshot_id is None:
2116             msg = _('snapshot_id required in create_info')
2117             raise exception.InternalError(msg)
2118 
2119         try:
2120             self._volume_snapshot_create(context, instance, guest,
2121                                          volume_id, create_info['new_file'])
2122         except Exception:
2123             with excutils.save_and_reraise_exception():
2124                 LOG.exception(_('Error occurred during '
2125                                 'volume_snapshot_create, '
2126                                 'sending error status to Cinder.'),
2127                               instance=instance)
2128                 self._volume_snapshot_update_status(
2129                     context, snapshot_id, 'error')
2130 
2131         self._volume_snapshot_update_status(
2132             context, snapshot_id, 'creating')
2133 
2134         def _wait_for_snapshot():
2135             snapshot = self._volume_api.get_snapshot(context, snapshot_id)
2136 
2137             if snapshot.get('status') != 'creating':
2138                 self._volume_refresh_connection_info(context, instance,
2139                                                      volume_id)
2140                 raise loopingcall.LoopingCallDone()
2141 
2142         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_snapshot)
2143         timer.start(interval=0.5).wait()
2144 
2145     @staticmethod
2146     def _rebase_with_qemu_img(guest, device, active_disk_object,
2147                               rebase_base):
2148         """Rebase a device tied to a guest using qemu-img.
2149 
2150         :param guest:the Guest which owns the device being rebased
2151         :type guest: nova.virt.libvirt.guest.Guest
2152         :param device: the guest block device to rebase
2153         :type device: nova.virt.libvirt.guest.BlockDevice
2154         :param active_disk_object: the guest block device to rebase
2155         :type active_disk_object: nova.virt.libvirt.config.\
2156                                     LibvirtConfigGuestDisk
2157         :param rebase_base: the new parent in the backing chain
2158         :type rebase_base: None or string
2159         """
2160 
2161         # It's unsure how well qemu-img handles network disks for
2162         # every protocol. So let's be safe.
2163         active_protocol = active_disk_object.source_protocol
2164         if active_protocol is not None:
2165             msg = _("Something went wrong when deleting a volume snapshot: "
2166                     "rebasing a %(protocol)s network disk using qemu-img "
2167                     "has not been fully tested") % {'protocol':
2168                     active_protocol}
2169             LOG.error(msg)
2170             raise exception.InternalError(msg)
2171 
2172         if rebase_base is None:
2173             # If backing_file is specified as "" (the empty string), then
2174             # the image is rebased onto no backing file (i.e. it will exist
2175             # independently of any backing file).
2176             backing_file = ""
2177             qemu_img_extra_arg = []
2178         else:
2179             # If the rebased image is going to have a backing file then
2180             # explicitly set the backing file format to avoid any security
2181             # concerns related to file format auto detection.
2182             backing_file = rebase_base
2183             b_file_fmt = images.qemu_img_info(backing_file).file_format
2184             qemu_img_extra_arg = ['-F', b_file_fmt]
2185 
2186         qemu_img_extra_arg.append(active_disk_object.source_path)
2187         utils.execute("qemu-img", "rebase", "-b", backing_file,
2188                       *qemu_img_extra_arg)
2189 
2190     def _volume_snapshot_delete(self, context, instance, volume_id,
2191                                 snapshot_id, delete_info=None):
2192         """Note:
2193             if file being merged into == active image:
2194                 do a blockRebase (pull) operation
2195             else:
2196                 do a blockCommit operation
2197             Files must be adjacent in snap chain.
2198 
2199         :param instance: instance object reference
2200         :param volume_id: volume UUID
2201         :param snapshot_id: snapshot UUID (unused currently)
2202         :param delete_info: {
2203             'type':              'qcow2',
2204             'file_to_merge':     'a.img',
2205             'merge_target_file': 'b.img' or None (if merging file_to_merge into
2206                                                   active image)
2207           }
2208         """
2209 
2210         LOG.debug('volume_snapshot_delete: delete_info: %s', delete_info,
2211                   instance=instance)
2212 
2213         if delete_info['type'] != 'qcow2':
2214             msg = _('Unknown delete_info type %s') % delete_info['type']
2215             raise exception.InternalError(msg)
2216 
2217         try:
2218             guest = self._host.get_guest(instance)
2219         except exception.InstanceNotFound:
2220             raise exception.InstanceNotRunning(instance_id=instance.uuid)
2221 
2222         # Find dev name
2223         my_dev = None
2224         active_disk = None
2225 
2226         xml = guest.get_xml_desc()
2227         xml_doc = etree.fromstring(xml)
2228 
2229         device_info = vconfig.LibvirtConfigGuest()
2230         device_info.parse_dom(xml_doc)
2231 
2232         active_disk_object = None
2233 
2234         for guest_disk in device_info.devices:
2235             if (guest_disk.root_name != 'disk'):
2236                 continue
2237 
2238             if (guest_disk.target_dev is None or guest_disk.serial is None):
2239                 continue
2240 
2241             if guest_disk.serial == volume_id:
2242                 my_dev = guest_disk.target_dev
2243 
2244                 active_disk = guest_disk.source_path
2245                 active_protocol = guest_disk.source_protocol
2246                 active_disk_object = guest_disk
2247                 break
2248 
2249         if my_dev is None or (active_disk is None and active_protocol is None):
2250             LOG.debug('Domain XML: %s', xml, instance=instance)
2251             msg = (_('Disk with id: %s not found attached to instance.')
2252                    % volume_id)
2253             raise exception.InternalError(msg)
2254 
2255         LOG.debug("found device at %s", my_dev, instance=instance)
2256 
2257         def _get_snap_dev(filename, backing_store):
2258             if filename is None:
2259                 msg = _('filename cannot be None')
2260                 raise exception.InternalError(msg)
2261 
2262             # libgfapi delete
2263             LOG.debug("XML: %s", xml)
2264 
2265             LOG.debug("active disk object: %s", active_disk_object)
2266 
2267             # determine reference within backing store for desired image
2268             filename_to_merge = filename
2269             matched_name = None
2270             b = backing_store
2271             index = None
2272 
2273             current_filename = active_disk_object.source_name.split('/')[1]
2274             if current_filename == filename_to_merge:
2275                 return my_dev + '[0]'
2276 
2277             while b is not None:
2278                 source_filename = b.source_name.split('/')[1]
2279                 if source_filename == filename_to_merge:
2280                     LOG.debug('found match: %s', b.source_name)
2281                     matched_name = b.source_name
2282                     index = b.index
2283                     break
2284 
2285                 b = b.backing_store
2286 
2287             if matched_name is None:
2288                 msg = _('no match found for %s') % (filename_to_merge)
2289                 raise exception.InternalError(msg)
2290 
2291             LOG.debug('index of match (%s) is %s', b.source_name, index)
2292 
2293             my_snap_dev = '%s[%s]' % (my_dev, index)
2294             return my_snap_dev
2295 
2296         if delete_info['merge_target_file'] is None:
2297             # pull via blockRebase()
2298 
2299             # Merge the most recent snapshot into the active image
2300 
2301             rebase_disk = my_dev
2302             rebase_base = delete_info['file_to_merge']  # often None
2303             if (active_protocol is not None) and (rebase_base is not None):
2304                 rebase_base = _get_snap_dev(rebase_base,
2305                                             active_disk_object.backing_store)
2306 
2307             # NOTE(deepakcs): libvirt added support for _RELATIVE in v1.2.7,
2308             # and when available this flag _must_ be used to ensure backing
2309             # paths are maintained relative by qemu.
2310             #
2311             # If _RELATIVE flag not found, continue with old behaviour
2312             # (relative backing path seems to work for this case)
2313             try:
2314                 libvirt.VIR_DOMAIN_BLOCK_REBASE_RELATIVE
2315                 relative = rebase_base is not None
2316             except AttributeError:
2317                 LOG.warning(
2318                     "Relative blockrebase support was not detected. "
2319                     "Continuing with old behaviour.")
2320                 relative = False
2321 
2322             LOG.debug(
2323                 'disk: %(disk)s, base: %(base)s, '
2324                 'bw: %(bw)s, relative: %(relative)s',
2325                 {'disk': rebase_disk,
2326                  'base': rebase_base,
2327                  'bw': libvirt_guest.BlockDevice.REBASE_DEFAULT_BANDWIDTH,
2328                  'relative': str(relative)}, instance=instance)
2329 
2330             dev = guest.get_block_device(rebase_disk)
2331             if guest.is_active():
2332                 result = dev.rebase(rebase_base, relative=relative)
2333                 if result == 0:
2334                     LOG.debug('blockRebase started successfully',
2335                               instance=instance)
2336 
2337                 while not dev.is_job_complete():
2338                     LOG.debug('waiting for blockRebase job completion',
2339                               instance=instance)
2340                     time.sleep(0.5)
2341 
2342             # If the guest is not running libvirt won't do a blockRebase.
2343             # In that case, let's ask qemu-img to rebase the disk.
2344             else:
2345                 LOG.debug('Guest is not running so doing a block rebase '
2346                           'using "qemu-img rebase"', instance=instance)
2347                 self._rebase_with_qemu_img(guest, dev, active_disk_object,
2348                                            rebase_base)
2349 
2350         else:
2351             # commit with blockCommit()
2352             my_snap_base = None
2353             my_snap_top = None
2354             commit_disk = my_dev
2355 
2356             if active_protocol is not None:
2357                 my_snap_base = _get_snap_dev(delete_info['merge_target_file'],
2358                                              active_disk_object.backing_store)
2359                 my_snap_top = _get_snap_dev(delete_info['file_to_merge'],
2360                                             active_disk_object.backing_store)
2361 
2362             commit_base = my_snap_base or delete_info['merge_target_file']
2363             commit_top = my_snap_top or delete_info['file_to_merge']
2364 
2365             LOG.debug('will call blockCommit with commit_disk=%(commit_disk)s '
2366                       'commit_base=%(commit_base)s '
2367                       'commit_top=%(commit_top)s ',
2368                       {'commit_disk': commit_disk,
2369                        'commit_base': commit_base,
2370                        'commit_top': commit_top}, instance=instance)
2371 
2372             dev = guest.get_block_device(commit_disk)
2373             result = dev.commit(commit_base, commit_top, relative=True)
2374 
2375             if result == 0:
2376                 LOG.debug('blockCommit started successfully',
2377                           instance=instance)
2378 
2379             while not dev.is_job_complete():
2380                 LOG.debug('waiting for blockCommit job completion',
2381                           instance=instance)
2382                 time.sleep(0.5)
2383 
2384     def volume_snapshot_delete(self, context, instance, volume_id, snapshot_id,
2385                                delete_info):
2386         try:
2387             self._volume_snapshot_delete(context, instance, volume_id,
2388                                          snapshot_id, delete_info=delete_info)
2389         except Exception:
2390             with excutils.save_and_reraise_exception():
2391                 LOG.exception(_('Error occurred during '
2392                                 'volume_snapshot_delete, '
2393                                 'sending error status to Cinder.'),
2394                               instance=instance)
2395                 self._volume_snapshot_update_status(
2396                     context, snapshot_id, 'error_deleting')
2397 
2398         self._volume_snapshot_update_status(context, snapshot_id, 'deleting')
2399         self._volume_refresh_connection_info(context, instance, volume_id)
2400 
2401     def reboot(self, context, instance, network_info, reboot_type,
2402                block_device_info=None, bad_volumes_callback=None):
2403         """Reboot a virtual machine, given an instance reference."""
2404         if reboot_type == 'SOFT':
2405             # NOTE(vish): This will attempt to do a graceful shutdown/restart.
2406             try:
2407                 soft_reboot_success = self._soft_reboot(instance)
2408             except libvirt.libvirtError as e:
2409                 LOG.debug("Instance soft reboot failed: %s", e,
2410                           instance=instance)
2411                 soft_reboot_success = False
2412 
2413             if soft_reboot_success:
2414                 LOG.info("Instance soft rebooted successfully.",
2415                          instance=instance)
2416                 return
2417             else:
2418                 LOG.warning("Failed to soft reboot instance. "
2419                             "Trying hard reboot.",
2420                             instance=instance)
2421         return self._hard_reboot(context, instance, network_info,
2422                                  block_device_info)
2423 
2424     def _soft_reboot(self, instance):
2425         """Attempt to shutdown and restart the instance gracefully.
2426 
2427         We use shutdown and create here so we can return if the guest
2428         responded and actually rebooted. Note that this method only
2429         succeeds if the guest responds to acpi. Therefore we return
2430         success or failure so we can fall back to a hard reboot if
2431         necessary.
2432 
2433         :returns: True if the reboot succeeded
2434         """
2435         guest = self._host.get_guest(instance)
2436 
2437         state = guest.get_power_state(self._host)
2438         old_domid = guest.id
2439         # NOTE(vish): This check allows us to reboot an instance that
2440         #             is already shutdown.
2441         if state == power_state.RUNNING:
2442             guest.shutdown()
2443         # NOTE(vish): This actually could take slightly longer than the
2444         #             FLAG defines depending on how long the get_info
2445         #             call takes to return.
2446         self._prepare_pci_devices_for_use(
2447             pci_manager.get_instance_pci_devs(instance, 'all'))
2448         for x in range(CONF.libvirt.wait_soft_reboot_seconds):
2449             guest = self._host.get_guest(instance)
2450 
2451             state = guest.get_power_state(self._host)
2452             new_domid = guest.id
2453 
2454             # NOTE(ivoks): By checking domain IDs, we make sure we are
2455             #              not recreating domain that's already running.
2456             if old_domid != new_domid:
2457                 if state in [power_state.SHUTDOWN,
2458                              power_state.CRASHED]:
2459                     LOG.info("Instance shutdown successfully.",
2460                              instance=instance)
2461                     self._create_domain(domain=guest._domain)
2462                     timer = loopingcall.FixedIntervalLoopingCall(
2463                         self._wait_for_running, instance)
2464                     timer.start(interval=0.5).wait()
2465                     return True
2466                 else:
2467                     LOG.info("Instance may have been rebooted during soft "
2468                              "reboot, so return now.", instance=instance)
2469                     return True
2470             greenthread.sleep(1)
2471         return False
2472 
2473     def _hard_reboot(self, context, instance, network_info,
2474                      block_device_info=None):
2475         """Reboot a virtual machine, given an instance reference.
2476 
2477         Performs a Libvirt reset (if supported) on the domain.
2478 
2479         If Libvirt reset is unavailable this method actually destroys and
2480         re-creates the domain to ensure the reboot happens, as the guest
2481         OS cannot ignore this action.
2482         """
2483 
2484         self._destroy(instance)
2485         # Domain XML will be redefined so we can safely undefine it
2486         # from libvirt. This ensure that such process as create serial
2487         # console for guest will run smoothly.
2488         self._undefine_domain(instance)
2489 
2490         # Convert the system metadata to image metadata
2491         # NOTE(mdbooth): This is a workaround for stateless Nova compute
2492         #                https://bugs.launchpad.net/nova/+bug/1349978
2493         instance_dir = libvirt_utils.get_instance_path(instance)
2494         fileutils.ensure_tree(instance_dir)
2495 
2496         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2497                                             instance,
2498                                             instance.image_meta,
2499                                             block_device_info)
2500         # NOTE(vish): This could generate the wrong device_format if we are
2501         #             using the raw backend and the images don't exist yet.
2502         #             The create_images_and_backing below doesn't properly
2503         #             regenerate raw backend images, however, so when it
2504         #             does we need to (re)generate the xml after the images
2505         #             are in place.
2506         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2507                                   instance.image_meta,
2508                                   block_device_info=block_device_info)
2509 
2510         # NOTE(mdbooth): context.auth_token will not be set when we call
2511         #                _hard_reboot from resume_state_on_host_boot()
2512         if context.auth_token is not None:
2513             # NOTE (rmk): Re-populate any missing backing files.
2514             config = vconfig.LibvirtConfigGuest()
2515             config.parse_str(xml)
2516             backing_disk_info = self._get_instance_disk_info_from_config(
2517                 config, block_device_info)
2518             self._create_images_and_backing(context, instance, instance_dir,
2519                                             backing_disk_info)
2520 
2521         # Initialize all the necessary networking, block devices and
2522         # start the instance.
2523         self._create_domain_and_network(context, xml, instance, network_info,
2524                                         block_device_info=block_device_info,
2525                                         reboot=True,
2526                                         vifs_already_plugged=True)
2527         self._prepare_pci_devices_for_use(
2528             pci_manager.get_instance_pci_devs(instance, 'all'))
2529 
2530         def _wait_for_reboot():
2531             """Called at an interval until the VM is running again."""
2532             state = self.get_info(instance).state
2533 
2534             if state == power_state.RUNNING:
2535                 LOG.info("Instance rebooted successfully.",
2536                          instance=instance)
2537                 raise loopingcall.LoopingCallDone()
2538 
2539         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_reboot)
2540         timer.start(interval=0.5).wait()
2541 
2542     def pause(self, instance):
2543         """Pause VM instance."""
2544         self._host.get_guest(instance).pause()
2545 
2546     def unpause(self, instance):
2547         """Unpause paused VM instance."""
2548         guest = self._host.get_guest(instance)
2549         guest.resume()
2550         guest.sync_guest_time()
2551 
2552     def _clean_shutdown(self, instance, timeout, retry_interval):
2553         """Attempt to shutdown the instance gracefully.
2554 
2555         :param instance: The instance to be shutdown
2556         :param timeout: How long to wait in seconds for the instance to
2557                         shutdown
2558         :param retry_interval: How often in seconds to signal the instance
2559                                to shutdown while waiting
2560 
2561         :returns: True if the shutdown succeeded
2562         """
2563 
2564         # List of states that represent a shutdown instance
2565         SHUTDOWN_STATES = [power_state.SHUTDOWN,
2566                            power_state.CRASHED]
2567 
2568         try:
2569             guest = self._host.get_guest(instance)
2570         except exception.InstanceNotFound:
2571             # If the instance has gone then we don't need to
2572             # wait for it to shutdown
2573             return True
2574 
2575         state = guest.get_power_state(self._host)
2576         if state in SHUTDOWN_STATES:
2577             LOG.info("Instance already shutdown.", instance=instance)
2578             return True
2579 
2580         LOG.debug("Shutting down instance from state %s", state,
2581                   instance=instance)
2582         guest.shutdown()
2583         retry_countdown = retry_interval
2584 
2585         for sec in range(timeout):
2586 
2587             guest = self._host.get_guest(instance)
2588             state = guest.get_power_state(self._host)
2589 
2590             if state in SHUTDOWN_STATES:
2591                 LOG.info("Instance shutdown successfully after %d seconds.",
2592                          sec, instance=instance)
2593                 return True
2594 
2595             # Note(PhilD): We can't assume that the Guest was able to process
2596             #              any previous shutdown signal (for example it may
2597             #              have still been startingup, so within the overall
2598             #              timeout we re-trigger the shutdown every
2599             #              retry_interval
2600             if retry_countdown == 0:
2601                 retry_countdown = retry_interval
2602                 # Instance could shutdown at any time, in which case we
2603                 # will get an exception when we call shutdown
2604                 try:
2605                     LOG.debug("Instance in state %s after %d seconds - "
2606                               "resending shutdown", state, sec,
2607                               instance=instance)
2608                     guest.shutdown()
2609                 except libvirt.libvirtError:
2610                     # Assume this is because its now shutdown, so loop
2611                     # one more time to clean up.
2612                     LOG.debug("Ignoring libvirt exception from shutdown "
2613                               "request.", instance=instance)
2614                     continue
2615             else:
2616                 retry_countdown -= 1
2617 
2618             time.sleep(1)
2619 
2620         LOG.info("Instance failed to shutdown in %d seconds.",
2621                  timeout, instance=instance)
2622         return False
2623 
2624     def power_off(self, instance, timeout=0, retry_interval=0):
2625         """Power off the specified instance."""
2626         if timeout:
2627             self._clean_shutdown(instance, timeout, retry_interval)
2628         self._destroy(instance)
2629 
2630     def power_on(self, context, instance, network_info,
2631                  block_device_info=None):
2632         """Power on the specified instance."""
2633         # We use _hard_reboot here to ensure that all backing files,
2634         # network, and block device connections, etc. are established
2635         # and available before we attempt to start the instance.
2636         self._hard_reboot(context, instance, network_info, block_device_info)
2637 
2638     def trigger_crash_dump(self, instance):
2639 
2640         """Trigger crash dump by injecting an NMI to the specified instance."""
2641         try:
2642             self._host.get_guest(instance).inject_nmi()
2643         except libvirt.libvirtError as ex:
2644             error_code = ex.get_error_code()
2645 
2646             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
2647                 raise exception.TriggerCrashDumpNotSupported()
2648             elif error_code == libvirt.VIR_ERR_OPERATION_INVALID:
2649                 raise exception.InstanceNotRunning(instance_id=instance.uuid)
2650 
2651             LOG.exception(_('Error from libvirt while injecting an NMI to '
2652                             '%(instance_uuid)s: '
2653                             '[Error Code %(error_code)s] %(ex)s'),
2654                           {'instance_uuid': instance.uuid,
2655                            'error_code': error_code, 'ex': ex})
2656             raise
2657 
2658     def suspend(self, context, instance):
2659         """Suspend the specified instance."""
2660         guest = self._host.get_guest(instance)
2661 
2662         self._detach_pci_devices(guest,
2663             pci_manager.get_instance_pci_devs(instance))
2664         self._detach_direct_passthrough_ports(context, instance, guest)
2665         guest.save_memory_state()
2666 
2667     def resume(self, context, instance, network_info, block_device_info=None):
2668         """resume the specified instance."""
2669         xml = self._get_existing_domain_xml(instance, network_info,
2670                                             block_device_info)
2671         guest = self._create_domain_and_network(context, xml, instance,
2672                            network_info, block_device_info=block_device_info,
2673                            vifs_already_plugged=True)
2674         self._attach_pci_devices(guest,
2675             pci_manager.get_instance_pci_devs(instance))
2676         self._attach_direct_passthrough_ports(
2677             context, instance, guest, network_info)
2678         timer = loopingcall.FixedIntervalLoopingCall(self._wait_for_running,
2679                                                      instance)
2680         timer.start(interval=0.5).wait()
2681         guest.sync_guest_time()
2682 
2683     def resume_state_on_host_boot(self, context, instance, network_info,
2684                                   block_device_info=None):
2685         """resume guest state when a host is booted."""
2686         # Check if the instance is running already and avoid doing
2687         # anything if it is.
2688         try:
2689             guest = self._host.get_guest(instance)
2690             state = guest.get_power_state(self._host)
2691 
2692             ignored_states = (power_state.RUNNING,
2693                               power_state.SUSPENDED,
2694                               power_state.NOSTATE,
2695                               power_state.PAUSED)
2696 
2697             if state in ignored_states:
2698                 return
2699         except (exception.InternalError, exception.InstanceNotFound):
2700             pass
2701 
2702         # Instance is not up and could be in an unknown state.
2703         # Be as absolute as possible about getting it back into
2704         # a known and running state.
2705         self._hard_reboot(context, instance, network_info, block_device_info)
2706 
2707     def rescue(self, context, instance, network_info, image_meta,
2708                rescue_password):
2709         """Loads a VM using rescue images.
2710 
2711         A rescue is normally performed when something goes wrong with the
2712         primary images and data needs to be corrected/recovered. Rescuing
2713         should not edit or over-ride the original image, only allow for
2714         data recovery.
2715 
2716         """
2717         instance_dir = libvirt_utils.get_instance_path(instance)
2718         unrescue_xml = self._get_existing_domain_xml(instance, network_info)
2719         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2720         libvirt_utils.write_to_file(unrescue_xml_path, unrescue_xml)
2721 
2722         rescue_image_id = None
2723         if image_meta.obj_attr_is_set("id"):
2724             rescue_image_id = image_meta.id
2725 
2726         rescue_images = {
2727             'image_id': (rescue_image_id or
2728                         CONF.libvirt.rescue_image_id or instance.image_ref),
2729             'kernel_id': (CONF.libvirt.rescue_kernel_id or
2730                           instance.kernel_id),
2731             'ramdisk_id': (CONF.libvirt.rescue_ramdisk_id or
2732                            instance.ramdisk_id),
2733         }
2734         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2735                                             instance,
2736                                             image_meta,
2737                                             rescue=True)
2738         injection_info = InjectionInfo(network_info=network_info,
2739                                        admin_pass=rescue_password,
2740                                        files=None)
2741         gen_confdrive = functools.partial(self._create_configdrive,
2742                                           context, instance, injection_info,
2743                                           rescue=True)
2744         self._create_image(context, instance, disk_info['mapping'],
2745                            injection_info=injection_info, suffix='.rescue',
2746                            disk_images=rescue_images)
2747         xml = self._get_guest_xml(context, instance, network_info, disk_info,
2748                                   image_meta, rescue=rescue_images)
2749         self._destroy(instance)
2750         self._create_domain(xml, post_xml_callback=gen_confdrive)
2751 
2752     def unrescue(self, instance, network_info):
2753         """Reboot the VM which is being rescued back into primary images.
2754         """
2755         instance_dir = libvirt_utils.get_instance_path(instance)
2756         unrescue_xml_path = os.path.join(instance_dir, 'unrescue.xml')
2757         xml = libvirt_utils.load_file(unrescue_xml_path)
2758         guest = self._host.get_guest(instance)
2759 
2760         # TODO(sahid): We are converting all calls from a
2761         # virDomain object to use nova.virt.libvirt.Guest.
2762         # We should be able to remove virt_dom at the end.
2763         virt_dom = guest._domain
2764         self._destroy(instance)
2765         self._create_domain(xml, virt_dom)
2766         os.unlink(unrescue_xml_path)
2767         rescue_files = os.path.join(instance_dir, "*.rescue")
2768         for rescue_file in glob.iglob(rescue_files):
2769             if os.path.isdir(rescue_file):
2770                 shutil.rmtree(rescue_file)
2771             else:
2772                 os.unlink(rescue_file)
2773         # cleanup rescue volume
2774         lvm.remove_volumes([lvmdisk for lvmdisk in self._lvm_disks(instance)
2775                                 if lvmdisk.endswith('.rescue')])
2776         if CONF.libvirt.images_type == 'rbd':
2777             filter_fn = lambda disk: (disk.startswith(instance.uuid) and
2778                                       disk.endswith('.rescue'))
2779             LibvirtDriver._get_rbd_driver().cleanup_volumes(filter_fn)
2780 
2781     def poll_rebooting_instances(self, timeout, instances):
2782         pass
2783 
2784     # NOTE(ilyaalekseyev): Implementation like in multinics
2785     # for xenapi(tr3buchet)
2786     def spawn(self, context, instance, image_meta, injected_files,
2787               admin_password, network_info=None, block_device_info=None):
2788         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
2789                                             instance,
2790                                             image_meta,
2791                                             block_device_info)
2792         injection_info = InjectionInfo(network_info=network_info,
2793                                        files=injected_files,
2794                                        admin_pass=admin_password)
2795         gen_confdrive = functools.partial(self._create_configdrive,
2796                                           context, instance,
2797                                           injection_info)
2798         self._create_image(context, instance, disk_info['mapping'],
2799                            injection_info=injection_info,
2800                            block_device_info=block_device_info)
2801 
2802         # Required by Quobyte CI
2803         self._ensure_console_log_for_instance(instance)
2804 
2805         xml = self._get_guest_xml(context, instance, network_info,
2806                                   disk_info, image_meta,
2807                                   block_device_info=block_device_info)
2808         self._create_domain_and_network(
2809             context, xml, instance, network_info,
2810             block_device_info=block_device_info,
2811             post_xml_callback=gen_confdrive,
2812             destroy_disks_on_failure=True)
2813         LOG.debug("Instance is running", instance=instance)
2814 
2815         def _wait_for_boot():
2816             """Called at an interval until the VM is running."""
2817             state = self.get_info(instance).state
2818 
2819             if state == power_state.RUNNING:
2820                 LOG.info("Instance spawned successfully.", instance=instance)
2821                 raise loopingcall.LoopingCallDone()
2822 
2823         timer = loopingcall.FixedIntervalLoopingCall(_wait_for_boot)
2824         timer.start(interval=0.5).wait()
2825 
2826     def _flush_libvirt_console(self, pty):
2827         out, err = utils.execute('dd',
2828                                  'if=%s' % pty,
2829                                  'iflag=nonblock',
2830                                  run_as_root=True,
2831                                  check_exit_code=False)
2832         return out
2833 
2834     def _get_console_output_file(self, instance, console_log):
2835         bytes_to_read = MAX_CONSOLE_BYTES
2836         log_data = b""  # The last N read bytes
2837         i = 0  # in case there is a log rotation (like "virtlogd")
2838         path = console_log
2839 
2840         while bytes_to_read > 0 and os.path.exists(path):
2841             read_log_data, remaining = nova.privsep.libvirt.last_bytes(
2842                                         path, bytes_to_read)
2843             # We need the log file content in chronological order,
2844             # that's why we *prepend* the log data.
2845             log_data = read_log_data + log_data
2846 
2847             # Prep to read the next file in the chain
2848             bytes_to_read -= len(read_log_data)
2849             path = console_log + "." + str(i)
2850             i += 1
2851 
2852             if remaining > 0:
2853                 LOG.info('Truncated console log returned, '
2854                          '%d bytes ignored', remaining, instance=instance)
2855         return log_data
2856 
2857     def get_console_output(self, context, instance):
2858         guest = self._host.get_guest(instance)
2859 
2860         xml = guest.get_xml_desc()
2861         tree = etree.fromstring(xml)
2862 
2863         # If the guest has a console logging to a file prefer to use that
2864         file_consoles = tree.findall("./devices/console[@type='file']")
2865         if file_consoles:
2866             for file_console in file_consoles:
2867                 source_node = file_console.find('./source')
2868                 if source_node is None:
2869                     continue
2870                 path = source_node.get("path")
2871                 if not path:
2872                     continue
2873 
2874                 if not os.path.exists(path):
2875                     LOG.info('Instance is configured with a file console, '
2876                              'but the backing file is not (yet?) present',
2877                              instance=instance)
2878                     return ""
2879 
2880                 return self._get_console_output_file(instance, path)
2881 
2882         # Try 'pty' types
2883         pty_consoles = tree.findall("./devices/console[@type='pty']")
2884         if pty_consoles:
2885             for pty_console in pty_consoles:
2886                 source_node = pty_console.find('./source')
2887                 if source_node is None:
2888                     continue
2889                 pty = source_node.get("path")
2890                 if not pty:
2891                     continue
2892                 break
2893             else:
2894                 raise exception.ConsoleNotAvailable()
2895         else:
2896             raise exception.ConsoleNotAvailable()
2897 
2898         console_log = self._get_console_log_path(instance)
2899         data = self._flush_libvirt_console(pty)
2900         # NOTE(markus_z): The virt_types kvm and qemu are the only ones
2901         # which create a dedicated file device for the console logging.
2902         # Other virt_types like xen, lxc, uml, parallels depend on the
2903         # flush of that pty device into the "console.log" file to ensure
2904         # that a series of "get_console_output" calls return the complete
2905         # content even after rebooting a guest.
2906         nova.privsep.path.writefile(console_log, 'a+', data)
2907         return self._get_console_output_file(instance, console_log)
2908 
2909     def get_host_ip_addr(self):
2910         ips = compute_utils.get_machine_ips()
2911         if CONF.my_ip not in ips:
2912             LOG.warning('my_ip address (%(my_ip)s) was not found on '
2913                         'any of the interfaces: %(ifaces)s',
2914                         {'my_ip': CONF.my_ip, 'ifaces': ", ".join(ips)})
2915         return CONF.my_ip
2916 
2917     def get_vnc_console(self, context, instance):
2918         def get_vnc_port_for_instance(instance_name):
2919             guest = self._host.get_guest(instance)
2920 
2921             xml = guest.get_xml_desc()
2922             xml_dom = etree.fromstring(xml)
2923 
2924             graphic = xml_dom.find("./devices/graphics[@type='vnc']")
2925             if graphic is not None:
2926                 return graphic.get('port')
2927             # NOTE(rmk): We had VNC consoles enabled but the instance in
2928             # question is not actually listening for connections.
2929             raise exception.ConsoleTypeUnavailable(console_type='vnc')
2930 
2931         port = get_vnc_port_for_instance(instance.name)
2932         host = CONF.vnc.server_proxyclient_address
2933 
2934         return ctype.ConsoleVNC(host=host, port=port)
2935 
2936     def get_spice_console(self, context, instance):
2937         def get_spice_ports_for_instance(instance_name):
2938             guest = self._host.get_guest(instance)
2939 
2940             xml = guest.get_xml_desc()
2941             xml_dom = etree.fromstring(xml)
2942 
2943             graphic = xml_dom.find("./devices/graphics[@type='spice']")
2944             if graphic is not None:
2945                 return (graphic.get('port'), graphic.get('tlsPort'))
2946             # NOTE(rmk): We had Spice consoles enabled but the instance in
2947             # question is not actually listening for connections.
2948             raise exception.ConsoleTypeUnavailable(console_type='spice')
2949 
2950         ports = get_spice_ports_for_instance(instance.name)
2951         host = CONF.spice.server_proxyclient_address
2952 
2953         return ctype.ConsoleSpice(host=host, port=ports[0], tlsPort=ports[1])
2954 
2955     def get_serial_console(self, context, instance):
2956         guest = self._host.get_guest(instance)
2957         for hostname, port in self._get_serial_ports_from_guest(
2958                 guest, mode='bind'):
2959             return ctype.ConsoleSerial(host=hostname, port=port)
2960         raise exception.ConsoleTypeUnavailable(console_type='serial')
2961 
2962     @staticmethod
2963     def _supports_direct_io(dirpath):
2964 
2965         if not hasattr(os, 'O_DIRECT'):
2966             LOG.debug("This python runtime does not support direct I/O")
2967             return False
2968 
2969         testfile = os.path.join(dirpath, ".directio.test")
2970 
2971         hasDirectIO = True
2972         fd = None
2973         try:
2974             fd = os.open(testfile, os.O_CREAT | os.O_WRONLY | os.O_DIRECT)
2975             # Check is the write allowed with 512 byte alignment
2976             align_size = 512
2977             m = mmap.mmap(-1, align_size)
2978             m.write(b"x" * align_size)
2979             os.write(fd, m)
2980             LOG.debug("Path '%(path)s' supports direct I/O",
2981                       {'path': dirpath})
2982         except OSError as e:
2983             if e.errno == errno.EINVAL:
2984                 LOG.debug("Path '%(path)s' does not support direct I/O: "
2985                           "'%(ex)s'", {'path': dirpath, 'ex': e})
2986                 hasDirectIO = False
2987             else:
2988                 with excutils.save_and_reraise_exception():
2989                     LOG.error("Error on '%(path)s' while checking "
2990                               "direct I/O: '%(ex)s'",
2991                               {'path': dirpath, 'ex': e})
2992         except Exception as e:
2993             with excutils.save_and_reraise_exception():
2994                 LOG.error("Error on '%(path)s' while checking direct I/O: "
2995                           "'%(ex)s'", {'path': dirpath, 'ex': e})
2996         finally:
2997             # ensure unlink(filepath) will actually remove the file by deleting
2998             # the remaining link to it in close(fd)
2999             if fd is not None:
3000                 os.close(fd)
3001 
3002             try:
3003                 os.unlink(testfile)
3004             except Exception:
3005                 pass
3006 
3007         return hasDirectIO
3008 
3009     @staticmethod
3010     def _create_ephemeral(target, ephemeral_size,
3011                           fs_label, os_type, is_block_dev=False,
3012                           context=None, specified_fs=None,
3013                           vm_mode=None):
3014         if not is_block_dev:
3015             if (CONF.libvirt.virt_type == "parallels" and
3016                     vm_mode == fields.VMMode.EXE):
3017 
3018                 libvirt_utils.create_ploop_image('expanded', target,
3019                                                  '%dG' % ephemeral_size,
3020                                                  specified_fs)
3021                 return
3022             libvirt_utils.create_image('raw', target, '%dG' % ephemeral_size)
3023 
3024         # Run as root only for block devices.
3025         disk_api.mkfs(os_type, fs_label, target, run_as_root=is_block_dev,
3026                       specified_fs=specified_fs)
3027 
3028     @staticmethod
3029     def _create_swap(target, swap_mb, context=None):
3030         """Create a swap file of specified size."""
3031         libvirt_utils.create_image('raw', target, '%dM' % swap_mb)
3032         utils.mkfs('swap', target)
3033 
3034     @staticmethod
3035     def _get_console_log_path(instance):
3036         return os.path.join(libvirt_utils.get_instance_path(instance),
3037                             'console.log')
3038 
3039     def _ensure_console_log_for_instance(self, instance):
3040         # NOTE(mdbooth): Although libvirt will create this file for us
3041         # automatically when it starts, it will initially create it with
3042         # root ownership and then chown it depending on the configuration of
3043         # the domain it is launching. Quobyte CI explicitly disables the
3044         # chown by setting dynamic_ownership=0 in libvirt's config.
3045         # Consequently when the domain starts it is unable to write to its
3046         # console.log. See bug https://bugs.launchpad.net/nova/+bug/1597644
3047         #
3048         # To work around this, we create the file manually before starting
3049         # the domain so it has the same ownership as Nova. This works
3050         # for Quobyte CI because it is also configured to run qemu as the same
3051         # user as the Nova service. Installations which don't set
3052         # dynamic_ownership=0 are not affected because libvirt will always
3053         # correctly configure permissions regardless of initial ownership.
3054         #
3055         # Setting dynamic_ownership=0 is dubious and potentially broken in
3056         # more ways than console.log (see comment #22 on the above bug), so
3057         # Future Maintainer who finds this code problematic should check to see
3058         # if we still support it.
3059         console_file = self._get_console_log_path(instance)
3060         LOG.debug('Ensure instance console log exists: %s', console_file,
3061                   instance=instance)
3062         try:
3063             libvirt_utils.file_open(console_file, 'a').close()
3064         # NOTE(sfinucan): We can safely ignore permission issues here and
3065         # assume that it is libvirt that has taken ownership of this file.
3066         except IOError as ex:
3067             if ex.errno != errno.EACCES:
3068                 raise
3069             LOG.debug('Console file already exists: %s.', console_file)
3070 
3071     @staticmethod
3072     def _get_disk_config_image_type():
3073         # TODO(mikal): there is a bug here if images_type has
3074         # changed since creation of the instance, but I am pretty
3075         # sure that this bug already exists.
3076         return 'rbd' if CONF.libvirt.images_type == 'rbd' else 'raw'
3077 
3078     @staticmethod
3079     def _is_booted_from_volume(block_device_info):
3080         """Determines whether the VM is booting from volume
3081 
3082         Determines whether the block device info indicates that the VM
3083         is booting from a volume.
3084         """
3085         block_device_mapping = driver.block_device_info_get_mapping(
3086             block_device_info)
3087         return bool(block_device.get_root_bdm(block_device_mapping))
3088 
3089     def _inject_data(self, disk, instance, injection_info):
3090         """Injects data in a disk image
3091 
3092         Helper used for injecting data in a disk image file system.
3093 
3094         :param disk: The disk we're injecting into (an Image object)
3095         :param instance: The instance we're injecting into
3096         :param injection_info: Injection info
3097         """
3098         # Handles the partition need to be used.
3099         LOG.debug('Checking root disk injection %(info)s',
3100                   info=str(injection_info), instance=instance)
3101         target_partition = None
3102         if not instance.kernel_id:
3103             target_partition = CONF.libvirt.inject_partition
3104             if target_partition == 0:
3105                 target_partition = None
3106         if CONF.libvirt.virt_type == 'lxc':
3107             target_partition = None
3108 
3109         # Handles the key injection.
3110         if CONF.libvirt.inject_key and instance.get('key_data'):
3111             key = str(instance.key_data)
3112         else:
3113             key = None
3114 
3115         # Handles the admin password injection.
3116         if not CONF.libvirt.inject_password:
3117             admin_pass = None
3118         else:
3119             admin_pass = injection_info.admin_pass
3120 
3121         # Handles the network injection.
3122         net = netutils.get_injected_network_template(
3123             injection_info.network_info,
3124             libvirt_virt_type=CONF.libvirt.virt_type)
3125 
3126         # Handles the metadata injection
3127         metadata = instance.get('metadata')
3128 
3129         if any((key, net, metadata, admin_pass, injection_info.files)):
3130             LOG.debug('Injecting %(info)s', info=str(injection_info),
3131                       instance=instance)
3132             img_id = instance.image_ref
3133             try:
3134                 disk_api.inject_data(disk.get_model(self._conn),
3135                                      key, net, metadata, admin_pass,
3136                                      injection_info.files,
3137                                      partition=target_partition,
3138                                      mandatory=('files',))
3139             except Exception as e:
3140                 with excutils.save_and_reraise_exception():
3141                     LOG.error('Error injecting data into image '
3142                               '%(img_id)s (%(e)s)',
3143                               {'img_id': img_id, 'e': e},
3144                               instance=instance)
3145 
3146     # NOTE(sileht): many callers of this method assume that this
3147     # method doesn't fail if an image already exists but instead
3148     # think that it will be reused (ie: (live)-migration/resize)
3149     def _create_image(self, context, instance,
3150                       disk_mapping, injection_info=None, suffix='',
3151                       disk_images=None, block_device_info=None,
3152                       fallback_from_host=None,
3153                       ignore_bdi_for_swap=False):
3154         booted_from_volume = self._is_booted_from_volume(block_device_info)
3155 
3156         def image(fname, image_type=CONF.libvirt.images_type):
3157             return self.image_backend.by_name(instance,
3158                                               fname + suffix, image_type)
3159 
3160         def raw(fname):
3161             return image(fname, image_type='raw')
3162 
3163         # ensure directories exist and are writable
3164         fileutils.ensure_tree(libvirt_utils.get_instance_path(instance))
3165 
3166         LOG.info('Creating image', instance=instance)
3167 
3168         inst_type = instance.get_flavor()
3169         swap_mb = 0
3170         if 'disk.swap' in disk_mapping:
3171             mapping = disk_mapping['disk.swap']
3172 
3173             if ignore_bdi_for_swap:
3174                 # This is a workaround to support legacy swap resizing,
3175                 # which does not touch swap size specified in bdm,
3176                 # but works with flavor specified size only.
3177                 # In this case we follow the legacy logic and ignore block
3178                 # device info completely.
3179                 # NOTE(ft): This workaround must be removed when a correct
3180                 # implementation of resize operation changing sizes in bdms is
3181                 # developed. Also at that stage we probably may get rid of
3182                 # the direct usage of flavor swap size here,
3183                 # leaving the work with bdm only.
3184                 swap_mb = inst_type['swap']
3185             else:
3186                 swap = driver.block_device_info_get_swap(block_device_info)
3187                 if driver.swap_is_usable(swap):
3188                     swap_mb = swap['swap_size']
3189                 elif (inst_type['swap'] > 0 and
3190                       not block_device.volume_in_mapping(
3191                         mapping['dev'], block_device_info)):
3192                     swap_mb = inst_type['swap']
3193 
3194             if swap_mb > 0:
3195                 if (CONF.libvirt.virt_type == "parallels" and
3196                         instance.vm_mode == fields.VMMode.EXE):
3197                     msg = _("Swap disk is not supported "
3198                             "for Virtuozzo container")
3199                     raise exception.Invalid(msg)
3200 
3201         if not disk_images:
3202             disk_images = {'image_id': instance.image_ref,
3203                            'kernel_id': instance.kernel_id,
3204                            'ramdisk_id': instance.ramdisk_id}
3205 
3206         if disk_images['kernel_id']:
3207             fname = imagecache.get_cache_fname(disk_images['kernel_id'])
3208             raw('kernel').cache(fetch_func=libvirt_utils.fetch_raw_image,
3209                                 context=context,
3210                                 filename=fname,
3211                                 image_id=disk_images['kernel_id'])
3212             if disk_images['ramdisk_id']:
3213                 fname = imagecache.get_cache_fname(disk_images['ramdisk_id'])
3214                 raw('ramdisk').cache(fetch_func=libvirt_utils.fetch_raw_image,
3215                                      context=context,
3216                                      filename=fname,
3217                                      image_id=disk_images['ramdisk_id'])
3218 
3219         if CONF.libvirt.virt_type == 'uml':
3220             # PONDERING(mikal): can I assume that root is UID zero in every
3221             # OS? Probably not.
3222             uid = pwd.getpwnam('root').pw_uid
3223             nova.privsep.path.chown(image('disk').path, uid=uid)
3224 
3225         self._create_and_inject_local_root(context, instance,
3226                                            booted_from_volume, suffix,
3227                                            disk_images, injection_info,
3228                                            fallback_from_host)
3229 
3230         # Lookup the filesystem type if required
3231         os_type_with_default = disk_api.get_fs_type_for_os_type(
3232             instance.os_type)
3233         # Generate a file extension based on the file system
3234         # type and the mkfs commands configured if any
3235         file_extension = disk_api.get_file_extension_for_os_type(
3236                                                           os_type_with_default)
3237 
3238         vm_mode = fields.VMMode.get_from_instance(instance)
3239         ephemeral_gb = instance.flavor.ephemeral_gb
3240         if 'disk.local' in disk_mapping:
3241             disk_image = image('disk.local')
3242             fn = functools.partial(self._create_ephemeral,
3243                                    fs_label='ephemeral0',
3244                                    os_type=instance.os_type,
3245                                    is_block_dev=disk_image.is_block_dev,
3246                                    vm_mode=vm_mode)
3247             fname = "ephemeral_%s_%s" % (ephemeral_gb, file_extension)
3248             size = ephemeral_gb * units.Gi
3249             disk_image.cache(fetch_func=fn,
3250                              context=context,
3251                              filename=fname,
3252                              size=size,
3253                              ephemeral_size=ephemeral_gb)
3254 
3255         for idx, eph in enumerate(driver.block_device_info_get_ephemerals(
3256                 block_device_info)):
3257             disk_image = image(blockinfo.get_eph_disk(idx))
3258 
3259             specified_fs = eph.get('guest_format')
3260             if specified_fs and not self.is_supported_fs_format(specified_fs):
3261                 msg = _("%s format is not supported") % specified_fs
3262                 raise exception.InvalidBDMFormat(details=msg)
3263 
3264             fn = functools.partial(self._create_ephemeral,
3265                                    fs_label='ephemeral%d' % idx,
3266                                    os_type=instance.os_type,
3267                                    is_block_dev=disk_image.is_block_dev,
3268                                    vm_mode=vm_mode)
3269             size = eph['size'] * units.Gi
3270             fname = "ephemeral_%s_%s" % (eph['size'], file_extension)
3271             disk_image.cache(fetch_func=fn,
3272                              context=context,
3273                              filename=fname,
3274                              size=size,
3275                              ephemeral_size=eph['size'],
3276                              specified_fs=specified_fs)
3277 
3278         if swap_mb > 0:
3279             size = swap_mb * units.Mi
3280             image('disk.swap').cache(fetch_func=self._create_swap,
3281                                      context=context,
3282                                      filename="swap_%s" % swap_mb,
3283                                      size=size,
3284                                      swap_mb=swap_mb)
3285 
3286     def _create_and_inject_local_root(self, context, instance,
3287                                       booted_from_volume, suffix, disk_images,
3288                                       injection_info, fallback_from_host):
3289         # File injection only if needed
3290         need_inject = (not configdrive.required_by(instance) and
3291                        injection_info is not None and
3292                        CONF.libvirt.inject_partition != -2)
3293 
3294         # NOTE(ndipanov): Even if disk_mapping was passed in, which
3295         # currently happens only on rescue - we still don't want to
3296         # create a base image.
3297         if not booted_from_volume:
3298             root_fname = imagecache.get_cache_fname(disk_images['image_id'])
3299             size = instance.flavor.root_gb * units.Gi
3300 
3301             if size == 0 or suffix == '.rescue':
3302                 size = None
3303 
3304             backend = self.image_backend.by_name(instance, 'disk' + suffix,
3305                                                  CONF.libvirt.images_type)
3306             if instance.task_state == task_states.RESIZE_FINISH:
3307                 backend.create_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
3308             if backend.SUPPORTS_CLONE:
3309                 def clone_fallback_to_fetch(*args, **kwargs):
3310                     try:
3311                         backend.clone(context, disk_images['image_id'])
3312                     except exception.ImageUnacceptable:
3313                         libvirt_utils.fetch_image(*args, **kwargs)
3314                 fetch_func = clone_fallback_to_fetch
3315             else:
3316                 fetch_func = libvirt_utils.fetch_image
3317             self._try_fetch_image_cache(backend, fetch_func, context,
3318                                         root_fname, disk_images['image_id'],
3319                                         instance, size, fallback_from_host)
3320 
3321             if need_inject:
3322                 self._inject_data(backend, instance, injection_info)
3323 
3324         elif need_inject:
3325             LOG.warning('File injection into a boot from volume '
3326                         'instance is not supported', instance=instance)
3327 
3328     def _create_configdrive(self, context, instance, injection_info,
3329                             rescue=False):
3330         # As this method being called right after the definition of a
3331         # domain, but before its actual launch, device metadata will be built
3332         # and saved in the instance for it to be used by the config drive and
3333         # the metadata service.
3334         instance.device_metadata = self._build_device_metadata(context,
3335                                                                instance)
3336         if configdrive.required_by(instance):
3337             LOG.info('Using config drive', instance=instance)
3338 
3339             name = 'disk.config'
3340             if rescue:
3341                 name += '.rescue'
3342 
3343             config_disk = self.image_backend.by_name(
3344                 instance, name, self._get_disk_config_image_type())
3345 
3346             # Don't overwrite an existing config drive
3347             if not config_disk.exists():
3348                 extra_md = {}
3349                 if injection_info.admin_pass:
3350                     extra_md['admin_pass'] = injection_info.admin_pass
3351 
3352                 inst_md = instance_metadata.InstanceMetadata(
3353                     instance, content=injection_info.files, extra_md=extra_md,
3354                     network_info=injection_info.network_info,
3355                     request_context=context)
3356 
3357                 cdb = configdrive.ConfigDriveBuilder(instance_md=inst_md)
3358                 with cdb:
3359                     # NOTE(mdbooth): We're hardcoding here the path of the
3360                     # config disk when using the flat backend. This isn't
3361                     # good, but it's required because we need a local path we
3362                     # know we can write to in case we're subsequently
3363                     # importing into rbd. This will be cleaned up when we
3364                     # replace this with a call to create_from_func, but that
3365                     # can't happen until we've updated the backends and we
3366                     # teach them not to cache config disks. This isn't
3367                     # possible while we're still using cache() under the hood.
3368                     config_disk_local_path = os.path.join(
3369                         libvirt_utils.get_instance_path(instance), name)
3370                     LOG.info('Creating config drive at %(path)s',
3371                              {'path': config_disk_local_path},
3372                              instance=instance)
3373 
3374                     try:
3375                         cdb.make_drive(config_disk_local_path)
3376                     except processutils.ProcessExecutionError as e:
3377                         with excutils.save_and_reraise_exception():
3378                             LOG.error('Creating config drive failed with '
3379                                       'error: %s', e, instance=instance)
3380 
3381                 try:
3382                     config_disk.import_file(
3383                         instance, config_disk_local_path, name)
3384                 finally:
3385                     # NOTE(mikal): if the config drive was imported into RBD,
3386                     # then we no longer need the local copy
3387                     if CONF.libvirt.images_type == 'rbd':
3388                         LOG.info('Deleting local config drive %(path)s '
3389                                  'because it was imported into RBD.',
3390                                  {'path': config_disk_local_path},
3391                                  instance=instance)
3392                         os.unlink(config_disk_local_path)
3393 
3394     def _prepare_pci_devices_for_use(self, pci_devices):
3395         # kvm , qemu support managed mode
3396         # In managed mode, the configured device will be automatically
3397         # detached from the host OS drivers when the guest is started,
3398         # and then re-attached when the guest shuts down.
3399         if CONF.libvirt.virt_type != 'xen':
3400             # we do manual detach only for xen
3401             return
3402         try:
3403             for dev in pci_devices:
3404                 libvirt_dev_addr = dev['hypervisor_name']
3405                 libvirt_dev = \
3406                         self._host.device_lookup_by_name(libvirt_dev_addr)
3407                 # Note(yjiang5) Spelling for 'dettach' is correct, see
3408                 # http://libvirt.org/html/libvirt-libvirt.html.
3409                 libvirt_dev.dettach()
3410 
3411             # Note(yjiang5): A reset of one PCI device may impact other
3412             # devices on the same bus, thus we need two separated loops
3413             # to detach and then reset it.
3414             for dev in pci_devices:
3415                 libvirt_dev_addr = dev['hypervisor_name']
3416                 libvirt_dev = \
3417                         self._host.device_lookup_by_name(libvirt_dev_addr)
3418                 libvirt_dev.reset()
3419 
3420         except libvirt.libvirtError as exc:
3421             raise exception.PciDevicePrepareFailed(id=dev['id'],
3422                                                    instance_uuid=
3423                                                    dev['instance_uuid'],
3424                                                    reason=six.text_type(exc))
3425 
3426     def _detach_pci_devices(self, guest, pci_devs):
3427         try:
3428             for dev in pci_devs:
3429                 guest.detach_device(self._get_guest_pci_device(dev), live=True)
3430                 # after detachDeviceFlags returned, we should check the dom to
3431                 # ensure the detaching is finished
3432                 xml = guest.get_xml_desc()
3433                 xml_doc = etree.fromstring(xml)
3434                 guest_config = vconfig.LibvirtConfigGuest()
3435                 guest_config.parse_dom(xml_doc)
3436 
3437                 for hdev in [d for d in guest_config.devices
3438                     if isinstance(d, vconfig.LibvirtConfigGuestHostdevPCI)]:
3439                     hdbsf = [hdev.domain, hdev.bus, hdev.slot, hdev.function]
3440                     dbsf = pci_utils.parse_address(dev.address)
3441                     if [int(x, 16) for x in hdbsf] ==\
3442                             [int(x, 16) for x in dbsf]:
3443                         raise exception.PciDeviceDetachFailed(reason=
3444                                                               "timeout",
3445                                                               dev=dev)
3446 
3447         except libvirt.libvirtError as ex:
3448             error_code = ex.get_error_code()
3449             if error_code == libvirt.VIR_ERR_NO_DOMAIN:
3450                 LOG.warning("Instance disappeared while detaching "
3451                             "a PCI device from it.")
3452             else:
3453                 raise
3454 
3455     def _attach_pci_devices(self, guest, pci_devs):
3456         try:
3457             for dev in pci_devs:
3458                 guest.attach_device(self._get_guest_pci_device(dev))
3459 
3460         except libvirt.libvirtError:
3461             LOG.error('Attaching PCI devices %(dev)s to %(dom)s failed.',
3462                       {'dev': pci_devs, 'dom': guest.id})
3463             raise
3464 
3465     @staticmethod
3466     def _has_direct_passthrough_port(network_info):
3467         for vif in network_info:
3468             if (vif['vnic_type'] in
3469                 network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3470                 return True
3471         return False
3472 
3473     def _attach_direct_passthrough_ports(
3474         self, context, instance, guest, network_info=None):
3475         if network_info is None:
3476             network_info = instance.info_cache.network_info
3477         if network_info is None:
3478             return
3479 
3480         if self._has_direct_passthrough_port(network_info):
3481             for vif in network_info:
3482                 if (vif['vnic_type'] in
3483                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH):
3484                     cfg = self.vif_driver.get_config(instance,
3485                                                      vif,
3486                                                      instance.image_meta,
3487                                                      instance.flavor,
3488                                                      CONF.libvirt.virt_type,
3489                                                      self._host)
3490                     LOG.debug('Attaching direct passthrough port %(port)s '
3491                               'to %(dom)s', {'port': vif, 'dom': guest.id},
3492                               instance=instance)
3493                     guest.attach_device(cfg)
3494 
3495     def _detach_direct_passthrough_ports(self, context, instance, guest):
3496         network_info = instance.info_cache.network_info
3497         if network_info is None:
3498             return
3499 
3500         if self._has_direct_passthrough_port(network_info):
3501             # In case of VNIC_TYPES_DIRECT_PASSTHROUGH ports we create
3502             # pci request per direct passthrough port. Therefore we can trust
3503             # that pci_slot value in the vif is correct.
3504             direct_passthrough_pci_addresses = [
3505                 vif['profile']['pci_slot']
3506                 for vif in network_info
3507                 if (vif['vnic_type'] in
3508                     network_model.VNIC_TYPES_DIRECT_PASSTHROUGH and
3509                     vif['profile'].get('pci_slot') is not None)
3510             ]
3511 
3512             # use detach_pci_devices to avoid failure in case of
3513             # multiple guest direct passthrough ports with the same MAC
3514             # (protection use-case, ports are on different physical
3515             # interfaces)
3516             pci_devs = pci_manager.get_instance_pci_devs(instance, 'all')
3517             direct_passthrough_pci_addresses = (
3518                 [pci_dev for pci_dev in pci_devs
3519                  if pci_dev.address in direct_passthrough_pci_addresses])
3520             self._detach_pci_devices(guest, direct_passthrough_pci_addresses)
3521 
3522     def _set_host_enabled(self, enabled,
3523                           disable_reason=DISABLE_REASON_UNDEFINED):
3524         """Enables / Disables the compute service on this host.
3525 
3526            This doesn't override non-automatic disablement with an automatic
3527            setting; thereby permitting operators to keep otherwise
3528            healthy hosts out of rotation.
3529         """
3530 
3531         status_name = {True: 'disabled',
3532                        False: 'enabled'}
3533 
3534         disable_service = not enabled
3535 
3536         ctx = nova_context.get_admin_context()
3537         try:
3538             service = objects.Service.get_by_compute_host(ctx, CONF.host)
3539 
3540             if service.disabled != disable_service:
3541                 # Note(jang): this is a quick fix to stop operator-
3542                 # disabled compute hosts from re-enabling themselves
3543                 # automatically. We prefix any automatic reason code
3544                 # with a fixed string. We only re-enable a host
3545                 # automatically if we find that string in place.
3546                 # This should probably be replaced with a separate flag.
3547                 if not service.disabled or (
3548                         service.disabled_reason and
3549                         service.disabled_reason.startswith(DISABLE_PREFIX)):
3550                     service.disabled = disable_service
3551                     service.disabled_reason = (
3552                        DISABLE_PREFIX + disable_reason
3553                        if disable_service and disable_reason else
3554                            DISABLE_REASON_UNDEFINED)
3555                     service.save()
3556                     LOG.debug('Updating compute service status to %s',
3557                               status_name[disable_service])
3558                 else:
3559                     LOG.debug('Not overriding manual compute service '
3560                               'status with: %s',
3561                               status_name[disable_service])
3562         except exception.ComputeHostNotFound:
3563             LOG.warning('Cannot update service status on host "%s" '
3564                         'since it is not registered.', CONF.host)
3565         except Exception:
3566             LOG.warning('Cannot update service status on host "%s" '
3567                         'due to an unexpected exception.', CONF.host,
3568                         exc_info=True)
3569 
3570         if enabled:
3571             mount.get_manager().host_up(self._host)
3572         else:
3573             mount.get_manager().host_down()
3574 
3575     def _get_guest_cpu_model_config(self):
3576         mode = CONF.libvirt.cpu_mode
3577         model = CONF.libvirt.cpu_model
3578 
3579         if (CONF.libvirt.virt_type == "kvm" or
3580             CONF.libvirt.virt_type == "qemu"):
3581             if mode is None:
3582                 mode = "host-model"
3583             if mode == "none":
3584                 return vconfig.LibvirtConfigGuestCPU()
3585         else:
3586             if mode is None or mode == "none":
3587                 return None
3588 
3589         if ((CONF.libvirt.virt_type != "kvm" and
3590              CONF.libvirt.virt_type != "qemu")):
3591             msg = _("Config requested an explicit CPU model, but "
3592                     "the current libvirt hypervisor '%s' does not "
3593                     "support selecting CPU models") % CONF.libvirt.virt_type
3594             raise exception.Invalid(msg)
3595 
3596         if mode == "custom" and model is None:
3597             msg = _("Config requested a custom CPU model, but no "
3598                     "model name was provided")
3599             raise exception.Invalid(msg)
3600         elif mode != "custom" and model is not None:
3601             msg = _("A CPU model name should not be set when a "
3602                     "host CPU model is requested")
3603             raise exception.Invalid(msg)
3604 
3605         LOG.debug("CPU mode '%(mode)s' model '%(model)s' was chosen",
3606                   {'mode': mode, 'model': (model or "")})
3607 
3608         cpu = vconfig.LibvirtConfigGuestCPU()
3609         cpu.mode = mode
3610         cpu.model = model
3611 
3612         return cpu
3613 
3614     def _get_guest_cpu_config(self, flavor, image_meta,
3615                               guest_cpu_numa_config, instance_numa_topology):
3616         cpu = self._get_guest_cpu_model_config()
3617 
3618         if cpu is None:
3619             return None
3620 
3621         topology = hardware.get_best_cpu_topology(
3622                 flavor, image_meta, numa_topology=instance_numa_topology)
3623 
3624         cpu.sockets = topology.sockets
3625         cpu.cores = topology.cores
3626         cpu.threads = topology.threads
3627         cpu.numa = guest_cpu_numa_config
3628 
3629         return cpu
3630 
3631     def _get_guest_disk_config(self, instance, name, disk_mapping, inst_type,
3632                                image_type=None):
3633         disk_unit = None
3634         disk = self.image_backend.by_name(instance, name, image_type)
3635         if (name == 'disk.config' and image_type == 'rbd' and
3636                 not disk.exists()):
3637             # This is likely an older config drive that has not been migrated
3638             # to rbd yet. Try to fall back on 'flat' image type.
3639             # TODO(melwitt): Add online migration of some sort so we can
3640             # remove this fall back once we know all config drives are in rbd.
3641             # NOTE(vladikr): make sure that the flat image exist, otherwise
3642             # the image will be created after the domain definition.
3643             flat_disk = self.image_backend.by_name(instance, name, 'flat')
3644             if flat_disk.exists():
3645                 disk = flat_disk
3646                 LOG.debug('Config drive not found in RBD, falling back to the '
3647                           'instance directory', instance=instance)
3648         disk_info = disk_mapping[name]
3649         if 'unit' in disk_mapping:
3650             disk_unit = disk_mapping['unit']
3651             disk_mapping['unit'] += 1  # Increments for the next disk added
3652         conf = disk.libvirt_info(disk_info['bus'],
3653                                  disk_info['dev'],
3654                                  disk_info['type'],
3655                                  self.disk_cachemode,
3656                                  inst_type['extra_specs'],
3657                                  self._host.get_version(),
3658                                  disk_unit=disk_unit)
3659         return conf
3660 
3661     def _get_guest_fs_config(self, instance, name, image_type=None):
3662         disk = self.image_backend.by_name(instance, name, image_type)
3663         return disk.libvirt_fs_info("/", "ploop")
3664 
3665     def _get_guest_storage_config(self, instance, image_meta,
3666                                   disk_info,
3667                                   rescue, block_device_info,
3668                                   inst_type, os_type):
3669         devices = []
3670         disk_mapping = disk_info['mapping']
3671 
3672         block_device_mapping = driver.block_device_info_get_mapping(
3673             block_device_info)
3674         mount_rootfs = CONF.libvirt.virt_type == "lxc"
3675         scsi_controller = self._get_scsi_controller(image_meta)
3676 
3677         if scsi_controller and scsi_controller.model == 'virtio-scsi':
3678             # The virtio-scsi can handle up to 256 devices but the
3679             # optional element "address" must be defined to describe
3680             # where the device is placed on the controller (see:
3681             # LibvirtConfigGuestDeviceAddressDrive).
3682             #
3683             # Note about why it's added in disk_mapping: It's not
3684             # possible to pass an 'int' by reference in Python, so we
3685             # use disk_mapping as container to keep reference of the
3686             # unit added and be able to increment it for each disk
3687             # added.
3688             disk_mapping['unit'] = 0
3689 
3690         def _get_ephemeral_devices():
3691             eph_devices = []
3692             for idx, eph in enumerate(
3693                 driver.block_device_info_get_ephemerals(
3694                     block_device_info)):
3695                 diskeph = self._get_guest_disk_config(
3696                     instance,
3697                     blockinfo.get_eph_disk(idx),
3698                     disk_mapping, inst_type)
3699                 eph_devices.append(diskeph)
3700             return eph_devices
3701 
3702         if mount_rootfs:
3703             fs = vconfig.LibvirtConfigGuestFilesys()
3704             fs.source_type = "mount"
3705             fs.source_dir = os.path.join(
3706                 libvirt_utils.get_instance_path(instance), 'rootfs')
3707             devices.append(fs)
3708         elif (os_type == fields.VMMode.EXE and
3709               CONF.libvirt.virt_type == "parallels"):
3710             if rescue:
3711                 fsrescue = self._get_guest_fs_config(instance, "disk.rescue")
3712                 devices.append(fsrescue)
3713 
3714                 fsos = self._get_guest_fs_config(instance, "disk")
3715                 fsos.target_dir = "/mnt/rescue"
3716                 devices.append(fsos)
3717             else:
3718                 if 'disk' in disk_mapping:
3719                     fs = self._get_guest_fs_config(instance, "disk")
3720                     devices.append(fs)
3721                 devices = devices + _get_ephemeral_devices()
3722         else:
3723 
3724             if rescue:
3725                 diskrescue = self._get_guest_disk_config(instance,
3726                                                          'disk.rescue',
3727                                                          disk_mapping,
3728                                                          inst_type)
3729                 devices.append(diskrescue)
3730 
3731                 diskos = self._get_guest_disk_config(instance,
3732                                                      'disk',
3733                                                      disk_mapping,
3734                                                      inst_type)
3735                 devices.append(diskos)
3736             else:
3737                 if 'disk' in disk_mapping:
3738                     diskos = self._get_guest_disk_config(instance,
3739                                                          'disk',
3740                                                          disk_mapping,
3741                                                          inst_type)
3742                     devices.append(diskos)
3743 
3744                 if 'disk.local' in disk_mapping:
3745                     disklocal = self._get_guest_disk_config(instance,
3746                                                             'disk.local',
3747                                                             disk_mapping,
3748                                                             inst_type)
3749                     devices.append(disklocal)
3750                     instance.default_ephemeral_device = (
3751                         block_device.prepend_dev(disklocal.target_dev))
3752 
3753                 devices = devices + _get_ephemeral_devices()
3754 
3755                 if 'disk.swap' in disk_mapping:
3756                     diskswap = self._get_guest_disk_config(instance,
3757                                                            'disk.swap',
3758                                                            disk_mapping,
3759                                                            inst_type)
3760                     devices.append(diskswap)
3761                     instance.default_swap_device = (
3762                         block_device.prepend_dev(diskswap.target_dev))
3763 
3764             config_name = 'disk.config.rescue' if rescue else 'disk.config'
3765             if config_name in disk_mapping:
3766                 diskconfig = self._get_guest_disk_config(
3767                     instance, config_name, disk_mapping, inst_type,
3768                     self._get_disk_config_image_type())
3769                 devices.append(diskconfig)
3770 
3771         for vol in block_device.get_bdms_to_connect(block_device_mapping,
3772                                                    mount_rootfs):
3773             connection_info = vol['connection_info']
3774             vol_dev = block_device.prepend_dev(vol['mount_device'])
3775             info = disk_mapping[vol_dev]
3776             self._connect_volume(connection_info, info, instance)
3777             if scsi_controller and scsi_controller.model == 'virtio-scsi':
3778                 info['unit'] = disk_mapping['unit']
3779                 disk_mapping['unit'] += 1
3780             cfg = self._get_volume_config(connection_info, info)
3781             devices.append(cfg)
3782             vol['connection_info'] = connection_info
3783             vol.save()
3784 
3785         if scsi_controller:
3786             devices.append(scsi_controller)
3787 
3788         return devices
3789 
3790     @staticmethod
3791     def _get_scsi_controller(image_meta):
3792         """Return scsi controller or None based on image meta"""
3793         # TODO(sahid): should raise an exception for an invalid controller
3794         if image_meta.properties.get('hw_scsi_model'):
3795             hw_scsi_model = image_meta.properties.hw_scsi_model
3796             scsi_controller = vconfig.LibvirtConfigGuestController()
3797             scsi_controller.type = 'scsi'
3798             scsi_controller.model = hw_scsi_model
3799             scsi_controller.index = 0
3800             return scsi_controller
3801 
3802     def _get_host_sysinfo_serial_hardware(self):
3803         """Get a UUID from the host hardware
3804 
3805         Get a UUID for the host hardware reported by libvirt.
3806         This is typically from the SMBIOS data, unless it has
3807         been overridden in /etc/libvirt/libvirtd.conf
3808         """
3809         caps = self._host.get_capabilities()
3810         return caps.host.uuid
3811 
3812     def _get_host_sysinfo_serial_os(self):
3813         """Get a UUID from the host operating system
3814 
3815         Get a UUID for the host operating system. Modern Linux
3816         distros based on systemd provide a /etc/machine-id
3817         file containing a UUID. This is also provided inside
3818         systemd based containers and can be provided by other
3819         init systems too, since it is just a plain text file.
3820         """
3821         if not os.path.exists("/etc/machine-id"):
3822             msg = _("Unable to get host UUID: /etc/machine-id does not exist")
3823             raise exception.InternalError(msg)
3824 
3825         with open("/etc/machine-id") as f:
3826             # We want to have '-' in the right place
3827             # so we parse & reformat the value
3828             lines = f.read().split()
3829             if not lines:
3830                 msg = _("Unable to get host UUID: /etc/machine-id is empty")
3831                 raise exception.InternalError(msg)
3832 
3833             return str(uuid.UUID(lines[0]))
3834 
3835     def _get_host_sysinfo_serial_auto(self):
3836         if os.path.exists("/etc/machine-id"):
3837             return self._get_host_sysinfo_serial_os()
3838         else:
3839             return self._get_host_sysinfo_serial_hardware()
3840 
3841     def _get_guest_config_sysinfo(self, instance):
3842         sysinfo = vconfig.LibvirtConfigGuestSysinfo()
3843 
3844         sysinfo.system_manufacturer = version.vendor_string()
3845         sysinfo.system_product = version.product_string()
3846         sysinfo.system_version = version.version_string_with_package()
3847 
3848         sysinfo.system_serial = self._sysinfo_serial_func()
3849         sysinfo.system_uuid = instance.uuid
3850 
3851         sysinfo.system_family = "Virtual Machine"
3852 
3853         return sysinfo
3854 
3855     def _get_guest_pci_device(self, pci_device):
3856 
3857         dbsf = pci_utils.parse_address(pci_device.address)
3858         dev = vconfig.LibvirtConfigGuestHostdevPCI()
3859         dev.domain, dev.bus, dev.slot, dev.function = dbsf
3860 
3861         # only kvm support managed mode
3862         if CONF.libvirt.virt_type in ('xen', 'parallels',):
3863             dev.managed = 'no'
3864         if CONF.libvirt.virt_type in ('kvm', 'qemu'):
3865             dev.managed = 'yes'
3866 
3867         return dev
3868 
3869     def _get_guest_config_meta(self, instance):
3870         """Get metadata config for guest."""
3871 
3872         meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
3873         meta.package = version.version_string_with_package()
3874         meta.name = instance.display_name
3875         meta.creationTime = time.time()
3876 
3877         if instance.image_ref not in ("", None):
3878             meta.roottype = "image"
3879             meta.rootid = instance.image_ref
3880 
3881         system_meta = instance.system_metadata
3882         ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
3883         ometa.userid = instance.user_id
3884         ometa.username = system_meta.get('owner_user_name', 'N/A')
3885         ometa.projectid = instance.project_id
3886         ometa.projectname = system_meta.get('owner_project_name', 'N/A')
3887         meta.owner = ometa
3888 
3889         fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
3890         flavor = instance.flavor
3891         fmeta.name = flavor.name
3892         fmeta.memory = flavor.memory_mb
3893         fmeta.vcpus = flavor.vcpus
3894         fmeta.ephemeral = flavor.ephemeral_gb
3895         fmeta.disk = flavor.root_gb
3896         fmeta.swap = flavor.swap
3897 
3898         meta.flavor = fmeta
3899 
3900         return meta
3901 
3902     def _machine_type_mappings(self):
3903         mappings = {}
3904         for mapping in CONF.libvirt.hw_machine_type:
3905             host_arch, _, machine_type = mapping.partition('=')
3906             mappings[host_arch] = machine_type
3907         return mappings
3908 
3909     def _get_machine_type(self, image_meta, caps):
3910         # The underlying machine type can be set as an image attribute,
3911         # or otherwise based on some architecture specific defaults
3912 
3913         mach_type = None
3914 
3915         if image_meta.properties.get('hw_machine_type') is not None:
3916             mach_type = image_meta.properties.hw_machine_type
3917         else:
3918             # For ARM systems we will default to vexpress-a15 for armv7
3919             # and virt for aarch64
3920             if caps.host.cpu.arch == fields.Architecture.ARMV7:
3921                 mach_type = "vexpress-a15"
3922 
3923             if caps.host.cpu.arch == fields.Architecture.AARCH64:
3924                 mach_type = "virt"
3925 
3926             if caps.host.cpu.arch in (fields.Architecture.S390,
3927                                       fields.Architecture.S390X):
3928                 mach_type = 's390-ccw-virtio'
3929 
3930             # If set in the config, use that as the default.
3931             if CONF.libvirt.hw_machine_type:
3932                 mappings = self._machine_type_mappings()
3933                 mach_type = mappings.get(caps.host.cpu.arch)
3934 
3935         return mach_type
3936 
3937     @staticmethod
3938     def _create_idmaps(klass, map_strings):
3939         idmaps = []
3940         if len(map_strings) > 5:
3941             map_strings = map_strings[0:5]
3942             LOG.warning("Too many id maps, only included first five.")
3943         for map_string in map_strings:
3944             try:
3945                 idmap = klass()
3946                 values = [int(i) for i in map_string.split(":")]
3947                 idmap.start = values[0]
3948                 idmap.target = values[1]
3949                 idmap.count = values[2]
3950                 idmaps.append(idmap)
3951             except (ValueError, IndexError):
3952                 LOG.warning("Invalid value for id mapping %s", map_string)
3953         return idmaps
3954 
3955     def _get_guest_idmaps(self):
3956         id_maps = []
3957         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.uid_maps:
3958             uid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestUIDMap,
3959                                            CONF.libvirt.uid_maps)
3960             id_maps.extend(uid_maps)
3961         if CONF.libvirt.virt_type == 'lxc' and CONF.libvirt.gid_maps:
3962             gid_maps = self._create_idmaps(vconfig.LibvirtConfigGuestGIDMap,
3963                                            CONF.libvirt.gid_maps)
3964             id_maps.extend(gid_maps)
3965         return id_maps
3966 
3967     def _update_guest_cputune(self, guest, flavor, virt_type):
3968         is_able = self._host.is_cpu_control_policy_capable()
3969 
3970         cputuning = ['shares', 'period', 'quota']
3971         wants_cputune = any([k for k in cputuning
3972             if "quota:cpu_" + k in flavor.extra_specs.keys()])
3973 
3974         if wants_cputune and not is_able:
3975             raise exception.UnsupportedHostCPUControlPolicy()
3976 
3977         if not is_able or virt_type not in ('lxc', 'kvm', 'qemu'):
3978             return
3979 
3980         if guest.cputune is None:
3981             guest.cputune = vconfig.LibvirtConfigGuestCPUTune()
3982             # Setting the default cpu.shares value to be a value
3983             # dependent on the number of vcpus
3984         guest.cputune.shares = 1024 * guest.vcpus
3985 
3986         for name in cputuning:
3987             key = "quota:cpu_" + name
3988             if key in flavor.extra_specs:
3989                 setattr(guest.cputune, name,
3990                         int(flavor.extra_specs[key]))
3991 
3992     def _get_cpu_numa_config_from_instance(self, instance_numa_topology,
3993                                            wants_hugepages):
3994         if instance_numa_topology:
3995             guest_cpu_numa = vconfig.LibvirtConfigGuestCPUNUMA()
3996             for instance_cell in instance_numa_topology.cells:
3997                 guest_cell = vconfig.LibvirtConfigGuestCPUNUMACell()
3998                 guest_cell.id = instance_cell.id
3999                 guest_cell.cpus = instance_cell.cpuset
4000                 guest_cell.memory = instance_cell.memory * units.Ki
4001 
4002                 # The vhost-user network backend requires file backed
4003                 # guest memory (ie huge pages) to be marked as shared
4004                 # access, not private, so an external process can read
4005                 # and write the pages.
4006                 #
4007                 # You can't change the shared vs private flag for an
4008                 # already running guest, and since we can't predict what
4009                 # types of NIC may be hotplugged, we have no choice but
4010                 # to unconditionally turn on the shared flag. This has
4011                 # no real negative functional effect on the guest, so
4012                 # is a reasonable approach to take
4013                 if wants_hugepages:
4014                     guest_cell.memAccess = "shared"
4015                 guest_cpu_numa.cells.append(guest_cell)
4016             return guest_cpu_numa
4017 
4018     def _has_cpu_policy_support(self):
4019         for ver in BAD_LIBVIRT_CPU_POLICY_VERSIONS:
4020             if self._host.has_version(ver):
4021                 ver_ = self._version_to_string(ver)
4022                 raise exception.CPUPinningNotSupported(reason=_(
4023                     'Invalid libvirt version %(version)s') % {'version': ver_})
4024         return True
4025 
4026     def _wants_hugepages(self, host_topology, instance_topology):
4027         """Determine if the guest / host topology implies the
4028            use of huge pages for guest RAM backing
4029         """
4030 
4031         if host_topology is None or instance_topology is None:
4032             return False
4033 
4034         avail_pagesize = [page.size_kb
4035                           for page in host_topology.cells[0].mempages]
4036         avail_pagesize.sort()
4037         # Remove smallest page size as that's not classed as a largepage
4038         avail_pagesize = avail_pagesize[1:]
4039 
4040         # See if we have page size set
4041         for cell in instance_topology.cells:
4042             if (cell.pagesize is not None and
4043                 cell.pagesize in avail_pagesize):
4044                 return True
4045 
4046         return False
4047 
4048     def _get_guest_numa_config(self, instance_numa_topology, flavor,
4049                                allowed_cpus=None, image_meta=None):
4050         """Returns the config objects for the guest NUMA specs.
4051 
4052         Determines the CPUs that the guest can be pinned to if the guest
4053         specifies a cell topology and the host supports it. Constructs the
4054         libvirt XML config object representing the NUMA topology selected
4055         for the guest. Returns a tuple of:
4056 
4057             (cpu_set, guest_cpu_tune, guest_cpu_numa, guest_numa_tune)
4058 
4059         With the following caveats:
4060 
4061             a) If there is no specified guest NUMA topology, then
4062                all tuple elements except cpu_set shall be None. cpu_set
4063                will be populated with the chosen CPUs that the guest
4064                allowed CPUs fit within, which could be the supplied
4065                allowed_cpus value if the host doesn't support NUMA
4066                topologies.
4067 
4068             b) If there is a specified guest NUMA topology, then
4069                cpu_set will be None and guest_cpu_numa will be the
4070                LibvirtConfigGuestCPUNUMA object representing the guest's
4071                NUMA topology. If the host supports NUMA, then guest_cpu_tune
4072                will contain a LibvirtConfigGuestCPUTune object representing
4073                the optimized chosen cells that match the host capabilities
4074                with the instance's requested topology. If the host does
4075                not support NUMA, then guest_cpu_tune and guest_numa_tune
4076                will be None.
4077         """
4078 
4079         if (not self._has_numa_support() and
4080                 instance_numa_topology is not None):
4081             # We should not get here, since we should have avoided
4082             # reporting NUMA topology from _get_host_numa_topology
4083             # in the first place. Just in case of a scheduler
4084             # mess up though, raise an exception
4085             raise exception.NUMATopologyUnsupported()
4086 
4087         topology = self._get_host_numa_topology()
4088 
4089         # We have instance NUMA so translate it to the config class
4090         guest_cpu_numa_config = self._get_cpu_numa_config_from_instance(
4091                 instance_numa_topology,
4092                 self._wants_hugepages(topology, instance_numa_topology))
4093 
4094         if not guest_cpu_numa_config:
4095             # No NUMA topology defined for instance - let the host kernel deal
4096             # with the NUMA effects.
4097             # TODO(ndipanov): Attempt to spread the instance
4098             # across NUMA nodes and expose the topology to the
4099             # instance as an optimisation
4100             return GuestNumaConfig(allowed_cpus, None, None, None)
4101         else:
4102             if topology:
4103                 # Now get the CpuTune configuration from the numa_topology
4104                 guest_cpu_tune = vconfig.LibvirtConfigGuestCPUTune()
4105                 guest_numa_tune = vconfig.LibvirtConfigGuestNUMATune()
4106                 emupcpus = []
4107 
4108                 numa_mem = vconfig.LibvirtConfigGuestNUMATuneMemory()
4109                 numa_memnodes = [vconfig.LibvirtConfigGuestNUMATuneMemNode()
4110                                  for _ in guest_cpu_numa_config.cells]
4111 
4112                 emulator_threads_isolated = (
4113                     instance_numa_topology.emulator_threads_isolated)
4114 
4115                 vcpus_rt = set([])
4116                 wants_realtime = hardware.is_realtime_enabled(flavor)
4117                 if wants_realtime:
4118                     if not self._host.has_min_version(
4119                             MIN_LIBVIRT_REALTIME_VERSION):
4120                         raise exception.RealtimePolicyNotSupported()
4121                     # Prepare realtime config for libvirt
4122                     vcpus_rt = hardware.vcpus_realtime_topology(
4123                         flavor, image_meta)
4124                     vcpusched = vconfig.LibvirtConfigGuestCPUTuneVCPUSched()
4125                     vcpusched.vcpus = vcpus_rt
4126                     vcpusched.scheduler = "fifo"
4127                     vcpusched.priority = (
4128                         CONF.libvirt.realtime_scheduler_priority)
4129                     guest_cpu_tune.vcpusched.append(vcpusched)
4130 
4131                 # TODO(sahid): Defining domain topology should be
4132                 # refactored.
4133                 for host_cell in topology.cells:
4134                     for guest_node_id, guest_config_cell in enumerate(
4135                             guest_cpu_numa_config.cells):
4136                         if guest_config_cell.id == host_cell.id:
4137                             node = numa_memnodes[guest_node_id]
4138                             node.cellid = guest_node_id
4139                             node.nodeset = [host_cell.id]
4140                             node.mode = "strict"
4141 
4142                             numa_mem.nodeset.append(host_cell.id)
4143 
4144                             object_numa_cell = (
4145                                     instance_numa_topology.cells[guest_node_id]
4146                                 )
4147                             for cpu in guest_config_cell.cpus:
4148                                 pin_cpuset = (
4149                                     vconfig.LibvirtConfigGuestCPUTuneVCPUPin())
4150                                 pin_cpuset.id = cpu
4151                                 # If there is pinning information in the cell
4152                                 # we pin to individual CPUs, otherwise we float
4153                                 # over the whole host NUMA node
4154 
4155                                 if (object_numa_cell.cpu_pinning and
4156                                         self._has_cpu_policy_support()):
4157                                     pcpu = object_numa_cell.cpu_pinning[cpu]
4158                                     pin_cpuset.cpuset = set([pcpu])
4159                                 else:
4160                                     pin_cpuset.cpuset = host_cell.cpuset
4161                                 if emulator_threads_isolated:
4162                                     emupcpus.extend(
4163                                         object_numa_cell.cpuset_reserved)
4164                                 elif not wants_realtime or cpu not in vcpus_rt:
4165                                     # - If realtime IS NOT enabled, the
4166                                     #   emulator threads are allowed to float
4167                                     #   across all the pCPUs associated with
4168                                     #   the guest vCPUs ("not wants_realtime"
4169                                     #   is true, so we add all pcpus)
4170                                     # - If realtime IS enabled, then at least
4171                                     #   1 vCPU is required to be set aside for
4172                                     #   non-realtime usage. The emulator
4173                                     #   threads are allowed to float acros the
4174                                     #   pCPUs that are associated with the
4175                                     #   non-realtime VCPUs (the "cpu not in
4176                                     #   vcpu_rt" check deals with this
4177                                     #   filtering)
4178                                     emupcpus.extend(pin_cpuset.cpuset)
4179                                 guest_cpu_tune.vcpupin.append(pin_cpuset)
4180 
4181                 # TODO(berrange) When the guest has >1 NUMA node, it will
4182                 # span multiple host NUMA nodes. By pinning emulator threads
4183                 # to the union of all nodes, we guarantee there will be
4184                 # cross-node memory access by the emulator threads when
4185                 # responding to guest I/O operations. The only way to avoid
4186                 # this would be to pin emulator threads to a single node and
4187                 # tell the guest OS to only do I/O from one of its virtual
4188                 # NUMA nodes. This is not even remotely practical.
4189                 #
4190                 # The long term solution is to make use of a new QEMU feature
4191                 # called "I/O Threads" which will let us configure an explicit
4192                 # I/O thread for each guest vCPU or guest NUMA node. It is
4193                 # still TBD how to make use of this feature though, especially
4194                 # how to associate IO threads with guest devices to eliminate
4195                 # cross NUMA node traffic. This is an area of investigation
4196                 # for QEMU community devs.
4197                 emulatorpin = vconfig.LibvirtConfigGuestCPUTuneEmulatorPin()
4198                 emulatorpin.cpuset = set(emupcpus)
4199                 guest_cpu_tune.emulatorpin = emulatorpin
4200                 # Sort the vcpupin list per vCPU id for human-friendlier XML
4201                 guest_cpu_tune.vcpupin.sort(key=operator.attrgetter("id"))
4202 
4203                 guest_numa_tune.memory = numa_mem
4204                 guest_numa_tune.memnodes = numa_memnodes
4205 
4206                 # normalize cell.id
4207                 for i, (cell, memnode) in enumerate(
4208                                             zip(guest_cpu_numa_config.cells,
4209                                                 guest_numa_tune.memnodes)):
4210                     cell.id = i
4211                     memnode.cellid = i
4212 
4213                 return GuestNumaConfig(None, guest_cpu_tune,
4214                                        guest_cpu_numa_config,
4215                                        guest_numa_tune)
4216             else:
4217                 return GuestNumaConfig(allowed_cpus, None,
4218                                        guest_cpu_numa_config, None)
4219 
4220     def _get_guest_os_type(self, virt_type):
4221         """Returns the guest OS type based on virt type."""
4222         if virt_type == "lxc":
4223             ret = fields.VMMode.EXE
4224         elif virt_type == "uml":
4225             ret = fields.VMMode.UML
4226         elif virt_type == "xen":
4227             ret = fields.VMMode.XEN
4228         else:
4229             ret = fields.VMMode.HVM
4230         return ret
4231 
4232     def _set_guest_for_rescue(self, rescue, guest, inst_path, virt_type,
4233                               root_device_name):
4234         if rescue.get('kernel_id'):
4235             guest.os_kernel = os.path.join(inst_path, "kernel.rescue")
4236             guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4237             if virt_type == "qemu":
4238                 guest.os_cmdline += " no_timer_check"
4239         if rescue.get('ramdisk_id'):
4240             guest.os_initrd = os.path.join(inst_path, "ramdisk.rescue")
4241 
4242     def _set_guest_for_inst_kernel(self, instance, guest, inst_path, virt_type,
4243                                 root_device_name, image_meta):
4244         guest.os_kernel = os.path.join(inst_path, "kernel")
4245         guest.os_cmdline = ("root=%s %s" % (root_device_name, CONSOLE))
4246         if virt_type == "qemu":
4247             guest.os_cmdline += " no_timer_check"
4248         if instance.ramdisk_id:
4249             guest.os_initrd = os.path.join(inst_path, "ramdisk")
4250         # we only support os_command_line with images with an explicit
4251         # kernel set and don't want to break nova if there's an
4252         # os_command_line property without a specified kernel_id param
4253         if image_meta.properties.get("os_command_line"):
4254             guest.os_cmdline = image_meta.properties.os_command_line
4255 
4256     def _set_clock(self, guest, os_type, image_meta, virt_type):
4257         # NOTE(mikal): Microsoft Windows expects the clock to be in
4258         # "localtime". If the clock is set to UTC, then you can use a
4259         # registry key to let windows know, but Microsoft says this is
4260         # buggy in http://support.microsoft.com/kb/2687252
4261         clk = vconfig.LibvirtConfigGuestClock()
4262         if os_type == 'windows':
4263             LOG.info('Configuring timezone for windows instance to localtime')
4264             clk.offset = 'localtime'
4265         else:
4266             clk.offset = 'utc'
4267         guest.set_clock(clk)
4268 
4269         if virt_type == "kvm":
4270             self._set_kvm_timers(clk, os_type, image_meta)
4271 
4272     def _set_kvm_timers(self, clk, os_type, image_meta):
4273         # TODO(berrange) One day this should be per-guest
4274         # OS type configurable
4275         tmpit = vconfig.LibvirtConfigGuestTimer()
4276         tmpit.name = "pit"
4277         tmpit.tickpolicy = "delay"
4278 
4279         tmrtc = vconfig.LibvirtConfigGuestTimer()
4280         tmrtc.name = "rtc"
4281         tmrtc.tickpolicy = "catchup"
4282 
4283         clk.add_timer(tmpit)
4284         clk.add_timer(tmrtc)
4285 
4286         guestarch = libvirt_utils.get_arch(image_meta)
4287         if guestarch in (fields.Architecture.I686,
4288                          fields.Architecture.X86_64):
4289             # NOTE(rfolco): HPET is a hardware timer for x86 arch.
4290             # qemu -no-hpet is not supported on non-x86 targets.
4291             tmhpet = vconfig.LibvirtConfigGuestTimer()
4292             tmhpet.name = "hpet"
4293             tmhpet.present = False
4294             clk.add_timer(tmhpet)
4295 
4296         # Provide Windows guests with the paravirtualized hyperv timer source.
4297         # This is the windows equiv of kvm-clock, allowing Windows
4298         # guests to accurately keep time.
4299         if os_type == 'windows':
4300             tmhyperv = vconfig.LibvirtConfigGuestTimer()
4301             tmhyperv.name = "hypervclock"
4302             tmhyperv.present = True
4303             clk.add_timer(tmhyperv)
4304 
4305     def _set_features(self, guest, os_type, caps, virt_type, image_meta):
4306         if virt_type == "xen":
4307             # PAE only makes sense in X86
4308             if caps.host.cpu.arch in (fields.Architecture.I686,
4309                                       fields.Architecture.X86_64):
4310                 guest.features.append(vconfig.LibvirtConfigGuestFeaturePAE())
4311 
4312         if (virt_type not in ("lxc", "uml", "parallels", "xen") or
4313                 (virt_type == "xen" and guest.os_type == fields.VMMode.HVM)):
4314             guest.features.append(vconfig.LibvirtConfigGuestFeatureACPI())
4315             guest.features.append(vconfig.LibvirtConfigGuestFeatureAPIC())
4316 
4317         if (virt_type in ("qemu", "kvm") and
4318                 os_type == 'windows'):
4319             hv = vconfig.LibvirtConfigGuestFeatureHyperV()
4320             hv.relaxed = True
4321 
4322             hv.spinlocks = True
4323             # Increase spinlock retries - value recommended by
4324             # KVM maintainers who certify Windows guests
4325             # with Microsoft
4326             hv.spinlock_retries = 8191
4327             hv.vapic = True
4328             guest.features.append(hv)
4329 
4330         if (virt_type in ("qemu", "kvm") and
4331                 image_meta.properties.get('img_hide_hypervisor_id')):
4332             guest.features.append(vconfig.LibvirtConfigGuestFeatureKvmHidden())
4333 
4334     def _check_number_of_serial_console(self, num_ports):
4335         virt_type = CONF.libvirt.virt_type
4336         if (virt_type in ("kvm", "qemu") and
4337             num_ports > ALLOWED_QEMU_SERIAL_PORTS):
4338             raise exception.SerialPortNumberLimitExceeded(
4339                 allowed=ALLOWED_QEMU_SERIAL_PORTS, virt_type=virt_type)
4340 
4341     def _add_video_driver(self, guest, image_meta, flavor):
4342         VALID_VIDEO_DEVICES = ("vga", "cirrus", "vmvga",
4343                                "xen", "qxl", "virtio")
4344         video = vconfig.LibvirtConfigGuestVideo()
4345         # NOTE(ldbragst): The following logic sets the video.type
4346         # depending on supported defaults given the architecture,
4347         # virtualization type, and features. The video.type attribute can
4348         # be overridden by the user with image_meta.properties, which
4349         # is carried out in the next if statement below this one.
4350         guestarch = libvirt_utils.get_arch(image_meta)
4351         if guest.os_type == fields.VMMode.XEN:
4352             video.type = 'xen'
4353         elif CONF.libvirt.virt_type == 'parallels':
4354             video.type = 'vga'
4355         elif guestarch in (fields.Architecture.PPC,
4356                            fields.Architecture.PPC64,
4357                            fields.Architecture.PPC64LE):
4358             # NOTE(ldbragst): PowerKVM doesn't support 'cirrus' be default
4359             # so use 'vga' instead when running on Power hardware.
4360             video.type = 'vga'
4361         elif guestarch in (fields.Architecture.AARCH64):
4362             # NOTE(kevinz): Only virtio device type is supported by AARCH64
4363             # so use 'virtio' instead when running on AArch64 hardware.
4364             video.type = 'virtio'
4365         elif CONF.spice.enabled:
4366             video.type = 'qxl'
4367         if image_meta.properties.get('hw_video_model'):
4368             video.type = image_meta.properties.hw_video_model
4369             if (video.type not in VALID_VIDEO_DEVICES):
4370                 raise exception.InvalidVideoMode(model=video.type)
4371 
4372         # Set video memory, only if the flavor's limit is set
4373         video_ram = image_meta.properties.get('hw_video_ram', 0)
4374         max_vram = int(flavor.extra_specs.get('hw_video:ram_max_mb', 0))
4375         if video_ram > max_vram:
4376             raise exception.RequestedVRamTooHigh(req_vram=video_ram,
4377                                                  max_vram=max_vram)
4378         if max_vram and video_ram:
4379             video.vram = video_ram * units.Mi / units.Ki
4380         guest.add_device(video)
4381 
4382     def _add_qga_device(self, guest, instance):
4383         qga = vconfig.LibvirtConfigGuestChannel()
4384         qga.type = "unix"
4385         qga.target_name = "org.qemu.guest_agent.0"
4386         qga.source_path = ("/var/lib/libvirt/qemu/%s.%s.sock" %
4387                           ("org.qemu.guest_agent.0", instance.name))
4388         guest.add_device(qga)
4389 
4390     def _add_rng_device(self, guest, flavor):
4391         rng_device = vconfig.LibvirtConfigGuestRng()
4392         rate_bytes = flavor.extra_specs.get('hw_rng:rate_bytes', 0)
4393         period = flavor.extra_specs.get('hw_rng:rate_period', 0)
4394         if rate_bytes:
4395             rng_device.rate_bytes = int(rate_bytes)
4396             rng_device.rate_period = int(period)
4397         rng_path = CONF.libvirt.rng_dev_path
4398         if (rng_path and not os.path.exists(rng_path)):
4399             raise exception.RngDeviceNotExist(path=rng_path)
4400         rng_device.backend = rng_path
4401         guest.add_device(rng_device)
4402 
4403     def _set_qemu_guest_agent(self, guest, flavor, instance, image_meta):
4404         # Enable qga only if the 'hw_qemu_guest_agent' is equal to yes
4405         if image_meta.properties.get('hw_qemu_guest_agent', False):
4406             LOG.debug("Qemu guest agent is enabled through image "
4407                       "metadata", instance=instance)
4408             self._add_qga_device(guest, instance)
4409         rng_is_virtio = image_meta.properties.get('hw_rng_model') == 'virtio'
4410         rng_allowed_str = flavor.extra_specs.get('hw_rng:allowed', '')
4411         rng_allowed = strutils.bool_from_string(rng_allowed_str)
4412         if rng_is_virtio and rng_allowed:
4413             self._add_rng_device(guest, flavor)
4414 
4415     def _get_guest_memory_backing_config(
4416             self, inst_topology, numatune, flavor):
4417         wantsmempages = False
4418         if inst_topology:
4419             for cell in inst_topology.cells:
4420                 if cell.pagesize:
4421                     wantsmempages = True
4422                     break
4423 
4424         wantsrealtime = hardware.is_realtime_enabled(flavor)
4425 
4426         membacking = None
4427         if wantsmempages:
4428             pages = self._get_memory_backing_hugepages_support(
4429                 inst_topology, numatune)
4430             if pages:
4431                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4432                 membacking.hugepages = pages
4433         if wantsrealtime:
4434             if not membacking:
4435                 membacking = vconfig.LibvirtConfigGuestMemoryBacking()
4436             membacking.locked = True
4437             membacking.sharedpages = False
4438 
4439         return membacking
4440 
4441     def _get_memory_backing_hugepages_support(self, inst_topology, numatune):
4442         if not self._has_numa_support():
4443             # We should not get here, since we should have avoided
4444             # reporting NUMA topology from _get_host_numa_topology
4445             # in the first place. Just in case of a scheduler
4446             # mess up though, raise an exception
4447             raise exception.MemoryPagesUnsupported()
4448 
4449         host_topology = self._get_host_numa_topology()
4450 
4451         if host_topology is None:
4452             # As above, we should not get here but just in case...
4453             raise exception.MemoryPagesUnsupported()
4454 
4455         # Currently libvirt does not support the smallest
4456         # pagesize set as a backend memory.
4457         # https://bugzilla.redhat.com/show_bug.cgi?id=1173507
4458         avail_pagesize = [page.size_kb
4459                           for page in host_topology.cells[0].mempages]
4460         avail_pagesize.sort()
4461         smallest = avail_pagesize[0]
4462 
4463         pages = []
4464         for guest_cellid, inst_cell in enumerate(inst_topology.cells):
4465             if inst_cell.pagesize and inst_cell.pagesize > smallest:
4466                 for memnode in numatune.memnodes:
4467                     if guest_cellid == memnode.cellid:
4468                         page = (
4469                             vconfig.LibvirtConfigGuestMemoryBackingPage())
4470                         page.nodeset = [guest_cellid]
4471                         page.size_kb = inst_cell.pagesize
4472                         pages.append(page)
4473                         break  # Quit early...
4474         return pages
4475 
4476     def _get_flavor(self, ctxt, instance, flavor):
4477         if flavor is not None:
4478             return flavor
4479         return instance.flavor
4480 
4481     def _has_uefi_support(self):
4482         # This means that the host can support uefi booting for guests
4483         supported_archs = [fields.Architecture.X86_64,
4484                            fields.Architecture.AARCH64]
4485         caps = self._host.get_capabilities()
4486         return ((caps.host.cpu.arch in supported_archs) and
4487                 os.path.exists(DEFAULT_UEFI_LOADER_PATH[caps.host.cpu.arch]))
4488 
4489     def _get_supported_perf_events(self):
4490 
4491         if (len(CONF.libvirt.enabled_perf_events) == 0 or
4492              not self._host.has_min_version(MIN_LIBVIRT_PERF_VERSION)):
4493             return []
4494 
4495         supported_events = []
4496         host_cpu_info = self._get_cpu_info()
4497         for event in CONF.libvirt.enabled_perf_events:
4498             if self._supported_perf_event(event, host_cpu_info['features']):
4499                 supported_events.append(event)
4500         return supported_events
4501 
4502     def _supported_perf_event(self, event, cpu_features):
4503 
4504         libvirt_perf_event_name = LIBVIRT_PERF_EVENT_PREFIX + event.upper()
4505 
4506         if not hasattr(libvirt, libvirt_perf_event_name):
4507             LOG.warning("Libvirt doesn't support event type %s.", event)
4508             return False
4509 
4510         if (event in PERF_EVENTS_CPU_FLAG_MAPPING
4511             and PERF_EVENTS_CPU_FLAG_MAPPING[event] not in cpu_features):
4512             LOG.warning("Host does not support event type %s.", event)
4513             return False
4514 
4515         return True
4516 
4517     def _configure_guest_by_virt_type(self, guest, virt_type, caps, instance,
4518                                       image_meta, flavor, root_device_name):
4519         if virt_type == "xen":
4520             if guest.os_type == fields.VMMode.HVM:
4521                 guest.os_loader = CONF.libvirt.xen_hvmloader_path
4522             else:
4523                 guest.os_cmdline = CONSOLE
4524         elif virt_type in ("kvm", "qemu"):
4525             if caps.host.cpu.arch in (fields.Architecture.I686,
4526                                       fields.Architecture.X86_64):
4527                 guest.sysinfo = self._get_guest_config_sysinfo(instance)
4528                 guest.os_smbios = vconfig.LibvirtConfigGuestSMBIOS()
4529             hw_firmware_type = image_meta.properties.get('hw_firmware_type')
4530             if hw_firmware_type == fields.FirmwareType.UEFI:
4531                 if self._has_uefi_support():
4532                     global uefi_logged
4533                     if not uefi_logged:
4534                         LOG.warning("uefi support is without some kind of "
4535                                     "functional testing and therefore "
4536                                     "considered experimental.")
4537                         uefi_logged = True
4538                     guest.os_loader = DEFAULT_UEFI_LOADER_PATH[
4539                         caps.host.cpu.arch]
4540                     guest.os_loader_type = "pflash"
4541                 else:
4542                     raise exception.UEFINotSupported()
4543             guest.os_mach_type = self._get_machine_type(image_meta, caps)
4544             if image_meta.properties.get('hw_boot_menu') is None:
4545                 guest.os_bootmenu = strutils.bool_from_string(
4546                     flavor.extra_specs.get('hw:boot_menu', 'no'))
4547             else:
4548                 guest.os_bootmenu = image_meta.properties.hw_boot_menu
4549 
4550         elif virt_type == "lxc":
4551             guest.os_init_path = "/sbin/init"
4552             guest.os_cmdline = CONSOLE
4553         elif virt_type == "uml":
4554             guest.os_kernel = "/usr/bin/linux"
4555             guest.os_root = root_device_name
4556         elif virt_type == "parallels":
4557             if guest.os_type == fields.VMMode.EXE:
4558                 guest.os_init_path = "/sbin/init"
4559 
4560     def _conf_non_lxc_uml(self, virt_type, guest, root_device_name, rescue,
4561                     instance, inst_path, image_meta, disk_info):
4562         if rescue:
4563             self._set_guest_for_rescue(rescue, guest, inst_path, virt_type,
4564                                        root_device_name)
4565         elif instance.kernel_id:
4566             self._set_guest_for_inst_kernel(instance, guest, inst_path,
4567                                             virt_type, root_device_name,
4568                                             image_meta)
4569         else:
4570             guest.os_boot_dev = blockinfo.get_boot_order(disk_info)
4571 
4572     def _create_consoles(self, virt_type, guest_cfg, instance, flavor,
4573                          image_meta):
4574         # NOTE(markus_z): Beware! Below are so many conditionals that it is
4575         # easy to lose track. Use this chart to figure out your case:
4576         #
4577         # case | is serial | has       | is qemu | resulting
4578         #      | enabled?  | virtlogd? | or kvm? | devices
4579         # --------------------------------------------------
4580         #    1 |        no |        no |     no  | pty*
4581         #    2 |        no |        no |     yes | file + pty
4582         #    3 |        no |       yes |      no | see case 1
4583         #    4 |        no |       yes |     yes | pty with logd
4584         #    5 |       yes |        no |      no | see case 1
4585         #    6 |       yes |        no |     yes | tcp + pty
4586         #    7 |       yes |       yes |      no | see case 1
4587         #    8 |       yes |       yes |     yes | tcp with logd
4588         #    * exception: virt_type "parallels" doesn't create a device
4589         if virt_type == 'parallels':
4590             pass
4591         elif virt_type not in ("qemu", "kvm"):
4592             log_path = self._get_console_log_path(instance)
4593             self._create_pty_device(guest_cfg,
4594                                     vconfig.LibvirtConfigGuestConsole,
4595                                     log_path=log_path)
4596         elif (virt_type in ("qemu", "kvm") and
4597                   self._is_s390x_guest(image_meta)):
4598             self._create_consoles_s390x(guest_cfg, instance,
4599                                         flavor, image_meta)
4600         elif virt_type in ("qemu", "kvm"):
4601             self._create_consoles_qemu_kvm(guest_cfg, instance,
4602                                         flavor, image_meta)
4603 
4604     def _is_s390x_guest(self, image_meta):
4605         s390x_archs = (fields.Architecture.S390, fields.Architecture.S390X)
4606         return libvirt_utils.get_arch(image_meta) in s390x_archs
4607 
4608     def _create_consoles_qemu_kvm(self, guest_cfg, instance, flavor,
4609                                   image_meta):
4610         char_dev_cls = vconfig.LibvirtConfigGuestSerial
4611         log_path = self._get_console_log_path(instance)
4612         if CONF.serial_console.enabled:
4613             if not self._serial_ports_already_defined(instance):
4614                 num_ports = hardware.get_number_of_serial_ports(flavor,
4615                                                                 image_meta)
4616                 self._check_number_of_serial_console(num_ports)
4617                 self._create_serial_consoles(guest_cfg, num_ports,
4618                                              char_dev_cls, log_path)
4619         else:
4620             self._create_file_device(guest_cfg, instance, char_dev_cls)
4621         self._create_pty_device(guest_cfg, char_dev_cls, log_path=log_path)
4622 
4623     def _create_consoles_s390x(self, guest_cfg, instance, flavor, image_meta):
4624         char_dev_cls = vconfig.LibvirtConfigGuestConsole
4625         log_path = self._get_console_log_path(instance)
4626         if CONF.serial_console.enabled:
4627             if not self._serial_ports_already_defined(instance):
4628                 num_ports = hardware.get_number_of_serial_ports(flavor,
4629                                                                 image_meta)
4630                 self._create_serial_consoles(guest_cfg, num_ports,
4631                                              char_dev_cls, log_path)
4632         else:
4633             self._create_file_device(guest_cfg, instance, char_dev_cls,
4634                                      "sclplm")
4635         self._create_pty_device(guest_cfg, char_dev_cls, "sclp", log_path)
4636 
4637     def _create_pty_device(self, guest_cfg, char_dev_cls, target_type=None,
4638                            log_path=None):
4639         def _create_base_dev():
4640             consolepty = char_dev_cls()
4641             consolepty.target_type = target_type
4642             consolepty.type = "pty"
4643             return consolepty
4644 
4645         def _create_logd_dev():
4646             consolepty = _create_base_dev()
4647             log = vconfig.LibvirtConfigGuestCharDeviceLog()
4648             log.file = log_path
4649             consolepty.log = log
4650             return consolepty
4651 
4652         if CONF.serial_console.enabled:
4653             if self._is_virtlogd_available():
4654                 return
4655             else:
4656                 # NOTE(markus_z): You may wonder why this is necessary and
4657                 # so do I. I'm certain that this is *not* needed in any
4658                 # real use case. It is, however, useful if you want to
4659                 # pypass the Nova API and use "virsh console <guest>" on
4660                 # an hypervisor, as this CLI command doesn't work with TCP
4661                 # devices (like the serial console is).
4662                 #     https://bugzilla.redhat.com/show_bug.cgi?id=781467
4663                 # Pypassing the Nova API however is a thing we don't want.
4664                 # Future changes should remove this and fix the unit tests
4665                 # which ask for the existence.
4666                 guest_cfg.add_device(_create_base_dev())
4667         else:
4668             if self._is_virtlogd_available():
4669                 guest_cfg.add_device(_create_logd_dev())
4670             else:
4671                 guest_cfg.add_device(_create_base_dev())
4672 
4673     def _create_file_device(self, guest_cfg, instance, char_dev_cls,
4674                             target_type=None):
4675         if self._is_virtlogd_available():
4676             return
4677 
4678         consolelog = char_dev_cls()
4679         consolelog.target_type = target_type
4680         consolelog.type = "file"
4681         consolelog.source_path = self._get_console_log_path(instance)
4682         guest_cfg.add_device(consolelog)
4683 
4684     def _serial_ports_already_defined(self, instance):
4685         try:
4686             guest = self._host.get_guest(instance)
4687             if list(self._get_serial_ports_from_guest(guest)):
4688                 # Serial port are already configured for instance that
4689                 # means we are in a context of migration.
4690                 return True
4691         except exception.InstanceNotFound:
4692             LOG.debug(
4693                 "Instance does not exist yet on libvirt, we can "
4694                 "safely pass on looking for already defined serial "
4695                 "ports in its domain XML", instance=instance)
4696         return False
4697 
4698     def _create_serial_consoles(self, guest_cfg, num_ports, char_dev_cls,
4699                                 log_path):
4700         for port in six.moves.range(num_ports):
4701             console = char_dev_cls()
4702             console.port = port
4703             console.type = "tcp"
4704             console.listen_host = CONF.serial_console.proxyclient_address
4705             listen_port = serial_console.acquire_port(console.listen_host)
4706             console.listen_port = listen_port
4707             # NOTE: only the first serial console gets the boot messages,
4708             # that's why we attach the logd subdevice only to that.
4709             if port == 0 and self._is_virtlogd_available():
4710                 log = vconfig.LibvirtConfigGuestCharDeviceLog()
4711                 log.file = log_path
4712                 console.log = log
4713             guest_cfg.add_device(console)
4714 
4715     def _cpu_config_to_vcpu_model(self, cpu_config, vcpu_model):
4716         """Update VirtCPUModel object according to libvirt CPU config.
4717 
4718         :param:cpu_config: vconfig.LibvirtConfigGuestCPU presenting the
4719                            instance's virtual cpu configuration.
4720         :param:vcpu_model: VirtCPUModel object. A new object will be created
4721                            if None.
4722 
4723         :return: Updated VirtCPUModel object, or None if cpu_config is None
4724 
4725         """
4726 
4727         if not cpu_config:
4728             return
4729         if not vcpu_model:
4730             vcpu_model = objects.VirtCPUModel()
4731 
4732         vcpu_model.arch = cpu_config.arch
4733         vcpu_model.vendor = cpu_config.vendor
4734         vcpu_model.model = cpu_config.model
4735         vcpu_model.mode = cpu_config.mode
4736         vcpu_model.match = cpu_config.match
4737 
4738         if cpu_config.sockets:
4739             vcpu_model.topology = objects.VirtCPUTopology(
4740                 sockets=cpu_config.sockets,
4741                 cores=cpu_config.cores,
4742                 threads=cpu_config.threads)
4743         else:
4744             vcpu_model.topology = None
4745 
4746         features = [objects.VirtCPUFeature(
4747             name=f.name,
4748             policy=f.policy) for f in cpu_config.features]
4749         vcpu_model.features = features
4750 
4751         return vcpu_model
4752 
4753     def _vcpu_model_to_cpu_config(self, vcpu_model):
4754         """Create libvirt CPU config according to VirtCPUModel object.
4755 
4756         :param:vcpu_model: VirtCPUModel object.
4757 
4758         :return: vconfig.LibvirtConfigGuestCPU.
4759 
4760         """
4761 
4762         cpu_config = vconfig.LibvirtConfigGuestCPU()
4763         cpu_config.arch = vcpu_model.arch
4764         cpu_config.model = vcpu_model.model
4765         cpu_config.mode = vcpu_model.mode
4766         cpu_config.match = vcpu_model.match
4767         cpu_config.vendor = vcpu_model.vendor
4768         if vcpu_model.topology:
4769             cpu_config.sockets = vcpu_model.topology.sockets
4770             cpu_config.cores = vcpu_model.topology.cores
4771             cpu_config.threads = vcpu_model.topology.threads
4772         if vcpu_model.features:
4773             for f in vcpu_model.features:
4774                 xf = vconfig.LibvirtConfigGuestCPUFeature()
4775                 xf.name = f.name
4776                 xf.policy = f.policy
4777                 cpu_config.features.add(xf)
4778         return cpu_config
4779 
4780     def _get_guest_config(self, instance, network_info, image_meta,
4781                           disk_info, rescue=None, block_device_info=None,
4782                           context=None):
4783         """Get config data for parameters.
4784 
4785         :param rescue: optional dictionary that should contain the key
4786             'ramdisk_id' if a ramdisk is needed for the rescue image and
4787             'kernel_id' if a kernel is needed for the rescue image.
4788         """
4789         flavor = instance.flavor
4790         inst_path = libvirt_utils.get_instance_path(instance)
4791         disk_mapping = disk_info['mapping']
4792 
4793         virt_type = CONF.libvirt.virt_type
4794         guest = vconfig.LibvirtConfigGuest()
4795         guest.virt_type = virt_type
4796         guest.name = instance.name
4797         guest.uuid = instance.uuid
4798         # We are using default unit for memory: KiB
4799         guest.memory = flavor.memory_mb * units.Ki
4800         guest.vcpus = flavor.vcpus
4801         allowed_cpus = hardware.get_vcpu_pin_set()
4802 
4803         guest_numa_config = self._get_guest_numa_config(
4804             instance.numa_topology, flavor, allowed_cpus, image_meta)
4805 
4806         guest.cpuset = guest_numa_config.cpuset
4807         guest.cputune = guest_numa_config.cputune
4808         guest.numatune = guest_numa_config.numatune
4809 
4810         guest.membacking = self._get_guest_memory_backing_config(
4811             instance.numa_topology,
4812             guest_numa_config.numatune,
4813             flavor)
4814 
4815         guest.metadata.append(self._get_guest_config_meta(instance))
4816         guest.idmaps = self._get_guest_idmaps()
4817 
4818         for event in self._supported_perf_events:
4819             guest.add_perf_event(event)
4820 
4821         self._update_guest_cputune(guest, flavor, virt_type)
4822 
4823         guest.cpu = self._get_guest_cpu_config(
4824             flavor, image_meta, guest_numa_config.numaconfig,
4825             instance.numa_topology)
4826 
4827         # Notes(yjiang5): we always sync the instance's vcpu model with
4828         # the corresponding config file.
4829         instance.vcpu_model = self._cpu_config_to_vcpu_model(
4830             guest.cpu, instance.vcpu_model)
4831 
4832         if 'root' in disk_mapping:
4833             root_device_name = block_device.prepend_dev(
4834                 disk_mapping['root']['dev'])
4835         else:
4836             root_device_name = None
4837 
4838         if root_device_name:
4839             # NOTE(yamahata):
4840             # for nova.api.ec2.cloud.CloudController.get_metadata()
4841             instance.root_device_name = root_device_name
4842 
4843         guest.os_type = (fields.VMMode.get_from_instance(instance) or
4844                 self._get_guest_os_type(virt_type))
4845         caps = self._host.get_capabilities()
4846 
4847         self._configure_guest_by_virt_type(guest, virt_type, caps, instance,
4848                                            image_meta, flavor,
4849                                            root_device_name)
4850         if virt_type not in ('lxc', 'uml'):
4851             self._conf_non_lxc_uml(virt_type, guest, root_device_name, rescue,
4852                     instance, inst_path, image_meta, disk_info)
4853 
4854         self._set_features(guest, instance.os_type, caps, virt_type,
4855                            image_meta)
4856         self._set_clock(guest, instance.os_type, image_meta, virt_type)
4857 
4858         storage_configs = self._get_guest_storage_config(
4859                 instance, image_meta, disk_info, rescue, block_device_info,
4860                 flavor, guest.os_type)
4861         for config in storage_configs:
4862             guest.add_device(config)
4863 
4864         for vif in network_info:
4865             config = self.vif_driver.get_config(
4866                 instance, vif, image_meta,
4867                 flavor, virt_type, self._host)
4868             guest.add_device(config)
4869 
4870         self._create_consoles(virt_type, guest, instance, flavor, image_meta)
4871 
4872         pointer = self._get_guest_pointer_model(guest.os_type, image_meta)
4873         if pointer:
4874             guest.add_device(pointer)
4875 
4876         self._guest_add_spice_channel(guest)
4877 
4878         if self._guest_add_video_device(guest):
4879             self._add_video_driver(guest, image_meta, flavor)
4880 
4881         # Qemu guest agent only support 'qemu' and 'kvm' hypervisor
4882         if virt_type in ('qemu', 'kvm'):
4883             self._set_qemu_guest_agent(guest, flavor, instance, image_meta)
4884 
4885         self._guest_add_pci_devices(guest, instance)
4886 
4887         self._guest_add_watchdog_action(guest, flavor, image_meta)
4888 
4889         self._guest_add_memory_balloon(guest)
4890 
4891         return guest
4892 
4893     @staticmethod
4894     def _guest_add_spice_channel(guest):
4895         if (CONF.spice.enabled and CONF.spice.agent_enabled
4896                 and guest.virt_type not in ('lxc', 'uml', 'xen')):
4897             channel = vconfig.LibvirtConfigGuestChannel()
4898             channel.type = 'spicevmc'
4899             channel.target_name = "com.redhat.spice.0"
4900             guest.add_device(channel)
4901 
4902     @staticmethod
4903     def _guest_add_memory_balloon(guest):
4904         virt_type = guest.virt_type
4905         # Memory balloon device only support 'qemu/kvm' and 'xen' hypervisor
4906         if (virt_type in ('xen', 'qemu', 'kvm') and
4907                     CONF.libvirt.mem_stats_period_seconds > 0):
4908             balloon = vconfig.LibvirtConfigMemoryBalloon()
4909             if virt_type in ('qemu', 'kvm'):
4910                 balloon.model = 'virtio'
4911             else:
4912                 balloon.model = 'xen'
4913             balloon.period = CONF.libvirt.mem_stats_period_seconds
4914             guest.add_device(balloon)
4915 
4916     @staticmethod
4917     def _guest_add_watchdog_action(guest, flavor, image_meta):
4918         # image meta takes precedence over flavor extra specs; disable the
4919         # watchdog action by default
4920         watchdog_action = (flavor.extra_specs.get('hw:watchdog_action')
4921                            or 'disabled')
4922         watchdog_action = image_meta.properties.get('hw_watchdog_action',
4923                                                     watchdog_action)
4924         # NB(sross): currently only actually supported by KVM/QEmu
4925         if watchdog_action != 'disabled':
4926             if watchdog_action in fields.WatchdogAction.ALL:
4927                 bark = vconfig.LibvirtConfigGuestWatchdog()
4928                 bark.action = watchdog_action
4929                 guest.add_device(bark)
4930             else:
4931                 raise exception.InvalidWatchdogAction(action=watchdog_action)
4932 
4933     def _guest_add_pci_devices(self, guest, instance):
4934         virt_type = guest.virt_type
4935         if virt_type in ('xen', 'qemu', 'kvm'):
4936             # Get all generic PCI devices (non-SR-IOV).
4937             for pci_dev in pci_manager.get_instance_pci_devs(instance):
4938                 guest.add_device(self._get_guest_pci_device(pci_dev))
4939         else:
4940             # PCI devices is only supported for hypervisors
4941             #  'xen', 'qemu' and 'kvm'.
4942             if pci_manager.get_instance_pci_devs(instance, 'all'):
4943                 raise exception.PciDeviceUnsupportedHypervisor(type=virt_type)
4944 
4945     @staticmethod
4946     def _guest_add_video_device(guest):
4947         # NB some versions of libvirt support both SPICE and VNC
4948         # at the same time. We're not trying to second guess which
4949         # those versions are. We'll just let libvirt report the
4950         # errors appropriately if the user enables both.
4951         add_video_driver = False
4952         if CONF.vnc.enabled and guest.virt_type not in ('lxc', 'uml'):
4953             graphics = vconfig.LibvirtConfigGuestGraphics()
4954             graphics.type = "vnc"
4955             if CONF.vnc.keymap:
4956                 # TODO(stephenfin): There are some issues here that may
4957                 # necessitate deprecating this option entirely in the future.
4958                 # Refer to bug #1682020 for more information.
4959                 graphics.keymap = CONF.vnc.keymap
4960             graphics.listen = CONF.vnc.server_listen
4961             guest.add_device(graphics)
4962             add_video_driver = True
4963         if CONF.spice.enabled and guest.virt_type not in ('lxc', 'uml', 'xen'):
4964             graphics = vconfig.LibvirtConfigGuestGraphics()
4965             graphics.type = "spice"
4966             if CONF.spice.keymap:
4967                 # TODO(stephenfin): There are some issues here that may
4968                 # necessitate deprecating this option entirely in the future.
4969                 # Refer to bug #1682020 for more information.
4970                 graphics.keymap = CONF.spice.keymap
4971             graphics.listen = CONF.spice.server_listen
4972             guest.add_device(graphics)
4973             add_video_driver = True
4974         return add_video_driver
4975 
4976     def _get_guest_pointer_model(self, os_type, image_meta):
4977         pointer_model = image_meta.properties.get(
4978             'hw_pointer_model', CONF.pointer_model)
4979         if pointer_model is None and CONF.libvirt.use_usb_tablet:
4980             # TODO(sahid): We set pointer_model to keep compatibility
4981             # until the next release O*. It means operators can continue
4982             # to use the deprecated option "use_usb_tablet" or set a
4983             # specific device to use
4984             pointer_model = "usbtablet"
4985             LOG.warning('The option "use_usb_tablet" has been '
4986                         'deprecated for Newton in favor of the more '
4987                         'generic "pointer_model". Please update '
4988                         'nova.conf to address this change.')
4989 
4990         if pointer_model == "usbtablet":
4991             # We want a tablet if VNC is enabled, or SPICE is enabled and
4992             # the SPICE agent is disabled. If the SPICE agent is enabled
4993             # it provides a paravirt mouse which drastically reduces
4994             # overhead (by eliminating USB polling).
4995             if CONF.vnc.enabled or (
4996                     CONF.spice.enabled and not CONF.spice.agent_enabled):
4997                 return self._get_guest_usb_tablet(os_type)
4998             else:
4999                 if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5000                     # For backward compatibility We don't want to break
5001                     # process of booting an instance if host is configured
5002                     # to use USB tablet without VNC or SPICE and SPICE
5003                     # agent disable.
5004                     LOG.warning('USB tablet requested for guests by host '
5005                                 'configuration. In order to accept this '
5006                                 'request VNC should be enabled or SPICE '
5007                                 'and SPICE agent disabled on host.')
5008                 else:
5009                     raise exception.UnsupportedPointerModelRequested(
5010                         model="usbtablet")
5011 
5012     def _get_guest_usb_tablet(self, os_type):
5013         tablet = None
5014         if os_type == fields.VMMode.HVM:
5015             tablet = vconfig.LibvirtConfigGuestInput()
5016             tablet.type = "tablet"
5017             tablet.bus = "usb"
5018         else:
5019             if CONF.pointer_model or CONF.libvirt.use_usb_tablet:
5020                 # For backward compatibility We don't want to break
5021                 # process of booting an instance if virtual machine mode
5022                 # is not configured as HVM.
5023                 LOG.warning('USB tablet requested for guests by host '
5024                             'configuration. In order to accept this '
5025                             'request the machine mode should be '
5026                             'configured as HVM.')
5027             else:
5028                 raise exception.UnsupportedPointerModelRequested(
5029                     model="usbtablet")
5030         return tablet
5031 
5032     def _get_guest_xml(self, context, instance, network_info, disk_info,
5033                        image_meta, rescue=None,
5034                        block_device_info=None):
5035         # NOTE(danms): Stringifying a NetworkInfo will take a lock. Do
5036         # this ahead of time so that we don't acquire it while also
5037         # holding the logging lock.
5038         network_info_str = str(network_info)
5039         msg = ('Start _get_guest_xml '
5040                'network_info=%(network_info)s '
5041                'disk_info=%(disk_info)s '
5042                'image_meta=%(image_meta)s rescue=%(rescue)s '
5043                'block_device_info=%(block_device_info)s' %
5044                {'network_info': network_info_str, 'disk_info': disk_info,
5045                 'image_meta': image_meta, 'rescue': rescue,
5046                 'block_device_info': block_device_info})
5047         # NOTE(mriedem): block_device_info can contain auth_password so we
5048         # need to sanitize the password in the message.
5049         LOG.debug(strutils.mask_password(msg), instance=instance)
5050         conf = self._get_guest_config(instance, network_info, image_meta,
5051                                       disk_info, rescue, block_device_info,
5052                                       context)
5053         xml = conf.to_xml()
5054 
5055         LOG.debug('End _get_guest_xml xml=%(xml)s',
5056                   {'xml': xml}, instance=instance)
5057         return xml
5058 
5059     def get_info(self, instance):
5060         """Retrieve information from libvirt for a specific instance.
5061 
5062         If a libvirt error is encountered during lookup, we might raise a
5063         NotFound exception or Error exception depending on how severe the
5064         libvirt error is.
5065 
5066         :param instance: nova.objects.instance.Instance object
5067         :returns: An InstanceInfo object
5068         """
5069         guest = self._host.get_guest(instance)
5070         # Kind of ugly but we need to pass host to get_info as for a
5071         # workaround, see libvirt/compat.py
5072         return guest.get_info(self._host)
5073 
5074     def _create_domain_setup_lxc(self, instance, image_meta,
5075                                  block_device_info):
5076         inst_path = libvirt_utils.get_instance_path(instance)
5077         block_device_mapping = driver.block_device_info_get_mapping(
5078             block_device_info)
5079         root_disk = block_device.get_root_bdm(block_device_mapping)
5080         if root_disk:
5081             disk_info = blockinfo.get_info_from_bdm(
5082                 instance, CONF.libvirt.virt_type, image_meta, root_disk)
5083             self._connect_volume(root_disk['connection_info'], disk_info,
5084                                  instance)
5085             disk_path = root_disk['connection_info']['data']['device_path']
5086 
5087             # NOTE(apmelton) - Even though the instance is being booted from a
5088             # cinder volume, it is still presented as a local block device.
5089             # LocalBlockImage is used here to indicate that the instance's
5090             # disk is backed by a local block device.
5091             image_model = imgmodel.LocalBlockImage(disk_path)
5092         else:
5093             root_disk = self.image_backend.by_name(instance, 'disk')
5094             image_model = root_disk.get_model(self._conn)
5095 
5096         container_dir = os.path.join(inst_path, 'rootfs')
5097         fileutils.ensure_tree(container_dir)
5098         rootfs_dev = disk_api.setup_container(image_model,
5099                                               container_dir=container_dir)
5100 
5101         try:
5102             # Save rootfs device to disconnect it when deleting the instance
5103             if rootfs_dev:
5104                 instance.system_metadata['rootfs_device_name'] = rootfs_dev
5105             if CONF.libvirt.uid_maps or CONF.libvirt.gid_maps:
5106                 id_maps = self._get_guest_idmaps()
5107                 libvirt_utils.chown_for_id_maps(container_dir, id_maps)
5108         except Exception:
5109             with excutils.save_and_reraise_exception():
5110                 self._create_domain_cleanup_lxc(instance)
5111 
5112     def _create_domain_cleanup_lxc(self, instance):
5113         inst_path = libvirt_utils.get_instance_path(instance)
5114         container_dir = os.path.join(inst_path, 'rootfs')
5115 
5116         try:
5117             state = self.get_info(instance).state
5118         except exception.InstanceNotFound:
5119             # The domain may not be present if the instance failed to start
5120             state = None
5121 
5122         if state == power_state.RUNNING:
5123             # NOTE(uni): Now the container is running with its own private
5124             # mount namespace and so there is no need to keep the container
5125             # rootfs mounted in the host namespace
5126             LOG.debug('Attempting to unmount container filesystem: %s',
5127                       container_dir, instance=instance)
5128             disk_api.clean_lxc_namespace(container_dir=container_dir)
5129         else:
5130             disk_api.teardown_container(container_dir=container_dir)
5131 
5132     @contextlib.contextmanager
5133     def _lxc_disk_handler(self, instance, image_meta, block_device_info):
5134         """Context manager to handle the pre and post instance boot,
5135            LXC specific disk operations.
5136 
5137            An image or a volume path will be prepared and setup to be
5138            used by the container, prior to starting it.
5139            The disk will be disconnected and unmounted if a container has
5140            failed to start.
5141         """
5142 
5143         if CONF.libvirt.virt_type != 'lxc':
5144             yield
5145             return
5146 
5147         self._create_domain_setup_lxc(instance, image_meta, block_device_info)
5148 
5149         try:
5150             yield
5151         finally:
5152             self._create_domain_cleanup_lxc(instance)
5153 
5154     # TODO(sahid): Consider renaming this to _create_guest.
5155     def _create_domain(self, xml=None, domain=None,
5156                        power_on=True, pause=False, post_xml_callback=None):
5157         """Create a domain.
5158 
5159         Either domain or xml must be passed in. If both are passed, then
5160         the domain definition is overwritten from the xml.
5161 
5162         :returns guest.Guest: Guest just created
5163         """
5164         if xml:
5165             guest = libvirt_guest.Guest.create(xml, self._host)
5166             if post_xml_callback is not None:
5167                 post_xml_callback()
5168         else:
5169             guest = libvirt_guest.Guest(domain)
5170 
5171         if power_on or pause:
5172             guest.launch(pause=pause)
5173 
5174         if not utils.is_neutron():
5175             guest.enable_hairpin()
5176 
5177         return guest
5178 
5179     def _neutron_failed_callback(self, event_name, instance):
5180         LOG.error('Neutron Reported failure on event '
5181                   '%(event)s for instance %(uuid)s',
5182                   {'event': event_name, 'uuid': instance.uuid},
5183                   instance=instance)
5184         if CONF.vif_plugging_is_fatal:
5185             raise exception.VirtualInterfaceCreateException()
5186 
5187     def _get_neutron_events(self, network_info):
5188         # NOTE(danms): We need to collect any VIFs that are currently
5189         # down that we expect a down->up event for. Anything that is
5190         # already up will not undergo that transition, and for
5191         # anything that might be stale (cache-wise) assume it's
5192         # already up so we don't block on it.
5193         return [('network-vif-plugged', vif['id'])
5194                 for vif in network_info if vif.get('active', True) is False]
5195 
5196     def _get_neutron_events_for_live_migration(self, network_info):
5197         # Neutron should send to Nova events indicating that the VIFs
5198         # are well plugged on destination host.
5199         return [('network-vif-plugged', vif['id']) for vif in network_info]
5200 
5201     def _cleanup_failed_start(self, context, instance, network_info,
5202                               block_device_info, guest, destroy_disks):
5203         try:
5204             if guest and guest.is_active():
5205                 guest.poweroff()
5206         finally:
5207             self.cleanup(context, instance, network_info=network_info,
5208                          block_device_info=block_device_info,
5209                          destroy_disks=destroy_disks)
5210 
5211     def _create_domain_and_network(self, context, xml, instance, network_info,
5212                                    block_device_info=None,
5213                                    power_on=True, reboot=False,
5214                                    vifs_already_plugged=False,
5215                                    post_xml_callback=None,
5216                                    destroy_disks_on_failure=False):
5217 
5218         """Do required network setup and create domain."""
5219         block_device_mapping = driver.block_device_info_get_mapping(
5220             block_device_info)
5221 
5222         for vol in block_device_mapping:
5223             connection_info = vol['connection_info']
5224 
5225             if (not reboot and 'data' in connection_info and
5226                     'volume_id' in connection_info['data']):
5227                 volume_id = connection_info['data']['volume_id']
5228                 encryption = encryptors.get_encryption_metadata(
5229                     context, self._volume_api, volume_id, connection_info)
5230 
5231                 if encryption:
5232                     encryptor = self._get_volume_encryptor(connection_info,
5233                                                            encryption)
5234                     encryptor.attach_volume(context, **encryption)
5235 
5236         timeout = CONF.vif_plugging_timeout
5237         if (self._conn_supports_start_paused and
5238             utils.is_neutron() and not
5239             vifs_already_plugged and power_on and timeout):
5240             events = self._get_neutron_events(network_info)
5241         else:
5242             events = []
5243 
5244         pause = bool(events)
5245         guest = None
5246         try:
5247             with self.virtapi.wait_for_instance_event(
5248                     instance, events, deadline=timeout,
5249                     error_callback=self._neutron_failed_callback):
5250                 self.plug_vifs(instance, network_info)
5251                 self.firewall_driver.setup_basic_filtering(instance,
5252                                                            network_info)
5253                 self.firewall_driver.prepare_instance_filter(instance,
5254                                                              network_info)
5255                 with self._lxc_disk_handler(instance, instance.image_meta,
5256                                             block_device_info):
5257                     guest = self._create_domain(
5258                         xml, pause=pause, power_on=power_on,
5259                         post_xml_callback=post_xml_callback)
5260 
5261                 self.firewall_driver.apply_instance_filter(instance,
5262                                                            network_info)
5263         except exception.VirtualInterfaceCreateException:
5264             # Neutron reported failure and we didn't swallow it, so
5265             # bail here
5266             with excutils.save_and_reraise_exception():
5267                 self._cleanup_failed_start(context, instance, network_info,
5268                                            block_device_info, guest,
5269                                            destroy_disks_on_failure)
5270         except eventlet.timeout.Timeout:
5271             # We never heard from Neutron
5272             LOG.warning('Timeout waiting for vif plugging callback for '
5273                         'instance with vm_state %(vm_state)s and '
5274                         'task_state %(task_state)s.',
5275                         {'vm_state': instance.vm_state,
5276                          'task_state': instance.task_state},
5277                         instance=instance)
5278             if CONF.vif_plugging_is_fatal:
5279                 self._cleanup_failed_start(context, instance, network_info,
5280                                            block_device_info, guest,
5281                                            destroy_disks_on_failure)
5282                 raise exception.VirtualInterfaceCreateException()
5283         except Exception:
5284             # Any other error, be sure to clean up
5285             LOG.error('Failed to start libvirt guest', instance=instance)
5286             with excutils.save_and_reraise_exception():
5287                 self._cleanup_failed_start(context, instance, network_info,
5288                                            block_device_info, guest,
5289                                            destroy_disks_on_failure)
5290 
5291         # Resume only if domain has been paused
5292         if pause:
5293             guest.resume()
5294         return guest
5295 
5296     def _get_vcpu_total(self):
5297         """Get available vcpu number of physical computer.
5298 
5299         :returns: the number of cpu core instances can be used.
5300 
5301         """
5302         try:
5303             total_pcpus = self._host.get_cpu_count()
5304         except libvirt.libvirtError:
5305             LOG.warning("Cannot get the number of cpu, because this "
5306                         "function is not implemented for this platform. ")
5307             return 0
5308 
5309         if not CONF.vcpu_pin_set:
5310             return total_pcpus
5311 
5312         available_ids = hardware.get_vcpu_pin_set()
5313         # We get the list of online CPUs on the host and see if the requested
5314         # set falls under these. If not, we retain the old behavior.
5315         online_pcpus = None
5316         try:
5317             online_pcpus = self._host.get_online_cpus()
5318         except libvirt.libvirtError as ex:
5319             error_code = ex.get_error_code()
5320             LOG.warning(
5321                 "Couldn't retrieve the online CPUs due to a Libvirt "
5322                 "error: %(error)s with error code: %(error_code)s",
5323                 {'error': ex, 'error_code': error_code})
5324         if online_pcpus:
5325             if not (available_ids <= online_pcpus):
5326                 msg = (_("Invalid vcpu_pin_set config, one or more of the "
5327                          "specified cpuset is not online. Online cpuset(s): "
5328                          "%(online)s, requested cpuset(s): %(req)s"),
5329                        {'online': sorted(online_pcpus),
5330                         'req': sorted(available_ids)})
5331                 raise exception.Invalid(msg)
5332         elif sorted(available_ids)[-1] >= total_pcpus:
5333             raise exception.Invalid(_("Invalid vcpu_pin_set config, "
5334                                       "out of hypervisor cpu range."))
5335         return len(available_ids)
5336 
5337     @staticmethod
5338     def _get_local_gb_info():
5339         """Get local storage info of the compute node in GB.
5340 
5341         :returns: A dict containing:
5342              :total: How big the overall usable filesystem is (in gigabytes)
5343              :free: How much space is free (in gigabytes)
5344              :used: How much space is used (in gigabytes)
5345         """
5346 
5347         if CONF.libvirt.images_type == 'lvm':
5348             info = lvm.get_volume_group_info(
5349                                CONF.libvirt.images_volume_group)
5350         elif CONF.libvirt.images_type == 'rbd':
5351             info = LibvirtDriver._get_rbd_driver().get_pool_info()
5352         else:
5353             info = libvirt_utils.get_fs_info(CONF.instances_path)
5354 
5355         for (k, v) in info.items():
5356             info[k] = v / units.Gi
5357 
5358         return info
5359 
5360     def _get_vcpu_used(self):
5361         """Get vcpu usage number of physical computer.
5362 
5363         :returns: The total number of vcpu(s) that are currently being used.
5364 
5365         """
5366 
5367         total = 0
5368 
5369         # Not all libvirt drivers will support the get_vcpus_info()
5370         #
5371         # For example, LXC does not have a concept of vCPUs, while
5372         # QEMU (TCG) traditionally handles all vCPUs in a single
5373         # thread. So both will report an exception when the vcpus()
5374         # API call is made. In such a case we should report the
5375         # guest as having 1 vCPU, since that lets us still do
5376         # CPU over commit calculations that apply as the total
5377         # guest count scales.
5378         #
5379         # It is also possible that we might see an exception if
5380         # the guest is just in middle of shutting down. Technically
5381         # we should report 0 for vCPU usage in this case, but we
5382         # we can't reliably distinguish the vcpu not supported
5383         # case from the just shutting down case. Thus we don't know
5384         # whether to report 1 or 0 for vCPU count.
5385         #
5386         # Under-reporting vCPUs is bad because it could conceivably
5387         # let the scheduler place too many guests on the host. Over-
5388         # reporting vCPUs is not a problem as it'll auto-correct on
5389         # the next refresh of usage data.
5390         #
5391         # Thus when getting an exception we always report 1 as the
5392         # vCPU count, as the least worst value.
5393         for guest in self._host.list_guests():
5394             try:
5395                 vcpus = guest.get_vcpus_info()
5396                 total += len(list(vcpus))
5397             except libvirt.libvirtError:
5398                 total += 1
5399             # NOTE(gtt116): give other tasks a chance.
5400             greenthread.sleep(0)
5401         return total
5402 
5403     def _get_instance_capabilities(self):
5404         """Get hypervisor instance capabilities
5405 
5406         Returns a list of tuples that describe instances the
5407         hypervisor is capable of hosting.  Each tuple consists
5408         of the triplet (arch, hypervisor_type, vm_mode).
5409 
5410         :returns: List of tuples describing instance capabilities
5411         """
5412         caps = self._host.get_capabilities()
5413         instance_caps = list()
5414         for g in caps.guests:
5415             for dt in g.domtype:
5416                 instance_cap = (
5417                     fields.Architecture.canonicalize(g.arch),
5418                     fields.HVType.canonicalize(dt),
5419                     fields.VMMode.canonicalize(g.ostype))
5420                 instance_caps.append(instance_cap)
5421 
5422         return instance_caps
5423 
5424     def _get_cpu_info(self):
5425         """Get cpuinfo information.
5426 
5427         Obtains cpu feature from virConnect.getCapabilities.
5428 
5429         :return: see above description
5430 
5431         """
5432 
5433         caps = self._host.get_capabilities()
5434         cpu_info = dict()
5435 
5436         cpu_info['arch'] = caps.host.cpu.arch
5437         cpu_info['model'] = caps.host.cpu.model
5438         cpu_info['vendor'] = caps.host.cpu.vendor
5439 
5440         topology = dict()
5441         topology['cells'] = len(getattr(caps.host.topology, 'cells', [1]))
5442         topology['sockets'] = caps.host.cpu.sockets
5443         topology['cores'] = caps.host.cpu.cores
5444         topology['threads'] = caps.host.cpu.threads
5445         cpu_info['topology'] = topology
5446 
5447         features = set()
5448         for f in caps.host.cpu.features:
5449             features.add(f.name)
5450         cpu_info['features'] = features
5451         return cpu_info
5452 
5453     def _get_pcinet_info(self, vf_address):
5454         """Returns a dict of NET device."""
5455         devname = pci_utils.get_net_name_by_vf_pci_address(vf_address)
5456         if not devname:
5457             return
5458 
5459         virtdev = self._host.device_lookup_by_name(devname)
5460         xmlstr = virtdev.XMLDesc(0)
5461         cfgdev = vconfig.LibvirtConfigNodeDevice()
5462         cfgdev.parse_str(xmlstr)
5463         return {'name': cfgdev.name,
5464                 'capabilities': cfgdev.pci_capability.features}
5465 
5466     def _get_pcidev_info(self, devname):
5467         """Returns a dict of PCI device."""
5468 
5469         def _get_device_type(cfgdev, pci_address):
5470             """Get a PCI device's device type.
5471 
5472             An assignable PCI device can be a normal PCI device,
5473             a SR-IOV Physical Function (PF), or a SR-IOV Virtual
5474             Function (VF). Only normal PCI devices or SR-IOV VFs
5475             are assignable, while SR-IOV PFs are always owned by
5476             hypervisor.
5477             """
5478             for fun_cap in cfgdev.pci_capability.fun_capability:
5479                 if fun_cap.type == 'virt_functions':
5480                     return {
5481                         'dev_type': fields.PciDeviceType.SRIOV_PF,
5482                     }
5483                 if (fun_cap.type == 'phys_function' and
5484                     len(fun_cap.device_addrs) != 0):
5485                     phys_address = "%04x:%02x:%02x.%01x" % (
5486                         fun_cap.device_addrs[0][0],
5487                         fun_cap.device_addrs[0][1],
5488                         fun_cap.device_addrs[0][2],
5489                         fun_cap.device_addrs[0][3])
5490                     return {
5491                         'dev_type': fields.PciDeviceType.SRIOV_VF,
5492                         'parent_addr': phys_address,
5493                     }
5494 
5495             # Note(moshele): libvirt < 1.3 reported virt_functions capability
5496             # only when VFs are enabled. The check below is a workaround
5497             # to get the correct report regardless of whether or not any
5498             # VFs are enabled for the device.
5499             if not self._host.has_min_version(
5500                 MIN_LIBVIRT_PF_WITH_NO_VFS_CAP_VERSION):
5501                 is_physical_function = pci_utils.is_physical_function(
5502                     *pci_utils.get_pci_address_fields(pci_address))
5503                 if is_physical_function:
5504                     return {'dev_type': fields.PciDeviceType.SRIOV_PF}
5505 
5506             return {'dev_type': fields.PciDeviceType.STANDARD}
5507 
5508         def _get_device_capabilities(device, address):
5509             """Get PCI VF device's additional capabilities.
5510 
5511             If a PCI device is a virtual function, this function reads the PCI
5512             parent's network capabilities (must be always a NIC device) and
5513             appends this information to the device's dictionary.
5514             """
5515             if device.get('dev_type') == fields.PciDeviceType.SRIOV_VF:
5516                 pcinet_info = self._get_pcinet_info(address)
5517                 if pcinet_info:
5518                     return {'capabilities':
5519                                 {'network': pcinet_info.get('capabilities')}}
5520             return {}
5521 
5522         virtdev = self._host.device_lookup_by_name(devname)
5523         xmlstr = virtdev.XMLDesc(0)
5524         cfgdev = vconfig.LibvirtConfigNodeDevice()
5525         cfgdev.parse_str(xmlstr)
5526 
5527         address = "%04x:%02x:%02x.%1x" % (
5528             cfgdev.pci_capability.domain,
5529             cfgdev.pci_capability.bus,
5530             cfgdev.pci_capability.slot,
5531             cfgdev.pci_capability.function)
5532 
5533         device = {
5534             "dev_id": cfgdev.name,
5535             "address": address,
5536             "product_id": "%04x" % cfgdev.pci_capability.product_id,
5537             "vendor_id": "%04x" % cfgdev.pci_capability.vendor_id,
5538             }
5539 
5540         device["numa_node"] = cfgdev.pci_capability.numa_node
5541 
5542         # requirement by DataBase Model
5543         device['label'] = 'label_%(vendor_id)s_%(product_id)s' % device
5544         device.update(_get_device_type(cfgdev, address))
5545         device.update(_get_device_capabilities(device, address))
5546         return device
5547 
5548     def _get_pci_passthrough_devices(self):
5549         """Get host PCI devices information.
5550 
5551         Obtains pci devices information from libvirt, and returns
5552         as a JSON string.
5553 
5554         Each device information is a dictionary, with mandatory keys
5555         of 'address', 'vendor_id', 'product_id', 'dev_type', 'dev_id',
5556         'label' and other optional device specific information.
5557 
5558         Refer to the objects/pci_device.py for more idea of these keys.
5559 
5560         :returns: a JSON string containing a list of the assignable PCI
5561                   devices information
5562         """
5563         # Bail early if we know we can't support `listDevices` to avoid
5564         # repeated warnings within a periodic task
5565         if not getattr(self, '_list_devices_supported', True):
5566             return jsonutils.dumps([])
5567 
5568         try:
5569             dev_names = self._host.list_pci_devices() or []
5570         except libvirt.libvirtError as ex:
5571             error_code = ex.get_error_code()
5572             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
5573                 self._list_devices_supported = False
5574                 LOG.warning("URI %(uri)s does not support "
5575                             "listDevices: %(error)s",
5576                             {'uri': self._uri(), 'error': ex})
5577                 return jsonutils.dumps([])
5578             else:
5579                 raise
5580 
5581         pci_info = []
5582         for name in dev_names:
5583             pci_info.append(self._get_pcidev_info(name))
5584 
5585         return jsonutils.dumps(pci_info)
5586 
5587     def _has_numa_support(self):
5588         # This means that the host can support LibvirtConfigGuestNUMATune
5589         # and the nodeset field in LibvirtConfigGuestMemoryBackingPage
5590         for ver in BAD_LIBVIRT_NUMA_VERSIONS:
5591             if self._host.has_version(ver):
5592                 if not getattr(self, '_bad_libvirt_numa_version_warn', False):
5593                     LOG.warning('You are running with libvirt version %s '
5594                                 'which is known to have broken NUMA support. '
5595                                 'Consider patching or updating libvirt on '
5596                                 'this host if you need NUMA support.',
5597                                 self._version_to_string(ver))
5598                     self._bad_libvirt_numa_version_warn = True
5599                 return False
5600 
5601         caps = self._host.get_capabilities()
5602 
5603         if (caps.host.cpu.arch in (fields.Architecture.I686,
5604                                    fields.Architecture.X86_64,
5605                                    fields.Architecture.AARCH64) and
5606                 self._host.has_min_version(hv_type=host.HV_DRIVER_QEMU)):
5607             return True
5608         elif (caps.host.cpu.arch in (fields.Architecture.PPC64,
5609                                      fields.Architecture.PPC64LE) and
5610                 self._host.has_min_version(MIN_LIBVIRT_NUMA_VERSION_PPC,
5611                                            hv_type=host.HV_DRIVER_QEMU)):
5612             return True
5613 
5614         return False
5615 
5616     def _get_host_numa_topology(self):
5617         if not self._has_numa_support():
5618             return
5619 
5620         caps = self._host.get_capabilities()
5621         topology = caps.host.topology
5622 
5623         if topology is None or not topology.cells:
5624             return
5625 
5626         cells = []
5627         allowed_cpus = hardware.get_vcpu_pin_set()
5628         online_cpus = self._host.get_online_cpus()
5629         if allowed_cpus:
5630             allowed_cpus &= online_cpus
5631         else:
5632             allowed_cpus = online_cpus
5633 
5634         def _get_reserved_memory_for_cell(self, cell_id, page_size):
5635             cell = self._reserved_hugepages.get(cell_id, {})
5636             return cell.get(page_size, 0)
5637 
5638         for cell in topology.cells:
5639             cpuset = set(cpu.id for cpu in cell.cpus)
5640             siblings = sorted(map(set,
5641                                   set(tuple(cpu.siblings)
5642                                         if cpu.siblings else ()
5643                                       for cpu in cell.cpus)
5644                                   ))
5645             cpuset &= allowed_cpus
5646             siblings = [sib & allowed_cpus for sib in siblings]
5647             # Filter out singles and empty sibling sets that may be left
5648             siblings = [sib for sib in siblings if len(sib) > 1]
5649 
5650             mempages = [
5651                 objects.NUMAPagesTopology(
5652                     size_kb=pages.size,
5653                     total=pages.total,
5654                     used=0,
5655                     reserved=_get_reserved_memory_for_cell(
5656                         self, cell.id, pages.size))
5657                 for pages in cell.mempages]
5658 
5659             cell = objects.NUMACell(id=cell.id, cpuset=cpuset,
5660                                     memory=cell.memory / units.Ki,
5661                                     cpu_usage=0, memory_usage=0,
5662                                     siblings=siblings,
5663                                     pinned_cpus=set([]),
5664                                     mempages=mempages)
5665             cells.append(cell)
5666 
5667         return objects.NUMATopology(cells=cells)
5668 
5669     def get_all_volume_usage(self, context, compute_host_bdms):
5670         """Return usage info for volumes attached to vms on
5671            a given host.
5672         """
5673         vol_usage = []
5674 
5675         for instance_bdms in compute_host_bdms:
5676             instance = instance_bdms['instance']
5677 
5678             for bdm in instance_bdms['instance_bdms']:
5679                 mountpoint = bdm['device_name']
5680                 if mountpoint.startswith('/dev/'):
5681                     mountpoint = mountpoint[5:]
5682                 volume_id = bdm['volume_id']
5683 
5684                 LOG.debug("Trying to get stats for the volume %s",
5685                           volume_id, instance=instance)
5686                 vol_stats = self.block_stats(instance, mountpoint)
5687 
5688                 if vol_stats:
5689                     stats = dict(volume=volume_id,
5690                                  instance=instance,
5691                                  rd_req=vol_stats[0],
5692                                  rd_bytes=vol_stats[1],
5693                                  wr_req=vol_stats[2],
5694                                  wr_bytes=vol_stats[3])
5695                     LOG.debug(
5696                         "Got volume usage stats for the volume=%(volume)s,"
5697                         " rd_req=%(rd_req)d, rd_bytes=%(rd_bytes)d, "
5698                         "wr_req=%(wr_req)d, wr_bytes=%(wr_bytes)d",
5699                         stats, instance=instance)
5700                     vol_usage.append(stats)
5701 
5702         return vol_usage
5703 
5704     def block_stats(self, instance, disk_id):
5705         """Note that this function takes an instance name."""
5706         try:
5707             guest = self._host.get_guest(instance)
5708 
5709             # TODO(sahid): We are converting all calls from a
5710             # virDomain object to use nova.virt.libvirt.Guest.
5711             # We should be able to remove domain at the end.
5712             domain = guest._domain
5713             return domain.blockStats(disk_id)
5714         except libvirt.libvirtError as e:
5715             errcode = e.get_error_code()
5716             LOG.info('Getting block stats failed, device might have '
5717                      'been detached. Instance=%(instance_name)s '
5718                      'Disk=%(disk)s Code=%(errcode)s Error=%(e)s',
5719                      {'instance_name': instance.name, 'disk': disk_id,
5720                       'errcode': errcode, 'e': e},
5721                      instance=instance)
5722         except exception.InstanceNotFound:
5723             LOG.info('Could not find domain in libvirt for instance %s. '
5724                      'Cannot get block stats for device', instance.name,
5725                      instance=instance)
5726 
5727     def get_console_pool_info(self, console_type):
5728         # TODO(mdragon): console proxy should be implemented for libvirt,
5729         #                in case someone wants to use it with kvm or
5730         #                such. For now return fake data.
5731         return {'address': '127.0.0.1',
5732                 'username': 'fakeuser',
5733                 'password': 'fakepassword'}
5734 
5735     def refresh_security_group_rules(self, security_group_id):
5736         self.firewall_driver.refresh_security_group_rules(security_group_id)
5737 
5738     def refresh_instance_security_rules(self, instance):
5739         self.firewall_driver.refresh_instance_security_rules(instance)
5740 
5741     def get_inventory(self, nodename):
5742         """Return a dict, keyed by resource class, of inventory information for
5743         the supplied node.
5744         """
5745         disk_gb = int(self._get_local_gb_info()['total'])
5746         memory_mb = int(self._host.get_memory_mb_total())
5747         vcpus = self._get_vcpu_total()
5748         # NOTE(jaypipes): We leave some fields like allocation_ratio and
5749         # reserved out of the returned dicts here because, for now at least,
5750         # the RT injects those values into the inventory dict based on the
5751         # compute_nodes record values.
5752         result = {
5753             fields.ResourceClass.VCPU: {
5754                 'total': vcpus,
5755                 'min_unit': 1,
5756                 'max_unit': vcpus,
5757                 'step_size': 1,
5758             },
5759             fields.ResourceClass.MEMORY_MB: {
5760                 'total': memory_mb,
5761                 'min_unit': 1,
5762                 'max_unit': memory_mb,
5763                 'step_size': 1,
5764             },
5765             fields.ResourceClass.DISK_GB: {
5766                 'total': disk_gb,
5767                 'min_unit': 1,
5768                 'max_unit': disk_gb,
5769                 'step_size': 1,
5770             },
5771         }
5772         return result
5773 
5774     def get_available_resource(self, nodename):
5775         """Retrieve resource information.
5776 
5777         This method is called when nova-compute launches, and
5778         as part of a periodic task that records the results in the DB.
5779 
5780         :param nodename: unused in this driver
5781         :returns: dictionary containing resource info
5782         """
5783 
5784         disk_info_dict = self._get_local_gb_info()
5785         data = {}
5786 
5787         # NOTE(dprince): calling capabilities before getVersion works around
5788         # an initialization issue with some versions of Libvirt (1.0.5.5).
5789         # See: https://bugzilla.redhat.com/show_bug.cgi?id=1000116
5790         # See: https://bugs.launchpad.net/nova/+bug/1215593
5791         data["supported_instances"] = self._get_instance_capabilities()
5792 
5793         data["vcpus"] = self._get_vcpu_total()
5794         data["memory_mb"] = self._host.get_memory_mb_total()
5795         data["local_gb"] = disk_info_dict['total']
5796         data["vcpus_used"] = self._get_vcpu_used()
5797         data["memory_mb_used"] = self._host.get_memory_mb_used()
5798         data["local_gb_used"] = disk_info_dict['used']
5799         data["hypervisor_type"] = self._host.get_driver_type()
5800         data["hypervisor_version"] = self._host.get_version()
5801         data["hypervisor_hostname"] = self._host.get_hostname()
5802         # TODO(berrange): why do we bother converting the
5803         # libvirt capabilities XML into a special JSON format ?
5804         # The data format is different across all the drivers
5805         # so we could just return the raw capabilities XML
5806         # which 'compare_cpu' could use directly
5807         #
5808         # That said, arch_filter.py now seems to rely on
5809         # the libvirt drivers format which suggests this
5810         # data format needs to be standardized across drivers
5811         data["cpu_info"] = jsonutils.dumps(self._get_cpu_info())
5812 
5813         disk_free_gb = disk_info_dict['free']
5814         disk_over_committed = self._get_disk_over_committed_size_total()
5815         available_least = disk_free_gb * units.Gi - disk_over_committed
5816         data['disk_available_least'] = available_least / units.Gi
5817 
5818         data['pci_passthrough_devices'] = \
5819             self._get_pci_passthrough_devices()
5820 
5821         numa_topology = self._get_host_numa_topology()
5822         if numa_topology:
5823             data['numa_topology'] = numa_topology._to_json()
5824         else:
5825             data['numa_topology'] = None
5826 
5827         return data
5828 
5829     def check_instance_shared_storage_local(self, context, instance):
5830         """Check if instance files located on shared storage.
5831 
5832         This runs check on the destination host, and then calls
5833         back to the source host to check the results.
5834 
5835         :param context: security context
5836         :param instance: nova.objects.instance.Instance object
5837         :returns:
5838          - tempfile: A dict containing the tempfile info on the destination
5839                      host
5840          - None:
5841 
5842             1. If the instance path is not existing.
5843             2. If the image backend is shared block storage type.
5844         """
5845         if self.image_backend.backend().is_shared_block_storage():
5846             return None
5847 
5848         dirpath = libvirt_utils.get_instance_path(instance)
5849 
5850         if not os.path.exists(dirpath):
5851             return None
5852 
5853         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
5854         LOG.debug("Creating tmpfile %s to verify with other "
5855                   "compute node that the instance is on "
5856                   "the same shared storage.",
5857                   tmp_file, instance=instance)
5858         os.close(fd)
5859         return {"filename": tmp_file}
5860 
5861     def check_instance_shared_storage_remote(self, context, data):
5862         return os.path.exists(data['filename'])
5863 
5864     def check_instance_shared_storage_cleanup(self, context, data):
5865         fileutils.delete_if_exists(data["filename"])
5866 
5867     def check_can_live_migrate_destination(self, context, instance,
5868                                            src_compute_info, dst_compute_info,
5869                                            block_migration=False,
5870                                            disk_over_commit=False):
5871         """Check if it is possible to execute live migration.
5872 
5873         This runs checks on the destination host, and then calls
5874         back to the source host to check the results.
5875 
5876         :param context: security context
5877         :param instance: nova.db.sqlalchemy.models.Instance
5878         :param block_migration: if true, prepare for block migration
5879         :param disk_over_commit: if true, allow disk over commit
5880         :returns: a LibvirtLiveMigrateData object
5881         """
5882         disk_available_gb = dst_compute_info['disk_available_least']
5883         disk_available_mb = (
5884             (disk_available_gb * units.Ki) - CONF.reserved_host_disk_mb)
5885 
5886         # Compare CPU
5887         if not instance.vcpu_model or not instance.vcpu_model.model:
5888             source_cpu_info = src_compute_info['cpu_info']
5889             self._compare_cpu(None, source_cpu_info, instance)
5890         else:
5891             self._compare_cpu(instance.vcpu_model, None, instance)
5892 
5893         # Create file on storage, to be checked on source host
5894         filename = self._create_shared_storage_test_file(instance)
5895 
5896         data = objects.LibvirtLiveMigrateData()
5897         data.filename = filename
5898         data.image_type = CONF.libvirt.images_type
5899         data.graphics_listen_addr_vnc = CONF.vnc.server_listen
5900         data.graphics_listen_addr_spice = CONF.spice.server_listen
5901         if CONF.serial_console.enabled:
5902             data.serial_listen_addr = CONF.serial_console.proxyclient_address
5903         else:
5904             data.serial_listen_addr = None
5905         # Notes(eliqiao): block_migration and disk_over_commit are not
5906         # nullable, so just don't set them if they are None
5907         if block_migration is not None:
5908             data.block_migration = block_migration
5909         if disk_over_commit is not None:
5910             data.disk_over_commit = disk_over_commit
5911         data.disk_available_mb = disk_available_mb
5912         return data
5913 
5914     def cleanup_live_migration_destination_check(self, context,
5915                                                  dest_check_data):
5916         """Do required cleanup on dest host after check_can_live_migrate calls
5917 
5918         :param context: security context
5919         """
5920         filename = dest_check_data.filename
5921         self._cleanup_shared_storage_test_file(filename)
5922 
5923     def check_can_live_migrate_source(self, context, instance,
5924                                       dest_check_data,
5925                                       block_device_info=None):
5926         """Check if it is possible to execute live migration.
5927 
5928         This checks if the live migration can succeed, based on the
5929         results from check_can_live_migrate_destination.
5930 
5931         :param context: security context
5932         :param instance: nova.db.sqlalchemy.models.Instance
5933         :param dest_check_data: result of check_can_live_migrate_destination
5934         :param block_device_info: result of _get_instance_block_device_info
5935         :returns: a LibvirtLiveMigrateData object
5936         """
5937         if not isinstance(dest_check_data, migrate_data_obj.LiveMigrateData):
5938             md_obj = objects.LibvirtLiveMigrateData()
5939             md_obj.from_legacy_dict(dest_check_data)
5940             dest_check_data = md_obj
5941 
5942         # Checking shared storage connectivity
5943         # if block migration, instances_path should not be on shared storage.
5944         source = CONF.host
5945 
5946         dest_check_data.is_shared_instance_path = (
5947             self._check_shared_storage_test_file(
5948                 dest_check_data.filename, instance))
5949 
5950         dest_check_data.is_shared_block_storage = (
5951             self._is_shared_block_storage(instance, dest_check_data,
5952                                           block_device_info))
5953 
5954         if 'block_migration' not in dest_check_data:
5955             dest_check_data.block_migration = (
5956                 not dest_check_data.is_on_shared_storage())
5957 
5958         if dest_check_data.block_migration:
5959             # TODO(eliqiao): Once block_migration flag is removed from the API
5960             # we can safely remove the if condition
5961             if dest_check_data.is_on_shared_storage():
5962                 reason = _("Block migration can not be used "
5963                            "with shared storage.")
5964                 raise exception.InvalidLocalStorage(reason=reason, path=source)
5965             if 'disk_over_commit' in dest_check_data:
5966                 self._assert_dest_node_has_enough_disk(context, instance,
5967                                         dest_check_data.disk_available_mb,
5968                                         dest_check_data.disk_over_commit,
5969                                         block_device_info)
5970             if block_device_info:
5971                 bdm = block_device_info.get('block_device_mapping')
5972                 # NOTE(pkoniszewski): libvirt from version 1.2.17 upwards
5973                 # supports selective block device migration. It means that it
5974                 # is possible to define subset of block devices to be copied
5975                 # during migration. If they are not specified - block devices
5976                 # won't be migrated. However, it does not work when live
5977                 # migration is tunnelled through libvirt.
5978                 if bdm and not self._host.has_min_version(
5979                         MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
5980                     # NOTE(stpierre): if this instance has mapped volumes,
5981                     # we can't do a block migration, since that will result
5982                     # in volumes being copied from themselves to themselves,
5983                     # which is a recipe for disaster.
5984                     ver = ".".join([str(x) for x in
5985                                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION])
5986                     msg = (_('Cannot block migrate instance %(uuid)s with'
5987                              ' mapped volumes. Selective block device'
5988                              ' migration feature requires libvirt version'
5989                              ' %(libvirt_ver)s') %
5990                            {'uuid': instance.uuid, 'libvirt_ver': ver})
5991                     LOG.error(msg, instance=instance)
5992                     raise exception.MigrationPreCheckError(reason=msg)
5993                 # NOTE(eliqiao): Selective disk migrations are not supported
5994                 # with tunnelled block migrations so we can block them early.
5995                 if (bdm and
5996                     (self._block_migration_flags &
5997                      libvirt.VIR_MIGRATE_TUNNELLED != 0)):
5998                     msg = (_('Cannot block migrate instance %(uuid)s with'
5999                              ' mapped volumes. Selective block device'
6000                              ' migration is not supported with tunnelled'
6001                              ' block migrations.') % {'uuid': instance.uuid})
6002                     LOG.error(msg, instance=instance)
6003                     raise exception.MigrationPreCheckError(reason=msg)
6004         elif not (dest_check_data.is_shared_block_storage or
6005                   dest_check_data.is_shared_instance_path):
6006             reason = _("Shared storage live-migration requires either shared "
6007                        "storage or boot-from-volume with no local disks.")
6008             raise exception.InvalidSharedStorage(reason=reason, path=source)
6009 
6010         # NOTE(mikal): include the instance directory name here because it
6011         # doesn't yet exist on the destination but we want to force that
6012         # same name to be used
6013         instance_path = libvirt_utils.get_instance_path(instance,
6014                                                         relative=True)
6015         dest_check_data.instance_relative_path = instance_path
6016 
6017         return dest_check_data
6018 
6019     def _is_shared_block_storage(self, instance, dest_check_data,
6020                                  block_device_info=None):
6021         """Check if all block storage of an instance can be shared
6022         between source and destination of a live migration.
6023 
6024         Returns true if the instance is volume backed and has no local disks,
6025         or if the image backend is the same on source and destination and the
6026         backend shares block storage between compute nodes.
6027 
6028         :param instance: nova.objects.instance.Instance object
6029         :param dest_check_data: dict with boolean fields image_type,
6030                                 is_shared_instance_path, and is_volume_backed
6031         """
6032         if (dest_check_data.obj_attr_is_set('image_type') and
6033                 CONF.libvirt.images_type == dest_check_data.image_type and
6034                 self.image_backend.backend().is_shared_block_storage()):
6035             # NOTE(dgenin): currently true only for RBD image backend
6036             return True
6037 
6038         if (dest_check_data.is_shared_instance_path and
6039                 self.image_backend.backend().is_file_in_instance_path()):
6040             # NOTE(angdraug): file based image backends (Flat, Qcow2)
6041             # place block device files under the instance path
6042             return True
6043 
6044         if (dest_check_data.is_volume_backed and
6045                 not bool(self._get_instance_disk_info(instance,
6046                                                       block_device_info))):
6047             return True
6048 
6049         return False
6050 
6051     def _assert_dest_node_has_enough_disk(self, context, instance,
6052                                              available_mb, disk_over_commit,
6053                                              block_device_info):
6054         """Checks if destination has enough disk for block migration."""
6055         # Libvirt supports qcow2 disk format,which is usually compressed
6056         # on compute nodes.
6057         # Real disk image (compressed) may enlarged to "virtual disk size",
6058         # that is specified as the maximum disk size.
6059         # (See qemu-img -f path-to-disk)
6060         # Scheduler recognizes destination host still has enough disk space
6061         # if real disk size < available disk size
6062         # if disk_over_commit is True,
6063         #  otherwise virtual disk size < available disk size.
6064 
6065         available = 0
6066         if available_mb:
6067             available = available_mb * units.Mi
6068 
6069         disk_infos = self._get_instance_disk_info(instance, block_device_info)
6070 
6071         necessary = 0
6072         if disk_over_commit:
6073             for info in disk_infos:
6074                 necessary += int(info['disk_size'])
6075         else:
6076             for info in disk_infos:
6077                 necessary += int(info['virt_disk_size'])
6078 
6079         # Check that available disk > necessary disk
6080         if (available - necessary) < 0:
6081             reason = (_('Unable to migrate %(instance_uuid)s: '
6082                         'Disk of instance is too large(available'
6083                         ' on destination host:%(available)s '
6084                         '< need:%(necessary)s)') %
6085                       {'instance_uuid': instance.uuid,
6086                        'available': available,
6087                        'necessary': necessary})
6088             raise exception.MigrationPreCheckError(reason=reason)
6089 
6090     def _compare_cpu(self, guest_cpu, host_cpu_str, instance):
6091         """Check the host is compatible with the requested CPU
6092 
6093         :param guest_cpu: nova.objects.VirtCPUModel or None
6094         :param host_cpu_str: JSON from _get_cpu_info() method
6095 
6096         If the 'guest_cpu' parameter is not None, this will be
6097         validated for migration compatibility with the host.
6098         Otherwise the 'host_cpu_str' JSON string will be used for
6099         validation.
6100 
6101         :returns:
6102             None. if given cpu info is not compatible to this server,
6103             raise exception.
6104         """
6105 
6106         # NOTE(kchamart): Comparing host to guest CPU model for emulated
6107         # guests (<domain type='qemu'>) should not matter -- in this
6108         # mode (QEMU "TCG") the CPU is fully emulated in software and no
6109         # hardware acceleration, like KVM, is involved. So, skip the CPU
6110         # compatibility check for the QEMU domain type, and retain it for
6111         # KVM guests.
6112         if CONF.libvirt.virt_type not in ['kvm']:
6113             return
6114 
6115         if guest_cpu is None:
6116             info = jsonutils.loads(host_cpu_str)
6117             LOG.info('Instance launched has CPU info: %s', host_cpu_str)
6118             cpu = vconfig.LibvirtConfigCPU()
6119             cpu.arch = info['arch']
6120             cpu.model = info['model']
6121             cpu.vendor = info['vendor']
6122             cpu.sockets = info['topology']['sockets']
6123             cpu.cores = info['topology']['cores']
6124             cpu.threads = info['topology']['threads']
6125             for f in info['features']:
6126                 cpu.add_feature(vconfig.LibvirtConfigCPUFeature(f))
6127         else:
6128             cpu = self._vcpu_model_to_cpu_config(guest_cpu)
6129 
6130         u = ("http://libvirt.org/html/libvirt-libvirt-host.html#"
6131              "virCPUCompareResult")
6132         m = _("CPU doesn't have compatibility.\n\n%(ret)s\n\nRefer to %(u)s")
6133         # unknown character exists in xml, then libvirt complains
6134         try:
6135             cpu_xml = cpu.to_xml()
6136             LOG.debug("cpu compare xml: %s", cpu_xml, instance=instance)
6137             ret = self._host.compare_cpu(cpu_xml)
6138         except libvirt.libvirtError as e:
6139             error_code = e.get_error_code()
6140             if error_code == libvirt.VIR_ERR_NO_SUPPORT:
6141                 LOG.debug("URI %(uri)s does not support cpu comparison. "
6142                           "It will be proceeded though. Error: %(error)s",
6143                           {'uri': self._uri(), 'error': e})
6144                 return
6145             else:
6146                 LOG.error(m, {'ret': e, 'u': u})
6147                 raise exception.MigrationPreCheckError(
6148                     reason=m % {'ret': e, 'u': u})
6149 
6150         if ret <= 0:
6151             LOG.error(m, {'ret': ret, 'u': u})
6152             raise exception.InvalidCPUInfo(reason=m % {'ret': ret, 'u': u})
6153 
6154     def _create_shared_storage_test_file(self, instance):
6155         """Makes tmpfile under CONF.instances_path."""
6156         dirpath = CONF.instances_path
6157         fd, tmp_file = tempfile.mkstemp(dir=dirpath)
6158         LOG.debug("Creating tmpfile %s to notify to other "
6159                   "compute nodes that they should mount "
6160                   "the same storage.", tmp_file, instance=instance)
6161         os.close(fd)
6162         return os.path.basename(tmp_file)
6163 
6164     def _check_shared_storage_test_file(self, filename, instance):
6165         """Confirms existence of the tmpfile under CONF.instances_path.
6166 
6167         Cannot confirm tmpfile return False.
6168         """
6169         # NOTE(tpatzig): if instances_path is a shared volume that is
6170         # under heavy IO (many instances on many compute nodes),
6171         # then checking the existence of the testfile fails,
6172         # just because it takes longer until the client refreshes and new
6173         # content gets visible.
6174         # os.utime (like touch) on the directory forces the client to refresh.
6175         os.utime(CONF.instances_path, None)
6176 
6177         tmp_file = os.path.join(CONF.instances_path, filename)
6178         if not os.path.exists(tmp_file):
6179             exists = False
6180         else:
6181             exists = True
6182         LOG.debug('Check if temp file %s exists to indicate shared storage '
6183                   'is being used for migration. Exists? %s', tmp_file, exists,
6184                   instance=instance)
6185         return exists
6186 
6187     def _cleanup_shared_storage_test_file(self, filename):
6188         """Removes existence of the tmpfile under CONF.instances_path."""
6189         tmp_file = os.path.join(CONF.instances_path, filename)
6190         os.remove(tmp_file)
6191 
6192     def ensure_filtering_rules_for_instance(self, instance, network_info):
6193         """Ensure that an instance's filtering rules are enabled.
6194 
6195         When migrating an instance, we need the filtering rules to
6196         be configured on the destination host before starting the
6197         migration.
6198 
6199         Also, when restarting the compute service, we need to ensure
6200         that filtering rules exist for all running services.
6201         """
6202 
6203         self.firewall_driver.setup_basic_filtering(instance, network_info)
6204         self.firewall_driver.prepare_instance_filter(instance,
6205                 network_info)
6206 
6207         # nwfilters may be defined in a separate thread in the case
6208         # of libvirt non-blocking mode, so we wait for completion
6209         timeout_count = list(range(CONF.live_migration_retry_count))
6210         while timeout_count:
6211             if self.firewall_driver.instance_filter_exists(instance,
6212                                                            network_info):
6213                 break
6214             timeout_count.pop()
6215             if len(timeout_count) == 0:
6216                 msg = _('The firewall filter for %s does not exist')
6217                 raise exception.InternalError(msg % instance.name)
6218             greenthread.sleep(1)
6219 
6220     def filter_defer_apply_on(self):
6221         self.firewall_driver.filter_defer_apply_on()
6222 
6223     def filter_defer_apply_off(self):
6224         self.firewall_driver.filter_defer_apply_off()
6225 
6226     def live_migration(self, context, instance, dest,
6227                        post_method, recover_method, block_migration=False,
6228                        migrate_data=None):
6229         """Spawning live_migration operation for distributing high-load.
6230 
6231         :param context: security context
6232         :param instance:
6233             nova.db.sqlalchemy.models.Instance object
6234             instance object that is migrated.
6235         :param dest: destination host
6236         :param post_method:
6237             post operation method.
6238             expected nova.compute.manager._post_live_migration.
6239         :param recover_method:
6240             recovery method when any exception occurs.
6241             expected nova.compute.manager._rollback_live_migration.
6242         :param block_migration: if true, do block migration.
6243         :param migrate_data: a LibvirtLiveMigrateData object
6244 
6245         """
6246 
6247         # 'dest' will be substituted into 'migration_uri' so ensure
6248         # it does't contain any characters that could be used to
6249         # exploit the URI accepted by libivrt
6250         if not libvirt_utils.is_valid_hostname(dest):
6251             raise exception.InvalidHostname(hostname=dest)
6252 
6253         self._live_migration(context, instance, dest,
6254                              post_method, recover_method, block_migration,
6255                              migrate_data)
6256 
6257     def live_migration_abort(self, instance):
6258         """Aborting a running live-migration.
6259 
6260         :param instance: instance object that is in migration
6261 
6262         """
6263 
6264         guest = self._host.get_guest(instance)
6265         dom = guest._domain
6266 
6267         try:
6268             dom.abortJob()
6269         except libvirt.libvirtError as e:
6270             LOG.error("Failed to cancel migration %s", e, instance=instance)
6271             raise
6272 
6273     def _verify_serial_console_is_disabled(self):
6274         if CONF.serial_console.enabled:
6275 
6276             msg = _('Your destination node does not support'
6277                     ' retrieving listen addresses.  In order'
6278                     ' for live migration to work properly you'
6279                     ' must disable serial console.')
6280             raise exception.MigrationError(reason=msg)
6281 
6282     def _live_migration_operation(self, context, instance, dest,
6283                                   block_migration, migrate_data, guest,
6284                                   device_names):
6285         """Invoke the live migration operation
6286 
6287         :param context: security context
6288         :param instance:
6289             nova.db.sqlalchemy.models.Instance object
6290             instance object that is migrated.
6291         :param dest: destination host
6292         :param block_migration: if true, do block migration.
6293         :param migrate_data: a LibvirtLiveMigrateData object
6294         :param guest: the guest domain object
6295         :param device_names: list of device names that are being migrated with
6296             instance
6297 
6298         This method is intended to be run in a background thread and will
6299         block that thread until the migration is finished or failed.
6300         """
6301         try:
6302             if migrate_data.block_migration:
6303                 migration_flags = self._block_migration_flags
6304             else:
6305                 migration_flags = self._live_migration_flags
6306 
6307             serial_listen_addr = libvirt_migrate.serial_listen_addr(
6308                 migrate_data)
6309             if not serial_listen_addr:
6310                 # In this context we want to ensure that serial console is
6311                 # disabled on source node. This is because nova couldn't
6312                 # retrieve serial listen address from destination node, so we
6313                 # consider that destination node might have serial console
6314                 # disabled as well.
6315                 self._verify_serial_console_is_disabled()
6316 
6317             # NOTE(aplanas) migrate_uri will have a value only in the
6318             # case that `live_migration_inbound_addr` parameter is
6319             # set, and we propose a non tunneled migration.
6320             migrate_uri = None
6321             if ('target_connect_addr' in migrate_data and
6322                     migrate_data.target_connect_addr is not None):
6323                 dest = migrate_data.target_connect_addr
6324                 if (migration_flags &
6325                     libvirt.VIR_MIGRATE_TUNNELLED == 0):
6326                     migrate_uri = self._migrate_uri(dest)
6327 
6328             params = None
6329             new_xml_str = None
6330             if CONF.libvirt.virt_type != "parallels":
6331                 new_xml_str = libvirt_migrate.get_updated_guest_xml(
6332                     # TODO(sahid): It's not a really good idea to pass
6333                     # the method _get_volume_config and we should to find
6334                     # a way to avoid this in future.
6335                     guest, migrate_data, self._get_volume_config)
6336             if self._host.has_min_version(
6337                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION):
6338                 params = {
6339                     'destination_xml': new_xml_str,
6340                     'migrate_disks': device_names,
6341                 }
6342                 # NOTE(pkoniszewski): Because of precheck which blocks
6343                 # tunnelled block live migration with mapped volumes we
6344                 # can safely remove migrate_disks when tunnelling is on.
6345                 # Otherwise we will block all tunnelled block migrations,
6346                 # even when an instance does not have volumes mapped.
6347                 # This is because selective disk migration is not
6348                 # supported in tunnelled block live migration. Also we
6349                 # cannot fallback to migrateToURI2 in this case because of
6350                 # bug #1398999
6351                 if (migration_flags &
6352                     libvirt.VIR_MIGRATE_TUNNELLED != 0):
6353                     params.pop('migrate_disks')
6354 
6355             # TODO(sahid): This should be in
6356             # post_live_migration_at_source but no way to retrieve
6357             # ports acquired on the host for the guest at this
6358             # step. Since the domain is going to be removed from
6359             # libvird on source host after migration, we backup the
6360             # serial ports to release them if all went well.
6361             serial_ports = []
6362             if CONF.serial_console.enabled:
6363                 serial_ports = list(self._get_serial_ports_from_guest(guest))
6364 
6365             guest.migrate(self._live_migration_uri(dest),
6366                           migrate_uri=migrate_uri,
6367                           flags=migration_flags,
6368                           params=params,
6369                           domain_xml=new_xml_str,
6370                           bandwidth=CONF.libvirt.live_migration_bandwidth)
6371 
6372             for hostname, port in serial_ports:
6373                 serial_console.release_port(host=hostname, port=port)
6374         except Exception as e:
6375             with excutils.save_and_reraise_exception():
6376                 LOG.error("Live Migration failure: %s", e, instance=instance)
6377 
6378         # If 'migrateToURI' fails we don't know what state the
6379         # VM instances on each host are in. Possibilities include
6380         #
6381         #  1. src==running, dst==none
6382         #
6383         #     Migration failed & rolled back, or never started
6384         #
6385         #  2. src==running, dst==paused
6386         #
6387         #     Migration started but is still ongoing
6388         #
6389         #  3. src==paused,  dst==paused
6390         #
6391         #     Migration data transfer completed, but switchover
6392         #     is still ongoing, or failed
6393         #
6394         #  4. src==paused,  dst==running
6395         #
6396         #     Migration data transfer completed, switchover
6397         #     happened but cleanup on source failed
6398         #
6399         #  5. src==none,    dst==running
6400         #
6401         #     Migration fully succeeded.
6402         #
6403         # Libvirt will aim to complete any migration operation
6404         # or roll it back. So even if the migrateToURI call has
6405         # returned an error, if the migration was not finished
6406         # libvirt should clean up.
6407         #
6408         # So we take the error raise here with a pinch of salt
6409         # and rely on the domain job info status to figure out
6410         # what really happened to the VM, which is a much more
6411         # reliable indicator.
6412         #
6413         # In particular we need to try very hard to ensure that
6414         # Nova does not "forget" about the guest. ie leaving it
6415         # running on a different host to the one recorded in
6416         # the database, as that would be a serious resource leak
6417 
6418         LOG.debug("Migration operation thread has finished",
6419                   instance=instance)
6420 
6421     def _live_migration_copy_disk_paths(self, context, instance, guest):
6422         '''Get list of disks to copy during migration
6423 
6424         :param context: security context
6425         :param instance: the instance being migrated
6426         :param guest: the Guest instance being migrated
6427 
6428         Get the list of disks to copy during migration.
6429 
6430         :returns: a list of local source paths and a list of device names to
6431             copy
6432         '''
6433 
6434         disk_paths = []
6435         device_names = []
6436         block_devices = []
6437 
6438         # TODO(pkoniszewski): Remove version check when we bump min libvirt
6439         # version to >= 1.2.17.
6440         if (self._block_migration_flags &
6441                 libvirt.VIR_MIGRATE_TUNNELLED == 0 and
6442                 self._host.has_min_version(
6443                     MIN_LIBVIRT_BLOCK_LM_WITH_VOLUMES_VERSION)):
6444             bdm_list = objects.BlockDeviceMappingList.get_by_instance_uuid(
6445                 context, instance.uuid)
6446             block_device_info = driver.get_block_device_info(instance,
6447                                                              bdm_list)
6448 
6449             block_device_mappings = driver.block_device_info_get_mapping(
6450                 block_device_info)
6451             for bdm in block_device_mappings:
6452                 device_name = str(bdm['mount_device'].rsplit('/', 1)[1])
6453                 block_devices.append(device_name)
6454 
6455         for dev in guest.get_all_disks():
6456             if dev.readonly or dev.shareable:
6457                 continue
6458             if dev.source_type not in ["file", "block"]:
6459                 continue
6460             if dev.target_dev in block_devices:
6461                 continue
6462             disk_paths.append(dev.source_path)
6463             device_names.append(dev.target_dev)
6464         return (disk_paths, device_names)
6465 
6466     def _live_migration_data_gb(self, instance, disk_paths):
6467         '''Calculate total amount of data to be transferred
6468 
6469         :param instance: the nova.objects.Instance being migrated
6470         :param disk_paths: list of disk paths that are being migrated
6471         with instance
6472 
6473         Calculates the total amount of data that needs to be
6474         transferred during the live migration. The actual
6475         amount copied will be larger than this, due to the
6476         guest OS continuing to dirty RAM while the migration
6477         is taking place. So this value represents the minimal
6478         data size possible.
6479 
6480         :returns: data size to be copied in GB
6481         '''
6482 
6483         ram_gb = instance.flavor.memory_mb * units.Mi / units.Gi
6484         if ram_gb < 2:
6485             ram_gb = 2
6486 
6487         disk_gb = 0
6488         for path in disk_paths:
6489             try:
6490                 size = os.stat(path).st_size
6491                 size_gb = (size / units.Gi)
6492                 if size_gb < 2:
6493                     size_gb = 2
6494                 disk_gb += size_gb
6495             except OSError as e:
6496                 LOG.warning("Unable to stat %(disk)s: %(ex)s",
6497                             {'disk': path, 'ex': e})
6498                 # Ignore error since we don't want to break
6499                 # the migration monitoring thread operation
6500 
6501         return ram_gb + disk_gb
6502 
6503     def _get_migration_flags(self, is_block_migration):
6504         if is_block_migration:
6505             return self._block_migration_flags
6506         return self._live_migration_flags
6507 
6508     def _live_migration_monitor(self, context, instance, guest,
6509                                 dest, post_method,
6510                                 recover_method, block_migration,
6511                                 migrate_data, finish_event,
6512                                 disk_paths):
6513         on_migration_failure = deque()
6514         data_gb = self._live_migration_data_gb(instance, disk_paths)
6515         downtime_steps = list(libvirt_migrate.downtime_steps(data_gb))
6516         migration = migrate_data.migration
6517         curdowntime = None
6518 
6519         migration_flags = self._get_migration_flags(
6520                                   migrate_data.block_migration)
6521 
6522         n = 0
6523         start = time.time()
6524         progress_time = start
6525         progress_watermark = None
6526         previous_data_remaining = -1
6527         is_post_copy_enabled = self._is_post_copy_enabled(migration_flags)
6528         while True:
6529             info = guest.get_job_info()
6530 
6531             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6532                 # Either still running, or failed or completed,
6533                 # lets untangle the mess
6534                 if not finish_event.ready():
6535                     LOG.debug("Operation thread is still running",
6536                               instance=instance)
6537                 else:
6538                     info.type = libvirt_migrate.find_job_type(guest, instance)
6539                     LOG.debug("Fixed incorrect job type to be %d",
6540                               info.type, instance=instance)
6541 
6542             if info.type == libvirt.VIR_DOMAIN_JOB_NONE:
6543                 # Migration is not yet started
6544                 LOG.debug("Migration not running yet",
6545                           instance=instance)
6546             elif info.type == libvirt.VIR_DOMAIN_JOB_UNBOUNDED:
6547                 # Migration is still running
6548                 #
6549                 # This is where we wire up calls to change live
6550                 # migration status. eg change max downtime, cancel
6551                 # the operation, change max bandwidth
6552                 libvirt_migrate.run_tasks(guest, instance,
6553                                           self.active_migrations,
6554                                           on_migration_failure,
6555                                           migration,
6556                                           is_post_copy_enabled)
6557 
6558                 now = time.time()
6559                 elapsed = now - start
6560 
6561                 if ((progress_watermark is None) or
6562                     (progress_watermark == 0) or
6563                     (progress_watermark > info.data_remaining)):
6564                     progress_watermark = info.data_remaining
6565                     progress_time = now
6566 
6567                 progress_timeout = CONF.libvirt.live_migration_progress_timeout
6568                 completion_timeout = int(
6569                     CONF.libvirt.live_migration_completion_timeout * data_gb)
6570                 if libvirt_migrate.should_abort(instance, now, progress_time,
6571                                                 progress_timeout, elapsed,
6572                                                 completion_timeout,
6573                                                 migration.status):
6574                     try:
6575                         guest.abort_job()
6576                     except libvirt.libvirtError as e:
6577                         LOG.warning("Failed to abort migration %s",
6578                                     e, instance=instance)
6579                         self._clear_empty_migration(instance)
6580                         raise
6581 
6582                 if (is_post_copy_enabled and
6583                     libvirt_migrate.should_switch_to_postcopy(
6584                     info.memory_iteration, info.data_remaining,
6585                     previous_data_remaining, migration.status)):
6586                     libvirt_migrate.trigger_postcopy_switch(guest,
6587                                                             instance,
6588                                                             migration)
6589                 previous_data_remaining = info.data_remaining
6590 
6591                 curdowntime = libvirt_migrate.update_downtime(
6592                     guest, instance, curdowntime,
6593                     downtime_steps, elapsed)
6594 
6595                 # We loop every 500ms, so don't log on every
6596                 # iteration to avoid spamming logs for long
6597                 # running migrations. Just once every 5 secs
6598                 # is sufficient for developers to debug problems.
6599                 # We log once every 30 seconds at info to help
6600                 # admins see slow running migration operations
6601                 # when debug logs are off.
6602                 if (n % 10) == 0:
6603                     # Ignoring memory_processed, as due to repeated
6604                     # dirtying of data, this can be way larger than
6605                     # memory_total. Best to just look at what's
6606                     # remaining to copy and ignore what's done already
6607                     #
6608                     # TODO(berrange) perhaps we could include disk
6609                     # transfer stats in the progress too, but it
6610                     # might make memory info more obscure as large
6611                     # disk sizes might dwarf memory size
6612                     remaining = 100
6613                     if info.memory_total != 0:
6614                         remaining = round(info.memory_remaining *
6615                                           100 / info.memory_total)
6616 
6617                     libvirt_migrate.save_stats(instance, migration,
6618                                                info, remaining)
6619 
6620                     lg = LOG.debug
6621                     if (n % 60) == 0:
6622                         lg = LOG.info
6623 
6624                     lg("Migration running for %(secs)d secs, "
6625                        "memory %(remaining)d%% remaining; "
6626                        "(bytes processed=%(processed_memory)d, "
6627                        "remaining=%(remaining_memory)d, "
6628                        "total=%(total_memory)d)",
6629                        {"secs": n / 2, "remaining": remaining,
6630                         "processed_memory": info.memory_processed,
6631                         "remaining_memory": info.memory_remaining,
6632                         "total_memory": info.memory_total}, instance=instance)
6633                     if info.data_remaining > progress_watermark:
6634                         lg("Data remaining %(remaining)d bytes, "
6635                            "low watermark %(watermark)d bytes "
6636                            "%(last)d seconds ago",
6637                            {"remaining": info.data_remaining,
6638                             "watermark": progress_watermark,
6639                             "last": (now - progress_time)}, instance=instance)
6640 
6641                 n = n + 1
6642             elif info.type == libvirt.VIR_DOMAIN_JOB_COMPLETED:
6643                 # Migration is all done
6644                 LOG.info("Migration operation has completed",
6645                          instance=instance)
6646                 post_method(context, instance, dest, block_migration,
6647                             migrate_data)
6648                 break
6649             elif info.type == libvirt.VIR_DOMAIN_JOB_FAILED:
6650                 # Migration did not succeed
6651                 LOG.error("Migration operation has aborted", instance=instance)
6652                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6653                                                   on_migration_failure)
6654                 recover_method(context, instance, dest, migrate_data)
6655                 break
6656             elif info.type == libvirt.VIR_DOMAIN_JOB_CANCELLED:
6657                 # Migration was stopped by admin
6658                 LOG.warning("Migration operation was cancelled",
6659                             instance=instance)
6660                 libvirt_migrate.run_recover_tasks(self._host, guest, instance,
6661                                                   on_migration_failure)
6662                 recover_method(context, instance, dest, migrate_data,
6663                                migration_status='cancelled')
6664                 break
6665             else:
6666                 LOG.warning("Unexpected migration job type: %d",
6667                             info.type, instance=instance)
6668 
6669             time.sleep(0.5)
6670         self._clear_empty_migration(instance)
6671 
6672     def _clear_empty_migration(self, instance):
6673         try:
6674             del self.active_migrations[instance.uuid]
6675         except KeyError:
6676             LOG.warning("There are no records in active migrations "
6677                         "for instance", instance=instance)
6678 
6679     def _live_migration(self, context, instance, dest, post_method,
6680                         recover_method, block_migration,
6681                         migrate_data):
6682         """Do live migration.
6683 
6684         :param context: security context
6685         :param instance:
6686             nova.db.sqlalchemy.models.Instance object
6687             instance object that is migrated.
6688         :param dest: destination host
6689         :param post_method:
6690             post operation method.
6691             expected nova.compute.manager._post_live_migration.
6692         :param recover_method:
6693             recovery method when any exception occurs.
6694             expected nova.compute.manager._rollback_live_migration.
6695         :param block_migration: if true, do block migration.
6696         :param migrate_data: a LibvirtLiveMigrateData object
6697 
6698         This fires off a new thread to run the blocking migration
6699         operation, and then this thread monitors the progress of
6700         migration and controls its operation
6701         """
6702 
6703         guest = self._host.get_guest(instance)
6704 
6705         disk_paths = []
6706         device_names = []
6707         if (migrate_data.block_migration and
6708                 CONF.libvirt.virt_type != "parallels"):
6709             disk_paths, device_names = self._live_migration_copy_disk_paths(
6710                 context, instance, guest)
6711 
6712         # Before to continue process of live migration we have to
6713         # ensure the VIFs on destination node are ready.
6714         LOG.debug("Slow the migration process (Pausing) until Nova receives "
6715                   "network events indicating everything is setup. "
6716                   "(default: %d)", CONF.libvirt.live_migration_bandwidth)
6717         guest.migrate_configure_max_speed(1)
6718 
6719         opthread = utils.spawn(self._live_migration_operation,
6720                                      context, instance, dest,
6721                                      block_migration,
6722                                      migrate_data, guest,
6723                                      device_names)
6724 
6725         finish_event = eventlet.event.Event()
6726         self.active_migrations[instance.uuid] = deque()
6727 
6728         def thread_finished(thread, event):
6729             LOG.debug("Migration operation thread notification",
6730                       instance=instance)
6731             event.send()
6732         opthread.link(thread_finished, finish_event)
6733 
6734         # Let eventlet schedule the new thread right away
6735         time.sleep(0)
6736 
6737         # In case of OVS hybrid plug, during pre-live-migration Nova
6738         # created on destination node the bridges which the Neutron
6739         # agent should detect. Then receiving the events from Neutron
6740         # will ensure that everything is configured correclty.
6741         #
6742         # In case of Linux Bridge, the agent is waiting for new TAPs
6743         # devices on destination node. They are going to be created by
6744         # libvirt at the very beginning of the live-migration
6745         # process. Then receiving the events from Neutron will ensure
6746         # that everything is configured correclty.
6747         network_info = instance.info_cache.network_info
6748         events = self._get_neutron_events_for_live_migration(network_info)
6749         try:
6750             with self.virtapi.wait_for_instance_event(
6751                     instance, events, deadline=CONF.vif_plugging_timeout):
6752                 LOG.debug("Waiting for VIFs plugged events: %s",
6753                           events, instance=instance)
6754         except eventlet.timeout.Timeout:
6755             LOG.warning('Timeout waiting for VIFs plugging events, '
6756                         'continuing anyway but network could have '
6757                         'connectivity issues', instance=instance)
6758         else:
6759             LOG.debug('VIFs events received, continuing migration, '
6760                       'bandwidth: %d', CONF.libvirt.live_migration_bandwidth,
6761                       instance=instance)
6762         finally:
6763             # Even if quit 'wait_for_instance_event' with a timeout or
6764             # events received we should reconfigure QEMU to use the
6765             # maximum bandwidth.
6766             guest.migrate_configure_max_speed(
6767                 CONF.libvirt.live_migration_bandwidth)
6768 
6769         try:
6770             LOG.debug("Starting monitoring of live migration",
6771                       instance=instance)
6772             self._live_migration_monitor(context, instance, guest, dest,
6773                                          post_method, recover_method,
6774                                          block_migration, migrate_data,
6775                                          finish_event, disk_paths)
6776         except Exception as ex:
6777             LOG.warning("Error monitoring migration: %(ex)s",
6778                         {"ex": ex}, instance=instance, exc_info=True)
6779             raise
6780         finally:
6781             LOG.debug("Live migration monitoring is all done",
6782                       instance=instance)
6783 
6784     def _is_post_copy_enabled(self, migration_flags):
6785         if self._is_post_copy_available():
6786             if (migration_flags & libvirt.VIR_MIGRATE_POSTCOPY) != 0:
6787                 return True
6788         return False
6789 
6790     def live_migration_force_complete(self, instance):
6791         try:
6792             self.active_migrations[instance.uuid].append('force-complete')
6793         except KeyError:
6794             raise exception.NoActiveMigrationForInstance(
6795                 instance_id=instance.uuid)
6796 
6797     def _try_fetch_image(self, context, path, image_id, instance,
6798                          fallback_from_host=None):
6799         try:
6800             libvirt_utils.fetch_image(context, path, image_id)
6801         except exception.ImageNotFound:
6802             if not fallback_from_host:
6803                 raise
6804             LOG.debug("Image %(image_id)s doesn't exist anymore on "
6805                       "image service, attempting to copy image "
6806                       "from %(host)s",
6807                       {'image_id': image_id, 'host': fallback_from_host})
6808             libvirt_utils.copy_image(src=path, dest=path,
6809                                      host=fallback_from_host,
6810                                      receive=True)
6811 
6812     def _fetch_instance_kernel_ramdisk(self, context, instance,
6813                                        fallback_from_host=None):
6814         """Download kernel and ramdisk for instance in instance directory."""
6815         instance_dir = libvirt_utils.get_instance_path(instance)
6816         if instance.kernel_id:
6817             kernel_path = os.path.join(instance_dir, 'kernel')
6818             # NOTE(dsanders): only fetch image if it's not available at
6819             # kernel_path. This also avoids ImageNotFound exception if
6820             # the image has been deleted from glance
6821             if not os.path.exists(kernel_path):
6822                 self._try_fetch_image(context,
6823                                       kernel_path,
6824                                       instance.kernel_id,
6825                                       instance, fallback_from_host)
6826             if instance.ramdisk_id:
6827                 ramdisk_path = os.path.join(instance_dir, 'ramdisk')
6828                 # NOTE(dsanders): only fetch image if it's not available at
6829                 # ramdisk_path. This also avoids ImageNotFound exception if
6830                 # the image has been deleted from glance
6831                 if not os.path.exists(ramdisk_path):
6832                     self._try_fetch_image(context,
6833                                           ramdisk_path,
6834                                           instance.ramdisk_id,
6835                                           instance, fallback_from_host)
6836 
6837     def rollback_live_migration_at_destination(self, context, instance,
6838                                                network_info,
6839                                                block_device_info,
6840                                                destroy_disks=True,
6841                                                migrate_data=None):
6842         """Clean up destination node after a failed live migration."""
6843         try:
6844             self.destroy(context, instance, network_info, block_device_info,
6845                          destroy_disks)
6846         finally:
6847             # NOTE(gcb): Failed block live migration may leave instance
6848             # directory at destination node, ensure it is always deleted.
6849             is_shared_instance_path = True
6850             if migrate_data:
6851                 is_shared_instance_path = migrate_data.is_shared_instance_path
6852                 if (migrate_data.obj_attr_is_set("serial_listen_ports")
6853                     and migrate_data.serial_listen_ports):
6854                     # Releases serial ports reserved.
6855                     for port in migrate_data.serial_listen_ports:
6856                         serial_console.release_port(
6857                             host=migrate_data.serial_listen_addr, port=port)
6858 
6859             if not is_shared_instance_path:
6860                 instance_dir = libvirt_utils.get_instance_path_at_destination(
6861                     instance, migrate_data)
6862                 if os.path.exists(instance_dir):
6863                     shutil.rmtree(instance_dir)
6864 
6865     def pre_live_migration(self, context, instance, block_device_info,
6866                            network_info, disk_info, migrate_data):
6867         """Preparation live migration."""
6868         if disk_info is not None:
6869             disk_info = jsonutils.loads(disk_info)
6870 
6871         LOG.debug('migrate_data in pre_live_migration: %s', migrate_data,
6872                   instance=instance)
6873         is_shared_block_storage = migrate_data.is_shared_block_storage
6874         is_shared_instance_path = migrate_data.is_shared_instance_path
6875         is_block_migration = migrate_data.block_migration
6876 
6877         if not is_shared_instance_path:
6878             instance_dir = libvirt_utils.get_instance_path_at_destination(
6879                             instance, migrate_data)
6880 
6881             if os.path.exists(instance_dir):
6882                 raise exception.DestinationDiskExists(path=instance_dir)
6883 
6884             LOG.debug('Creating instance directory: %s', instance_dir,
6885                       instance=instance)
6886             os.mkdir(instance_dir)
6887 
6888             # Recreate the disk.info file and in doing so stop the
6889             # imagebackend from recreating it incorrectly by inspecting the
6890             # contents of each file when using the Raw backend.
6891             if disk_info:
6892                 image_disk_info = {}
6893                 for info in disk_info:
6894                     image_file = os.path.basename(info['path'])
6895                     image_path = os.path.join(instance_dir, image_file)
6896                     image_disk_info[image_path] = info['type']
6897 
6898                 LOG.debug('Creating disk.info with the contents: %s',
6899                           image_disk_info, instance=instance)
6900 
6901                 image_disk_info_path = os.path.join(instance_dir,
6902                                                     'disk.info')
6903                 libvirt_utils.write_to_file(image_disk_info_path,
6904                                             jsonutils.dumps(image_disk_info))
6905 
6906             if not is_shared_block_storage:
6907                 # Ensure images and backing files are present.
6908                 LOG.debug('Checking to make sure images and backing files are '
6909                           'present before live migration.', instance=instance)
6910                 self._create_images_and_backing(
6911                     context, instance, instance_dir, disk_info,
6912                     fallback_from_host=instance.host)
6913                 if (configdrive.required_by(instance) and
6914                         CONF.config_drive_format == 'iso9660'):
6915                     # NOTE(pkoniszewski): Due to a bug in libvirt iso config
6916                     # drive needs to be copied to destination prior to
6917                     # migration when instance path is not shared and block
6918                     # storage is not shared. Files that are already present
6919                     # on destination are excluded from a list of files that
6920                     # need to be copied to destination. If we don't do that
6921                     # live migration will fail on copying iso config drive to
6922                     # destination and writing to read-only device.
6923                     # Please see bug/1246201 for more details.
6924                     src = "%s:%s/disk.config" % (instance.host, instance_dir)
6925                     self._remotefs.copy_file(src, instance_dir)
6926 
6927             if not is_block_migration:
6928                 # NOTE(angdraug): when block storage is shared between source
6929                 # and destination and instance path isn't (e.g. volume backed
6930                 # or rbd backed instance), instance path on destination has to
6931                 # be prepared
6932 
6933                 # Required by Quobyte CI
6934                 self._ensure_console_log_for_instance(instance)
6935 
6936                 # if image has kernel and ramdisk, just download
6937                 # following normal way.
6938                 self._fetch_instance_kernel_ramdisk(context, instance)
6939 
6940         # Establishing connection to volume server.
6941         block_device_mapping = driver.block_device_info_get_mapping(
6942             block_device_info)
6943 
6944         if len(block_device_mapping):
6945             LOG.debug('Connecting volumes before live migration.',
6946                       instance=instance)
6947 
6948         for bdm in block_device_mapping:
6949             connection_info = bdm['connection_info']
6950             disk_info = blockinfo.get_info_from_bdm(
6951                 instance, CONF.libvirt.virt_type,
6952                 instance.image_meta, bdm)
6953             self._connect_volume(connection_info, disk_info, instance)
6954 
6955         # We call plug_vifs before the compute manager calls
6956         # ensure_filtering_rules_for_instance, to ensure bridge is set up
6957         # Retry operation is necessary because continuously request comes,
6958         # concurrent request occurs to iptables, then it complains.
6959         LOG.debug('Plugging VIFs before live migration.', instance=instance)
6960         max_retry = CONF.live_migration_retry_count
6961         for cnt in range(max_retry):
6962             try:
6963                 self.plug_vifs(instance, network_info)
6964                 break
6965             except processutils.ProcessExecutionError:
6966                 if cnt == max_retry - 1:
6967                     raise
6968                 else:
6969                     LOG.warning('plug_vifs() failed %(cnt)d. Retry up to '
6970                                 '%(max_retry)d.',
6971                                 {'cnt': cnt, 'max_retry': max_retry},
6972                                 instance=instance)
6973                     greenthread.sleep(1)
6974 
6975         # Store server_listen and latest disk device info
6976         if not migrate_data:
6977             migrate_data = objects.LibvirtLiveMigrateData(bdms=[])
6978         else:
6979             migrate_data.bdms = []
6980         # Store live_migration_inbound_addr
6981         migrate_data.target_connect_addr = \
6982             CONF.libvirt.live_migration_inbound_addr
6983         migrate_data.supported_perf_events = self._supported_perf_events
6984 
6985         migrate_data.serial_listen_ports = []
6986         if CONF.serial_console.enabled:
6987             num_ports = hardware.get_number_of_serial_ports(
6988                 instance.flavor, instance.image_meta)
6989             for port in six.moves.range(num_ports):
6990                 migrate_data.serial_listen_ports.append(
6991                     serial_console.acquire_port(
6992                         migrate_data.serial_listen_addr))
6993 
6994         for vol in block_device_mapping:
6995             connection_info = vol['connection_info']
6996             if connection_info.get('serial'):
6997                 disk_info = blockinfo.get_info_from_bdm(
6998                     instance, CONF.libvirt.virt_type,
6999                     instance.image_meta, vol)
7000 
7001                 bdmi = objects.LibvirtLiveMigrateBDMInfo()
7002                 bdmi.serial = connection_info['serial']
7003                 bdmi.connection_info = connection_info
7004                 bdmi.bus = disk_info['bus']
7005                 bdmi.dev = disk_info['dev']
7006                 bdmi.type = disk_info['type']
7007                 bdmi.format = disk_info.get('format')
7008                 bdmi.boot_index = disk_info.get('boot_index')
7009                 migrate_data.bdms.append(bdmi)
7010 
7011         return migrate_data
7012 
7013     def _try_fetch_image_cache(self, image, fetch_func, context, filename,
7014                                image_id, instance, size,
7015                                fallback_from_host=None):
7016         try:
7017             image.cache(fetch_func=fetch_func,
7018                         context=context,
7019                         filename=filename,
7020                         image_id=image_id,
7021                         size=size)
7022         except exception.ImageNotFound:
7023             if not fallback_from_host:
7024                 raise
7025             LOG.debug("Image %(image_id)s doesn't exist anymore "
7026                       "on image service, attempting to copy "
7027                       "image from %(host)s",
7028                       {'image_id': image_id, 'host': fallback_from_host},
7029                       instance=instance)
7030 
7031             def copy_from_host(target):
7032                 libvirt_utils.copy_image(src=target,
7033                                          dest=target,
7034                                          host=fallback_from_host,
7035                                          receive=True)
7036             image.cache(fetch_func=copy_from_host,
7037                         filename=filename)
7038 
7039     def _create_images_and_backing(self, context, instance, instance_dir,
7040                                    disk_info, fallback_from_host=None):
7041         """:param context: security context
7042            :param instance:
7043                nova.db.sqlalchemy.models.Instance object
7044                instance object that is migrated.
7045            :param instance_dir:
7046                instance path to use, calculated externally to handle block
7047                migrating an instance with an old style instance path
7048            :param disk_info:
7049                disk info specified in _get_instance_disk_info_from_config
7050                (list of dicts)
7051            :param fallback_from_host:
7052                host where we can retrieve images if the glance images are
7053                not available.
7054 
7055         """
7056 
7057         # Virtuozzo containers don't use backing file
7058         if (CONF.libvirt.virt_type == "parallels" and
7059                 instance.vm_mode == fields.VMMode.EXE):
7060             return
7061 
7062         if not disk_info:
7063             disk_info = []
7064 
7065         for info in disk_info:
7066             base = os.path.basename(info['path'])
7067             # Get image type and create empty disk image, and
7068             # create backing file in case of qcow2.
7069             instance_disk = os.path.join(instance_dir, base)
7070             if not info['backing_file'] and not os.path.exists(instance_disk):
7071                 libvirt_utils.create_image(info['type'], instance_disk,
7072                                            info['virt_disk_size'])
7073             elif info['backing_file']:
7074                 # Creating backing file follows same way as spawning instances.
7075                 cache_name = os.path.basename(info['backing_file'])
7076 
7077                 disk = self.image_backend.by_name(instance, instance_disk,
7078                                                   CONF.libvirt.images_type)
7079                 if cache_name.startswith('ephemeral'):
7080                     # The argument 'size' is used by image.cache to
7081                     # validate disk size retrieved from cache against
7082                     # the instance disk size (should always return OK)
7083                     # and ephemeral_size is used by _create_ephemeral
7084                     # to build the image if the disk is not already
7085                     # cached.
7086                     disk.cache(
7087                         fetch_func=self._create_ephemeral,
7088                         fs_label=cache_name,
7089                         os_type=instance.os_type,
7090                         filename=cache_name,
7091                         size=info['virt_disk_size'],
7092                         ephemeral_size=info['virt_disk_size'] / units.Gi)
7093                 elif cache_name.startswith('swap'):
7094                     inst_type = instance.get_flavor()
7095                     swap_mb = inst_type.swap
7096                     disk.cache(fetch_func=self._create_swap,
7097                                 filename="swap_%s" % swap_mb,
7098                                 size=swap_mb * units.Mi,
7099                                 swap_mb=swap_mb)
7100                 else:
7101                     self._try_fetch_image_cache(disk,
7102                                                 libvirt_utils.fetch_image,
7103                                                 context, cache_name,
7104                                                 instance.image_ref,
7105                                                 instance,
7106                                                 info['virt_disk_size'],
7107                                                 fallback_from_host)
7108 
7109         # if disk has kernel and ramdisk, just download
7110         # following normal way.
7111         self._fetch_instance_kernel_ramdisk(
7112             context, instance, fallback_from_host=fallback_from_host)
7113 
7114     def post_live_migration(self, context, instance, block_device_info,
7115                             migrate_data=None):
7116         # Disconnect from volume server
7117         block_device_mapping = driver.block_device_info_get_mapping(
7118                 block_device_info)
7119         connector = self.get_volume_connector(instance)
7120         volume_api = self._volume_api
7121         for vol in block_device_mapping:
7122             # Retrieve connection info from Cinder's initialize_connection API.
7123             # The info returned will be accurate for the source server.
7124             volume_id = vol['connection_info']['serial']
7125             connection_info = volume_api.initialize_connection(context,
7126                                                                volume_id,
7127                                                                connector)
7128 
7129             # TODO(leeantho) The following multipath_id logic is temporary
7130             # and will be removed in the future once os-brick is updated
7131             # to handle multipath for drivers in a more efficient way.
7132             # For now this logic is needed to ensure the connection info
7133             # data is correct.
7134 
7135             # Pull out multipath_id from the bdm information. The
7136             # multipath_id can be placed into the connection info
7137             # because it is based off of the volume and will be the
7138             # same on the source and destination hosts.
7139             if 'multipath_id' in vol['connection_info']['data']:
7140                 multipath_id = vol['connection_info']['data']['multipath_id']
7141                 connection_info['data']['multipath_id'] = multipath_id
7142 
7143             disk_dev = vol['mount_device'].rpartition("/")[2]
7144             self._disconnect_volume(connection_info, disk_dev, instance)
7145 
7146     def post_live_migration_at_source(self, context, instance, network_info):
7147         """Unplug VIFs from networks at source.
7148 
7149         :param context: security context
7150         :param instance: instance object reference
7151         :param network_info: instance network information
7152         """
7153         self.unplug_vifs(instance, network_info)
7154 
7155     def post_live_migration_at_destination(self, context,
7156                                            instance,
7157                                            network_info,
7158                                            block_migration=False,
7159                                            block_device_info=None):
7160         """Post operation of live migration at destination host.
7161 
7162         :param context: security context
7163         :param instance:
7164             nova.db.sqlalchemy.models.Instance object
7165             instance object that is migrated.
7166         :param network_info: instance network information
7167         :param block_migration: if true, post operation of block_migration.
7168         """
7169         guest = self._host.get_guest(instance)
7170 
7171         # TODO(sahid): In Ocata we have added the migration flag
7172         # VIR_MIGRATE_PERSIST_DEST to libvirt, which means that the
7173         # guest XML is going to be set in libvirtd on destination node
7174         # automatically. However we do not remove that part until P*
7175         # because during an upgrade, to ensure migrating instances
7176         # from node running Newton is still going to set the guest XML
7177         # in libvirtd on destination node.
7178 
7179         # Make sure we define the migrated instance in libvirt
7180         xml = guest.get_xml_desc()
7181         self._host.write_instance_config(xml)
7182 
7183     def _get_instance_disk_info_from_config(self, guest_config,
7184                                             block_device_info):
7185         """Get the non-volume disk information from the domain xml
7186 
7187         :param LibvirtConfigGuest guest_config: the libvirt domain config
7188                                                 for the instance
7189         :param dict block_device_info: block device info for BDMs
7190         :returns disk_info: list of dicts with keys:
7191 
7192           * 'type': the disk type (str)
7193           * 'path': the disk path (str)
7194           * 'virt_disk_size': the virtual disk size (int)
7195           * 'backing_file': backing file of a disk image (str)
7196           * 'disk_size': physical disk size (int)
7197           * 'over_committed_disk_size': virt_disk_size - disk_size or 0
7198         """
7199         block_device_mapping = driver.block_device_info_get_mapping(
7200             block_device_info)
7201 
7202         volume_devices = set()
7203         for vol in block_device_mapping:
7204             disk_dev = vol['mount_device'].rpartition("/")[2]
7205             volume_devices.add(disk_dev)
7206 
7207         disk_info = []
7208 
7209         if (guest_config.virt_type == 'parallels' and
7210                 guest_config.os_type == fields.VMMode.EXE):
7211             node_type = 'filesystem'
7212         else:
7213             node_type = 'disk'
7214 
7215         for device in guest_config.devices:
7216             if device.root_name != node_type:
7217                 continue
7218             disk_type = device.source_type
7219             if device.root_name == 'filesystem':
7220                 target = device.target_dir
7221                 if device.source_type == 'file':
7222                     path = device.source_file
7223                 elif device.source_type == 'block':
7224                     path = device.source_dev
7225                 else:
7226                     path = None
7227             else:
7228                 target = device.target_dev
7229                 path = device.source_path
7230 
7231             if not path:
7232                 LOG.debug('skipping disk for %s as it does not have a path',
7233                           guest_config.name)
7234                 continue
7235 
7236             if disk_type not in ['file', 'block']:
7237                 LOG.debug('skipping disk because it looks like a volume', path)
7238                 continue
7239 
7240             if target in volume_devices:
7241                 LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
7242                           'volume', {'path': path, 'target': target})
7243                 continue
7244 
7245             if device.root_name == 'filesystem':
7246                 driver_type = device.driver_type
7247             else:
7248                 driver_type = device.driver_format
7249             # get the real disk size or
7250             # raise a localized error if image is unavailable
7251             if disk_type == 'file':
7252                 if driver_type == 'ploop':
7253                     dk_size = 0
7254                     for dirpath, dirnames, filenames in os.walk(path):
7255                         for f in filenames:
7256                             fp = os.path.join(dirpath, f)
7257                             dk_size += os.path.getsize(fp)
7258                 else:
7259                     dk_size = int(os.path.getsize(path))
7260             elif disk_type == 'block' and block_device_info:
7261                 dk_size = lvm.get_volume_size(path)
7262             else:
7263                 LOG.debug('skipping disk %(path)s (%(target)s) - unable to '
7264                           'determine if volume',
7265                           {'path': path, 'target': target})
7266                 continue
7267 
7268             if driver_type in ("qcow2", "ploop"):
7269                 backing_file = libvirt_utils.get_disk_backing_file(path)
7270                 virt_size = disk_api.get_disk_size(path)
7271                 over_commit_size = int(virt_size) - dk_size
7272             else:
7273                 backing_file = ""
7274                 virt_size = dk_size
7275                 over_commit_size = 0
7276 
7277             disk_info.append({'type': driver_type,
7278                               'path': path,
7279                               'virt_disk_size': virt_size,
7280                               'backing_file': backing_file,
7281                               'disk_size': dk_size,
7282                               'over_committed_disk_size': over_commit_size})
7283         return disk_info
7284 
7285     def _get_instance_disk_info(self, instance, block_device_info):
7286         try:
7287             guest = self._host.get_guest(instance)
7288             config = guest.get_config()
7289         except libvirt.libvirtError as ex:
7290             error_code = ex.get_error_code()
7291             LOG.warning('Error from libvirt while getting description of '
7292                         '%(instance_name)s: [Error Code %(error_code)s] '
7293                         '%(ex)s',
7294                         {'instance_name': instance.name,
7295                          'error_code': error_code,
7296                          'ex': ex},
7297                         instance=instance)
7298             raise exception.InstanceNotFound(instance_id=instance.uuid)
7299 
7300         return self._get_instance_disk_info_from_config(config,
7301                                                         block_device_info)
7302 
7303     def get_instance_disk_info(self, instance,
7304                                block_device_info=None):
7305         return jsonutils.dumps(
7306             self._get_instance_disk_info(instance, block_device_info))
7307 
7308     def _get_disk_over_committed_size_total(self):
7309         """Return total over committed disk size for all instances."""
7310         # Disk size that all instance uses : virtual_size - disk_size
7311         disk_over_committed_size = 0
7312         instance_domains = self._host.list_instance_domains(only_running=False)
7313         if not instance_domains:
7314             return disk_over_committed_size
7315 
7316         # Get all instance uuids
7317         instance_uuids = [dom.UUIDString() for dom in instance_domains]
7318         ctx = nova_context.get_admin_context()
7319         # Get instance object list by uuid filter
7320         filters = {'uuid': instance_uuids}
7321         # NOTE(ankit): objects.InstanceList.get_by_filters method is
7322         # getting called twice one is here and another in the
7323         # _update_available_resource method of resource_tracker. Since
7324         # _update_available_resource method is synchronized, there is a
7325         # possibility the instances list retrieved here to calculate
7326         # disk_over_committed_size would differ to the list you would get
7327         # in _update_available_resource method for calculating usages based
7328         # on instance utilization.
7329         local_instance_list = objects.InstanceList.get_by_filters(
7330             ctx, filters, use_slave=True)
7331         # Convert instance list to dictionary with instance uuid as key.
7332         local_instances = {inst.uuid: inst for inst in local_instance_list}
7333 
7334         # Get bdms by instance uuids
7335         bdms = objects.BlockDeviceMappingList.bdms_by_instance_uuid(
7336             ctx, instance_uuids)
7337 
7338         for dom in instance_domains:
7339             try:
7340                 guest = libvirt_guest.Guest(dom)
7341                 config = guest.get_config()
7342 
7343                 block_device_info = None
7344                 if guest.uuid in local_instances \
7345                         and (bdms and guest.uuid in bdms):
7346                     # Get block device info for instance
7347                     block_device_info = driver.get_block_device_info(
7348                         local_instances[guest.uuid], bdms[guest.uuid])
7349 
7350                 disk_infos = self._get_instance_disk_info_from_config(
7351                     config, block_device_info)
7352                 if not disk_infos:
7353                     continue
7354 
7355                 for info in disk_infos:
7356                     disk_over_committed_size += int(
7357                         info['over_committed_disk_size'])
7358             except libvirt.libvirtError as ex:
7359                 error_code = ex.get_error_code()
7360                 LOG.warning(
7361                     'Error from libvirt while getting description of '
7362                     '%(instance_name)s: [Error Code %(error_code)s] %(ex)s',
7363                     {'instance_name': guest.name,
7364                      'error_code': error_code,
7365                      'ex': ex})
7366             except OSError as e:
7367                 if e.errno in (errno.ENOENT, errno.ESTALE):
7368                     LOG.warning('Periodic task is updating the host stat, '
7369                                 'it is trying to get disk %(i_name)s, '
7370                                 'but disk file was removed by concurrent '
7371                                 'operations such as resize.',
7372                                 {'i_name': guest.name})
7373                 elif e.errno == errno.EACCES:
7374                     LOG.warning('Periodic task is updating the host stat, '
7375                                 'it is trying to get disk %(i_name)s, '
7376                                 'but access is denied. It is most likely '
7377                                 'due to a VM that exists on the compute '
7378                                 'node but is not managed by Nova.',
7379                                 {'i_name': guest.name})
7380                 else:
7381                     raise
7382             except exception.VolumeBDMPathNotFound as e:
7383                 LOG.warning('Periodic task is updating the host stats, '
7384                             'it is trying to get disk info for %(i_name)s, '
7385                             'but the backing volume block device was removed '
7386                             'by concurrent operations such as resize. '
7387                             'Error: %(error)s',
7388                             {'i_name': guest.name, 'error': e})
7389             # NOTE(gtt116): give other tasks a chance.
7390             greenthread.sleep(0)
7391         return disk_over_committed_size
7392 
7393     def unfilter_instance(self, instance, network_info):
7394         """See comments of same method in firewall_driver."""
7395         self.firewall_driver.unfilter_instance(instance,
7396                                                network_info=network_info)
7397 
7398     def get_available_nodes(self, refresh=False):
7399         return [self._host.get_hostname()]
7400 
7401     def get_host_cpu_stats(self):
7402         """Return the current CPU state of the host."""
7403         return self._host.get_cpu_stats()
7404 
7405     def get_host_uptime(self):
7406         """Returns the result of calling "uptime"."""
7407         out, err = utils.execute('env', 'LANG=C', 'uptime')
7408         return out
7409 
7410     def manage_image_cache(self, context, all_instances):
7411         """Manage the local cache of images."""
7412         self.image_cache_manager.update(context, all_instances)
7413 
7414     def _cleanup_remote_migration(self, dest, inst_base, inst_base_resize,
7415                                   shared_storage=False):
7416         """Used only for cleanup in case migrate_disk_and_power_off fails."""
7417         try:
7418             if os.path.exists(inst_base_resize):
7419                 utils.execute('rm', '-rf', inst_base)
7420                 utils.execute('mv', inst_base_resize, inst_base)
7421                 if not shared_storage:
7422                     self._remotefs.remove_dir(dest, inst_base)
7423         except Exception:
7424             pass
7425 
7426     def _is_storage_shared_with(self, dest, inst_base):
7427         # NOTE (rmk): There are two methods of determining whether we are
7428         #             on the same filesystem: the source and dest IP are the
7429         #             same, or we create a file on the dest system via SSH
7430         #             and check whether the source system can also see it.
7431         # NOTE (drwahl): Actually, there is a 3rd way: if images_type is rbd,
7432         #                it will always be shared storage
7433         if CONF.libvirt.images_type == 'rbd':
7434             return True
7435         shared_storage = (dest == self.get_host_ip_addr())
7436         if not shared_storage:
7437             tmp_file = uuidutils.generate_uuid(dashed=False) + '.tmp'
7438             tmp_path = os.path.join(inst_base, tmp_file)
7439 
7440             try:
7441                 self._remotefs.create_file(dest, tmp_path)
7442                 if os.path.exists(tmp_path):
7443                     shared_storage = True
7444                     os.unlink(tmp_path)
7445                 else:
7446                     self._remotefs.remove_file(dest, tmp_path)
7447             except Exception:
7448                 pass
7449         return shared_storage
7450 
7451     def migrate_disk_and_power_off(self, context, instance, dest,
7452                                    flavor, network_info,
7453                                    block_device_info=None,
7454                                    timeout=0, retry_interval=0):
7455         LOG.debug("Starting migrate_disk_and_power_off",
7456                    instance=instance)
7457 
7458         ephemerals = driver.block_device_info_get_ephemerals(block_device_info)
7459 
7460         # get_bdm_ephemeral_disk_size() will return 0 if the new
7461         # instance's requested block device mapping contain no
7462         # ephemeral devices. However, we still want to check if
7463         # the original instance's ephemeral_gb property was set and
7464         # ensure that the new requested flavor ephemeral size is greater
7465         eph_size = (block_device.get_bdm_ephemeral_disk_size(ephemerals) or
7466                     instance.flavor.ephemeral_gb)
7467 
7468         # Checks if the migration needs a disk resize down.
7469         root_down = flavor.root_gb < instance.flavor.root_gb
7470         ephemeral_down = flavor.ephemeral_gb < eph_size
7471         booted_from_volume = self._is_booted_from_volume(block_device_info)
7472 
7473         if (root_down and not booted_from_volume) or ephemeral_down:
7474             reason = _("Unable to resize disk down.")
7475             raise exception.InstanceFaultRollback(
7476                 exception.ResizeError(reason=reason))
7477 
7478         # NOTE(dgenin): Migration is not implemented for LVM backed instances.
7479         if CONF.libvirt.images_type == 'lvm' and not booted_from_volume:
7480             reason = _("Migration is not supported for LVM backed instances")
7481             raise exception.InstanceFaultRollback(
7482                 exception.MigrationPreCheckError(reason=reason))
7483 
7484         # copy disks to destination
7485         # rename instance dir to +_resize at first for using
7486         # shared storage for instance dir (eg. NFS).
7487         inst_base = libvirt_utils.get_instance_path(instance)
7488         inst_base_resize = inst_base + "_resize"
7489         shared_storage = self._is_storage_shared_with(dest, inst_base)
7490 
7491         # try to create the directory on the remote compute node
7492         # if this fails we pass the exception up the stack so we can catch
7493         # failures here earlier
7494         if not shared_storage:
7495             try:
7496                 self._remotefs.create_dir(dest, inst_base)
7497             except processutils.ProcessExecutionError as e:
7498                 reason = _("not able to execute ssh command: %s") % e
7499                 raise exception.InstanceFaultRollback(
7500                     exception.ResizeError(reason=reason))
7501 
7502         self.power_off(instance, timeout, retry_interval)
7503 
7504         block_device_mapping = driver.block_device_info_get_mapping(
7505             block_device_info)
7506         for vol in block_device_mapping:
7507             connection_info = vol['connection_info']
7508             disk_dev = vol['mount_device'].rpartition("/")[2]
7509             self._disconnect_volume(connection_info, disk_dev, instance)
7510 
7511         disk_info = self._get_instance_disk_info(instance, block_device_info)
7512 
7513         try:
7514             utils.execute('mv', inst_base, inst_base_resize)
7515             # if we are migrating the instance with shared storage then
7516             # create the directory.  If it is a remote node the directory
7517             # has already been created
7518             if shared_storage:
7519                 dest = None
7520                 utils.execute('mkdir', '-p', inst_base)
7521 
7522             on_execute = lambda process: \
7523                 self.job_tracker.add_job(instance, process.pid)
7524             on_completion = lambda process: \
7525                 self.job_tracker.remove_job(instance, process.pid)
7526 
7527             for info in disk_info:
7528                 # assume inst_base == dirname(info['path'])
7529                 img_path = info['path']
7530                 fname = os.path.basename(img_path)
7531                 from_path = os.path.join(inst_base_resize, fname)
7532 
7533                 # We will not copy over the swap disk here, and rely on
7534                 # finish_migration to re-create it for us. This is ok because
7535                 # the OS is shut down, and as recreating a swap disk is very
7536                 # cheap it is more efficient than copying either locally or
7537                 # over the network. This also means we don't have to resize it.
7538                 if fname == 'disk.swap':
7539                     continue
7540 
7541                 compression = info['type'] not in NO_COMPRESSION_TYPES
7542                 libvirt_utils.copy_image(from_path, img_path, host=dest,
7543                                          on_execute=on_execute,
7544                                          on_completion=on_completion,
7545                                          compression=compression)
7546 
7547             # Ensure disk.info is written to the new path to avoid disks being
7548             # reinspected and potentially changing format.
7549             src_disk_info_path = os.path.join(inst_base_resize, 'disk.info')
7550             if os.path.exists(src_disk_info_path):
7551                 dst_disk_info_path = os.path.join(inst_base, 'disk.info')
7552                 libvirt_utils.copy_image(src_disk_info_path,
7553                                          dst_disk_info_path,
7554                                          host=dest, on_execute=on_execute,
7555                                          on_completion=on_completion)
7556         except Exception:
7557             with excutils.save_and_reraise_exception():
7558                 self._cleanup_remote_migration(dest, inst_base,
7559                                                inst_base_resize,
7560                                                shared_storage)
7561 
7562         return jsonutils.dumps(disk_info)
7563 
7564     def _wait_for_running(self, instance):
7565         state = self.get_info(instance).state
7566 
7567         if state == power_state.RUNNING:
7568             LOG.info("Instance running successfully.", instance=instance)
7569             raise loopingcall.LoopingCallDone()
7570 
7571     @staticmethod
7572     def _disk_raw_to_qcow2(path):
7573         """Converts a raw disk to qcow2."""
7574         path_qcow = path + '_qcow'
7575         utils.execute('qemu-img', 'convert', '-f', 'raw',
7576                       '-O', 'qcow2', path, path_qcow)
7577         utils.execute('mv', path_qcow, path)
7578 
7579     @staticmethod
7580     def _disk_qcow2_to_raw(path):
7581         """Converts a qcow2 disk to raw."""
7582         path_raw = path + '_raw'
7583         utils.execute('qemu-img', 'convert', '-f', 'qcow2',
7584                       '-O', 'raw', path, path_raw)
7585         utils.execute('mv', path_raw, path)
7586 
7587     def finish_migration(self, context, migration, instance, disk_info,
7588                          network_info, image_meta, resize_instance,
7589                          block_device_info=None, power_on=True):
7590         LOG.debug("Starting finish_migration", instance=instance)
7591 
7592         block_disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7593                                                   instance,
7594                                                   image_meta,
7595                                                   block_device_info)
7596         # assume _create_image does nothing if a target file exists.
7597         # NOTE: This has the intended side-effect of fetching a missing
7598         # backing file.
7599         self._create_image(context, instance, block_disk_info['mapping'],
7600                            block_device_info=block_device_info,
7601                            ignore_bdi_for_swap=True,
7602                            fallback_from_host=migration.source_compute)
7603 
7604         # Required by Quobyte CI
7605         self._ensure_console_log_for_instance(instance)
7606 
7607         gen_confdrive = functools.partial(
7608             self._create_configdrive, context, instance,
7609             InjectionInfo(admin_pass=None, network_info=network_info,
7610                           files=None))
7611 
7612         # Convert raw disks to qcow2 if migrating to host which uses
7613         # qcow2 from host which uses raw.
7614         disk_info = jsonutils.loads(disk_info)
7615         for info in disk_info:
7616             path = info['path']
7617             disk_name = os.path.basename(path)
7618 
7619             # NOTE(mdbooth): The code below looks wrong, but is actually
7620             # required to prevent a security hole when migrating from a host
7621             # with use_cow_images=False to one with use_cow_images=True.
7622             # Imagebackend uses use_cow_images to select between the
7623             # atrociously-named-Raw and Qcow2 backends. The Qcow2 backend
7624             # writes to disk.info, but does not read it as it assumes qcow2.
7625             # Therefore if we don't convert raw to qcow2 here, a raw disk will
7626             # be incorrectly assumed to be qcow2, which is a severe security
7627             # flaw. The reverse is not true, because the atrociously-named-Raw
7628             # backend supports both qcow2 and raw disks, and will choose
7629             # appropriately between them as long as disk.info exists and is
7630             # correctly populated, which it is because Qcow2 writes to
7631             # disk.info.
7632             #
7633             # In general, we do not yet support format conversion during
7634             # migration. For example:
7635             #   * Converting from use_cow_images=True to use_cow_images=False
7636             #     isn't handled. This isn't a security bug, but is almost
7637             #     certainly buggy in other cases, as the 'Raw' backend doesn't
7638             #     expect a backing file.
7639             #   * Converting to/from lvm and rbd backends is not supported.
7640             #
7641             # This behaviour is inconsistent, and therefore undesirable for
7642             # users. It is tightly-coupled to implementation quirks of 2
7643             # out of 5 backends in imagebackend and defends against a severe
7644             # security flaw which is not at all obvious without deep analysis,
7645             # and is therefore undesirable to developers. We should aim to
7646             # remove it. This will not be possible, though, until we can
7647             # represent the storage layout of a specific instance
7648             # independent of the default configuration of the local compute
7649             # host.
7650 
7651             # Config disks are hard-coded to be raw even when
7652             # use_cow_images=True (see _get_disk_config_image_type),so don't
7653             # need to be converted.
7654             if (disk_name != 'disk.config' and
7655                         info['type'] == 'raw' and CONF.use_cow_images):
7656                 self._disk_raw_to_qcow2(info['path'])
7657 
7658         xml = self._get_guest_xml(context, instance, network_info,
7659                                   block_disk_info, image_meta,
7660                                   block_device_info=block_device_info)
7661         # NOTE(mriedem): vifs_already_plugged=True here, regardless of whether
7662         # or not we've migrated to another host, because we unplug VIFs locally
7663         # and the status change in the port might go undetected by the neutron
7664         # L2 agent (or neutron server) so neutron may not know that the VIF was
7665         # unplugged in the first place and never send an event.
7666         guest = self._create_domain_and_network(context, xml, instance,
7667                                         network_info,
7668                                         block_device_info=block_device_info,
7669                                         power_on=power_on,
7670                                         vifs_already_plugged=True,
7671                                         post_xml_callback=gen_confdrive)
7672         if power_on:
7673             timer = loopingcall.FixedIntervalLoopingCall(
7674                                                     self._wait_for_running,
7675                                                     instance)
7676             timer.start(interval=0.5).wait()
7677 
7678             # Sync guest time after migration.
7679             guest.sync_guest_time()
7680 
7681         LOG.debug("finish_migration finished successfully.", instance=instance)
7682 
7683     def _cleanup_failed_migration(self, inst_base):
7684         """Make sure that a failed migrate doesn't prevent us from rolling
7685         back in a revert.
7686         """
7687         try:
7688             shutil.rmtree(inst_base)
7689         except OSError as e:
7690             if e.errno != errno.ENOENT:
7691                 raise
7692 
7693     def finish_revert_migration(self, context, instance, network_info,
7694                                 block_device_info=None, power_on=True):
7695         LOG.debug("Starting finish_revert_migration",
7696                   instance=instance)
7697 
7698         inst_base = libvirt_utils.get_instance_path(instance)
7699         inst_base_resize = inst_base + "_resize"
7700 
7701         # NOTE(danms): if we're recovering from a failed migration,
7702         # make sure we don't have a left-over same-host base directory
7703         # that would conflict. Also, don't fail on the rename if the
7704         # failure happened early.
7705         if os.path.exists(inst_base_resize):
7706             self._cleanup_failed_migration(inst_base)
7707             utils.execute('mv', inst_base_resize, inst_base)
7708 
7709         root_disk = self.image_backend.by_name(instance, 'disk')
7710         # Once we rollback, the snapshot is no longer needed, so remove it
7711         # TODO(nic): Remove the try/except/finally in a future release
7712         # To avoid any upgrade issues surrounding instances being in pending
7713         # resize state when the software is updated, this portion of the
7714         # method logs exceptions rather than failing on them.  Once it can be
7715         # reasonably assumed that no such instances exist in the wild
7716         # anymore, the try/except/finally should be removed,
7717         # and ignore_errors should be set back to False (the default) so
7718         # that problems throw errors, like they should.
7719         if root_disk.exists():
7720             try:
7721                 root_disk.rollback_to_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME)
7722             except exception.SnapshotNotFound:
7723                 LOG.warning("Failed to rollback snapshot (%s)",
7724                             libvirt_utils.RESIZE_SNAPSHOT_NAME)
7725             finally:
7726                 root_disk.remove_snap(libvirt_utils.RESIZE_SNAPSHOT_NAME,
7727                                       ignore_errors=True)
7728 
7729         disk_info = blockinfo.get_disk_info(CONF.libvirt.virt_type,
7730                                             instance,
7731                                             instance.image_meta,
7732                                             block_device_info)
7733         xml = self._get_guest_xml(context, instance, network_info, disk_info,
7734                                   instance.image_meta,
7735                                   block_device_info=block_device_info)
7736         self._create_domain_and_network(context, xml, instance, network_info,
7737                                         block_device_info=block_device_info,
7738                                         power_on=power_on,
7739                                         vifs_already_plugged=True)
7740 
7741         if power_on:
7742             timer = loopingcall.FixedIntervalLoopingCall(
7743                                                     self._wait_for_running,
7744                                                     instance)
7745             timer.start(interval=0.5).wait()
7746 
7747         LOG.debug("finish_revert_migration finished successfully.",
7748                   instance=instance)
7749 
7750     def confirm_migration(self, context, migration, instance, network_info):
7751         """Confirms a resize, destroying the source VM."""
7752         self._cleanup_resize(instance, network_info)
7753 
7754     @staticmethod
7755     def _get_io_devices(xml_doc):
7756         """get the list of io devices from the xml document."""
7757         result = {"volumes": [], "ifaces": []}
7758         try:
7759             doc = etree.fromstring(xml_doc)
7760         except Exception:
7761             return result
7762         blocks = [('./devices/disk', 'volumes'),
7763             ('./devices/interface', 'ifaces')]
7764         for block, key in blocks:
7765             section = doc.findall(block)
7766             for node in section:
7767                 for child in node.getchildren():
7768                     if child.tag == 'target' and child.get('dev'):
7769                         result[key].append(child.get('dev'))
7770         return result
7771 
7772     def get_diagnostics(self, instance):
7773         guest = self._host.get_guest(instance)
7774 
7775         # TODO(sahid): We are converting all calls from a
7776         # virDomain object to use nova.virt.libvirt.Guest.
7777         # We should be able to remove domain at the end.
7778         domain = guest._domain
7779         output = {}
7780         # get cpu time, might launch an exception if the method
7781         # is not supported by the underlying hypervisor being
7782         # used by libvirt
7783         try:
7784             for vcpu in guest.get_vcpus_info():
7785                 output["cpu" + str(vcpu.id) + "_time"] = vcpu.time
7786         except libvirt.libvirtError:
7787             pass
7788         # get io status
7789         xml = guest.get_xml_desc()
7790         dom_io = LibvirtDriver._get_io_devices(xml)
7791         for guest_disk in dom_io["volumes"]:
7792             try:
7793                 # blockStats might launch an exception if the method
7794                 # is not supported by the underlying hypervisor being
7795                 # used by libvirt
7796                 stats = domain.blockStats(guest_disk)
7797                 output[guest_disk + "_read_req"] = stats[0]
7798                 output[guest_disk + "_read"] = stats[1]
7799                 output[guest_disk + "_write_req"] = stats[2]
7800                 output[guest_disk + "_write"] = stats[3]
7801                 output[guest_disk + "_errors"] = stats[4]
7802             except libvirt.libvirtError:
7803                 pass
7804         for interface in dom_io["ifaces"]:
7805             try:
7806                 # interfaceStats might launch an exception if the method
7807                 # is not supported by the underlying hypervisor being
7808                 # used by libvirt
7809                 stats = domain.interfaceStats(interface)
7810                 output[interface + "_rx"] = stats[0]
7811                 output[interface + "_rx_packets"] = stats[1]
7812                 output[interface + "_rx_errors"] = stats[2]
7813                 output[interface + "_rx_drop"] = stats[3]
7814                 output[interface + "_tx"] = stats[4]
7815                 output[interface + "_tx_packets"] = stats[5]
7816                 output[interface + "_tx_errors"] = stats[6]
7817                 output[interface + "_tx_drop"] = stats[7]
7818             except libvirt.libvirtError:
7819                 pass
7820         output["memory"] = domain.maxMemory()
7821         # memoryStats might launch an exception if the method
7822         # is not supported by the underlying hypervisor being
7823         # used by libvirt
7824         try:
7825             mem = domain.memoryStats()
7826             for key in mem.keys():
7827                 output["memory-" + key] = mem[key]
7828         except (libvirt.libvirtError, AttributeError):
7829             pass
7830         return output
7831 
7832     def get_instance_diagnostics(self, instance):
7833         guest = self._host.get_guest(instance)
7834 
7835         # TODO(sahid): We are converting all calls from a
7836         # virDomain object to use nova.virt.libvirt.Guest.
7837         # We should be able to remove domain at the end.
7838         domain = guest._domain
7839 
7840         xml = guest.get_xml_desc()
7841         xml_doc = etree.fromstring(xml)
7842 
7843         # TODO(sahid): Needs to use get_info but more changes have to
7844         # be done since a mapping STATE_MAP LIBVIRT_POWER_STATE is
7845         # needed.
7846         (state, max_mem, mem, num_cpu, cpu_time) = \
7847             guest._get_domain_info(self._host)
7848         config_drive = configdrive.required_by(instance)
7849         launched_at = timeutils.normalize_time(instance.launched_at)
7850         uptime = timeutils.delta_seconds(launched_at,
7851                                          timeutils.utcnow())
7852         diags = diagnostics_obj.Diagnostics(state=power_state.STATE_MAP[state],
7853                                         driver='libvirt',
7854                                         config_drive=config_drive,
7855                                         hypervisor=CONF.libvirt.virt_type,
7856                                         hypervisor_os='linux',
7857                                         uptime=uptime)
7858         diags.memory_details = diagnostics_obj.MemoryDiagnostics(
7859             maximum=max_mem / units.Mi,
7860             used=mem / units.Mi)
7861 
7862         # get cpu time, might launch an exception if the method
7863         # is not supported by the underlying hypervisor being
7864         # used by libvirt
7865         try:
7866             for vcpu in guest.get_vcpus_info():
7867                 diags.add_cpu(id=vcpu.id, time=vcpu.time)
7868         except libvirt.libvirtError:
7869             pass
7870         # get io status
7871         dom_io = LibvirtDriver._get_io_devices(xml)
7872         for guest_disk in dom_io["volumes"]:
7873             try:
7874                 # blockStats might launch an exception if the method
7875                 # is not supported by the underlying hypervisor being
7876                 # used by libvirt
7877                 stats = domain.blockStats(guest_disk)
7878                 diags.add_disk(read_bytes=stats[1],
7879                                read_requests=stats[0],
7880                                write_bytes=stats[3],
7881                                write_requests=stats[2],
7882                                errors_count=stats[4])
7883             except libvirt.libvirtError:
7884                 pass
7885         for interface in dom_io["ifaces"]:
7886             try:
7887                 # interfaceStats might launch an exception if the method
7888                 # is not supported by the underlying hypervisor being
7889                 # used by libvirt
7890                 stats = domain.interfaceStats(interface)
7891                 diags.add_nic(rx_octets=stats[0],
7892                               rx_errors=stats[2],
7893                               rx_drop=stats[3],
7894                               rx_packets=stats[1],
7895                               tx_octets=stats[4],
7896                               tx_errors=stats[6],
7897                               tx_drop=stats[7],
7898                               tx_packets=stats[5])
7899             except libvirt.libvirtError:
7900                 pass
7901 
7902         # Update mac addresses of interface if stats have been reported
7903         if diags.nic_details:
7904             nodes = xml_doc.findall('./devices/interface/mac')
7905             for index, node in enumerate(nodes):
7906                 diags.nic_details[index].mac_address = node.get('address')
7907         return diags
7908 
7909     @staticmethod
7910     def _prepare_device_bus(dev):
7911         """Determines the device bus and its hypervisor assigned address
7912         """
7913         bus = None
7914         address = (dev.device_addr.format_address() if
7915                    dev.device_addr else None)
7916         if isinstance(dev.device_addr,
7917                       vconfig.LibvirtConfigGuestDeviceAddressPCI):
7918             bus = objects.PCIDeviceBus()
7919         elif isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7920             if dev.target_bus == 'scsi':
7921                 bus = objects.SCSIDeviceBus()
7922             elif dev.target_bus == 'ide':
7923                 bus = objects.IDEDeviceBus()
7924             elif dev.target_bus == 'usb':
7925                 bus = objects.USBDeviceBus()
7926         if address is not None and bus is not None:
7927             bus.address = address
7928         return bus
7929 
7930     def _build_device_metadata(self, context, instance):
7931         """Builds a metadata object for instance devices, that maps the user
7932            provided tag to the hypervisor assigned device address.
7933         """
7934         def _get_device_name(bdm):
7935             return block_device.strip_dev(bdm.device_name)
7936 
7937         network_info = instance.info_cache.network_info
7938         vlans_by_mac = netutils.get_cached_vifs_with_vlan(network_info)
7939         vifs = objects.VirtualInterfaceList.get_by_instance_uuid(context,
7940                                                                  instance.uuid)
7941         vifs_to_expose = {vif.address: vif for vif in vifs
7942                           if ('tag' in vif and vif.tag) or
7943                              vlans_by_mac.get(vif.address)}
7944         # TODO(mriedem): We should be able to avoid the DB query here by using
7945         # block_device_info['block_device_mapping'] which is passed into most
7946         # methods that call this function.
7947         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7948             context, instance.uuid)
7949         tagged_bdms = {_get_device_name(bdm): bdm for bdm in bdms if bdm.tag}
7950 
7951         devices = []
7952         guest = self._host.get_guest(instance)
7953         xml = guest.get_xml_desc()
7954         xml_dom = etree.fromstring(xml)
7955         guest_config = vconfig.LibvirtConfigGuest()
7956         guest_config.parse_dom(xml_dom)
7957 
7958         for dev in guest_config.devices:
7959             # Build network interfaces related metadata
7960             if isinstance(dev, vconfig.LibvirtConfigGuestInterface):
7961                 vif = vifs_to_expose.get(dev.mac_addr)
7962                 if not vif:
7963                     continue
7964                 bus = self._prepare_device_bus(dev)
7965                 device = objects.NetworkInterfaceMetadata(mac=vif.address)
7966                 if 'tag' in vif and vif.tag:
7967                     device.tags = [vif.tag]
7968                 if bus:
7969                     device.bus = bus
7970                 vlan = vlans_by_mac.get(vif.address)
7971                 if vlan:
7972                     device.vlan = int(vlan)
7973                 devices.append(device)
7974 
7975             # Build disks related metadata
7976             if isinstance(dev, vconfig.LibvirtConfigGuestDisk):
7977                 bdm = tagged_bdms.get(dev.target_dev)
7978                 if not bdm:
7979                     continue
7980                 bus = self._prepare_device_bus(dev)
7981                 device = objects.DiskMetadata(tags=[bdm.tag])
7982                 # NOTE(artom) Setting the serial (which corresponds to
7983                 # volume_id in BlockDeviceMapping) in DiskMetadata allows us to
7984                 # find the disks's BlockDeviceMapping object when we detach the
7985                 # volume and want to clean up its metadata.
7986                 device.serial = bdm.volume_id
7987                 if bus:
7988                     device.bus = bus
7989                 devices.append(device)
7990         if devices:
7991             dev_meta = objects.InstanceDeviceMetadata(devices=devices)
7992             return dev_meta
7993 
7994     def instance_on_disk(self, instance):
7995         # ensure directories exist and are writable
7996         instance_path = libvirt_utils.get_instance_path(instance)
7997         LOG.debug('Checking instance files accessibility %s', instance_path,
7998                   instance=instance)
7999         shared_instance_path = os.access(instance_path, os.W_OK)
8000         # NOTE(flwang): For shared block storage scenario, the file system is
8001         # not really shared by the two hosts, but the volume of evacuated
8002         # instance is reachable.
8003         shared_block_storage = (self.image_backend.backend().
8004                                 is_shared_block_storage())
8005         return shared_instance_path or shared_block_storage
8006 
8007     def inject_network_info(self, instance, nw_info):
8008         self.firewall_driver.setup_basic_filtering(instance, nw_info)
8009 
8010     def delete_instance_files(self, instance):
8011         target = libvirt_utils.get_instance_path(instance)
8012         # A resize may be in progress
8013         target_resize = target + '_resize'
8014         # Other threads may attempt to rename the path, so renaming the path
8015         # to target + '_del' (because it is atomic) and iterating through
8016         # twice in the unlikely event that a concurrent rename occurs between
8017         # the two rename attempts in this method. In general this method
8018         # should be fairly thread-safe without these additional checks, since
8019         # other operations involving renames are not permitted when the task
8020         # state is not None and the task state should be set to something
8021         # other than None by the time this method is invoked.
8022         target_del = target + '_del'
8023         for i in range(2):
8024             try:
8025                 utils.execute('mv', target, target_del)
8026                 break
8027             except Exception:
8028                 pass
8029             try:
8030                 utils.execute('mv', target_resize, target_del)
8031                 break
8032             except Exception:
8033                 pass
8034         # Either the target or target_resize path may still exist if all
8035         # rename attempts failed.
8036         remaining_path = None
8037         for p in (target, target_resize):
8038             if os.path.exists(p):
8039                 remaining_path = p
8040                 break
8041 
8042         # A previous delete attempt may have been interrupted, so target_del
8043         # may exist even if all rename attempts during the present method
8044         # invocation failed due to the absence of both target and
8045         # target_resize.
8046         if not remaining_path and os.path.exists(target_del):
8047             self.job_tracker.terminate_jobs(instance)
8048 
8049             LOG.info('Deleting instance files %s', target_del,
8050                      instance=instance)
8051             remaining_path = target_del
8052             try:
8053                 shutil.rmtree(target_del)
8054             except OSError as e:
8055                 LOG.error('Failed to cleanup directory %(target)s: %(e)s',
8056                           {'target': target_del, 'e': e}, instance=instance)
8057 
8058         # It is possible that the delete failed, if so don't mark the instance
8059         # as cleaned.
8060         if remaining_path and os.path.exists(remaining_path):
8061             LOG.info('Deletion of %s failed', remaining_path,
8062                      instance=instance)
8063             return False
8064 
8065         LOG.info('Deletion of %s complete', target_del, instance=instance)
8066         return True
8067 
8068     @property
8069     def need_legacy_block_device_info(self):
8070         return False
8071 
8072     def default_root_device_name(self, instance, image_meta, root_bdm):
8073         disk_bus = blockinfo.get_disk_bus_for_device_type(
8074             instance, CONF.libvirt.virt_type, image_meta, "disk")
8075         cdrom_bus = blockinfo.get_disk_bus_for_device_type(
8076             instance, CONF.libvirt.virt_type, image_meta, "cdrom")
8077         root_info = blockinfo.get_root_info(
8078             instance, CONF.libvirt.virt_type, image_meta,
8079             root_bdm, disk_bus, cdrom_bus)
8080         return block_device.prepend_dev(root_info['dev'])
8081 
8082     def default_device_names_for_instance(self, instance, root_device_name,
8083                                           *block_device_lists):
8084         block_device_mapping = list(itertools.chain(*block_device_lists))
8085         # NOTE(ndipanov): Null out the device names so that blockinfo code
8086         #                 will assign them
8087         for bdm in block_device_mapping:
8088             if bdm.device_name is not None:
8089                 LOG.warning(
8090                     "Ignoring supplied device name: %(device_name)s. "
8091                     "Libvirt can't honour user-supplied dev names",
8092                     {'device_name': bdm.device_name}, instance=instance)
8093                 bdm.device_name = None
8094         block_device_info = driver.get_block_device_info(instance,
8095                                                          block_device_mapping)
8096 
8097         blockinfo.default_device_names(CONF.libvirt.virt_type,
8098                                        nova_context.get_admin_context(),
8099                                        instance,
8100                                        block_device_info,
8101                                        instance.image_meta)
8102 
8103     def get_device_name_for_instance(self, instance, bdms, block_device_obj):
8104         block_device_info = driver.get_block_device_info(instance, bdms)
8105         instance_info = blockinfo.get_disk_info(
8106                 CONF.libvirt.virt_type, instance,
8107                 instance.image_meta, block_device_info=block_device_info)
8108 
8109         suggested_dev_name = block_device_obj.device_name
8110         if suggested_dev_name is not None:
8111             LOG.warning(
8112                 'Ignoring supplied device name: %(suggested_dev)s',
8113                 {'suggested_dev': suggested_dev_name}, instance=instance)
8114 
8115         # NOTE(ndipanov): get_info_from_bdm will generate the new device name
8116         #                 only when it's actually not set on the bd object
8117         block_device_obj.device_name = None
8118         disk_info = blockinfo.get_info_from_bdm(
8119             instance, CONF.libvirt.virt_type, instance.image_meta,
8120             block_device_obj, mapping=instance_info['mapping'])
8121         return block_device.prepend_dev(disk_info['dev'])
8122 
8123     def is_supported_fs_format(self, fs_type):
8124         return fs_type in [disk_api.FS_FORMAT_EXT2, disk_api.FS_FORMAT_EXT3,
8125                            disk_api.FS_FORMAT_EXT4, disk_api.FS_FORMAT_XFS]
