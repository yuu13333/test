I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Licensed under the Apache License, Version 2.0 (the "License");
2 # you may not use this file except in compliance with the License.
3 # You may obtain a copy of the License at
4 #
5 #    http://www.apache.org/licenses/LICENSE-2.0
6 #
7 # Unless required by applicable law or agreed to in writing, software
8 # distributed under the License is distributed on an "AS IS" BASIS,
9 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
10 # See the License for the specific language governing permissions and
11 # limitations under the License.
12 
13 import time
14 
15 from oslo_log import log as logging
16 
17 from nova import test
18 from nova.tests import fixtures as nova_fixtures
19 from nova.tests.functional import integrated_helpers
20 from nova.tests.unit import fake_network
21 import nova.tests.unit.image.fake
22 from nova.tests.unit import policy_fixture
23 from nova.virt import fake
24 
25 LOG = logging.getLogger(__name__)
26 
27 
28 class TestParallelEvacuationWithServerGroup(
29         test.TestCase, integrated_helpers.InstanceHelperMixin):
30     """Verifies that the server group policy is not violated during parallel
31     evacuation.
32     """
33 
34     def setUp(self):
35         super(TestParallelEvacuationWithServerGroup, self).setUp()
36 
37         self.useFixture(policy_fixture.RealPolicyFixture())
38 
39         # The NeutronFixture is needed to stub out validate_networks in API.
40         self.useFixture(nova_fixtures.NeutronFixture(self))
41 
42         # This stubs out the network allocation in compute.
43         fake_network.set_stub_network_methods(self)
44 
45         # We need the computes reporting into placement for the filter
46         # scheduler to pick a host.
47         self.useFixture(nova_fixtures.PlacementFixture())
48 
49         api_fixture = self.useFixture(nova_fixtures.OSAPIFixture(
50             api_version='v2.1'))
51         self.api = api_fixture.admin_api
52         # needed for force_down
53         self.api.microversion = '2.11'
54 
55         # the image fake backend needed for image discovery
56         nova.tests.unit.image.fake.stub_out_image_service(self)
57         self.addCleanup(nova.tests.unit.image.fake.FakeImageService_reset)
58 
59         self.start_service('conductor')
60         self.start_service('scheduler')
61 
62         # We start two compute services because we need two instances with
63         # anti-affinity server group policy to be booted
64         fake.set_nodes(['host1'])
65         self.addCleanup(fake.restore_nodes)
66         self.compute1 = self.start_service('compute', host='host1')
67         fake.set_nodes(['host2'])
68         self.addCleanup(fake.restore_nodes)
69         self.compute2 = self.start_service('compute', host='host2')
70 
71         self.image_id = self.api.get_images()[0]['id']
72         self.flavor_id = self.api.get_flavors()[0]['id']
73 
74         manager_class = nova.compute.manager.ComputeManager
75         original_rebuild = manager_class._do_rebuild_instance
76 
77         def fake_rebuild(self, context, instance, *args, **kwargs):
78             # Simulate that the rebuild request of one of the instances
79             # reaches the target compute manager significantly later so the
80             # rebuild of the other instance can finish before the late
81             # validation of the first rebuild.
82             # We cannot simply delay the virt driver's rebuild or the
83             # manager's _rebuild_default_impl as those run after the late
84             # validation
85             if instance.host == 'host1':
86                 time.sleep(0.5)
87 
88             original_rebuild(self, context, instance, *args, **kwargs)
89 
90         self.stub_out('nova.compute.manager.ComputeManager.'
91                       '_do_rebuild_instance', fake_rebuild)
92 
93     def test_parallel_evacuate_with_server_group(self):
94         group_req = {'name': 'a-name', 'policies': ['anti-affinity']}
95         group = self.api.post_server_groups(group_req)
96 
97         # boot two instances with anti-affinity
98         server = {'name': 'server',
99                   'imageRef': self.image_id,
100                   'flavorRef': self.flavor_id}
101         hints = {'group': group['id']}
102         created_server1 = self.api.post_server({'server': server,
103                                                 'os:scheduler_hints': hints})
104         server1 = self._wait_for_state_change(self.api,
105                                               created_server1, 'ACTIVE')
106 
107         created_server2 = self.api.post_server({'server': server,
108                                                 'os:scheduler_hints': hints})
109         server2 = self._wait_for_state_change(self.api,
110                                               created_server2, 'ACTIVE')
111 
112         # assert that the anti-affinity policy is enforced during the boot
113         self.assertNotEqual(server1['OS-EXT-SRV-ATTR:host'],
114                             server2['OS-EXT-SRV-ATTR:host'])
115 
116         # simulate compute failure on both compute host to allow evacuation
117         self.compute1.stop()
118         # force it down to avoid waiting for the service group to time out
119         self.api.force_down_service('host1', 'nova-compute', True)
120 
121         self.compute2.stop()
122         self.api.force_down_service('host2', 'nova-compute', True)
123 
124         # start a third compute to have place for one of the instances
125         fake.set_nodes(['host3'])
126         self.compute3 = self.start_service('compute', host='host3')
127 
128         # evacuate both instances
129         post = {'evacuate': {'onSharedStorage': False}}
130         self.api.post_server_action(server1['id'], post)
131         self.api.post_server_action(server2['id'], post)
132 
133         # make sure that the rebuild is started and then finished
134         server1 = self._wait_for_server_parameter(
135             self.api, server1, {'OS-EXT-STS:task_state': 'rebuilding'})
136         server2 = self._wait_for_server_parameter(
137             self.api, server2, {'OS-EXT-STS:task_state': 'rebuilding'})
138         server1 = self._wait_for_server_parameter(
139             self.api, server1, {'OS-EXT-STS:task_state': None})
140         server2 = self._wait_for_server_parameter(
141             self.api, server2, {'OS-EXT-STS:task_state': None})
142 
143         # NOTE(gibi): The instance.host set _after_ the instance state and
144         # tast_state is set back to normal so it is not enough to wait for
145         # that. The only thing that happens after the instance.host is set to
146         # the target host is the migration status setting to done. So we have
147         # to wait for that to avoid asserting the wrong host below.
148         self._wait_for_migration_status(server1, 'done')
149         self._wait_for_migration_status(server2, 'done')
150 
151         # get the servers again to have the latest information about their
152         # hosts
153         server1 = self.api.get_server(server1['id'])
154         server2 = self.api.get_server(server2['id'])
155 
156         # assert that the anti-affinity policy is enforced during the
157         # evacuation
158         # NOTE(gibi): This shows bug 1735407 as both instance ends up on the
159         # same host.
160         self.assertEqual(server1['OS-EXT-SRV-ATTR:host'],
161                          server2['OS-EXT-SRV-ATTR:host'])
162         # After the bug 1735407 is fixed the following is expected:
163         # self.assertNotEqual(server1['OS-EXT-SRV-ATTR:host'],
164         #                     server2['OS-EXT-SRV-ATTR:host'])
165 
166         # assert that one of the evacuation was successful and that server is
167         # moved to another host and the evacuation of the other server is
168         # failed
169         # NOTE(gibi): This shows bug 1735407 as both instance is moved
170         self.assertNotIn(server1['OS-EXT-SRV-ATTR:host'], {'host1', 'host2'})
171         self.assertNotIn(server2['OS-EXT-SRV-ATTR:host'], {'host1', 'host2'})
172         # After fixing the bug 1735407 the following is expected
173         # if server1['status'] == 'ERROR':
174         #     failed_server = server1
175         #     evacuated_server = server2
176         # else:
177         #     failed_server = server2
178         #     evacuated_server = server1
179         # self.assertEqual('ERROR', failed_server['status'])
180         # self.assertNotEqual('host3', failed_server['OS-EXT-SRV-ATTR:host'])
181         # self.assertEqual('ACTIVE', evacuated_server['status'])
182         # self.assertEqual('host3', evacuated_server['OS-EXT-SRV-ATTR:host'])
