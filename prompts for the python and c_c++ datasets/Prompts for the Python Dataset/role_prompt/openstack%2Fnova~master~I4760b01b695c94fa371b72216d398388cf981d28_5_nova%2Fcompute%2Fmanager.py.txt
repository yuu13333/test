I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import copy
32 import functools
33 import inspect
34 import sys
35 import time
36 import traceback
37 
38 from cinderclient import exceptions as cinder_exception
39 from cursive import exception as cursive_exception
40 import eventlet.event
41 from eventlet import greenthread
42 import eventlet.semaphore
43 import eventlet.timeout
44 import futurist
45 from keystoneauth1 import exceptions as keystone_exception
46 import os_traits
47 from oslo_log import log as logging
48 import oslo_messaging as messaging
49 from oslo_serialization import jsonutils
50 from oslo_service import loopingcall
51 from oslo_service import periodic_task
52 from oslo_utils import excutils
53 from oslo_utils import strutils
54 from oslo_utils import timeutils
55 from oslo_utils import units
56 import six
57 from six.moves import range
58 
59 from nova.accelerator import cyborg
60 from nova import block_device
61 from nova.compute import api as compute
62 from nova.compute import build_results
63 from nova.compute import claims
64 from nova.compute import power_state
65 from nova.compute import resource_tracker
66 from nova.compute import rpcapi as compute_rpcapi
67 from nova.compute import task_states
68 from nova.compute import utils as compute_utils
69 from nova.compute.utils import wrap_instance_event
70 from nova.compute import vm_states
71 from nova import conductor
72 import nova.conf
73 import nova.context
74 from nova import exception
75 from nova import exception_wrapper
76 from nova.i18n import _
77 from nova.image import glance
78 from nova import manager
79 from nova.network import model as network_model
80 from nova.network import neutron
81 from nova import objects
82 from nova.objects import base as obj_base
83 from nova.objects import external_event as external_event_obj
84 from nova.objects import fields
85 from nova.objects import instance as obj_instance
86 from nova.objects import migrate_data as migrate_data_obj
87 from nova.pci import request as pci_req_module
88 from nova.pci import whitelist
89 from nova import rpc
90 from nova import safe_utils
91 from nova.scheduler.client import query
92 from nova.scheduler.client import report
93 from nova.scheduler import utils as scheduler_utils
94 from nova import utils
95 from nova.virt import block_device as driver_block_device
96 from nova.virt import configdrive
97 from nova.virt import driver
98 from nova.virt import event as virtevent
99 from nova.virt import hardware
100 from nova.virt import storage_users
101 from nova.virt import virtapi
102 from nova.volume import cinder
103 
104 CONF = nova.conf.CONF
105 
106 LOG = logging.getLogger(__name__)
107 
108 get_notifier = functools.partial(rpc.get_notifier, service='compute')
109 wrap_exception = functools.partial(exception_wrapper.wrap_exception,
110                                    get_notifier=get_notifier,
111                                    binary='nova-compute')
112 
113 
114 @contextlib.contextmanager
115 def errors_out_migration_ctxt(migration):
116     """Context manager to error out migration on failure."""
117 
118     try:
119         yield
120     except Exception:
121         with excutils.save_and_reraise_exception():
122             if migration:
123                 # We may have been passed None for our migration if we're
124                 # receiving from an older client. The migration will be
125                 # errored via the legacy path.
126                 migration.status = 'error'
127                 try:
128                     migration.save()
129                 except Exception:
130                     LOG.debug(
131                         'Error setting migration status for instance %s.',
132                         migration.instance_uuid, exc_info=True)
133 
134 
135 @utils.expects_func_args('migration')
136 def errors_out_migration(function):
137     """Decorator to error out migration on failure."""
138 
139     @functools.wraps(function)
140     def decorated_function(self, context, *args, **kwargs):
141         wrapped_func = safe_utils.get_wrapped_function(function)
142         keyed_args = inspect.getcallargs(wrapped_func, self, context,
143                                          *args, **kwargs)
144         migration = keyed_args['migration']
145         with errors_out_migration_ctxt(migration):
146             return function(self, context, *args, **kwargs)
147 
148     return decorated_function
149 
150 
151 @utils.expects_func_args('instance')
152 def reverts_task_state(function):
153     """Decorator to revert task_state on failure."""
154 
155     @functools.wraps(function)
156     def decorated_function(self, context, *args, **kwargs):
157         try:
158             return function(self, context, *args, **kwargs)
159         except exception.UnexpectedTaskStateError as e:
160             # Note(maoy): unexpected task state means the current
161             # task is preempted. Do not clear task state in this
162             # case.
163             with excutils.save_and_reraise_exception():
164                 LOG.info("Task possibly preempted: %s",
165                          e.format_message())
166         except Exception:
167             with excutils.save_and_reraise_exception():
168                 wrapped_func = safe_utils.get_wrapped_function(function)
169                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
170                                                  *args, **kwargs)
171                 # NOTE(mriedem): 'instance' must be in keyed_args because we
172                 # have utils.expects_func_args('instance') decorating this
173                 # method.
174                 instance = keyed_args['instance']
175                 original_task_state = instance.task_state
176                 try:
177                     self._instance_update(context, instance, task_state=None)
178                     LOG.info("Successfully reverted task state from %s on "
179                              "failure for instance.",
180                              original_task_state, instance=instance)
181                 except exception.InstanceNotFound:
182                     # We might delete an instance that failed to build shortly
183                     # after it errored out this is an expected case and we
184                     # should not trace on it.
185                     pass
186                 except Exception as e:
187                     LOG.warning("Failed to revert task state for instance. "
188                                 "Error: %s", e, instance=instance)
189 
190     return decorated_function
191 
192 
193 @utils.expects_func_args('instance')
194 def wrap_instance_fault(function):
195     """Wraps a method to catch exceptions related to instances.
196 
197     This decorator wraps a method to catch any exceptions having to do with
198     an instance that may get thrown. It then logs an instance fault in the db.
199     """
200 
201     @functools.wraps(function)
202     def decorated_function(self, context, *args, **kwargs):
203         try:
204             return function(self, context, *args, **kwargs)
205         except exception.InstanceNotFound:
206             raise
207         except Exception as e:
208             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
209             # we will get a KeyError exception which will cover up the real
210             # exception. So, we update kwargs with the values from args first.
211             # then, we can get 'instance' from kwargs easily.
212             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
213 
214             with excutils.save_and_reraise_exception():
215                 compute_utils.add_instance_fault_from_exc(context,
216                         kwargs['instance'], e, sys.exc_info())
217 
218     return decorated_function
219 
220 
221 @utils.expects_func_args('image_id', 'instance')
222 def delete_image_on_error(function):
223     """Used for snapshot related method to ensure the image created in
224     compute.api is deleted when an error occurs.
225     """
226 
227     @functools.wraps(function)
228     def decorated_function(self, context, image_id, instance,
229                            *args, **kwargs):
230         try:
231             return function(self, context, image_id, instance,
232                             *args, **kwargs)
233         except Exception:
234             with excutils.save_and_reraise_exception():
235                 compute_utils.delete_image(
236                     context, instance, self.image_api, image_id,
237                     log_exc_info=True)
238 
239     return decorated_function
240 
241 
242 class InstanceEvents(object):
243     def __init__(self):
244         self._events = {}
245 
246     @staticmethod
247     def _lock_name(instance):
248         return '%s-%s' % (instance.uuid, 'events')
249 
250     def prepare_for_instance_event(self, instance, name, tag):
251         """Prepare to receive an event for an instance.
252 
253         This will register an event for the given instance that we will
254         wait on later. This should be called before initiating whatever
255         action will trigger the event. The resulting eventlet.event.Event
256         object should be wait()'d on to ensure completion.
257 
258         :param instance: the instance for which the event will be generated
259         :param name: the name of the event we're expecting
260         :param tag: the tag associated with the event we're expecting
261         :returns: an event object that should be wait()'d on
262         """
263         if self._events is None:
264             # NOTE(danms): We really should have a more specific error
265             # here, but this is what we use for our default error case
266             raise exception.NovaException('In shutdown, no new events '
267                                           'can be scheduled')
268 
269         @utils.synchronized(self._lock_name(instance))
270         def _create_or_get_event():
271             instance_events = self._events.setdefault(instance.uuid, {})
272             return instance_events.setdefault((name, tag),
273                                               eventlet.event.Event())
274         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
275                   {'name': name, 'tag': tag}, instance=instance)
276         return _create_or_get_event()
277 
278     def pop_instance_event(self, instance, event):
279         """Remove a pending event from the wait list.
280 
281         This will remove a pending event from the wait list so that it
282         can be used to signal the waiters to wake up.
283 
284         :param instance: the instance for which the event was generated
285         :param event: the nova.objects.external_event.InstanceExternalEvent
286                       that describes the event
287         :returns: the eventlet.event.Event object on which the waiters
288                   are blocked
289         """
290         no_events_sentinel = object()
291         no_matching_event_sentinel = object()
292 
293         @utils.synchronized(self._lock_name(instance))
294         def _pop_event():
295             if self._events is None:
296                 LOG.debug('Unexpected attempt to pop events during shutdown',
297                           instance=instance)
298                 return no_events_sentinel
299             events = self._events.get(instance.uuid)
300             if not events:
301                 return no_events_sentinel
302             _event = events.pop((event.name, event.tag), None)
303             if not events:
304                 del self._events[instance.uuid]
305             if _event is None:
306                 return no_matching_event_sentinel
307             return _event
308 
309         result = _pop_event()
310         if result is no_events_sentinel:
311             LOG.debug('No waiting events found dispatching %(event)s',
312                       {'event': event.key},
313                       instance=instance)
314             return None
315         elif result is no_matching_event_sentinel:
316             LOG.debug('No event matching %(event)s in %(events)s',
317                       {'event': event.key,
318                        'events': self._events.get(instance.uuid, {}).keys()},
319                       instance=instance)
320             return None
321         else:
322             return result
323 
324     def clear_events_for_instance(self, instance):
325         """Remove all pending events for an instance.
326 
327         This will remove all events currently pending for an instance
328         and return them (indexed by event name).
329 
330         :param instance: the instance for which events should be purged
331         :returns: a dictionary of {event_name: eventlet.event.Event}
332         """
333         @utils.synchronized(self._lock_name(instance))
334         def _clear_events():
335             if self._events is None:
336                 LOG.debug('Unexpected attempt to clear events during shutdown',
337                           instance=instance)
338                 return dict()
339             # NOTE(danms): We have historically returned the raw internal
340             # format here, which is {event.key: [events, ...])} so just
341             # trivially convert it here.
342             return {'%s-%s' % k: e
343                     for k, e in self._events.pop(instance.uuid, {}).items()}
344         return _clear_events()
345 
346     def cancel_all_events(self):
347         if self._events is None:
348             LOG.debug('Unexpected attempt to cancel events during shutdown.')
349             return
350         our_events = self._events
351         # NOTE(danms): Block new events
352         self._events = None
353 
354         for instance_uuid, events in our_events.items():
355             for (name, tag), eventlet_event in events.items():
356                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
357                           'instance %(instance_uuid)s',
358                           {'name': name,
359                            'tag': tag,
360                            'instance_uuid': instance_uuid})
361                 event = objects.InstanceExternalEvent(
362                     instance_uuid=instance_uuid,
363                     name=name, status='failed',
364                     tag=tag, data={})
365                 eventlet_event.send(event)
366 
367 
368 class ComputeVirtAPI(virtapi.VirtAPI):
369     def __init__(self, compute):
370         super(ComputeVirtAPI, self).__init__()
371         self._compute = compute
372         self.reportclient = compute.reportclient
373 
374         class ExitEarly(Exception):
375             def __init__(self, events):
376                 super(Exception, self).__init__()
377                 self.events = events
378 
379         self._exit_early_exc = ExitEarly
380 
381     def exit_wait_early(self, events):
382         """Exit a wait_for_instance_event() immediately and avoid
383         waiting for some events.
384 
385         :param: events: A list of (name, tag) tuples for events that we should
386                         skip waiting for during a wait_for_instance_event().
387         """
388         raise self._exit_early_exc(events=events)
389 
390     def _default_error_callback(self, event_name, instance):
391         raise exception.NovaException(_('Instance event failed'))
392 
393     @contextlib.contextmanager
394     def wait_for_instance_event(self, instance, event_names, deadline=300,
395                                 error_callback=None):
396         """Plan to wait for some events, run some code, then wait.
397 
398         This context manager will first create plans to wait for the
399         provided event_names, yield, and then wait for all the scheduled
400         events to complete.
401 
402         Note that this uses an eventlet.timeout.Timeout to bound the
403         operation, so callers should be prepared to catch that
404         failure and handle that situation appropriately.
405 
406         If the event is not received by the specified timeout deadline,
407         eventlet.timeout.Timeout is raised.
408 
409         If the event is received but did not have a 'completed'
410         status, a NovaException is raised.  If an error_callback is
411         provided, instead of raising an exception as detailed above
412         for the failure case, the callback will be called with the
413         event_name and instance, and can return True to continue
414         waiting for the rest of the events, False to stop processing,
415         or raise an exception which will bubble up to the waiter.
416 
417         If the inner code wishes to abort waiting for one or more
418         events because it knows some state to be finished or condition
419         to be satisfied, it can use VirtAPI.exit_wait_early() with a
420         list of event (name,tag) items to avoid waiting for those
421         events upon context exit. Note that exit_wait_early() exits
422         the context immediately and should be used to signal that all
423         work has been completed and provide the unified list of events
424         that need not be waited for. Waiting for the remaining events
425         will begin immediately upon early exit as if the context was
426         exited normally.
427 
428         :param instance: The instance for which an event is expected
429         :param event_names: A list of event names. Each element is a
430                             tuple of strings to indicate (name, tag),
431                             where name is required, but tag may be None.
432         :param deadline: Maximum number of seconds we should wait for all
433                          of the specified events to arrive.
434         :param error_callback: A function to be called if an event arrives
435 
436         """
437 
438         if error_callback is None:
439             error_callback = self._default_error_callback
440         events = {}
441         for event_name in event_names:
442             name, tag = event_name
443             event_name = objects.InstanceExternalEvent.make_key(name, tag)
444             try:
445                 events[event_name] = (
446                     self._compute.instance_events.prepare_for_instance_event(
447                         instance, name, tag))
448             except exception.NovaException:
449                 error_callback(event_name, instance)
450                 # NOTE(danms): Don't wait for any of the events. They
451                 # should all be canceled and fired immediately below,
452                 # but don't stick around if not.
453                 deadline = 0
454         try:
455             yield
456         except self._exit_early_exc as e:
457             early_events = set([objects.InstanceExternalEvent.make_key(n, t)
458                                 for n, t in e.events])
459         else:
460             early_events = []
461 
462         with eventlet.timeout.Timeout(deadline):
463             for event_name, event in events.items():
464                 if event_name in early_events:
465                     continue
466                 else:
467                     actual_event = event.wait()
468                     if actual_event.status == 'completed':
469                         continue
470                 # If we get here, we have an event that was not completed,
471                 # nor skipped via exit_wait_early(). Decide whether to
472                 # keep waiting by calling the error_callback() hook.
473                 decision = error_callback(event_name, instance)
474                 if decision is False:
475                     break
476 
477     def update_compute_provider_status(self, context, rp_uuid, enabled):
478         """Used to add/remove the COMPUTE_STATUS_DISABLED trait on the provider
479 
480         :param context: nova auth RequestContext
481         :param rp_uuid: UUID of a compute node resource provider in Placement
482         :param enabled: True if the node is enabled in which case the trait
483             would be removed, False if the node is disabled in which case
484             the trait would be added.
485         :raises: ResourceProviderTraitRetrievalFailed
486         :raises: ResourceProviderUpdateConflict
487         :raises: ResourceProviderUpdateFailed
488         :raises: TraitRetrievalFailed
489         :raises: keystoneauth1.exceptions.ClientException
490         """
491         trait_name = os_traits.COMPUTE_STATUS_DISABLED
492         # Get the current traits (and generation) for the provider.
493         # TODO(mriedem): Leverage the ProviderTree cache in get_provider_traits
494         trait_info = self.reportclient.get_provider_traits(context, rp_uuid)
495         # If the host is enabled, remove the trait (if set), else add
496         # the trait if it doesn't already exist.
497         original_traits = trait_info.traits
498         new_traits = None
499         if enabled and trait_name in original_traits:
500             new_traits = original_traits - {trait_name}
501             LOG.debug('Removing trait %s from compute node resource '
502                       'provider %s in placement.', trait_name, rp_uuid)
503         elif not enabled and trait_name not in original_traits:
504             new_traits = original_traits | {trait_name}
505             LOG.debug('Adding trait %s to compute node resource '
506                       'provider %s in placement.', trait_name, rp_uuid)
507 
508         if new_traits is not None:
509             self.reportclient.set_traits_for_provider(
510                 context, rp_uuid, new_traits, generation=trait_info.generation)
511 
512 
513 class ComputeManager(manager.Manager):
514     """Manages the running instances from creation to destruction."""
515 
516     target = messaging.Target(version='5.11')
517 
518     def __init__(self, compute_driver=None, *args, **kwargs):
519         """Load configuration options and connect to the hypervisor."""
520         # We want the ComputeManager, ResourceTracker and ComputeVirtAPI all
521         # using the same instance of SchedulerReportClient which has the
522         # ProviderTree cache for this compute service.
523         self.reportclient = report.SchedulerReportClient()
524         self.virtapi = ComputeVirtAPI(self)
525         self.network_api = neutron.API()
526         self.volume_api = cinder.API()
527         self.image_api = glance.API()
528         self._last_bw_usage_poll = 0
529         self._bw_usage_supported = True
530         self.compute_api = compute.API()
531         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
532         self.compute_task_api = conductor.ComputeTaskAPI()
533         self.query_client = query.SchedulerQueryClient()
534         self.instance_events = InstanceEvents()
535         self._sync_power_pool = eventlet.GreenPool(
536             size=CONF.sync_power_state_pool_size)
537         self._syncs_in_progress = {}
538         self.send_instance_updates = (
539             CONF.filter_scheduler.track_instance_changes)
540         if CONF.max_concurrent_builds != 0:
541             self._build_semaphore = eventlet.semaphore.Semaphore(
542                 CONF.max_concurrent_builds)
543         else:
544             self._build_semaphore = compute_utils.UnlimitedSemaphore()
545         if CONF.max_concurrent_snapshots > 0:
546             self._snapshot_semaphore = eventlet.semaphore.Semaphore(
547                 CONF.max_concurrent_snapshots)
548         else:
549             self._snapshot_semaphore = compute_utils.UnlimitedSemaphore()
550         if CONF.max_concurrent_live_migrations > 0:
551             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
552                 max_workers=CONF.max_concurrent_live_migrations)
553         else:
554             # CONF.max_concurrent_live_migrations is 0 (unlimited)
555             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
556         # This is a dict, keyed by instance uuid, to a two-item tuple of
557         # migration object and Future for the queued live migration.
558         self._waiting_live_migrations = {}
559 
560         super(ComputeManager, self).__init__(service_name="compute",
561                                              *args, **kwargs)
562 
563         # NOTE(russellb) Load the driver last.  It may call back into the
564         # compute manager via the virtapi, so we want it to be fully
565         # initialized before that happens.
566         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
567         self.use_legacy_block_device_info = \
568                             self.driver.need_legacy_block_device_info
569         self.rt = resource_tracker.ResourceTracker(
570             self.host, self.driver, reportclient=self.reportclient)
571 
572     def reset(self):
573         LOG.info('Reloading compute RPC API')
574         compute_rpcapi.reset_globals()
575         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
576         self.reportclient.clear_provider_cache()
577 
578     def _update_resource_tracker(self, context, instance):
579         """Let the resource tracker know that an instance has changed state."""
580 
581         if instance.host == self.host:
582             self.rt.update_usage(context, instance, instance.node)
583 
584     def _instance_update(self, context, instance, **kwargs):
585         """Update an instance in the database using kwargs as value."""
586 
587         for k, v in kwargs.items():
588             setattr(instance, k, v)
589         instance.save()
590         self._update_resource_tracker(context, instance)
591 
592     def _nil_out_instance_obj_host_and_node(self, instance):
593         # NOTE(jwcroppe): We don't do instance.save() here for performance
594         # reasons; a call to this is expected to be immediately followed by
595         # another call that does instance.save(), thus avoiding two writes
596         # to the database layer.
597         instance.host = None
598         instance.node = None
599         # ResourceTracker._set_instance_host_and_node also sets launched_on
600         # to the same value as host and is really only ever used by legacy
601         # nova-network code, but we should also null it out to avoid confusion
602         # if there is an instance in the database with no host set but
603         # launched_on is set. Note that we do not care about using launched_on
604         # as some kind of debug helper if diagnosing a build failure, that is
605         # what instance action events are for.
606         instance.launched_on = None
607         # If the instance is not on a host, it's not in an aggregate and
608         # therefore is not in an availability zone.
609         instance.availability_zone = None
610 
611     def _set_instance_obj_error_state(self, instance, clean_task_state=False):
612         try:
613             instance.vm_state = vm_states.ERROR
614             if clean_task_state:
615                 instance.task_state = None
616             instance.save()
617         except exception.InstanceNotFound:
618             LOG.debug('Instance has been destroyed from under us while '
619                       'trying to set it to ERROR', instance=instance)
620 
621     def _get_instances_on_driver(self, context, filters=None):
622         """Return a list of instance records for the instances found
623         on the hypervisor which satisfy the specified filters. If filters=None
624         return a list of instance records for all the instances found on the
625         hypervisor.
626         """
627         if not filters:
628             filters = {}
629         try:
630             driver_uuids = self.driver.list_instance_uuids()
631             if len(driver_uuids) == 0:
632                 # Short circuit, don't waste a DB call
633                 return objects.InstanceList()
634             filters['uuid'] = driver_uuids
635             local_instances = objects.InstanceList.get_by_filters(
636                 context, filters, use_slave=True)
637             return local_instances
638         except NotImplementedError:
639             pass
640 
641         # The driver doesn't support uuids listing, so we'll have
642         # to brute force.
643         driver_instances = self.driver.list_instances()
644         # NOTE(mjozefcz): In this case we need to apply host filter.
645         # Without this all instance data would be fetched from db.
646         filters['host'] = self.host
647         instances = objects.InstanceList.get_by_filters(context, filters,
648                                                         use_slave=True)
649         name_map = {instance.name: instance for instance in instances}
650         local_instances = []
651         for driver_instance in driver_instances:
652             instance = name_map.get(driver_instance)
653             if not instance:
654                 continue
655             local_instances.append(instance)
656         return local_instances
657 
658     def _destroy_evacuated_instances(self, context, node_cache):
659         """Destroys evacuated instances.
660 
661         While nova-compute was down, the instances running on it could be
662         evacuated to another host. This method looks for evacuation migration
663         records where this is the source host and which were either started
664         (accepted), in-progress (pre-migrating) or migrated (done). From those
665         migration records, local instances reported by the hypervisor are
666         compared to the instances for the migration records and those local
667         guests are destroyed, along with instance allocation records in
668         Placement for this node.
669         Then allocations are removed from Placement for every instance that is
670         evacuated from this host regardless if the instance is reported by the
671         hypervisor or not.
672 
673         :param context: The request context
674         :param node_cache: A dict of ComputeNode objects keyed by the UUID of
675             the compute node
676         :return: A dict keyed by instance uuid mapped to Migration objects
677             for instances that were migrated away from this host
678         """
679         filters = {
680             'source_compute': self.host,
681             # NOTE(mriedem): Migration records that have been accepted are
682             # included in case the source node comes back up while instances
683             # are being evacuated to another host. We don't want the same
684             # instance being reported from multiple hosts.
685             # NOTE(lyarwood): pre-migrating is also included here as the
686             # source compute can come back online shortly after the RT
687             # claims on the destination that in-turn moves the migration to
688             # pre-migrating. If the evacuate fails on the destination host,
689             # the user can rebuild the instance (in ERROR state) on the source
690             # host.
691             'status': ['accepted', 'pre-migrating', 'done'],
692             'migration_type': fields.MigrationType.EVACUATION,
693         }
694         with utils.temporary_mutation(context, read_deleted='yes'):
695             evacuations = objects.MigrationList.get_by_filters(context,
696                                                                filters)
697         if not evacuations:
698             return {}
699         evacuations = {mig.instance_uuid: mig for mig in evacuations}
700 
701         # TODO(mriedem): We could optimize by pre-loading the joined fields
702         # we know we'll use, like info_cache and flavor.
703         local_instances = self._get_instances_on_driver(context)
704         evacuated_local_instances = {inst.uuid: inst
705                                      for inst in local_instances
706                                      if inst.uuid in evacuations}
707 
708         for instance in evacuated_local_instances.values():
709             LOG.info('Destroying instance as it has been evacuated from '
710                      'this host but still exists in the hypervisor',
711                      instance=instance)
712             try:
713                 network_info = self.network_api.get_instance_nw_info(
714                     context, instance)
715                 bdi = self._get_instance_block_device_info(context,
716                                                            instance)
717                 destroy_disks = not (self._is_instance_storage_shared(
718                     context, instance))
719             except exception.InstanceNotFound:
720                 network_info = network_model.NetworkInfo()
721                 bdi = {}
722                 LOG.info('Instance has been marked deleted already, '
723                          'removing it from the hypervisor.',
724                          instance=instance)
725                 # always destroy disks if the instance was deleted
726                 destroy_disks = True
727             self.driver.destroy(context, instance,
728                                 network_info,
729                                 bdi, destroy_disks)
730 
731         hostname_to_cn_uuid = {
732             cn.hypervisor_hostname: cn.uuid
733             for cn in node_cache.values()}
734 
735         for instance_uuid, migration in evacuations.items():
736             try:
737                 if instance_uuid in evacuated_local_instances:
738                     # Avoid the db call if we already have the instance loaded
739                     # above
740                     instance = evacuated_local_instances[instance_uuid]
741                 else:
742                     instance = objects.Instance.get_by_uuid(
743                         context, instance_uuid)
744             except exception.InstanceNotFound:
745                 # The instance already deleted so we expect that every
746                 # allocation of that instance has already been cleaned up
747                 continue
748 
749             LOG.info('Cleaning up allocations of the instance as it has been '
750                      'evacuated from this host',
751                      instance=instance)
752             if migration.source_node not in hostname_to_cn_uuid:
753                 LOG.error("Failed to clean allocation of evacuated "
754                           "instance as the source node %s is not found",
755                           migration.source_node, instance=instance)
756                 continue
757             cn_uuid = hostname_to_cn_uuid[migration.source_node]
758 
759             # If the instance was deleted in the interim, assume its
760             # allocations were properly cleaned up (either by its hosting
761             # compute service or the API).
762             if (not instance.deleted and
763                     not self.reportclient.
764                         remove_provider_tree_from_instance_allocation(
765                             context, instance.uuid, cn_uuid)):
766                 LOG.error("Failed to clean allocation of evacuated instance "
767                           "on the source node %s",
768                           cn_uuid, instance=instance)
769 
770             migration.status = 'completed'
771             migration.save()
772         return evacuations
773 
774     def _is_instance_storage_shared(self, context, instance, host=None):
775         shared_storage = True
776         data = None
777         try:
778             data = self.driver.check_instance_shared_storage_local(context,
779                                                        instance)
780             if data:
781                 shared_storage = (self.compute_rpcapi.
782                                   check_instance_shared_storage(context,
783                                   instance, data, host=host))
784         except NotImplementedError:
785             LOG.debug('Hypervisor driver does not support '
786                       'instance shared storage check, '
787                       'assuming it\'s not on shared storage',
788                       instance=instance)
789             shared_storage = False
790         except Exception:
791             LOG.exception('Failed to check if instance shared',
792                           instance=instance)
793         finally:
794             if data:
795                 self.driver.check_instance_shared_storage_cleanup(context,
796                                                                   data)
797         return shared_storage
798 
799     def _complete_partial_deletion(self, context, instance):
800         """Complete deletion for instances in DELETED status but not marked as
801         deleted in the DB
802         """
803         instance.destroy()
804         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
805                 context, instance.uuid)
806         self._complete_deletion(context,
807                                 instance)
808         self._notify_about_instance_usage(context, instance, "delete.end")
809         compute_utils.notify_about_instance_action(context, instance,
810                 self.host, action=fields.NotificationAction.DELETE,
811                 phase=fields.NotificationPhase.END, bdms=bdms)
812 
813     def _complete_deletion(self, context, instance):
814         self._update_resource_tracker(context, instance)
815 
816         self.reportclient.delete_allocation_for_instance(context,
817                                                          instance.uuid)
818 
819         self._clean_instance_console_tokens(context, instance)
820         self._delete_scheduler_instance_info(context, instance.uuid)
821 
822     def _validate_pinning_configuration(self, instances):
823         if not self.driver.capabilities.get('supports_pcpus', False):
824             return
825 
826         for instance in instances:
827             # ignore deleted instances
828             if instance.deleted:
829                 continue
830 
831             # if this is an unpinned instance and the host only has
832             # 'cpu_dedicated_set' configured, we need to tell the operator to
833             # correct their configuration
834             if not instance.numa_topology or (
835                 instance.numa_topology.cpu_policy in (
836                     None, fields.CPUAllocationPolicy.SHARED
837                 )
838             ):
839                 # we don't need to check 'vcpu_pin_set' since it can't coexist
840                 # alongside 'cpu_dedicated_set'
841                 if (CONF.compute.cpu_dedicated_set and
842                         not CONF.compute.cpu_shared_set):
843                     msg = _("This host has unpinned instances but has no CPUs "
844                             "set aside for this purpose; configure '[compute] "
845                             "cpu_shared_set' instead of, or in addition to, "
846                             "'[compute] cpu_dedicated_set'")
847                     raise exception.InvalidConfiguration(msg)
848 
849                 continue
850 
851             # ditto for pinned instances if only 'cpu_shared_set' is configured
852             if (CONF.compute.cpu_shared_set and
853                     not CONF.compute.cpu_dedicated_set and
854                     not CONF.vcpu_pin_set):
855                 msg = _("This host has pinned instances but has no CPUs "
856                         "set aside for this purpose; configure '[compute] "
857                         "cpu_dedicated_set' instead of, or in addition to, "
858                         "'[compute] cpu_shared_set'.")
859                 raise exception.InvalidConfiguration(msg)
860 
861             # if this is a mixed instance with both pinned and unpinned CPUs,
862             # the host must have both 'cpu_dedicated_set' and 'cpu_shared_set'
863             # configured. check if 'cpu_shared_set' is set.
864             if (instance.numa_topology.cpu_policy ==
865                     fields.CPUAllocationPolicy.MIXED and
866                     not CONF.compute.cpu_shared_set):
867                 msg = _("This host has mixed instance requesting both pinned "
868                         "and unpinned CPUs but hasn't set aside unpinned CPUs "
869                         "for this purpose; Configure "
870                         "'[compute] cpu_shared_set'.")
871                 raise exception.InvalidConfiguration(msg)
872 
873             # for mixed instance check if 'cpu_dedicated_set' is set.
874             if (instance.numa_topology.cpu_policy ==
875                     fields.CPUAllocationPolicy.MIXED and
876                     not CONF.compute.cpu_dedicated_set):
877                 msg = _("This host has mixed instance requesting both pinned "
878                         "and unpinned CPUs but hasn't set aside pinned CPUs "
879                         "for this purpose; Configure "
880                         "'[compute] cpu_dedicated_set'")
881                 raise exception.InvalidConfiguration(msg)
882 
883             # also check to make sure the operator hasn't accidentally
884             # dropped some cores that instances are currently using
885             available_dedicated_cpus = (hardware.get_vcpu_pin_set() or
886                                         hardware.get_cpu_dedicated_set())
887             pinned_cpus = instance.numa_topology.cpu_pinning
888             if available_dedicated_cpus and (
889                     pinned_cpus - available_dedicated_cpus):
890                 # we can't raise an exception because of bug #1289064,
891                 # which meant we didn't recalculate CPU pinning information
892                 # when we live migrated a pinned instance
893                 LOG.warning(
894                     "Instance is pinned to host CPUs %(cpus)s "
895                     "but one or more of these CPUs are not included in "
896                     "either '[compute] cpu_dedicated_set' or "
897                     "'vcpu_pin_set'; you should update these "
898                     "configuration options to include the missing CPUs "
899                     "or rebuild or cold migrate this instance.",
900                     {'cpus': list(pinned_cpus)},
901                     instance=instance)
902 
903     def _validate_vtpm_configuration(self, instances):
904         if self.driver.capabilities.get('supports_vtpm', False):
905             return
906 
907         for instance in instances:
908             if instance.deleted:
909                 continue
910 
911             # NOTE(stephenfin): We don't have an attribute on the instance to
912             # check for this, so we need to inspect the flavor/image metadata
913             if hardware.get_vtpm_constraint(
914                 instance.flavor, instance.image_meta,
915             ):
916                 msg = _(
917                     'This host has instances with the vTPM feature enabled, '
918                     'but the host is not correctly configured; enable '
919                     'vTPM support.'
920                 )
921                 raise exception.InvalidConfiguration(msg)
922 
923     def _reset_live_migration(self, context, instance):
924         migration = None
925         try:
926             migration = objects.Migration.get_by_instance_and_status(
927                                       context, instance.uuid, 'running')
928             if migration:
929                 self.live_migration_abort(context, instance, migration.id)
930         except Exception:
931             LOG.exception('Failed to abort live-migration',
932                           instance=instance)
933         finally:
934             if migration:
935                 self._set_migration_status(migration, 'error')
936             LOG.info('Instance found in migrating state during '
937                      'startup. Resetting task_state',
938                      instance=instance)
939             instance.task_state = None
940             instance.save(expected_task_state=[task_states.MIGRATING])
941 
942     def _init_instance(self, context, instance):
943         """Initialize this instance during service init."""
944 
945         # NOTE(danms): If the instance appears to not be owned by this
946         # host, it may have been evacuated away, but skipped by the
947         # evacuation cleanup code due to configuration. Thus, if that
948         # is a possibility, don't touch the instance in any way, but
949         # log the concern. This will help avoid potential issues on
950         # startup due to misconfiguration.
951         if instance.host != self.host:
952             LOG.warning('Instance %(uuid)s appears to not be owned '
953                         'by this host, but by %(host)s. Startup '
954                         'processing is being skipped.',
955                         {'uuid': instance.uuid,
956                          'host': instance.host})
957             return
958 
959         # Instances that are shut down, or in an error state can not be
960         # initialized and are not attempted to be recovered. The exception
961         # to this are instances that are in RESIZE_MIGRATING or DELETING,
962         # which are dealt with further down.
963         if (instance.vm_state == vm_states.SOFT_DELETED or
964             (instance.vm_state == vm_states.ERROR and
965             instance.task_state not in
966             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
967             LOG.debug("Instance is in %s state.",
968                       instance.vm_state, instance=instance)
969             return
970 
971         if instance.vm_state == vm_states.DELETED:
972             try:
973                 self._complete_partial_deletion(context, instance)
974             except Exception:
975                 # we don't want that an exception blocks the init_host
976                 LOG.exception('Failed to complete a deletion',
977                               instance=instance)
978             return
979 
980         if (instance.vm_state == vm_states.BUILDING or
981             instance.task_state in [task_states.SCHEDULING,
982                                     task_states.BLOCK_DEVICE_MAPPING,
983                                     task_states.NETWORKING,
984                                     task_states.SPAWNING]):
985             # NOTE(dave-mcnally) compute stopped before instance was fully
986             # spawned so set to ERROR state. This is safe to do as the state
987             # may be set by the api but the host is not so if we get here the
988             # instance has already been scheduled to this particular host.
989             LOG.debug("Instance failed to spawn correctly, "
990                       "setting to ERROR state", instance=instance)
991             self._set_instance_obj_error_state(instance, clean_task_state=True)
992             return
993 
994         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
995             instance.task_state in [task_states.REBUILDING,
996                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
997                                     task_states.REBUILD_SPAWNING]):
998             # NOTE(jichenjc) compute stopped before instance was fully
999             # spawned so set to ERROR state. This is consistent to BUILD
1000             LOG.debug("Instance failed to rebuild correctly, "
1001                       "setting to ERROR state", instance=instance)
1002             self._set_instance_obj_error_state(instance, clean_task_state=True)
1003             return
1004 
1005         if (instance.vm_state != vm_states.ERROR and
1006             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
1007                                     task_states.IMAGE_PENDING_UPLOAD,
1008                                     task_states.IMAGE_UPLOADING,
1009                                     task_states.IMAGE_SNAPSHOT]):
1010             LOG.debug("Instance in transitional state %s at start-up "
1011                       "clearing task state",
1012                       instance.task_state, instance=instance)
1013             try:
1014                 self._post_interrupted_snapshot_cleanup(context, instance)
1015             except Exception:
1016                 # we don't want that an exception blocks the init_host
1017                 LOG.exception('Failed to cleanup snapshot.', instance=instance)
1018             instance.task_state = None
1019             instance.save()
1020 
1021         if (instance.vm_state != vm_states.ERROR and
1022             instance.task_state in [task_states.RESIZE_PREP]):
1023             LOG.debug("Instance in transitional state %s at start-up "
1024                       "clearing task state",
1025                       instance['task_state'], instance=instance)
1026             instance.task_state = None
1027             instance.save()
1028 
1029         if instance.task_state == task_states.DELETING:
1030             try:
1031                 LOG.info('Service started deleting the instance during '
1032                          'the previous run, but did not finish. Restarting'
1033                          ' the deletion now.', instance=instance)
1034                 instance.obj_load_attr('metadata')
1035                 instance.obj_load_attr('system_metadata')
1036                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1037                         context, instance.uuid)
1038                 self._delete_instance(context, instance, bdms)
1039             except Exception:
1040                 # we don't want that an exception blocks the init_host
1041                 LOG.exception('Failed to complete a deletion',
1042                               instance=instance)
1043                 self._set_instance_obj_error_state(instance)
1044             return
1045 
1046         current_power_state = self._get_power_state(instance)
1047         try_reboot, reboot_type = self._retry_reboot(
1048             instance, current_power_state)
1049 
1050         if try_reboot:
1051             LOG.debug("Instance in transitional state (%(task_state)s) at "
1052                       "start-up and power state is (%(power_state)s), "
1053                       "triggering reboot",
1054                       {'task_state': instance.task_state,
1055                        'power_state': current_power_state},
1056                       instance=instance)
1057 
1058             # NOTE(mikal): if the instance was doing a soft reboot that got as
1059             # far as shutting down the instance but not as far as starting it
1060             # again, then we've just become a hard reboot. That means the
1061             # task state for the instance needs to change so that we're in one
1062             # of the expected task states for a hard reboot.
1063             if (instance.task_state in task_states.soft_reboot_states and
1064                 reboot_type == 'HARD'):
1065                 instance.task_state = task_states.REBOOT_PENDING_HARD
1066                 instance.save()
1067 
1068             self.reboot_instance(context, instance, block_device_info=None,
1069                                  reboot_type=reboot_type)
1070             return
1071 
1072         elif (current_power_state == power_state.RUNNING and
1073               instance.task_state in [task_states.REBOOT_STARTED,
1074                                       task_states.REBOOT_STARTED_HARD,
1075                                       task_states.PAUSING,
1076                                       task_states.UNPAUSING]):
1077             LOG.warning("Instance in transitional state "
1078                         "(%(task_state)s) at start-up and power state "
1079                         "is (%(power_state)s), clearing task state",
1080                         {'task_state': instance.task_state,
1081                          'power_state': current_power_state},
1082                         instance=instance)
1083             instance.task_state = None
1084             instance.vm_state = vm_states.ACTIVE
1085             instance.save()
1086         elif (current_power_state == power_state.PAUSED and
1087               instance.task_state == task_states.UNPAUSING):
1088             LOG.warning("Instance in transitional state "
1089                         "(%(task_state)s) at start-up and power state "
1090                         "is (%(power_state)s), clearing task state "
1091                         "and unpausing the instance",
1092                         {'task_state': instance.task_state,
1093                          'power_state': current_power_state},
1094                         instance=instance)
1095             try:
1096                 self.unpause_instance(context, instance)
1097             except NotImplementedError:
1098                 # Some virt driver didn't support pause and unpause
1099                 pass
1100             except Exception:
1101                 LOG.exception('Failed to unpause instance', instance=instance)
1102             return
1103 
1104         if instance.task_state == task_states.POWERING_OFF:
1105             try:
1106                 LOG.debug("Instance in transitional state %s at start-up "
1107                           "retrying stop request",
1108                           instance.task_state, instance=instance)
1109                 self.stop_instance(context, instance, True)
1110             except Exception:
1111                 # we don't want that an exception blocks the init_host
1112                 LOG.exception('Failed to stop instance', instance=instance)
1113             return
1114 
1115         if instance.task_state == task_states.POWERING_ON:
1116             try:
1117                 LOG.debug("Instance in transitional state %s at start-up "
1118                           "retrying start request",
1119                           instance.task_state, instance=instance)
1120                 self.start_instance(context, instance)
1121             except Exception:
1122                 # we don't want that an exception blocks the init_host
1123                 LOG.exception('Failed to start instance', instance=instance)
1124             return
1125 
1126         net_info = instance.get_network_info()
1127         try:
1128             self.driver.plug_vifs(instance, net_info)
1129         except NotImplementedError as e:
1130             LOG.debug(e, instance=instance)
1131         except exception.VirtualInterfacePlugException:
1132             # NOTE(mriedem): If we get here, it could be because the vif_type
1133             # in the cache is "binding_failed" or "unbound".
1134             # The periodic task _heal_instance_info_cache checks for this
1135             # condition. It should fix this by binding the ports again when
1136             # it gets to this instance.
1137             LOG.exception('Virtual interface plugging failed for instance. '
1138                           'The port binding:host_id may need to be manually '
1139                           'updated.', instance=instance)
1140             self._set_instance_obj_error_state(instance)
1141             return
1142 
1143         if instance.task_state == task_states.RESIZE_MIGRATING:
1144             # We crashed during resize/migration, so roll back for safety
1145             try:
1146                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
1147                 # not in system_metadata we default to True for backwards
1148                 # compatibility
1149                 power_on = (instance.system_metadata.get('old_vm_state') !=
1150                             vm_states.STOPPED)
1151 
1152                 block_dev_info = self._get_instance_block_device_info(context,
1153                                                                       instance)
1154 
1155                 migration = objects.Migration.get_by_id_and_instance(
1156                     context, instance.migration_context.migration_id,
1157                     instance.uuid)
1158                 self.driver.finish_revert_migration(context, instance,
1159                     net_info, migration, block_dev_info, power_on)
1160 
1161             except Exception:
1162                 LOG.exception('Failed to revert crashed migration',
1163                               instance=instance)
1164             finally:
1165                 LOG.info('Instance found in migrating state during '
1166                          'startup. Resetting task_state',
1167                          instance=instance)
1168                 instance.task_state = None
1169                 instance.save()
1170         if instance.task_state == task_states.MIGRATING:
1171             # Live migration did not complete, but instance is on this
1172             # host. Abort ongoing migration if still running and reset state.
1173             self._reset_live_migration(context, instance)
1174 
1175         db_state = instance.power_state
1176         drv_state = self._get_power_state(instance)
1177         expect_running = (db_state == power_state.RUNNING and
1178                           drv_state != db_state)
1179 
1180         LOG.debug('Current state is %(drv_state)s, state in DB is '
1181                   '%(db_state)s.',
1182                   {'drv_state': drv_state, 'db_state': db_state},
1183                   instance=instance)
1184 
1185         if expect_running and CONF.resume_guests_state_on_host_boot:
1186             self._resume_guests_state(context, instance, net_info)
1187 
1188     def _resume_guests_state(self, context, instance, net_info):
1189         LOG.info('Rebooting instance after nova-compute restart.',
1190                  instance=instance)
1191         block_device_info = \
1192             self._get_instance_block_device_info(context, instance)
1193 
1194         try:
1195             self.driver.resume_state_on_host_boot(
1196                 context, instance, net_info, block_device_info)
1197         except NotImplementedError:
1198             LOG.warning('Hypervisor driver does not support '
1199                         'resume guests', instance=instance)
1200         except Exception:
1201             # NOTE(vish): The instance failed to resume, so we set the
1202             #             instance to error and attempt to continue.
1203             LOG.warning('Failed to resume instance',
1204                         instance=instance)
1205             self._set_instance_obj_error_state(instance)
1206 
1207     def _retry_reboot(self, instance, current_power_state):
1208         current_task_state = instance.task_state
1209         retry_reboot = False
1210         reboot_type = compute_utils.get_reboot_type(current_task_state,
1211                                                     current_power_state)
1212 
1213         pending_soft = (
1214             current_task_state == task_states.REBOOT_PENDING and
1215             instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1216         pending_hard = (
1217             current_task_state == task_states.REBOOT_PENDING_HARD and
1218             instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1219         started_not_running = (current_task_state in
1220                                [task_states.REBOOT_STARTED,
1221                                 task_states.REBOOT_STARTED_HARD] and
1222                                current_power_state != power_state.RUNNING)
1223 
1224         if pending_soft or pending_hard or started_not_running:
1225             retry_reboot = True
1226 
1227         return retry_reboot, reboot_type
1228 
1229     def handle_lifecycle_event(self, event):
1230         LOG.info("VM %(state)s (Lifecycle Event)",
1231                  {'state': event.get_name()},
1232                  instance_uuid=event.get_instance_uuid())
1233         context = nova.context.get_admin_context(read_deleted='yes')
1234         vm_power_state = None
1235         event_transition = event.get_transition()
1236         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1237             vm_power_state = power_state.SHUTDOWN
1238         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1239             vm_power_state = power_state.RUNNING
1240         elif event_transition in (
1241                 virtevent.EVENT_LIFECYCLE_PAUSED,
1242                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1243                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1244             vm_power_state = power_state.PAUSED
1245         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1246             vm_power_state = power_state.RUNNING
1247         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1248             vm_power_state = power_state.SUSPENDED
1249         else:
1250             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1251 
1252         migrate_finish_statuses = {
1253             # This happens on the source node and indicates live migration
1254             # entered post-copy mode.
1255             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1256             # Suspended for offline migration.
1257             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1258         }
1259 
1260         expected_attrs = []
1261         if event_transition in migrate_finish_statuses:
1262             # Join on info_cache since that's needed in migrate_instance_start.
1263             expected_attrs.append('info_cache')
1264         instance = objects.Instance.get_by_uuid(context,
1265                                                 event.get_instance_uuid(),
1266                                                 expected_attrs=expected_attrs)
1267 
1268         # Note(lpetrut): The event may be delayed, thus not reflecting
1269         # the current instance power state. In that case, ignore the event.
1270         current_power_state = self._get_power_state(instance)
1271         if current_power_state == vm_power_state:
1272             LOG.debug('Synchronizing instance power state after lifecycle '
1273                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1274                       'current task_state: %(task_state)s, current DB '
1275                       'power_state: %(db_power_state)s, VM power_state: '
1276                       '%(vm_power_state)s',
1277                       {'event': event.get_name(),
1278                        'vm_state': instance.vm_state,
1279                        'task_state': instance.task_state,
1280                        'db_power_state': instance.power_state,
1281                        'vm_power_state': vm_power_state},
1282                       instance_uuid=instance.uuid)
1283             self._sync_instance_power_state(context,
1284                                             instance,
1285                                             vm_power_state)
1286 
1287         # The following checks are for live migration. We want to activate
1288         # the port binding for the destination host before the live migration
1289         # is resumed on the destination host in order to reduce network
1290         # downtime. Otherwise the ports are bound to the destination host
1291         # in post_live_migration_at_destination.
1292         # TODO(danms): Explore options for using a different live migration
1293         # specific callback for this instead of piggy-backing on the
1294         # handle_lifecycle_event callback.
1295         if (instance.task_state == task_states.MIGRATING and
1296                 event_transition in migrate_finish_statuses):
1297             status = migrate_finish_statuses[event_transition]
1298             try:
1299                 migration = objects.Migration.get_by_instance_and_status(
1300                             context, instance.uuid, status)
1301                 LOG.debug('Binding ports to destination host: %s',
1302                           migration.dest_compute, instance=instance)
1303                 # For neutron, migrate_instance_start will activate the
1304                 # destination host port bindings, if there are any created by
1305                 # conductor before live migration started.
1306                 self.network_api.migrate_instance_start(
1307                     context, instance, migration)
1308             except exception.MigrationNotFoundByStatus:
1309                 LOG.warning("Unable to find migration record with status "
1310                             "'%s' for instance. Port binding will happen in "
1311                             "post live migration.", status, instance=instance)
1312 
1313     def handle_events(self, event):
1314         if isinstance(event, virtevent.LifecycleEvent):
1315             try:
1316                 self.handle_lifecycle_event(event)
1317             except exception.InstanceNotFound:
1318                 LOG.debug("Event %s arrived for non-existent instance. The "
1319                           "instance was probably deleted.", event)
1320         else:
1321             LOG.debug("Ignoring event %s", event)
1322 
1323     def init_virt_events(self):
1324         if CONF.workarounds.handle_virt_lifecycle_events:
1325             self.driver.register_event_listener(self.handle_events)
1326         else:
1327             # NOTE(mriedem): If the _sync_power_states periodic task is
1328             # disabled we should emit a warning in the logs.
1329             if CONF.sync_power_state_interval < 0:
1330                 LOG.warning('Instance lifecycle events from the compute '
1331                             'driver have been disabled. Note that lifecycle '
1332                             'changes to an instance outside of the compute '
1333                             'service will not be synchronized '
1334                             'automatically since the _sync_power_states '
1335                             'periodic task is also disabled.')
1336             else:
1337                 LOG.info('Instance lifecycle events from the compute '
1338                          'driver have been disabled. Note that lifecycle '
1339                          'changes to an instance outside of the compute '
1340                          'service will only be synchronized by the '
1341                          '_sync_power_states periodic task.')
1342 
1343     def _get_nodes(self, context):
1344         """Queried the ComputeNode objects from the DB that are reported by the
1345         hypervisor.
1346 
1347         :param context: the request context
1348         :return: a dict of ComputeNode objects keyed by the UUID of the given
1349             node.
1350         """
1351         nodes_by_uuid = {}
1352         try:
1353             node_names = self.driver.get_available_nodes()
1354         except exception.VirtDriverNotReady:
1355             LOG.warning(
1356                 "Virt driver is not ready. If this is the first time this "
1357                 "service is starting on this host, then you can ignore this "
1358                 "warning.")
1359             return {}
1360 
1361         for node_name in node_names:
1362             try:
1363                 node = objects.ComputeNode.get_by_host_and_nodename(
1364                     context, self.host, node_name)
1365                 nodes_by_uuid[node.uuid] = node
1366             except exception.ComputeHostNotFound:
1367                 LOG.warning(
1368                     "Compute node %s not found in the database. If this is "
1369                     "the first time this service is starting on this host, "
1370                     "then you can ignore this warning.", node_name)
1371         return nodes_by_uuid
1372 
1373     def init_host(self):
1374         """Initialization for a standalone compute service."""
1375 
1376         if CONF.pci.passthrough_whitelist:
1377             # Simply loading the PCI passthrough whitelist will do a bunch of
1378             # validation that would otherwise wait until the PciDevTracker is
1379             # constructed when updating available resources for the compute
1380             # node(s) in the resource tracker, effectively killing that task.
1381             # So load up the whitelist when starting the compute service to
1382             # flush any invalid configuration early so we can kill the service
1383             # if the configuration is wrong.
1384             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1385 
1386         nova.conf.neutron.register_dynamic_opts(CONF)
1387         # Even if only libvirt uses them, make it available for all drivers
1388         nova.conf.devices.register_dynamic_opts(CONF)
1389 
1390         # Override the number of concurrent disk operations allowed if the
1391         # user has specified a limit.
1392         if CONF.compute.max_concurrent_disk_ops != 0:
1393             compute_utils.disk_ops_semaphore = \
1394                 eventlet.semaphore.BoundedSemaphore(
1395                     CONF.compute.max_concurrent_disk_ops)
1396 
1397         self.driver.init_host(host=self.host)
1398         context = nova.context.get_admin_context()
1399         instances = objects.InstanceList.get_by_host(
1400             context, self.host,
1401             expected_attrs=['info_cache', 'metadata', 'numa_topology'])
1402 
1403         self.init_virt_events()
1404 
1405         self._validate_pinning_configuration(instances)
1406         self._validate_vtpm_configuration(instances)
1407 
1408         # NOTE(gibi): At this point the compute_nodes of the resource tracker
1409         # has not been populated yet so we cannot rely on the resource tracker
1410         # here.
1411         # NOTE(gibi): If ironic and vcenter virt driver slow start time
1412         # becomes problematic here then we should consider adding a config
1413         # option or a driver flag to tell us if we should thread
1414         # _destroy_evacuated_instances and
1415         # _error_out_instances_whose_build_was_interrupted out in the
1416         # background on startup
1417         nodes_by_uuid = self._get_nodes(context)
1418 
1419         try:
1420             # checking that instance was not already evacuated to other host
1421             evacuated_instances = self._destroy_evacuated_instances(
1422                 context, nodes_by_uuid)
1423 
1424             # Initialise instances on the host that are not evacuating
1425             for instance in instances:
1426                 if instance.uuid not in evacuated_instances:
1427                     self._init_instance(context, instance)
1428 
1429             # NOTE(gibi): collect all the instance uuids that is in some way
1430             # was already handled above. Either by init_instance or by
1431             # _destroy_evacuated_instances. This way we can limit the scope of
1432             # the _error_out_instances_whose_build_was_interrupted call to look
1433             # only for instances that have allocations on this node and not
1434             # handled by the above calls.
1435             already_handled = {instance.uuid for instance in instances}.union(
1436                 evacuated_instances)
1437             self._error_out_instances_whose_build_was_interrupted(
1438                 context, already_handled, nodes_by_uuid.keys())
1439 
1440         finally:
1441             if instances:
1442                 # We only send the instance info to the scheduler on startup
1443                 # if there is anything to send, otherwise this host might
1444                 # not be mapped yet in a cell and the scheduler may have
1445                 # issues dealing with the information. Later changes to
1446                 # instances on this host will update the scheduler, or the
1447                 # _sync_scheduler_instance_info periodic task will.
1448                 self._update_scheduler_instance_info(context, instances)
1449 
1450     def _error_out_instances_whose_build_was_interrupted(
1451             self, context, already_handled_instances, node_uuids):
1452         """If there are instances in BUILDING state that are not
1453         assigned to this host but have allocations in placement towards
1454         this compute that means the nova-compute service was
1455         restarted while those instances waited for the resource claim
1456         to finish and the _set_instance_host_and_node() to update the
1457         instance.host field. We need to push them to ERROR state here to
1458         prevent keeping them in BUILDING state forever.
1459 
1460         :param context: The request context
1461         :param already_handled_instances: The set of instance UUIDs that the
1462             host initialization process already handled in some way.
1463         :param node_uuids: The list of compute node uuids handled by this
1464             service
1465         """
1466 
1467         # Strategy:
1468         # 1) Get the allocations from placement for our compute node(s)
1469         # 2) Remove the already handled instances from the consumer list;
1470         #    they are either already initialized or need to be skipped.
1471         # 3) Check which remaining consumer is an instance in BUILDING state
1472         #    and push it to ERROR state.
1473 
1474         LOG.info(
1475             "Looking for unclaimed instances stuck in BUILDING status for "
1476             "nodes managed by this host")
1477         for cn_uuid in node_uuids:
1478             try:
1479                 f = self.reportclient.get_allocations_for_resource_provider
1480                 allocations = f(context, cn_uuid).allocations
1481             except (exception.ResourceProviderAllocationRetrievalFailed,
1482                     keystone_exception.ClientException) as e:
1483                 LOG.error(
1484                     "Could not retrieve compute node resource provider %s and "
1485                     "therefore unable to error out any instances stuck in "
1486                     "BUILDING state. Error: %s", cn_uuid, six.text_type(e))
1487                 continue
1488 
1489             not_handled_consumers = (set(allocations) -
1490                                      already_handled_instances)
1491 
1492             if not not_handled_consumers:
1493                 continue
1494 
1495             filters = {
1496                 'vm_state': vm_states.BUILDING,
1497                 'uuid': not_handled_consumers
1498             }
1499 
1500             instances = objects.InstanceList.get_by_filters(
1501                 context, filters, expected_attrs=[])
1502 
1503             for instance in instances:
1504                 LOG.debug(
1505                     "Instance spawn was interrupted before instance_claim, "
1506                     "setting instance to ERROR state", instance=instance)
1507                 self._set_instance_obj_error_state(
1508                     instance, clean_task_state=True)
1509 
1510     def cleanup_host(self):
1511         self.driver.register_event_listener(None)
1512         self.instance_events.cancel_all_events()
1513         self.driver.cleanup_host(host=self.host)
1514         self._cleanup_live_migrations_in_pool()
1515 
1516     def _cleanup_live_migrations_in_pool(self):
1517         # Shutdown the pool so we don't get new requests.
1518         self._live_migration_executor.shutdown(wait=False)
1519         # For any queued migrations, cancel the migration and update
1520         # its status.
1521         for migration, future in self._waiting_live_migrations.values():
1522             # If we got here before the Future was submitted then we need
1523             # to move on since there isn't anything we can do.
1524             if future is None:
1525                 continue
1526             if future.cancel():
1527                 self._set_migration_status(migration, 'cancelled')
1528                 LOG.info('Successfully cancelled queued live migration.',
1529                          instance_uuid=migration.instance_uuid)
1530             else:
1531                 LOG.warning('Unable to cancel live migration.',
1532                             instance_uuid=migration.instance_uuid)
1533         self._waiting_live_migrations.clear()
1534 
1535     def pre_start_hook(self):
1536         """After the service is initialized, but before we fully bring
1537         the service up by listening on RPC queues, make sure to update
1538         our available resources (and indirectly our available nodes).
1539         """
1540         self.update_available_resource(nova.context.get_admin_context(),
1541                                        startup=True)
1542 
1543     def _get_power_state(self, instance):
1544         """Retrieve the power state for the given instance."""
1545         LOG.debug('Checking state', instance=instance)
1546         try:
1547             return self.driver.get_info(instance, use_cache=False).state
1548         except exception.InstanceNotFound:
1549             return power_state.NOSTATE
1550 
1551     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1552     def get_console_topic(self, context):
1553         """Retrieves the console host for a project on this host.
1554 
1555         Currently this is just set in the flags for each compute host.
1556 
1557         """
1558         # TODO(mdragon): perhaps make this variable by console_type?
1559         return 'console.%s' % CONF.console_host
1560 
1561     # TODO(stephenfin): Remove this once we bump the compute API to v6.0
1562     @wrap_exception()
1563     def get_console_pool_info(self, context, console_type):
1564         return self.driver.get_console_pool_info(console_type)
1565 
1566     # TODO(stephenfin): Remove this as it's nova-network only
1567     @wrap_exception()
1568     def refresh_instance_security_rules(self, context, instance):
1569         """Tell the virtualization driver to refresh security rules for
1570         an instance.
1571 
1572         Passes straight through to the virtualization driver.
1573 
1574         Synchronize the call because we may still be in the middle of
1575         creating the instance.
1576         """
1577         pass
1578 
1579     def _await_block_device_map_created(self, context, vol_id):
1580         # TODO(yamahata): creating volume simultaneously
1581         #                 reduces creation time?
1582         # TODO(yamahata): eliminate dumb polling
1583         start = time.time()
1584         retries = CONF.block_device_allocate_retries
1585         # (1) if the configured value is 0, one attempt should be made
1586         # (2) if the configured value is > 0, then the total number attempts
1587         #      is (retries + 1)
1588         attempts = 1
1589         if retries >= 1:
1590             attempts = retries + 1
1591         for attempt in range(1, attempts + 1):
1592             volume = self.volume_api.get(context, vol_id)
1593             volume_status = volume['status']
1594             if volume_status not in ['creating', 'downloading']:
1595                 if volume_status == 'available':
1596                     return attempt
1597                 LOG.warning("Volume id: %(vol_id)s finished being "
1598                             "created but its status is %(vol_status)s.",
1599                             {'vol_id': vol_id,
1600                              'vol_status': volume_status})
1601                 break
1602             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1603         raise exception.VolumeNotCreated(volume_id=vol_id,
1604                                          seconds=int(time.time() - start),
1605                                          attempts=attempt,
1606                                          volume_status=volume_status)
1607 
1608     def _decode_files(self, injected_files):
1609         """Base64 decode the list of files to inject."""
1610         if not injected_files:
1611             return []
1612 
1613         def _decode(f):
1614             path, contents = f
1615             # Py3 raises binascii.Error instead of TypeError as in Py27
1616             try:
1617                 decoded = base64.b64decode(contents)
1618                 return path, decoded
1619             except (TypeError, binascii.Error):
1620                 raise exception.Base64Exception(path=path)
1621 
1622         return [_decode(f) for f in injected_files]
1623 
1624     def _validate_instance_group_policy(self, context, instance,
1625                                         scheduler_hints):
1626         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1627         # However, there is a race condition with the enforcement of
1628         # the policy.  Since more than one instance may be scheduled at the
1629         # same time, it's possible that more than one instance with an
1630         # anti-affinity policy may end up here.  It's also possible that
1631         # multiple instances with an affinity policy could end up on different
1632         # hosts.  This is a validation step to make sure that starting the
1633         # instance here doesn't violate the policy.
1634         group_hint = scheduler_hints.get('group')
1635         if not group_hint:
1636             return
1637 
1638         # The RequestSpec stores scheduler_hints as key=list pairs so we need
1639         # to check the type on the value and pull the single entry out. The
1640         # API request schema validates that the 'group' hint is a single value.
1641         if isinstance(group_hint, list):
1642             group_hint = group_hint[0]
1643 
1644         @utils.synchronized(group_hint)
1645         def _do_validation(context, instance, group_hint):
1646             group = objects.InstanceGroup.get_by_hint(context, group_hint)
1647             if group.policy and 'anti-affinity' == group.policy:
1648                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1649                     context, self.host)
1650                 ins_on_host = set(instances_uuids)
1651                 members = set(group.members)
1652                 # Determine the set of instance group members on this host
1653                 # which are not the instance in question. This is used to
1654                 # determine how many other members from the same anti-affinity
1655                 # group can be on this host.
1656                 members_on_host = ins_on_host & members - set([instance.uuid])
1657                 rules = group.rules
1658                 if rules and 'max_server_per_host' in rules:
1659                     max_server = rules['max_server_per_host']
1660                 else:
1661                     max_server = 1
1662                 if len(members_on_host) >= max_server:
1663                     msg = _("Anti-affinity instance group policy "
1664                             "was violated.")
1665                     raise exception.RescheduledException(
1666                             instance_uuid=instance.uuid,
1667                             reason=msg)
1668             elif group.policy and 'affinity' == group.policy:
1669                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1670                 if group_hosts and self.host not in group_hosts:
1671                     msg = _("Affinity instance group policy was violated.")
1672                     raise exception.RescheduledException(
1673                             instance_uuid=instance.uuid,
1674                             reason=msg)
1675 
1676         if not CONF.workarounds.disable_group_policy_check_upcall:
1677             _do_validation(context, instance, group_hint)
1678 
1679     def _log_original_error(self, exc_info, instance_uuid):
1680         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1681                   exc_info=exc_info)
1682 
1683     @periodic_task.periodic_task
1684     def _check_instance_build_time(self, context):
1685         """Ensure that instances are not stuck in build."""
1686         timeout = CONF.instance_build_timeout
1687         if timeout == 0:
1688             return
1689 
1690         filters = {'vm_state': vm_states.BUILDING,
1691                    'host': self.host}
1692 
1693         building_insts = objects.InstanceList.get_by_filters(context,
1694                            filters, expected_attrs=[], use_slave=True)
1695 
1696         for instance in building_insts:
1697             if timeutils.is_older_than(instance.created_at, timeout):
1698                 self._set_instance_obj_error_state(instance)
1699                 LOG.warning("Instance build timed out. Set to error "
1700                             "state.", instance=instance)
1701 
1702     def _check_instance_exists(self, instance):
1703         """Ensure an instance with the same name is not already present."""
1704         if self.driver.instance_exists(instance):
1705             raise exception.InstanceExists(name=instance.name)
1706 
1707     def _allocate_network_async(self, context, instance, requested_networks,
1708                                 security_groups, is_vpn,
1709                                 resource_provider_mapping):
1710         """Method used to allocate networks in the background.
1711 
1712         Broken out for testing.
1713         """
1714         # First check to see if we're specifically not supposed to allocate
1715         # networks because if so, we can exit early.
1716         if requested_networks and requested_networks.no_allocate:
1717             LOG.debug("Not allocating networking since 'none' was specified.",
1718                       instance=instance)
1719             return network_model.NetworkInfo([])
1720 
1721         LOG.debug("Allocating IP information in the background.",
1722                   instance=instance)
1723         retries = CONF.network_allocate_retries
1724         attempts = retries + 1
1725         retry_time = 1
1726         bind_host_id = self.driver.network_binding_host_id(context, instance)
1727         for attempt in range(1, attempts + 1):
1728             try:
1729                 nwinfo = self.network_api.allocate_for_instance(
1730                         context, instance, vpn=is_vpn,
1731                         requested_networks=requested_networks,
1732                         security_groups=security_groups,
1733                         bind_host_id=bind_host_id,
1734                         resource_provider_mapping=resource_provider_mapping)
1735                 LOG.debug('Instance network_info: |%s|', nwinfo,
1736                           instance=instance)
1737                 instance.system_metadata['network_allocated'] = 'True'
1738                 # NOTE(JoshNang) do not save the instance here, as it can cause
1739                 # races. The caller shares a reference to instance and waits
1740                 # for this async greenthread to finish before calling
1741                 # instance.save().
1742                 return nwinfo
1743             except Exception as e:
1744                 log_info = {'attempt': attempt,
1745                             'attempts': attempts}
1746                 if attempt == attempts:
1747                     LOG.exception('Instance failed network setup '
1748                                   'after %(attempts)d attempt(s)',
1749                                   log_info)
1750                     raise e
1751                 LOG.warning('Instance failed network setup '
1752                             '(attempt %(attempt)d of %(attempts)d)',
1753                             log_info, instance=instance)
1754                 time.sleep(retry_time)
1755                 retry_time *= 2
1756                 if retry_time > 30:
1757                     retry_time = 30
1758         # Not reached.
1759 
1760     def _build_networks_for_instance(self, context, instance,
1761             requested_networks, security_groups, resource_provider_mapping):
1762 
1763         # If we're here from a reschedule the network may already be allocated.
1764         if strutils.bool_from_string(
1765                 instance.system_metadata.get('network_allocated', 'False')):
1766             # NOTE(alex_xu): The network_allocated is True means the network
1767             # resource already allocated at previous scheduling, and the
1768             # network setup is cleanup at previous. After rescheduling, the
1769             # network resource need setup on the new host.
1770             self.network_api.setup_instance_network_on_host(
1771                 context, instance, instance.host)
1772             return self.network_api.get_instance_nw_info(context, instance)
1773 
1774         network_info = self._allocate_network(context, instance,
1775                 requested_networks, security_groups,
1776                 resource_provider_mapping)
1777 
1778         return network_info
1779 
1780     def _allocate_network(self, context, instance, requested_networks,
1781                           security_groups, resource_provider_mapping):
1782         """Start network allocation asynchronously.  Return an instance
1783         of NetworkInfoAsyncWrapper that can be used to retrieve the
1784         allocated networks when the operation has finished.
1785         """
1786         # NOTE(comstud): Since we're allocating networks asynchronously,
1787         # this task state has little meaning, as we won't be in this
1788         # state for very long.
1789         instance.vm_state = vm_states.BUILDING
1790         instance.task_state = task_states.NETWORKING
1791         instance.save(expected_task_state=[None])
1792 
1793         is_vpn = False
1794         return network_model.NetworkInfoAsyncWrapper(
1795                 self._allocate_network_async, context, instance,
1796                 requested_networks, security_groups, is_vpn,
1797                 resource_provider_mapping)
1798 
1799     def _default_root_device_name(self, instance, image_meta, root_bdm):
1800         """Gets a default root device name from the driver.
1801 
1802         :param nova.objects.Instance instance:
1803             The instance for which to get the root device name.
1804         :param nova.objects.ImageMeta image_meta:
1805             The metadata of the image of the instance.
1806         :param nova.objects.BlockDeviceMapping root_bdm:
1807             The description of the root device.
1808         :returns: str -- The default root device name.
1809         :raises: InternalError, TooManyDiskDevices
1810         """
1811         try:
1812             return self.driver.default_root_device_name(instance,
1813                                                         image_meta,
1814                                                         root_bdm)
1815         except NotImplementedError:
1816             return compute_utils.get_next_device_name(instance, [])
1817 
1818     def _default_device_names_for_instance(self, instance,
1819                                            root_device_name,
1820                                            *block_device_lists):
1821         """Default the missing device names in the BDM from the driver.
1822 
1823         :param nova.objects.Instance instance:
1824             The instance for which to get default device names.
1825         :param str root_device_name: The root device name.
1826         :param list block_device_lists: List of block device mappings.
1827         :returns: None
1828         :raises: InternalError, TooManyDiskDevices
1829         """
1830         try:
1831             self.driver.default_device_names_for_instance(instance,
1832                                                           root_device_name,
1833                                                           *block_device_lists)
1834         except NotImplementedError:
1835             compute_utils.default_device_names_for_instance(
1836                 instance, root_device_name, *block_device_lists)
1837 
1838     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1839         """Get the next device name from the driver, based on the BDM.
1840 
1841         :param nova.objects.Instance instance:
1842             The instance whose volume is requesting a device name.
1843         :param nova.objects.BlockDeviceMappingList bdms:
1844             The block device mappings for the instance.
1845         :param nova.objects.BlockDeviceMapping block_device_obj:
1846             A block device mapping containing info about the requested block
1847             device.
1848         :returns: The next device name.
1849         :raises: InternalError, TooManyDiskDevices
1850         """
1851         # NOTE(ndipanov): Copy obj to avoid changing the original
1852         block_device_obj = block_device_obj.obj_clone()
1853         try:
1854             return self.driver.get_device_name_for_instance(
1855                 instance, bdms, block_device_obj)
1856         except NotImplementedError:
1857             return compute_utils.get_device_name_for_instance(
1858                 instance, bdms, block_device_obj.get("device_name"))
1859 
1860     def _default_block_device_names(self, instance, image_meta, block_devices):
1861         """Verify that all the devices have the device_name set. If not,
1862         provide a default name.
1863 
1864         It also ensures that there is a root_device_name and is set to the
1865         first block device in the boot sequence (boot_index=0).
1866         """
1867         root_bdm = block_device.get_root_bdm(block_devices)
1868         if not root_bdm:
1869             return
1870 
1871         # Get the root_device_name from the root BDM or the instance
1872         root_device_name = None
1873         update_root_bdm = False
1874 
1875         if root_bdm.device_name:
1876             root_device_name = root_bdm.device_name
1877             instance.root_device_name = root_device_name
1878         elif instance.root_device_name:
1879             root_device_name = instance.root_device_name
1880             root_bdm.device_name = root_device_name
1881             update_root_bdm = True
1882         else:
1883             root_device_name = self._default_root_device_name(instance,
1884                                                               image_meta,
1885                                                               root_bdm)
1886 
1887             instance.root_device_name = root_device_name
1888             root_bdm.device_name = root_device_name
1889             update_root_bdm = True
1890 
1891         if update_root_bdm:
1892             root_bdm.save()
1893 
1894         ephemerals = list(filter(block_device.new_format_is_ephemeral,
1895                             block_devices))
1896         swap = list(filter(block_device.new_format_is_swap,
1897                       block_devices))
1898         block_device_mapping = list(filter(
1899               driver_block_device.is_block_device_mapping, block_devices))
1900 
1901         self._default_device_names_for_instance(instance,
1902                                                 root_device_name,
1903                                                 ephemerals,
1904                                                 swap,
1905                                                 block_device_mapping)
1906 
1907     def _block_device_info_to_legacy(self, block_device_info):
1908         """Convert BDI to the old format for drivers that need it."""
1909 
1910         if self.use_legacy_block_device_info:
1911             ephemerals = driver_block_device.legacy_block_devices(
1912                 driver.block_device_info_get_ephemerals(block_device_info))
1913             mapping = driver_block_device.legacy_block_devices(
1914                 driver.block_device_info_get_mapping(block_device_info))
1915             swap = block_device_info['swap']
1916             if swap:
1917                 swap = swap.legacy()
1918 
1919             block_device_info.update({
1920                 'ephemerals': ephemerals,
1921                 'swap': swap,
1922                 'block_device_mapping': mapping})
1923 
1924     def _add_missing_dev_names(self, bdms, instance):
1925         for bdm in bdms:
1926             if bdm.device_name is not None:
1927                 continue
1928 
1929             device_name = self._get_device_name_for_instance(instance,
1930                                                              bdms, bdm)
1931             values = {'device_name': device_name}
1932             bdm.update(values)
1933             bdm.save()
1934 
1935     def _prep_block_device(self, context, instance, bdms):
1936         """Set up the block device for an instance with error logging."""
1937         try:
1938             self._add_missing_dev_names(bdms, instance)
1939             block_device_info = driver.get_block_device_info(instance, bdms)
1940             mapping = driver.block_device_info_get_mapping(block_device_info)
1941             driver_block_device.attach_block_devices(
1942                 mapping, context, instance, self.volume_api, self.driver,
1943                 wait_func=self._await_block_device_map_created)
1944 
1945             self._block_device_info_to_legacy(block_device_info)
1946             return block_device_info
1947 
1948         except exception.OverQuota as e:
1949             LOG.warning('Failed to create block device for instance due'
1950                         ' to exceeding volume related resource quota.'
1951                         ' Error: %s', e.message, instance=instance)
1952             raise
1953 
1954         except Exception as ex:
1955             LOG.exception('Instance failed block device setup',
1956                           instance=instance)
1957             # InvalidBDM will eventually result in a BuildAbortException when
1958             # booting from volume, and will be recorded as an instance fault.
1959             # Maintain the original exception message which most likely has
1960             # useful details which the standard InvalidBDM error message lacks.
1961             raise exception.InvalidBDM(six.text_type(ex))
1962 
1963     def _update_instance_after_spawn(self, instance,
1964                                      vm_state=vm_states.ACTIVE):
1965         instance.power_state = self._get_power_state(instance)
1966         instance.vm_state = vm_state
1967         instance.task_state = None
1968         # NOTE(sean-k-mooney): configdrive.update_instance checks
1969         # instance.launched_at to determine if it is the first or
1970         # subsequent spawn of an instance. We need to call update_instance
1971         # first before setting instance.launched_at or instance.config_drive
1972         # will never be set to true based on the value of force_config_drive.
1973         # As a result the config drive will be lost on a hard reboot of the
1974         # instance even when force_config_drive=true. see bug #1835822.
1975         configdrive.update_instance(instance)
1976         instance.launched_at = timeutils.utcnow()
1977 
1978     def _update_scheduler_instance_info(self, context, instance):
1979         """Sends an InstanceList with created or updated Instance objects to
1980         the Scheduler client.
1981 
1982         In the case of init_host, the value passed will already be an
1983         InstanceList. Other calls will send individual Instance objects that
1984         have been created or resized. In this case, we create an InstanceList
1985         object containing that Instance.
1986         """
1987         if not self.send_instance_updates:
1988             return
1989         if isinstance(instance, obj_instance.Instance):
1990             instance = objects.InstanceList(objects=[instance])
1991         context = context.elevated()
1992         self.query_client.update_instance_info(context, self.host,
1993                                                instance)
1994 
1995     def _delete_scheduler_instance_info(self, context, instance_uuid):
1996         """Sends the uuid of the deleted Instance to the Scheduler client."""
1997         if not self.send_instance_updates:
1998             return
1999         context = context.elevated()
2000         self.query_client.delete_instance_info(context, self.host,
2001                                                instance_uuid)
2002 
2003     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
2004     def _sync_scheduler_instance_info(self, context):
2005         if not self.send_instance_updates:
2006             return
2007         context = context.elevated()
2008         instances = objects.InstanceList.get_by_host(context, self.host,
2009                                                      expected_attrs=[],
2010                                                      use_slave=True)
2011         uuids = [instance.uuid for instance in instances]
2012         self.query_client.sync_instance_info(context, self.host, uuids)
2013 
2014     def _notify_about_instance_usage(self, context, instance, event_suffix,
2015                                      network_info=None, extra_usage_info=None,
2016                                      fault=None):
2017         compute_utils.notify_about_instance_usage(
2018             self.notifier, context, instance, event_suffix,
2019             network_info=network_info,
2020             extra_usage_info=extra_usage_info, fault=fault)
2021 
2022     def _deallocate_network(self, context, instance,
2023                             requested_networks=None):
2024         # If we were told not to allocate networks let's save ourselves
2025         # the trouble of calling the network API.
2026         if requested_networks and requested_networks.no_allocate:
2027             LOG.debug("Skipping network deallocation for instance since "
2028                       "networking was not requested.", instance=instance)
2029             return
2030 
2031         LOG.debug('Deallocating network for instance', instance=instance)
2032         with timeutils.StopWatch() as timer:
2033             self.network_api.deallocate_for_instance(
2034                 context, instance, requested_networks=requested_networks)
2035         # nova-network does an rpc call so we're OK tracking time spent here
2036         LOG.info('Took %0.2f seconds to deallocate network for instance.',
2037                  timer.elapsed(), instance=instance)
2038 
2039     def _get_instance_block_device_info(self, context, instance,
2040                                         refresh_conn_info=False,
2041                                         bdms=None):
2042         """Transform block devices to the driver block_device format."""
2043 
2044         if bdms is None:
2045             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2046                     context, instance.uuid)
2047         block_device_info = driver.get_block_device_info(instance, bdms)
2048 
2049         if not refresh_conn_info:
2050             # if the block_device_mapping has no value in connection_info
2051             # (returned as None), don't include in the mapping
2052             block_device_info['block_device_mapping'] = [
2053                 bdm for bdm in driver.block_device_info_get_mapping(
2054                                     block_device_info)
2055                 if bdm.get('connection_info')]
2056         else:
2057             driver_block_device.refresh_conn_infos(
2058                 driver.block_device_info_get_mapping(block_device_info),
2059                 context, instance, self.volume_api, self.driver)
2060 
2061         self._block_device_info_to_legacy(block_device_info)
2062 
2063         return block_device_info
2064 
2065     def _build_failed(self, node):
2066         if CONF.compute.consecutive_build_service_disable_threshold:
2067             # NOTE(danms): Update our counter, but wait for the next
2068             # update_available_resource() periodic to flush it to the DB
2069             self.rt.build_failed(node)
2070 
2071     def _build_succeeded(self, node):
2072         self.rt.build_succeeded(node)
2073 
2074     @wrap_exception()
2075     @reverts_task_state
2076     @wrap_instance_fault
2077     def build_and_run_instance(self, context, instance, image, request_spec,
2078                      filter_properties, admin_password=None,
2079                      injected_files=None, requested_networks=None,
2080                      security_groups=None, block_device_mapping=None,
2081                      node=None, limits=None, host_list=None, accel_uuids=None):
2082 
2083         @utils.synchronized(instance.uuid)
2084         def _locked_do_build_and_run_instance(*args, **kwargs):
2085             # NOTE(danms): We grab the semaphore with the instance uuid
2086             # locked because we could wait in line to build this instance
2087             # for a while and we want to make sure that nothing else tries
2088             # to do anything with this instance while we wait.
2089             with self._build_semaphore:
2090                 try:
2091                     result = self._do_build_and_run_instance(*args, **kwargs)
2092                 except Exception:
2093                     # NOTE(mriedem): This should really only happen if
2094                     # _decode_files in _do_build_and_run_instance fails, and
2095                     # that's before a guest is spawned so it's OK to remove
2096                     # allocations for the instance for this node from Placement
2097                     # below as there is no guest consuming resources anyway.
2098                     # The _decode_files case could be handled more specifically
2099                     # but that's left for another day.
2100                     result = build_results.FAILED
2101                     raise
2102                 finally:
2103                     if result == build_results.FAILED:
2104                         # Remove the allocation records from Placement for the
2105                         # instance if the build failed. The instance.host is
2106                         # likely set to None in _do_build_and_run_instance
2107                         # which means if the user deletes the instance, it
2108                         # will be deleted in the API, not the compute service.
2109                         # Setting the instance.host to None in
2110                         # _do_build_and_run_instance means that the
2111                         # ResourceTracker will no longer consider this instance
2112                         # to be claiming resources against it, so we want to
2113                         # reflect that same thing in Placement.  No need to
2114                         # call this for a reschedule, as the allocations will
2115                         # have already been removed in
2116                         # self._do_build_and_run_instance().
2117                         self.reportclient.delete_allocation_for_instance(
2118                             context, instance.uuid)
2119 
2120                     if result in (build_results.FAILED,
2121                                   build_results.RESCHEDULED):
2122                         self._build_failed(node)
2123                     else:
2124                         self._build_succeeded(node)
2125 
2126         # NOTE(danms): We spawn here to return the RPC worker thread back to
2127         # the pool. Since what follows could take a really long time, we don't
2128         # want to tie up RPC workers.
2129         utils.spawn_n(_locked_do_build_and_run_instance,
2130                       context, instance, image, request_spec,
2131                       filter_properties, admin_password, injected_files,
2132                       requested_networks, security_groups,
2133                       block_device_mapping, node, limits, host_list,
2134                       accel_uuids)
2135 
2136     def _check_device_tagging(self, requested_networks, block_device_mapping):
2137         tagging_requested = False
2138         if requested_networks:
2139             for net in requested_networks:
2140                 if 'tag' in net and net.tag is not None:
2141                     tagging_requested = True
2142                     break
2143         if block_device_mapping and not tagging_requested:
2144             for bdm in block_device_mapping:
2145                 if 'tag' in bdm and bdm.tag is not None:
2146                     tagging_requested = True
2147                     break
2148         if (tagging_requested and
2149                 not self.driver.capabilities.get('supports_device_tagging',
2150                                                  False)):
2151             raise exception.BuildAbortException('Attempt to boot guest with '
2152                                                 'tagged devices on host that '
2153                                                 'does not support tagging.')
2154 
2155     def _check_trusted_certs(self, instance):
2156         if (instance.trusted_certs and
2157                 not self.driver.capabilities.get('supports_trusted_certs',
2158                                                  False)):
2159             raise exception.BuildAbortException(
2160                 'Trusted image certificates provided on host that does not '
2161                 'support certificate validation.')
2162 
2163     @wrap_exception()
2164     @reverts_task_state
2165     @wrap_instance_event(prefix='compute')
2166     @wrap_instance_fault
2167     def _do_build_and_run_instance(self, context, instance, image,
2168             request_spec, filter_properties, admin_password, injected_files,
2169             requested_networks, security_groups, block_device_mapping,
2170             node=None, limits=None, host_list=None, accel_uuids=None):
2171 
2172         try:
2173             LOG.debug('Starting instance...', instance=instance)
2174             instance.vm_state = vm_states.BUILDING
2175             instance.task_state = None
2176             instance.save(expected_task_state=
2177                     (task_states.SCHEDULING, None))
2178         except exception.InstanceNotFound:
2179             msg = 'Instance disappeared before build.'
2180             LOG.debug(msg, instance=instance)
2181             return build_results.FAILED
2182         except exception.UnexpectedTaskStateError as e:
2183             LOG.debug(e.format_message(), instance=instance)
2184             return build_results.FAILED
2185 
2186         # b64 decode the files to inject:
2187         decoded_files = self._decode_files(injected_files)
2188 
2189         if limits is None:
2190             limits = {}
2191 
2192         if node is None:
2193             node = self._get_nodename(instance, refresh=True)
2194 
2195         try:
2196             with timeutils.StopWatch() as timer:
2197                 self._build_and_run_instance(context, instance, image,
2198                         decoded_files, admin_password, requested_networks,
2199                         security_groups, block_device_mapping, node, limits,
2200                         filter_properties, request_spec, accel_uuids)
2201             LOG.info('Took %0.2f seconds to build instance.',
2202                      timer.elapsed(), instance=instance)
2203             return build_results.ACTIVE
2204         except exception.RescheduledException as e:
2205             retry = filter_properties.get('retry')
2206             if not retry:
2207                 # no retry information, do not reschedule.
2208                 LOG.debug("Retry info not present, will not reschedule",
2209                     instance=instance)
2210                 self._cleanup_allocated_networks(context, instance,
2211                     requested_networks)
2212                 self._cleanup_volumes(context, instance,
2213                     block_device_mapping, raise_exc=False)
2214                 compute_utils.add_instance_fault_from_exc(context,
2215                         instance, e, sys.exc_info(),
2216                         fault_message=e.kwargs['reason'])
2217                 self._nil_out_instance_obj_host_and_node(instance)
2218                 self._set_instance_obj_error_state(instance,
2219                                                    clean_task_state=True)
2220                 return build_results.FAILED
2221             LOG.debug(e.format_message(), instance=instance)
2222             # This will be used for logging the exception
2223             retry['exc'] = traceback.format_exception(*sys.exc_info())
2224             # This will be used for setting the instance fault message
2225             retry['exc_reason'] = e.kwargs['reason']
2226 
2227             self._cleanup_allocated_networks(context, instance,
2228                                              requested_networks)
2229 
2230             self._nil_out_instance_obj_host_and_node(instance)
2231             instance.task_state = task_states.SCHEDULING
2232             instance.save()
2233             # The instance will have already claimed resources from this host
2234             # before this build was attempted. Now that it has failed, we need
2235             # to unclaim those resources before casting to the conductor, so
2236             # that if there are alternate hosts available for a retry, it can
2237             # claim resources on that new host for the instance.
2238             self.reportclient.delete_allocation_for_instance(context,
2239                                                              instance.uuid)
2240 
2241             self.compute_task_api.build_instances(context, [instance],
2242                     image, filter_properties, admin_password,
2243                     injected_files, requested_networks, security_groups,
2244                     block_device_mapping, request_spec=request_spec,
2245                     host_lists=[host_list])
2246             return build_results.RESCHEDULED
2247         except (exception.InstanceNotFound,
2248                 exception.UnexpectedDeletingTaskStateError):
2249             msg = 'Instance disappeared during build.'
2250             LOG.debug(msg, instance=instance)
2251             self._cleanup_allocated_networks(context, instance,
2252                     requested_networks)
2253             return build_results.FAILED
2254         except Exception as e:
2255             if isinstance(e, exception.BuildAbortException):
2256                 LOG.error(e.format_message(), instance=instance)
2257             else:
2258                 # Should not reach here.
2259                 LOG.exception('Unexpected build failure, not rescheduling '
2260                               'build.', instance=instance)
2261             self._cleanup_allocated_networks(context, instance,
2262                     requested_networks)
2263             self._cleanup_volumes(context, instance,
2264                     block_device_mapping, raise_exc=False)
2265             compute_utils.add_instance_fault_from_exc(context, instance,
2266                     e, sys.exc_info())
2267             self._nil_out_instance_obj_host_and_node(instance)
2268             self._set_instance_obj_error_state(instance, clean_task_state=True)
2269             return build_results.FAILED
2270 
2271     @staticmethod
2272     def _get_scheduler_hints(filter_properties, request_spec=None):
2273         """Helper method to get scheduler hints.
2274 
2275         This method prefers to get the hints out of the request spec, but that
2276         might not be provided. Conductor will pass request_spec down to the
2277         first compute chosen for a build but older computes will not pass
2278         the request_spec to conductor's build_instances method for a
2279         a reschedule, so if we're on a host via a retry, request_spec may not
2280         be provided so we need to fallback to use the filter_properties
2281         to get scheduler hints.
2282         """
2283         hints = {}
2284         if request_spec is not None and 'scheduler_hints' in request_spec:
2285             hints = request_spec.scheduler_hints
2286         if not hints:
2287             hints = filter_properties.get('scheduler_hints') or {}
2288         return hints
2289 
2290     @staticmethod
2291     def _get_request_group_mapping(request_spec):
2292         """Return request group resource - provider mapping. This is currently
2293         used for Neutron ports that have resource request due to the port
2294         having QoS minimum bandwidth policy rule attached.
2295 
2296         :param request_spec: A RequestSpec object or None
2297         :returns: A dict keyed by RequestGroup requester_id, currently Neutron
2298         port_id, to resource provider UUID that provides resource for that
2299         RequestGroup. Or None if the request_spec was None.
2300         """
2301         if request_spec:
2302             return request_spec.get_request_group_mapping()
2303         else:
2304             return None
2305 
2306     def _build_and_run_instance(self, context, instance, image, injected_files,
2307             admin_password, requested_networks, security_groups,
2308             block_device_mapping, node, limits, filter_properties,
2309             request_spec=None, accel_uuids=None):
2310 
2311         image_name = image.get('name')
2312         self._notify_about_instance_usage(context, instance, 'create.start',
2313                 extra_usage_info={'image_name': image_name})
2314         compute_utils.notify_about_instance_create(
2315             context, instance, self.host,
2316             phase=fields.NotificationPhase.START,
2317             bdms=block_device_mapping)
2318 
2319         # NOTE(mikal): cache the keystone roles associated with the instance
2320         # at boot time for later reference
2321         instance.system_metadata.update(
2322             {'boot_roles': ','.join(context.roles)})
2323 
2324         self._check_device_tagging(requested_networks, block_device_mapping)
2325         self._check_trusted_certs(instance)
2326 
2327         provider_mapping = self._get_request_group_mapping(request_spec)
2328 
2329         if provider_mapping:
2330             try:
2331                 compute_utils\
2332                     .update_pci_request_spec_with_allocated_interface_name(
2333                         context, self.reportclient, instance, provider_mapping)
2334             except (exception.AmbiguousResourceProviderForPCIRequest,
2335                     exception.UnexpectedResourceProviderNameForPCIRequest
2336                     ) as e:
2337                 raise exception.BuildAbortException(
2338                     reason=six.text_type(e), instance_uuid=instance.uuid)
2339 
2340         # TODO(Luyao) cut over to get_allocs_for_consumer
2341         allocs = self.reportclient.get_allocations_for_consumer(
2342                 context, instance.uuid)
2343 
2344         try:
2345             scheduler_hints = self._get_scheduler_hints(filter_properties,
2346                                                         request_spec)
2347             with self.rt.instance_claim(context, instance, node, allocs,
2348                                         limits):
2349                 # NOTE(russellb) It's important that this validation be done
2350                 # *after* the resource tracker instance claim, as that is where
2351                 # the host is set on the instance.
2352                 self._validate_instance_group_policy(context, instance,
2353                                                      scheduler_hints)
2354                 image_meta = objects.ImageMeta.from_dict(image)
2355 
2356                 with self._build_resources(context, instance,
2357                         requested_networks, security_groups, image_meta,
2358                         block_device_mapping, provider_mapping,
2359                         accel_uuids) as resources:
2360                     instance.vm_state = vm_states.BUILDING
2361                     instance.task_state = task_states.SPAWNING
2362                     # NOTE(JoshNang) This also saves the changes to the
2363                     # instance from _allocate_network_async, as they aren't
2364                     # saved in that function to prevent races.
2365                     instance.save(expected_task_state=
2366                             task_states.BLOCK_DEVICE_MAPPING)
2367                     block_device_info = resources['block_device_info']
2368                     network_info = resources['network_info']
2369                     accel_info = resources['accel_info']
2370                     LOG.debug('Start spawning the instance on the hypervisor.',
2371                               instance=instance)
2372                     with timeutils.StopWatch() as timer:
2373                         self.driver.spawn(context, instance, image_meta,
2374                                           injected_files, admin_password,
2375                                           allocs, network_info=network_info,
2376                                           block_device_info=block_device_info,
2377                                           accel_info=accel_info)
2378                     LOG.info('Took %0.2f seconds to spawn the instance on '
2379                              'the hypervisor.', timer.elapsed(),
2380                              instance=instance)
2381         except (exception.InstanceNotFound,
2382                 exception.UnexpectedDeletingTaskStateError) as e:
2383             with excutils.save_and_reraise_exception():
2384                 self._notify_about_instance_usage(context, instance,
2385                     'create.error', fault=e)
2386                 compute_utils.notify_about_instance_create(
2387                     context, instance, self.host,
2388                     phase=fields.NotificationPhase.ERROR, exception=e,
2389                     bdms=block_device_mapping)
2390         except exception.ComputeResourcesUnavailable as e:
2391             LOG.debug(e.format_message(), instance=instance)
2392             self._notify_about_instance_usage(context, instance,
2393                     'create.error', fault=e)
2394             compute_utils.notify_about_instance_create(
2395                     context, instance, self.host,
2396                     phase=fields.NotificationPhase.ERROR, exception=e,
2397                     bdms=block_device_mapping)
2398             raise exception.RescheduledException(
2399                     instance_uuid=instance.uuid, reason=e.format_message())
2400         except exception.BuildAbortException as e:
2401             with excutils.save_and_reraise_exception():
2402                 LOG.debug(e.format_message(), instance=instance)
2403                 self._notify_about_instance_usage(context, instance,
2404                     'create.error', fault=e)
2405                 compute_utils.notify_about_instance_create(
2406                     context, instance, self.host,
2407                     phase=fields.NotificationPhase.ERROR, exception=e,
2408                     bdms=block_device_mapping)
2409         except exception.NoMoreFixedIps as e:
2410             LOG.warning('No more fixed IP to be allocated',
2411                         instance=instance)
2412             self._notify_about_instance_usage(context, instance,
2413                     'create.error', fault=e)
2414             compute_utils.notify_about_instance_create(
2415                     context, instance, self.host,
2416                     phase=fields.NotificationPhase.ERROR, exception=e,
2417                     bdms=block_device_mapping)
2418             msg = _('Failed to allocate the network(s) with error %s, '
2419                     'not rescheduling.') % e.format_message()
2420             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2421                     reason=msg)
2422         except (exception.ExternalNetworkAttachForbidden,
2423                 exception.VirtualInterfaceCreateException,
2424                 exception.VirtualInterfaceMacAddressException,
2425                 exception.FixedIpInvalidOnHost,
2426                 exception.UnableToAutoAllocateNetwork,
2427                 exception.NetworksWithQoSPolicyNotSupported) as e:
2428             LOG.exception('Failed to allocate network(s)',
2429                           instance=instance)
2430             self._notify_about_instance_usage(context, instance,
2431                     'create.error', fault=e)
2432             compute_utils.notify_about_instance_create(
2433                     context, instance, self.host,
2434                     phase=fields.NotificationPhase.ERROR, exception=e,
2435                     bdms=block_device_mapping)
2436             msg = _('Failed to allocate the network(s), not rescheduling.')
2437             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2438                     reason=msg)
2439         except (exception.FlavorDiskTooSmall,
2440                 exception.FlavorMemoryTooSmall,
2441                 exception.ImageNotActive,
2442                 exception.ImageUnacceptable,
2443                 exception.InvalidDiskInfo,
2444                 exception.InvalidDiskFormat,
2445                 cursive_exception.SignatureVerificationError,
2446                 exception.CertificateValidationFailed,
2447                 exception.VolumeEncryptionNotSupported,
2448                 exception.InvalidInput,
2449                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2450                 # in the API during server create and rebuild.
2451                 exception.RequestedVRamTooHigh) as e:
2452             self._notify_about_instance_usage(context, instance,
2453                     'create.error', fault=e)
2454             compute_utils.notify_about_instance_create(
2455                     context, instance, self.host,
2456                     phase=fields.NotificationPhase.ERROR, exception=e,
2457                     bdms=block_device_mapping)
2458             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2459                     reason=e.format_message())
2460         except Exception as e:
2461             LOG.exception('Failed to build and run instance',
2462                           instance=instance)
2463             self._notify_about_instance_usage(context, instance,
2464                     'create.error', fault=e)
2465             compute_utils.notify_about_instance_create(
2466                     context, instance, self.host,
2467                     phase=fields.NotificationPhase.ERROR, exception=e,
2468                     bdms=block_device_mapping)
2469             raise exception.RescheduledException(
2470                     instance_uuid=instance.uuid, reason=six.text_type(e))
2471 
2472         # NOTE(alaski): This is only useful during reschedules, remove it now.
2473         instance.system_metadata.pop('network_allocated', None)
2474 
2475         # If CONF.default_access_ip_network_name is set, grab the
2476         # corresponding network and set the access ip values accordingly.
2477         network_name = CONF.default_access_ip_network_name
2478         if (network_name and not instance.access_ip_v4 and
2479                 not instance.access_ip_v6):
2480             # Note that when there are multiple ips to choose from, an
2481             # arbitrary one will be chosen.
2482             for vif in network_info:
2483                 if vif['network']['label'] == network_name:
2484                     for ip in vif.fixed_ips():
2485                         if not instance.access_ip_v4 and ip['version'] == 4:
2486                             instance.access_ip_v4 = ip['address']
2487                         if not instance.access_ip_v6 and ip['version'] == 6:
2488                             instance.access_ip_v6 = ip['address']
2489                     break
2490 
2491         self._update_instance_after_spawn(instance)
2492 
2493         try:
2494             instance.save(expected_task_state=task_states.SPAWNING)
2495         except (exception.InstanceNotFound,
2496                 exception.UnexpectedDeletingTaskStateError) as e:
2497             with excutils.save_and_reraise_exception():
2498                 self._notify_about_instance_usage(context, instance,
2499                     'create.error', fault=e)
2500                 compute_utils.notify_about_instance_create(
2501                     context, instance, self.host,
2502                     phase=fields.NotificationPhase.ERROR, exception=e,
2503                     bdms=block_device_mapping)
2504 
2505         self._update_scheduler_instance_info(context, instance)
2506         self._notify_about_instance_usage(context, instance, 'create.end',
2507                 extra_usage_info={'message': _('Success')},
2508                 network_info=network_info)
2509         compute_utils.notify_about_instance_create(context, instance,
2510                 self.host, phase=fields.NotificationPhase.END,
2511                 bdms=block_device_mapping)
2512 
2513     def _build_resources_cleanup(self, instance, network_info):
2514         # Make sure the async call finishes
2515         if network_info is not None:
2516             network_info.wait(do_raise=False)
2517             self.driver.clean_networks_preparation(instance,
2518                                                    network_info)
2519         self.driver.failed_spawn_cleanup(instance)
2520 
2521     @contextlib.contextmanager
2522     def _build_resources(self, context, instance, requested_networks,
2523                          security_groups, image_meta, block_device_mapping,
2524                          resource_provider_mapping, accel_uuids):
2525         resources = {}
2526         network_info = None
2527         try:
2528             LOG.debug('Start building networks asynchronously for instance.',
2529                       instance=instance)
2530             network_info = self._build_networks_for_instance(context, instance,
2531                     requested_networks, security_groups,
2532                     resource_provider_mapping)
2533             resources['network_info'] = network_info
2534         except (exception.InstanceNotFound,
2535                 exception.UnexpectedDeletingTaskStateError):
2536             raise
2537         except exception.UnexpectedTaskStateError as e:
2538             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2539                     reason=e.format_message())
2540         except Exception:
2541             # Because this allocation is async any failures are likely to occur
2542             # when the driver accesses network_info during spawn().
2543             LOG.exception('Failed to allocate network(s)',
2544                           instance=instance)
2545             msg = _('Failed to allocate the network(s), not rescheduling.')
2546             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2547                     reason=msg)
2548 
2549         try:
2550             # Perform any driver preparation work for the driver.
2551             self.driver.prepare_for_spawn(instance)
2552 
2553             # Depending on a virt driver, some network configuration is
2554             # necessary before preparing block devices.
2555             self.driver.prepare_networks_before_block_device_mapping(
2556                 instance, network_info)
2557 
2558             # Verify that all the BDMs have a device_name set and assign a
2559             # default to the ones missing it with the help of the driver.
2560             self._default_block_device_names(instance, image_meta,
2561                                              block_device_mapping)
2562 
2563             LOG.debug('Start building block device mappings for instance.',
2564                       instance=instance)
2565             instance.vm_state = vm_states.BUILDING
2566             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2567             instance.save()
2568 
2569             block_device_info = self._prep_block_device(context, instance,
2570                     block_device_mapping)
2571             resources['block_device_info'] = block_device_info
2572         except (exception.InstanceNotFound,
2573                 exception.UnexpectedDeletingTaskStateError):
2574             with excutils.save_and_reraise_exception():
2575                 self._build_resources_cleanup(instance, network_info)
2576         except (exception.UnexpectedTaskStateError,
2577                 exception.OverQuota, exception.InvalidBDM) as e:
2578             self._build_resources_cleanup(instance, network_info)
2579             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2580                     reason=e.format_message())
2581         except Exception:
2582             LOG.exception('Failure prepping block device',
2583                           instance=instance)
2584             self._build_resources_cleanup(instance, network_info)
2585             msg = _('Failure prepping block device.')
2586             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2587                     reason=msg)
2588 
2589         arqs = []
2590         if instance.flavor.extra_specs.get('accel:device_profile'):
2591             try:
2592                 arqs = self._get_bound_arq_resources(
2593                     context, instance, accel_uuids)
2594             except (Exception, eventlet.timeout.Timeout) as exc:
2595                 LOG.exception(exc)
2596                 self._build_resources_cleanup(instance, network_info)
2597                 compute_utils.delete_arqs_if_needed(context, instance)
2598                 msg = _('Failure getting accelerator requests.')
2599                 raise exception.BuildAbortException(
2600                     reason=msg, instance_uuid=instance.uuid)
2601 
2602         resources['accel_info'] = arqs
2603         try:
2604             yield resources
2605         except Exception as exc:
2606             with excutils.save_and_reraise_exception() as ctxt:
2607                 if not isinstance(exc, (
2608                         exception.InstanceNotFound,
2609                         exception.UnexpectedDeletingTaskStateError)):
2610                     LOG.exception('Instance failed to spawn',
2611                                   instance=instance)
2612                 # Make sure the async call finishes
2613                 if network_info is not None:
2614                     network_info.wait(do_raise=False)
2615                 # if network_info is empty we're likely here because of
2616                 # network allocation failure. Since nothing can be reused on
2617                 # rescheduling it's better to deallocate network to eliminate
2618                 # the chance of orphaned ports in neutron
2619                 deallocate_networks = False if network_info else True
2620                 try:
2621                     self._shutdown_instance(context, instance,
2622                             block_device_mapping, requested_networks,
2623                             try_deallocate_networks=deallocate_networks)
2624                 except Exception as exc2:
2625                     ctxt.reraise = False
2626                     LOG.warning('Could not clean up failed build,'
2627                                 ' not rescheduling. Error: %s',
2628                                 six.text_type(exc2))
2629                     raise exception.BuildAbortException(
2630                             instance_uuid=instance.uuid,
2631                             reason=six.text_type(exc))
2632                 finally:
2633                     # Call Cyborg to delete accelerator requests
2634                     compute_utils.delete_arqs_if_needed(context, instance)
2635 
2636     def _get_bound_arq_resources(self, context, instance, arq_uuids):
2637         """Get bound accelerator requests.
2638 
2639         The ARQ binding was kicked off in the conductor as an async
2640         operation. Here we wait for the notification from Cyborg.
2641 
2642         If the notification arrived before this point, which can happen
2643         in many/most cases (see [1]), it will be lost. To handle that,
2644         we use exit_wait_early.
2645         [1] https://review.opendev.org/#/c/631244/46/nova/compute/
2646             manager.py@2627
2647 
2648         :param instance: instance object
2649         :param arq_uuids: List of accelerator request (ARQ) UUIDs.
2650         :returns: List of ARQs for which bindings have completed,
2651                   successfully or otherwise
2652         """
2653 
2654         cyclient = cyborg.get_client(context)
2655         if arq_uuids is None:
2656             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2657             arq_uuids = [arq['uuid'] for arq in arqs]
2658         events = [('accelerator-request-bound', arq_uuid)
2659                   for arq_uuid in arq_uuids]
2660 
2661         timeout = CONF.arq_binding_timeout
2662         with self.virtapi.wait_for_instance_event(
2663                 instance, events, deadline=timeout):
2664             resolved_arqs = cyclient.get_arqs_for_instance(
2665                     instance.uuid, only_resolved=True)
2666             # Events for these resolved ARQs may have already arrived.
2667             # Such 'early' events need to be ignored.
2668             early_events = [('accelerator-request-bound', arq['uuid'])
2669                              for arq in resolved_arqs]
2670             if early_events:
2671                 self.virtapi.exit_wait_early(early_events)
2672 
2673         # Since a timeout in wait_for_instance_event will raise, we get
2674         # here only if all binding events have been received.
2675         resolved_uuids = [arq['uuid'] for arq in resolved_arqs]
2676         if sorted(resolved_uuids) != sorted(arq_uuids):
2677             # Query Cyborg to get all.
2678             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2679         else:
2680             arqs = resolved_arqs
2681         return arqs
2682 
2683     def _cleanup_allocated_networks(self, context, instance,
2684             requested_networks):
2685         """Cleanup networks allocated for instance.
2686 
2687         :param context: nova request context
2688         :param instance: nova.objects.instance.Instance object
2689         :param requested_networks: nova.objects.NetworkRequestList
2690         """
2691         LOG.debug('Unplugging VIFs for instance', instance=instance)
2692 
2693         network_info = instance.get_network_info()
2694 
2695         # NOTE(stephenfin) to avoid nova destroying the instance without
2696         # unplugging the interface, refresh network_info if it is empty.
2697         if not network_info:
2698             try:
2699                 network_info = self.network_api.get_instance_nw_info(
2700                     context, instance,
2701                 )
2702             except Exception as exc:
2703                 LOG.warning(
2704                     'Failed to update network info cache when cleaning up '
2705                     'allocated networks. Stale VIFs may be left on this host.'
2706                     'Error: %s', six.text_type(exc)
2707                 )
2708                 return
2709 
2710         try:
2711             self.driver.unplug_vifs(instance, network_info)
2712         except NotImplementedError:
2713             # This is an optional method so ignore things if it doesn't exist
2714             LOG.debug(
2715                 'Virt driver does not provide unplug_vifs method, so it '
2716                 'is not possible determine if VIFs should be unplugged.'
2717             )
2718         except exception.NovaException as exc:
2719             # It's possible that the instance never got as far as plugging
2720             # VIFs, in which case we would see an exception which can be
2721             # mostly ignored
2722             LOG.warning(
2723                 'Cleaning up VIFs failed for instance. Error: %s',
2724                 six.text_type(exc), instance=instance,
2725             )
2726         else:
2727             LOG.debug('Unplugged VIFs for instance', instance=instance)
2728 
2729         try:
2730             self._deallocate_network(context, instance, requested_networks)
2731         except Exception:
2732             LOG.exception('Failed to deallocate networks', instance=instance)
2733             return
2734 
2735         instance.system_metadata['network_allocated'] = 'False'
2736         try:
2737             instance.save()
2738         except exception.InstanceNotFound:
2739             # NOTE(alaski): It's possible that we're cleaning up the networks
2740             # because the instance was deleted.  If that's the case then this
2741             # exception will be raised by instance.save()
2742             pass
2743 
2744     def _try_deallocate_network(self, context, instance,
2745                                 requested_networks=None):
2746 
2747         # During auto-scale cleanup, we could be deleting a large number
2748         # of servers at the same time and overloading parts of the system,
2749         # so we retry a few times in case of connection failures to the
2750         # networking service.
2751         @loopingcall.RetryDecorator(
2752             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2753             exceptions=(keystone_exception.connection.ConnectFailure,))
2754         def _deallocate_network_with_retries():
2755             try:
2756                 self._deallocate_network(
2757                     context, instance, requested_networks)
2758             except keystone_exception.connection.ConnectFailure as e:
2759                 # Provide a warning that something is amiss.
2760                 with excutils.save_and_reraise_exception():
2761                     LOG.warning('Failed to deallocate network for instance; '
2762                                 'retrying. Error: %s', six.text_type(e),
2763                                 instance=instance)
2764 
2765         try:
2766             # tear down allocated network structure
2767             _deallocate_network_with_retries()
2768         except Exception as ex:
2769             with excutils.save_and_reraise_exception():
2770                 LOG.error('Failed to deallocate network for instance. '
2771                           'Error: %s', ex, instance=instance)
2772                 self._set_instance_obj_error_state(instance)
2773 
2774     def _get_power_off_values(self, instance, clean_shutdown):
2775         """Get the timing configuration for powering down this instance."""
2776         if clean_shutdown:
2777             timeout = compute_utils.get_value_from_system_metadata(instance,
2778                           key='image_os_shutdown_timeout', type=int,
2779                           default=CONF.shutdown_timeout)
2780             retry_interval = CONF.compute.shutdown_retry_interval
2781         else:
2782             timeout = 0
2783             retry_interval = 0
2784 
2785         return timeout, retry_interval
2786 
2787     def _power_off_instance(self, instance, clean_shutdown=True):
2788         """Power off an instance on this host."""
2789         timeout, retry_interval = self._get_power_off_values(
2790             instance, clean_shutdown)
2791         self.driver.power_off(instance, timeout, retry_interval)
2792 
2793     def _shutdown_instance(self, context, instance,
2794                            bdms, requested_networks=None, notify=True,
2795                            try_deallocate_networks=True):
2796         """Shutdown an instance on this host.
2797 
2798         :param:context: security context
2799         :param:instance: a nova.objects.Instance object
2800         :param:bdms: the block devices for the instance to be torn
2801                      down
2802         :param:requested_networks: the networks on which the instance
2803                                    has ports
2804         :param:notify: true if a final usage notification should be
2805                        emitted
2806         :param:try_deallocate_networks: false if we should avoid
2807                                         trying to teardown networking
2808         """
2809         context = context.elevated()
2810         LOG.info('Terminating instance', instance=instance)
2811 
2812         if notify:
2813             self._notify_about_instance_usage(context, instance,
2814                                               "shutdown.start")
2815             compute_utils.notify_about_instance_action(context, instance,
2816                     self.host, action=fields.NotificationAction.SHUTDOWN,
2817                     phase=fields.NotificationPhase.START, bdms=bdms)
2818 
2819         network_info = instance.get_network_info()
2820 
2821         # NOTE(arnaudmorin) to avoid nova destroying the instance without
2822         # unplugging the interface, refresh network_info if it is empty.
2823         if not network_info:
2824             network_info = self.network_api.get_instance_nw_info(
2825                 context, instance)
2826 
2827         # NOTE(vish) get bdms before destroying the instance
2828         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2829         block_device_info = self._get_instance_block_device_info(
2830             context, instance, bdms=bdms)
2831 
2832         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2833         #                want to keep ip allocated for certain failures
2834         try:
2835             LOG.debug('Start destroying the instance on the hypervisor.',
2836                       instance=instance)
2837             with timeutils.StopWatch() as timer:
2838                 self.driver.destroy(context, instance, network_info,
2839                                     block_device_info)
2840             LOG.info('Took %0.2f seconds to destroy the instance on the '
2841                      'hypervisor.', timer.elapsed(), instance=instance)
2842         except exception.InstancePowerOffFailure:
2843             # if the instance can't power off, don't release the ip
2844             with excutils.save_and_reraise_exception():
2845                 pass
2846         except Exception:
2847             with excutils.save_and_reraise_exception():
2848                 # deallocate ip and fail without proceeding to
2849                 # volume api calls, preserving current behavior
2850                 if try_deallocate_networks:
2851                     self._try_deallocate_network(context, instance,
2852                                                  requested_networks)
2853 
2854         if try_deallocate_networks:
2855             self._try_deallocate_network(context, instance, requested_networks)
2856 
2857         timer.restart()
2858         for bdm in vol_bdms:
2859             try:
2860                 if bdm.attachment_id:
2861                     self.volume_api.attachment_delete(context,
2862                                                       bdm.attachment_id)
2863                 else:
2864                     # NOTE(vish): actual driver detach done in driver.destroy,
2865                     #             so just tell cinder that we are done with it.
2866                     connector = self.driver.get_volume_connector(instance)
2867                     self.volume_api.terminate_connection(context,
2868                                                          bdm.volume_id,
2869                                                          connector)
2870                     self.volume_api.detach(context, bdm.volume_id,
2871                                            instance.uuid)
2872 
2873             except exception.VolumeAttachmentNotFound as exc:
2874                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2875                           instance=instance)
2876             except exception.DiskNotFound as exc:
2877                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2878                           instance=instance)
2879             except exception.VolumeNotFound as exc:
2880                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2881                           instance=instance)
2882             except (cinder_exception.EndpointNotFound,
2883                     keystone_exception.EndpointNotFound) as exc:
2884                 LOG.warning('Ignoring EndpointNotFound for '
2885                             'volume %(volume_id)s: %(exc)s',
2886                             {'exc': exc, 'volume_id': bdm.volume_id},
2887                             instance=instance)
2888             except cinder_exception.ClientException as exc:
2889                 LOG.warning('Ignoring unknown cinder exception for '
2890                             'volume %(volume_id)s: %(exc)s',
2891                             {'exc': exc, 'volume_id': bdm.volume_id},
2892                             instance=instance)
2893             except Exception as exc:
2894                 LOG.warning('Ignoring unknown exception for '
2895                             'volume %(volume_id)s: %(exc)s',
2896                             {'exc': exc, 'volume_id': bdm.volume_id},
2897                             instance=instance)
2898         if vol_bdms:
2899             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2900                      'for instance.',
2901                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2902                      instance=instance)
2903 
2904         if notify:
2905             self._notify_about_instance_usage(context, instance,
2906                                               "shutdown.end")
2907             compute_utils.notify_about_instance_action(context, instance,
2908                     self.host, action=fields.NotificationAction.SHUTDOWN,
2909                     phase=fields.NotificationPhase.END, bdms=bdms)
2910 
2911     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2912                          detach=True):
2913         original_exception = None
2914         for bdm in bdms:
2915             if detach and bdm.volume_id:
2916                 try:
2917                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2918                               instance_uuid=instance.uuid)
2919                     destroy = bdm.delete_on_termination
2920                     self._detach_volume(context, bdm, instance,
2921                                         destroy_bdm=destroy)
2922                 except Exception as exc:
2923                     original_exception = exc
2924                     LOG.warning('Failed to detach volume: %(volume_id)s '
2925                                 'due to %(exc)s',
2926                                 {'volume_id': bdm.volume_id, 'exc': exc})
2927 
2928             if bdm.volume_id and bdm.delete_on_termination:
2929                 try:
2930                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2931                               instance_uuid=instance.uuid)
2932                     self.volume_api.delete(context, bdm.volume_id)
2933                 except Exception as exc:
2934                     original_exception = exc
2935                     LOG.warning('Failed to delete volume: %(volume_id)s '
2936                                 'due to %(exc)s',
2937                                 {'volume_id': bdm.volume_id, 'exc': exc})
2938         if original_exception is not None and raise_exc:
2939             raise original_exception
2940 
2941     def _delete_instance(self, context, instance, bdms):
2942         """Delete an instance on this host.
2943 
2944         :param context: nova request context
2945         :param instance: nova.objects.instance.Instance object
2946         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2947         """
2948         events = self.instance_events.clear_events_for_instance(instance)
2949         if events:
2950             LOG.debug('Events pending at deletion: %(events)s',
2951                       {'events': ','.join(events.keys())},
2952                       instance=instance)
2953         self._notify_about_instance_usage(context, instance,
2954                                           "delete.start")
2955         compute_utils.notify_about_instance_action(context, instance,
2956                 self.host, action=fields.NotificationAction.DELETE,
2957                 phase=fields.NotificationPhase.START, bdms=bdms)
2958 
2959         self._shutdown_instance(context, instance, bdms)
2960 
2961         # NOTE(vish): We have already deleted the instance, so we have
2962         #             to ignore problems cleaning up the volumes. It
2963         #             would be nice to let the user know somehow that
2964         #             the volume deletion failed, but it is not
2965         #             acceptable to have an instance that can not be
2966         #             deleted. Perhaps this could be reworked in the
2967         #             future to set an instance fault the first time
2968         #             and to only ignore the failure if the instance
2969         #             is already in ERROR.
2970 
2971         # NOTE(ameeda): The volumes already detached during the above
2972         #               _shutdown_instance() call and this is why
2973         #               detach is not requested from _cleanup_volumes()
2974         #               in this case
2975 
2976         self._cleanup_volumes(context, instance, bdms,
2977                 raise_exc=False, detach=False)
2978         # Delete Cyborg ARQs if the instance has a device profile.
2979         compute_utils.delete_arqs_if_needed(context, instance)
2980         # if a delete task succeeded, always update vm state and task
2981         # state without expecting task state to be DELETING
2982         instance.vm_state = vm_states.DELETED
2983         instance.task_state = None
2984         instance.power_state = power_state.NOSTATE
2985         instance.terminated_at = timeutils.utcnow()
2986         instance.save()
2987 
2988         self._complete_deletion(context, instance)
2989         # only destroy the instance in the db if the _complete_deletion
2990         # doesn't raise and therefore allocation is successfully
2991         # deleted in placement
2992         instance.destroy()
2993 
2994         self._notify_about_instance_usage(context, instance, "delete.end")
2995         compute_utils.notify_about_instance_action(context, instance,
2996                 self.host, action=fields.NotificationAction.DELETE,
2997                 phase=fields.NotificationPhase.END, bdms=bdms)
2998 
2999     @wrap_exception()
3000     @reverts_task_state
3001     @wrap_instance_event(prefix='compute')
3002     @wrap_instance_fault
3003     def terminate_instance(self, context, instance, bdms):
3004         """Terminate an instance on this host."""
3005         @utils.synchronized(instance.uuid)
3006         def do_terminate_instance(instance, bdms):
3007             # NOTE(mriedem): If we are deleting the instance while it was
3008             # booting from volume, we could be racing with a database update of
3009             # the BDM volume_id. Since the compute API passes the BDMs over RPC
3010             # to compute here, the BDMs may be stale at this point. So check
3011             # for any volume BDMs that don't have volume_id set and if we
3012             # detect that, we need to refresh the BDM list before proceeding.
3013             # TODO(mriedem): Move this into _delete_instance and make the bdms
3014             # parameter optional.
3015             for bdm in list(bdms):
3016                 if bdm.is_volume and not bdm.volume_id:
3017                     LOG.debug('There are potentially stale BDMs during '
3018                               'delete, refreshing the BlockDeviceMappingList.',
3019                               instance=instance)
3020                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3021                         context, instance.uuid)
3022                     break
3023             try:
3024                 self._delete_instance(context, instance, bdms)
3025             except exception.InstanceNotFound:
3026                 LOG.info("Instance disappeared during terminate",
3027                          instance=instance)
3028             except Exception:
3029                 # As we're trying to delete always go to Error if something
3030                 # goes wrong that _delete_instance can't handle.
3031                 with excutils.save_and_reraise_exception():
3032                     LOG.exception('Setting instance vm_state to ERROR',
3033                                   instance=instance)
3034                     self._set_instance_obj_error_state(instance)
3035 
3036         do_terminate_instance(instance, bdms)
3037 
3038     # NOTE(johannes): This is probably better named power_off_instance
3039     # so it matches the driver method, but because of other issues, we
3040     # can't use that name in grizzly.
3041     @wrap_exception()
3042     @reverts_task_state
3043     @wrap_instance_event(prefix='compute')
3044     @wrap_instance_fault
3045     def stop_instance(self, context, instance, clean_shutdown):
3046         """Stopping an instance on this host."""
3047 
3048         @utils.synchronized(instance.uuid)
3049         def do_stop_instance():
3050             current_power_state = self._get_power_state(instance)
3051             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
3052                       'current task_state: %(task_state)s, current DB '
3053                       'power_state: %(db_power_state)s, current VM '
3054                       'power_state: %(current_power_state)s',
3055                       {'vm_state': instance.vm_state,
3056                        'task_state': instance.task_state,
3057                        'db_power_state': instance.power_state,
3058                        'current_power_state': current_power_state},
3059                       instance_uuid=instance.uuid)
3060 
3061             # NOTE(mriedem): If the instance is already powered off, we are
3062             # possibly tearing down and racing with other operations, so we can
3063             # expect the task_state to be None if something else updates the
3064             # instance and we're not locking it.
3065             expected_task_state = [task_states.POWERING_OFF]
3066             # The list of power states is from _sync_instance_power_state.
3067             if current_power_state in (power_state.NOSTATE,
3068                                        power_state.SHUTDOWN,
3069                                        power_state.CRASHED):
3070                 LOG.info('Instance is already powered off in the '
3071                          'hypervisor when stop is called.',
3072                          instance=instance)
3073                 expected_task_state.append(None)
3074 
3075             self._notify_about_instance_usage(context, instance,
3076                                               "power_off.start")
3077 
3078             compute_utils.notify_about_instance_action(context, instance,
3079                         self.host, action=fields.NotificationAction.POWER_OFF,
3080                         phase=fields.NotificationPhase.START)
3081 
3082             self._power_off_instance(instance, clean_shutdown)
3083             instance.power_state = self._get_power_state(instance)
3084             instance.vm_state = vm_states.STOPPED
3085             instance.task_state = None
3086             instance.save(expected_task_state=expected_task_state)
3087             self._notify_about_instance_usage(context, instance,
3088                                               "power_off.end")
3089 
3090             compute_utils.notify_about_instance_action(context, instance,
3091                         self.host, action=fields.NotificationAction.POWER_OFF,
3092                         phase=fields.NotificationPhase.END)
3093 
3094         do_stop_instance()
3095 
3096     def _power_on(self, context, instance):
3097         network_info = self.network_api.get_instance_nw_info(context, instance)
3098         block_device_info = self._get_instance_block_device_info(context,
3099                                                                  instance)
3100         accel_info = self._get_accel_info(context, instance)
3101         self.driver.power_on(context, instance,
3102                              network_info,
3103                              block_device_info, accel_info)
3104 
3105     def _delete_snapshot_of_shelved_instance(self, context, instance,
3106                                              snapshot_id):
3107         """Delete snapshot of shelved instance."""
3108         try:
3109             self.image_api.delete(context, snapshot_id)
3110         except (exception.ImageNotFound,
3111                 exception.ImageNotAuthorized) as exc:
3112             LOG.warning("Failed to delete snapshot "
3113                         "from shelved instance (%s).",
3114                         exc.format_message(), instance=instance)
3115         except Exception:
3116             LOG.exception("Something wrong happened when trying to "
3117                           "delete snapshot from shelved instance.",
3118                           instance=instance)
3119 
3120     # NOTE(johannes): This is probably better named power_on_instance
3121     # so it matches the driver method, but because of other issues, we
3122     # can't use that name in grizzly.
3123     @wrap_exception()
3124     @reverts_task_state
3125     @wrap_instance_event(prefix='compute')
3126     @wrap_instance_fault
3127     def start_instance(self, context, instance):
3128         """Starting an instance on this host."""
3129         self._notify_about_instance_usage(context, instance, "power_on.start")
3130         compute_utils.notify_about_instance_action(context, instance,
3131             self.host, action=fields.NotificationAction.POWER_ON,
3132             phase=fields.NotificationPhase.START)
3133         self._power_on(context, instance)
3134         instance.power_state = self._get_power_state(instance)
3135         instance.vm_state = vm_states.ACTIVE
3136         instance.task_state = None
3137 
3138         # Delete an image(VM snapshot) for a shelved instance
3139         snapshot_id = instance.system_metadata.get('shelved_image_id')
3140         if snapshot_id:
3141             self._delete_snapshot_of_shelved_instance(context, instance,
3142                                                       snapshot_id)
3143 
3144         # Delete system_metadata for a shelved instance
3145         compute_utils.remove_shelved_keys_from_system_metadata(instance)
3146 
3147         instance.save(expected_task_state=task_states.POWERING_ON)
3148         self._notify_about_instance_usage(context, instance, "power_on.end")
3149         compute_utils.notify_about_instance_action(context, instance,
3150             self.host, action=fields.NotificationAction.POWER_ON,
3151             phase=fields.NotificationPhase.END)
3152 
3153     @messaging.expected_exceptions(NotImplementedError,
3154                                    exception.TriggerCrashDumpNotSupported,
3155                                    exception.InstanceNotRunning)
3156     @wrap_exception()
3157     @wrap_instance_event(prefix='compute')
3158     @wrap_instance_fault
3159     def trigger_crash_dump(self, context, instance):
3160         """Trigger crash dump in an instance."""
3161 
3162         self._notify_about_instance_usage(context, instance,
3163                                           "trigger_crash_dump.start")
3164         compute_utils.notify_about_instance_action(context, instance,
3165                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3166                 phase=fields.NotificationPhase.START)
3167 
3168         # This method does not change task_state and power_state because the
3169         # effect of a trigger depends on user's configuration.
3170         self.driver.trigger_crash_dump(instance)
3171 
3172         self._notify_about_instance_usage(context, instance,
3173                                           "trigger_crash_dump.end")
3174         compute_utils.notify_about_instance_action(context, instance,
3175                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3176                 phase=fields.NotificationPhase.END)
3177 
3178     @wrap_exception()
3179     @reverts_task_state
3180     @wrap_instance_event(prefix='compute')
3181     @wrap_instance_fault
3182     def soft_delete_instance(self, context, instance):
3183         """Soft delete an instance on this host."""
3184         with compute_utils.notify_about_instance_delete(
3185                 self.notifier, context, instance, 'soft_delete',
3186                 source=fields.NotificationSource.COMPUTE):
3187             try:
3188                 self.driver.soft_delete(instance)
3189             except NotImplementedError:
3190                 # Fallback to just powering off the instance if the
3191                 # hypervisor doesn't implement the soft_delete method
3192                 self.driver.power_off(instance)
3193             instance.power_state = self._get_power_state(instance)
3194             instance.vm_state = vm_states.SOFT_DELETED
3195             instance.task_state = None
3196             instance.save(expected_task_state=[task_states.SOFT_DELETING])
3197 
3198     @wrap_exception()
3199     @reverts_task_state
3200     @wrap_instance_event(prefix='compute')
3201     @wrap_instance_fault
3202     def restore_instance(self, context, instance):
3203         """Restore a soft-deleted instance on this host."""
3204         self._notify_about_instance_usage(context, instance, "restore.start")
3205         compute_utils.notify_about_instance_action(context, instance,
3206             self.host, action=fields.NotificationAction.RESTORE,
3207             phase=fields.NotificationPhase.START)
3208         try:
3209             self.driver.restore(instance)
3210         except NotImplementedError:
3211             # Fallback to just powering on the instance if the hypervisor
3212             # doesn't implement the restore method
3213             self._power_on(context, instance)
3214         instance.power_state = self._get_power_state(instance)
3215         instance.vm_state = vm_states.ACTIVE
3216         instance.task_state = None
3217         instance.save(expected_task_state=task_states.RESTORING)
3218         self._notify_about_instance_usage(context, instance, "restore.end")
3219         compute_utils.notify_about_instance_action(context, instance,
3220             self.host, action=fields.NotificationAction.RESTORE,
3221             phase=fields.NotificationPhase.END)
3222 
3223     @staticmethod
3224     def _set_migration_status(migration, status):
3225         """Set the status, and guard against a None being passed in.
3226 
3227         This is useful as some of the compute RPC calls will not pass
3228         a migration object in older versions. The check can be removed when
3229         we move past 4.x major version of the RPC API.
3230         """
3231         if migration:
3232             migration.status = status
3233             migration.save()
3234 
3235     def _rebuild_default_impl(self, context, instance, image_meta,
3236                               injected_files, admin_password, allocations,
3237                               bdms, detach_block_devices, attach_block_devices,
3238                               network_info=None,
3239                               evacuate=False, block_device_info=None,
3240                               preserve_ephemeral=False):
3241         if preserve_ephemeral:
3242             # The default code path does not support preserving ephemeral
3243             # partitions.
3244             raise exception.PreserveEphemeralNotSupported()
3245 
3246         if evacuate:
3247             detach_block_devices(context, bdms)
3248         else:
3249             self._power_off_instance(instance, clean_shutdown=True)
3250             detach_block_devices(context, bdms)
3251             self.driver.destroy(context, instance,
3252                                 network_info=network_info,
3253                                 block_device_info=block_device_info)
3254 
3255         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
3256         instance.save(expected_task_state=[task_states.REBUILDING])
3257 
3258         new_block_device_info = attach_block_devices(context, instance, bdms)
3259 
3260         instance.task_state = task_states.REBUILD_SPAWNING
3261         instance.save(
3262             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
3263 
3264         with instance.mutated_migration_context():
3265             self.driver.spawn(context, instance, image_meta, injected_files,
3266                               admin_password, allocations,
3267                               network_info=network_info,
3268                               block_device_info=new_block_device_info)
3269 
3270     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
3271         self._notify_about_instance_usage(context, instance,
3272                                           'rebuild.error', fault=error)
3273         compute_utils.notify_about_instance_rebuild(
3274             context, instance, self.host,
3275             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms)
3276 
3277     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported)
3278     @wrap_exception()
3279     @reverts_task_state
3280     @wrap_instance_event(prefix='compute')
3281     @wrap_instance_fault
3282     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
3283                          injected_files, new_pass, orig_sys_metadata,
3284                          bdms, recreate, on_shared_storage,
3285                          preserve_ephemeral, migration,
3286                          scheduled_node, limits, request_spec):
3287         """Destroy and re-make this instance.
3288 
3289         A 'rebuild' effectively purges all existing data from the system and
3290         remakes the VM with given 'metadata' and 'personalities'.
3291 
3292         :param context: `nova.RequestContext` object
3293         :param instance: Instance object
3294         :param orig_image_ref: Original image_ref before rebuild
3295         :param image_ref: New image_ref for rebuild
3296         :param injected_files: Files to inject
3297         :param new_pass: password to set on rebuilt instance
3298         :param orig_sys_metadata: instance system metadata from pre-rebuild
3299         :param bdms: block-device-mappings to use for rebuild
3300         :param recreate: True if the instance is being evacuated (e.g. the
3301             hypervisor it was on failed) - cleanup of old state will be
3302             skipped.
3303         :param on_shared_storage: True if instance files on shared storage.
3304                                   If not provided then information from the
3305                                   driver will be used to decide if the instance
3306                                   files are available or not on the target host
3307         :param preserve_ephemeral: True if the default ephemeral storage
3308                                    partition must be preserved on rebuild
3309         :param migration: a Migration object if one was created for this
3310                           rebuild operation (if it's a part of evacuate)
3311         :param scheduled_node: A node of the host chosen by the scheduler. If a
3312                                host was specified by the user, this will be
3313                                None
3314         :param limits: Overcommit limits set by the scheduler. If a host was
3315                        specified by the user, this will be None
3316         :param request_spec: a RequestSpec object used to schedule the instance
3317 
3318         """
3319         # recreate=True means the instance is being evacuated from a failed
3320         # host to a new destination host (this host). The 'recreate' variable
3321         # name is confusing, so rename it to evacuate here at the top, which
3322         # is simpler than renaming a parameter in an RPC versioned method.
3323         evacuate = recreate
3324         context = context.elevated()
3325 
3326         if evacuate:
3327             LOG.info("Evacuating instance", instance=instance)
3328         else:
3329             LOG.info("Rebuilding instance", instance=instance)
3330 
3331         if evacuate:
3332             # This is an evacuation to a new host, so we need to perform a
3333             # resource claim.
3334             rebuild_claim = self.rt.rebuild_claim
3335         else:
3336             # This is a rebuild to the same host, so we don't need to make
3337             # a claim since the instance is already on this host.
3338             rebuild_claim = claims.NopClaim
3339 
3340         if image_ref:
3341             image_meta = objects.ImageMeta.from_image_ref(
3342                 context, self.image_api, image_ref)
3343         elif evacuate:
3344             # For evacuate the API does not send down the image_ref since the
3345             # image does not change so just get it from what was stashed in
3346             # the instance system_metadata when the instance was created (or
3347             # last rebuilt). This also works for volume-backed instances.
3348             image_meta = instance.image_meta
3349         else:
3350             image_meta = objects.ImageMeta()
3351 
3352         # NOTE(mriedem): On an evacuate, we need to update
3353         # the instance's host and node properties to reflect it's
3354         # destination node for the evacuate.
3355         if not scheduled_node:
3356             if evacuate:
3357                 try:
3358                     compute_node = self._get_compute_info(context, self.host)
3359                     scheduled_node = compute_node.hypervisor_hostname
3360                 except exception.ComputeHostNotFound:
3361                     LOG.exception('Failed to get compute_info for %s',
3362                                   self.host)
3363             else:
3364                 scheduled_node = instance.node
3365 
3366         allocs = self.reportclient.get_allocations_for_consumer(
3367                     context, instance.uuid)
3368 
3369         # If the resource claim or group policy validation fails before we
3370         # do anything to the guest or its networking/volumes we want to keep
3371         # the current status rather than put the instance into ERROR status.
3372         instance_state = instance.vm_state
3373         with self._error_out_instance_on_exception(
3374                 context, instance, instance_state=instance_state):
3375             try:
3376                 self._do_rebuild_instance_with_claim(
3377                     context, instance, orig_image_ref,
3378                     image_meta, injected_files, new_pass, orig_sys_metadata,
3379                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3380                     migration, request_spec, allocs, rebuild_claim,
3381                     scheduled_node, limits)
3382             except (exception.ComputeResourcesUnavailable,
3383                     exception.RescheduledException) as e:
3384                 if isinstance(e, exception.ComputeResourcesUnavailable):
3385                     LOG.debug("Could not rebuild instance on this host, not "
3386                               "enough resources available.", instance=instance)
3387                 else:
3388                     # RescheduledException is raised by the late server group
3389                     # policy check during evacuation if a parallel scheduling
3390                     # violated the policy.
3391                     # We catch the RescheduledException here but we don't have
3392                     # the plumbing to do an actual reschedule so we abort the
3393                     # operation.
3394                     LOG.debug("Could not rebuild instance on this host, "
3395                               "late server group check failed.",
3396                               instance=instance)
3397                 # NOTE(ndipanov): We just abort the build for now and leave a
3398                 # migration record for potential cleanup later
3399                 self._set_migration_status(migration, 'failed')
3400                 # Since the claim failed, we need to remove the allocation
3401                 # created against the destination node. Note that we can only
3402                 # get here when evacuating to a destination node. Rebuilding
3403                 # on the same host (not evacuate) uses the NopClaim which will
3404                 # not raise ComputeResourcesUnavailable.
3405                 self.rt.delete_allocation_for_evacuated_instance(
3406                     context, instance, scheduled_node, node_type='destination')
3407                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3408                 # Wrap this in InstanceFaultRollback so that the
3409                 # _error_out_instance_on_exception context manager keeps the
3410                 # vm_state unchanged.
3411                 raise exception.InstanceFaultRollback(
3412                     inner_exception=exception.BuildAbortException(
3413                         instance_uuid=instance.uuid,
3414                         reason=e.format_message()))
3415             except (exception.InstanceNotFound,
3416                     exception.UnexpectedDeletingTaskStateError) as e:
3417                 LOG.debug('Instance was deleted while rebuilding',
3418                           instance=instance)
3419                 self._set_migration_status(migration, 'failed')
3420                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3421             except Exception as e:
3422                 self._set_migration_status(migration, 'failed')
3423                 if evacuate or scheduled_node is not None:
3424                     self.rt.delete_allocation_for_evacuated_instance(
3425                         context, instance, scheduled_node,
3426                         node_type='destination')
3427                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3428                 raise
3429             else:
3430                 instance.apply_migration_context()
3431                 # NOTE (ndipanov): This save will now update the host and node
3432                 # attributes making sure that next RT pass is consistent since
3433                 # it will be based on the instance and not the migration DB
3434                 # entry.
3435                 instance.host = self.host
3436                 instance.node = scheduled_node
3437                 instance.save()
3438                 instance.drop_migration_context()
3439 
3440                 # NOTE (ndipanov): Mark the migration as done only after we
3441                 # mark the instance as belonging to this host.
3442                 self._set_migration_status(migration, 'done')
3443 
3444     def _do_rebuild_instance_with_claim(
3445             self, context, instance, orig_image_ref, image_meta,
3446             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3447             on_shared_storage, preserve_ephemeral, migration, request_spec,
3448             allocations, rebuild_claim, scheduled_node, limits):
3449         """Helper to avoid deep nesting in the top-level method."""
3450 
3451         provider_mapping = None
3452         if evacuate:
3453             provider_mapping = self._get_request_group_mapping(request_spec)
3454 
3455             if provider_mapping:
3456                 compute_utils.\
3457                     update_pci_request_spec_with_allocated_interface_name(
3458                         context, self.reportclient, instance, provider_mapping)
3459 
3460         claim_context = rebuild_claim(
3461             context, instance, scheduled_node, allocations,
3462             limits=limits, image_meta=image_meta, migration=migration)
3463 
3464         with claim_context:
3465             self._do_rebuild_instance(
3466                 context, instance, orig_image_ref, image_meta, injected_files,
3467                 new_pass, orig_sys_metadata, bdms, evacuate, on_shared_storage,
3468                 preserve_ephemeral, migration, request_spec, allocations,
3469                 provider_mapping)
3470 
3471     @staticmethod
3472     def _get_image_name(image_meta):
3473         if image_meta.obj_attr_is_set("name"):
3474             return image_meta.name
3475         else:
3476             return ''
3477 
3478     def _do_rebuild_instance(self, context, instance, orig_image_ref,
3479                              image_meta, injected_files, new_pass,
3480                              orig_sys_metadata, bdms, evacuate,
3481                              on_shared_storage, preserve_ephemeral,
3482                              migration, request_spec, allocations,
3483                              request_group_resource_providers_mapping):
3484         orig_vm_state = instance.vm_state
3485 
3486         if evacuate:
3487             if request_spec:
3488                 # NOTE(gibi): Do a late check of server group policy as
3489                 # parallel scheduling could violate such policy. This will
3490                 # cause the evacuate to fail as rebuild does not implement
3491                 # reschedule.
3492                 hints = self._get_scheduler_hints({}, request_spec)
3493                 self._validate_instance_group_policy(context, instance, hints)
3494 
3495             if not self.driver.capabilities.get("supports_evacuate", False):
3496                 raise exception.InstanceEvacuateNotSupported
3497 
3498             self._check_instance_exists(instance)
3499 
3500             if on_shared_storage is None:
3501                 LOG.debug('on_shared_storage is not provided, using driver '
3502                           'information to decide if the instance needs to '
3503                           'be evacuated')
3504                 on_shared_storage = self.driver.instance_on_disk(instance)
3505 
3506             elif (on_shared_storage !=
3507                     self.driver.instance_on_disk(instance)):
3508                 # To cover case when admin expects that instance files are
3509                 # on shared storage, but not accessible and vice versa
3510                 raise exception.InvalidSharedStorage(
3511                         _("Invalid state of instance files on shared"
3512                             " storage"))
3513 
3514             if on_shared_storage:
3515                 LOG.info('disk on shared storage, evacuating using'
3516                          ' existing disk')
3517             elif instance.image_ref:
3518                 orig_image_ref = instance.image_ref
3519                 LOG.info("disk not on shared storage, evacuating from "
3520                          "image: '%s'", str(orig_image_ref))
3521             else:
3522                 LOG.info('disk on volume, evacuating using existing '
3523                          'volume')
3524 
3525         # We check trusted certs capabilities for both evacuate (rebuild on
3526         # another host) and rebuild (rebuild on the same host) because for
3527         # evacuate we need to make sure an instance with trusted certs can
3528         # have the image verified with those certs during rebuild, and for
3529         # rebuild we could be rebuilding a server that started out with no
3530         # trusted certs on this host, and then was rebuilt with trusted certs
3531         # for a new image, in which case we need to validate that new image
3532         # with the trusted certs during the rebuild.
3533         self._check_trusted_certs(instance)
3534 
3535         # This instance.exists message should contain the original
3536         # image_ref, not the new one.  Since the DB has been updated
3537         # to point to the new one... we have to override it.
3538         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3539                                                                context)
3540         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3541         compute_utils.notify_usage_exists(
3542                 self.notifier, context, instance, self.host,
3543                 current_period=True, system_metadata=orig_sys_metadata,
3544                 extra_usage_info=extra_usage_info)
3545 
3546         # This message should contain the new image_ref
3547         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3548         self._notify_about_instance_usage(context, instance,
3549                 "rebuild.start", extra_usage_info=extra_usage_info)
3550         # NOTE: image_name is not included in the versioned notification
3551         # because we already provide the image_uuid in the notification
3552         # payload and the image details can be looked up via the uuid.
3553         compute_utils.notify_about_instance_rebuild(
3554             context, instance, self.host,
3555             phase=fields.NotificationPhase.START,
3556             bdms=bdms)
3557 
3558         instance.power_state = self._get_power_state(instance)
3559         instance.task_state = task_states.REBUILDING
3560         instance.save(expected_task_state=[task_states.REBUILDING])
3561 
3562         if evacuate:
3563             self.network_api.setup_networks_on_host(
3564                     context, instance, self.host)
3565             # For nova-network this is needed to move floating IPs
3566             # For neutron this updates the host in the port binding
3567             # TODO(cfriesen): this network_api call and the one above
3568             # are so similar, we should really try to unify them.
3569             self.network_api.setup_instance_network_on_host(
3570                 context, instance, self.host, migration,
3571                 provider_mappings=request_group_resource_providers_mapping)
3572             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3573             # with @api.refresh_cache and then we wouldn't need this explicit
3574             # call to get_instance_nw_info.
3575             network_info = self.network_api.get_instance_nw_info(context,
3576                                                                  instance)
3577         else:
3578             network_info = instance.get_network_info()
3579 
3580         if bdms is None:
3581             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3582                     context, instance.uuid)
3583 
3584         block_device_info = \
3585             self._get_instance_block_device_info(
3586                     context, instance, bdms=bdms)
3587 
3588         def detach_block_devices(context, bdms):
3589             for bdm in bdms:
3590                 if bdm.is_volume:
3591                     # NOTE (ildikov): Having the attachment_id set in the BDM
3592                     # means that it's the new Cinder attach/detach flow
3593                     # (available from v3.44). In that case we explicitly
3594                     # attach and detach the volumes through attachment level
3595                     # operations. In this scenario _detach_volume will delete
3596                     # the existing attachment which would make the volume
3597                     # status change to 'available' if we don't pre-create
3598                     # another empty attachment before deleting the old one.
3599                     attachment_id = None
3600                     if bdm.attachment_id:
3601                         attachment_id = self.volume_api.attachment_create(
3602                             context, bdm['volume_id'], instance.uuid)['id']
3603                     self._detach_volume(context, bdm, instance,
3604                                         destroy_bdm=False)
3605                     if attachment_id:
3606                         bdm.attachment_id = attachment_id
3607                         bdm.save()
3608 
3609         files = self._decode_files(injected_files)
3610 
3611         kwargs = dict(
3612             context=context,
3613             instance=instance,
3614             image_meta=image_meta,
3615             injected_files=files,
3616             admin_password=new_pass,
3617             allocations=allocations,
3618             bdms=bdms,
3619             detach_block_devices=detach_block_devices,
3620             attach_block_devices=self._prep_block_device,
3621             block_device_info=block_device_info,
3622             network_info=network_info,
3623             preserve_ephemeral=preserve_ephemeral,
3624             evacuate=evacuate)
3625         try:
3626             with instance.mutated_migration_context():
3627                 self.driver.rebuild(**kwargs)
3628         except NotImplementedError:
3629             # NOTE(rpodolyaka): driver doesn't provide specialized version
3630             # of rebuild, fall back to the default implementation
3631             self._rebuild_default_impl(**kwargs)
3632         self._update_instance_after_spawn(instance)
3633         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3634 
3635         if orig_vm_state == vm_states.STOPPED:
3636             LOG.info("bringing vm to original state: '%s'",
3637                      orig_vm_state, instance=instance)
3638             instance.vm_state = vm_states.ACTIVE
3639             instance.task_state = task_states.POWERING_OFF
3640             instance.progress = 0
3641             instance.save()
3642             self.stop_instance(context, instance, False)
3643         # TODO(melwitt): We should clean up instance console tokens here in the
3644         # case of evacuate. The instance is on a new host and will need to
3645         # establish a new console connection.
3646         self._update_scheduler_instance_info(context, instance)
3647         self._notify_about_instance_usage(
3648                 context, instance, "rebuild.end",
3649                 network_info=network_info,
3650                 extra_usage_info=extra_usage_info)
3651         compute_utils.notify_about_instance_rebuild(
3652             context, instance, self.host,
3653             phase=fields.NotificationPhase.END,
3654             bdms=bdms)
3655 
3656     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3657                                      block_device_info):
3658         """Handle cases where the virt-layer had to detach non-working volumes
3659         in order to complete an operation.
3660         """
3661         for bdm in block_device_info['block_device_mapping']:
3662             if bdm.get('mount_device') in bad_devices:
3663                 try:
3664                     volume_id = bdm['connection_info']['data']['volume_id']
3665                 except KeyError:
3666                     continue
3667 
3668                 # NOTE(sirp): ideally we'd just call
3669                 # `compute_api.detach_volume` here but since that hits the
3670                 # DB directly, that's off limits from within the
3671                 # compute-manager.
3672                 #
3673                 # API-detach
3674                 LOG.info("Detaching from volume api: %s", volume_id)
3675                 self.volume_api.begin_detaching(context, volume_id)
3676 
3677                 # Manager-detach
3678                 self.detach_volume(context, volume_id, instance)
3679 
3680     def _get_accel_info(self, context, instance):
3681         dp_name = instance.flavor.extra_specs.get('accel:device_profile')
3682         if dp_name:
3683             cyclient = cyborg.get_client(context)
3684             accel_info = cyclient.get_arqs_for_instance(instance.uuid)
3685         else:
3686             accel_info = []
3687         return accel_info
3688 
3689     @wrap_exception()
3690     @reverts_task_state
3691     @wrap_instance_event(prefix='compute')
3692     @wrap_instance_fault
3693     def reboot_instance(self, context, instance, block_device_info,
3694                         reboot_type):
3695         @utils.synchronized(instance.uuid)
3696         def do_reboot_instance(context, instance, block_device_info,
3697                                reboot_type):
3698             self._reboot_instance(context, instance, block_device_info,
3699                                   reboot_type)
3700         do_reboot_instance(context, instance, block_device_info, reboot_type)
3701 
3702     def _reboot_instance(self, context, instance, block_device_info,
3703                          reboot_type):
3704         """Reboot an instance on this host."""
3705         # acknowledge the request made it to the manager
3706         if reboot_type == "SOFT":
3707             instance.task_state = task_states.REBOOT_PENDING
3708             expected_states = task_states.soft_reboot_states
3709         else:
3710             instance.task_state = task_states.REBOOT_PENDING_HARD
3711             expected_states = task_states.hard_reboot_states
3712 
3713         context = context.elevated()
3714         LOG.info("Rebooting instance", instance=instance)
3715 
3716         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3717             context, instance.uuid)
3718         block_device_info = self._get_instance_block_device_info(
3719             context, instance, bdms=bdms)
3720 
3721         network_info = self.network_api.get_instance_nw_info(context, instance)
3722 
3723         accel_info = self._get_accel_info(context, instance)
3724 
3725         self._notify_about_instance_usage(context, instance, "reboot.start")
3726         compute_utils.notify_about_instance_action(
3727             context, instance, self.host,
3728             action=fields.NotificationAction.REBOOT,
3729             phase=fields.NotificationPhase.START,
3730             bdms=bdms
3731         )
3732 
3733         instance.power_state = self._get_power_state(instance)
3734         instance.save(expected_task_state=expected_states)
3735 
3736         if instance.power_state != power_state.RUNNING:
3737             state = instance.power_state
3738             running = power_state.RUNNING
3739             LOG.warning('trying to reboot a non-running instance:'
3740                         ' (state: %(state)s expected: %(running)s)',
3741                         {'state': state, 'running': running},
3742                         instance=instance)
3743 
3744         def bad_volumes_callback(bad_devices):
3745             self._handle_bad_volumes_detached(
3746                     context, instance, bad_devices, block_device_info)
3747 
3748         try:
3749             # Don't change it out of rescue mode
3750             if instance.vm_state == vm_states.RESCUED:
3751                 new_vm_state = vm_states.RESCUED
3752             else:
3753                 new_vm_state = vm_states.ACTIVE
3754             new_power_state = None
3755             if reboot_type == "SOFT":
3756                 instance.task_state = task_states.REBOOT_STARTED
3757                 expected_state = task_states.REBOOT_PENDING
3758             else:
3759                 instance.task_state = task_states.REBOOT_STARTED_HARD
3760                 expected_state = task_states.REBOOT_PENDING_HARD
3761             instance.save(expected_task_state=expected_state)
3762             self.driver.reboot(context, instance,
3763                                network_info,
3764                                reboot_type,
3765                                block_device_info=block_device_info,
3766                                accel_info=accel_info,
3767                                bad_volumes_callback=bad_volumes_callback)
3768 
3769         except Exception as error:
3770             with excutils.save_and_reraise_exception() as ctxt:
3771                 exc_info = sys.exc_info()
3772                 # if the reboot failed but the VM is running don't
3773                 # put it into an error state
3774                 new_power_state = self._get_power_state(instance)
3775                 if new_power_state == power_state.RUNNING:
3776                     LOG.warning('Reboot failed but instance is running',
3777                                 instance=instance)
3778                     compute_utils.add_instance_fault_from_exc(context,
3779                             instance, error, exc_info)
3780                     self._notify_about_instance_usage(context, instance,
3781                             'reboot.error', fault=error)
3782                     compute_utils.notify_about_instance_action(
3783                         context, instance, self.host,
3784                         action=fields.NotificationAction.REBOOT,
3785                         phase=fields.NotificationPhase.ERROR,
3786                         exception=error, bdms=bdms
3787                     )
3788                     ctxt.reraise = False
3789                 else:
3790                     LOG.error('Cannot reboot instance: %s', error,
3791                               instance=instance)
3792                     self._set_instance_obj_error_state(instance)
3793 
3794         if not new_power_state:
3795             new_power_state = self._get_power_state(instance)
3796         try:
3797             instance.power_state = new_power_state
3798             instance.vm_state = new_vm_state
3799             instance.task_state = None
3800             instance.save()
3801         except exception.InstanceNotFound:
3802             LOG.warning("Instance disappeared during reboot",
3803                         instance=instance)
3804 
3805         self._notify_about_instance_usage(context, instance, "reboot.end")
3806         compute_utils.notify_about_instance_action(
3807             context, instance, self.host,
3808             action=fields.NotificationAction.REBOOT,
3809             phase=fields.NotificationPhase.END,
3810             bdms=bdms
3811         )
3812 
3813     @delete_image_on_error
3814     def _do_snapshot_instance(self, context, image_id, instance):
3815         self._snapshot_instance(context, image_id, instance,
3816                                 task_states.IMAGE_BACKUP)
3817 
3818     @wrap_exception()
3819     @reverts_task_state
3820     @wrap_instance_event(prefix='compute')
3821     @wrap_instance_fault
3822     def backup_instance(self, context, image_id, instance, backup_type,
3823                         rotation):
3824         """Backup an instance on this host.
3825 
3826         :param backup_type: daily | weekly
3827         :param rotation: int representing how many backups to keep around
3828         """
3829         self._do_snapshot_instance(context, image_id, instance)
3830         self._rotate_backups(context, instance, backup_type, rotation)
3831 
3832     @wrap_exception()
3833     @reverts_task_state
3834     @wrap_instance_event(prefix='compute')
3835     @wrap_instance_fault
3836     @delete_image_on_error
3837     def snapshot_instance(self, context, image_id, instance):
3838         """Snapshot an instance on this host.
3839 
3840         :param context: security context
3841         :param image_id: glance.db.sqlalchemy.models.Image.Id
3842         :param instance: a nova.objects.instance.Instance object
3843         """
3844         # NOTE(dave-mcnally) the task state will already be set by the api
3845         # but if the compute manager has crashed/been restarted prior to the
3846         # request getting here the task state may have been cleared so we set
3847         # it again and things continue normally
3848         try:
3849             instance.task_state = task_states.IMAGE_SNAPSHOT
3850             instance.save(
3851                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3852         except exception.InstanceNotFound:
3853             # possibility instance no longer exists, no point in continuing
3854             LOG.debug("Instance not found, could not set state %s "
3855                       "for instance.",
3856                       task_states.IMAGE_SNAPSHOT, instance=instance)
3857             return
3858 
3859         except exception.UnexpectedDeletingTaskStateError:
3860             LOG.debug("Instance being deleted, snapshot cannot continue",
3861                       instance=instance)
3862             return
3863 
3864         with self._snapshot_semaphore:
3865             self._snapshot_instance(context, image_id, instance,
3866                                     task_states.IMAGE_SNAPSHOT)
3867 
3868     def _snapshot_instance(self, context, image_id, instance,
3869                            expected_task_state):
3870         context = context.elevated()
3871 
3872         instance.power_state = self._get_power_state(instance)
3873         try:
3874             instance.save()
3875 
3876             LOG.info('instance snapshotting', instance=instance)
3877 
3878             if instance.power_state != power_state.RUNNING:
3879                 state = instance.power_state
3880                 running = power_state.RUNNING
3881                 LOG.warning('trying to snapshot a non-running instance: '
3882                             '(state: %(state)s expected: %(running)s)',
3883                             {'state': state, 'running': running},
3884                             instance=instance)
3885 
3886             self._notify_about_instance_usage(
3887                 context, instance, "snapshot.start")
3888             compute_utils.notify_about_instance_snapshot(context, instance,
3889                 self.host, phase=fields.NotificationPhase.START,
3890                 snapshot_image_id=image_id)
3891 
3892             def update_task_state(task_state,
3893                                   expected_state=expected_task_state):
3894                 instance.task_state = task_state
3895                 instance.save(expected_task_state=expected_state)
3896 
3897             with timeutils.StopWatch() as timer:
3898                 self.driver.snapshot(context, instance, image_id,
3899                                      update_task_state)
3900             LOG.info('Took %0.2f seconds to snapshot the instance on '
3901                      'the hypervisor.', timer.elapsed(), instance=instance)
3902 
3903             instance.task_state = None
3904             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3905 
3906             self._notify_about_instance_usage(context, instance,
3907                                               "snapshot.end")
3908             compute_utils.notify_about_instance_snapshot(context, instance,
3909                 self.host, phase=fields.NotificationPhase.END,
3910                 snapshot_image_id=image_id)
3911         except (exception.InstanceNotFound,
3912                 exception.InstanceNotRunning,
3913                 exception.UnexpectedDeletingTaskStateError):
3914             # the instance got deleted during the snapshot
3915             # Quickly bail out of here
3916             msg = 'Instance disappeared during snapshot'
3917             LOG.debug(msg, instance=instance)
3918             try:
3919                 image = self.image_api.get(context, image_id)
3920                 if image['status'] != 'active':
3921                     self.image_api.delete(context, image_id)
3922             except exception.ImageNotFound:
3923                 LOG.debug('Image not found during clean up %s', image_id)
3924             except Exception:
3925                 LOG.warning("Error while trying to clean up image %s",
3926                             image_id, instance=instance)
3927         except exception.ImageNotFound:
3928             instance.task_state = None
3929             instance.save()
3930             LOG.warning("Image not found during snapshot", instance=instance)
3931 
3932     def _post_interrupted_snapshot_cleanup(self, context, instance):
3933         self.driver.post_interrupted_snapshot_cleanup(context, instance)
3934 
3935     @messaging.expected_exceptions(NotImplementedError)
3936     @wrap_exception()
3937     def volume_snapshot_create(self, context, instance, volume_id,
3938                                create_info):
3939         try:
3940             self.driver.volume_snapshot_create(context, instance, volume_id,
3941                                                create_info)
3942         except exception.InstanceNotRunning:
3943             # Libvirt driver can raise this exception
3944             LOG.debug('Instance disappeared during volume snapshot create',
3945                       instance=instance)
3946 
3947     @messaging.expected_exceptions(NotImplementedError)
3948     @wrap_exception()
3949     def volume_snapshot_delete(self, context, instance, volume_id,
3950                                snapshot_id, delete_info):
3951         try:
3952             self.driver.volume_snapshot_delete(context, instance, volume_id,
3953                                                snapshot_id, delete_info)
3954         except exception.InstanceNotRunning:
3955             # Libvirt driver can raise this exception
3956             LOG.debug('Instance disappeared during volume snapshot delete',
3957                       instance=instance)
3958 
3959     @wrap_instance_fault
3960     def _rotate_backups(self, context, instance, backup_type, rotation):
3961         """Delete excess backups associated to an instance.
3962 
3963         Instances are allowed a fixed number of backups (the rotation number);
3964         this method deletes the oldest backups that exceed the rotation
3965         threshold.
3966 
3967         :param context: security context
3968         :param instance: Instance dict
3969         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
3970         :param rotation: int representing how many backups to keep around;
3971             None if rotation shouldn't be used (as in the case of snapshots)
3972         """
3973         filters = {'property-image_type': 'backup',
3974                    'property-backup_type': backup_type,
3975                    'property-instance_uuid': instance.uuid}
3976 
3977         images = self.image_api.get_all(context, filters=filters,
3978                                         sort_key='created_at', sort_dir='desc')
3979         num_images = len(images)
3980         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
3981                   {'num_images': num_images, 'rotation': rotation},
3982                   instance=instance)
3983 
3984         if num_images > rotation:
3985             # NOTE(sirp): this deletes all backups that exceed the rotation
3986             # limit
3987             excess = len(images) - rotation
3988             LOG.debug("Rotating out %d backups", excess,
3989                       instance=instance)
3990             for i in range(excess):
3991                 image = images.pop()
3992                 image_id = image['id']
3993                 LOG.debug("Deleting image %s", image_id,
3994                           instance=instance)
3995                 try:
3996                     self.image_api.delete(context, image_id)
3997                 except exception.ImageNotFound:
3998                     LOG.info("Failed to find image %(image_id)s to "
3999                              "delete", {'image_id': image_id},
4000                              instance=instance)
4001                 except (exception.ImageDeleteConflict, Exception) as exc:
4002                     LOG.info("Failed to delete image %(image_id)s during "
4003                              "deleting excess backups. "
4004                              "Continuing for next image.. %(exc)s",
4005                              {'image_id': image_id, 'exc': exc},
4006                              instance=instance)
4007 
4008     @wrap_exception()
4009     @reverts_task_state
4010     @wrap_instance_event(prefix='compute')
4011     @wrap_instance_fault
4012     def set_admin_password(self, context, instance, new_pass):
4013         """Set the root/admin password for an instance on this host.
4014 
4015         This is generally only called by API password resets after an
4016         image has been built.
4017 
4018         @param context: Nova auth context.
4019         @param instance: Nova instance object.
4020         @param new_pass: The admin password for the instance.
4021         """
4022 
4023         context = context.elevated()
4024         current_power_state = self._get_power_state(instance)
4025         expected_state = power_state.RUNNING
4026 
4027         if current_power_state != expected_state:
4028             instance.task_state = None
4029             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
4030             _msg = _('instance %s is not running') % instance.uuid
4031             raise exception.InstancePasswordSetFailed(
4032                 instance=instance.uuid, reason=_msg)
4033 
4034         try:
4035             self.driver.set_admin_password(instance, new_pass)
4036             LOG.info("Admin password set", instance=instance)
4037             instance.task_state = None
4038             instance.save(
4039                 expected_task_state=task_states.UPDATING_PASSWORD)
4040         except exception.InstanceAgentNotEnabled:
4041             with excutils.save_and_reraise_exception():
4042                 LOG.debug('Guest agent is not enabled for the instance.',
4043                           instance=instance)
4044                 instance.task_state = None
4045                 instance.save(
4046                     expected_task_state=task_states.UPDATING_PASSWORD)
4047         except exception.SetAdminPasswdNotSupported:
4048             with excutils.save_and_reraise_exception():
4049                 LOG.info('set_admin_password is not supported '
4050                          'by this driver or guest instance.',
4051                          instance=instance)
4052                 instance.task_state = None
4053                 instance.save(
4054                     expected_task_state=task_states.UPDATING_PASSWORD)
4055         except NotImplementedError:
4056             LOG.warning('set_admin_password is not implemented '
4057                         'by this driver or guest instance.',
4058                         instance=instance)
4059             instance.task_state = None
4060             instance.save(
4061                 expected_task_state=task_states.UPDATING_PASSWORD)
4062             raise NotImplementedError(_('set_admin_password is not '
4063                                         'implemented by this driver or guest '
4064                                         'instance.'))
4065         except exception.UnexpectedTaskStateError:
4066             # interrupted by another (most likely delete) task
4067             # do not retry
4068             raise
4069         except Exception:
4070             # Catch all here because this could be anything.
4071             LOG.exception('set_admin_password failed', instance=instance)
4072             # We create a new exception here so that we won't
4073             # potentially reveal password information to the
4074             # API caller.  The real exception is logged above
4075             _msg = _('error setting admin password')
4076             raise exception.InstancePasswordSetFailed(
4077                 instance=instance.uuid, reason=_msg)
4078 
4079     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
4080         """Determine what image should be used to boot the rescue VM."""
4081         # 1. If rescue_image_ref is passed in, use that for rescue.
4082         # 2. Else, use the base image associated with instance's current image.
4083         #       The idea here is to provide the customer with a rescue
4084         #       environment which they are familiar with.
4085         #       So, if they built their instance off of a Debian image,
4086         #       their rescue VM will also be Debian.
4087         # 3. As a last resort, use instance's current image.
4088         if not rescue_image_ref:
4089             system_meta = utils.instance_sys_meta(instance)
4090             rescue_image_ref = system_meta.get('image_base_image_ref')
4091 
4092         if not rescue_image_ref:
4093             LOG.warning('Unable to find a different image to use for '
4094                         'rescue VM, using instance\'s current image',
4095                         instance=instance)
4096             rescue_image_ref = instance.image_ref
4097 
4098         return objects.ImageMeta.from_image_ref(
4099             context, self.image_api, rescue_image_ref)
4100 
4101     @wrap_exception()
4102     @reverts_task_state
4103     @wrap_instance_event(prefix='compute')
4104     @wrap_instance_fault
4105     def rescue_instance(self, context, instance, rescue_password,
4106                         rescue_image_ref, clean_shutdown):
4107         context = context.elevated()
4108         LOG.info('Rescuing', instance=instance)
4109 
4110         admin_password = (rescue_password if rescue_password else
4111                       utils.generate_password())
4112 
4113         network_info = self.network_api.get_instance_nw_info(context, instance)
4114 
4115         rescue_image_meta = self._get_rescue_image(context, instance,
4116                                                    rescue_image_ref)
4117 
4118         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4119                                               context, instance.uuid)
4120         block_device_info = self._get_instance_block_device_info(
4121                                 context, instance, bdms=bdms)
4122 
4123         extra_usage_info = {'rescue_image_name':
4124                             self._get_image_name(rescue_image_meta)}
4125         self._notify_about_instance_usage(context, instance,
4126                 "rescue.start", extra_usage_info=extra_usage_info,
4127                 network_info=network_info)
4128         compute_utils.notify_about_instance_rescue_action(
4129             context, instance, self.host, rescue_image_ref,
4130             phase=fields.NotificationPhase.START)
4131 
4132         try:
4133             self._power_off_instance(instance, clean_shutdown)
4134 
4135             self.driver.rescue(context, instance, network_info,
4136                                rescue_image_meta, admin_password,
4137                                block_device_info)
4138         except Exception as e:
4139             LOG.exception("Error trying to Rescue Instance",
4140                           instance=instance)
4141             self._set_instance_obj_error_state(instance)
4142             raise exception.InstanceNotRescuable(
4143                 instance_id=instance.uuid,
4144                 reason=_("Driver Error: %s") % e)
4145 
4146         compute_utils.notify_usage_exists(self.notifier, context, instance,
4147                                           self.host, current_period=True)
4148 
4149         instance.vm_state = vm_states.RESCUED
4150         instance.task_state = None
4151         instance.power_state = self._get_power_state(instance)
4152         instance.launched_at = timeutils.utcnow()
4153         instance.save(expected_task_state=task_states.RESCUING)
4154 
4155         self._notify_about_instance_usage(context, instance,
4156                 "rescue.end", extra_usage_info=extra_usage_info,
4157                 network_info=network_info)
4158         compute_utils.notify_about_instance_rescue_action(
4159             context, instance, self.host, rescue_image_ref,
4160             phase=fields.NotificationPhase.END)
4161 
4162     @wrap_exception()
4163     @reverts_task_state
4164     @wrap_instance_event(prefix='compute')
4165     @wrap_instance_fault
4166     def unrescue_instance(self, context, instance):
4167         orig_context = context
4168         context = context.elevated()
4169         LOG.info('Unrescuing', instance=instance)
4170 
4171         network_info = self.network_api.get_instance_nw_info(context, instance)
4172         self._notify_about_instance_usage(context, instance,
4173                 "unrescue.start", network_info=network_info)
4174         compute_utils.notify_about_instance_action(context, instance,
4175             self.host, action=fields.NotificationAction.UNRESCUE,
4176             phase=fields.NotificationPhase.START)
4177 
4178         with self._error_out_instance_on_exception(context, instance):
4179             self.driver.unrescue(orig_context, instance)
4180 
4181         instance.vm_state = vm_states.ACTIVE
4182         instance.task_state = None
4183         instance.power_state = self._get_power_state(instance)
4184         instance.save(expected_task_state=task_states.UNRESCUING)
4185 
4186         self._notify_about_instance_usage(context,
4187                                           instance,
4188                                           "unrescue.end",
4189                                           network_info=network_info)
4190         compute_utils.notify_about_instance_action(context, instance,
4191             self.host, action=fields.NotificationAction.UNRESCUE,
4192             phase=fields.NotificationPhase.END)
4193 
4194     @wrap_exception()
4195     @wrap_instance_fault
4196     def change_instance_metadata(self, context, diff, instance):
4197         """Update the metadata published to the instance."""
4198         LOG.debug("Changing instance metadata according to %r",
4199                   diff, instance=instance)
4200         self.driver.change_instance_metadata(context, instance, diff)
4201 
4202     @wrap_exception()
4203     @wrap_instance_event(prefix='compute')
4204     @errors_out_migration
4205     @wrap_instance_fault
4206     def confirm_resize(self, context, instance, migration):
4207         """Confirms a migration/resize and deletes the 'old' instance.
4208 
4209         This is called from the API and runs on the source host.
4210 
4211         Nothing needs to happen on the destination host at this point since
4212         the instance is already running there. This routine just cleans up the
4213         source host.
4214         """
4215         @utils.synchronized(instance.uuid)
4216         def do_confirm_resize(context, instance, migration):
4217             LOG.debug("Going to confirm migration %s", migration.id,
4218                       instance=instance)
4219 
4220             if migration.status == 'confirmed':
4221                 LOG.info("Migration %s is already confirmed",
4222                          migration.id, instance=instance)
4223                 return
4224 
4225             if migration.status not in ('finished', 'confirming'):
4226                 LOG.warning("Unexpected confirmation status '%(status)s' "
4227                             "of migration %(id)s, exit confirmation process",
4228                             {"status": migration.status, "id": migration.id},
4229                             instance=instance)
4230                 return
4231 
4232             # NOTE(wangpan): Get the instance from db, if it has been
4233             #                deleted, we do nothing and return here
4234             expected_attrs = ['metadata', 'system_metadata', 'flavor']
4235             try:
4236                 instance = objects.Instance.get_by_uuid(
4237                         context, instance.uuid,
4238                         expected_attrs=expected_attrs)
4239             except exception.InstanceNotFound:
4240                 LOG.info("Instance is not found during confirmation",
4241                          instance=instance)
4242                 return
4243 
4244             with self._error_out_instance_on_exception(context, instance):
4245                 try:
4246                     self._confirm_resize(
4247                         context, instance, migration=migration)
4248                 except Exception:
4249                     # Something failed when cleaning up the source host so
4250                     # log a traceback and leave a hint about hard rebooting
4251                     # the server to correct its state in the DB.
4252                     with excutils.save_and_reraise_exception(logger=LOG):
4253                         LOG.exception(
4254                             'Confirm resize failed on source host %s. '
4255                             'Resource allocations in the placement service '
4256                             'will be removed regardless because the instance '
4257                             'is now on the destination host %s. You can try '
4258                             'hard rebooting the instance to correct its '
4259                             'state.', self.host, migration.dest_compute,
4260                             instance=instance)
4261                 finally:
4262                     # Whether an error occurred or not, at this point the
4263                     # instance is on the dest host. Avoid leaking allocations
4264                     # in placement by deleting them here...
4265                     self._delete_allocation_after_move(
4266                         context, instance, migration)
4267                     # ...inform the scheduler about the move...
4268                     self._delete_scheduler_instance_info(
4269                         context, instance.uuid)
4270                     # ...and unset the cached flavor information (this is done
4271                     # last since the resource tracker relies on it for its
4272                     # periodic tasks)
4273                     self._delete_stashed_flavor_info(instance)
4274 
4275         do_confirm_resize(context, instance, migration)
4276 
4277     def _get_updated_nw_info_with_pci_mapping(self, nw_info, pci_mapping):
4278         # NOTE(adrianc): This method returns a copy of nw_info if modifications
4279         # are made else it returns the original nw_info.
4280         updated_nw_info = nw_info
4281         if nw_info and pci_mapping:
4282             updated_nw_info = copy.deepcopy(nw_info)
4283             for vif in updated_nw_info:
4284                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
4285                     try:
4286                         vif_pci_addr = vif['profile']['pci_slot']
4287                         new_addr = pci_mapping[vif_pci_addr].address
4288                         vif['profile']['pci_slot'] = new_addr
4289                         LOG.debug("Updating VIF's PCI address for VIF %(id)s. "
4290                                   "Original value %(orig_val)s, "
4291                                   "new value %(new_val)s",
4292                                   {'id': vif['id'],
4293                                    'orig_val': vif_pci_addr,
4294                                    'new_val': new_addr})
4295                     except (KeyError, AttributeError):
4296                         with excutils.save_and_reraise_exception():
4297                             # NOTE(adrianc): This should never happen. If we
4298                             # get here it means there is some inconsistency
4299                             # with either 'nw_info' or 'pci_mapping'.
4300                             LOG.error("Unexpected error when updating network "
4301                                       "information with PCI mapping.")
4302         return updated_nw_info
4303 
4304     def _confirm_resize(self, context, instance, migration=None):
4305         """Destroys the source instance."""
4306         self._notify_about_instance_usage(context, instance,
4307                                           "resize.confirm.start")
4308         compute_utils.notify_about_instance_action(context, instance,
4309             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4310             phase=fields.NotificationPhase.START)
4311 
4312         # NOTE(tr3buchet): tear down networks on source host
4313         self.network_api.setup_networks_on_host(context, instance,
4314                            migration.source_compute, teardown=True)
4315         network_info = self.network_api.get_instance_nw_info(context,
4316                                                              instance)
4317 
4318         # NOTE(adrianc): Populate old PCI device in VIF profile
4319         # to allow virt driver to properly unplug it from Hypervisor.
4320         pci_mapping = (instance.migration_context.
4321                        get_pci_mapping_for_migration(True))
4322         network_info = self._get_updated_nw_info_with_pci_mapping(
4323             network_info, pci_mapping)
4324 
4325         self.driver.confirm_migration(context, migration, instance,
4326                                       network_info)
4327 
4328         migration.status = 'confirmed'
4329         migration.save()
4330 
4331         # NOTE(mriedem): drop_move_claim relies on
4332         # instance.migration_context so make sure to not call
4333         # instance.drop_migration_context() until after drop_move_claim
4334         # is called.
4335         self.rt.drop_move_claim(
4336             context, instance, migration.source_node, instance.old_flavor,
4337             prefix='old_')
4338         instance.drop_migration_context()
4339 
4340         # NOTE(mriedem): The old_vm_state could be STOPPED but the user
4341         # might have manually powered up the instance to confirm the
4342         # resize/migrate, so we need to check the current power state
4343         # on the instance and set the vm_state appropriately. We default
4344         # to ACTIVE because if the power state is not SHUTDOWN, we
4345         # assume _sync_instance_power_state will clean it up.
4346         p_state = instance.power_state
4347         vm_state = None
4348         if p_state == power_state.SHUTDOWN:
4349             vm_state = vm_states.STOPPED
4350             LOG.debug("Resized/migrated instance is powered off. "
4351                       "Setting vm_state to '%s'.", vm_state,
4352                       instance=instance)
4353         else:
4354             vm_state = vm_states.ACTIVE
4355 
4356         instance.vm_state = vm_state
4357         instance.task_state = None
4358         instance.save(expected_task_state=[None, task_states.DELETING,
4359                                            task_states.SOFT_DELETING])
4360 
4361         self._notify_about_instance_usage(
4362             context, instance, "resize.confirm.end",
4363             network_info=network_info)
4364         compute_utils.notify_about_instance_action(context, instance,
4365                self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4366                phase=fields.NotificationPhase.END)
4367 
4368     def _delete_allocation_after_move(self, context, instance, migration):
4369         """Deletes resource allocations held by the migration record against
4370         the source compute node resource provider after a confirmed cold /
4371         successful live migration.
4372         """
4373         try:
4374             # NOTE(danms): We're finishing on the source node, so try
4375             # to delete the allocation based on the migration uuid
4376             self.reportclient.delete_allocation_for_instance(
4377                 context, migration.uuid, consumer_type='migration')
4378         except exception.AllocationDeleteFailed:
4379             LOG.error('Deleting allocation in placement for migration '
4380                       '%(migration_uuid)s failed. The instance '
4381                       '%(instance_uuid)s will be put to ERROR state '
4382                       'but the allocation held by the migration is '
4383                       'leaked.',
4384                       {'instance_uuid': instance.uuid,
4385                        'migration_uuid': migration.uuid})
4386             raise
4387 
4388     def _delete_stashed_flavor_info(self, instance):
4389         """Remove information about the flavor change after a resize."""
4390         instance.old_flavor = None
4391         instance.new_flavor = None
4392         instance.system_metadata.pop('old_vm_state', None)
4393         instance.save()
4394 
4395     @wrap_exception()
4396     @wrap_instance_event(prefix='compute')
4397     @errors_out_migration
4398     @wrap_instance_fault
4399     def confirm_snapshot_based_resize_at_source(
4400             self, ctxt, instance, migration):
4401         """Confirms a snapshot-based resize on the source host.
4402 
4403         Cleans the guest from the source hypervisor including disks and drops
4404         the MoveClaim which will free up "old_flavor" usage from the
4405         ResourceTracker.
4406 
4407         Deletes the allocations held by the migration consumer against the
4408         source compute node resource provider.
4409 
4410         :param ctxt: nova auth request context targeted at the source cell
4411         :param instance: Instance object being resized which should have the
4412             "old_flavor" attribute set
4413         :param migration: Migration object for the resize operation
4414         """
4415 
4416         @utils.synchronized(instance.uuid)
4417         def do_confirm():
4418             LOG.info('Confirming resize on source host.', instance=instance)
4419             with self._error_out_instance_on_exception(ctxt, instance):
4420                 # TODO(mriedem): Could probably make this try/except/finally
4421                 # a context manager to share with confirm_resize().
4422                 try:
4423                     self._confirm_snapshot_based_resize_at_source(
4424                         ctxt, instance, migration)
4425                 except Exception:
4426                     # Something failed when cleaning up the source host so
4427                     # log a traceback and leave a hint about hard rebooting
4428                     # the server to correct its state in the DB.
4429                     with excutils.save_and_reraise_exception(logger=LOG):
4430                         LOG.exception(
4431                             'Confirm resize failed on source host %s. '
4432                             'Resource allocations in the placement service '
4433                             'will be removed regardless because the instance '
4434                             'is now on the destination host %s. You can try '
4435                             'hard rebooting the instance to correct its '
4436                             'state.', self.host, migration.dest_compute,
4437                             instance=instance)
4438                 finally:
4439                     # Whether an error occurred or not, at this point the
4440                     # instance is on the dest host so to avoid leaking
4441                     # allocations in placement, delete them here.
4442                     # TODO(mriedem): Should we catch and just log
4443                     # AllocationDeleteFailed? What is the user's recourse if
4444                     # we got this far but this fails? At this point the
4445                     # instance is on the target host and the allocations
4446                     # could just be manually cleaned up by the operator.
4447                     self._delete_allocation_after_move(ctxt, instance,
4448                                                        migration)
4449         do_confirm()
4450 
4451     def _confirm_snapshot_based_resize_at_source(
4452             self, ctxt, instance, migration):
4453         """Private version of confirm_snapshot_based_resize_at_source
4454 
4455         This allows the main method to be decorated with error handlers.
4456 
4457         :param ctxt: nova auth request context targeted at the source cell
4458         :param instance: Instance object being resized which should have the
4459             "old_flavor" attribute set
4460         :param migration: Migration object for the resize operation
4461         """
4462         # Cleanup the guest from the hypervisor including local disks.
4463         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4464         LOG.debug('Cleaning up guest from source hypervisor including disks.',
4465                   instance=instance)
4466 
4467         # FIXME(mriedem): Per bug 1809095, _confirm_resize calls
4468         # _get_updated_nw_info_with_pci_mapping here prior to unplugging
4469         # VIFs on the source, but in our case we have already unplugged
4470         # VIFs during prep_snapshot_based_resize_at_source, so what do we
4471         # need to do about those kinds of ports? Do we need to wait to unplug
4472         # VIFs until confirm like normal resize?
4473 
4474         # Note that prep_snapshot_based_resize_at_source already destroyed the
4475         # guest which disconnected volumes and unplugged VIFs but did not
4476         # destroy disks in case something failed during the resize and the
4477         # instance needed to be rebooted or rebuilt on the source host. Now
4478         # that we are confirming the resize we want to cleanup the disks left
4479         # on the source host. We call cleanup() instead of destroy() to avoid
4480         # any InstanceNotFound confusion from the driver since the guest was
4481         # already destroyed on this host. block_device_info=None and
4482         # destroy_vifs=False means cleanup() will not try to disconnect volumes
4483         # or unplug VIFs.
4484         self.driver.cleanup(
4485             ctxt, instance, network_info, block_device_info=None,
4486             destroy_disks=True, destroy_vifs=False)
4487 
4488         # Delete port bindings for the source host.
4489         self._confirm_snapshot_based_resize_delete_port_bindings(
4490             ctxt, instance)
4491 
4492         # Delete volume attachments for the source host.
4493         self._delete_volume_attachments(ctxt, instance.get_bdms())
4494 
4495         # Free up the old_flavor usage from the resource tracker for this host.
4496         self.rt.drop_move_claim(
4497             ctxt, instance, migration.source_node, instance.old_flavor,
4498             prefix='old_')
4499         instance.drop_migration_context()
4500 
4501         migration.status = 'confirmed'
4502         migration.save()
4503 
4504     def _confirm_snapshot_based_resize_delete_port_bindings(
4505             self, ctxt, instance):
4506         """Delete port bindings for the source host when confirming
4507         snapshot-based resize on the source host."
4508 
4509         :param ctxt: nova auth RequestContext
4510         :param instance: Instance object that was resized/cold migrated
4511         """
4512         LOG.debug('Deleting port bindings for source host.',
4513                   instance=instance)
4514         try:
4515             self.network_api.cleanup_instance_network_on_host(
4516                 ctxt, instance, self.host)
4517         except exception.PortBindingDeletionFailed as e:
4518             # Do not let this stop us from cleaning up since the guest
4519             # is already gone.
4520             LOG.error('Failed to delete port bindings from source host. '
4521                       'Error: %s', six.text_type(e), instance=instance)
4522 
4523     def _delete_volume_attachments(self, ctxt, bdms):
4524         """Deletes volume attachment records for the given bdms.
4525 
4526         This method will log but not re-raise any exceptions if the volume
4527         attachment delete fails.
4528 
4529         :param ctxt: nova auth request context used to make
4530             DELETE /attachments/{attachment_id} requests to cinder.
4531         :param bdms: objects.BlockDeviceMappingList representing volume
4532             attachments to delete based on BlockDeviceMapping.attachment_id.
4533         """
4534         for bdm in bdms:
4535             if bdm.attachment_id:
4536                 try:
4537                     self.volume_api.attachment_delete(ctxt, bdm.attachment_id)
4538                 except Exception as e:
4539                     LOG.error('Failed to delete volume attachment with ID %s. '
4540                               'Error: %s', bdm.attachment_id, six.text_type(e),
4541                               instance_uuid=bdm.instance_uuid)
4542 
4543     @wrap_exception()
4544     @reverts_task_state
4545     @wrap_instance_event(prefix='compute')
4546     @errors_out_migration
4547     @wrap_instance_fault
4548     def revert_snapshot_based_resize_at_dest(self, ctxt, instance, migration):
4549         """Reverts a snapshot-based resize at the destination host.
4550 
4551         Cleans the guest from the destination compute service host hypervisor
4552         and related resources (ports, volumes) and frees resource usage from
4553         the compute service on that host.
4554 
4555         :param ctxt: nova auth request context targeted at the target cell
4556         :param instance: Instance object whose vm_state is "resized" and
4557             task_state is "resize_reverting".
4558         :param migration: Migration object whose status is "reverting".
4559         """
4560         # A resize revert is essentially a resize back to the old size, so we
4561         # need to send a usage event here.
4562         compute_utils.notify_usage_exists(
4563             self.notifier, ctxt, instance, self.host, current_period=True)
4564 
4565         @utils.synchronized(instance.uuid)
4566         def do_revert():
4567             LOG.info('Reverting resize on destination host.',
4568                      instance=instance)
4569             with self._error_out_instance_on_exception(ctxt, instance):
4570                 self._revert_snapshot_based_resize_at_dest(
4571                     ctxt, instance, migration)
4572         do_revert()
4573 
4574         # Broadcast to all schedulers that the instance is no longer on
4575         # this host and clear any waiting callback events. This is best effort
4576         # so if anything fails just log it.
4577         try:
4578             self._delete_scheduler_instance_info(ctxt, instance.uuid)
4579             self.instance_events.clear_events_for_instance(instance)
4580         except Exception as e:
4581             LOG.warning('revert_snapshot_based_resize_at_dest failed during '
4582                         'post-processing. Error: %s', e, instance=instance)
4583 
4584     def _revert_snapshot_based_resize_at_dest(
4585             self, ctxt, instance, migration):
4586         """Private version of revert_snapshot_based_resize_at_dest.
4587 
4588         This allows the main method to be decorated with error handlers.
4589 
4590         :param ctxt: nova auth request context targeted at the target cell
4591         :param instance: Instance object whose vm_state is "resized" and
4592             task_state is "resize_reverting".
4593         :param migration: Migration object whose status is "reverting".
4594         """
4595         # Cleanup the guest from the hypervisor including local disks.
4596         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4597         bdms = instance.get_bdms()
4598         block_device_info = self._get_instance_block_device_info(
4599             ctxt, instance, bdms=bdms)
4600         LOG.debug('Destroying guest from destination hypervisor including '
4601                   'disks.', instance=instance)
4602         self.driver.destroy(
4603             ctxt, instance, network_info, block_device_info=block_device_info)
4604 
4605         # Activate source host port bindings. We need to do this before
4606         # deleting the (active) dest host port bindings in
4607         # setup_networks_on_host otherwise the ports will be unbound and
4608         # finish on the source will fail.
4609         # migrate_instance_start uses migration.dest_compute for the port
4610         # binding host and since we want to activate the source host port
4611         # bindings, we need to temporarily mutate the migration object.
4612         with utils.temporary_mutation(
4613                 migration, dest_compute=migration.source_compute):
4614             LOG.debug('Activating port bindings for source host %s.',
4615                       migration.source_compute, instance=instance)
4616             # TODO(mriedem): https://review.opendev.org/#/c/594139/ would allow
4617             # us to remove this and make setup_networks_on_host do it.
4618             # TODO(mriedem): Should we try/except/log any errors but continue?
4619             self.network_api.migrate_instance_start(
4620                 ctxt, instance, migration)
4621 
4622         # Delete port bindings for the target host.
4623         LOG.debug('Deleting port bindings for target host %s.',
4624                   self.host, instance=instance)
4625         try:
4626             # Note that deleting the destination host port bindings does
4627             # not automatically activate the source host port bindings.
4628             self.network_api.cleanup_instance_network_on_host(
4629                 ctxt, instance, self.host)
4630         except exception.PortBindingDeletionFailed as e:
4631             # Do not let this stop us from cleaning up since the guest
4632             # is already gone.
4633             LOG.error('Failed to delete port bindings from target host. '
4634                       'Error: %s', six.text_type(e), instance=instance)
4635 
4636         # Delete any volume attachments remaining for this target host.
4637         LOG.debug('Deleting volume attachments for target host.',
4638                   instance=instance)
4639         self._delete_volume_attachments(ctxt, bdms)
4640 
4641         # Free up the new_flavor usage from the resource tracker for this host.
4642         instance.revert_migration_context()
4643         instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4644         self.rt.drop_move_claim(ctxt, instance, instance.node,
4645                                 instance_type=instance.new_flavor)
4646 
4647     def _revert_instance_flavor_host_node(self, instance, migration):
4648         """Revert host, node and flavor fields after a resize-revert."""
4649         self._set_instance_info(instance, instance.old_flavor)
4650         instance.host = migration.source_compute
4651         instance.node = migration.source_node
4652         instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4653 
4654     @wrap_exception()
4655     @reverts_task_state
4656     @wrap_instance_event(prefix='compute')
4657     @errors_out_migration
4658     @wrap_instance_fault
4659     def finish_revert_snapshot_based_resize_at_source(
4660             self, ctxt, instance, migration):
4661         """Reverts a snapshot-based resize at the source host.
4662 
4663         Spawn the guest and re-connect volumes/VIFs on the source host and
4664         revert the instance to use the old_flavor for resource usage reporting.
4665 
4666         Updates allocations in the placement service to move the source node
4667         allocations, held by the migration record, to the instance and drop
4668         the allocations held by the instance on the destination node.
4669 
4670         :param ctxt: nova auth request context targeted at the target cell
4671         :param instance: Instance object whose vm_state is "resized" and
4672             task_state is "resize_reverting".
4673         :param migration: Migration object whose status is "reverting".
4674         """
4675 
4676         @utils.synchronized(instance.uuid)
4677         def do_revert():
4678             LOG.info('Reverting resize on source host.', instance=instance)
4679             with self._error_out_instance_on_exception(ctxt, instance):
4680                 self._finish_revert_snapshot_based_resize_at_source(
4681                     ctxt, instance, migration)
4682 
4683         try:
4684             do_revert()
4685         finally:
4686             self._delete_stashed_flavor_info(instance)
4687 
4688         # Broadcast to all schedulers that the instance is on this host.
4689         # This is best effort so if anything fails just log it.
4690         try:
4691             self._update_scheduler_instance_info(ctxt, instance)
4692         except Exception as e:
4693             LOG.warning('finish_revert_snapshot_based_resize_at_source failed '
4694                         'during post-processing. Error: %s', e,
4695                         instance=instance)
4696 
4697     def _finish_revert_snapshot_based_resize_at_source(
4698             self, ctxt, instance, migration):
4699         """Private version of finish_revert_snapshot_based_resize_at_source.
4700 
4701         This allows the main method to be decorated with error handlers.
4702 
4703         :param ctxt: nova auth request context targeted at the source cell
4704         :param instance: Instance object whose vm_state is "resized" and
4705             task_state is "resize_reverting".
4706         :param migration: Migration object whose status is "reverting".
4707         """
4708         # Get stashed old_vm_state information to determine if guest should
4709         # be powered on after spawn; we default to ACTIVE for backwards
4710         # compatibility if old_vm_state is not set
4711         old_vm_state = instance.system_metadata.get(
4712             'old_vm_state', vm_states.ACTIVE)
4713 
4714         # Revert the flavor and host/node fields to their previous values
4715         self._revert_instance_flavor_host_node(instance, migration)
4716 
4717         # Move the allocations against the source compute node resource
4718         # provider, held by the migration, to the instance which will drop
4719         # the destination compute node resource provider allocations held by
4720         # the instance. This puts the allocations against the source node
4721         # back to the old_flavor and owned by the instance.
4722         try:
4723             self._revert_allocation(ctxt, instance, migration)
4724         except exception.AllocationMoveFailed:
4725             # Log the error but do not re-raise because we want to continue to
4726             # process ports and volumes below.
4727             LOG.error('Reverting allocation in placement for migration '
4728                       '%(migration_uuid)s failed. You may need to manually '
4729                       'remove the allocations for the migration consumer '
4730                       'against the source node resource provider '
4731                       '%(source_provider)s and the allocations for the '
4732                       'instance consumer against the destination node '
4733                       'resource provider %(dest_provider)s and then run the '
4734                       '"nova-manage placement heal_allocations" command.',
4735                       {'instance_uuid': instance.uuid,
4736                        'migration_uuid': migration.uuid,
4737                        'source_provider': migration.source_node,
4738                        'dest_provider': migration.dest_node},
4739                       instance=instance)
4740 
4741         bdms = instance.get_bdms()
4742         # prep_snapshot_based_resize_at_source created empty volume attachments
4743         # that we need to update here to get the connection_info before calling
4744         # driver.finish_revert_migration which will connect the volumes to this
4745         # host.
4746         LOG.debug('Updating volume attachments for target host %s.',
4747                   self.host, instance=instance)
4748         # TODO(mriedem): We should probably make _update_volume_attachments
4749         # (optionally) graceful to errors so we (1) try to process all
4750         # attachments and (2) continue to process networking below.
4751         self._update_volume_attachments(ctxt, instance, bdms)
4752 
4753         LOG.debug('Updating port bindings for source host %s.',
4754                   self.host, instance=instance)
4755         # TODO(mriedem): Calculate provider mappings when we support
4756         # cross-cell resize/migrate with ports having resource requests.
4757         self._finish_revert_resize_network_migrate_finish(
4758             ctxt, instance, migration, provider_mappings=None)
4759         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4760 
4761         # Remember that prep_snapshot_based_resize_at_source destroyed the
4762         # guest but left the disks intact so we cannot call spawn() here but
4763         # finish_revert_migration should do the job.
4764         block_device_info = self._get_instance_block_device_info(
4765             ctxt, instance, bdms=bdms)
4766         power_on = old_vm_state == vm_states.ACTIVE
4767         driver_error = None
4768         try:
4769             self.driver.finish_revert_migration(
4770                 ctxt, instance, network_info, migration,
4771                 block_device_info=block_device_info, power_on=power_on)
4772         except Exception as e:
4773             driver_error = e
4774             # Leave a hint about hard rebooting the guest and reraise so the
4775             # instance is put into ERROR state.
4776             with excutils.save_and_reraise_exception(logger=LOG):
4777                 LOG.error('An error occurred during finish_revert_migration. '
4778                           'The instance may need to be hard rebooted. Error: '
4779                           '%s', driver_error, instance=instance)
4780         else:
4781             # Perform final cleanup of the instance in the database.
4782             instance.drop_migration_context()
4783             # If the original vm_state was STOPPED, set it back to STOPPED.
4784             vm_state = vm_states.ACTIVE if power_on else vm_states.STOPPED
4785             self._update_instance_after_spawn(instance, vm_state=vm_state)
4786             instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4787         finally:
4788             # Complete any volume attachments so the volumes are in-use. We
4789             # do this regardless of finish_revert_migration failing because
4790             # the instance is back on this host now and we do not want to leave
4791             # the volumes in a pending state in case the instance is hard
4792             # rebooted.
4793             LOG.debug('Completing volume attachments for instance on source '
4794                       'host.', instance=instance)
4795             with excutils.save_and_reraise_exception(
4796                     reraise=driver_error is not None, logger=LOG):
4797                 self._complete_volume_attachments(ctxt, bdms)
4798 
4799         migration.status = 'reverted'
4800         migration.save()
4801 
4802     @wrap_exception()
4803     @reverts_task_state
4804     @wrap_instance_event(prefix='compute')
4805     @errors_out_migration
4806     @wrap_instance_fault
4807     def revert_resize(self, context, instance, migration, request_spec=None):
4808         """Destroys the new instance on the destination machine.
4809 
4810         Reverts the model changes, and powers on the old instance on the
4811         source machine.
4812 
4813         """
4814         # NOTE(comstud): A revert_resize is essentially a resize back to
4815         # the old size, so we need to send a usage event here.
4816         compute_utils.notify_usage_exists(self.notifier, context, instance,
4817                                           self.host, current_period=True)
4818 
4819         with self._error_out_instance_on_exception(context, instance):
4820             # NOTE(tr3buchet): tear down networks on destination host
4821             self.network_api.setup_networks_on_host(context, instance,
4822                                                     teardown=True)
4823 
4824             self.network_api.migrate_instance_start(context,
4825                                                     instance,
4826                                                     migration)
4827 
4828             network_info = self.network_api.get_instance_nw_info(context,
4829                                                                  instance)
4830             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4831                     context, instance.uuid)
4832             block_device_info = self._get_instance_block_device_info(
4833                                 context, instance, bdms=bdms)
4834 
4835             destroy_disks = not self._is_instance_storage_shared(
4836                 context, instance, host=migration.source_compute)
4837             self.driver.destroy(context, instance, network_info,
4838                                 block_device_info, destroy_disks)
4839 
4840             self._terminate_volume_connections(context, instance, bdms)
4841 
4842             migration.status = 'reverted'
4843             migration.save()
4844 
4845             # NOTE(ndipanov): We need to do this here because dropping the
4846             # claim means we lose the migration_context data. We really should
4847             # fix this by moving the drop_move_claim call to the
4848             # finish_revert_resize method as this is racy (revert is dropped,
4849             # but instance resources will be tracked with the new flavor until
4850             # it gets rolled back in finish_revert_resize, which is
4851             # potentially wrong for a period of time).
4852             instance.revert_migration_context()
4853             instance.save()
4854 
4855             self.rt.drop_move_claim(context, instance, instance.node)
4856 
4857             # RPC cast back to the source host to finish the revert there.
4858             self.compute_rpcapi.finish_revert_resize(context, instance,
4859                     migration, migration.source_compute, request_spec)
4860 
4861     def _finish_revert_resize_network_migrate_finish(
4862             self, context, instance, migration, provider_mappings):
4863         """Causes port binding to be updated. In some Neutron or port
4864         configurations - see NetworkModel.get_bind_time_events() - we
4865         expect the vif-plugged event from Neutron immediately and wait for it.
4866         The rest of the time, the event is expected further along in the
4867         virt driver, so we don't wait here.
4868 
4869         :param context: The request context.
4870         :param instance: The instance undergoing the revert resize.
4871         :param migration: The Migration object of the resize being reverted.
4872         :param provider_mappings: a dict of list of resource provider uuids
4873             keyed by port uuid
4874         :raises: eventlet.timeout.Timeout or
4875                  exception.VirtualInterfacePlugException.
4876         """
4877         network_info = instance.get_network_info()
4878         events = []
4879         deadline = CONF.vif_plugging_timeout
4880         if deadline and network_info:
4881             events = network_info.get_bind_time_events(migration)
4882             if events:
4883                 LOG.debug('Will wait for bind-time events: %s', events)
4884         error_cb = self._neutron_failed_migration_callback
4885         try:
4886             with self.virtapi.wait_for_instance_event(instance, events,
4887                                                       deadline=deadline,
4888                                                       error_callback=error_cb):
4889                 # NOTE(hanrong): we need to change migration.dest_compute to
4890                 # source host temporarily.
4891                 # "network_api.migrate_instance_finish" will setup the network
4892                 # for the instance on the destination host. For revert resize,
4893                 # the instance will back to the source host, the setup of the
4894                 # network for instance should be on the source host. So set
4895                 # the migration.dest_compute to source host at here.
4896                 with utils.temporary_mutation(
4897                         migration, dest_compute=migration.source_compute):
4898                     self.network_api.migrate_instance_finish(
4899                         context, instance, migration, provider_mappings)
4900         except eventlet.timeout.Timeout:
4901             with excutils.save_and_reraise_exception():
4902                 LOG.error('Timeout waiting for Neutron events: %s', events,
4903                           instance=instance)
4904 
4905     @wrap_exception()
4906     @reverts_task_state
4907     @wrap_instance_event(prefix='compute')
4908     @errors_out_migration
4909     @wrap_instance_fault
4910     def finish_revert_resize(
4911             self, context, instance, migration, request_spec=None):
4912         """Finishes the second half of reverting a resize on the source host.
4913 
4914         Bring the original source instance state back (active/shutoff) and
4915         revert the resized attributes in the database.
4916 
4917         """
4918         try:
4919             self._finish_revert_resize(
4920                 context, instance, migration, request_spec)
4921         finally:
4922             self._delete_stashed_flavor_info(instance)
4923 
4924     def _finish_revert_resize(
4925         self, context, instance, migration, request_spec=None,
4926     ):
4927         """Inner version of finish_revert_resize."""
4928         with self._error_out_instance_on_exception(context, instance):
4929             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4930                 context, instance.uuid)
4931             self._notify_about_instance_usage(
4932                     context, instance, "resize.revert.start")
4933             compute_utils.notify_about_instance_action(context, instance,
4934                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4935                     phase=fields.NotificationPhase.START, bdms=bdms)
4936 
4937             # Get stashed old_vm_state information to determine if guest should
4938             # be powered on after spawn; we default to ACTIVE for backwards
4939             # compatibility if old_vm_state is not set
4940             old_vm_state = instance.system_metadata.get(
4941                 'old_vm_state', vm_states.ACTIVE)
4942 
4943             # Revert the flavor and host/node fields to their previous values
4944             self._revert_instance_flavor_host_node(instance, migration)
4945 
4946             try:
4947                 source_allocations = self._revert_allocation(
4948                     context, instance, migration)
4949             except exception.AllocationMoveFailed:
4950                 LOG.error('Reverting allocation in placement for migration '
4951                           '%(migration_uuid)s failed. The instance '
4952                           '%(instance_uuid)s will be put into ERROR state but '
4953                           'the allocation held by the migration is leaked.',
4954                           {'instance_uuid': instance.uuid,
4955                            'migration_uuid': migration.uuid})
4956                 raise
4957 
4958             provider_mappings = self._fill_provider_mapping_based_on_allocs(
4959                 context, source_allocations, request_spec)
4960 
4961             self.network_api.setup_networks_on_host(context, instance,
4962                                                     migration.source_compute)
4963             self._finish_revert_resize_network_migrate_finish(
4964                 context, instance, migration, provider_mappings)
4965             network_info = self.network_api.get_instance_nw_info(context,
4966                                                                  instance)
4967 
4968             # revert_resize deleted any volume attachments for the instance
4969             # and created new ones to be used on this host, but we
4970             # have to update those attachments with the host connector so the
4971             # BDM.connection_info will get set in the call to
4972             # _get_instance_block_device_info below with refresh_conn_info=True
4973             # and then the volumes can be re-connected via the driver on this
4974             # host.
4975             self._update_volume_attachments(context, instance, bdms)
4976 
4977             block_device_info = self._get_instance_block_device_info(
4978                     context, instance, refresh_conn_info=True, bdms=bdms)
4979 
4980             power_on = old_vm_state != vm_states.STOPPED
4981             self.driver.finish_revert_migration(
4982                 context, instance, network_info, migration, block_device_info,
4983                 power_on)
4984 
4985             instance.drop_migration_context()
4986             instance.launched_at = timeutils.utcnow()
4987             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4988 
4989             # Complete any volume attachments so the volumes are in-use.
4990             self._complete_volume_attachments(context, bdms)
4991 
4992             # if the original vm state was STOPPED, set it back to STOPPED
4993             LOG.info("Updating instance to original state: '%s'",
4994                      old_vm_state, instance=instance)
4995             if power_on:
4996                 instance.vm_state = vm_states.ACTIVE
4997                 instance.task_state = None
4998                 instance.save()
4999             else:
5000                 instance.task_state = task_states.POWERING_OFF
5001                 instance.save()
5002                 self.stop_instance(context, instance=instance,
5003                                    clean_shutdown=True)
5004 
5005             self._notify_about_instance_usage(
5006                     context, instance, "resize.revert.end")
5007             compute_utils.notify_about_instance_action(context, instance,
5008                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
5009                     phase=fields.NotificationPhase.END, bdms=bdms)
5010 
5011     def _fill_provider_mapping_based_on_allocs(
5012             self, context, allocations, request_spec):
5013         """Fills and returns the request group - resource provider mapping
5014         based on the allocation passed in.
5015 
5016         :param context: The security context
5017         :param allocation: allocation dict keyed by RP UUID.
5018         :param request_spec: The RequestSpec object associated with the
5019             operation
5020         :returns: None if the request_spec is None. Otherwise a mapping
5021             between RequestGroup requester_id, currently Neutron port_id,
5022             and a list of resource provider UUIDs providing resource for
5023             that RequestGroup.
5024         """
5025         if request_spec:
5026             # NOTE(gibi): We need to re-calculate the resource provider -
5027             # port mapping as we have to have the neutron ports allocate
5028             # from the source compute after revert.
5029             scheduler_utils.fill_provider_mapping_based_on_allocation(
5030                 context, self.reportclient, request_spec, allocations)
5031             provider_mappings = self._get_request_group_mapping(
5032                 request_spec)
5033         else:
5034             # NOTE(gibi): The compute RPC is pinned to be older than 5.2
5035             # and therefore request_spec is not sent. We cannot calculate
5036             # the provider mappings. If the instance has ports with
5037             # resource request then the port update will fail in
5038             # _update_port_binding_for_instance() called via
5039             # _finish_revert_resize_network_migrate_finish() in
5040             # finish_revert_resize.
5041             provider_mappings = None
5042         return provider_mappings
5043 
5044     def _revert_allocation(self, context, instance, migration):
5045         """Revert an allocation that is held by migration to our instance."""
5046 
5047         # Fetch the original allocation that the instance had on the source
5048         # node, which are now held by the migration
5049         orig_alloc = self.reportclient.get_allocations_for_consumer(
5050             context, migration.uuid)
5051         if not orig_alloc:
5052             LOG.error('Did not find resource allocations for migration '
5053                       '%s on source node %s. Unable to revert source node '
5054                       'allocations back to the instance.',
5055                       migration.uuid, migration.source_node, instance=instance)
5056             return False
5057 
5058         LOG.info('Swapping old allocation on %(rp_uuids)s held by migration '
5059                  '%(mig)s for instance',
5060                  {'rp_uuids': orig_alloc.keys(), 'mig': migration.uuid},
5061                  instance=instance)
5062         # FIXME(gibi): This method is flawed in that it does not handle
5063         # allocations against sharing providers in any special way. This leads
5064         # to duplicate allocations against the sharing provider during
5065         # migration.
5066         # TODO(cdent): Should we be doing anything with return values here?
5067         self.reportclient.move_allocations(context, migration.uuid,
5068                                            instance.uuid)
5069         return orig_alloc
5070 
5071     def _prep_resize(self, context, image, instance, instance_type,
5072                      filter_properties, node, migration, request_spec,
5073                      clean_shutdown=True):
5074 
5075         if not filter_properties:
5076             filter_properties = {}
5077 
5078         if not instance.host:
5079             self._set_instance_obj_error_state(instance)
5080             msg = _('Instance has no source host')
5081             raise exception.MigrationError(reason=msg)
5082 
5083         same_host = instance.host == self.host
5084         # if the flavor IDs match, it's migrate; otherwise resize
5085         if same_host and instance_type.id == instance['instance_type_id']:
5086             # check driver whether support migrate to same host
5087             if not self.driver.capabilities.get(
5088                     'supports_migrate_to_same_host', False):
5089                 # Raise InstanceFaultRollback so that the
5090                 # _error_out_instance_on_exception context manager in
5091                 # prep_resize will set the instance.vm_state properly.
5092                 raise exception.InstanceFaultRollback(
5093                     inner_exception=exception.UnableToMigrateToSelf(
5094                         instance_id=instance.uuid, host=self.host))
5095 
5096         # NOTE(danms): Stash the new instance_type to avoid having to
5097         # look it up in the database later
5098         instance.new_flavor = instance_type
5099         # NOTE(mriedem): Stash the old vm_state so we can set the
5100         # resized/reverted instance back to the same state later.
5101         vm_state = instance.vm_state
5102         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
5103         instance.system_metadata['old_vm_state'] = vm_state
5104         instance.save()
5105 
5106         if not isinstance(request_spec, objects.RequestSpec):
5107             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
5108             # version of the request spec to compute and since Stein compute
5109             # could be sending that back to conductor on reschedule, so if we
5110             # got a dict convert it to an object.
5111             # TODO(mriedem): We can drop this compat code when we only support
5112             # compute RPC API >=6.0.
5113             request_spec = objects.RequestSpec.from_primitives(
5114                 context, request_spec, filter_properties)
5115             # We don't have to set the new flavor on the request spec because
5116             # if we got here it was due to a reschedule from the compute and
5117             # the request spec would already have the new flavor in it from the
5118             # else block below.
5119 
5120         provider_mapping = self._get_request_group_mapping(request_spec)
5121 
5122         if provider_mapping:
5123             try:
5124                 compute_utils.\
5125                     update_pci_request_spec_with_allocated_interface_name(
5126                         context, self.reportclient, instance, provider_mapping)
5127             except (exception.AmbiguousResourceProviderForPCIRequest,
5128                     exception.UnexpectedResourceProviderNameForPCIRequest
5129                     ) as e:
5130                 raise exception.BuildAbortException(
5131                     reason=six.text_type(e), instance_uuid=instance.uuid)
5132 
5133         limits = filter_properties.get('limits', {})
5134         allocs = self.reportclient.get_allocations_for_consumer(
5135             context, instance.uuid)
5136         with self.rt.resize_claim(context, instance, instance_type, node,
5137                                   migration, allocs, image_meta=image,
5138                                   limits=limits) as claim:
5139             LOG.info('Migrating', instance=instance)
5140             # RPC cast to the source host to start the actual resize/migration.
5141             self.compute_rpcapi.resize_instance(
5142                     context, instance, claim.migration, image,
5143                     instance_type, request_spec, clean_shutdown)
5144 
5145     def _send_prep_resize_notifications(
5146             self, context, instance, phase, flavor):
5147         """Send "resize.prep.*" notifications.
5148 
5149         :param context: nova auth request context
5150         :param instance: The instance being resized
5151         :param phase: The phase of the action (NotificationPhase enum)
5152         :param flavor: The (new) flavor for the resize (same as existing
5153             instance.flavor for a cold migration)
5154         """
5155         # Only send notify_usage_exists if it's the "start" phase.
5156         if phase == fields.NotificationPhase.START:
5157             compute_utils.notify_usage_exists(
5158                 self.notifier, context, instance, self.host,
5159                 current_period=True)
5160 
5161         # Send extra usage info about the flavor if it's the "end" phase for
5162         # the legacy unversioned notification.
5163         extra_usage_info = None
5164         if phase == fields.NotificationPhase.END:
5165             extra_usage_info = dict(
5166                 new_instance_type=flavor.name,
5167                 new_instance_type_id=flavor.id)
5168         self._notify_about_instance_usage(
5169             context, instance, "resize.prep.%s" % phase,
5170             extra_usage_info=extra_usage_info)
5171 
5172         # Send the versioned notification.
5173         compute_utils.notify_about_resize_prep_instance(
5174             context, instance, self.host, phase, flavor)
5175 
5176     @wrap_exception()
5177     @reverts_task_state
5178     @wrap_instance_event(prefix='compute')
5179     @wrap_instance_fault
5180     def prep_resize(self, context, image, instance, instance_type,
5181                     request_spec, filter_properties, node,
5182                     clean_shutdown, migration, host_list):
5183         """Initiates the process of moving a running instance to another host.
5184 
5185         Possibly changes the VCPU, RAM and disk size in the process.
5186 
5187         This is initiated from conductor and runs on the destination host.
5188 
5189         The main purpose of this method is performing some checks on the
5190         destination host and making a claim for resources. If the claim fails
5191         then a reschedule to another host may be attempted which involves
5192         calling back to conductor to start the process over again.
5193         """
5194         if node is None:
5195             node = self._get_nodename(instance, refresh=True)
5196 
5197         # Pass instance_state=instance.vm_state because we can resize
5198         # a STOPPED server and we don't want to set it back to ACTIVE
5199         # in case _prep_resize fails.
5200         instance_state = instance.vm_state
5201         with self._error_out_instance_on_exception(
5202                 context, instance, instance_state=instance_state),\
5203                 errors_out_migration_ctxt(migration):
5204             self._send_prep_resize_notifications(
5205                 context, instance, fields.NotificationPhase.START,
5206                 instance_type)
5207             try:
5208                 self._prep_resize(context, image, instance,
5209                                   instance_type, filter_properties,
5210                                   node, migration, request_spec,
5211                                   clean_shutdown)
5212             except exception.BuildAbortException:
5213                 # NOTE(gibi): We failed
5214                 # update_pci_request_spec_with_allocated_interface_name so
5215                 # there is no reason to re-schedule. Just revert the allocation
5216                 # and fail the migration.
5217                 with excutils.save_and_reraise_exception():
5218                     self._revert_allocation(context, instance, migration)
5219             except Exception:
5220                 # Since we hit a failure, we're either rescheduling or dead
5221                 # and either way we need to cleanup any allocations created
5222                 # by the scheduler for the destination node.
5223                 self._revert_allocation(context, instance, migration)
5224                 # try to re-schedule the resize elsewhere:
5225                 exc_info = sys.exc_info()
5226                 self._reschedule_resize_or_reraise(context, instance,
5227                         exc_info, instance_type, request_spec,
5228                         filter_properties, host_list)
5229             finally:
5230                 self._send_prep_resize_notifications(
5231                     context, instance, fields.NotificationPhase.END,
5232                     instance_type)
5233 
5234     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
5235             instance_type, request_spec, filter_properties, host_list):
5236         """Try to re-schedule the resize or re-raise the original error to
5237         error out the instance.
5238         """
5239         if not filter_properties:
5240             filter_properties = {}
5241 
5242         rescheduled = False
5243         instance_uuid = instance.uuid
5244 
5245         try:
5246             retry = filter_properties.get('retry')
5247             if retry:
5248                 LOG.debug('Rescheduling, attempt %d', retry['num_attempts'],
5249                           instance_uuid=instance_uuid)
5250 
5251                 # reset the task state
5252                 task_state = task_states.RESIZE_PREP
5253                 self._instance_update(context, instance, task_state=task_state)
5254 
5255                 if exc_info:
5256                     # stringify to avoid circular ref problem in json
5257                     # serialization
5258                     retry['exc'] = traceback.format_exception_only(
5259                         exc_info[0], exc_info[1])
5260 
5261                 scheduler_hint = {'filter_properties': filter_properties}
5262 
5263                 self.compute_task_api.resize_instance(
5264                     context, instance, scheduler_hint, instance_type,
5265                     request_spec=request_spec, host_list=host_list)
5266 
5267                 rescheduled = True
5268             else:
5269                 # no retry information, do not reschedule.
5270                 LOG.debug('Retry info not present, will not reschedule',
5271                           instance_uuid=instance_uuid)
5272                 rescheduled = False
5273         except Exception as error:
5274             rescheduled = False
5275             LOG.exception("Error trying to reschedule",
5276                           instance_uuid=instance_uuid)
5277             compute_utils.add_instance_fault_from_exc(context,
5278                     instance, error,
5279                     exc_info=sys.exc_info())
5280             self._notify_about_instance_usage(context, instance,
5281                     'resize.error', fault=error)
5282             compute_utils.notify_about_instance_action(
5283                 context, instance, self.host,
5284                 action=fields.NotificationAction.RESIZE,
5285                 phase=fields.NotificationPhase.ERROR,
5286                 exception=error,
5287             )
5288 
5289         if rescheduled:
5290             self._log_original_error(exc_info, instance_uuid)
5291             compute_utils.add_instance_fault_from_exc(context,
5292                     instance, exc_info[1], exc_info=exc_info)
5293             self._notify_about_instance_usage(context, instance,
5294                     'resize.error', fault=exc_info[1])
5295             compute_utils.notify_about_instance_action(
5296                 context, instance, self.host,
5297                 action=fields.NotificationAction.RESIZE,
5298                 phase=fields.NotificationPhase.ERROR,
5299                 exception=exc_info[1],
5300             )
5301         else:
5302             # not re-scheduling
5303             if exc_info[1] is None:
5304                 exc_info[1] = exc_info[0]()
5305             if exc_info[1].__traceback__ is not exc_info[2]:
5306                 raise exc_info[1].with_traceback(exc_info[2])
5307             raise exc_info[1]
5308 
5309     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5310     @messaging.expected_exceptions(exception.MigrationPreCheckError)
5311     @wrap_exception()
5312     @wrap_instance_event(prefix='compute')
5313     @wrap_instance_fault
5314     def prep_snapshot_based_resize_at_dest(
5315             self, ctxt, instance, flavor, nodename, migration, limits,
5316             request_spec):
5317         """Performs pre-cross-cell resize resource claim on the dest host.
5318 
5319         This runs on the destination host in a cross-cell resize operation
5320         before the resize is actually started.
5321 
5322         Performs a resize_claim for resources that are not claimed in placement
5323         like PCI devices and NUMA topology.
5324 
5325         Note that this is different from same-cell prep_resize in that this:
5326 
5327         * Does not RPC cast to the source compute, that is orchestrated from
5328           conductor.
5329         * This does not reschedule on failure, conductor handles that since
5330           conductor is synchronously RPC calling this method. As such, the
5331           reverts_task_state decorator is not used on this method.
5332 
5333         :param ctxt: user auth request context
5334         :param instance: the instance being resized
5335         :param flavor: the flavor being resized to (unchanged for cold migrate)
5336         :param nodename: Name of the target compute node
5337         :param migration: nova.objects.Migration object for the operation
5338         :param limits: nova.objects.SchedulerLimits object of resource limits
5339         :param request_spec: nova.objects.RequestSpec object for the operation
5340         :returns: nova.objects.MigrationContext; the migration context created
5341             on the destination host during the resize_claim.
5342         :raises: nova.exception.MigrationPreCheckError if the pre-check
5343             validation fails for the given host selection
5344         """
5345         LOG.debug('Checking if we can cross-cell migrate instance to this '
5346                   'host (%s).', self.host, instance=instance)
5347         self._send_prep_resize_notifications(
5348             ctxt, instance, fields.NotificationPhase.START, flavor)
5349         # TODO(mriedem): update_pci_request_spec_with_allocated_interface_name
5350         # should be called here if the request spec has request group mappings,
5351         # e.g. for things like QoS ports with resource requests. Do it outside
5352         # the try/except so if it raises BuildAbortException we do not attempt
5353         # to reschedule.
5354         try:
5355             # Get the allocations within the try/except block in case we get
5356             # an error so MigrationPreCheckError is raised up.
5357             allocations = self.reportclient.get_allocs_for_consumer(
5358                 ctxt, instance.uuid)['allocations']
5359             # Claim resources on this target host using the new flavor which
5360             # will create the MigrationContext object. Note that in the future
5361             # if we want to do other validation here we should do it within
5362             # the MoveClaim context so we can drop the claim if anything fails.
5363             self.rt.resize_claim(
5364                 ctxt, instance, flavor, nodename, migration, allocations,
5365                 image_meta=instance.image_meta, limits=limits)
5366         except Exception as ex:
5367             err = six.text_type(ex)
5368             LOG.warning(
5369                 'Cross-cell resize pre-checks failed for this host (%s). '
5370                 'Cleaning up. Failure: %s', self.host, err,
5371                 instance=instance, exc_info=True)
5372             raise exception.MigrationPreCheckError(
5373                 reason=(_("Pre-checks failed on host '%(host)s'. "
5374                           "Error: %(error)s") %
5375                         {'host': self.host, 'error': err}))
5376         finally:
5377             self._send_prep_resize_notifications(
5378                 ctxt, instance, fields.NotificationPhase.END, flavor)
5379 
5380         # ResourceTracker.resize_claim() sets instance.migration_context.
5381         return instance.migration_context
5382 
5383     @messaging.expected_exceptions(exception.InstancePowerOffFailure)
5384     @wrap_exception()
5385     @reverts_task_state
5386     @wrap_instance_event(prefix='compute')
5387     @errors_out_migration
5388     @wrap_instance_fault
5389     def prep_snapshot_based_resize_at_source(
5390             self, ctxt, instance, migration, snapshot_id=None):
5391         """Prepares the instance at the source host for cross-cell resize
5392 
5393         Performs actions like powering off the guest, upload snapshot data if
5394         the instance is not volume-backed, disconnecting volumes, unplugging
5395         VIFs and activating the destination host port bindings.
5396 
5397         :param ctxt: user auth request context targeted at source cell
5398         :param instance: nova.objects.Instance; the instance being resized.
5399             The expected instance.task_state is "resize_migrating" when calling
5400             this method, and the expected task_state upon successful completion
5401             is "resize_migrated".
5402         :param migration: nova.objects.Migration object for the operation.
5403             The expected migration.status is "pre-migrating" when calling this
5404             method and the expected status upon successful completion is
5405             "post-migrating".
5406         :param snapshot_id: ID of the image snapshot to upload if not a
5407             volume-backed instance
5408         :raises: nova.exception.InstancePowerOffFailure if stopping the
5409             instance fails
5410         """
5411         LOG.info('Preparing for snapshot based resize on source host %s.',
5412                  self.host, instance=instance)
5413         # Note that if anything fails here, the migration-based allocations
5414         # created in conductor should be reverted by conductor as well,
5415         # see MigrationTask.rollback.
5416         self._prep_snapshot_based_resize_at_source(
5417             ctxt, instance, migration, snapshot_id=snapshot_id)
5418 
5419     @delete_image_on_error
5420     def _snapshot_for_resize(self, ctxt, image_id, instance):
5421         """Uploads snapshot for the instance during a snapshot-based resize
5422 
5423         If the snapshot operation fails the image will be deleted.
5424 
5425         :param ctxt: the nova auth request context for the resize operation
5426         :param image_id: the snapshot image ID
5427         :param instance: the instance to snapshot/resize
5428         """
5429         LOG.debug('Uploading snapshot data for image %s', image_id,
5430                   instance=instance)
5431         # Note that we do not track the snapshot phase task states
5432         # during resize since we do not want to reflect those into the
5433         # actual instance.task_state.
5434         update_task_state = lambda *args, **kwargs: None
5435         with timeutils.StopWatch() as timer:
5436             self.driver.snapshot(ctxt, instance, image_id, update_task_state)
5437             LOG.debug('Took %0.2f seconds to snapshot the instance on '
5438                       'the hypervisor.', timer.elapsed(), instance=instance)
5439 
5440     def _prep_snapshot_based_resize_at_source(
5441             self, ctxt, instance, migration, snapshot_id=None):
5442         """Private method for prep_snapshot_based_resize_at_source so calling
5443         code can handle errors and perform rollbacks as necessary.
5444         """
5445         # Fetch and update the instance.info_cache.
5446         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5447         # Get the BDMs attached to this instance on this source host.
5448         bdms = instance.get_bdms()
5449         # Send the resize.start notification.
5450         self._send_resize_instance_notifications(
5451             ctxt, instance, bdms, network_info, fields.NotificationPhase.START)
5452         # Update the migration status from "pre-migrating" to "migrating".
5453         migration.status = 'migrating'
5454         migration.save()
5455 
5456         # Since the instance is going to be left on the source host during the
5457         # resize, we need to power it off so we do not have the instance
5458         # potentially running in two places.
5459         LOG.debug('Stopping instance', instance=instance)
5460         try:
5461             self._power_off_instance(instance)
5462         except Exception as e:
5463             LOG.exception('Failed to power off instance.', instance=instance)
5464             raise exception.InstancePowerOffFailure(reason=six.text_type(e))
5465         instance.power_state = self._get_power_state(instance)
5466 
5467         # If a snapshot image ID was provided, we need to snapshot the guest
5468         # disk image and upload it to the image service.
5469         if snapshot_id:
5470             self._snapshot_for_resize(ctxt, snapshot_id, instance)
5471 
5472         block_device_info = self._get_instance_block_device_info(
5473             ctxt, instance, bdms=bdms)
5474 
5475         # If something fails at this point the instance must go to ERROR
5476         # status for operator intervention or to reboot/rebuild the instance.
5477         with self._error_out_instance_on_exception(
5478                 ctxt, instance, instance_state=vm_states.ERROR):
5479 
5480             # Destroy the guest on the source host which will disconnect
5481             # volumes and unplug VIFs. Note that we DO NOT destroy disks since
5482             # we want to leave those on the source host in case of a later
5483             # failure and disks are needed to recover the guest or in case the
5484             # resize is reverted.
5485             LOG.debug('Destroying guest on source host but retaining disks.',
5486                       instance=instance)
5487             self.driver.destroy(
5488                 ctxt, instance, network_info,
5489                 block_device_info=block_device_info, destroy_disks=False)
5490 
5491             # At this point the volumes are disconnected from this source host.
5492             # Delete the old volume attachment records and create new empty
5493             # ones which will be used later if the resize is reverted.
5494             LOG.debug('Deleting volume attachments for the source host.',
5495                       instance=instance)
5496             self._terminate_volume_connections(ctxt, instance, bdms)
5497 
5498             # At this point the VIFs are unplugged from this source host.
5499             # Activate the dest host port bindings created by conductor.
5500             self.network_api.migrate_instance_start(ctxt, instance, migration)
5501 
5502             # Update the migration status from "migrating" to "post-migrating".
5503             migration.status = 'post-migrating'
5504             migration.save()
5505 
5506             # At this point, the traditional resize_instance would update the
5507             # instance host/node values to point at the dest host/node because
5508             # that is where the disk is transferred during resize_instance, but
5509             # with cross-cell resize the instance is not yet at the dest host
5510             # so we do not make that update here.
5511             instance.task_state = task_states.RESIZE_MIGRATED
5512             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5513 
5514         self._send_resize_instance_notifications(
5515             ctxt, instance, bdms, network_info,
5516             fields.NotificationPhase.END)
5517         self.instance_events.clear_events_for_instance(instance)
5518 
5519     @wrap_exception()
5520     @reverts_task_state
5521     @wrap_instance_event(prefix='compute')
5522     @wrap_instance_fault
5523     def resize_instance(self, context, instance, image,
5524                         migration, instance_type, clean_shutdown,
5525                         request_spec=None):
5526         """Starts the migration of a running instance to another host.
5527 
5528         This is initiated from the destination host's ``prep_resize`` routine
5529         and runs on the source host.
5530         """
5531         try:
5532             self._resize_instance(context, instance, image, migration,
5533                                   instance_type, clean_shutdown, request_spec)
5534         except Exception:
5535             with excutils.save_and_reraise_exception():
5536                 self._revert_allocation(context, instance, migration)
5537 
5538     def _resize_instance(self, context, instance, image,
5539                          migration, instance_type, clean_shutdown,
5540                          request_spec):
5541         # Pass instance_state=instance.vm_state because we can resize
5542         # a STOPPED server and we don't want to set it back to ACTIVE
5543         # in case migrate_disk_and_power_off raises InstanceFaultRollback.
5544         instance_state = instance.vm_state
5545         with self._error_out_instance_on_exception(
5546                 context, instance, instance_state=instance_state), \
5547              errors_out_migration_ctxt(migration):
5548             network_info = self.network_api.get_instance_nw_info(context,
5549                                                                  instance)
5550 
5551             migration.status = 'migrating'
5552             migration.save()
5553 
5554             instance.task_state = task_states.RESIZE_MIGRATING
5555             instance.save(expected_task_state=task_states.RESIZE_PREP)
5556 
5557             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5558                     context, instance.uuid)
5559             self._send_resize_instance_notifications(
5560                 context, instance, bdms, network_info,
5561                 fields.NotificationPhase.START)
5562 
5563             block_device_info = self._get_instance_block_device_info(
5564                                 context, instance, bdms=bdms)
5565 
5566             timeout, retry_interval = self._get_power_off_values(
5567                 instance, clean_shutdown)
5568             disk_info = self.driver.migrate_disk_and_power_off(
5569                     context, instance, migration.dest_host,
5570                     instance_type, network_info,
5571                     block_device_info,
5572                     timeout, retry_interval)
5573 
5574             self._terminate_volume_connections(context, instance, bdms)
5575 
5576             self.network_api.migrate_instance_start(context,
5577                                                     instance,
5578                                                     migration)
5579 
5580             migration.status = 'post-migrating'
5581             migration.save()
5582 
5583             instance.host = migration.dest_compute
5584             instance.node = migration.dest_node
5585             instance.task_state = task_states.RESIZE_MIGRATED
5586             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5587 
5588             # RPC cast to the destination host to finish the resize/migration.
5589             self.compute_rpcapi.finish_resize(context, instance,
5590                 migration, image, disk_info, migration.dest_compute,
5591                 request_spec)
5592 
5593         self._send_resize_instance_notifications(
5594             context, instance, bdms, network_info,
5595             fields.NotificationPhase.END)
5596         self.instance_events.clear_events_for_instance(instance)
5597 
5598     def _send_resize_instance_notifications(
5599             self, context, instance, bdms, network_info, phase):
5600         """Send "resize.(start|end)" notifications.
5601 
5602         :param context: nova auth request context
5603         :param instance: The instance being resized
5604         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5605             instance
5606         :param network_info: NetworkInfo for the instance info cache of ports
5607         :param phase: The phase of the action (NotificationPhase enum, either
5608             ``start`` or ``end``)
5609         """
5610         action = fields.NotificationAction.RESIZE
5611         # Send the legacy unversioned notification.
5612         self._notify_about_instance_usage(
5613             context, instance, "%s.%s" % (action, phase),
5614             network_info=network_info)
5615         # Send the versioned notification.
5616         compute_utils.notify_about_instance_action(
5617             context, instance, self.host, action=action, phase=phase,
5618             bdms=bdms)
5619 
5620     def _terminate_volume_connections(self, context, instance, bdms):
5621         connector = None
5622         for bdm in bdms:
5623             if bdm.is_volume:
5624                 if bdm.attachment_id:
5625                     # NOTE(jdg): So here's the thing, the idea behind the new
5626                     # attach API's was to have a new code fork/path that we
5627                     # followed, we're not going to do that so we have to do
5628                     # some extra work in here to make it *behave* just like the
5629                     # old code. Cinder doesn't allow disconnect/reconnect (you
5630                     # just delete the attachment and get a new one)
5631                     # attachments in the new attach code so we have to do
5632                     # a delete and create without a connector (reserve),
5633                     # in other words, beware
5634                     attachment_id = self.volume_api.attachment_create(
5635                         context, bdm.volume_id, instance.uuid)['id']
5636                     self.volume_api.attachment_delete(context,
5637                                                       bdm.attachment_id)
5638                     bdm.attachment_id = attachment_id
5639                     bdm.save()
5640 
5641                 else:
5642                     if connector is None:
5643                         connector = self.driver.get_volume_connector(instance)
5644                     self.volume_api.terminate_connection(context,
5645                                                          bdm.volume_id,
5646                                                          connector)
5647 
5648     @staticmethod
5649     def _set_instance_info(instance, instance_type):
5650         instance.instance_type_id = instance_type.id
5651         instance.memory_mb = instance_type.memory_mb
5652         instance.vcpus = instance_type.vcpus
5653         instance.root_gb = instance_type.root_gb
5654         instance.ephemeral_gb = instance_type.ephemeral_gb
5655         instance.flavor = instance_type
5656 
5657     def _update_volume_attachments(self, context, instance, bdms):
5658         """Updates volume attachments using the virt driver host connector.
5659 
5660         :param context: nova.context.RequestContext - user request context
5661         :param instance: nova.objects.Instance
5662         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5663                      device mappings for the given instance
5664         """
5665         if bdms:
5666             connector = None
5667             for bdm in bdms:
5668                 if bdm.is_volume and bdm.attachment_id:
5669                     if connector is None:
5670                         connector = self.driver.get_volume_connector(instance)
5671                     self.volume_api.attachment_update(
5672                         context, bdm.attachment_id, connector, bdm.device_name)
5673 
5674     def _complete_volume_attachments(self, context, bdms):
5675         """Completes volume attachments for the instance
5676 
5677         :param context: nova.context.RequestContext - user request context
5678         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5679                      device mappings for the given instance
5680         """
5681         if bdms:
5682             for bdm in bdms:
5683                 if bdm.is_volume and bdm.attachment_id:
5684                     self.volume_api.attachment_complete(
5685                         context, bdm.attachment_id)
5686 
5687     def _finish_resize(self, context, instance, migration, disk_info,
5688                        image_meta, bdms, request_spec):
5689         resize_instance = False  # indicates disks have been resized
5690         old_instance_type_id = migration['old_instance_type_id']
5691         new_instance_type_id = migration['new_instance_type_id']
5692         old_flavor = instance.flavor  # the current flavor is now old
5693         # NOTE(mriedem): Get the old_vm_state so we know if we should
5694         # power on the instance. If old_vm_state is not set we need to default
5695         # to ACTIVE for backwards compatibility
5696         old_vm_state = instance.system_metadata.get('old_vm_state',
5697                                                     vm_states.ACTIVE)
5698         instance.old_flavor = old_flavor
5699 
5700         if old_instance_type_id != new_instance_type_id:
5701             new_flavor = instance.new_flavor  # this is set in _prep_resize
5702             # Set the flavor-related fields on the instance object including
5703             # making instance.flavor = new_flavor.
5704             self._set_instance_info(instance, new_flavor)
5705             for key in ('root_gb', 'swap', 'ephemeral_gb'):
5706                 if old_flavor[key] != new_flavor[key]:
5707                     resize_instance = True
5708                     break
5709         instance.apply_migration_context()
5710 
5711         # NOTE(tr3buchet): setup networks on destination host
5712         self.network_api.setup_networks_on_host(context, instance,
5713                                                 migration.dest_compute)
5714         provider_mappings = self._get_request_group_mapping(request_spec)
5715 
5716         # For neutron, migrate_instance_finish updates port bindings for this
5717         # host including any PCI devices claimed for SR-IOV ports.
5718         self.network_api.migrate_instance_finish(
5719             context, instance, migration, provider_mappings)
5720 
5721         network_info = self.network_api.get_instance_nw_info(context, instance)
5722 
5723         instance.task_state = task_states.RESIZE_FINISH
5724         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5725 
5726         self._send_finish_resize_notifications(
5727             context, instance, bdms, network_info,
5728             fields.NotificationPhase.START)
5729 
5730         # We need to update any volume attachments using the destination
5731         # host connector so that we can update the BDM.connection_info
5732         # before calling driver.finish_migration otherwise the driver
5733         # won't know how to connect the volumes to this host.
5734         # Note that _get_instance_block_device_info with
5735         # refresh_conn_info=True will update the BDM.connection_info value
5736         # in the database so we must do this before calling that method.
5737         self._update_volume_attachments(context, instance, bdms)
5738 
5739         block_device_info = self._get_instance_block_device_info(
5740             context, instance, refresh_conn_info=True, bdms=bdms)
5741 
5742         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
5743         # automatically power on the instance after it's migrated
5744         power_on = old_vm_state != vm_states.STOPPED
5745 
5746         # NOTE(sbauza): During a migration, the original allocation is against
5747         # the migration UUID while the target allocation (for the destination
5748         # node) is related to the instance UUID, so here we need to pass the
5749         # new ones.
5750         allocations = self.reportclient.get_allocs_for_consumer(
5751             context, instance.uuid)['allocations']
5752 
5753         try:
5754             self.driver.finish_migration(context, migration, instance,
5755                                          disk_info,
5756                                          network_info,
5757                                          image_meta, resize_instance,
5758                                          allocations,
5759                                          block_device_info, power_on)
5760         except Exception:
5761             # Note that we do not rollback port bindings to the source host
5762             # because resize_instance (on the source host) updated the
5763             # instance.host to point to *this* host (the destination host)
5764             # so the port bindings pointing at this host are correct even
5765             # though we failed to create the guest.
5766             with excutils.save_and_reraise_exception():
5767                 # If we failed to create the guest on this host, reset the
5768                 # instance flavor-related fields to the old flavor. An
5769                 # error handler like reverts_task_state will save the changes.
5770                 if old_instance_type_id != new_instance_type_id:
5771                     self._set_instance_info(instance, old_flavor)
5772 
5773         # Now complete any volume attachments that were previously updated.
5774         self._complete_volume_attachments(context, bdms)
5775 
5776         migration.status = 'finished'
5777         migration.save()
5778 
5779         instance.vm_state = vm_states.RESIZED
5780         instance.task_state = None
5781         instance.launched_at = timeutils.utcnow()
5782         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5783 
5784         return network_info
5785 
5786     @wrap_exception()
5787     @reverts_task_state
5788     @wrap_instance_event(prefix='compute')
5789     @errors_out_migration
5790     @wrap_instance_fault
5791     def finish_resize(self, context, disk_info, image, instance,
5792                       migration, request_spec=None):
5793         """Completes the migration process.
5794 
5795         Sets up the newly transferred disk and turns on the instance at its
5796         new host machine.
5797 
5798         """
5799         try:
5800             self._finish_resize_helper(context, disk_info, image, instance,
5801                                        migration, request_spec)
5802         except Exception:
5803             with excutils.save_and_reraise_exception():
5804                 # At this point, resize_instance (which runs on the source) has
5805                 # already updated the instance host/node values to point to
5806                 # this (the dest) compute, so we need to leave the allocations
5807                 # against the dest node resource provider intact and drop the
5808                 # allocations against the source node resource provider. If the
5809                 # user tries to recover the server by hard rebooting it, it
5810                 # will happen on this host so that's where the allocations
5811                 # should go. Note that this is the same method called from
5812                 # confirm_resize to cleanup the source node allocations held
5813                 # by the migration record.
5814                 LOG.info('Deleting allocations for old flavor on source node '
5815                          '%s after finish_resize failure. You may be able to '
5816                          'recover the instance by hard rebooting it.',
5817                          migration.source_compute, instance=instance)
5818                 self._delete_allocation_after_move(
5819                     context, instance, migration)
5820 
5821     def _finish_resize_helper(self, context, disk_info, image, instance,
5822                               migration, request_spec):
5823         """Completes the migration process.
5824 
5825         The caller must revert the instance's allocations if the migration
5826         process failed.
5827         """
5828         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5829             context, instance.uuid)
5830 
5831         with self._error_out_instance_on_exception(context, instance):
5832             image_meta = objects.ImageMeta.from_dict(image)
5833             network_info = self._finish_resize(context, instance, migration,
5834                                                disk_info, image_meta, bdms,
5835                                                request_spec)
5836 
5837         # TODO(melwitt): We should clean up instance console tokens here. The
5838         # instance is on a new host and will need to establish a new console
5839         # connection.
5840         self._update_scheduler_instance_info(context, instance)
5841         self._send_finish_resize_notifications(
5842             context, instance, bdms, network_info,
5843             fields.NotificationPhase.END)
5844 
5845     def _send_finish_resize_notifications(
5846             self, context, instance, bdms, network_info, phase):
5847         """Send notifications for the finish_resize flow.
5848 
5849         :param context: nova auth request context
5850         :param instance: The instance being resized
5851         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5852             instance
5853         :param network_info: NetworkInfo for the instance info cache of ports
5854         :param phase: The phase of the action (NotificationPhase enum, either
5855             ``start`` or ``end``)
5856         """
5857         # Send the legacy unversioned notification.
5858         self._notify_about_instance_usage(
5859             context, instance, "finish_resize.%s" % phase,
5860             network_info=network_info)
5861         # Send the versioned notification.
5862         compute_utils.notify_about_instance_action(
5863             context, instance, self.host,
5864             action=fields.NotificationAction.RESIZE_FINISH, phase=phase,
5865             bdms=bdms)
5866 
5867     # TODO(stephenfin): Remove unused request_spec parameter in API v6.0
5868     @wrap_exception()
5869     @reverts_task_state
5870     @wrap_instance_event(prefix='compute')
5871     @errors_out_migration
5872     @wrap_instance_fault
5873     def finish_snapshot_based_resize_at_dest(
5874             self, ctxt, instance, migration, snapshot_id, request_spec):
5875         """Finishes the snapshot-based resize at the destination compute.
5876 
5877         Sets up block devices and networking on the destination compute and
5878         spawns the guest.
5879 
5880         :param ctxt: nova auth request context targeted at the target cell DB
5881         :param instance: The Instance object being resized with the
5882             ``migration_context`` field set. Upon successful completion of this
5883             method the vm_state should be "resized", the task_state should be
5884             None, and migration context, host/node and flavor-related fields
5885             should be set on the instance.
5886         :param migration: The Migration object for this resize operation. Upon
5887             successful completion of this method the migration status should
5888             be "finished".
5889         :param snapshot_id: ID of the image snapshot created for a
5890             non-volume-backed instance, else None.
5891         :param request_spec: nova.objects.RequestSpec object for the operation
5892         """
5893         LOG.info('Finishing snapshot based resize on destination host %s.',
5894                  self.host, instance=instance)
5895         with self._error_out_instance_on_exception(ctxt, instance):
5896             # Note that if anything fails here, the migration-based allocations
5897             # created in conductor should be reverted by conductor as well,
5898             # see MigrationTask.rollback.
5899             self._finish_snapshot_based_resize_at_dest(
5900                 ctxt, instance, migration, snapshot_id)
5901 
5902     def _finish_snapshot_based_resize_at_dest(
5903             self, ctxt, instance, migration, snapshot_id):
5904         """Private variant of finish_snapshot_based_resize_at_dest so the
5905         caller can handle reverting resource allocations on failure and perform
5906         other generic error handling.
5907         """
5908         # Figure out the image metadata to use when spawning the guest.
5909         if snapshot_id:
5910             image_meta = objects.ImageMeta.from_image_ref(
5911                 ctxt, self.image_api, snapshot_id)
5912         else:
5913             # Just use what is already on the volume-backed instance.
5914             image_meta = instance.image_meta
5915 
5916         resize = migration.migration_type == 'resize'
5917         instance.old_flavor = instance.flavor
5918         if resize:
5919             flavor = instance.new_flavor
5920             # If we are resizing to a new flavor we need to set the
5921             # flavor-related fields on the instance.
5922             # NOTE(mriedem): This is likely where storing old/new_flavor on
5923             # the MigrationContext would make this cleaner.
5924             self._set_instance_info(instance, flavor)
5925 
5926         instance.apply_migration_context()
5927         instance.task_state = task_states.RESIZE_FINISH
5928         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5929 
5930         # This seems a bit late to be sending the start notification but
5931         # it is what traditional resize has always done as well and it does
5932         # contain the changes to the instance with the new_flavor and
5933         # task_state.
5934         bdms = instance.get_bdms()
5935         network_info = instance.get_network_info()
5936         self._send_finish_resize_notifications(
5937             ctxt, instance, bdms, network_info,
5938             fields.NotificationPhase.START)
5939 
5940         # Setup volumes and networking and spawn the guest in the hypervisor.
5941         self._finish_snapshot_based_resize_at_dest_spawn(
5942             ctxt, instance, migration, image_meta, bdms)
5943 
5944         # If we spawned from a temporary snapshot image we can delete that now,
5945         # similar to how unshelve works.
5946         if snapshot_id:
5947             compute_utils.delete_image(
5948                 ctxt, instance, self.image_api, snapshot_id)
5949 
5950         migration.status = 'finished'
5951         migration.save()
5952 
5953         self._update_instance_after_spawn(instance, vm_state=vm_states.RESIZED)
5954         # Setting the host/node values will make the ResourceTracker continue
5955         # to track usage for this instance on this host.
5956         instance.host = migration.dest_compute
5957         instance.node = migration.dest_node
5958         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5959 
5960         # Broadcast to all schedulers that the instance is on this host.
5961         self._update_scheduler_instance_info(ctxt, instance)
5962         self._send_finish_resize_notifications(
5963             ctxt, instance, bdms, network_info,
5964             fields.NotificationPhase.END)
5965 
5966     def _finish_snapshot_based_resize_at_dest_spawn(
5967             self, ctxt, instance, migration, image_meta, bdms):
5968         """Sets up volumes and networking and spawns the guest on the dest host
5969 
5970         If the instance was stopped when the resize was initiated the guest
5971         will be created but remain in a shutdown power state.
5972 
5973         If the spawn fails, port bindings are rolled back to the source host
5974         and volume connections are terminated for this dest host.
5975 
5976         :param ctxt: nova auth request context
5977         :param instance: Instance object being migrated
5978         :param migration: Migration object for the operation
5979         :param image_meta: ImageMeta object used during driver.spawn
5980         :param bdms: BlockDeviceMappingList of BDMs for the instance
5981         """
5982         # Update the volume attachments using this host's connector.
5983         # That will update the BlockDeviceMapping.connection_info which
5984         # will be used to connect the volumes on this host during spawn().
5985         block_device_info = self._prep_block_device(ctxt, instance, bdms)
5986 
5987         allocations = self.reportclient.get_allocations_for_consumer(
5988             ctxt, instance.uuid)
5989 
5990         # We do not call self.network_api.setup_networks_on_host here because
5991         # for neutron that sets up the port migration profile which is only
5992         # used during live migration with DVR. Yes it is gross knowing what
5993         # that method does internally. We could change this when bug 1814837
5994         # is fixed if setup_networks_on_host is made smarter by passing the
5995         # migration record and the method checks the migration_type.
5996 
5997         # Activate the port bindings for this host.
5998         # FIXME(mriedem): We're going to have the same issue as bug 1813789
5999         # here because this will update the port bindings and send the
6000         # network-vif-plugged event and that means when driver.spawn waits for
6001         # it we might have already gotten the event and neutron won't send
6002         # another one so we could timeout.
6003         # TODO(mriedem): Calculate provider mappings when we support cross-cell
6004         # resize/migrate with ports having resource requests.
6005         self.network_api.migrate_instance_finish(
6006             ctxt, instance, migration, provider_mappings=None)
6007         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6008 
6009         # If the original vm_state was STOPPED, we do not automatically
6010         # power on the instance after it is migrated.
6011         power_on = instance.system_metadata['old_vm_state'] == vm_states.ACTIVE
6012         try:
6013             # NOTE(mriedem): If this instance uses a config drive, it will get
6014             # rebuilt here which means any personality files will be lost,
6015             # similar to unshelve. If the instance is not using a config drive
6016             # and getting metadata from the metadata API service, personality
6017             # files would be lost regardless of the move operation.
6018             self.driver.spawn(
6019                 ctxt, instance, image_meta, injected_files=[],
6020                 admin_password=None, allocations=allocations,
6021                 network_info=network_info, block_device_info=block_device_info,
6022                 power_on=power_on)
6023         except Exception:
6024             with excutils.save_and_reraise_exception(logger=LOG):
6025                 # Rollback port bindings to the source host.
6026                 try:
6027                     # This is gross but migrate_instance_start looks at the
6028                     # migration.dest_compute to determine where to activate the
6029                     # port bindings and we want the source compute port
6030                     # bindings to be re-activated. Remember at this point the
6031                     # instance.host is still pointing at the source compute.
6032                     # TODO(mriedem): Maybe we should be calling
6033                     # setup_instance_network_on_host here to deal with pci
6034                     # devices?
6035                     with utils.temporary_mutation(
6036                             migration, dest_compute=migration.source_compute):
6037                         self.network_api.migrate_instance_start(
6038                             ctxt, instance, migration)
6039                 except Exception:
6040                     LOG.exception(
6041                         'Failed to activate port bindings on the source '
6042                         'host: %s', migration.source_compute,
6043                         instance=instance)
6044 
6045                 # Rollback volume connections on this host.
6046                 for bdm in bdms:
6047                     if bdm.is_volume:
6048                         try:
6049                             self._remove_volume_connection(
6050                                 ctxt, bdm, instance, delete_attachment=True)
6051                         except Exception:
6052                             LOG.exception('Failed to remove volume connection '
6053                                           'on this host %s for volume %s.',
6054                                           self.host, bdm.volume_id,
6055                                           instance=instance)
6056 
6057     @wrap_exception()
6058     @wrap_instance_fault
6059     def add_fixed_ip_to_instance(self, context, network_id, instance):
6060         """Calls network_api to add new fixed_ip to instance
6061         then injects the new network info and resets instance networking.
6062 
6063         """
6064         self._notify_about_instance_usage(
6065                 context, instance, "create_ip.start")
6066 
6067         network_info = self.network_api.add_fixed_ip_to_instance(context,
6068                                                                  instance,
6069                                                                  network_id)
6070         self._inject_network_info(instance, network_info)
6071         self.reset_network(context, instance)
6072 
6073         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6074         instance.updated_at = timeutils.utcnow()
6075         instance.save()
6076 
6077         self._notify_about_instance_usage(
6078             context, instance, "create_ip.end", network_info=network_info)
6079 
6080     @wrap_exception()
6081     @wrap_instance_fault
6082     def remove_fixed_ip_from_instance(self, context, address, instance):
6083         """Calls network_api to remove existing fixed_ip from instance
6084         by injecting the altered network info and resetting
6085         instance networking.
6086         """
6087         self._notify_about_instance_usage(
6088                 context, instance, "delete_ip.start")
6089 
6090         network_info = self.network_api.remove_fixed_ip_from_instance(context,
6091                                                                       instance,
6092                                                                       address)
6093         self._inject_network_info(instance, network_info)
6094         self.reset_network(context, instance)
6095 
6096         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6097         instance.updated_at = timeutils.utcnow()
6098         instance.save()
6099 
6100         self._notify_about_instance_usage(
6101             context, instance, "delete_ip.end", network_info=network_info)
6102 
6103     @wrap_exception()
6104     @reverts_task_state
6105     @wrap_instance_event(prefix='compute')
6106     @wrap_instance_fault
6107     def pause_instance(self, context, instance):
6108         """Pause an instance on this host."""
6109         context = context.elevated()
6110         LOG.info('Pausing', instance=instance)
6111         self._notify_about_instance_usage(context, instance, 'pause.start')
6112         compute_utils.notify_about_instance_action(context, instance,
6113                self.host, action=fields.NotificationAction.PAUSE,
6114                phase=fields.NotificationPhase.START)
6115         self.driver.pause(instance)
6116         instance.power_state = self._get_power_state(instance)
6117         instance.vm_state = vm_states.PAUSED
6118         instance.task_state = None
6119         instance.save(expected_task_state=task_states.PAUSING)
6120         self._notify_about_instance_usage(context, instance, 'pause.end')
6121         compute_utils.notify_about_instance_action(context, instance,
6122                self.host, action=fields.NotificationAction.PAUSE,
6123                phase=fields.NotificationPhase.END)
6124 
6125     @wrap_exception()
6126     @reverts_task_state
6127     @wrap_instance_event(prefix='compute')
6128     @wrap_instance_fault
6129     def unpause_instance(self, context, instance):
6130         """Unpause a paused instance on this host."""
6131         context = context.elevated()
6132         LOG.info('Unpausing', instance=instance)
6133         self._notify_about_instance_usage(context, instance, 'unpause.start')
6134         compute_utils.notify_about_instance_action(context, instance,
6135             self.host, action=fields.NotificationAction.UNPAUSE,
6136             phase=fields.NotificationPhase.START)
6137         self.driver.unpause(instance)
6138         instance.power_state = self._get_power_state(instance)
6139         instance.vm_state = vm_states.ACTIVE
6140         instance.task_state = None
6141         instance.save(expected_task_state=task_states.UNPAUSING)
6142         self._notify_about_instance_usage(context, instance, 'unpause.end')
6143         compute_utils.notify_about_instance_action(context, instance,
6144             self.host, action=fields.NotificationAction.UNPAUSE,
6145             phase=fields.NotificationPhase.END)
6146 
6147     @wrap_exception()
6148     def host_power_action(self, context, action):
6149         """Reboots, shuts down or powers up the host."""
6150         return self.driver.host_power_action(action)
6151 
6152     @wrap_exception()
6153     def host_maintenance_mode(self, context, host, mode):
6154         """Start/Stop host maintenance window. On start, it triggers
6155         guest VMs evacuation.
6156         """
6157         return self.driver.host_maintenance_mode(host, mode)
6158 
6159     def _update_compute_provider_status(self, context, enabled):
6160         """Adds or removes the COMPUTE_STATUS_DISABLED trait for this host.
6161 
6162         For each ComputeNode managed by this service, adds or removes the
6163         COMPUTE_STATUS_DISABLED traits to/from the associated resource provider
6164         in Placement.
6165 
6166         :param context: nova auth RequestContext
6167         :param enabled: True if the node is enabled in which case the trait
6168             would be removed, False if the node is disabled in which case
6169             the trait would be added.
6170         :raises: ComputeHostNotFound if there are no compute nodes found in
6171             the ResourceTracker for this service.
6172         """
6173         # Get the compute node(s) on this host. Remember that ironic can be
6174         # managing more than one compute node.
6175         nodes = self.rt.compute_nodes.values()
6176         if not nodes:
6177             raise exception.ComputeHostNotFound(host=self.host)
6178         # For each node, we want to add (or remove) the COMPUTE_STATUS_DISABLED
6179         # trait on the related resource provider in placement so the scheduler
6180         # (pre-)filters the provider based on its status.
6181         for node in nodes:
6182             try:
6183                 self.virtapi.update_compute_provider_status(
6184                     context, node.uuid, enabled)
6185             except (exception.ResourceProviderTraitRetrievalFailed,
6186                     exception.ResourceProviderUpdateConflict,
6187                     exception.ResourceProviderUpdateFailed,
6188                     exception.TraitRetrievalFailed) as e:
6189                 # This is best effort so just log a warning and continue.
6190                 LOG.warning('An error occurred while updating '
6191                             'COMPUTE_STATUS_DISABLED trait on compute node '
6192                             'resource provider %s. The trait will be '
6193                             'synchronized when the update_available_resource '
6194                             'periodic task runs. Error: %s',
6195                             node.uuid, e.format_message())
6196             except Exception:
6197                 LOG.exception('An error occurred while updating '
6198                               'COMPUTE_STATUS_DISABLED trait on compute node '
6199                               'resource provider %s. The trait will be '
6200                               'synchronized when the '
6201                               'update_available_resource periodic task runs.',
6202                               node.uuid)
6203 
6204     @wrap_exception()
6205     def set_host_enabled(self, context, enabled):
6206         """Sets the specified host's ability to accept new instances.
6207 
6208         This method will add or remove the COMPUTE_STATUS_DISABLED trait
6209         to/from the associated compute node resource provider(s) for this
6210         compute service.
6211         """
6212         try:
6213             self._update_compute_provider_status(context, enabled)
6214         except exception.ComputeHostNotFound:
6215             LOG.warning('Unable to add/remove trait COMPUTE_STATUS_DISABLED. '
6216                         'No ComputeNode(s) found for host: %s', self.host)
6217 
6218         try:
6219             return self.driver.set_host_enabled(enabled)
6220         except NotImplementedError:
6221             # Only the xenapi driver implements set_host_enabled but we don't
6222             # want NotImplementedError to get raised back to the API. We still
6223             # need to honor the compute RPC API contract and return 'enabled'
6224             # or 'disabled' though.
6225             return 'enabled' if enabled else 'disabled'
6226 
6227     @wrap_exception()
6228     def get_host_uptime(self, context):
6229         """Returns the result of calling "uptime" on the target host."""
6230         return self.driver.get_host_uptime()
6231 
6232     @wrap_exception()
6233     @wrap_instance_fault
6234     def get_diagnostics(self, context, instance):
6235         """Retrieve diagnostics for an instance on this host."""
6236         current_power_state = self._get_power_state(instance)
6237         if current_power_state == power_state.RUNNING:
6238             LOG.info("Retrieving diagnostics", instance=instance)
6239             return self.driver.get_diagnostics(instance)
6240         else:
6241             raise exception.InstanceInvalidState(
6242                 attr='power state',
6243                 instance_uuid=instance.uuid,
6244                 state=power_state.STATE_MAP[instance.power_state],
6245                 method='get_diagnostics')
6246 
6247     @wrap_exception()
6248     @wrap_instance_fault
6249     def get_instance_diagnostics(self, context, instance):
6250         """Retrieve diagnostics for an instance on this host."""
6251         current_power_state = self._get_power_state(instance)
6252         if current_power_state == power_state.RUNNING:
6253             LOG.info("Retrieving diagnostics", instance=instance)
6254             return self.driver.get_instance_diagnostics(instance)
6255         else:
6256             raise exception.InstanceInvalidState(
6257                 attr='power state',
6258                 instance_uuid=instance.uuid,
6259                 state=power_state.STATE_MAP[instance.power_state],
6260                 method='get_diagnostics')
6261 
6262     @wrap_exception()
6263     @reverts_task_state
6264     @wrap_instance_event(prefix='compute')
6265     @wrap_instance_fault
6266     def suspend_instance(self, context, instance):
6267         """Suspend the given instance."""
6268         context = context.elevated()
6269 
6270         # Store the old state
6271         instance.system_metadata['old_vm_state'] = instance.vm_state
6272         self._notify_about_instance_usage(context, instance, 'suspend.start')
6273         compute_utils.notify_about_instance_action(context, instance,
6274                 self.host, action=fields.NotificationAction.SUSPEND,
6275                 phase=fields.NotificationPhase.START)
6276         with self._error_out_instance_on_exception(context, instance,
6277              instance_state=instance.vm_state):
6278             self.driver.suspend(context, instance)
6279         instance.power_state = self._get_power_state(instance)
6280         instance.vm_state = vm_states.SUSPENDED
6281         instance.task_state = None
6282         instance.save(expected_task_state=task_states.SUSPENDING)
6283         self._notify_about_instance_usage(context, instance, 'suspend.end')
6284         compute_utils.notify_about_instance_action(context, instance,
6285                 self.host, action=fields.NotificationAction.SUSPEND,
6286                 phase=fields.NotificationPhase.END)
6287 
6288     @wrap_exception()
6289     @reverts_task_state
6290     @wrap_instance_event(prefix='compute')
6291     @wrap_instance_fault
6292     def resume_instance(self, context, instance):
6293         """Resume the given suspended instance."""
6294         context = context.elevated()
6295         LOG.info('Resuming', instance=instance)
6296 
6297         self._notify_about_instance_usage(context, instance, 'resume.start')
6298 
6299         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6300             context, instance.uuid)
6301         block_device_info = self._get_instance_block_device_info(
6302             context, instance, bdms=bdms)
6303 
6304         compute_utils.notify_about_instance_action(context, instance,
6305             self.host, action=fields.NotificationAction.RESUME,
6306             phase=fields.NotificationPhase.START, bdms=bdms)
6307 
6308         network_info = self.network_api.get_instance_nw_info(context, instance)
6309 
6310         with self._error_out_instance_on_exception(context, instance,
6311              instance_state=instance.vm_state):
6312             self.driver.resume(context, instance, network_info,
6313                                block_device_info)
6314 
6315         instance.power_state = self._get_power_state(instance)
6316 
6317         # We default to the ACTIVE state for backwards compatibility
6318         instance.vm_state = instance.system_metadata.pop('old_vm_state',
6319                                                          vm_states.ACTIVE)
6320 
6321         instance.task_state = None
6322         instance.save(expected_task_state=task_states.RESUMING)
6323         self._notify_about_instance_usage(context, instance, 'resume.end')
6324         compute_utils.notify_about_instance_action(context, instance,
6325             self.host, action=fields.NotificationAction.RESUME,
6326             phase=fields.NotificationPhase.END, bdms=bdms)
6327 
6328     @wrap_exception()
6329     @reverts_task_state
6330     @wrap_instance_event(prefix='compute')
6331     @wrap_instance_fault
6332     def shelve_instance(self, context, instance, image_id,
6333                         clean_shutdown):
6334         """Shelve an instance.
6335 
6336         This should be used when you want to take a snapshot of the instance.
6337         It also adds system_metadata that can be used by a periodic task to
6338         offload the shelved instance after a period of time.
6339 
6340         :param context: request context
6341         :param instance: an Instance object
6342         :param image_id: an image id to snapshot to.
6343         :param clean_shutdown: give the GuestOS a chance to stop
6344         """
6345 
6346         @utils.synchronized(instance.uuid)
6347         def do_shelve_instance():
6348             self._shelve_instance(context, instance, image_id, clean_shutdown)
6349         do_shelve_instance()
6350 
6351     def _shelve_instance(self, context, instance, image_id,
6352                          clean_shutdown):
6353         LOG.info('Shelving', instance=instance)
6354         offload = CONF.shelved_offload_time == 0
6355         if offload:
6356             # Get the BDMs early so we can pass them into versioned
6357             # notifications since _shelve_offload_instance needs the
6358             # BDMs anyway.
6359             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6360                 context, instance.uuid)
6361         else:
6362             bdms = None
6363         compute_utils.notify_usage_exists(self.notifier, context, instance,
6364                                           self.host, current_period=True)
6365         self._notify_about_instance_usage(context, instance, 'shelve.start')
6366         compute_utils.notify_about_instance_action(context, instance,
6367                 self.host, action=fields.NotificationAction.SHELVE,
6368                 phase=fields.NotificationPhase.START, bdms=bdms)
6369 
6370         def update_task_state(task_state, expected_state=task_states.SHELVING):
6371             shelving_state_map = {
6372                     task_states.IMAGE_PENDING_UPLOAD:
6373                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
6374                     task_states.IMAGE_UPLOADING:
6375                         task_states.SHELVING_IMAGE_UPLOADING,
6376                     task_states.SHELVING: task_states.SHELVING}
6377             task_state = shelving_state_map[task_state]
6378             expected_state = shelving_state_map[expected_state]
6379             instance.task_state = task_state
6380             instance.save(expected_task_state=expected_state)
6381         # Do not attempt a clean shutdown of a paused guest since some
6382         # hypervisors will fail the clean shutdown if the guest is not
6383         # running.
6384         if instance.power_state == power_state.PAUSED:
6385             clean_shutdown = False
6386         self._power_off_instance(instance, clean_shutdown)
6387         self.driver.snapshot(context, instance, image_id, update_task_state)
6388 
6389         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
6390         instance.system_metadata['shelved_image_id'] = image_id
6391         instance.system_metadata['shelved_host'] = self.host
6392         instance.vm_state = vm_states.SHELVED
6393         instance.task_state = None
6394         if CONF.shelved_offload_time == 0:
6395             instance.task_state = task_states.SHELVING_OFFLOADING
6396         instance.power_state = self._get_power_state(instance)
6397         instance.save(expected_task_state=[
6398                 task_states.SHELVING,
6399                 task_states.SHELVING_IMAGE_UPLOADING])
6400 
6401         self._notify_about_instance_usage(context, instance, 'shelve.end')
6402         compute_utils.notify_about_instance_action(context, instance,
6403                 self.host, action=fields.NotificationAction.SHELVE,
6404                 phase=fields.NotificationPhase.END, bdms=bdms)
6405 
6406         if offload:
6407             self._shelve_offload_instance(context, instance,
6408                                           clean_shutdown=False, bdms=bdms)
6409 
6410     @wrap_exception()
6411     @reverts_task_state
6412     @wrap_instance_event(prefix='compute')
6413     @wrap_instance_fault
6414     def shelve_offload_instance(self, context, instance, clean_shutdown):
6415         """Remove a shelved instance from the hypervisor.
6416 
6417         This frees up those resources for use by other instances, but may lead
6418         to slower unshelve times for this instance.  This method is used by
6419         volume backed instances since restoring them doesn't involve the
6420         potentially large download of an image.
6421 
6422         :param context: request context
6423         :param instance: nova.objects.instance.Instance
6424         :param clean_shutdown: give the GuestOS a chance to stop
6425         """
6426 
6427         @utils.synchronized(instance.uuid)
6428         def do_shelve_offload_instance():
6429             self._shelve_offload_instance(context, instance, clean_shutdown)
6430         do_shelve_offload_instance()
6431 
6432     def _shelve_offload_instance(self, context, instance, clean_shutdown,
6433                                  bdms=None):
6434         LOG.info('Shelve offloading', instance=instance)
6435         if bdms is None:
6436             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6437                 context, instance.uuid)
6438         self._notify_about_instance_usage(context, instance,
6439                 'shelve_offload.start')
6440         compute_utils.notify_about_instance_action(context, instance,
6441                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6442                 phase=fields.NotificationPhase.START, bdms=bdms)
6443 
6444         self._power_off_instance(instance, clean_shutdown)
6445         current_power_state = self._get_power_state(instance)
6446         network_info = self.network_api.get_instance_nw_info(context, instance)
6447 
6448         block_device_info = self._get_instance_block_device_info(context,
6449                                                                  instance,
6450                                                                  bdms=bdms)
6451         self.driver.destroy(context, instance, network_info,
6452                 block_device_info)
6453 
6454         # the instance is going to be removed from the host so we want to
6455         # terminate all the connections with the volume server and the host
6456         self._terminate_volume_connections(context, instance, bdms)
6457 
6458         # Free up the resource allocations in the placement service.
6459         # This should happen *before* the vm_state is changed to
6460         # SHELVED_OFFLOADED in case client-side code is polling the API to
6461         # schedule more instances (or unshelve) once this server is offloaded.
6462         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
6463                                                                 instance)
6464 
6465         instance.power_state = current_power_state
6466         # NOTE(mriedem): The vm_state has to be set before updating the
6467         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
6468         # values cannot be nulled out until after updating the resource tracker
6469         # though.
6470         instance.vm_state = vm_states.SHELVED_OFFLOADED
6471         instance.task_state = None
6472         instance.save(expected_task_state=[task_states.SHELVING,
6473                                            task_states.SHELVING_OFFLOADING])
6474 
6475         # NOTE(ndipanov): Free resources from the resource tracker
6476         self._update_resource_tracker(context, instance)
6477 
6478         # NOTE(sfinucan): RPC calls should no longer be attempted against this
6479         # instance, so ensure any calls result in errors
6480         self._nil_out_instance_obj_host_and_node(instance)
6481         instance.save(expected_task_state=None)
6482 
6483         # TODO(melwitt): We should clean up instance console tokens here. The
6484         # instance has no host at this point and will need to establish a new
6485         # console connection in the future after it is unshelved.
6486         self._delete_scheduler_instance_info(context, instance.uuid)
6487         self._notify_about_instance_usage(context, instance,
6488                 'shelve_offload.end')
6489         compute_utils.notify_about_instance_action(context, instance,
6490                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6491                 phase=fields.NotificationPhase.END, bdms=bdms)
6492 
6493     @wrap_exception()
6494     @reverts_task_state
6495     @wrap_instance_event(prefix='compute')
6496     @wrap_instance_fault
6497     def unshelve_instance(self, context, instance, image,
6498                           filter_properties, node, request_spec=None):
6499         """Unshelve the instance.
6500 
6501         :param context: request context
6502         :param instance: a nova.objects.instance.Instance object
6503         :param image: an image to build from.  If None we assume a
6504             volume backed instance.
6505         :param filter_properties: dict containing limits, retry info etc.
6506         :param node: target compute node
6507         :param request_spec: the RequestSpec object used to schedule the
6508             instance
6509         """
6510         if filter_properties is None:
6511             filter_properties = {}
6512 
6513         @utils.synchronized(instance.uuid)
6514         def do_unshelve_instance():
6515             self._unshelve_instance(
6516                 context, instance, image, filter_properties, node,
6517                 request_spec)
6518         do_unshelve_instance()
6519 
6520     def _unshelve_instance_key_scrub(self, instance):
6521         """Remove data from the instance that may cause side effects."""
6522         cleaned_keys = dict(
6523                 key_data=instance.key_data,
6524                 auto_disk_config=instance.auto_disk_config)
6525         instance.key_data = None
6526         instance.auto_disk_config = False
6527         return cleaned_keys
6528 
6529     def _unshelve_instance_key_restore(self, instance, keys):
6530         """Restore previously scrubbed keys before saving the instance."""
6531         instance.update(keys)
6532 
6533     def _unshelve_instance(self, context, instance, image, filter_properties,
6534                            node, request_spec):
6535         LOG.info('Unshelving', instance=instance)
6536         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6537                 context, instance.uuid)
6538 
6539         self._notify_about_instance_usage(context, instance, 'unshelve.start')
6540         compute_utils.notify_about_instance_action(context, instance,
6541                 self.host, action=fields.NotificationAction.UNSHELVE,
6542                 phase=fields.NotificationPhase.START, bdms=bdms)
6543 
6544         instance.task_state = task_states.SPAWNING
6545         instance.save()
6546 
6547         block_device_info = self._prep_block_device(context, instance, bdms)
6548         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
6549 
6550         if node is None:
6551             node = self._get_nodename(instance)
6552 
6553         limits = filter_properties.get('limits', {})
6554 
6555         allocations = self.reportclient.get_allocations_for_consumer(
6556             context, instance.uuid)
6557 
6558         shelved_image_ref = instance.image_ref
6559         if image:
6560             instance.image_ref = image['id']
6561             image_meta = objects.ImageMeta.from_dict(image)
6562         else:
6563             image_meta = objects.ImageMeta.from_dict(
6564                 utils.get_image_from_system_metadata(
6565                     instance.system_metadata))
6566 
6567         provider_mappings = self._get_request_group_mapping(request_spec)
6568 
6569         try:
6570             if provider_mappings:
6571                 update = (
6572                     compute_utils.
6573                         update_pci_request_spec_with_allocated_interface_name)
6574                 update(context, self.reportclient, instance, provider_mappings)
6575 
6576             self.network_api.setup_instance_network_on_host(
6577                 context, instance, self.host,
6578                 provider_mappings=provider_mappings)
6579             network_info = self.network_api.get_instance_nw_info(
6580                 context, instance)
6581             with self.rt.instance_claim(context, instance, node, allocations,
6582                                         limits):
6583                 self.driver.spawn(context, instance, image_meta,
6584                                   injected_files=[],
6585                                   admin_password=None,
6586                                   allocations=allocations,
6587                                   network_info=network_info,
6588                                   block_device_info=block_device_info)
6589         except Exception:
6590             with excutils.save_and_reraise_exception(logger=LOG):
6591                 LOG.exception('Instance failed to spawn',
6592                               instance=instance)
6593                 # Cleanup allocations created by the scheduler on this host
6594                 # since we failed to spawn the instance. We do this both if
6595                 # the instance claim failed with ComputeResourcesUnavailable
6596                 # or if we did claim but the spawn failed, because aborting the
6597                 # instance claim will not remove the allocations.
6598                 self.reportclient.delete_allocation_for_instance(context,
6599                                                                  instance.uuid)
6600                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
6601                 self._terminate_volume_connections(context, instance, bdms)
6602                 # The reverts_task_state decorator on unshelve_instance will
6603                 # eventually save these updates.
6604                 self._nil_out_instance_obj_host_and_node(instance)
6605 
6606         if image:
6607             instance.image_ref = shelved_image_ref
6608             self._delete_snapshot_of_shelved_instance(context, instance,
6609                                                       image['id'])
6610 
6611         self._unshelve_instance_key_restore(instance, scrubbed_keys)
6612         self._update_instance_after_spawn(instance)
6613         # Delete system_metadata for a shelved instance
6614         compute_utils.remove_shelved_keys_from_system_metadata(instance)
6615 
6616         instance.save(expected_task_state=task_states.SPAWNING)
6617         self._update_scheduler_instance_info(context, instance)
6618         self._notify_about_instance_usage(context, instance, 'unshelve.end')
6619         compute_utils.notify_about_instance_action(context, instance,
6620                 self.host, action=fields.NotificationAction.UNSHELVE,
6621                 phase=fields.NotificationPhase.END, bdms=bdms)
6622 
6623     # TODO(stephenfin): Remove this in RPC 6.0 since it's nova-network only
6624     @messaging.expected_exceptions(NotImplementedError)
6625     @wrap_instance_fault
6626     def reset_network(self, context, instance):
6627         """Reset networking on the given instance."""
6628         LOG.debug('Reset network', instance=instance)
6629         self.driver.reset_network(instance)
6630 
6631     def _inject_network_info(self, instance, network_info):
6632         """Inject network info for the given instance."""
6633         LOG.debug('Inject network info', instance=instance)
6634         LOG.debug('network_info to inject: |%s|', network_info,
6635                   instance=instance)
6636 
6637         self.driver.inject_network_info(instance, network_info)
6638 
6639     @wrap_instance_fault
6640     def inject_network_info(self, context, instance):
6641         """Inject network info, but don't return the info."""
6642         network_info = self.network_api.get_instance_nw_info(context, instance)
6643         self._inject_network_info(instance, network_info)
6644 
6645     @messaging.expected_exceptions(NotImplementedError,
6646                                    exception.ConsoleNotAvailable,
6647                                    exception.InstanceNotFound)
6648     @wrap_exception()
6649     @wrap_instance_fault
6650     def get_console_output(self, context, instance, tail_length):
6651         """Send the console output for the given instance."""
6652         context = context.elevated()
6653         LOG.info("Get console output", instance=instance)
6654         output = self.driver.get_console_output(context, instance)
6655 
6656         if type(output) is six.text_type:
6657             output = six.b(output)
6658 
6659         if tail_length is not None:
6660             output = self._tail_log(output, tail_length)
6661 
6662         return output.decode('ascii', 'replace')
6663 
6664     def _tail_log(self, log, length):
6665         try:
6666             length = int(length)
6667         except ValueError:
6668             length = 0
6669 
6670         if length == 0:
6671             return b''
6672         else:
6673             return b'\n'.join(log.split(b'\n')[-int(length):])
6674 
6675     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6676                                    exception.InstanceNotReady,
6677                                    exception.InstanceNotFound,
6678                                    exception.ConsoleTypeUnavailable,
6679                                    NotImplementedError)
6680     @wrap_exception()
6681     @wrap_instance_fault
6682     def get_vnc_console(self, context, console_type, instance):
6683         """Return connection information for a vnc console."""
6684         context = context.elevated()
6685         LOG.debug("Getting vnc console", instance=instance)
6686 
6687         if not CONF.vnc.enabled:
6688             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6689 
6690         if console_type == 'novnc':
6691             # For essex, novncproxy_base_url must include the full path
6692             # including the html file (like http://myhost/vnc_auto.html)
6693             access_url_base = CONF.vnc.novncproxy_base_url
6694         else:
6695             raise exception.ConsoleTypeInvalid(console_type=console_type)
6696 
6697         try:
6698             # Retrieve connect info from driver, and then decorate with our
6699             # access info token
6700             console = self.driver.get_vnc_console(context, instance)
6701             console_auth = objects.ConsoleAuthToken(
6702                 context=context,
6703                 console_type=console_type,
6704                 host=console.host,
6705                 port=console.port,
6706                 internal_access_path=console.internal_access_path,
6707                 instance_uuid=instance.uuid,
6708                 access_url_base=access_url_base,
6709             )
6710             console_auth.authorize(CONF.consoleauth.token_ttl)
6711             connect_info = console.get_connection_info(
6712                 console_auth.token, console_auth.access_url)
6713 
6714         except exception.InstanceNotFound:
6715             if instance.vm_state != vm_states.BUILDING:
6716                 raise
6717             raise exception.InstanceNotReady(instance_id=instance.uuid)
6718 
6719         return connect_info
6720 
6721     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6722                                    exception.InstanceNotReady,
6723                                    exception.InstanceNotFound,
6724                                    exception.ConsoleTypeUnavailable,
6725                                    NotImplementedError)
6726     @wrap_exception()
6727     @wrap_instance_fault
6728     def get_spice_console(self, context, console_type, instance):
6729         """Return connection information for a spice console."""
6730         context = context.elevated()
6731         LOG.debug("Getting spice console", instance=instance)
6732 
6733         if not CONF.spice.enabled:
6734             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6735 
6736         if console_type != 'spice-html5':
6737             raise exception.ConsoleTypeInvalid(console_type=console_type)
6738 
6739         try:
6740             # Retrieve connect info from driver, and then decorate with our
6741             # access info token
6742             console = self.driver.get_spice_console(context, instance)
6743             console_auth = objects.ConsoleAuthToken(
6744                 context=context,
6745                 console_type=console_type,
6746                 host=console.host,
6747                 port=console.port,
6748                 internal_access_path=console.internal_access_path,
6749                 instance_uuid=instance.uuid,
6750                 access_url_base=CONF.spice.html5proxy_base_url,
6751             )
6752             console_auth.authorize(CONF.consoleauth.token_ttl)
6753             connect_info = console.get_connection_info(
6754                 console_auth.token, console_auth.access_url)
6755 
6756         except exception.InstanceNotFound:
6757             if instance.vm_state != vm_states.BUILDING:
6758                 raise
6759             raise exception.InstanceNotReady(instance_id=instance.uuid)
6760 
6761         return connect_info
6762 
6763     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6764                                    exception.InstanceNotReady,
6765                                    exception.InstanceNotFound,
6766                                    exception.ConsoleTypeUnavailable,
6767                                    NotImplementedError)
6768     @wrap_exception()
6769     @wrap_instance_fault
6770     def get_rdp_console(self, context, console_type, instance):
6771         """Return connection information for a RDP console."""
6772         context = context.elevated()
6773         LOG.debug("Getting RDP console", instance=instance)
6774 
6775         if not CONF.rdp.enabled:
6776             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6777 
6778         if console_type != 'rdp-html5':
6779             raise exception.ConsoleTypeInvalid(console_type=console_type)
6780 
6781         try:
6782             # Retrieve connect info from driver, and then decorate with our
6783             # access info token
6784             console = self.driver.get_rdp_console(context, instance)
6785             console_auth = objects.ConsoleAuthToken(
6786                 context=context,
6787                 console_type=console_type,
6788                 host=console.host,
6789                 port=console.port,
6790                 internal_access_path=console.internal_access_path,
6791                 instance_uuid=instance.uuid,
6792                 access_url_base=CONF.rdp.html5_proxy_base_url,
6793             )
6794             console_auth.authorize(CONF.consoleauth.token_ttl)
6795             connect_info = console.get_connection_info(
6796                 console_auth.token, console_auth.access_url)
6797 
6798         except exception.InstanceNotFound:
6799             if instance.vm_state != vm_states.BUILDING:
6800                 raise
6801             raise exception.InstanceNotReady(instance_id=instance.uuid)
6802 
6803         return connect_info
6804 
6805     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6806                                    exception.InstanceNotReady,
6807                                    exception.InstanceNotFound,
6808                                    exception.ConsoleTypeUnavailable,
6809                                    NotImplementedError)
6810     @wrap_exception()
6811     @wrap_instance_fault
6812     def get_mks_console(self, context, console_type, instance):
6813         """Return connection information for a MKS console."""
6814         context = context.elevated()
6815         LOG.debug("Getting MKS console", instance=instance)
6816 
6817         if not CONF.mks.enabled:
6818             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6819 
6820         if console_type != 'webmks':
6821             raise exception.ConsoleTypeInvalid(console_type=console_type)
6822 
6823         try:
6824             # Retrieve connect info from driver, and then decorate with our
6825             # access info token
6826             console = self.driver.get_mks_console(context, instance)
6827             console_auth = objects.ConsoleAuthToken(
6828                 context=context,
6829                 console_type=console_type,
6830                 host=console.host,
6831                 port=console.port,
6832                 internal_access_path=console.internal_access_path,
6833                 instance_uuid=instance.uuid,
6834                 access_url_base=CONF.mks.mksproxy_base_url,
6835             )
6836             console_auth.authorize(CONF.consoleauth.token_ttl)
6837             connect_info = console.get_connection_info(
6838                 console_auth.token, console_auth.access_url)
6839 
6840         except exception.InstanceNotFound:
6841             if instance.vm_state != vm_states.BUILDING:
6842                 raise
6843             raise exception.InstanceNotReady(instance_id=instance.uuid)
6844 
6845         return connect_info
6846 
6847     @messaging.expected_exceptions(
6848         exception.ConsoleTypeInvalid,
6849         exception.InstanceNotReady,
6850         exception.InstanceNotFound,
6851         exception.ConsoleTypeUnavailable,
6852         exception.SocketPortRangeExhaustedException,
6853         exception.ImageSerialPortNumberInvalid,
6854         exception.ImageSerialPortNumberExceedFlavorValue,
6855         NotImplementedError)
6856     @wrap_exception()
6857     @wrap_instance_fault
6858     def get_serial_console(self, context, console_type, instance):
6859         """Returns connection information for a serial console."""
6860 
6861         LOG.debug("Getting serial console", instance=instance)
6862 
6863         if not CONF.serial_console.enabled:
6864             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6865 
6866         context = context.elevated()
6867 
6868         try:
6869             # Retrieve connect info from driver, and then decorate with our
6870             # access info token
6871             console = self.driver.get_serial_console(context, instance)
6872             console_auth = objects.ConsoleAuthToken(
6873                 context=context,
6874                 console_type=console_type,
6875                 host=console.host,
6876                 port=console.port,
6877                 internal_access_path=console.internal_access_path,
6878                 instance_uuid=instance.uuid,
6879                 access_url_base=CONF.serial_console.base_url,
6880             )
6881             console_auth.authorize(CONF.consoleauth.token_ttl)
6882             connect_info = console.get_connection_info(
6883                 console_auth.token, console_auth.access_url)
6884 
6885         except exception.InstanceNotFound:
6886             if instance.vm_state != vm_states.BUILDING:
6887                 raise
6888             raise exception.InstanceNotReady(instance_id=instance.uuid)
6889 
6890         return connect_info
6891 
6892     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6893                                    exception.InstanceNotReady,
6894                                    exception.InstanceNotFound)
6895     @wrap_exception()
6896     @wrap_instance_fault
6897     def validate_console_port(self, ctxt, instance, port, console_type):
6898         if console_type == "spice-html5":
6899             console_info = self.driver.get_spice_console(ctxt, instance)
6900         elif console_type == "rdp-html5":
6901             console_info = self.driver.get_rdp_console(ctxt, instance)
6902         elif console_type == "serial":
6903             console_info = self.driver.get_serial_console(ctxt, instance)
6904         elif console_type == "webmks":
6905             console_info = self.driver.get_mks_console(ctxt, instance)
6906         else:
6907             console_info = self.driver.get_vnc_console(ctxt, instance)
6908 
6909         # Some drivers may return an int on console_info.port but the port
6910         # variable in this method is a string, so cast to be sure we are
6911         # comparing the correct types.
6912         return str(console_info.port) == port
6913 
6914     @wrap_exception()
6915     @reverts_task_state
6916     @wrap_instance_fault
6917     def reserve_block_device_name(self, context, instance, device,
6918                                   volume_id, disk_bus, device_type, tag,
6919                                   multiattach):
6920         if (tag and not
6921                 self.driver.capabilities.get('supports_tagged_attach_volume',
6922                                              False)):
6923             raise exception.VolumeTaggedAttachNotSupported()
6924 
6925         if (multiattach and not
6926                 self.driver.capabilities.get('supports_multiattach', False)):
6927             raise exception.MultiattachNotSupportedByVirtDriver(
6928                 volume_id=volume_id)
6929 
6930         @utils.synchronized(instance.uuid)
6931         def do_reserve():
6932             bdms = (
6933                 objects.BlockDeviceMappingList.get_by_instance_uuid(
6934                     context, instance.uuid))
6935 
6936             # NOTE(ndipanov): We need to explicitly set all the fields on the
6937             #                 object so that obj_load_attr does not fail
6938             new_bdm = objects.BlockDeviceMapping(
6939                     context=context,
6940                     source_type='volume', destination_type='volume',
6941                     instance_uuid=instance.uuid, boot_index=None,
6942                     volume_id=volume_id,
6943                     device_name=device, guest_format=None,
6944                     disk_bus=disk_bus, device_type=device_type, tag=tag)
6945 
6946             new_bdm.device_name = self._get_device_name_for_instance(
6947                     instance, bdms, new_bdm)
6948 
6949             # NOTE(vish): create bdm here to avoid race condition
6950             new_bdm.create()
6951             return new_bdm
6952 
6953         return do_reserve()
6954 
6955     @wrap_exception()
6956     @wrap_instance_event(prefix='compute')
6957     @wrap_instance_fault
6958     def attach_volume(self, context, instance, bdm):
6959         """Attach a volume to an instance."""
6960         driver_bdm = driver_block_device.convert_volume(bdm)
6961 
6962         @utils.synchronized(instance.uuid)
6963         def do_attach_volume(context, instance, driver_bdm):
6964             try:
6965                 return self._attach_volume(context, instance, driver_bdm)
6966             except Exception:
6967                 with excutils.save_and_reraise_exception():
6968                     bdm.destroy()
6969 
6970         do_attach_volume(context, instance, driver_bdm)
6971 
6972     def _attach_volume(self, context, instance, bdm):
6973         context = context.elevated()
6974         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
6975                  {'volume_id': bdm.volume_id,
6976                   'mountpoint': bdm['mount_device']},
6977                  instance=instance)
6978         compute_utils.notify_about_volume_attach_detach(
6979             context, instance, self.host,
6980             action=fields.NotificationAction.VOLUME_ATTACH,
6981             phase=fields.NotificationPhase.START,
6982             volume_id=bdm.volume_id)
6983         try:
6984             bdm.attach(context, instance, self.volume_api, self.driver,
6985                        do_driver_attach=True)
6986         except Exception as e:
6987             with excutils.save_and_reraise_exception():
6988                 LOG.exception("Failed to attach %(volume_id)s "
6989                               "at %(mountpoint)s",
6990                               {'volume_id': bdm.volume_id,
6991                                'mountpoint': bdm['mount_device']},
6992                               instance=instance)
6993                 if bdm['attachment_id']:
6994                     # Try to delete the attachment to make the volume
6995                     # available again. Note that DriverVolumeBlockDevice
6996                     # may have already deleted the attachment so ignore
6997                     # VolumeAttachmentNotFound.
6998                     try:
6999                         self.volume_api.attachment_delete(
7000                             context, bdm['attachment_id'])
7001                     except exception.VolumeAttachmentNotFound as exc:
7002                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
7003                                   exc, instance=instance)
7004                 else:
7005                     self.volume_api.unreserve_volume(context, bdm.volume_id)
7006                 compute_utils.notify_about_volume_attach_detach(
7007                     context, instance, self.host,
7008                     action=fields.NotificationAction.VOLUME_ATTACH,
7009                     phase=fields.NotificationPhase.ERROR,
7010                     exception=e,
7011                     volume_id=bdm.volume_id)
7012 
7013         info = {'volume_id': bdm.volume_id}
7014         self._notify_about_instance_usage(
7015             context, instance, "volume.attach", extra_usage_info=info)
7016         compute_utils.notify_about_volume_attach_detach(
7017             context, instance, self.host,
7018             action=fields.NotificationAction.VOLUME_ATTACH,
7019             phase=fields.NotificationPhase.END,
7020             volume_id=bdm.volume_id)
7021 
7022     def _notify_volume_usage_detach(self, context, instance, bdm):
7023         if CONF.volume_usage_poll_interval <= 0:
7024             return
7025 
7026         mp = bdm.device_name
7027         # Handle bootable volumes which will not contain /dev/
7028         if '/dev/' in mp:
7029             mp = mp[5:]
7030         try:
7031             vol_stats = self.driver.block_stats(instance, mp)
7032             if vol_stats is None:
7033                 return
7034         except NotImplementedError:
7035             return
7036 
7037         LOG.debug("Updating volume usage cache with totals", instance=instance)
7038         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
7039         vol_usage = objects.VolumeUsage(context)
7040         vol_usage.volume_id = bdm.volume_id
7041         vol_usage.instance_uuid = instance.uuid
7042         vol_usage.project_id = instance.project_id
7043         vol_usage.user_id = instance.user_id
7044         vol_usage.availability_zone = instance.availability_zone
7045         vol_usage.curr_reads = rd_req
7046         vol_usage.curr_read_bytes = rd_bytes
7047         vol_usage.curr_writes = wr_req
7048         vol_usage.curr_write_bytes = wr_bytes
7049         vol_usage.save(update_totals=True)
7050         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
7051         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
7052 
7053     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
7054                        attachment_id=None):
7055         """Detach a volume from an instance.
7056 
7057         :param context: security context
7058         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
7059         :param instance: the Instance object to detach the volume from
7060         :param destroy_bdm: if True, the corresponding BDM entry will be marked
7061                             as deleted. Disabling this is useful for operations
7062                             like rebuild, when we don't want to destroy BDM
7063         :param attachment_id: The volume attachment_id for the given instance
7064                               and volume.
7065         """
7066         volume_id = bdm.volume_id
7067         compute_utils.notify_about_volume_attach_detach(
7068             context, instance, self.host,
7069             action=fields.NotificationAction.VOLUME_DETACH,
7070             phase=fields.NotificationPhase.START,
7071             volume_id=volume_id)
7072 
7073         self._notify_volume_usage_detach(context, instance, bdm)
7074 
7075         LOG.info('Detaching volume %(volume_id)s',
7076                  {'volume_id': volume_id}, instance=instance)
7077 
7078         driver_bdm = driver_block_device.convert_volume(bdm)
7079         driver_bdm.detach(context, instance, self.volume_api, self.driver,
7080                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
7081 
7082         info = dict(volume_id=volume_id)
7083         self._notify_about_instance_usage(
7084             context, instance, "volume.detach", extra_usage_info=info)
7085         compute_utils.notify_about_volume_attach_detach(
7086             context, instance, self.host,
7087             action=fields.NotificationAction.VOLUME_DETACH,
7088             phase=fields.NotificationPhase.END,
7089             volume_id=volume_id)
7090 
7091         if 'tag' in bdm and bdm.tag:
7092             self._delete_disk_metadata(instance, bdm)
7093         if destroy_bdm:
7094             bdm.destroy()
7095 
7096     def _delete_disk_metadata(self, instance, bdm):
7097         for device in instance.device_metadata.devices:
7098             if isinstance(device, objects.DiskMetadata):
7099                 if 'serial' in device:
7100                     if device.serial == bdm.volume_id:
7101                         instance.device_metadata.devices.remove(device)
7102                         instance.save()
7103                         break
7104                 else:
7105                     # NOTE(artom) We log the entire device object because all
7106                     # fields are nullable and may not be set
7107                     LOG.warning('Unable to determine whether to clean up '
7108                                 'device metadata for disk %s', device,
7109                                 instance=instance)
7110 
7111     @wrap_exception()
7112     @wrap_instance_event(prefix='compute')
7113     @wrap_instance_fault
7114     def detach_volume(self, context, volume_id, instance, attachment_id):
7115         """Detach a volume from an instance.
7116 
7117         :param context: security context
7118         :param volume_id: the volume id
7119         :param instance: the Instance object to detach the volume from
7120         :param attachment_id: The volume attachment_id for the given instance
7121                               and volume.
7122 
7123         """
7124         @utils.synchronized(instance.uuid)
7125         def do_detach_volume(context, volume_id, instance, attachment_id):
7126             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7127                     context, volume_id, instance.uuid)
7128             self._detach_volume(context, bdm, instance,
7129                                 attachment_id=attachment_id)
7130 
7131         do_detach_volume(context, volume_id, instance, attachment_id)
7132 
7133     def _init_volume_connection(self, context, new_volume,
7134                                 old_volume_id, connector, bdm,
7135                                 new_attachment_id, mountpoint):
7136         new_volume_id = new_volume['id']
7137         if new_attachment_id is None:
7138             # We're dealing with an old-style attachment so initialize the
7139             # connection so we can get the connection_info.
7140             new_cinfo = self.volume_api.initialize_connection(context,
7141                                                               new_volume_id,
7142                                                               connector)
7143         else:
7144             # Check for multiattach on the new volume and if True, check to
7145             # see if the virt driver supports multiattach.
7146             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
7147             # and should be consolidated into some common code at some point.
7148             vol_multiattach = new_volume.get('multiattach', False)
7149             virt_multiattach = self.driver.capabilities.get(
7150                 'supports_multiattach', False)
7151             if vol_multiattach and not virt_multiattach:
7152                 raise exception.MultiattachNotSupportedByVirtDriver(
7153                     volume_id=new_volume_id)
7154 
7155             # This is a new style attachment and the API created the new
7156             # volume attachment and passed the id to the compute over RPC.
7157             # At this point we need to update the new volume attachment with
7158             # the host connector, which will give us back the new attachment
7159             # connection_info.
7160             new_cinfo = self.volume_api.attachment_update(
7161                 context, new_attachment_id, connector,
7162                 mountpoint)['connection_info']
7163 
7164             if vol_multiattach:
7165                 # This will be used by the volume driver to determine the
7166                 # proper disk configuration.
7167                 new_cinfo['multiattach'] = True
7168 
7169         old_cinfo = jsonutils.loads(bdm['connection_info'])
7170         if old_cinfo and 'serial' not in old_cinfo:
7171             old_cinfo['serial'] = old_volume_id
7172         # NOTE(lyarwood): serial is not always present in the returned
7173         # connection_info so set it if it is missing as we do in
7174         # DriverVolumeBlockDevice.attach().
7175         if 'serial' not in new_cinfo:
7176             new_cinfo['serial'] = new_volume_id
7177         return (old_cinfo, new_cinfo)
7178 
7179     def _swap_volume(self, context, instance, bdm, connector,
7180                      old_volume_id, new_volume, resize_to,
7181                      new_attachment_id, is_cinder_migration):
7182         new_volume_id = new_volume['id']
7183         mountpoint = bdm['device_name']
7184         failed = False
7185         new_cinfo = None
7186         try:
7187             old_cinfo, new_cinfo = self._init_volume_connection(
7188                 context, new_volume, old_volume_id, connector,
7189                 bdm, new_attachment_id, mountpoint)
7190             # NOTE(lyarwood): The Libvirt driver, the only virt driver
7191             # currently implementing swap_volume, will modify the contents of
7192             # new_cinfo when connect_volume is called. This is then saved to
7193             # the BDM in swap_volume for future use outside of this flow.
7194             msg = ("swap_volume: Calling driver volume swap with "
7195                    "connection infos: new: %(new_cinfo)s; "
7196                    "old: %(old_cinfo)s" %
7197                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
7198             # Both new and old info might contain password
7199             LOG.debug(strutils.mask_password(msg), instance=instance)
7200 
7201             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
7202                                     mountpoint, resize_to)
7203             if new_attachment_id:
7204                 self.volume_api.attachment_complete(context, new_attachment_id)
7205             msg = ("swap_volume: Driver volume swap returned, new "
7206                    "connection_info is now : %(new_cinfo)s" %
7207                    {'new_cinfo': new_cinfo})
7208             LOG.debug(strutils.mask_password(msg))
7209         except Exception as ex:
7210             failed = True
7211             with excutils.save_and_reraise_exception():
7212                 compute_utils.notify_about_volume_swap(
7213                     context, instance, self.host,
7214                     fields.NotificationPhase.ERROR,
7215                     old_volume_id, new_volume_id, ex)
7216                 if new_cinfo:
7217                     msg = ("Failed to swap volume %(old_volume_id)s "
7218                            "for %(new_volume_id)s")
7219                     LOG.exception(msg, {'old_volume_id': old_volume_id,
7220                                         'new_volume_id': new_volume_id},
7221                                   instance=instance)
7222                 else:
7223                     msg = ("Failed to connect to volume %(volume_id)s "
7224                            "with volume at %(mountpoint)s")
7225                     LOG.exception(msg, {'volume_id': new_volume_id,
7226                                         'mountpoint': bdm['device_name']},
7227                                   instance=instance)
7228 
7229                 # The API marked the volume as 'detaching' for the old volume
7230                 # so we need to roll that back so the volume goes back to
7231                 # 'in-use' state.
7232                 self.volume_api.roll_detaching(context, old_volume_id)
7233 
7234                 if new_attachment_id is None:
7235                     # The API reserved the new volume so it would be in
7236                     # 'attaching' status, so we need to unreserve it so it
7237                     # goes back to 'available' status.
7238                     self.volume_api.unreserve_volume(context, new_volume_id)
7239                 else:
7240                     # This is a new style attachment for the new volume, which
7241                     # was created in the API. We just need to delete it here
7242                     # to put the new volume back into 'available' status.
7243                     self.volume_api.attachment_delete(
7244                         context, new_attachment_id)
7245         finally:
7246             # TODO(mriedem): This finally block is terribly confusing and is
7247             # trying to do too much. We should consider removing the finally
7248             # block and move whatever needs to happen on success and failure
7249             # into the blocks above for clarity, even if it means a bit of
7250             # redundant code.
7251             conn_volume = new_volume_id if failed else old_volume_id
7252             if new_cinfo:
7253                 LOG.debug("swap_volume: removing Cinder connection "
7254                           "for volume %(volume)s", {'volume': conn_volume},
7255                           instance=instance)
7256                 if bdm.attachment_id is None:
7257                     # This is the pre-3.44 flow for new-style volume
7258                     # attachments so just terminate the connection.
7259                     self.volume_api.terminate_connection(context,
7260                                                          conn_volume,
7261                                                          connector)
7262                 else:
7263                     # This is a new style volume attachment. If we failed, then
7264                     # the new attachment was already deleted above in the
7265                     # exception block and we have nothing more to do here. If
7266                     # swap_volume was successful in the driver, then we need to
7267                     # "detach" the original attachment by deleting it.
7268                     if not failed:
7269                         self.volume_api.attachment_delete(
7270                             context, bdm.attachment_id)
7271 
7272             # Need to make some decisions based on whether this was
7273             # a Cinder initiated migration or not. The callback to
7274             # migration completion isn't needed in the case of a
7275             # nova initiated simple swap of two volume
7276             # "volume-update" call so skip that. The new attachment
7277             # scenarios will give us a new attachment record and
7278             # that's what we want.
7279             if bdm.attachment_id and not is_cinder_migration:
7280                 # we don't callback to cinder
7281                 comp_ret = {'save_volume_id': new_volume_id}
7282             else:
7283                 # NOTE(lyarwood): The following call to
7284                 # os-migrate-volume-completion returns a dict containing
7285                 # save_volume_id, this volume id has two possible values :
7286                 # 1. old_volume_id if we are migrating (retyping) volumes
7287                 # 2. new_volume_id if we are swapping between two existing
7288                 #    volumes
7289                 # This volume id is later used to update the volume_id and
7290                 # connection_info['serial'] of the BDM.
7291                 comp_ret = self.volume_api.migrate_volume_completion(
7292                                                           context,
7293                                                           old_volume_id,
7294                                                           new_volume_id,
7295                                                           error=failed)
7296                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
7297                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
7298                           instance=instance)
7299 
7300         return (comp_ret, new_cinfo)
7301 
7302     @wrap_exception()
7303     @wrap_instance_event(prefix='compute')
7304     @wrap_instance_fault
7305     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
7306                     new_attachment_id):
7307         """Swap volume for an instance."""
7308         context = context.elevated()
7309 
7310         compute_utils.notify_about_volume_swap(
7311             context, instance, self.host,
7312             fields.NotificationPhase.START,
7313             old_volume_id, new_volume_id)
7314 
7315         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7316                 context, old_volume_id, instance.uuid)
7317         connector = self.driver.get_volume_connector(instance)
7318 
7319         resize_to = 0
7320         old_volume = self.volume_api.get(context, old_volume_id)
7321         # Yes this is a tightly-coupled state check of what's going on inside
7322         # cinder, but we need this while we still support old (v1/v2) and
7323         # new style attachments (v3.44). Once we drop support for old style
7324         # attachments we could think about cleaning up the cinder-initiated
7325         # swap volume API flows.
7326         is_cinder_migration = False
7327         if 'migration_status' in old_volume:
7328             is_cinder_migration = old_volume['migration_status'] == 'migrating'
7329         old_vol_size = old_volume['size']
7330         new_volume = self.volume_api.get(context, new_volume_id)
7331         new_vol_size = new_volume['size']
7332         if new_vol_size > old_vol_size:
7333             resize_to = new_vol_size
7334 
7335         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
7336                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
7337                  instance=instance)
7338         comp_ret, new_cinfo = self._swap_volume(context,
7339                                                 instance,
7340                                                 bdm,
7341                                                 connector,
7342                                                 old_volume_id,
7343                                                 new_volume,
7344                                                 resize_to,
7345                                                 new_attachment_id,
7346                                                 is_cinder_migration)
7347 
7348         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
7349         # correct volume_id returned by Cinder.
7350         save_volume_id = comp_ret['save_volume_id']
7351         new_cinfo['serial'] = save_volume_id
7352         values = {
7353             'connection_info': jsonutils.dumps(new_cinfo),
7354             'source_type': 'volume',
7355             'destination_type': 'volume',
7356             'snapshot_id': None,
7357             'volume_id': save_volume_id,
7358             'no_device': None}
7359 
7360         if resize_to:
7361             values['volume_size'] = resize_to
7362 
7363         if new_attachment_id is not None:
7364             # This was a volume swap for a new-style attachment so we
7365             # need to update the BDM attachment_id for the new attachment.
7366             values['attachment_id'] = new_attachment_id
7367 
7368         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
7369                   "%(updates)s", {'volume_id': bdm.volume_id,
7370                                   'updates': values},
7371                   instance=instance)
7372         bdm.update(values)
7373         bdm.save()
7374 
7375         compute_utils.notify_about_volume_swap(
7376             context, instance, self.host,
7377             fields.NotificationPhase.END,
7378             old_volume_id, new_volume_id)
7379 
7380     @wrap_exception()
7381     def remove_volume_connection(self, context, volume_id, instance):
7382         """Remove the volume connection on this host
7383 
7384         Detach the volume from this instance on this host, and if this is
7385         the cinder v2 flow, call cinder to terminate the connection.
7386         """
7387         try:
7388             # NOTE(mriedem): If the BDM was just passed directly we would not
7389             # need to do this DB query, but this is an RPC interface so
7390             # changing that requires some care.
7391             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7392                     context, volume_id, instance.uuid)
7393             # NOTE(mriedem): Normally we would pass delete_attachment=True to
7394             # _remove_volume_connection to delete a v3 style volume attachment,
7395             # but this method is RPC called from _rollback_live_migration which
7396             # already deletes the attachment, so because of that tight coupling
7397             # we cannot simply delete a v3 style attachment here without
7398             # needing to do some behavior modification of that
7399             # _rollback_live_migration flow which gets messy.
7400             self._remove_volume_connection(context, bdm, instance)
7401         except exception.NotFound:
7402             pass
7403 
7404     def _remove_volume_connection(self, context, bdm, instance,
7405                                   delete_attachment=False):
7406         """Remove the volume connection on this host
7407 
7408         Detach the volume from this instance on this host.
7409 
7410         :param context: nova auth request context
7411         :param bdm: BlockDeviceMapping object for a volume attached to the
7412             instance
7413         :param instance: Instance object with a volume attached represented
7414             by ``bdm``
7415         :param delete_attachment: If ``bdm.attachment_id`` is not None the
7416             attachment was made as a cinder v3 style attachment and if True,
7417             then deletes the volume attachment, otherwise just terminates
7418             the connection for a cinder legacy style connection.
7419         """
7420         driver_bdm = driver_block_device.convert_volume(bdm)
7421         driver_bdm.driver_detach(context, instance,
7422                                  self.volume_api, self.driver)
7423         if bdm.attachment_id is None:
7424             # cinder v2 api flow
7425             connector = self.driver.get_volume_connector(instance)
7426             self.volume_api.terminate_connection(context, bdm.volume_id,
7427                                                  connector)
7428         elif delete_attachment:
7429             # cinder v3 api flow
7430             self.volume_api.attachment_delete(context, bdm.attachment_id)
7431 
7432     def _deallocate_port_for_instance(self, context, instance, port_id,
7433                                       raise_on_failure=False):
7434         try:
7435             result = self.network_api.deallocate_port_for_instance(
7436                 context, instance, port_id)
7437             __, port_allocation = result
7438         except Exception as ex:
7439             with excutils.save_and_reraise_exception(
7440                     reraise=raise_on_failure):
7441                 LOG.warning('Failed to deallocate port %(port_id)s '
7442                             'for instance. Error: %(error)s',
7443                             {'port_id': port_id, 'error': ex},
7444                             instance=instance)
7445         else:
7446             if port_allocation:
7447                 # Deallocate the resources in placement that were used by the
7448                 # detached port.
7449                 try:
7450                     client = self.reportclient
7451                     client.remove_resources_from_instance_allocation(
7452                         context, instance.uuid, port_allocation)
7453                 except Exception as ex:
7454                     # We always raise here as it is not a race condition where
7455                     # somebody has already deleted the port we want to cleanup.
7456                     # Here we see that the port exists, the allocation exists,
7457                     # but we cannot clean it up so we will actually leak
7458                     # allocations.
7459                     with excutils.save_and_reraise_exception():
7460                         LOG.warning('Failed to remove resource allocation '
7461                                     'of port %(port_id)s for instance. Error: '
7462                                     '%(error)s',
7463                                     {'port_id': port_id, 'error': ex},
7464                                     instance=instance)
7465 
7466     # TODO(mriedem): There are likely race failures which can result in
7467     # NotFound and QuotaError exceptions getting traced as well.
7468     @messaging.expected_exceptions(
7469         # Do not log a traceback for user errors. We use Invalid generically
7470         # since this method can raise lots of different exceptions:
7471         # AttachInterfaceNotSupported
7472         # NetworkInterfaceTaggedAttachNotSupported
7473         # NetworkAmbiguous
7474         # PortNotUsable
7475         # PortInUse
7476         # PortNotUsableDNS
7477         # AttachSRIOVPortNotSupported
7478         # NetworksWithQoSPolicyNotSupported
7479         exception.Invalid)
7480     @wrap_exception()
7481     @wrap_instance_event(prefix='compute')
7482     @wrap_instance_fault
7483     def attach_interface(self, context, instance, network_id, port_id,
7484                          requested_ip, tag):
7485         """Use hotplug to add an network adapter to an instance."""
7486         if not self.driver.capabilities.get('supports_attach_interface',
7487                                             False):
7488             raise exception.AttachInterfaceNotSupported(
7489                 instance_uuid=instance.uuid)
7490         if (tag and not
7491             self.driver.capabilities.get('supports_tagged_attach_interface',
7492                                          False)):
7493             raise exception.NetworkInterfaceTaggedAttachNotSupported()
7494 
7495         compute_utils.notify_about_instance_action(
7496             context, instance, self.host,
7497             action=fields.NotificationAction.INTERFACE_ATTACH,
7498             phase=fields.NotificationPhase.START)
7499 
7500         bind_host_id = self.driver.network_binding_host_id(context, instance)
7501         network_info = self.network_api.allocate_port_for_instance(
7502             context, instance, port_id, network_id, requested_ip,
7503             bind_host_id=bind_host_id, tag=tag)
7504         if len(network_info) != 1:
7505             LOG.error('allocate_port_for_instance returned %(ports)s '
7506                       'ports', {'ports': len(network_info)})
7507             # TODO(elod.illes): an instance.interface_attach.error notification
7508             # should be sent here
7509             raise exception.InterfaceAttachFailed(
7510                     instance_uuid=instance.uuid)
7511         image_meta = objects.ImageMeta.from_instance(instance)
7512 
7513         try:
7514             self.driver.attach_interface(context, instance, image_meta,
7515                                          network_info[0])
7516         except exception.NovaException as ex:
7517             port_id = network_info[0].get('id')
7518             LOG.warning("attach interface failed , try to deallocate "
7519                         "port %(port_id)s, reason: %(msg)s",
7520                         {'port_id': port_id, 'msg': ex},
7521                         instance=instance)
7522             self._deallocate_port_for_instance(context, instance, port_id)
7523 
7524             compute_utils.notify_about_instance_action(
7525                 context, instance, self.host,
7526                 action=fields.NotificationAction.INTERFACE_ATTACH,
7527                 phase=fields.NotificationPhase.ERROR,
7528                 exception=ex)
7529 
7530             raise exception.InterfaceAttachFailed(
7531                 instance_uuid=instance.uuid)
7532 
7533         compute_utils.notify_about_instance_action(
7534             context, instance, self.host,
7535             action=fields.NotificationAction.INTERFACE_ATTACH,
7536             phase=fields.NotificationPhase.END)
7537 
7538         return network_info[0]
7539 
7540     @wrap_exception()
7541     @wrap_instance_event(prefix='compute')
7542     @wrap_instance_fault
7543     def detach_interface(self, context, instance, port_id):
7544         """Detach a network adapter from an instance."""
7545         network_info = instance.info_cache.network_info
7546         condemned = None
7547         for vif in network_info:
7548             if vif['id'] == port_id:
7549                 condemned = vif
7550                 break
7551         if condemned is None:
7552             raise exception.PortNotFound(_("Port %s is not "
7553                                            "attached") % port_id)
7554 
7555         compute_utils.notify_about_instance_action(
7556             context, instance, self.host,
7557             action=fields.NotificationAction.INTERFACE_DETACH,
7558             phase=fields.NotificationPhase.START)
7559 
7560         try:
7561             self.driver.detach_interface(context, instance, condemned)
7562         except exception.NovaException as ex:
7563             # If the instance was deleted before the interface was detached,
7564             # just log it at debug.
7565             log_level = (logging.DEBUG
7566                          if isinstance(ex, exception.InstanceNotFound)
7567                          else logging.WARNING)
7568             LOG.log(log_level,
7569                     "Detach interface failed, port_id=%(port_id)s, reason: "
7570                     "%(msg)s", {'port_id': port_id, 'msg': ex},
7571                     instance=instance)
7572             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
7573         else:
7574             self._deallocate_port_for_instance(
7575                 context, instance, port_id, raise_on_failure=True)
7576 
7577         compute_utils.notify_about_instance_action(
7578             context, instance, self.host,
7579             action=fields.NotificationAction.INTERFACE_DETACH,
7580             phase=fields.NotificationPhase.END)
7581 
7582     def _get_compute_info(self, context, host):
7583         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
7584             context, host)
7585 
7586     # TODO(stephenfin): Remove the unused instance argument in RPC version 6.0
7587     @wrap_exception()
7588     def check_instance_shared_storage(self, ctxt, instance, data):
7589         """Check if the instance files are shared
7590 
7591         :param ctxt: security context
7592         :param instance: dict of instance data
7593         :param data: result of driver.check_instance_shared_storage_local
7594 
7595         Returns True if instance disks located on shared storage and
7596         False otherwise.
7597         """
7598         return self.driver.check_instance_shared_storage_remote(ctxt, data)
7599 
7600     def _dest_can_numa_live_migrate(self, dest_check_data, migration):
7601         # TODO(artom) If we have a libvirt driver we expect it to set
7602         # dst_supports_numa_live_migration, but we have to remove it if we
7603         # did not get a migration from the conductor, indicating that it
7604         # cannot send RPC 5.3. This check can be removed in RPC 6.0.
7605         if ('dst_supports_numa_live_migration' in dest_check_data and
7606                 dest_check_data.dst_supports_numa_live_migration and
7607                 not migration):
7608             delattr(dest_check_data, 'dst_supports_numa_live_migration')
7609         return dest_check_data
7610 
7611     @wrap_exception()
7612     @wrap_instance_event(prefix='compute')
7613     @wrap_instance_fault
7614     def check_can_live_migrate_destination(self, ctxt, instance,
7615                                            block_migration, disk_over_commit,
7616                                            migration=None, limits=None):
7617         """Check if it is possible to execute live migration.
7618 
7619         This runs checks on the destination host, and then calls
7620         back to the source host to check the results.
7621 
7622         :param context: security context
7623         :param instance: dict of instance data
7624         :param block_migration: if true, prepare for block migration
7625                                 if None, calculate it in driver
7626         :param disk_over_commit: if true, allow disk over commit
7627                                  if None, ignore disk usage checking
7628         :param migration: objects.Migration object for this live migration.
7629         :param limits: objects.SchedulerLimits object for this live migration.
7630         :returns: a LiveMigrateData object (hypervisor-dependent)
7631         """
7632         src_compute_info = obj_base.obj_to_primitive(
7633             self._get_compute_info(ctxt, instance.host))
7634         dst_compute_info = obj_base.obj_to_primitive(
7635             self._get_compute_info(ctxt, self.host))
7636         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
7637             instance, src_compute_info, dst_compute_info,
7638             block_migration, disk_over_commit)
7639         dest_check_data = self._dest_can_numa_live_migrate(dest_check_data,
7640                                                            migration)
7641         LOG.debug('destination check data is %s', dest_check_data)
7642         try:
7643             allocs = self.reportclient.get_allocations_for_consumer(
7644                 ctxt, instance.uuid)
7645             migrate_data = self.compute_rpcapi.check_can_live_migrate_source(
7646                 ctxt, instance, dest_check_data)
7647             if ('src_supports_numa_live_migration' in migrate_data and
7648                     migrate_data.src_supports_numa_live_migration):
7649                 migrate_data = self._live_migration_claim(
7650                     ctxt, instance, migrate_data, migration, limits, allocs)
7651             elif 'dst_supports_numa_live_migration' in dest_check_data:
7652                 LOG.info('Destination was ready for NUMA live migration, '
7653                          'but source is either too old, or is set to an '
7654                          'older upgrade level.', instance=instance)
7655             # Create migrate_data vifs
7656             migrate_data.vifs = \
7657                 migrate_data_obj.VIFMigrateData.create_skeleton_migrate_vifs(
7658                     instance.get_network_info())
7659             # Claim PCI devices for VIFs on destination (if needed)
7660             port_id_to_pci = self._claim_pci_for_instance_vifs(ctxt, instance)
7661             # Update migrate VIFs with the newly claimed PCI devices
7662             self._update_migrate_vifs_profile_with_pci(migrate_data.vifs,
7663                                                        port_id_to_pci)
7664         finally:
7665             self.driver.cleanup_live_migration_destination_check(ctxt,
7666                     dest_check_data)
7667         return migrate_data
7668 
7669     def _live_migration_claim(self, ctxt, instance, migrate_data,
7670                               migration, limits, allocs):
7671         """Runs on the destination and does a resources claim, if necessary.
7672         Currently, only NUMA live migrations require it.
7673 
7674         :param ctxt: Request context
7675         :param instance: The Instance being live migrated
7676         :param migrate_data: The MigrateData object for this live migration
7677         :param migration: The Migration object for this live migration
7678         :param limits: The SchedulerLimits object for this live migration
7679         :returns: migrate_data with dst_numa_info set if necessary
7680         """
7681         try:
7682             # NOTE(artom) We might have gotten here from _find_destination() in
7683             # the conductor live migrate task. At that point,
7684             # migration.dest_node is not set yet (nor should it be, we're still
7685             # looking for a destination, after all). Therefore, we cannot use
7686             # migration.dest_node here and must use self._get_nodename().
7687             claim = self.rt.live_migration_claim(
7688                 ctxt, instance, self._get_nodename(instance), migration,
7689                 limits, allocs)
7690             LOG.debug('Created live migration claim.', instance=instance)
7691         except exception.ComputeResourcesUnavailable as e:
7692             raise exception.MigrationPreCheckError(
7693                 reason=e.format_message())
7694         return self.driver.post_claim_migrate_data(ctxt, instance,
7695                                                    migrate_data, claim)
7696 
7697     def _source_can_numa_live_migrate(self, ctxt, dest_check_data,
7698                                       source_check_data):
7699         # TODO(artom) Our virt driver may have told us that it supports NUMA
7700         # live migration. However, the following other conditions must be met
7701         # for a NUMA live migration to happen:
7702         # 1. We got a True dst_supports_numa_live_migration in
7703         #    dest_check_data, indicating that the dest virt driver supports
7704         #    NUMA live migration and that the conductor can send RPC 5.3 and
7705         #    that the destination compute manager can receive it.
7706         # 2. Ourselves, the source, can send RPC 5.3. There's no
7707         #    sentinel/parameter for this, so we just ask our rpcapi directly.
7708         # If any of these are not met, we need to remove the
7709         # src_supports_numa_live_migration flag from source_check_data to avoid
7710         # incorrectly initiating a NUMA live migration.
7711         # All of this can be removed in RPC 6.0/objects 2.0.
7712         can_numa_live_migrate = (
7713             'dst_supports_numa_live_migration' in dest_check_data and
7714             dest_check_data.dst_supports_numa_live_migration and
7715             self.compute_rpcapi.supports_numa_live_migration(ctxt))
7716         if ('src_supports_numa_live_migration' in source_check_data and
7717                 source_check_data.src_supports_numa_live_migration and
7718                 not can_numa_live_migrate):
7719             delattr(source_check_data, 'src_supports_numa_live_migration')
7720         return source_check_data
7721 
7722     @wrap_exception()
7723     @wrap_instance_event(prefix='compute')
7724     @wrap_instance_fault
7725     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
7726         """Check if it is possible to execute live migration.
7727 
7728         This checks if the live migration can succeed, based on the
7729         results from check_can_live_migrate_destination.
7730 
7731         :param ctxt: security context
7732         :param instance: dict of instance data
7733         :param dest_check_data: result of check_can_live_migrate_destination
7734         :returns: a LiveMigrateData object
7735         """
7736         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7737             ctxt, instance.uuid)
7738         is_volume_backed = compute_utils.is_volume_backed_instance(
7739             ctxt, instance, bdms)
7740         dest_check_data.is_volume_backed = is_volume_backed
7741         block_device_info = self._get_instance_block_device_info(
7742                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
7743         result = self.driver.check_can_live_migrate_source(ctxt, instance,
7744                                                            dest_check_data,
7745                                                            block_device_info)
7746         result = self._source_can_numa_live_migrate(ctxt, dest_check_data,
7747                                                     result)
7748         LOG.debug('source check data is %s', result)
7749         return result
7750 
7751     # TODO(mriedem): Remove the block_migration argument in v6.0 of the compute
7752     # RPC API.
7753     @wrap_exception()
7754     @wrap_instance_event(prefix='compute')
7755     @wrap_instance_fault
7756     def pre_live_migration(self, context, instance, block_migration, disk,
7757                            migrate_data):
7758         """Preparations for live migration at dest host.
7759 
7760         :param context: security context
7761         :param instance: dict of instance data
7762         :param block_migration: if true, prepare for block migration
7763         :param disk: disk info of instance
7764         :param migrate_data: A dict or LiveMigrateData object holding data
7765                              required for live migration without shared
7766                              storage.
7767         :returns: migrate_data containing additional migration info
7768         """
7769         LOG.debug('pre_live_migration data is %s', migrate_data)
7770 
7771         migrate_data.old_vol_attachment_ids = {}
7772         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
7773             context, instance.uuid)
7774         network_info = self.network_api.get_instance_nw_info(context, instance)
7775         self._notify_about_instance_usage(
7776             context, instance, "live_migration.pre.start",
7777             network_info=network_info)
7778         compute_utils.notify_about_instance_action(
7779             context, instance, self.host,
7780             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7781             phase=fields.NotificationPhase.START, bdms=bdms)
7782 
7783         connector = self.driver.get_volume_connector(instance)
7784         try:
7785             for bdm in bdms:
7786                 if bdm.is_volume and bdm.attachment_id is not None:
7787                     # This bdm uses the new cinder v3.44 API.
7788                     # We will create a new attachment for this
7789                     # volume on this migration destination host. The old
7790                     # attachment will be deleted on the source host
7791                     # when the migration succeeds. The old attachment_id
7792                     # is stored in dict with the key being the bdm.volume_id
7793                     # so it can be restored on rollback.
7794                     #
7795                     # Also note that attachment_update is not needed as we
7796                     # are providing the connector in the create call.
7797                     attach_ref = self.volume_api.attachment_create(
7798                         context, bdm.volume_id, bdm.instance_uuid,
7799                         connector=connector, mountpoint=bdm.device_name)
7800 
7801                     # save current attachment so we can detach it on success,
7802                     # or restore it on a rollback.
7803                     # NOTE(mdbooth): This data is no longer used by the source
7804                     # host since change Ibe9215c0. We can't remove it until we
7805                     # are sure the source host has been upgraded.
7806                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
7807                         bdm.attachment_id
7808 
7809                     # update the bdm with the new attachment_id.
7810                     bdm.attachment_id = attach_ref['id']
7811                     bdm.save()
7812 
7813             block_device_info = self._get_instance_block_device_info(
7814                                 context, instance, refresh_conn_info=True,
7815                                 bdms=bdms)
7816 
7817             # The driver pre_live_migration will plug vifs on the host
7818             migrate_data = self.driver.pre_live_migration(context,
7819                                            instance,
7820                                            block_device_info,
7821                                            network_info,
7822                                            disk,
7823                                            migrate_data)
7824             LOG.debug('driver pre_live_migration data is %s', migrate_data)
7825             # driver.pre_live_migration is what plugs vifs on the destination
7826             # host so now we can set the wait_for_vif_plugged flag in the
7827             # migrate_data object which the source compute will use to
7828             # determine if it should wait for a 'network-vif-plugged' event
7829             # from neutron before starting the actual guest transfer in the
7830             # hypervisor
7831             migrate_data.wait_for_vif_plugged = (
7832                 CONF.compute.live_migration_wait_for_vif_plug)
7833 
7834             # NOTE(tr3buchet): setup networks on destination host
7835             self.network_api.setup_networks_on_host(context, instance,
7836                                                              self.host)
7837 
7838         except Exception:
7839             # If we raise, migrate_data with the updated attachment ids
7840             # will not be returned to the source host for rollback.
7841             # So we need to rollback new attachments here.
7842             with excutils.save_and_reraise_exception():
7843                 old_attachments = migrate_data.old_vol_attachment_ids
7844                 for bdm in bdms:
7845                     if (bdm.is_volume and bdm.attachment_id is not None and
7846                             bdm.volume_id in old_attachments):
7847                         self.volume_api.attachment_delete(context,
7848                                                           bdm.attachment_id)
7849                         bdm.attachment_id = old_attachments[bdm.volume_id]
7850                         bdm.save()
7851 
7852         # Volume connections are complete, tell cinder that all the
7853         # attachments have completed.
7854         for bdm in bdms:
7855             if bdm.is_volume and bdm.attachment_id is not None:
7856                 self.volume_api.attachment_complete(context,
7857                                                     bdm.attachment_id)
7858 
7859         self._notify_about_instance_usage(
7860                      context, instance, "live_migration.pre.end",
7861                      network_info=network_info)
7862         compute_utils.notify_about_instance_action(
7863             context, instance, self.host,
7864             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
7865             phase=fields.NotificationPhase.END, bdms=bdms)
7866 
7867         LOG.debug('pre_live_migration result data is %s', migrate_data)
7868         return migrate_data
7869 
7870     @staticmethod
7871     def _neutron_failed_migration_callback(event_name, instance):
7872         msg = ('Neutron reported failure during migration '
7873                'with %(event)s for instance %(uuid)s')
7874         msg_args = {'event': event_name, 'uuid': instance.uuid}
7875         if CONF.vif_plugging_is_fatal:
7876             raise exception.VirtualInterfacePlugException(msg % msg_args)
7877         LOG.error(msg, msg_args)
7878 
7879     @staticmethod
7880     def _get_neutron_events_for_live_migration(instance):
7881         # We don't generate events if CONF.vif_plugging_timeout=0
7882         # meaning that the operator disabled using them.
7883         if CONF.vif_plugging_timeout:
7884             return [('network-vif-plugged', vif['id'])
7885                     for vif in instance.get_network_info()]
7886         else:
7887             return []
7888 
7889     def _cleanup_pre_live_migration(self, context, dest, instance,
7890                                     migration, migrate_data, source_bdms):
7891         """Helper method for when pre_live_migration fails
7892 
7893         Sets the migration status to "error" and rolls back the live migration
7894         setup on the destination host.
7895 
7896         :param context: The user request context.
7897         :type context: nova.context.RequestContext
7898         :param dest: The live migration destination hostname.
7899         :type dest: str
7900         :param instance: The instance being live migrated.
7901         :type instance: nova.objects.Instance
7902         :param migration: The migration record tracking this live migration.
7903         :type migration: nova.objects.Migration
7904         :param migrate_data: Data about the live migration, populated from
7905                              the destination host.
7906         :type migrate_data: Subclass of nova.objects.LiveMigrateData
7907         :param source_bdms: BDMs prior to modification by the destination
7908                             compute host. Set by _do_live_migration and not
7909                             part of the callback interface, so this is never
7910                             None
7911         """
7912         self._set_migration_status(migration, 'error')
7913         # Make sure we set this for _rollback_live_migration()
7914         # so it can find it, as expected if it was called later
7915         migrate_data.migration = migration
7916         self._rollback_live_migration(context, instance, dest,
7917                                       migrate_data=migrate_data,
7918                                       source_bdms=source_bdms)
7919 
7920     def _do_pre_live_migration_from_source(self, context, dest, instance,
7921                                            block_migration, migration,
7922                                            migrate_data, source_bdms):
7923         """Prepares for pre-live-migration on the source host and calls dest
7924 
7925         Will setup a callback networking event handler (if configured) and
7926         then call the dest host's pre_live_migration method to prepare the
7927         dest host for live migration (plugs vifs, connect volumes, etc).
7928 
7929         _rollback_live_migration (on the source) will be called if
7930         pre_live_migration (on the dest) fails.
7931 
7932         :param context: nova auth request context for this operation
7933         :param dest: name of the destination compute service host
7934         :param instance: Instance object being live migrated
7935         :param block_migration: If true, prepare for block migration.
7936         :param migration: Migration object tracking this operation
7937         :param migrate_data: MigrateData object for this operation populated
7938             by the destination host compute driver as part of the
7939             check_can_live_migrate_destination call.
7940         :param source_bdms: BlockDeviceMappingList of BDMs currently attached
7941             to the instance from the source host.
7942         :returns: MigrateData object which is a modified version of the
7943             ``migrate_data`` argument from the compute driver on the dest
7944             host during the ``pre_live_migration`` call.
7945         :raises: MigrationError if waiting for the network-vif-plugged event
7946             timed out and is fatal.
7947         """
7948         class _BreakWaitForInstanceEvent(Exception):
7949             """Used as a signal to stop waiting for the network-vif-plugged
7950             event when we discover that
7951             [compute]/live_migration_wait_for_vif_plug is not set on the
7952             destination.
7953             """
7954             pass
7955 
7956         events = self._get_neutron_events_for_live_migration(instance)
7957         try:
7958             if ('block_migration' in migrate_data and
7959                     migrate_data.block_migration):
7960                 block_device_info = self._get_instance_block_device_info(
7961                     context, instance, bdms=source_bdms)
7962                 disk = self.driver.get_instance_disk_info(
7963                     instance, block_device_info=block_device_info)
7964             else:
7965                 disk = None
7966 
7967             deadline = CONF.vif_plugging_timeout
7968             error_cb = self._neutron_failed_migration_callback
7969             # In order to avoid a race with the vif plugging that the virt
7970             # driver does on the destination host, we register our events
7971             # to wait for before calling pre_live_migration. Then if the
7972             # dest host reports back that we shouldn't wait, we can break
7973             # out of the context manager using _BreakWaitForInstanceEvent.
7974             with self.virtapi.wait_for_instance_event(
7975                     instance, events, deadline=deadline,
7976                     error_callback=error_cb):
7977                 with timeutils.StopWatch() as timer:
7978                     # TODO(mriedem): The "block_migration" parameter passed
7979                     # here is not actually used in pre_live_migration but it
7980                     # is not optional in the RPC interface either.
7981                     migrate_data = self.compute_rpcapi.pre_live_migration(
7982                         context, instance,
7983                         block_migration, disk, dest, migrate_data)
7984                 LOG.info('Took %0.2f seconds for pre_live_migration on '
7985                          'destination host %s.',
7986                          timer.elapsed(), dest, instance=instance)
7987                 wait_for_vif_plugged = (
7988                     'wait_for_vif_plugged' in migrate_data and
7989                     migrate_data.wait_for_vif_plugged)
7990                 if events and not wait_for_vif_plugged:
7991                     raise _BreakWaitForInstanceEvent
7992         except _BreakWaitForInstanceEvent:
7993             if events:
7994                 LOG.debug('Not waiting for events after pre_live_migration: '
7995                           '%s. ', events, instance=instance)
7996         except exception.VirtualInterfacePlugException:
7997             with excutils.save_and_reraise_exception():
7998                 LOG.exception('Failed waiting for network virtual interfaces '
7999                               'to be plugged on the destination host %s.',
8000                               dest, instance=instance)
8001                 self._cleanup_pre_live_migration(
8002                     context, dest, instance, migration, migrate_data,
8003                     source_bdms)
8004         except eventlet.timeout.Timeout:
8005             # We only get here if wait_for_vif_plugged is True which means
8006             # live_migration_wait_for_vif_plug=True on the destination host.
8007             msg = (
8008                 'Timed out waiting for events: %(events)s. If these timeouts '
8009                 'are a persistent issue it could mean the networking backend '
8010                 'on host %(dest)s does not support sending these events '
8011                 'unless there are port binding host changes which does not '
8012                 'happen at this point in the live migration process. You may '
8013                 'need to disable the live_migration_wait_for_vif_plug option '
8014                 'on host %(dest)s.')
8015             subs = {'events': events, 'dest': dest}
8016             LOG.warning(msg, subs, instance=instance)
8017             if CONF.vif_plugging_is_fatal:
8018                 self._cleanup_pre_live_migration(
8019                     context, dest, instance, migration, migrate_data,
8020                     source_bdms)
8021                 raise exception.MigrationError(reason=msg % subs)
8022         except Exception:
8023             with excutils.save_and_reraise_exception():
8024                 LOG.exception('Pre live migration failed at %s',
8025                               dest, instance=instance)
8026                 self._cleanup_pre_live_migration(
8027                     context, dest, instance, migration, migrate_data,
8028                     source_bdms)
8029         return migrate_data
8030 
8031     def _do_live_migration(self, context, dest, instance, block_migration,
8032                            migration, migrate_data):
8033         # NOTE(danms): We should enhance the RT to account for migrations
8034         # and use the status field to denote when the accounting has been
8035         # done on source/destination. For now, this is just here for status
8036         # reporting
8037         self._set_migration_status(migration, 'preparing')
8038         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8039                 context, instance.uuid)
8040 
8041         migrate_data = self._do_pre_live_migration_from_source(
8042             context, dest, instance, block_migration, migration, migrate_data,
8043             source_bdms)
8044 
8045         # Set migrate_data.migration because that is how _post_live_migration
8046         # and _rollback_live_migration get the migration object for cleanup.
8047         # Yes this is gross but changing the _post_live_migration and
8048         # _rollback_live_migration interfaces would also mean changing how the
8049         # virt drivers call them from the driver.live_migration method, i.e.
8050         # we would have to pass the migration object through the driver (or
8051         # consider using a partial but some do not like that pattern).
8052         migrate_data.migration = migration
8053 
8054         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
8055         # if it exist in the queue, then we are good to moving on, if
8056         # not, some other process must have aborted it, then we should
8057         # rollback.
8058         try:
8059             self._waiting_live_migrations.pop(instance.uuid)
8060         except KeyError:
8061             LOG.debug('Migration %s aborted by another process, rollback.',
8062                       migration.uuid, instance=instance)
8063             self._rollback_live_migration(context, instance, dest,
8064                                           migrate_data, 'cancelled',
8065                                           source_bdms=source_bdms)
8066             self._notify_live_migrate_abort_end(context, instance)
8067             return
8068 
8069         self._set_migration_status(migration, 'running')
8070 
8071         # NOTE(mdbooth): pre_live_migration will update connection_info and
8072         # attachment_id on all volume BDMS to reflect the new destination
8073         # host attachment. We fetch BDMs before that to retain connection_info
8074         # and attachment_id relating to the source host for post migration
8075         # cleanup.
8076         post_live_migration = functools.partial(self._post_live_migration,
8077                                                 source_bdms=source_bdms)
8078         rollback_live_migration = functools.partial(
8079             self._rollback_live_migration, source_bdms=source_bdms)
8080 
8081         LOG.debug('live_migration data is %s', migrate_data)
8082         try:
8083             self.driver.live_migration(context, instance, dest,
8084                                        post_live_migration,
8085                                        rollback_live_migration,
8086                                        block_migration, migrate_data)
8087         except Exception:
8088             LOG.exception('Live migration failed.', instance=instance)
8089             with excutils.save_and_reraise_exception():
8090                 # Put instance and migration into error state,
8091                 # as its almost certainly too late to rollback
8092                 self._set_migration_status(migration, 'error')
8093                 # first refresh instance as it may have got updated by
8094                 # post_live_migration_at_destination
8095                 instance.refresh()
8096                 self._set_instance_obj_error_state(instance,
8097                                                    clean_task_state=True)
8098 
8099     @wrap_exception()
8100     @wrap_instance_event(prefix='compute')
8101     @errors_out_migration
8102     @wrap_instance_fault
8103     def live_migration(self, context, dest, instance, block_migration,
8104                        migration, migrate_data):
8105         """Executing live migration.
8106 
8107         :param context: security context
8108         :param dest: destination host
8109         :param instance: a nova.objects.instance.Instance object
8110         :param block_migration: if true, prepare for block migration
8111         :param migration: an nova.objects.Migration object
8112         :param migrate_data: implementation specific params
8113 
8114         """
8115         self._set_migration_status(migration, 'queued')
8116         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
8117         # put the returned Future object into dict mapped with migration.uuid
8118         # in order to be able to track and abort it in the future.
8119         self._waiting_live_migrations[instance.uuid] = (None, None)
8120         try:
8121             future = self._live_migration_executor.submit(
8122                 self._do_live_migration, context, dest, instance,
8123                 block_migration, migration, migrate_data)
8124             self._waiting_live_migrations[instance.uuid] = (migration, future)
8125         except RuntimeError:
8126             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
8127             # pool is shutdown, which happens in
8128             # _cleanup_live_migrations_in_pool.
8129             LOG.info('Migration %s failed to submit as the compute service '
8130                      'is shutting down.', migration.uuid, instance=instance)
8131             raise exception.LiveMigrationNotSubmitted(
8132                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
8133 
8134     @wrap_exception()
8135     @wrap_instance_event(prefix='compute')
8136     @wrap_instance_fault
8137     def live_migration_force_complete(self, context, instance):
8138         """Force live migration to complete.
8139 
8140         :param context: Security context
8141         :param instance: The instance that is being migrated
8142         """
8143 
8144         self._notify_about_instance_usage(
8145             context, instance, 'live.migration.force.complete.start')
8146         compute_utils.notify_about_instance_action(
8147             context, instance, self.host,
8148             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8149             phase=fields.NotificationPhase.START)
8150         self.driver.live_migration_force_complete(instance)
8151         self._notify_about_instance_usage(
8152             context, instance, 'live.migration.force.complete.end')
8153         compute_utils.notify_about_instance_action(
8154             context, instance, self.host,
8155             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8156             phase=fields.NotificationPhase.END)
8157 
8158     def _notify_live_migrate_abort_end(self, context, instance):
8159         self._notify_about_instance_usage(
8160             context, instance, 'live.migration.abort.end')
8161         compute_utils.notify_about_instance_action(
8162             context, instance, self.host,
8163             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8164             phase=fields.NotificationPhase.END)
8165 
8166     @wrap_exception()
8167     @wrap_instance_event(prefix='compute')
8168     @wrap_instance_fault
8169     def live_migration_abort(self, context, instance, migration_id):
8170         """Abort an in-progress live migration.
8171 
8172         :param context: Security context
8173         :param instance: The instance that is being migrated
8174         :param migration_id: ID of in-progress live migration
8175 
8176         """
8177         self._notify_about_instance_usage(
8178             context, instance, 'live.migration.abort.start')
8179         compute_utils.notify_about_instance_action(
8180             context, instance, self.host,
8181             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8182             phase=fields.NotificationPhase.START)
8183         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
8184         # lead to 3 scenarios:
8185         # 1. The selected migration is still in queue, and the future.cancel()
8186         #    succeed, then the abort action is succeed, mark the migration
8187         #    status to 'cancelled'.
8188         # 2. The selected migration is still in queue, but the future.cancel()
8189         #    failed, then the _do_live_migration() has started executing, and
8190         #    the migration status is 'preparing', then we just pop it from the
8191         #    queue, and the migration process will handle it later. And the
8192         #    migration status couldn't be 'running' in this scenario because
8193         #    if _do_live_migration has started executing and we've already
8194         #    popped it from the queue and set the migration status to
8195         #    'running' at this point, popping it here will raise KeyError at
8196         #    which point we check if it's running and if so, we abort the old
8197         #    way.
8198         # 3. The selected migration is not in the queue, then the migration
8199         #    status is 'running', let the driver handle it.
8200         try:
8201             migration, future = (
8202                 self._waiting_live_migrations.pop(instance.uuid))
8203             if future and future.cancel():
8204                 # If we got here, we've successfully aborted the queued
8205                 # migration and _do_live_migration won't run so we need
8206                 # to set the migration status to cancelled and send the
8207                 # notification. If Future.cancel() fails, it means
8208                 # _do_live_migration is running and the migration status
8209                 # is preparing, and _do_live_migration() itself will attempt
8210                 # to pop the queued migration, hit a KeyError, and rollback,
8211                 # set the migration to cancelled and send the
8212                 # live.migration.abort.end notification.
8213                 self._set_migration_status(migration, 'cancelled')
8214         except KeyError:
8215             migration = objects.Migration.get_by_id(context, migration_id)
8216             if migration.status != 'running':
8217                 raise exception.InvalidMigrationState(
8218                     migration_id=migration_id, instance_uuid=instance.uuid,
8219                     state=migration.status, method='abort live migration')
8220             self.driver.live_migration_abort(instance)
8221         self._notify_live_migrate_abort_end(context, instance)
8222 
8223     def _live_migration_cleanup_flags(self, migrate_data, migr_ctxt=None):
8224         """Determine whether disks, instance path or other resources
8225         need to be cleaned up after live migration (at source on success,
8226         at destination on rollback)
8227 
8228         Block migration needs empty image at destination host before migration
8229         starts, so if any failure occurs, any empty images has to be deleted.
8230 
8231         Also Volume backed live migration w/o shared storage needs to delete
8232         newly created instance-xxx dir on the destination as a part of its
8233         rollback process
8234 
8235         There may be other resources which need cleanup; currently this is
8236         limited to vPMEM devices with the libvirt driver.
8237 
8238         :param migrate_data: implementation specific data
8239         :param migr_ctxt: specific resources stored in migration_context
8240         :returns: (bool, bool) -- do_cleanup, destroy_disks
8241         """
8242         # NOTE(pkoniszewski): block migration specific params are set inside
8243         # migrate_data objects for drivers that expose block live migration
8244         # information (i.e. Libvirt, Xenapi and HyperV). For other drivers
8245         # cleanup is not needed.
8246         do_cleanup = False
8247         destroy_disks = False
8248         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
8249             has_vpmem = False
8250             if migr_ctxt and migr_ctxt.old_resources:
8251                 for resource in migr_ctxt.old_resources:
8252                     if ('metadata' in resource and
8253                         isinstance(resource.metadata,
8254                                    objects.LibvirtVPMEMDevice)):
8255                         has_vpmem = True
8256                         break
8257             # No instance booting at source host, but instance dir
8258             # must be deleted for preparing next block migration
8259             # must be deleted for preparing next live migration w/o shared
8260             # storage
8261             # vpmem must be cleanped
8262             do_cleanup = not migrate_data.is_shared_instance_path or has_vpmem
8263             destroy_disks = not migrate_data.is_shared_block_storage
8264         elif isinstance(migrate_data, migrate_data_obj.XenapiLiveMigrateData):
8265             do_cleanup = migrate_data.block_migration
8266             destroy_disks = migrate_data.block_migration
8267         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
8268             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
8269             do_cleanup = True
8270             destroy_disks = not migrate_data.is_shared_instance_path
8271 
8272         return (do_cleanup, destroy_disks)
8273 
8274     def _post_live_migration_remove_source_vol_connections(
8275             self, context, instance, source_bdms):
8276         """Disconnect volume connections from the source host during
8277         _post_live_migration.
8278 
8279         :param context: nova auth RequestContext
8280         :param instance: Instance object being live migrated
8281         :param source_bdms: BlockDeviceMappingList representing the attached
8282             volumes with connection_info set for the source host
8283         """
8284         # Detaching volumes.
8285         connector = self.driver.get_volume_connector(instance)
8286         for bdm in source_bdms:
8287             if bdm.is_volume:
8288                 # Detaching volumes is a call to an external API that can fail.
8289                 # If it does, we need to handle it gracefully so that the call
8290                 # to post_live_migration_at_destination - where we set instance
8291                 # host and task state - still happens. We need to rethink the
8292                 # current approach of setting instance host and task state
8293                 # AFTER a whole bunch of things that could fail in unhandled
8294                 # ways, but that is left as a TODO(artom).
8295                 try:
8296                     if bdm.attachment_id is None:
8297                         # Prior to cinder v3.44:
8298                         # We don't want to actually mark the volume detached,
8299                         # or delete the bdm, just remove the connection from
8300                         # this host.
8301                         #
8302                         # remove the volume connection without detaching from
8303                         # hypervisor because the instance is not running
8304                         # anymore on the current host
8305                         self.volume_api.terminate_connection(context,
8306                                                              bdm.volume_id,
8307                                                              connector)
8308                     else:
8309                         # cinder v3.44 api flow - delete the old attachment
8310                         # for the source host
8311                         self.volume_api.attachment_delete(context,
8312                                                           bdm.attachment_id)
8313 
8314                 except Exception as e:
8315                     if bdm.attachment_id is None:
8316                         LOG.error('Connection for volume %s not terminated on '
8317                                   'source host %s during post_live_migration: '
8318                                   '%s', bdm.volume_id, self.host,
8319                                   six.text_type(e), instance=instance)
8320                     else:
8321                         LOG.error('Volume attachment %s not deleted on source '
8322                                   'host %s during post_live_migration: %s',
8323                                   bdm.attachment_id, self.host,
8324                                   six.text_type(e), instance=instance)
8325 
8326     @wrap_exception()
8327     @wrap_instance_fault
8328     def _post_live_migration(self, ctxt, instance, dest,
8329                              block_migration=False, migrate_data=None,
8330                              source_bdms=None):
8331         """Post operations for live migration.
8332 
8333         This method is called from live_migration
8334         and mainly updating database record.
8335 
8336         :param ctxt: security context
8337         :param instance: instance dict
8338         :param dest: destination host
8339         :param block_migration: if true, prepare for block migration
8340         :param migrate_data: if not None, it is a dict which has data
8341         :param source_bdms: BDMs prior to modification by the destination
8342                             compute host. Set by _do_live_migration and not
8343                             part of the callback interface, so this is never
8344                             None
8345         required for live migration without shared storage
8346 
8347         """
8348         LOG.info('_post_live_migration() is started..',
8349                  instance=instance)
8350 
8351         # Cleanup source host post live-migration
8352         block_device_info = self._get_instance_block_device_info(
8353                             ctxt, instance, bdms=source_bdms)
8354         self.driver.post_live_migration(ctxt, instance, block_device_info,
8355                                         migrate_data)
8356 
8357         # Disconnect volumes from this (the source) host.
8358         self._post_live_migration_remove_source_vol_connections(
8359             ctxt, instance, source_bdms)
8360 
8361         # Releasing vlan.
8362         # (not necessary in current implementation?)
8363 
8364         network_info = None
8365         try:
8366             network_info = self.network_api.get_instance_nw_info(
8367                 ctxt, instance)
8368         except Exception as e:
8369             LOG.info('Unable to obtain network info: %s. Network info in '
8370                      'live.migration._post.start notification will be '
8371                      'omitted.', e, instance=instance)
8372 
8373         self._notify_about_instance_usage(ctxt, instance,
8374                                           "live_migration._post.start",
8375                                           network_info=network_info)
8376         compute_utils.notify_about_instance_action(
8377             ctxt, instance, self.host,
8378             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8379             phase=fields.NotificationPhase.START)
8380 
8381         migration = {'source_compute': self.host,
8382                      'dest_compute': dest, }
8383         # For neutron, migrate_instance_start will activate the destination
8384         # host port bindings, if there are any created by conductor before live
8385         # migration started.
8386         self.network_api.migrate_instance_start(ctxt,
8387                                                 instance,
8388                                                 migration)
8389 
8390         destroy_vifs = False
8391         try:
8392             # It's possible that the vif type changed on the destination
8393             # host and is already bound and active, so we need to use the
8394             # stashed source vifs in migrate_data.vifs (if present) to unplug
8395             # on the source host.
8396             unplug_nw_info = network_info
8397             if migrate_data and 'vifs' in migrate_data:
8398                 nw_info = []
8399                 for migrate_vif in migrate_data.vifs:
8400                     nw_info.append(migrate_vif.source_vif)
8401                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
8402                 LOG.debug('Calling driver.post_live_migration_at_source '
8403                           'with original source VIFs from migrate_data: %s',
8404                           unplug_nw_info, instance=instance)
8405             self.driver.post_live_migration_at_source(ctxt, instance,
8406                                                       unplug_nw_info)
8407         except NotImplementedError as ex:
8408             LOG.debug(ex, instance=instance)
8409             # For all hypervisors other than libvirt, there is a possibility
8410             # they are unplugging networks from source node in the cleanup
8411             # method
8412             destroy_vifs = True
8413 
8414         # Free instance allocations on source before claims are allocated on
8415         # destination node
8416         self.rt.free_pci_device_allocations_for_instance(ctxt, instance)
8417         # NOTE(danms): Save source node before calling post method on
8418         # destination, which will update it
8419         source_node = instance.node
8420 
8421         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8422             migrate_data, migr_ctxt=instance.migration_context)
8423 
8424         if do_cleanup:
8425             LOG.debug('Calling driver.cleanup from _post_live_migration',
8426                       instance=instance)
8427             self.driver.cleanup(ctxt, instance, unplug_nw_info,
8428                                 destroy_disks=destroy_disks,
8429                                 migrate_data=migrate_data,
8430                                 destroy_vifs=destroy_vifs)
8431 
8432         # Define domain at destination host, without doing it,
8433         # pause/suspend/terminate do not work.
8434         post_at_dest_success = True
8435         try:
8436             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
8437                     instance, block_migration, dest)
8438         except Exception as error:
8439             post_at_dest_success = False
8440             # We don't want to break _post_live_migration() if
8441             # post_live_migration_at_destination() fails as it should never
8442             # affect cleaning up source node.
8443             LOG.exception("Post live migration at destination %s failed",
8444                           dest, instance=instance, error=error)
8445 
8446         self.instance_events.clear_events_for_instance(instance)
8447 
8448         # NOTE(timello): make sure we update available resources on source
8449         # host even before next periodic task.
8450         self.update_available_resource(ctxt)
8451 
8452         self._update_scheduler_instance_info(ctxt, instance)
8453         self._notify_about_instance_usage(ctxt, instance,
8454                                           "live_migration._post.end",
8455                                           network_info=network_info)
8456         compute_utils.notify_about_instance_action(
8457             ctxt, instance, self.host,
8458             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8459             phase=fields.NotificationPhase.END)
8460         if post_at_dest_success:
8461             LOG.info('Migrating instance to %s finished successfully.',
8462                      dest, instance=instance)
8463 
8464         self._clean_instance_console_tokens(ctxt, instance)
8465         if migrate_data and migrate_data.obj_attr_is_set('migration'):
8466             migrate_data.migration.status = 'completed'
8467             migrate_data.migration.save()
8468             self._delete_allocation_after_move(ctxt,
8469                                                instance,
8470                                                migrate_data.migration)
8471         else:
8472             # We didn't have data on a migration, which means we can't
8473             # look up to see if we had new-style migration-based
8474             # allocations. This should really only happen in cases of
8475             # a buggy virt driver. Log a warning so we know it happened.
8476             LOG.warning('Live migration ended with no migrate_data '
8477                         'record. Unable to clean up migration-based '
8478                         'allocations for node %s which is almost certainly '
8479                         'not an expected situation.', source_node,
8480                         instance=instance)
8481 
8482     def _consoles_enabled(self):
8483         """Returns whether a console is enable."""
8484         return (CONF.vnc.enabled or CONF.spice.enabled or
8485                 CONF.rdp.enabled or CONF.serial_console.enabled or
8486                 CONF.mks.enabled)
8487 
8488     def _clean_instance_console_tokens(self, ctxt, instance):
8489         """Clean console tokens stored for an instance."""
8490         # If the database backend isn't in use, don't bother trying to clean
8491         # tokens.
8492         if self._consoles_enabled():
8493             objects.ConsoleAuthToken.\
8494                 clean_console_auths_for_instance(ctxt, instance.uuid)
8495 
8496     @wrap_exception()
8497     @wrap_instance_event(prefix='compute')
8498     @wrap_instance_fault
8499     def post_live_migration_at_destination(self, context, instance,
8500                                            block_migration):
8501         """Post operations for live migration .
8502 
8503         :param context: security context
8504         :param instance: Instance dict
8505         :param block_migration: if true, prepare for block migration
8506 
8507         """
8508         LOG.info('Post operation of migration started',
8509                  instance=instance)
8510 
8511         # NOTE(tr3buchet): setup networks on destination host
8512         #                  this is called a second time because
8513         #                  multi_host does not create the bridge in
8514         #                  plug_vifs
8515         # NOTE(mriedem): This is a no-op for neutron.
8516         self.network_api.setup_networks_on_host(context, instance,
8517                                                          self.host)
8518         migration = objects.Migration(
8519             source_compute=instance.host,
8520             dest_compute=self.host,
8521             migration_type=fields.MigrationType.LIVE_MIGRATION)
8522         self.network_api.migrate_instance_finish(
8523             context, instance, migration, provider_mappings=None)
8524 
8525         network_info = self.network_api.get_instance_nw_info(context, instance)
8526         self._notify_about_instance_usage(
8527                      context, instance, "live_migration.post.dest.start",
8528                      network_info=network_info)
8529         compute_utils.notify_about_instance_action(context, instance,
8530                 self.host,
8531                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8532                 phase=fields.NotificationPhase.START)
8533         block_device_info = self._get_instance_block_device_info(context,
8534                                                                  instance)
8535         # Allocate the claimed PCI resources at destination.
8536         self.rt.allocate_pci_devices_for_instance(context, instance)
8537 
8538         try:
8539             self.driver.post_live_migration_at_destination(
8540                 context, instance, network_info, block_migration,
8541                 block_device_info)
8542         except Exception:
8543             with excutils.save_and_reraise_exception():
8544                 instance.vm_state = vm_states.ERROR
8545                 LOG.error('Unexpected error during post live migration at '
8546                           'destination host.', instance=instance)
8547         finally:
8548             # Restore instance state and update host
8549             current_power_state = self._get_power_state(instance)
8550             node_name = None
8551             prev_host = instance.host
8552             try:
8553                 compute_node = self._get_compute_info(context, self.host)
8554                 node_name = compute_node.hypervisor_hostname
8555             except exception.ComputeHostNotFound:
8556                 LOG.exception('Failed to get compute_info for %s', self.host)
8557             finally:
8558                 # NOTE(artom) We need to apply the migration context here
8559                 # regardless of whether the driver's
8560                 # post_live_migration_at_destination succeeded or not: the
8561                 # instance is on the destination, potentially with a new NUMA
8562                 # topology and resource usage. We need to persist that.
8563                 # NOTE(artom) Apply followed by drop looks weird, but apply
8564                 # just saves the new fields while drop actually removes the
8565                 # migration context from the instance.
8566                 instance.apply_migration_context()
8567                 instance.drop_migration_context()
8568                 instance.host = self.host
8569                 instance.power_state = current_power_state
8570                 instance.task_state = None
8571                 instance.node = node_name
8572                 instance.progress = 0
8573                 instance.save(expected_task_state=task_states.MIGRATING)
8574 
8575         # NOTE(tr3buchet): tear down networks on source host (nova-net)
8576         # NOTE(mriedem): For neutron, this will delete any inactive source
8577         # host port bindings.
8578         try:
8579             self.network_api.setup_networks_on_host(context, instance,
8580                                                     prev_host, teardown=True)
8581         except exception.PortBindingDeletionFailed as e:
8582             # Removing the inactive port bindings from the source host is not
8583             # critical so just log an error but don't fail.
8584             LOG.error('Network cleanup failed for source host %s during post '
8585                       'live migration. You may need to manually clean up '
8586                       'resources in the network service. Error: %s',
8587                       prev_host, six.text_type(e))
8588         # NOTE(vish): this is necessary to update dhcp for nova-network
8589         # NOTE(mriedem): This is a no-op for neutron.
8590         self.network_api.setup_networks_on_host(context, instance, self.host)
8591         self._notify_about_instance_usage(
8592                      context, instance, "live_migration.post.dest.end",
8593                      network_info=network_info)
8594         compute_utils.notify_about_instance_action(context, instance,
8595                 self.host,
8596                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8597                 phase=fields.NotificationPhase.END)
8598 
8599     def _remove_remote_volume_connections(self, context, dest, bdms, instance):
8600         """Rollback remote volume connections on the dest"""
8601         for bdm in bdms:
8602             try:
8603                 # remove the connection on the destination host
8604                 # NOTE(lyarwood): This actually calls the cinderv2
8605                 # os-terminate_connection API if required.
8606                 self.compute_rpcapi.remove_volume_connection(
8607                         context, instance, bdm.volume_id, dest)
8608             except Exception:
8609                 LOG.warning("Ignoring exception while attempting "
8610                             "to rollback volume connections for "
8611                             "volume %s on host %s.", bdm.volume_id,
8612                             dest, instance=instance)
8613 
8614     def _rollback_volume_bdms(self, context, bdms, original_bdms, instance):
8615         """Rollback the connection_info and attachment_id for each bdm"""
8616         original_bdms_by_volid = {bdm.volume_id: bdm for bdm in original_bdms
8617                                   if bdm.is_volume}
8618         for bdm in bdms:
8619             try:
8620                 original_bdm = original_bdms_by_volid[bdm.volume_id]
8621                 # NOTE(lyarwood): Only delete the referenced attachment if it
8622                 # is different to the original in order to avoid accidentally
8623                 # removing the source host volume attachment after it has
8624                 # already been rolled back by a failure in pre_live_migration.
8625                 if (bdm.attachment_id and original_bdm.attachment_id and
8626                     bdm.attachment_id != original_bdm.attachment_id):
8627                     # NOTE(lyarwood): 3.44 cinder api flow. Delete the
8628                     # attachment used by the bdm and reset it to that of
8629                     # the original bdm.
8630                     self.volume_api.attachment_delete(context,
8631                                                       bdm.attachment_id)
8632                     bdm.attachment_id = original_bdm.attachment_id
8633                 # NOTE(lyarwood): Reset the connection_info to the original
8634                 bdm.connection_info = original_bdm.connection_info
8635                 bdm.save()
8636             except cinder_exception.ClientException:
8637                 LOG.warning("Ignoring cinderclient exception when "
8638                             "attempting to delete attachment %s for volume "
8639                             "%s while rolling back volume bdms.",
8640                             bdm.attachment_id, bdm.volume_id,
8641                             instance=instance)
8642             except Exception:
8643                 with excutils.save_and_reraise_exception():
8644                     LOG.exception("Exception while attempting to rollback "
8645                                   "BDM for volume %s.", bdm.volume_id,
8646                                   instance=instance)
8647 
8648     @wrap_exception()
8649     @wrap_instance_fault
8650     def _rollback_live_migration(self, context, instance,
8651                                  dest, migrate_data=None,
8652                                  migration_status='error',
8653                                  source_bdms=None):
8654         """Recovers Instance/volume state from migrating -> running.
8655 
8656         :param context: security context
8657         :param instance: nova.objects.instance.Instance object
8658         :param dest:
8659             This method is called from live migration src host.
8660             This param specifies destination host.
8661         :param migrate_data:
8662             if not none, contains implementation specific data.
8663         :param migration_status:
8664             Contains the status we want to set for the migration object
8665         :param source_bdms: BDMs prior to modification by the destination
8666                             compute host. Set by _do_live_migration and not
8667                             part of the callback interface, so this is never
8668                             None
8669 
8670         """
8671         # NOTE(gibi): We need to refresh pci_requests of the instance as it
8672         # might be changed by the conductor during scheduling based on the
8673         # selected destination host. If the instance has SRIOV ports with
8674         # resource request then the LiveMigrationTask._find_destination call
8675         # updated the instance.pci_requests.requests[].spec with the SRIOV PF
8676         # device name to be used on the destination host. As the migration is
8677         # rolling back to the source host now we don't want to persist the
8678         # destination host related changes in the DB.
8679         instance.pci_requests = \
8680             objects.InstancePCIRequests.get_by_instance_uuid(
8681                 context, instance.uuid)
8682 
8683         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
8684               migrate_data.obj_attr_is_set('migration')):
8685             migration = migrate_data.migration
8686         else:
8687             migration = None
8688 
8689         if migration:
8690             # Remove allocations created in Placement for the dest node.
8691             # If migration is None, the virt driver didn't pass it which is
8692             # a bug.
8693             self._revert_allocation(context, instance, migration)
8694         else:
8695             LOG.error('Unable to revert allocations during live migration '
8696                       'rollback; compute driver did not provide migrate_data',
8697                       instance=instance)
8698 
8699         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
8700         #                  for nova-network)
8701         # NOTE(mriedem): This is a no-op for neutron.
8702         self.network_api.setup_networks_on_host(context, instance, self.host)
8703         self.driver.rollback_live_migration_at_source(context, instance,
8704                                                       migrate_data)
8705 
8706         # NOTE(lyarwood): Fetch the current list of BDMs, disconnect any
8707         # connected volumes from the dest and delete any volume attachments
8708         # used by the destination host before rolling back to the original
8709         # still valid source host volume attachments.
8710         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8711                 context, instance.uuid)
8712         # TODO(lyarwood): Turn the following into a lookup method within
8713         # BlockDeviceMappingList.
8714         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
8715         self._remove_remote_volume_connections(context, dest, vol_bdms,
8716                                                instance)
8717         self._rollback_volume_bdms(context, vol_bdms, source_bdms, instance)
8718 
8719         self._notify_about_instance_usage(context, instance,
8720                                           "live_migration._rollback.start")
8721         compute_utils.notify_about_instance_action(context, instance,
8722                 self.host,
8723                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8724                 phase=fields.NotificationPhase.START,
8725                 bdms=bdms)
8726 
8727         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8728                 migrate_data, migr_ctxt=instance.migration_context)
8729 
8730         if do_cleanup:
8731             self.compute_rpcapi.rollback_live_migration_at_destination(
8732                     context, instance, dest, destroy_disks=destroy_disks,
8733                     migrate_data=migrate_data)
8734         else:
8735             # The port binding profiles need to be cleaned up.
8736             with errors_out_migration_ctxt(migration):
8737                 try:
8738                     # This call will delete any inactive destination host
8739                     # port bindings.
8740                     self.network_api.setup_networks_on_host(
8741                         context, instance, host=dest, teardown=True)
8742                 except exception.PortBindingDeletionFailed as e:
8743                     # Removing the inactive port bindings from the destination
8744                     # host is not critical so just log an error but don't fail.
8745                     LOG.error(
8746                         'Network cleanup failed for destination host %s '
8747                         'during live migration rollback. You may need to '
8748                         'manually clean up resources in the network service. '
8749                         'Error: %s', dest, six.text_type(e))
8750                 except Exception:
8751                     with excutils.save_and_reraise_exception():
8752                         LOG.exception(
8753                             'An error occurred while cleaning up networking '
8754                             'during live migration rollback.',
8755                             instance=instance)
8756 
8757         # NOTE(luyao): We drop move_claim and migration_context after cleanup
8758         # is complete, to ensure the specific resources claimed on destination
8759         # are released safely.
8760         # TODO(artom) drop_move_claim_at_destination() is new in RPC 5.3, only
8761         # call it if we performed a NUMA-aware live migration (which implies us
8762         # being able to send RPC 5.3). To check this, we can use the
8763         # src_supports_numa_live_migration flag, as it will be set if and only
8764         # if:
8765         # - dst_supports_numa_live_migration made its way to the source
8766         #   (meaning both dest and source are new and conductor can speak
8767         #   RPC 5.3)
8768         # - src_supports_numa_live_migration was set by the source driver and
8769         #   passed the send-RPC-5.3 check.
8770         # This check can be removed in RPC 6.0.
8771         if ('src_supports_numa_live_migration' in migrate_data and
8772                 migrate_data.src_supports_numa_live_migration):
8773             LOG.debug('Calling destination to drop move claim.',
8774                       instance=instance)
8775             self.compute_rpcapi.drop_move_claim_at_destination(context,
8776                                                                instance, dest)
8777 
8778         # NOTE(luyao): We only update instance info after rollback operations
8779         # are complete
8780         instance.task_state = None
8781         instance.progress = 0
8782         instance.drop_migration_context()
8783         instance.save(expected_task_state=[task_states.MIGRATING])
8784 
8785         self._notify_about_instance_usage(context, instance,
8786                                           "live_migration._rollback.end")
8787         compute_utils.notify_about_instance_action(context, instance,
8788                 self.host,
8789                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
8790                 phase=fields.NotificationPhase.END,
8791                 bdms=bdms)
8792 
8793         # TODO(luyao): set migration status to 'failed' but not 'error'
8794         # which means rollback_live_migration is done, we have successfully
8795         # cleaned up and returned instance back to normal status.
8796         self._set_migration_status(migration, migration_status)
8797 
8798     @wrap_exception()
8799     @wrap_instance_fault
8800     def drop_move_claim_at_destination(self, context, instance):
8801         """Called by the source of a live migration during rollback to ask the
8802         destination to drop the MoveClaim object that was created for the live
8803         migration on the destination.
8804         """
8805         nodename = self._get_nodename(instance)
8806         LOG.debug('Dropping live migration resource claim on destination '
8807                   'node %s', nodename, instance=instance)
8808         self.rt.drop_move_claim(
8809             context, instance, nodename, instance_type=instance.flavor)
8810 
8811     @wrap_exception()
8812     @wrap_instance_event(prefix='compute')
8813     @wrap_instance_fault
8814     def rollback_live_migration_at_destination(self, context, instance,
8815                                                destroy_disks,
8816                                                migrate_data):
8817         """Cleaning up image directory that is created pre_live_migration.
8818 
8819         :param context: security context
8820         :param instance: a nova.objects.instance.Instance object sent over rpc
8821         :param destroy_disks: whether to destroy volumes or not
8822         :param migrate_data: contains migration info
8823         """
8824         network_info = self.network_api.get_instance_nw_info(context, instance)
8825         self._notify_about_instance_usage(
8826                       context, instance, "live_migration.rollback.dest.start",
8827                       network_info=network_info)
8828         compute_utils.notify_about_instance_action(
8829             context, instance, self.host,
8830             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8831             phase=fields.NotificationPhase.START)
8832         try:
8833             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
8834             # NOTE(mriedem): For neutron, this call will delete any
8835             # destination host port bindings.
8836             # TODO(mriedem): We should eventually remove this call from
8837             # this method (rollback_live_migration_at_destination) since this
8838             # method is only called conditionally based on whether or not the
8839             # instance is running on shared storage. _rollback_live_migration
8840             # already calls this method for neutron if we are running on
8841             # shared storage.
8842             self.network_api.setup_networks_on_host(context, instance,
8843                                                     self.host, teardown=True)
8844         except exception.PortBindingDeletionFailed as e:
8845             # Removing the inactive port bindings from the destination
8846             # host is not critical so just log an error but don't fail.
8847             LOG.error(
8848                 'Network cleanup failed for destination host %s '
8849                 'during live migration rollback. You may need to '
8850                 'manually clean up resources in the network service. '
8851                 'Error: %s', self.host, six.text_type(e))
8852         except Exception:
8853             with excutils.save_and_reraise_exception():
8854                 # NOTE(tdurakov): even if teardown networks fails driver
8855                 # should try to rollback live migration on destination.
8856                 LOG.exception('An error occurred while deallocating network.',
8857                               instance=instance)
8858         finally:
8859             # always run this even if setup_networks_on_host fails
8860             # NOTE(vish): The mapping is passed in so the driver can disconnect
8861             #             from remote volumes if necessary
8862             block_device_info = self._get_instance_block_device_info(context,
8863                                                                      instance)
8864             # free any instance PCI claims done on destination during
8865             # check_can_live_migrate_destination()
8866             self.rt.free_pci_device_claims_for_instance(context, instance)
8867 
8868             # NOTE(luyao): Apply migration_context temporarily since it's
8869             # on destination host, we rely on instance object to cleanup
8870             # specific resources like vpmem
8871             with instance.mutated_migration_context():
8872                 self.driver.rollback_live_migration_at_destination(
8873                     context, instance, network_info, block_device_info,
8874                     destroy_disks=destroy_disks, migrate_data=migrate_data)
8875 
8876         self._notify_about_instance_usage(
8877                         context, instance, "live_migration.rollback.dest.end",
8878                         network_info=network_info)
8879         compute_utils.notify_about_instance_action(
8880             context, instance, self.host,
8881             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
8882             phase=fields.NotificationPhase.END)
8883 
8884     def _require_nw_info_update(self, context, instance):
8885         """Detect whether there is a mismatch in binding:host_id, or
8886         binding_failed or unbound binding:vif_type for any of the instances
8887         ports.
8888         """
8889         # Only update port bindings if compute manager does manage port
8890         # bindings instead of the compute driver. For example IronicDriver
8891         # manages the port binding for baremetal instance ports, hence,
8892         # external intervention with the binding is not desired.
8893         if self.driver.manages_network_binding_host_id():
8894             return False
8895 
8896         search_opts = {'device_id': instance.uuid,
8897                        'fields': ['binding:host_id', 'binding:vif_type']}
8898         ports = self.network_api.list_ports(context, **search_opts)
8899         for p in ports['ports']:
8900             if p.get('binding:host_id') != self.host:
8901                 return True
8902             vif_type = p.get('binding:vif_type')
8903             if (vif_type == network_model.VIF_TYPE_UNBOUND or
8904                     vif_type == network_model.VIF_TYPE_BINDING_FAILED):
8905                 return True
8906         return False
8907 
8908     @periodic_task.periodic_task(
8909         spacing=CONF.heal_instance_info_cache_interval)
8910     def _heal_instance_info_cache(self, context):
8911         """Called periodically.  On every call, try to update the
8912         info_cache's network information for another instance by
8913         calling to the network manager.
8914 
8915         This is implemented by keeping a cache of uuids of instances
8916         that live on this host.  On each call, we pop one off of a
8917         list, pull the DB record, and try the call to the network API.
8918         If anything errors don't fail, as it's possible the instance
8919         has been deleted, etc.
8920         """
8921         heal_interval = CONF.heal_instance_info_cache_interval
8922         if not heal_interval:
8923             return
8924 
8925         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
8926         instance = None
8927 
8928         LOG.debug('Starting heal instance info cache')
8929 
8930         if not instance_uuids:
8931             # The list of instances to heal is empty so rebuild it
8932             LOG.debug('Rebuilding the list of instances to heal')
8933             db_instances = objects.InstanceList.get_by_host(
8934                 context, self.host, expected_attrs=[], use_slave=True)
8935             for inst in db_instances:
8936                 # We don't want to refresh the cache for instances
8937                 # which are building or deleting so don't put them
8938                 # in the list. If they are building they will get
8939                 # added to the list next time we build it.
8940                 if (inst.vm_state == vm_states.BUILDING):
8941                     LOG.debug('Skipping network cache update for instance '
8942                               'because it is Building.', instance=inst)
8943                     continue
8944                 if (inst.task_state == task_states.DELETING):
8945                     LOG.debug('Skipping network cache update for instance '
8946                               'because it is being deleted.', instance=inst)
8947                     continue
8948 
8949                 if not instance:
8950                     # Save the first one we find so we don't
8951                     # have to get it again
8952                     instance = inst
8953                 else:
8954                     instance_uuids.append(inst['uuid'])
8955 
8956             self._instance_uuids_to_heal = instance_uuids
8957         else:
8958             # Find the next valid instance on the list
8959             while instance_uuids:
8960                 try:
8961                     inst = objects.Instance.get_by_uuid(
8962                             context, instance_uuids.pop(0),
8963                             expected_attrs=['system_metadata', 'info_cache',
8964                                             'flavor'],
8965                             use_slave=True)
8966                 except exception.InstanceNotFound:
8967                     # Instance is gone.  Try to grab another.
8968                     continue
8969 
8970                 # Check the instance hasn't been migrated
8971                 if inst.host != self.host:
8972                     LOG.debug('Skipping network cache update for instance '
8973                               'because it has been migrated to another '
8974                               'host.', instance=inst)
8975                 # Check the instance isn't being deleting
8976                 elif inst.task_state == task_states.DELETING:
8977                     LOG.debug('Skipping network cache update for instance '
8978                               'because it is being deleted.', instance=inst)
8979                 else:
8980                     instance = inst
8981                     break
8982 
8983         if instance:
8984             # We have an instance now to refresh
8985             try:
8986                 # Fix potential mismatch in port binding if evacuation failed
8987                 # after reassigning the port binding to the dest host but
8988                 # before the instance host is changed.
8989                 # Do this only when instance has no pending task.
8990                 if instance.task_state is None and \
8991                         self._require_nw_info_update(context, instance):
8992                     LOG.info("Updating ports in neutron", instance=instance)
8993                     self.network_api.setup_instance_network_on_host(
8994                         context, instance, self.host)
8995                 # Call to network API to get instance info.. this will
8996                 # force an update to the instance's info_cache
8997                 self.network_api.get_instance_nw_info(
8998                     context, instance, force_refresh=True)
8999                 LOG.debug('Updated the network info_cache for instance',
9000                           instance=instance)
9001             except exception.InstanceNotFound:
9002                 # Instance is gone.
9003                 LOG.debug('Instance no longer exists. Unable to refresh',
9004                           instance=instance)
9005                 return
9006             except exception.InstanceInfoCacheNotFound:
9007                 # InstanceInfoCache is gone.
9008                 LOG.debug('InstanceInfoCache no longer exists. '
9009                           'Unable to refresh', instance=instance)
9010             except Exception:
9011                 LOG.error('An error occurred while refreshing the network '
9012                           'cache.', instance=instance, exc_info=True)
9013         else:
9014             LOG.debug("Didn't find any instances for network info cache "
9015                       "update.")
9016 
9017     @periodic_task.periodic_task
9018     def _poll_rebooting_instances(self, context):
9019         if CONF.reboot_timeout > 0:
9020             filters = {'task_state':
9021                        [task_states.REBOOTING,
9022                         task_states.REBOOT_STARTED,
9023                         task_states.REBOOT_PENDING],
9024                        'host': self.host}
9025             rebooting = objects.InstanceList.get_by_filters(
9026                 context, filters, expected_attrs=[], use_slave=True)
9027 
9028             to_poll = []
9029             for instance in rebooting:
9030                 if timeutils.is_older_than(instance.updated_at,
9031                                            CONF.reboot_timeout):
9032                     to_poll.append(instance)
9033 
9034             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
9035 
9036     @periodic_task.periodic_task
9037     def _poll_rescued_instances(self, context):
9038         if CONF.rescue_timeout > 0:
9039             filters = {'vm_state': vm_states.RESCUED,
9040                        'host': self.host}
9041             rescued_instances = objects.InstanceList.get_by_filters(
9042                 context, filters, expected_attrs=["system_metadata"],
9043                 use_slave=True)
9044 
9045             to_unrescue = []
9046             for instance in rescued_instances:
9047                 if timeutils.is_older_than(instance.launched_at,
9048                                            CONF.rescue_timeout):
9049                     to_unrescue.append(instance)
9050 
9051             for instance in to_unrescue:
9052                 self.compute_api.unrescue(context, instance)
9053 
9054     @periodic_task.periodic_task
9055     def _poll_unconfirmed_resizes(self, context):
9056         if CONF.resize_confirm_window == 0:
9057             return
9058 
9059         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
9060                 context, CONF.resize_confirm_window, self.host,
9061                 use_slave=True)
9062 
9063         migrations_info = dict(migration_count=len(migrations),
9064                 confirm_window=CONF.resize_confirm_window)
9065 
9066         if migrations_info["migration_count"] > 0:
9067             LOG.info("Found %(migration_count)d unconfirmed migrations "
9068                      "older than %(confirm_window)d seconds",
9069                      migrations_info)
9070 
9071         def _set_migration_to_error(migration, reason, **kwargs):
9072             LOG.warning("Setting migration %(migration_id)s to error: "
9073                         "%(reason)s",
9074                         {'migration_id': migration['id'], 'reason': reason},
9075                         **kwargs)
9076             migration.status = 'error'
9077             migration.save()
9078 
9079         for migration in migrations:
9080             instance_uuid = migration.instance_uuid
9081             LOG.info("Automatically confirming migration "
9082                      "%(migration_id)s for instance %(instance_uuid)s",
9083                      {'migration_id': migration.id,
9084                       'instance_uuid': instance_uuid})
9085             expected_attrs = ['metadata', 'system_metadata']
9086             try:
9087                 instance = objects.Instance.get_by_uuid(context,
9088                             instance_uuid, expected_attrs=expected_attrs,
9089                             use_slave=True)
9090             except exception.InstanceNotFound:
9091                 reason = (_("Instance %s not found") %
9092                           instance_uuid)
9093                 _set_migration_to_error(migration, reason)
9094                 continue
9095             if instance.vm_state == vm_states.ERROR:
9096                 reason = _("In ERROR state")
9097                 _set_migration_to_error(migration, reason,
9098                                         instance=instance)
9099                 continue
9100             # race condition: The instance in DELETING state should not be
9101             # set the migration state to error, otherwise the instance in
9102             # to be deleted which is in RESIZED state
9103             # will not be able to confirm resize
9104             if instance.task_state in [task_states.DELETING,
9105                                        task_states.SOFT_DELETING]:
9106                 msg = ("Instance being deleted or soft deleted during resize "
9107                        "confirmation. Skipping.")
9108                 LOG.debug(msg, instance=instance)
9109                 continue
9110 
9111             # race condition: This condition is hit when this method is
9112             # called between the save of the migration record with a status of
9113             # finished and the save of the instance object with a state of
9114             # RESIZED. The migration record should not be set to error.
9115             if instance.task_state == task_states.RESIZE_FINISH:
9116                 msg = ("Instance still resizing during resize "
9117                        "confirmation. Skipping.")
9118                 LOG.debug(msg, instance=instance)
9119                 continue
9120 
9121             vm_state = instance.vm_state
9122             task_state = instance.task_state
9123             if vm_state != vm_states.RESIZED or task_state is not None:
9124                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
9125                            "RESIZED/None") %
9126                           {'vm_state': vm_state,
9127                            'task_state': task_state})
9128                 _set_migration_to_error(migration, reason,
9129                                         instance=instance)
9130                 continue
9131             try:
9132                 self.compute_api.confirm_resize(context, instance,
9133                                                 migration=migration)
9134             except Exception as e:
9135                 LOG.info("Error auto-confirming resize: %s. "
9136                          "Will retry later.", e, instance=instance)
9137 
9138     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
9139     def _poll_shelved_instances(self, context):
9140 
9141         if CONF.shelved_offload_time <= 0:
9142             return
9143 
9144         filters = {'vm_state': vm_states.SHELVED,
9145                    'task_state': None,
9146                    'host': self.host}
9147         shelved_instances = objects.InstanceList.get_by_filters(
9148             context, filters=filters, expected_attrs=['system_metadata'],
9149             use_slave=True)
9150 
9151         to_gc = []
9152         for instance in shelved_instances:
9153             sys_meta = instance.system_metadata
9154             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
9155             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
9156                 to_gc.append(instance)
9157 
9158         for instance in to_gc:
9159             try:
9160                 instance.task_state = task_states.SHELVING_OFFLOADING
9161                 instance.save(expected_task_state=(None,))
9162                 self.shelve_offload_instance(context, instance,
9163                                              clean_shutdown=False)
9164             except Exception:
9165                 LOG.exception('Periodic task failed to offload instance.',
9166                               instance=instance)
9167 
9168     @periodic_task.periodic_task
9169     def _instance_usage_audit(self, context):
9170         if not CONF.instance_usage_audit:
9171             return
9172 
9173         begin, end = utils.last_completed_audit_period()
9174         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
9175                                self.host):
9176             return
9177 
9178         instances = objects.InstanceList.get_active_by_window_joined(
9179             context, begin, end, host=self.host,
9180             expected_attrs=['system_metadata', 'info_cache', 'metadata',
9181                             'flavor'],
9182             use_slave=True)
9183         num_instances = len(instances)
9184         errors = 0
9185         successes = 0
9186         LOG.info("Running instance usage audit for host %(host)s "
9187                  "from %(begin_time)s to %(end_time)s. "
9188                  "%(number_instances)s instances.",
9189                  {'host': self.host,
9190                   'begin_time': begin,
9191                   'end_time': end,
9192                   'number_instances': num_instances})
9193         start_time = time.time()
9194         task_log = objects.TaskLog(context)
9195         task_log.task_name = 'instance_usage_audit'
9196         task_log.period_beginning = begin
9197         task_log.period_ending = end
9198         task_log.host = self.host
9199         task_log.task_items = num_instances
9200         task_log.message = 'Instance usage audit started...'
9201         task_log.begin_task()
9202         for instance in instances:
9203             try:
9204                 compute_utils.notify_usage_exists(
9205                     self.notifier, context, instance, self.host,
9206                     ignore_missing_network_data=False)
9207                 successes += 1
9208             except Exception:
9209                 LOG.exception('Failed to generate usage '
9210                               'audit for instance '
9211                               'on host %s', self.host,
9212                               instance=instance)
9213                 errors += 1
9214         task_log.errors = errors
9215         task_log.message = (
9216             'Instance usage audit ran for host %s, %s instances in %s seconds.'
9217             % (self.host, num_instances, time.time() - start_time))
9218         task_log.end_task()
9219 
9220     @periodic_task.periodic_task(spacing=CONF.bandwidth_poll_interval)
9221     def _poll_bandwidth_usage(self, context):
9222 
9223         if not self._bw_usage_supported:
9224             return
9225 
9226         prev_time, start_time = utils.last_completed_audit_period()
9227 
9228         curr_time = time.time()
9229         if (curr_time - self._last_bw_usage_poll >
9230                 CONF.bandwidth_poll_interval):
9231             self._last_bw_usage_poll = curr_time
9232             LOG.info("Updating bandwidth usage cache")
9233 
9234             instances = objects.InstanceList.get_by_host(context,
9235                                                               self.host,
9236                                                               use_slave=True)
9237             try:
9238                 bw_counters = self.driver.get_all_bw_counters(instances)
9239             except NotImplementedError:
9240                 # NOTE(mdragon): Not all hypervisors have bandwidth polling
9241                 # implemented yet.  If they don't it doesn't break anything,
9242                 # they just don't get the info in the usage events.
9243                 # NOTE(PhilDay): Record that its not supported so we can
9244                 # skip fast on future calls rather than waste effort getting
9245                 # the list of instances.
9246                 LOG.info("Bandwidth usage not supported by %(driver)s.",
9247                          {'driver': CONF.compute_driver})
9248                 self._bw_usage_supported = False
9249                 return
9250 
9251             refreshed = timeutils.utcnow()
9252             for bw_ctr in bw_counters:
9253                 # Allow switching of greenthreads between queries.
9254                 greenthread.sleep(0)
9255                 bw_in = 0
9256                 bw_out = 0
9257                 last_ctr_in = None
9258                 last_ctr_out = None
9259                 usage = objects.BandwidthUsage.get_by_instance_uuid_and_mac(
9260                     context, bw_ctr['uuid'], bw_ctr['mac_address'],
9261                     start_period=start_time, use_slave=True)
9262                 if usage:
9263                     bw_in = usage.bw_in
9264                     bw_out = usage.bw_out
9265                     last_ctr_in = usage.last_ctr_in
9266                     last_ctr_out = usage.last_ctr_out
9267                 else:
9268                     usage = (objects.BandwidthUsage.
9269                              get_by_instance_uuid_and_mac(
9270                         context, bw_ctr['uuid'], bw_ctr['mac_address'],
9271                         start_period=prev_time, use_slave=True))
9272                     if usage:
9273                         last_ctr_in = usage.last_ctr_in
9274                         last_ctr_out = usage.last_ctr_out
9275 
9276                 if last_ctr_in is not None:
9277                     if bw_ctr['bw_in'] < last_ctr_in:
9278                         # counter rollover
9279                         bw_in += bw_ctr['bw_in']
9280                     else:
9281                         bw_in += (bw_ctr['bw_in'] - last_ctr_in)
9282 
9283                 if last_ctr_out is not None:
9284                     if bw_ctr['bw_out'] < last_ctr_out:
9285                         # counter rollover
9286                         bw_out += bw_ctr['bw_out']
9287                     else:
9288                         bw_out += (bw_ctr['bw_out'] - last_ctr_out)
9289 
9290                 objects.BandwidthUsage(context=context).create(
9291                                               bw_ctr['uuid'],
9292                                               bw_ctr['mac_address'],
9293                                               bw_in,
9294                                               bw_out,
9295                                               bw_ctr['bw_in'],
9296                                               bw_ctr['bw_out'],
9297                                               start_period=start_time,
9298                                               last_refreshed=refreshed)
9299 
9300     def _get_host_volume_bdms(self, context, use_slave=False):
9301         """Return all block device mappings on a compute host."""
9302         compute_host_bdms = []
9303         instances = objects.InstanceList.get_by_host(context, self.host,
9304             use_slave=use_slave)
9305         for instance in instances:
9306             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9307                     context, instance.uuid, use_slave=use_slave)
9308             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
9309             compute_host_bdms.append(dict(instance=instance,
9310                                           instance_bdms=instance_bdms))
9311 
9312         return compute_host_bdms
9313 
9314     def _update_volume_usage_cache(self, context, vol_usages):
9315         """Updates the volume usage cache table with a list of stats."""
9316         for usage in vol_usages:
9317             # Allow switching of greenthreads between queries.
9318             greenthread.sleep(0)
9319             vol_usage = objects.VolumeUsage(context)
9320             vol_usage.volume_id = usage['volume']
9321             vol_usage.instance_uuid = usage['instance'].uuid
9322             vol_usage.project_id = usage['instance'].project_id
9323             vol_usage.user_id = usage['instance'].user_id
9324             vol_usage.availability_zone = usage['instance'].availability_zone
9325             vol_usage.curr_reads = usage['rd_req']
9326             vol_usage.curr_read_bytes = usage['rd_bytes']
9327             vol_usage.curr_writes = usage['wr_req']
9328             vol_usage.curr_write_bytes = usage['wr_bytes']
9329             vol_usage.save()
9330             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
9331             compute_utils.notify_about_volume_usage(context, vol_usage,
9332                                                     self.host)
9333 
9334     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
9335     def _poll_volume_usage(self, context):
9336         if CONF.volume_usage_poll_interval == 0:
9337             return
9338 
9339         compute_host_bdms = self._get_host_volume_bdms(context,
9340                                                        use_slave=True)
9341         if not compute_host_bdms:
9342             return
9343 
9344         LOG.debug("Updating volume usage cache")
9345         try:
9346             vol_usages = self.driver.get_all_volume_usage(context,
9347                                                           compute_host_bdms)
9348         except NotImplementedError:
9349             return
9350 
9351         self._update_volume_usage_cache(context, vol_usages)
9352 
9353     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
9354                                  run_immediately=True)
9355     def _sync_power_states(self, context):
9356         """Align power states between the database and the hypervisor.
9357 
9358         To sync power state data we make a DB call to get the number of
9359         virtual machines known by the hypervisor and if the number matches the
9360         number of virtual machines known by the database, we proceed in a lazy
9361         loop, one database record at a time, checking if the hypervisor has the
9362         same power state as is in the database.
9363         """
9364         db_instances = objects.InstanceList.get_by_host(context, self.host,
9365                                                         expected_attrs=[],
9366                                                         use_slave=True)
9367 
9368         try:
9369             num_vm_instances = self.driver.get_num_instances()
9370         except exception.VirtDriverNotReady as e:
9371             # If the virt driver is not ready, like ironic-api not being up
9372             # yet in the case of ironic, just log it and exit.
9373             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
9374             return
9375 
9376         num_db_instances = len(db_instances)
9377 
9378         if num_vm_instances != num_db_instances:
9379             LOG.warning("While synchronizing instance power states, found "
9380                         "%(num_db_instances)s instances in the database "
9381                         "and %(num_vm_instances)s instances on the "
9382                         "hypervisor.",
9383                         {'num_db_instances': num_db_instances,
9384                          'num_vm_instances': num_vm_instances})
9385 
9386         def _sync(db_instance):
9387             # NOTE(melwitt): This must be synchronized as we query state from
9388             #                two separate sources, the driver and the database.
9389             #                They are set (in stop_instance) and read, in sync.
9390             @utils.synchronized(db_instance.uuid)
9391             def query_driver_power_state_and_sync():
9392                 self._query_driver_power_state_and_sync(context, db_instance)
9393 
9394             try:
9395                 query_driver_power_state_and_sync()
9396             except Exception:
9397                 LOG.exception("Periodic sync_power_state task had an "
9398                               "error while processing an instance.",
9399                               instance=db_instance)
9400 
9401             self._syncs_in_progress.pop(db_instance.uuid)
9402 
9403         for db_instance in db_instances:
9404             # process syncs asynchronously - don't want instance locking to
9405             # block entire periodic task thread
9406             uuid = db_instance.uuid
9407             if uuid in self._syncs_in_progress:
9408                 LOG.debug('Sync already in progress for %s', uuid)
9409             else:
9410                 LOG.debug('Triggering sync for uuid %s', uuid)
9411                 self._syncs_in_progress[uuid] = True
9412                 self._sync_power_pool.spawn_n(_sync, db_instance)
9413 
9414     def _query_driver_power_state_and_sync(self, context, db_instance):
9415         if db_instance.task_state is not None:
9416             LOG.info("During sync_power_state the instance has a "
9417                      "pending task (%(task)s). Skip.",
9418                      {'task': db_instance.task_state}, instance=db_instance)
9419             return
9420         # No pending tasks. Now try to figure out the real vm_power_state.
9421         try:
9422             vm_instance = self.driver.get_info(db_instance)
9423             vm_power_state = vm_instance.state
9424         except exception.InstanceNotFound:
9425             vm_power_state = power_state.NOSTATE
9426         # Note(maoy): the above get_info call might take a long time,
9427         # for example, because of a broken libvirt driver.
9428         try:
9429             self._sync_instance_power_state(context,
9430                                             db_instance,
9431                                             vm_power_state,
9432                                             use_slave=True)
9433         except exception.InstanceNotFound:
9434             # NOTE(hanlind): If the instance gets deleted during sync,
9435             # silently ignore.
9436             pass
9437 
9438     def _stop_unexpected_shutdown_instance(self, context, vm_state,
9439                                            db_instance, orig_db_power_state):
9440         # this is an exceptional case; make sure our data is up
9441         # to date before slamming through a power off
9442         vm_instance = self.driver.get_info(db_instance,
9443                                            use_cache=False)
9444         vm_power_state = vm_instance.state
9445 
9446         # if it still looks off, go ahead and call stop()
9447         if vm_power_state in (power_state.SHUTDOWN,
9448                               power_state.CRASHED):
9449 
9450             LOG.warning("Instance shutdown by itself. Calling the "
9451                         "stop API. Current vm_state: %(vm_state)s, "
9452                         "current task_state: %(task_state)s, "
9453                         "original DB power_state: %(db_power_state)s, "
9454                         "current VM power_state: %(vm_power_state)s",
9455                         {'vm_state': vm_state,
9456                          'task_state': db_instance.task_state,
9457                          'db_power_state': orig_db_power_state,
9458                          'vm_power_state': vm_power_state},
9459                         instance=db_instance)
9460             try:
9461                 # Note(maoy): here we call the API instead of
9462                 # brutally updating the vm_state in the database
9463                 # to allow all the hooks and checks to be performed.
9464                 if db_instance.shutdown_terminate:
9465                     self.compute_api.delete(context, db_instance)
9466                 else:
9467                     self.compute_api.stop(context, db_instance)
9468             except Exception:
9469                 # Note(maoy): there is no need to propagate the error
9470                 # because the same power_state will be retrieved next
9471                 # time and retried.
9472                 # For example, there might be another task scheduled.
9473                 LOG.exception("error during stop() in sync_power_state.",
9474                               instance=db_instance)
9475 
9476     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
9477                                    use_slave=False):
9478         """Align instance power state between the database and hypervisor.
9479 
9480         If the instance is not found on the hypervisor, but is in the database,
9481         then a stop() API will be called on the instance.
9482         """
9483 
9484         # We re-query the DB to get the latest instance info to minimize
9485         # (not eliminate) race condition.
9486         db_instance.refresh(use_slave=use_slave)
9487         db_power_state = db_instance.power_state
9488         vm_state = db_instance.vm_state
9489 
9490         if self.host != db_instance.host:
9491             # on the sending end of nova-compute _sync_power_state
9492             # may have yielded to the greenthread performing a live
9493             # migration; this in turn has changed the resident-host
9494             # for the VM; However, the instance is still active, it
9495             # is just in the process of migrating to another host.
9496             # This implies that the compute source must relinquish
9497             # control to the compute destination.
9498             LOG.info("During the sync_power process the "
9499                      "instance has moved from "
9500                      "host %(src)s to host %(dst)s",
9501                      {'src': db_instance.host,
9502                       'dst': self.host},
9503                      instance=db_instance)
9504             return
9505         elif db_instance.task_state is not None:
9506             # on the receiving end of nova-compute, it could happen
9507             # that the DB instance already report the new resident
9508             # but the actual VM has not showed up on the hypervisor
9509             # yet. In this case, let's allow the loop to continue
9510             # and run the state sync in a later round
9511             LOG.info("During sync_power_state the instance has a "
9512                      "pending task (%(task)s). Skip.",
9513                      {'task': db_instance.task_state},
9514                      instance=db_instance)
9515             return
9516 
9517         orig_db_power_state = db_power_state
9518         if vm_power_state != db_power_state:
9519             LOG.info('During _sync_instance_power_state the DB '
9520                      'power_state (%(db_power_state)s) does not match '
9521                      'the vm_power_state from the hypervisor '
9522                      '(%(vm_power_state)s). Updating power_state in the '
9523                      'DB to match the hypervisor.',
9524                      {'db_power_state': db_power_state,
9525                       'vm_power_state': vm_power_state},
9526                      instance=db_instance)
9527             # power_state is always updated from hypervisor to db
9528             db_instance.power_state = vm_power_state
9529             db_instance.save()
9530             db_power_state = vm_power_state
9531 
9532         # Note(maoy): Now resolve the discrepancy between vm_state and
9533         # vm_power_state. We go through all possible vm_states.
9534         if vm_state in (vm_states.BUILDING,
9535                         vm_states.RESCUED,
9536                         vm_states.RESIZED,
9537                         vm_states.SUSPENDED,
9538                         vm_states.ERROR):
9539             # TODO(maoy): we ignore these vm_state for now.
9540             pass
9541         elif vm_state == vm_states.ACTIVE:
9542             # The only rational power state should be RUNNING
9543             if vm_power_state in (power_state.SHUTDOWN,
9544                                   power_state.CRASHED):
9545                 self._stop_unexpected_shutdown_instance(
9546                     context, vm_state, db_instance, orig_db_power_state)
9547             elif vm_power_state == power_state.SUSPENDED:
9548                 LOG.warning("Instance is suspended unexpectedly. Calling "
9549                             "the stop API.", instance=db_instance)
9550                 try:
9551                     self.compute_api.stop(context, db_instance)
9552                 except Exception:
9553                     LOG.exception("error during stop() in sync_power_state.",
9554                                   instance=db_instance)
9555             elif vm_power_state == power_state.PAUSED:
9556                 # Note(maoy): a VM may get into the paused state not only
9557                 # because the user request via API calls, but also
9558                 # due to (temporary) external instrumentations.
9559                 # Before the virt layer can reliably report the reason,
9560                 # we simply ignore the state discrepancy. In many cases,
9561                 # the VM state will go back to running after the external
9562                 # instrumentation is done. See bug 1097806 for details.
9563                 LOG.warning("Instance is paused unexpectedly. Ignore.",
9564                             instance=db_instance)
9565             elif vm_power_state == power_state.NOSTATE:
9566                 # Occasionally, depending on the status of the hypervisor,
9567                 # which could be restarting for example, an instance may
9568                 # not be found.  Therefore just log the condition.
9569                 LOG.warning("Instance is unexpectedly not found. Ignore.",
9570                             instance=db_instance)
9571         elif vm_state == vm_states.STOPPED:
9572             if vm_power_state not in (power_state.NOSTATE,
9573                                       power_state.SHUTDOWN,
9574                                       power_state.CRASHED):
9575                 LOG.warning("Instance is not stopped. Calling "
9576                             "the stop API. Current vm_state: %(vm_state)s,"
9577                             " current task_state: %(task_state)s, "
9578                             "original DB power_state: %(db_power_state)s, "
9579                             "current VM power_state: %(vm_power_state)s",
9580                             {'vm_state': vm_state,
9581                              'task_state': db_instance.task_state,
9582                              'db_power_state': orig_db_power_state,
9583                              'vm_power_state': vm_power_state},
9584                             instance=db_instance)
9585                 try:
9586                     # NOTE(russellb) Force the stop, because normally the
9587                     # compute API would not allow an attempt to stop a stopped
9588                     # instance.
9589                     self.compute_api.force_stop(context, db_instance)
9590                 except Exception:
9591                     LOG.exception("error during stop() in sync_power_state.",
9592                                   instance=db_instance)
9593         elif vm_state == vm_states.PAUSED:
9594             if vm_power_state in (power_state.SHUTDOWN,
9595                                   power_state.CRASHED):
9596                 LOG.warning("Paused instance shutdown by itself. Calling "
9597                             "the stop API.", instance=db_instance)
9598                 try:
9599                     self.compute_api.force_stop(context, db_instance)
9600                 except Exception:
9601                     LOG.exception("error during stop() in sync_power_state.",
9602                                   instance=db_instance)
9603         elif vm_state in (vm_states.SOFT_DELETED,
9604                           vm_states.DELETED):
9605             if vm_power_state not in (power_state.NOSTATE,
9606                                       power_state.SHUTDOWN):
9607                 # Note(maoy): this should be taken care of periodically in
9608                 # _cleanup_running_deleted_instances().
9609                 LOG.warning("Instance is not (soft-)deleted.",
9610                             instance=db_instance)
9611 
9612     @periodic_task.periodic_task
9613     def _reclaim_queued_deletes(self, context):
9614         """Reclaim instances that are queued for deletion."""
9615         interval = CONF.reclaim_instance_interval
9616         if interval <= 0:
9617             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
9618             return
9619 
9620         filters = {'vm_state': vm_states.SOFT_DELETED,
9621                    'task_state': None,
9622                    'host': self.host}
9623         instances = objects.InstanceList.get_by_filters(
9624             context, filters,
9625             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
9626             use_slave=True)
9627         for instance in instances:
9628             if self._deleted_old_enough(instance, interval):
9629                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9630                         context, instance.uuid)
9631                 LOG.info('Reclaiming deleted instance', instance=instance)
9632                 try:
9633                     self._delete_instance(context, instance, bdms)
9634                 except Exception as e:
9635                     LOG.warning("Periodic reclaim failed to delete "
9636                                 "instance: %s",
9637                                 e, instance=instance)
9638 
9639     def _get_nodename(self, instance, refresh=False):
9640         """Helper method to get the name of the first available node
9641         on this host. This method should not be used with any operations
9642         on ironic instances since it does not handle multiple nodes.
9643         """
9644         node = self.driver.get_available_nodes(refresh=refresh)[0]
9645         LOG.debug("No node specified, defaulting to %s", node,
9646                   instance=instance)
9647         return node
9648 
9649     def _update_available_resource_for_node(self, context, nodename,
9650                                             startup=False):
9651 
9652         try:
9653             self.rt.update_available_resource(context, nodename,
9654                                               startup=startup)
9655         except exception.ComputeHostNotFound:
9656             LOG.warning("Compute node '%s' not found in "
9657                         "update_available_resource.", nodename)
9658         except exception.ReshapeFailed:
9659             # We're only supposed to get here on startup, if a reshape was
9660             # needed, was attempted, and failed. We want to kill the service.
9661             with excutils.save_and_reraise_exception():
9662                 LOG.critical("Resource provider data migration failed "
9663                              "fatally during startup for node %s.", nodename)
9664         except exception.ReshapeNeeded:
9665             # This exception should only find its way here if the virt driver's
9666             # update_provider_tree raised it incorrectly: either
9667             # a) After the resource tracker already caught it once and
9668             # reinvoked update_provider_tree with allocations. At this point
9669             # the driver is just supposed to *do* the reshape, so if it raises
9670             # ReshapeNeeded, it's a bug, and we want to kill the compute
9671             # service.
9672             # b) On periodic rather than startup (we only allow reshapes to
9673             # happen on startup). In this case we'll just make the logs red and
9674             # go again at the next periodic interval, where the same thing may
9675             # or may not happen again. Depending on the previous and intended
9676             # shape of the providers/inventories, this may not actually cause
9677             # any immediately visible symptoms (in terms of scheduling, etc.)
9678             # If this becomes a problem, we may wish to make it pop immediately
9679             # (e.g. disable the service).
9680             with excutils.save_and_reraise_exception():
9681                 LOG.exception("ReshapeNeeded exception is unexpected here!")
9682         except Exception:
9683             LOG.exception("Error updating resources for node %(node)s.",
9684                           {'node': nodename})
9685 
9686     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
9687     def update_available_resource(self, context, startup=False):
9688         """See driver.get_available_resource()
9689 
9690         Periodic process that keeps that the compute host's understanding of
9691         resource availability and usage in sync with the underlying hypervisor.
9692 
9693         :param context: security context
9694         :param startup: True if this is being called when the nova-compute
9695             service is starting, False otherwise.
9696         """
9697         try:
9698             nodenames = set(self.driver.get_available_nodes())
9699         except exception.VirtDriverNotReady:
9700             LOG.warning("Virt driver is not ready.")
9701             return
9702 
9703         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
9704                                                             nodenames,
9705                                                             use_slave=True,
9706                                                             startup=startup)
9707 
9708         # Delete orphan compute node not reported by driver but still in db
9709         for cn in compute_nodes_in_db:
9710             if cn.hypervisor_hostname not in nodenames:
9711                 LOG.info("Deleting orphan compute node %(id)s "
9712                          "hypervisor host is %(hh)s, "
9713                          "nodes are %(nodes)s",
9714                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
9715                           'nodes': nodenames})
9716                 cn.destroy()
9717                 self.rt.remove_node(cn.hypervisor_hostname)
9718                 # Delete the corresponding resource provider in placement,
9719                 # along with any associated allocations.
9720                 try:
9721                     self.reportclient.delete_resource_provider(context, cn,
9722                                                                cascade=True)
9723                 except keystone_exception.ClientException as e:
9724                     LOG.error(
9725                         "Failed to delete compute node resource provider "
9726                         "for compute node %s: %s", cn.uuid, six.text_type(e))
9727 
9728         for nodename in nodenames:
9729             self._update_available_resource_for_node(context, nodename,
9730                                                      startup=startup)
9731 
9732     def _get_compute_nodes_in_db(self, context, nodenames, use_slave=False,
9733                                  startup=False):
9734         try:
9735             return objects.ComputeNodeList.get_all_by_host(context, self.host,
9736                                                            use_slave=use_slave)
9737         except exception.NotFound:
9738             # If the driver is not reporting any nodenames we should not
9739             # expect there to be compute nodes so we just return in that case.
9740             # For example, this could be an ironic compute and it is not
9741             # managing any nodes yet.
9742             if nodenames:
9743                 if startup:
9744                     LOG.warning(
9745                         "No compute node record found for host %s. If this is "
9746                         "the first time this service is starting on this "
9747                         "host, then you can ignore this warning.", self.host)
9748                 else:
9749                     LOG.error("No compute node record for host %s", self.host)
9750             return []
9751 
9752     @periodic_task.periodic_task(
9753         spacing=CONF.running_deleted_instance_poll_interval,
9754         run_immediately=True)
9755     def _cleanup_running_deleted_instances(self, context):
9756         """Cleanup any instances which are erroneously still running after
9757         having been deleted.
9758 
9759         Valid actions to take are:
9760 
9761             1. noop - do nothing
9762             2. log - log which instances are erroneously running
9763             3. reap - shutdown and cleanup any erroneously running instances
9764             4. shutdown - power off *and disable* any erroneously running
9765                           instances
9766 
9767         The use-case for this cleanup task is: for various reasons, it may be
9768         possible for the database to show an instance as deleted but for that
9769         instance to still be running on a host machine (see bug
9770         https://bugs.launchpad.net/nova/+bug/911366).
9771 
9772         This cleanup task is a cross-hypervisor utility for finding these
9773         zombied instances and either logging the discrepancy (likely what you
9774         should do in production), or automatically reaping the instances (more
9775         appropriate for dev environments).
9776         """
9777         action = CONF.running_deleted_instance_action
9778 
9779         if action == "noop":
9780             return
9781 
9782         # NOTE(sirp): admin contexts don't ordinarily return deleted records
9783         with utils.temporary_mutation(context, read_deleted="yes"):
9784 
9785             try:
9786                 instances = self._running_deleted_instances(context)
9787             except exception.VirtDriverNotReady:
9788                 # Since this task runs immediately on startup, if the
9789                 # hypervisor is not yet ready handle it gracefully.
9790                 LOG.debug('Unable to check for running deleted instances '
9791                           'at this time since the hypervisor is not ready.')
9792                 return
9793 
9794             for instance in instances:
9795                 if action == "log":
9796                     LOG.warning("Detected instance with name label "
9797                                 "'%s' which is marked as "
9798                                 "DELETED but still present on host.",
9799                                 instance.name, instance=instance)
9800 
9801                 elif action == 'shutdown':
9802                     LOG.info("Powering off instance with name label "
9803                              "'%s' which is marked as "
9804                              "DELETED but still present on host.",
9805                              instance.name, instance=instance)
9806                     try:
9807                         try:
9808                             # disable starting the instance
9809                             self.driver.set_bootable(instance, False)
9810                         except NotImplementedError:
9811                             LOG.debug("set_bootable is not implemented "
9812                                       "for the current driver")
9813                         # and power it off
9814                         self.driver.power_off(instance)
9815                     except Exception:
9816                         LOG.warning("Failed to power off instance",
9817                                     instance=instance, exc_info=True)
9818 
9819                 elif action == 'reap':
9820                     LOG.info("Destroying instance with name label "
9821                              "'%s' which is marked as "
9822                              "DELETED but still present on host.",
9823                              instance.name, instance=instance)
9824                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9825                         context, instance.uuid, use_slave=True)
9826                     self.instance_events.clear_events_for_instance(instance)
9827                     try:
9828                         self._shutdown_instance(context, instance, bdms,
9829                                                 notify=False)
9830                         self._cleanup_volumes(context, instance, bdms,
9831                                               detach=False)
9832                     except Exception as e:
9833                         LOG.warning("Periodic cleanup failed to delete "
9834                                     "instance: %s",
9835                                     e, instance=instance)
9836                 else:
9837                     raise Exception(_("Unrecognized value '%s'"
9838                                       " for CONF.running_deleted_"
9839                                       "instance_action") % action)
9840 
9841     def _running_deleted_instances(self, context):
9842         """Returns a list of instances nova thinks is deleted,
9843         but the hypervisor thinks is still running.
9844         """
9845         timeout = CONF.running_deleted_instance_timeout
9846         filters = {'deleted': True,
9847                    'soft_deleted': False}
9848         instances = self._get_instances_on_driver(context, filters)
9849         return [i for i in instances if self._deleted_old_enough(i, timeout)]
9850 
9851     def _deleted_old_enough(self, instance, timeout):
9852         deleted_at = instance.deleted_at
9853         if deleted_at:
9854             deleted_at = deleted_at.replace(tzinfo=None)
9855         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
9856 
9857     @contextlib.contextmanager
9858     def _error_out_instance_on_exception(self, context, instance,
9859                                          instance_state=vm_states.ACTIVE):
9860         """Context manager to set instance.vm_state after some operation raises
9861 
9862         Used to handle NotImplementedError and InstanceFaultRollback errors
9863         and reset the instance vm_state and task_state. The vm_state is set
9864         to the $instance_state parameter and task_state is set to None.
9865         For all other types of exceptions, the vm_state is set to ERROR and
9866         the task_state is left unchanged (although most callers will have the
9867         @reverts_task_state decorator which will set the task_state to None).
9868 
9869         Re-raises the original exception *except* in the case of
9870         InstanceFaultRollback in which case the wrapped `inner_exception` is
9871         re-raised.
9872 
9873         :param context: The nova auth request context for the operation.
9874         :param instance: The instance to update. The vm_state will be set by
9875             this context manager when an exception is raised.
9876         :param instance_state: For NotImplementedError and
9877             InstanceFaultRollback this is the vm_state to set the instance to
9878             when handling one of those types of exceptions. By default the
9879             instance will be set to ACTIVE, but the caller should control this
9880             in case there have been no changes to the running state of the
9881             instance. For example, resizing a stopped server where prep_resize
9882             fails early and does not change the power state of the guest should
9883             not set the instance status to ACTIVE but remain STOPPED.
9884             This parameter is ignored for all other types of exceptions and the
9885             instance vm_state is set to ERROR.
9886         """
9887         # NOTE(mriedem): Why doesn't this method just save off the
9888         # original instance.vm_state here rather than use a parameter? Or use
9889         # instance_state=None as an override but default to the current
9890         # vm_state when rolling back.
9891         instance_uuid = instance.uuid
9892         try:
9893             yield
9894         except (NotImplementedError, exception.InstanceFaultRollback) as error:
9895             # Use reraise=False to determine if we want to raise the original
9896             # exception or something else.
9897             with excutils.save_and_reraise_exception(reraise=False) as ctxt:
9898                 LOG.info("Setting instance back to %(state)s after: %(error)s",
9899                          {'state': instance_state, 'error': error},
9900                          instance_uuid=instance_uuid)
9901                 self._instance_update(context, instance,
9902                                       vm_state=instance_state,
9903                                       task_state=None)
9904                 if isinstance(error, exception.InstanceFaultRollback):
9905                     # Raise the wrapped exception.
9906                     raise error.inner_exception
9907                 # Else re-raise the NotImplementedError.
9908                 ctxt.reraise = True
9909         except Exception:
9910             LOG.exception('Setting instance vm_state to ERROR',
9911                           instance_uuid=instance_uuid)
9912             with excutils.save_and_reraise_exception():
9913                 # NOTE(mriedem): Why don't we pass clean_task_state=True here?
9914                 self._set_instance_obj_error_state(instance)
9915 
9916     @wrap_exception()
9917     def add_aggregate_host(self, context, aggregate, host, slave_info):
9918         """Notify hypervisor of change (for hypervisor pools)."""
9919         try:
9920             self.driver.add_to_aggregate(context, aggregate, host,
9921                                          slave_info=slave_info)
9922         except NotImplementedError:
9923             LOG.debug('Hypervisor driver does not support '
9924                       'add_aggregate_host')
9925         except exception.AggregateError:
9926             with excutils.save_and_reraise_exception():
9927                 self.driver.undo_aggregate_operation(
9928                                     context,
9929                                     aggregate.delete_host,
9930                                     aggregate, host)
9931 
9932     @wrap_exception()
9933     def remove_aggregate_host(self, context, host, slave_info, aggregate):
9934         """Removes a host from a physical hypervisor pool."""
9935         try:
9936             self.driver.remove_from_aggregate(context, aggregate, host,
9937                                               slave_info=slave_info)
9938         except NotImplementedError:
9939             LOG.debug('Hypervisor driver does not support '
9940                       'remove_aggregate_host')
9941         except (exception.AggregateError,
9942                 exception.InvalidAggregateAction) as e:
9943             with excutils.save_and_reraise_exception():
9944                 self.driver.undo_aggregate_operation(
9945                                     context,
9946                                     aggregate.add_host,
9947                                     aggregate, host,
9948                                     isinstance(e, exception.AggregateError))
9949 
9950     def _process_instance_event(self, instance, event):
9951         _event = self.instance_events.pop_instance_event(instance, event)
9952         if _event:
9953             LOG.debug('Processing event %(event)s',
9954                       {'event': event.key}, instance=instance)
9955             _event.send(event)
9956         else:
9957             # If it's a network-vif-unplugged event and the instance is being
9958             # deleted or live migrated then we don't need to make this a
9959             # warning as it's expected. There are other expected things which
9960             # could trigger this event like detaching an interface, but we
9961             # don't have a task state for that.
9962             # TODO(mriedem): We have other move operations and things like
9963             # hard reboot (probably rebuild as well) which trigger this event
9964             # but nothing listens for network-vif-unplugged. We should either
9965             # handle those other known cases or consider just not logging a
9966             # warning if we get this event and the instance is undergoing some
9967             # task state transition.
9968             if (event.name == 'network-vif-unplugged' and
9969                     instance.task_state in (
9970                         task_states.DELETING, task_states.MIGRATING)):
9971                 LOG.debug('Received event %s for instance with task_state %s.',
9972                           event.key, instance.task_state, instance=instance)
9973             else:
9974                 LOG.warning('Received unexpected event %(event)s for '
9975                             'instance with vm_state %(vm_state)s and '
9976                             'task_state %(task_state)s.',
9977                             {'event': event.key,
9978                              'vm_state': instance.vm_state,
9979                              'task_state': instance.task_state},
9980                             instance=instance)
9981 
9982     def _process_instance_vif_deleted_event(self, context, instance,
9983                                             deleted_vif_id):
9984         # If an attached port is deleted by neutron, it needs to
9985         # be detached from the instance.
9986         # And info cache needs to be updated.
9987         network_info = instance.info_cache.network_info
9988         for index, vif in enumerate(network_info):
9989             if vif['id'] == deleted_vif_id:
9990                 LOG.info('Neutron deleted interface %(intf)s; '
9991                          'detaching it from the instance and '
9992                          'deleting it from the info cache',
9993                          {'intf': vif['id']},
9994                          instance=instance)
9995                 profile = vif.get('profile', {}) or {}  # profile can be None
9996                 if profile.get('allocation'):
9997                     LOG.error(
9998                         'The bound port %(port_id)s is deleted in Neutron but '
9999                         'the resource allocation on the resource provider '
10000                         '%(rp_uuid)s is leaked until the server '
10001                         '%(server_uuid)s is deleted.',
10002                         {'port_id': vif['id'],
10003                          'rp_uuid': vif['profile']['allocation'],
10004                          'server_uuid': instance.uuid})
10005 
10006                 del network_info[index]
10007                 neutron.update_instance_cache_with_nw_info(
10008                     self.network_api, context, instance, nw_info=network_info)
10009                 try:
10010                     self.driver.detach_interface(context, instance, vif)
10011                 except NotImplementedError:
10012                     # Not all virt drivers support attach/detach of interfaces
10013                     # yet (like Ironic), so just ignore this.
10014                     pass
10015                 except exception.NovaException as ex:
10016                     # If the instance was deleted before the interface was
10017                     # detached, just log it at debug.
10018                     log_level = (logging.DEBUG
10019                                  if isinstance(ex, exception.InstanceNotFound)
10020                                  else logging.WARNING)
10021                     LOG.log(log_level,
10022                             "Detach interface failed, "
10023                             "port_id=%(port_id)s, reason: %(msg)s",
10024                             {'port_id': deleted_vif_id, 'msg': ex},
10025                             instance=instance)
10026                 break
10027 
10028     @wrap_instance_event(prefix='compute')
10029     @wrap_instance_fault
10030     def extend_volume(self, context, instance, extended_volume_id):
10031 
10032         # If an attached volume is extended by cinder, it needs to
10033         # be extended by virt driver so host can detect its new size.
10034         # And bdm needs to be updated.
10035         LOG.debug('Handling volume-extended event for volume %(vol)s',
10036                   {'vol': extended_volume_id}, instance=instance)
10037 
10038         try:
10039             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
10040                    context, extended_volume_id, instance.uuid)
10041         except exception.NotFound:
10042             LOG.warning('Extend volume failed, '
10043                         'volume %(vol)s is not attached to instance.',
10044                         {'vol': extended_volume_id},
10045                         instance=instance)
10046             return
10047 
10048         LOG.info('Cinder extended volume %(vol)s; '
10049                  'extending it to detect new size',
10050                  {'vol': extended_volume_id},
10051                  instance=instance)
10052         volume = self.volume_api.get(context, bdm.volume_id)
10053 
10054         if bdm.connection_info is None:
10055             LOG.warning('Extend volume failed, '
10056                         'attached volume %(vol)s has no connection_info',
10057                         {'vol': extended_volume_id},
10058                         instance=instance)
10059             return
10060 
10061         connection_info = jsonutils.loads(bdm.connection_info)
10062         bdm.volume_size = volume['size']
10063         bdm.save()
10064 
10065         if not self.driver.capabilities.get('supports_extend_volume', False):
10066             raise exception.ExtendVolumeNotSupported()
10067 
10068         try:
10069             self.driver.extend_volume(context, connection_info, instance,
10070                                       bdm.volume_size * units.Gi)
10071         except Exception as ex:
10072             LOG.warning('Extend volume failed, '
10073                         'volume_id=%(volume_id)s, reason: %(msg)s',
10074                         {'volume_id': extended_volume_id, 'msg': ex},
10075                         instance=instance)
10076             raise
10077 
10078     @staticmethod
10079     def _is_state_valid_for_power_update_event(instance, target_power_state):
10080         """Check if the current state of the instance allows it to be
10081         a candidate for the power-update event.
10082 
10083         :param instance: The nova instance object.
10084         :param target_power_state: The desired target power state; this should
10085                                    either be "POWER_ON" or "POWER_OFF".
10086         :returns Boolean: True if the instance can be subjected to the
10087                           power-update event.
10088         """
10089         if ((target_power_state == external_event_obj.POWER_ON and
10090                 instance.task_state is None and
10091                 instance.vm_state == vm_states.STOPPED and
10092                 instance.power_state == power_state.SHUTDOWN) or
10093             (target_power_state == external_event_obj.POWER_OFF and
10094                 instance.task_state is None and
10095                 instance.vm_state == vm_states.ACTIVE and
10096                 instance.power_state == power_state.RUNNING)):
10097             return True
10098         return False
10099 
10100     @wrap_exception()
10101     @reverts_task_state
10102     @wrap_instance_event(prefix='compute')
10103     @wrap_instance_fault
10104     def power_update(self, context, instance, target_power_state):
10105         """Power update of an instance prompted by an external event.
10106         :param context: The API request context.
10107         :param instance: The nova instance object.
10108         :param target_power_state: The desired target power state;
10109                                    this should either be "POWER_ON" or
10110                                    "POWER_OFF".
10111         """
10112 
10113         @utils.synchronized(instance.uuid)
10114         def do_power_update():
10115             LOG.debug('Handling power-update event with target_power_state %s '
10116                       'for instance', target_power_state, instance=instance)
10117             if not self._is_state_valid_for_power_update_event(
10118                     instance, target_power_state):
10119                 pow_state = fields.InstancePowerState.from_index(
10120                     instance.power_state)
10121                 LOG.info('The power-update %(tag)s event for instance '
10122                          '%(uuid)s is a no-op since the instance is in '
10123                          'vm_state %(vm_state)s, task_state '
10124                          '%(task_state)s and power_state '
10125                          '%(power_state)s.',
10126                          {'tag': target_power_state, 'uuid': instance.uuid,
10127                          'vm_state': instance.vm_state,
10128                          'task_state': instance.task_state,
10129                          'power_state': pow_state})
10130                 return
10131             LOG.debug("Trying to %s instance",
10132                       target_power_state, instance=instance)
10133             if target_power_state == external_event_obj.POWER_ON:
10134                 action = fields.NotificationAction.POWER_ON
10135                 notification_name = "power_on."
10136                 instance.task_state = task_states.POWERING_ON
10137             else:
10138                 # It's POWER_OFF
10139                 action = fields.NotificationAction.POWER_OFF
10140                 notification_name = "power_off."
10141                 instance.task_state = task_states.POWERING_OFF
10142                 instance.progress = 0
10143 
10144             try:
10145                 # Note that the task_state is set here rather than the API
10146                 # because this is a best effort operation and deferring
10147                 # updating the task_state until we get to the compute service
10148                 # avoids error handling in the API and needing to account for
10149                 # older compute services during rolling upgrades from Stein.
10150                 # If we lose a race, UnexpectedTaskStateError is handled
10151                 # below.
10152                 instance.save(expected_task_state=[None])
10153                 self._notify_about_instance_usage(context, instance,
10154                                                   notification_name + "start")
10155                 compute_utils.notify_about_instance_action(context, instance,
10156                     self.host, action=action,
10157                     phase=fields.NotificationPhase.START)
10158                 # UnexpectedTaskStateError raised from the driver will be
10159                 # handled below and not result in a fault, error notification
10160                 # or failure of the instance action. Other driver errors like
10161                 # NotImplementedError will be record a fault, send an error
10162                 # notification and mark the instance action as failed.
10163                 self.driver.power_update_event(instance, target_power_state)
10164                 self._notify_about_instance_usage(context, instance,
10165                                                   notification_name + "end")
10166                 compute_utils.notify_about_instance_action(context, instance,
10167                     self.host, action=action,
10168                     phase=fields.NotificationPhase.END)
10169             except exception.UnexpectedTaskStateError as e:
10170                 # Handling the power-update event is best effort and if we lost
10171                 # a race with some other action happening to the instance we
10172                 # just log it and return rather than fail the action.
10173                 LOG.info("The power-update event was possibly preempted: %s ",
10174                          e.format_message(), instance=instance)
10175                 return
10176         do_power_update()
10177 
10178     @wrap_exception()
10179     def external_instance_event(self, context, instances, events):
10180         # NOTE(danms): Some event types are handled by the manager, such
10181         # as when we're asked to update the instance's info_cache. If it's
10182         # not one of those, look for some thread(s) waiting for the event and
10183         # unblock them if so.
10184         for event in events:
10185             instance = [inst for inst in instances
10186                         if inst.uuid == event.instance_uuid][0]
10187             LOG.debug('Received event %(event)s',
10188                       {'event': event.key},
10189                       instance=instance)
10190             if event.name == 'network-changed':
10191                 try:
10192                     LOG.debug('Refreshing instance network info cache due to '
10193                               'event %s.', event.key, instance=instance)
10194                     self.network_api.get_instance_nw_info(
10195                         context, instance, refresh_vif_id=event.tag)
10196                 except exception.NotFound as e:
10197                     LOG.info('Failed to process external instance event '
10198                              '%(event)s due to: %(error)s',
10199                              {'event': event.key, 'error': six.text_type(e)},
10200                              instance=instance)
10201             elif event.name == 'network-vif-deleted':
10202                 try:
10203                     self._process_instance_vif_deleted_event(context,
10204                                                              instance,
10205                                                              event.tag)
10206                 except exception.NotFound as e:
10207                     LOG.info('Failed to process external instance event '
10208                              '%(event)s due to: %(error)s',
10209                              {'event': event.key, 'error': six.text_type(e)},
10210                              instance=instance)
10211             elif event.name == 'volume-extended':
10212                 self.extend_volume(context, instance, event.tag)
10213             elif event.name == 'power-update':
10214                 self.power_update(context, instance, event.tag)
10215             else:
10216                 self._process_instance_event(instance, event)
10217 
10218     @periodic_task.periodic_task(spacing=CONF.image_cache.manager_interval,
10219                                  external_process_ok=True)
10220     def _run_image_cache_manager_pass(self, context):
10221         """Run a single pass of the image cache manager."""
10222 
10223         if not self.driver.capabilities.get("has_imagecache", False):
10224             return
10225 
10226         # Determine what other nodes use this storage
10227         storage_users.register_storage_use(CONF.instances_path, CONF.host)
10228         nodes = storage_users.get_storage_users(CONF.instances_path)
10229 
10230         # Filter all_instances to only include those nodes which share this
10231         # storage path.
10232         # TODO(mikal): this should be further refactored so that the cache
10233         # cleanup code doesn't know what those instances are, just a remote
10234         # count, and then this logic should be pushed up the stack.
10235         filters = {'deleted': False,
10236                    'soft_deleted': True,
10237                    'host': nodes}
10238         filtered_instances = objects.InstanceList.get_by_filters(context,
10239                                  filters, expected_attrs=[], use_slave=True)
10240 
10241         self.driver.manage_image_cache(context, filtered_instances)
10242 
10243     def cache_images(self, context, image_ids):
10244         """Ask the virt driver to pre-cache a set of base images.
10245 
10246         :param context: The RequestContext
10247         :param image_ids: The image IDs to be cached
10248         :return: A dict, keyed by image-id where the values are one of:
10249                  'cached' if the image was downloaded,
10250                  'existing' if the image was already in the cache,
10251                  'unsupported' if the virt driver does not support caching,
10252                  'error' if the virt driver raised an exception.
10253         """
10254 
10255         results = {}
10256 
10257         LOG.info('Caching %i image(s) by request', len(image_ids))
10258         for image_id in image_ids:
10259             try:
10260                 cached = self.driver.cache_image(context, image_id)
10261                 if cached:
10262                     results[image_id] = 'cached'
10263                 else:
10264                     results[image_id] = 'existing'
10265             except NotImplementedError:
10266                 LOG.warning('Virt driver does not support image pre-caching;'
10267                             ' ignoring request')
10268                 # NOTE(danms): Yes, technically we could short-circuit here to
10269                 # avoid trying the rest of the images, but it's very cheap to
10270                 # just keep hitting the NotImplementedError to keep the logic
10271                 # clean.
10272                 results[image_id] = 'unsupported'
10273             except Exception as e:
10274                 results[image_id] = 'error'
10275                 LOG.error('Failed to cache image %(image_id)s: %(err)s',
10276                           {'image_id': image_id,
10277                            'err': e})
10278 
10279         return results
10280 
10281     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10282     def _run_pending_deletes(self, context):
10283         """Retry any pending instance file deletes."""
10284         LOG.debug('Cleaning up deleted instances')
10285         filters = {'deleted': True,
10286                    'soft_deleted': False,
10287                    'host': CONF.host,
10288                    'cleaned': False}
10289         attrs = ['system_metadata']
10290         with utils.temporary_mutation(context, read_deleted='yes'):
10291             instances = objects.InstanceList.get_by_filters(
10292                 context, filters, expected_attrs=attrs, use_slave=True)
10293         LOG.debug('There are %d instances to clean', len(instances))
10294 
10295         for instance in instances:
10296             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
10297             LOG.debug('Instance has had %(attempts)s of %(max)s '
10298                       'cleanup attempts',
10299                       {'attempts': attempts,
10300                        'max': CONF.maximum_instance_delete_attempts},
10301                       instance=instance)
10302             if attempts < CONF.maximum_instance_delete_attempts:
10303                 success = self.driver.delete_instance_files(instance)
10304 
10305                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
10306                 if success:
10307                     instance.cleaned = True
10308                 with utils.temporary_mutation(context, read_deleted='yes'):
10309                     instance.save()
10310 
10311     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10312     def _cleanup_incomplete_migrations(self, context):
10313         """Delete instance files on failed resize/revert-resize operation
10314 
10315         During resize/revert-resize operation, if that instance gets deleted
10316         in-between then instance files might remain either on source or
10317         destination compute node because of race condition.
10318         """
10319         LOG.debug('Cleaning up deleted instances with incomplete migration ')
10320         migration_filters = {'host': CONF.host,
10321                              'status': 'error'}
10322         migrations = objects.MigrationList.get_by_filters(context,
10323                                                           migration_filters)
10324 
10325         if not migrations:
10326             return
10327 
10328         inst_uuid_from_migrations = set([migration.instance_uuid for migration
10329                                          in migrations])
10330 
10331         inst_filters = {'deleted': True, 'soft_deleted': False,
10332                         'uuid': inst_uuid_from_migrations}
10333         attrs = ['info_cache', 'security_groups', 'system_metadata']
10334         with utils.temporary_mutation(context, read_deleted='yes'):
10335             instances = objects.InstanceList.get_by_filters(
10336                 context, inst_filters, expected_attrs=attrs, use_slave=True)
10337 
10338         for instance in instances:
10339             if instance.host != CONF.host:
10340                 for migration in migrations:
10341                     if instance.uuid == migration.instance_uuid:
10342                         # Delete instance files if not cleanup properly either
10343                         # from the source or destination compute nodes when
10344                         # the instance is deleted during resizing.
10345                         self.driver.delete_instance_files(instance)
10346                         try:
10347                             migration.status = 'failed'
10348                             migration.save()
10349                         except exception.MigrationNotFound:
10350                             LOG.warning("Migration %s is not found.",
10351                                         migration.id,
10352                                         instance=instance)
10353                         break
10354 
10355     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10356                                    exception.QemuGuestAgentNotEnabled,
10357                                    exception.NovaException,
10358                                    NotImplementedError)
10359     @wrap_exception()
10360     def quiesce_instance(self, context, instance):
10361         """Quiesce an instance on this host."""
10362         context = context.elevated()
10363         image_meta = objects.ImageMeta.from_instance(instance)
10364         self.driver.quiesce(context, instance, image_meta)
10365 
10366     def _wait_for_snapshots_completion(self, context, mapping):
10367         for mapping_dict in mapping:
10368             if mapping_dict.get('source_type') == 'snapshot':
10369 
10370                 def _wait_snapshot():
10371                     snapshot = self.volume_api.get_snapshot(
10372                         context, mapping_dict['snapshot_id'])
10373                     if snapshot.get('status') != 'creating':
10374                         raise loopingcall.LoopingCallDone()
10375 
10376                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
10377                 timer.start(interval=0.5).wait()
10378 
10379     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10380                                    exception.QemuGuestAgentNotEnabled,
10381                                    exception.NovaException,
10382                                    NotImplementedError)
10383     @wrap_exception()
10384     def unquiesce_instance(self, context, instance, mapping=None):
10385         """Unquiesce an instance on this host.
10386 
10387         If snapshots' image mapping is provided, it waits until snapshots are
10388         completed before unqueiscing.
10389         """
10390         context = context.elevated()
10391         if mapping:
10392             try:
10393                 self._wait_for_snapshots_completion(context, mapping)
10394             except Exception as error:
10395                 LOG.exception("Exception while waiting completion of "
10396                               "volume snapshots: %s",
10397                               error, instance=instance)
10398         image_meta = objects.ImageMeta.from_instance(instance)
10399         self.driver.unquiesce(context, instance, image_meta)
10400 
10401     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10402     def _cleanup_expired_console_auth_tokens(self, context):
10403         """Remove all expired console auth tokens.
10404 
10405         Console authorization tokens and their connection data are stored
10406         in the database when a user asks for a console connection to an
10407         instance. After a time they expire. We periodically remove any expired
10408         tokens from the database.
10409         """
10410         objects.ConsoleAuthToken.clean_expired_console_auths(context)
10411 
10412     def _claim_pci_for_instance_vifs(self, ctxt, instance):
10413         """Claim PCI devices for the instance's VIFs on the compute node
10414 
10415         :param ctxt: Context
10416         :param instance: Instance object
10417         :return: <port ID: PciDevice> mapping for the VIFs that yielded a
10418                 PCI claim on the compute node
10419         """
10420         pci_req_id_to_port_id = {}
10421         pci_reqs = []
10422         port_id_to_pci_dev = {}
10423 
10424         for vif in instance.get_network_info():
10425             pci_req = pci_req_module.get_instance_pci_request_from_vif(
10426                 ctxt,
10427                 instance,
10428                 vif)
10429             if pci_req:
10430                 pci_req_id_to_port_id[pci_req.request_id] = vif['id']
10431                 pci_reqs.append(pci_req)
10432 
10433         if pci_reqs:
10434             # Create PCI requests and claim against PCI resource tracker
10435             # NOTE(adrianc): We claim against the same requests as on the
10436             # source node.
10437             vif_pci_requests = objects.InstancePCIRequests(
10438                 requests=pci_reqs,
10439                 instance_uuid=instance.uuid)
10440 
10441             claimed_pci_devices_objs = self.rt.claim_pci_devices(
10442                 ctxt,
10443                 vif_pci_requests)
10444 
10445             # Update VIFMigrateData profile with the newly claimed PCI
10446             # device
10447             for pci_dev in claimed_pci_devices_objs:
10448                 LOG.debug("PCI device: %s Claimed on destination node",
10449                           pci_dev.address)
10450                 port_id = pci_req_id_to_port_id[pci_dev.request_id]
10451                 port_id_to_pci_dev[port_id] = pci_dev
10452 
10453         return port_id_to_pci_dev
10454 
10455     def _update_migrate_vifs_profile_with_pci(self,
10456                                               migrate_vifs,
10457                                               port_id_to_pci_dev):
10458         """Update migrate vifs profile with the claimed PCI devices
10459 
10460         :param migrate_vifs: list of VIFMigrateData objects
10461         :param port_id_to_pci_dev: a <port_id: PciDevice> mapping
10462         :return: None.
10463         """
10464         for mig_vif in migrate_vifs:
10465             port_id = mig_vif.port_id
10466             if port_id not in port_id_to_pci_dev:
10467                 continue
10468 
10469             pci_dev = port_id_to_pci_dev[port_id]
10470             profile = copy.deepcopy(mig_vif.source_vif['profile'])
10471             profile['pci_slot'] = pci_dev.address
10472             profile['pci_vendor_info'] = ':'.join([pci_dev.vendor_id,
10473                                                    pci_dev.product_id])
10474             mig_vif.profile = profile
10475             LOG.debug("Updating migrate VIF profile for port %(port_id)s:"
10476                       "%(profile)s", {'port_id': port_id,
10477                                       'profile': profile})
