I want you to act as a code reviewer of Nova in OpenStack. Please review the code below to detect security defects. If any are found, please describe the security defect in detail and indicate the corresponding line number of code and solution. If none are found, please state '''No security defects are detected in the code'''.

1 # Copyright 2010 United States Government as represented by the
2 # Administrator of the National Aeronautics and Space Administration.
3 # Copyright 2011 Justin Santa Barbara
4 # All Rights Reserved.
5 #
6 #    Licensed under the Apache License, Version 2.0 (the "License"); you may
7 #    not use this file except in compliance with the License. You may obtain
8 #    a copy of the License at
9 #
10 #         http://www.apache.org/licenses/LICENSE-2.0
11 #
12 #    Unless required by applicable law or agreed to in writing, software
13 #    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
14 #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
15 #    License for the specific language governing permissions and limitations
16 #    under the License.
17 
18 """Handles all processes relating to instances (guest vms).
19 
20 The :py:class:`ComputeManager` class is a :py:class:`nova.manager.Manager` that
21 handles RPC calls relating to creating instances.  It is responsible for
22 building a disk image, launching it via the underlying virtualization driver,
23 responding to calls to check its state, attaching persistent storage, and
24 terminating it.
25 
26 """
27 
28 import base64
29 import binascii
30 import contextlib
31 import copy
32 import functools
33 import inspect
34 import sys
35 import time
36 import traceback
37 import typing as ty
38 
39 from cinderclient import exceptions as cinder_exception
40 from cursive import exception as cursive_exception
41 import eventlet.event
42 from eventlet import greenthread
43 import eventlet.semaphore
44 import eventlet.timeout
45 import futurist
46 from keystoneauth1 import exceptions as keystone_exception
47 import os_traits
48 from oslo_log import log as logging
49 import oslo_messaging as messaging
50 from oslo_serialization import jsonutils
51 from oslo_service import loopingcall
52 from oslo_service import periodic_task
53 from oslo_utils import excutils
54 from oslo_utils import strutils
55 from oslo_utils import timeutils
56 from oslo_utils import units
57 
58 from nova.accelerator import cyborg
59 from nova import block_device
60 from nova.compute import api as compute
61 from nova.compute import build_results
62 from nova.compute import claims
63 from nova.compute import power_state
64 from nova.compute import resource_tracker
65 from nova.compute import rpcapi as compute_rpcapi
66 from nova.compute import task_states
67 from nova.compute import utils as compute_utils
68 from nova.compute.utils import wrap_instance_event
69 from nova.compute import vm_states
70 from nova import conductor
71 import nova.conf
72 import nova.context
73 from nova import exception
74 from nova import exception_wrapper
75 from nova.i18n import _
76 from nova.image import glance
77 from nova import manager
78 from nova.network import model as network_model
79 from nova.network import neutron
80 from nova import objects
81 from nova.objects import base as obj_base
82 from nova.objects import external_event as external_event_obj
83 from nova.objects import fields
84 from nova.objects import instance as obj_instance
85 from nova.objects import migrate_data as migrate_data_obj
86 from nova.pci import request as pci_req_module
87 from nova.pci import whitelist
88 from nova import safe_utils
89 from nova.scheduler.client import query
90 from nova.scheduler.client import report
91 from nova.scheduler import utils as scheduler_utils
92 from nova import utils
93 from nova.virt import block_device as driver_block_device
94 from nova.virt import configdrive
95 from nova.virt import driver
96 from nova.virt import event as virtevent
97 from nova.virt import hardware
98 from nova.virt import storage_users
99 from nova.virt import virtapi
100 from nova.volume import cinder
101 
102 CONF = nova.conf.CONF
103 
104 LOG = logging.getLogger(__name__)
105 
106 wrap_exception = functools.partial(
107     exception_wrapper.wrap_exception, service='compute', binary='nova-compute')
108 
109 
110 @contextlib.contextmanager
111 def errors_out_migration_ctxt(migration):
112     """Context manager to error out migration on failure."""
113 
114     try:
115         yield
116     except Exception:
117         with excutils.save_and_reraise_exception():
118             if migration:
119                 # We may have been passed None for our migration if we're
120                 # receiving from an older client. The migration will be
121                 # errored via the legacy path.
122                 migration.status = 'error'
123                 try:
124                     migration.save()
125                 except Exception:
126                     LOG.debug(
127                         'Error setting migration status for instance %s.',
128                         migration.instance_uuid, exc_info=True)
129 
130 
131 @utils.expects_func_args('migration')
132 def errors_out_migration(function):
133     """Decorator to error out migration on failure."""
134 
135     @functools.wraps(function)
136     def decorated_function(self, context, *args, **kwargs):
137         wrapped_func = safe_utils.get_wrapped_function(function)
138         keyed_args = inspect.getcallargs(wrapped_func, self, context,
139                                          *args, **kwargs)
140         migration = keyed_args['migration']
141         with errors_out_migration_ctxt(migration):
142             return function(self, context, *args, **kwargs)
143 
144     return decorated_function
145 
146 
147 @utils.expects_func_args('instance')
148 def reverts_task_state(function):
149     """Decorator to revert task_state on failure."""
150 
151     @functools.wraps(function)
152     def decorated_function(self, context, *args, **kwargs):
153         try:
154             return function(self, context, *args, **kwargs)
155         except exception.UnexpectedTaskStateError as e:
156             # Note(maoy): unexpected task state means the current
157             # task is preempted. Do not clear task state in this
158             # case.
159             with excutils.save_and_reraise_exception():
160                 LOG.info("Task possibly preempted: %s",
161                          e.format_message())
162         except Exception:
163             with excutils.save_and_reraise_exception():
164                 wrapped_func = safe_utils.get_wrapped_function(function)
165                 keyed_args = inspect.getcallargs(wrapped_func, self, context,
166                                                  *args, **kwargs)
167                 # NOTE(mriedem): 'instance' must be in keyed_args because we
168                 # have utils.expects_func_args('instance') decorating this
169                 # method.
170                 instance = keyed_args['instance']
171                 original_task_state = instance.task_state
172                 try:
173                     self._instance_update(context, instance, task_state=None)
174                     LOG.info("Successfully reverted task state from %s on "
175                              "failure for instance.",
176                              original_task_state, instance=instance)
177                 except exception.InstanceNotFound:
178                     # We might delete an instance that failed to build shortly
179                     # after it errored out this is an expected case and we
180                     # should not trace on it.
181                     pass
182                 except Exception as e:
183                     LOG.warning("Failed to revert task state for instance. "
184                                 "Error: %s", e, instance=instance)
185 
186     return decorated_function
187 
188 
189 @utils.expects_func_args('instance')
190 def wrap_instance_fault(function):
191     """Wraps a method to catch exceptions related to instances.
192 
193     This decorator wraps a method to catch any exceptions having to do with
194     an instance that may get thrown. It then logs an instance fault in the db.
195     """
196 
197     @functools.wraps(function)
198     def decorated_function(self, context, *args, **kwargs):
199         try:
200             return function(self, context, *args, **kwargs)
201         except exception.InstanceNotFound:
202             raise
203         except Exception as e:
204             # NOTE(gtt): If argument 'instance' is in args rather than kwargs,
205             # we will get a KeyError exception which will cover up the real
206             # exception. So, we update kwargs with the values from args first.
207             # then, we can get 'instance' from kwargs easily.
208             kwargs.update(dict(zip(function.__code__.co_varnames[2:], args)))
209 
210             with excutils.save_and_reraise_exception():
211                 compute_utils.add_instance_fault_from_exc(context,
212                         kwargs['instance'], e, sys.exc_info())
213 
214     return decorated_function
215 
216 
217 @utils.expects_func_args('image_id', 'instance')
218 def delete_image_on_error(function):
219     """Used for snapshot related method to ensure the image created in
220     compute.api is deleted when an error occurs.
221     """
222 
223     @functools.wraps(function)
224     def decorated_function(self, context, image_id, instance,
225                            *args, **kwargs):
226         try:
227             return function(self, context, image_id, instance,
228                             *args, **kwargs)
229         except Exception:
230             with excutils.save_and_reraise_exception():
231                 compute_utils.delete_image(
232                     context, instance, self.image_api, image_id,
233                     log_exc_info=True)
234 
235     return decorated_function
236 
237 
238 # Each collection of events is a dict of eventlet Events keyed by a tuple of
239 # event name and associated tag
240 _InstanceEvents = ty.Dict[ty.Tuple[str, str], eventlet.event.Event]
241 
242 
243 class InstanceEvents(object):
244     def __init__(self):
245         self._events: ty.Optional[ty.Dict[str, _InstanceEvents]] = {}
246 
247     @staticmethod
248     def _lock_name(instance) -> str:
249         return '%s-%s' % (instance.uuid, 'events')
250 
251     def prepare_for_instance_event(
252         self,
253         instance: 'objects.Instance',
254         name: str,
255         tag: str,
256     ) -> eventlet.event.Event:
257         """Prepare to receive an event for an instance.
258 
259         This will register an event for the given instance that we will
260         wait on later. This should be called before initiating whatever
261         action will trigger the event. The resulting eventlet.event.Event
262         object should be wait()'d on to ensure completion.
263 
264         :param instance: the instance for which the event will be generated
265         :param name: the name of the event we're expecting
266         :param tag: the tag associated with the event we're expecting
267         :returns: an event object that should be wait()'d on
268         """
269         @utils.synchronized(self._lock_name(instance))
270         def _create_or_get_event():
271             if self._events is None:
272                 # NOTE(danms): We really should have a more specific error
273                 # here, but this is what we use for our default error case
274                 raise exception.NovaException(
275                     'In shutdown, no new events can be scheduled')
276 
277             instance_events = self._events.setdefault(instance.uuid, {})
278             return instance_events.setdefault((name, tag),
279                                               eventlet.event.Event())
280         LOG.debug('Preparing to wait for external event %(name)s-%(tag)s',
281                   {'name': name, 'tag': tag}, instance=instance)
282         return _create_or_get_event()
283 
284     def pop_instance_event(self, instance, event):
285         """Remove a pending event from the wait list.
286 
287         This will remove a pending event from the wait list so that it
288         can be used to signal the waiters to wake up.
289 
290         :param instance: the instance for which the event was generated
291         :param event: the nova.objects.external_event.InstanceExternalEvent
292                       that describes the event
293         :returns: the eventlet.event.Event object on which the waiters
294                   are blocked
295         """
296         no_events_sentinel = object()
297         no_matching_event_sentinel = object()
298 
299         @utils.synchronized(self._lock_name(instance))
300         def _pop_event():
301             if self._events is None:
302                 LOG.debug('Unexpected attempt to pop events during shutdown',
303                           instance=instance)
304                 return no_events_sentinel
305             events = self._events.get(instance.uuid)
306             if not events:
307                 return no_events_sentinel
308             _event = events.pop((event.name, event.tag), None)
309             if not events:
310                 del self._events[instance.uuid]
311             if _event is None:
312                 return no_matching_event_sentinel
313             return _event
314 
315         result = _pop_event()
316         if result is no_events_sentinel:
317             LOG.debug('No waiting events found dispatching %(event)s',
318                       {'event': event.key},
319                       instance=instance)
320             return None
321         elif result is no_matching_event_sentinel:
322             LOG.debug(
323                 'No event matching %(event)s in %(events)s',
324                 {
325                     'event': event.key,
326                     # mypy can't identify the none check in _pop_event
327                     'events': self._events.get(  # type: ignore
328                         instance.uuid, {}).keys(),
329                 },
330                 instance=instance,
331             )
332             return None
333         else:
334             return result
335 
336     def clear_events_for_instance(self, instance):
337         """Remove all pending events for an instance.
338 
339         This will remove all events currently pending for an instance
340         and return them (indexed by event name).
341 
342         :param instance: the instance for which events should be purged
343         :returns: a dictionary of {event_name: eventlet.event.Event}
344         """
345         @utils.synchronized(self._lock_name(instance))
346         def _clear_events():
347             if self._events is None:
348                 LOG.debug('Unexpected attempt to clear events during shutdown',
349                           instance=instance)
350                 return dict()
351             # NOTE(danms): We have historically returned the raw internal
352             # format here, which is {event.key: [events, ...])} so just
353             # trivially convert it here.
354             return {'%s-%s' % k: e
355                     for k, e in self._events.pop(instance.uuid, {}).items()}
356         return _clear_events()
357 
358     def cancel_all_events(self):
359         if self._events is None:
360             LOG.debug('Unexpected attempt to cancel events during shutdown.')
361             return
362         our_events = self._events
363         # NOTE(danms): Block new events
364         self._events = None
365 
366         for instance_uuid, events in our_events.items():
367             for (name, tag), eventlet_event in events.items():
368                 LOG.debug('Canceling in-flight event %(name)s-%(tag)s for '
369                           'instance %(instance_uuid)s',
370                           {'name': name,
371                            'tag': tag,
372                            'instance_uuid': instance_uuid})
373                 event = objects.InstanceExternalEvent(
374                     instance_uuid=instance_uuid,
375                     name=name, status='failed',
376                     tag=tag, data={})
377                 eventlet_event.send(event)
378 
379 
380 class ComputeVirtAPI(virtapi.VirtAPI):
381     def __init__(self, compute):
382         super(ComputeVirtAPI, self).__init__()
383         self._compute = compute
384         self.reportclient = compute.reportclient
385 
386         class ExitEarly(Exception):
387             def __init__(self, events):
388                 super(Exception, self).__init__()
389                 self.events = events
390 
391         self._exit_early_exc = ExitEarly
392 
393     def exit_wait_early(self, events):
394         """Exit a wait_for_instance_event() immediately and avoid
395         waiting for some events.
396 
397         :param: events: A list of (name, tag) tuples for events that we should
398                         skip waiting for during a wait_for_instance_event().
399         """
400         raise self._exit_early_exc(events=events)
401 
402     def _default_error_callback(self, event_name, instance):
403         raise exception.NovaException(_('Instance event failed'))
404 
405     @contextlib.contextmanager
406     def wait_for_instance_event(self, instance, event_names, deadline=300,
407                                 error_callback=None):
408         """Plan to wait for some events, run some code, then wait.
409 
410         This context manager will first create plans to wait for the
411         provided event_names, yield, and then wait for all the scheduled
412         events to complete.
413 
414         Note that this uses an eventlet.timeout.Timeout to bound the
415         operation, so callers should be prepared to catch that
416         failure and handle that situation appropriately.
417 
418         If the event is not received by the specified timeout deadline,
419         eventlet.timeout.Timeout is raised.
420 
421         If the event is received but did not have a 'completed'
422         status, a NovaException is raised.  If an error_callback is
423         provided, instead of raising an exception as detailed above
424         for the failure case, the callback will be called with the
425         event_name and instance, and can return True to continue
426         waiting for the rest of the events, False to stop processing,
427         or raise an exception which will bubble up to the waiter.
428 
429         If the inner code wishes to abort waiting for one or more
430         events because it knows some state to be finished or condition
431         to be satisfied, it can use VirtAPI.exit_wait_early() with a
432         list of event (name,tag) items to avoid waiting for those
433         events upon context exit. Note that exit_wait_early() exits
434         the context immediately and should be used to signal that all
435         work has been completed and provide the unified list of events
436         that need not be waited for. Waiting for the remaining events
437         will begin immediately upon early exit as if the context was
438         exited normally.
439 
440         :param instance: The instance for which an event is expected
441         :param event_names: A list of event names. Each element is a
442                             tuple of strings to indicate (name, tag),
443                             where name is required, but tag may be None.
444         :param deadline: Maximum number of seconds we should wait for all
445                          of the specified events to arrive.
446         :param error_callback: A function to be called if an event arrives
447 
448         """
449 
450         if error_callback is None:
451             error_callback = self._default_error_callback
452         events = {}
453         for event_name in event_names:
454             name, tag = event_name
455             event_name = objects.InstanceExternalEvent.make_key(name, tag)
456             try:
457                 events[event_name] = (
458                     self._compute.instance_events.prepare_for_instance_event(
459                         instance, name, tag))
460             except exception.NovaException:
461                 error_callback(event_name, instance)
462                 # NOTE(danms): Don't wait for any of the events. They
463                 # should all be canceled and fired immediately below,
464                 # but don't stick around if not.
465                 deadline = 0
466         try:
467             yield
468         except self._exit_early_exc as e:
469             early_events = set([objects.InstanceExternalEvent.make_key(n, t)
470                                 for n, t in e.events])
471         else:
472             early_events = set([])
473 
474         with eventlet.timeout.Timeout(deadline):
475             for event_name, event in events.items():
476                 if event_name in early_events:
477                     continue
478                 else:
479                     actual_event = event.wait()
480                     if actual_event.status == 'completed':
481                         continue
482                 # If we get here, we have an event that was not completed,
483                 # nor skipped via exit_wait_early(). Decide whether to
484                 # keep waiting by calling the error_callback() hook.
485                 decision = error_callback(event_name, instance)
486                 if decision is False:
487                     break
488 
489     def update_compute_provider_status(self, context, rp_uuid, enabled):
490         """Used to add/remove the COMPUTE_STATUS_DISABLED trait on the provider
491 
492         :param context: nova auth RequestContext
493         :param rp_uuid: UUID of a compute node resource provider in Placement
494         :param enabled: True if the node is enabled in which case the trait
495             would be removed, False if the node is disabled in which case
496             the trait would be added.
497         :raises: ResourceProviderTraitRetrievalFailed
498         :raises: ResourceProviderUpdateConflict
499         :raises: ResourceProviderUpdateFailed
500         :raises: TraitRetrievalFailed
501         :raises: keystoneauth1.exceptions.ClientException
502         """
503         trait_name = os_traits.COMPUTE_STATUS_DISABLED
504         # Get the current traits (and generation) for the provider.
505         # TODO(mriedem): Leverage the ProviderTree cache in get_provider_traits
506         trait_info = self.reportclient.get_provider_traits(context, rp_uuid)
507         # If the host is enabled, remove the trait (if set), else add
508         # the trait if it doesn't already exist.
509         original_traits = trait_info.traits
510         new_traits = None
511         if enabled and trait_name in original_traits:
512             new_traits = original_traits - {trait_name}
513             LOG.debug('Removing trait %s from compute node resource '
514                       'provider %s in placement.', trait_name, rp_uuid)
515         elif not enabled and trait_name not in original_traits:
516             new_traits = original_traits | {trait_name}
517             LOG.debug('Adding trait %s to compute node resource '
518                       'provider %s in placement.', trait_name, rp_uuid)
519 
520         if new_traits is not None:
521             self.reportclient.set_traits_for_provider(
522                 context, rp_uuid, new_traits, generation=trait_info.generation)
523 
524 
525 class ComputeManager(manager.Manager):
526     """Manages the running instances from creation to destruction."""
527 
528     target = messaging.Target(version='6.0')
529 
530     def __init__(self, compute_driver=None, *args, **kwargs):
531         """Load configuration options and connect to the hypervisor."""
532         # We want the ComputeManager, ResourceTracker and ComputeVirtAPI all
533         # using the same instance of SchedulerReportClient which has the
534         # ProviderTree cache for this compute service.
535         self.reportclient = report.SchedulerReportClient()
536         self.virtapi = ComputeVirtAPI(self)
537         self.network_api = neutron.API()
538         self.volume_api = cinder.API()
539         self.image_api = glance.API()
540         self._last_bw_usage_poll = 0.0
541         self.compute_api = compute.API()
542         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
543         self.compute_task_api = conductor.ComputeTaskAPI()
544         self.query_client = query.SchedulerQueryClient()
545         self.instance_events = InstanceEvents()
546         self._sync_power_pool = eventlet.GreenPool(
547             size=CONF.sync_power_state_pool_size)
548         self._syncs_in_progress = {}
549         self.send_instance_updates = (
550             CONF.filter_scheduler.track_instance_changes)
551         if CONF.max_concurrent_builds != 0:
552             self._build_semaphore = eventlet.semaphore.Semaphore(
553                 CONF.max_concurrent_builds)
554         else:
555             self._build_semaphore = compute_utils.UnlimitedSemaphore()
556         if CONF.max_concurrent_snapshots > 0:
557             self._snapshot_semaphore = eventlet.semaphore.Semaphore(
558                 CONF.max_concurrent_snapshots)
559         else:
560             self._snapshot_semaphore = compute_utils.UnlimitedSemaphore()
561         if CONF.max_concurrent_live_migrations > 0:
562             self._live_migration_executor = futurist.GreenThreadPoolExecutor(
563                 max_workers=CONF.max_concurrent_live_migrations)
564         else:
565             # CONF.max_concurrent_live_migrations is 0 (unlimited)
566             self._live_migration_executor = futurist.GreenThreadPoolExecutor()
567         # This is a dict, keyed by instance uuid, to a two-item tuple of
568         # migration object and Future for the queued live migration.
569         self._waiting_live_migrations = {}
570 
571         super(ComputeManager, self).__init__(service_name="compute",
572                                              *args, **kwargs)
573 
574         # TODO(sbauza): Remove this call once we delete the V5Proxy class
575         self.additional_endpoints.append(_ComputeV5Proxy(self))
576 
577         # NOTE(russellb) Load the driver last.  It may call back into the
578         # compute manager via the virtapi, so we want it to be fully
579         # initialized before that happens.
580         self.driver = driver.load_compute_driver(self.virtapi, compute_driver)
581         self.use_legacy_block_device_info = \
582                             self.driver.need_legacy_block_device_info
583         self.rt = resource_tracker.ResourceTracker(
584             self.host, self.driver, reportclient=self.reportclient)
585 
586     def reset(self):
587         LOG.info('Reloading compute RPC API')
588         compute_rpcapi.reset_globals()
589         self.compute_rpcapi = compute_rpcapi.ComputeAPI()
590         self.reportclient.clear_provider_cache()
591 
592     def _update_resource_tracker(self, context, instance):
593         """Let the resource tracker know that an instance has changed state."""
594 
595         if instance.host == self.host:
596             self.rt.update_usage(context, instance, instance.node)
597 
598     def _instance_update(self, context, instance, **kwargs):
599         """Update an instance in the database using kwargs as value."""
600 
601         for k, v in kwargs.items():
602             setattr(instance, k, v)
603         instance.save()
604         self._update_resource_tracker(context, instance)
605 
606     def _nil_out_instance_obj_host_and_node(self, instance):
607         # NOTE(jwcroppe): We don't do instance.save() here for performance
608         # reasons; a call to this is expected to be immediately followed by
609         # another call that does instance.save(), thus avoiding two writes
610         # to the database layer.
611         instance.host = None
612         instance.node = None
613         # ResourceTracker._set_instance_host_and_node also sets launched_on
614         # to the same value as host and is really only ever used by legacy
615         # nova-network code, but we should also null it out to avoid confusion
616         # if there is an instance in the database with no host set but
617         # launched_on is set. Note that we do not care about using launched_on
618         # as some kind of debug helper if diagnosing a build failure, that is
619         # what instance action events are for.
620         instance.launched_on = None
621         # If the instance is not on a host, it's not in an aggregate and
622         # therefore is not in an availability zone.
623         instance.availability_zone = None
624 
625     def _set_instance_obj_error_state(self, instance, clean_task_state=False):
626         try:
627             instance.vm_state = vm_states.ERROR
628             if clean_task_state:
629                 instance.task_state = None
630             instance.save()
631         except exception.InstanceNotFound:
632             LOG.debug('Instance has been destroyed from under us while '
633                       'trying to set it to ERROR', instance=instance)
634 
635     def _get_instances_on_driver(self, context, filters=None):
636         """Return a list of instance records for the instances found
637         on the hypervisor which satisfy the specified filters. If filters=None
638         return a list of instance records for all the instances found on the
639         hypervisor.
640         """
641         if not filters:
642             filters = {}
643         try:
644             driver_uuids = self.driver.list_instance_uuids()
645             if len(driver_uuids) == 0:
646                 # Short circuit, don't waste a DB call
647                 return objects.InstanceList()
648             filters['uuid'] = driver_uuids
649             local_instances = objects.InstanceList.get_by_filters(
650                 context, filters, use_slave=True)
651             return local_instances
652         except NotImplementedError:
653             pass
654 
655         # The driver doesn't support uuids listing, so we'll have
656         # to brute force.
657         driver_instances = self.driver.list_instances()
658         # NOTE(mjozefcz): In this case we need to apply host filter.
659         # Without this all instance data would be fetched from db.
660         filters['host'] = self.host
661         instances = objects.InstanceList.get_by_filters(context, filters,
662                                                         use_slave=True)
663         name_map = {instance.name: instance for instance in instances}
664         local_instances = []
665         for driver_instance in driver_instances:
666             instance = name_map.get(driver_instance)
667             if not instance:
668                 continue
669             local_instances.append(instance)
670         return local_instances
671 
672     def _destroy_evacuated_instances(self, context, node_cache):
673         """Destroys evacuated instances.
674 
675         While nova-compute was down, the instances running on it could be
676         evacuated to another host. This method looks for evacuation migration
677         records where this is the source host and which were either started
678         (accepted), in-progress (pre-migrating) or migrated (done). From those
679         migration records, local instances reported by the hypervisor are
680         compared to the instances for the migration records and those local
681         guests are destroyed, along with instance allocation records in
682         Placement for this node.
683         Then allocations are removed from Placement for every instance that is
684         evacuated from this host regardless if the instance is reported by the
685         hypervisor or not.
686 
687         :param context: The request context
688         :param node_cache: A dict of ComputeNode objects keyed by the UUID of
689             the compute node
690         :return: A dict keyed by instance uuid mapped to Migration objects
691             for instances that were migrated away from this host
692         """
693         filters = {
694             'source_compute': self.host,
695             # NOTE(mriedem): Migration records that have been accepted are
696             # included in case the source node comes back up while instances
697             # are being evacuated to another host. We don't want the same
698             # instance being reported from multiple hosts.
699             # NOTE(lyarwood): pre-migrating is also included here as the
700             # source compute can come back online shortly after the RT
701             # claims on the destination that in-turn moves the migration to
702             # pre-migrating. If the evacuate fails on the destination host,
703             # the user can rebuild the instance (in ERROR state) on the source
704             # host.
705             'status': ['accepted', 'pre-migrating', 'done'],
706             'migration_type': fields.MigrationType.EVACUATION,
707         }
708         with utils.temporary_mutation(context, read_deleted='yes'):
709             evacuations = objects.MigrationList.get_by_filters(context,
710                                                                filters)
711         if not evacuations:
712             return {}
713         evacuations = {mig.instance_uuid: mig for mig in evacuations}
714 
715         # TODO(mriedem): We could optimize by pre-loading the joined fields
716         # we know we'll use, like info_cache and flavor.
717         local_instances = self._get_instances_on_driver(context)
718         evacuated_local_instances = {inst.uuid: inst
719                                      for inst in local_instances
720                                      if inst.uuid in evacuations}
721 
722         for instance in evacuated_local_instances.values():
723             LOG.info('Destroying instance as it has been evacuated from '
724                      'this host but still exists in the hypervisor',
725                      instance=instance)
726             try:
727                 network_info = self.network_api.get_instance_nw_info(
728                     context, instance)
729                 bdi = self._get_instance_block_device_info(context,
730                                                            instance)
731                 destroy_disks = not (self._is_instance_storage_shared(
732                     context, instance))
733             except exception.InstanceNotFound:
734                 network_info = network_model.NetworkInfo()
735                 bdi = {}
736                 LOG.info('Instance has been marked deleted already, '
737                          'removing it from the hypervisor.',
738                          instance=instance)
739                 # always destroy disks if the instance was deleted
740                 destroy_disks = True
741             self.driver.destroy(context, instance,
742                                 network_info,
743                                 bdi, destroy_disks)
744 
745         hostname_to_cn_uuid = {
746             cn.hypervisor_hostname: cn.uuid
747             for cn in node_cache.values()}
748 
749         for instance_uuid, migration in evacuations.items():
750             try:
751                 if instance_uuid in evacuated_local_instances:
752                     # Avoid the db call if we already have the instance loaded
753                     # above
754                     instance = evacuated_local_instances[instance_uuid]
755                 else:
756                     instance = objects.Instance.get_by_uuid(
757                         context, instance_uuid)
758             except exception.InstanceNotFound:
759                 # The instance already deleted so we expect that every
760                 # allocation of that instance has already been cleaned up
761                 continue
762 
763             LOG.info('Cleaning up allocations of the instance as it has been '
764                      'evacuated from this host',
765                      instance=instance)
766             if migration.source_node not in hostname_to_cn_uuid:
767                 LOG.error("Failed to clean allocation of evacuated "
768                           "instance as the source node %s is not found",
769                           migration.source_node, instance=instance)
770                 continue
771             cn_uuid = hostname_to_cn_uuid[migration.source_node]
772 
773             # If the instance was deleted in the interim, assume its
774             # allocations were properly cleaned up (either by its hosting
775             # compute service or the API).
776             if (not instance.deleted and
777                     not self.reportclient.
778                         remove_provider_tree_from_instance_allocation(
779                             context, instance.uuid, cn_uuid)):
780                 LOG.error("Failed to clean allocation of evacuated instance "
781                           "on the source node %s",
782                           cn_uuid, instance=instance)
783 
784             migration.status = 'completed'
785             migration.save()
786         return evacuations
787 
788     def _is_instance_storage_shared(self, context, instance, host=None):
789         shared_storage = True
790         data = None
791         try:
792             data = self.driver.check_instance_shared_storage_local(context,
793                                                        instance)
794             if data:
795                 shared_storage = (self.compute_rpcapi.
796                                   check_instance_shared_storage(context,
797                                   data, instance=instance, host=host))
798         except NotImplementedError:
799             LOG.debug('Hypervisor driver does not support '
800                       'instance shared storage check, '
801                       'assuming it\'s not on shared storage',
802                       instance=instance)
803             shared_storage = False
804         except Exception:
805             LOG.exception('Failed to check if instance shared',
806                           instance=instance)
807         finally:
808             if data:
809                 self.driver.check_instance_shared_storage_cleanup(context,
810                                                                   data)
811         return shared_storage
812 
813     def _complete_partial_deletion(self, context, instance):
814         """Complete deletion for instances in DELETED status but not marked as
815         deleted in the DB
816         """
817         instance.destroy()
818         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
819                 context, instance.uuid)
820         self._complete_deletion(context,
821                                 instance)
822         self._notify_about_instance_usage(context, instance, "delete.end")
823         compute_utils.notify_about_instance_action(context, instance,
824                 self.host, action=fields.NotificationAction.DELETE,
825                 phase=fields.NotificationPhase.END, bdms=bdms)
826 
827     def _complete_deletion(self, context, instance):
828         self._update_resource_tracker(context, instance)
829 
830         self.reportclient.delete_allocation_for_instance(context,
831                                                          instance.uuid)
832 
833         self._clean_instance_console_tokens(context, instance)
834         self._delete_scheduler_instance_info(context, instance.uuid)
835 
836     def _validate_pinning_configuration(self, instances):
837         if not self.driver.capabilities.get('supports_pcpus', False):
838             return
839 
840         for instance in instances:
841             # ignore deleted instances
842             if instance.deleted:
843                 continue
844 
845             # if this is an unpinned instance and the host only has
846             # 'cpu_dedicated_set' configured, we need to tell the operator to
847             # correct their configuration
848             if not instance.numa_topology or (
849                 instance.numa_topology.cpu_policy in (
850                     None, fields.CPUAllocationPolicy.SHARED
851                 )
852             ):
853                 # we don't need to check 'vcpu_pin_set' since it can't coexist
854                 # alongside 'cpu_dedicated_set'
855                 if (CONF.compute.cpu_dedicated_set and
856                         not CONF.compute.cpu_shared_set):
857                     msg = _("This host has unpinned instances but has no CPUs "
858                             "set aside for this purpose; configure '[compute] "
859                             "cpu_shared_set' instead of, or in addition to, "
860                             "'[compute] cpu_dedicated_set'")
861                     raise exception.InvalidConfiguration(msg)
862 
863                 continue
864 
865             # ditto for pinned instances if only 'cpu_shared_set' is configured
866             if (CONF.compute.cpu_shared_set and
867                     not CONF.compute.cpu_dedicated_set and
868                     not CONF.vcpu_pin_set):
869                 msg = _("This host has pinned instances but has no CPUs "
870                         "set aside for this purpose; configure '[compute] "
871                         "cpu_dedicated_set' instead of, or in addition to, "
872                         "'[compute] cpu_shared_set'.")
873                 raise exception.InvalidConfiguration(msg)
874 
875             # if this is a mixed instance with both pinned and unpinned CPUs,
876             # the host must have both 'cpu_dedicated_set' and 'cpu_shared_set'
877             # configured. check if 'cpu_shared_set' is set.
878             if (instance.numa_topology.cpu_policy ==
879                     fields.CPUAllocationPolicy.MIXED and
880                     not CONF.compute.cpu_shared_set):
881                 msg = _("This host has mixed instance requesting both pinned "
882                         "and unpinned CPUs but hasn't set aside unpinned CPUs "
883                         "for this purpose; Configure "
884                         "'[compute] cpu_shared_set'.")
885                 raise exception.InvalidConfiguration(msg)
886 
887             # for mixed instance check if 'cpu_dedicated_set' is set.
888             if (instance.numa_topology.cpu_policy ==
889                     fields.CPUAllocationPolicy.MIXED and
890                     not CONF.compute.cpu_dedicated_set):
891                 msg = _("This host has mixed instance requesting both pinned "
892                         "and unpinned CPUs but hasn't set aside pinned CPUs "
893                         "for this purpose; Configure "
894                         "'[compute] cpu_dedicated_set'")
895                 raise exception.InvalidConfiguration(msg)
896 
897             # also check to make sure the operator hasn't accidentally
898             # dropped some cores that instances are currently using
899             available_dedicated_cpus = (hardware.get_vcpu_pin_set() or
900                                         hardware.get_cpu_dedicated_set())
901             pinned_cpus = instance.numa_topology.cpu_pinning
902             if available_dedicated_cpus and (
903                     pinned_cpus - available_dedicated_cpus):
904                 # we can't raise an exception because of bug #1289064,
905                 # which meant we didn't recalculate CPU pinning information
906                 # when we live migrated a pinned instance
907                 LOG.warning(
908                     "Instance is pinned to host CPUs %(cpus)s "
909                     "but one or more of these CPUs are not included in "
910                     "either '[compute] cpu_dedicated_set' or "
911                     "'vcpu_pin_set'; you should update these "
912                     "configuration options to include the missing CPUs "
913                     "or rebuild or cold migrate this instance.",
914                     {'cpus': list(pinned_cpus)},
915                     instance=instance)
916 
917     def _validate_vtpm_configuration(self, instances):
918         if self.driver.capabilities.get('supports_vtpm', False):
919             return
920 
921         for instance in instances:
922             if instance.deleted:
923                 continue
924 
925             # NOTE(stephenfin): We don't have an attribute on the instance to
926             # check for this, so we need to inspect the flavor/image metadata
927             if hardware.get_vtpm_constraint(
928                 instance.flavor, instance.image_meta,
929             ):
930                 msg = _(
931                     'This host has instances with the vTPM feature enabled, '
932                     'but the host is not correctly configured; enable '
933                     'vTPM support.'
934                 )
935                 raise exception.InvalidConfiguration(msg)
936 
937     def _reset_live_migration(self, context, instance):
938         migration = None
939         try:
940             migration = objects.Migration.get_by_instance_and_status(
941                                       context, instance.uuid, 'running')
942             if migration:
943                 self.live_migration_abort(context, instance, migration.id)
944         except Exception:
945             LOG.exception('Failed to abort live-migration',
946                           instance=instance)
947         finally:
948             if migration:
949                 self._set_migration_status(migration, 'error')
950             LOG.info('Instance found in migrating state during '
951                      'startup. Resetting task_state',
952                      instance=instance)
953             instance.task_state = None
954             instance.save(expected_task_state=[task_states.MIGRATING])
955 
956     def _init_instance(self, context, instance):
957         """Initialize this instance during service init."""
958 
959         # NOTE(danms): If the instance appears to not be owned by this
960         # host, it may have been evacuated away, but skipped by the
961         # evacuation cleanup code due to configuration. Thus, if that
962         # is a possibility, don't touch the instance in any way, but
963         # log the concern. This will help avoid potential issues on
964         # startup due to misconfiguration.
965         if instance.host != self.host:
966             LOG.warning('Instance %(uuid)s appears to not be owned '
967                         'by this host, but by %(host)s. Startup '
968                         'processing is being skipped.',
969                         {'uuid': instance.uuid,
970                          'host': instance.host})
971             return
972 
973         # Instances that are shut down, or in an error state can not be
974         # initialized and are not attempted to be recovered. The exception
975         # to this are instances that are in RESIZE_MIGRATING or DELETING,
976         # which are dealt with further down.
977         if (instance.vm_state == vm_states.SOFT_DELETED or
978             (instance.vm_state == vm_states.ERROR and
979             instance.task_state not in
980             (task_states.RESIZE_MIGRATING, task_states.DELETING))):
981             LOG.debug("Instance is in %s state.",
982                       instance.vm_state, instance=instance)
983             return
984 
985         if instance.vm_state == vm_states.DELETED:
986             try:
987                 self._complete_partial_deletion(context, instance)
988             except Exception:
989                 # we don't want that an exception blocks the init_host
990                 LOG.exception('Failed to complete a deletion',
991                               instance=instance)
992             return
993 
994         if (instance.vm_state == vm_states.BUILDING or
995             instance.task_state in [task_states.SCHEDULING,
996                                     task_states.BLOCK_DEVICE_MAPPING,
997                                     task_states.NETWORKING,
998                                     task_states.SPAWNING]):
999             # NOTE(dave-mcnally) compute stopped before instance was fully
1000             # spawned so set to ERROR state. This is safe to do as the state
1001             # may be set by the api but the host is not so if we get here the
1002             # instance has already been scheduled to this particular host.
1003             LOG.debug("Instance failed to spawn correctly, "
1004                       "setting to ERROR state", instance=instance)
1005             self._set_instance_obj_error_state(instance, clean_task_state=True)
1006             return
1007 
1008         if (instance.vm_state in [vm_states.ACTIVE, vm_states.STOPPED] and
1009             instance.task_state in [task_states.REBUILDING,
1010                                     task_states.REBUILD_BLOCK_DEVICE_MAPPING,
1011                                     task_states.REBUILD_SPAWNING]):
1012             # NOTE(jichenjc) compute stopped before instance was fully
1013             # spawned so set to ERROR state. This is consistent to BUILD
1014             LOG.debug("Instance failed to rebuild correctly, "
1015                       "setting to ERROR state", instance=instance)
1016             self._set_instance_obj_error_state(instance, clean_task_state=True)
1017             return
1018 
1019         if (instance.vm_state != vm_states.ERROR and
1020             instance.task_state in [task_states.IMAGE_SNAPSHOT_PENDING,
1021                                     task_states.IMAGE_PENDING_UPLOAD,
1022                                     task_states.IMAGE_UPLOADING,
1023                                     task_states.IMAGE_SNAPSHOT]):
1024             LOG.debug("Instance in transitional state %s at start-up "
1025                       "clearing task state",
1026                       instance.task_state, instance=instance)
1027             instance.task_state = None
1028             instance.save()
1029 
1030         if (instance.vm_state != vm_states.ERROR and
1031             instance.task_state in [task_states.RESIZE_PREP]):
1032             LOG.debug("Instance in transitional state %s at start-up "
1033                       "clearing task state",
1034                       instance['task_state'], instance=instance)
1035             instance.task_state = None
1036             instance.save()
1037 
1038         if instance.task_state == task_states.DELETING:
1039             try:
1040                 LOG.info('Service started deleting the instance during '
1041                          'the previous run, but did not finish. Restarting'
1042                          ' the deletion now.', instance=instance)
1043                 instance.obj_load_attr('metadata')
1044                 instance.obj_load_attr('system_metadata')
1045                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
1046                         context, instance.uuid)
1047                 self._delete_instance(context, instance, bdms)
1048             except Exception:
1049                 # we don't want that an exception blocks the init_host
1050                 LOG.exception('Failed to complete a deletion',
1051                               instance=instance)
1052                 self._set_instance_obj_error_state(instance)
1053             return
1054 
1055         current_power_state = self._get_power_state(instance)
1056         try_reboot, reboot_type = self._retry_reboot(
1057             instance, current_power_state)
1058 
1059         if try_reboot:
1060             LOG.debug("Instance in transitional state (%(task_state)s) at "
1061                       "start-up and power state is (%(power_state)s), "
1062                       "triggering reboot",
1063                       {'task_state': instance.task_state,
1064                        'power_state': current_power_state},
1065                       instance=instance)
1066 
1067             # NOTE(mikal): if the instance was doing a soft reboot that got as
1068             # far as shutting down the instance but not as far as starting it
1069             # again, then we've just become a hard reboot. That means the
1070             # task state for the instance needs to change so that we're in one
1071             # of the expected task states for a hard reboot.
1072             if (instance.task_state in task_states.soft_reboot_states and
1073                 reboot_type == 'HARD'):
1074                 instance.task_state = task_states.REBOOT_PENDING_HARD
1075                 instance.save()
1076 
1077             self.reboot_instance(context, instance, block_device_info=None,
1078                                  reboot_type=reboot_type)
1079             return
1080 
1081         elif (current_power_state == power_state.RUNNING and
1082               instance.task_state in [task_states.REBOOT_STARTED,
1083                                       task_states.REBOOT_STARTED_HARD,
1084                                       task_states.PAUSING,
1085                                       task_states.UNPAUSING]):
1086             LOG.warning("Instance in transitional state "
1087                         "(%(task_state)s) at start-up and power state "
1088                         "is (%(power_state)s), clearing task state",
1089                         {'task_state': instance.task_state,
1090                          'power_state': current_power_state},
1091                         instance=instance)
1092             instance.task_state = None
1093             instance.vm_state = vm_states.ACTIVE
1094             instance.save()
1095         elif (current_power_state == power_state.PAUSED and
1096               instance.task_state == task_states.UNPAUSING):
1097             LOG.warning("Instance in transitional state "
1098                         "(%(task_state)s) at start-up and power state "
1099                         "is (%(power_state)s), clearing task state "
1100                         "and unpausing the instance",
1101                         {'task_state': instance.task_state,
1102                          'power_state': current_power_state},
1103                         instance=instance)
1104             try:
1105                 self.unpause_instance(context, instance)
1106             except NotImplementedError:
1107                 # Some virt driver didn't support pause and unpause
1108                 pass
1109             except Exception:
1110                 LOG.exception('Failed to unpause instance', instance=instance)
1111             return
1112 
1113         if instance.task_state == task_states.POWERING_OFF:
1114             try:
1115                 LOG.debug("Instance in transitional state %s at start-up "
1116                           "retrying stop request",
1117                           instance.task_state, instance=instance)
1118                 self.stop_instance(context, instance, True)
1119             except Exception:
1120                 # we don't want that an exception blocks the init_host
1121                 LOG.exception('Failed to stop instance', instance=instance)
1122             return
1123 
1124         if instance.task_state == task_states.POWERING_ON:
1125             try:
1126                 LOG.debug("Instance in transitional state %s at start-up "
1127                           "retrying start request",
1128                           instance.task_state, instance=instance)
1129                 self.start_instance(context, instance)
1130             except Exception:
1131                 # we don't want that an exception blocks the init_host
1132                 LOG.exception('Failed to start instance', instance=instance)
1133             return
1134 
1135         net_info = instance.get_network_info()
1136         try:
1137             self.driver.plug_vifs(instance, net_info)
1138         except NotImplementedError as e:
1139             LOG.debug(e, instance=instance)
1140         except exception.VirtualInterfacePlugException:
1141             # NOTE(mriedem): If we get here, it could be because the vif_type
1142             # in the cache is "binding_failed" or "unbound".
1143             # The periodic task _heal_instance_info_cache checks for this
1144             # condition. It should fix this by binding the ports again when
1145             # it gets to this instance.
1146             LOG.exception('Virtual interface plugging failed for instance. '
1147                           'The port binding:host_id may need to be manually '
1148                           'updated.', instance=instance)
1149             self._set_instance_obj_error_state(instance)
1150             return
1151 
1152         if instance.task_state == task_states.RESIZE_MIGRATING:
1153             # We crashed during resize/migration, so roll back for safety
1154             try:
1155                 # NOTE(mriedem): check old_vm_state for STOPPED here, if it's
1156                 # not in system_metadata we default to True for backwards
1157                 # compatibility
1158                 power_on = (instance.system_metadata.get('old_vm_state') !=
1159                             vm_states.STOPPED)
1160 
1161                 block_dev_info = self._get_instance_block_device_info(context,
1162                                                                       instance)
1163 
1164                 migration = objects.Migration.get_by_id_and_instance(
1165                     context, instance.migration_context.migration_id,
1166                     instance.uuid)
1167                 self.driver.finish_revert_migration(context, instance,
1168                     net_info, migration, block_dev_info, power_on)
1169 
1170             except Exception:
1171                 LOG.exception('Failed to revert crashed migration',
1172                               instance=instance)
1173             finally:
1174                 LOG.info('Instance found in migrating state during '
1175                          'startup. Resetting task_state',
1176                          instance=instance)
1177                 instance.task_state = None
1178                 instance.save()
1179         if instance.task_state == task_states.MIGRATING:
1180             # Live migration did not complete, but instance is on this
1181             # host. Abort ongoing migration if still running and reset state.
1182             self._reset_live_migration(context, instance)
1183 
1184         db_state = instance.power_state
1185         drv_state = self._get_power_state(instance)
1186         expect_running = (db_state == power_state.RUNNING and
1187                           drv_state != db_state)
1188 
1189         LOG.debug('Current state is %(drv_state)s, state in DB is '
1190                   '%(db_state)s.',
1191                   {'drv_state': drv_state, 'db_state': db_state},
1192                   instance=instance)
1193 
1194         if expect_running and CONF.resume_guests_state_on_host_boot:
1195             self._resume_guests_state(context, instance, net_info)
1196 
1197     def _resume_guests_state(self, context, instance, net_info):
1198         LOG.info('Rebooting instance after nova-compute restart.',
1199                  instance=instance)
1200         block_device_info = \
1201             self._get_instance_block_device_info(context, instance)
1202 
1203         try:
1204             self.driver.resume_state_on_host_boot(
1205                 context, instance, net_info, block_device_info)
1206         except NotImplementedError:
1207             LOG.warning('Hypervisor driver does not support '
1208                         'resume guests', instance=instance)
1209         except Exception:
1210             # NOTE(vish): The instance failed to resume, so we set the
1211             #             instance to error and attempt to continue.
1212             LOG.warning('Failed to resume instance',
1213                         instance=instance)
1214             self._set_instance_obj_error_state(instance)
1215 
1216     def _retry_reboot(self, instance, current_power_state):
1217         current_task_state = instance.task_state
1218         retry_reboot = False
1219         reboot_type = compute_utils.get_reboot_type(current_task_state,
1220                                                     current_power_state)
1221 
1222         pending_soft = (
1223             current_task_state == task_states.REBOOT_PENDING and
1224             instance.vm_state in vm_states.ALLOW_SOFT_REBOOT)
1225         pending_hard = (
1226             current_task_state == task_states.REBOOT_PENDING_HARD and
1227             instance.vm_state in vm_states.ALLOW_HARD_REBOOT)
1228         started_not_running = (current_task_state in
1229                                [task_states.REBOOT_STARTED,
1230                                 task_states.REBOOT_STARTED_HARD] and
1231                                current_power_state != power_state.RUNNING)
1232 
1233         if pending_soft or pending_hard or started_not_running:
1234             retry_reboot = True
1235 
1236         return retry_reboot, reboot_type
1237 
1238     def handle_lifecycle_event(self, event):
1239         LOG.info("VM %(state)s (Lifecycle Event)",
1240                  {'state': event.get_name()},
1241                  instance_uuid=event.get_instance_uuid())
1242         context = nova.context.get_admin_context(read_deleted='yes')
1243         vm_power_state = None
1244         event_transition = event.get_transition()
1245         if event_transition == virtevent.EVENT_LIFECYCLE_STOPPED:
1246             vm_power_state = power_state.SHUTDOWN
1247         elif event_transition == virtevent.EVENT_LIFECYCLE_STARTED:
1248             vm_power_state = power_state.RUNNING
1249         elif event_transition in (
1250                 virtevent.EVENT_LIFECYCLE_PAUSED,
1251                 virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED,
1252                 virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED):
1253             vm_power_state = power_state.PAUSED
1254         elif event_transition == virtevent.EVENT_LIFECYCLE_RESUMED:
1255             vm_power_state = power_state.RUNNING
1256         elif event_transition == virtevent.EVENT_LIFECYCLE_SUSPENDED:
1257             vm_power_state = power_state.SUSPENDED
1258         else:
1259             LOG.warning("Unexpected lifecycle event: %d", event_transition)
1260 
1261         migrate_finish_statuses = {
1262             # This happens on the source node and indicates live migration
1263             # entered post-copy mode.
1264             virtevent.EVENT_LIFECYCLE_POSTCOPY_STARTED: 'running (post-copy)',
1265             # Suspended for offline migration.
1266             virtevent.EVENT_LIFECYCLE_MIGRATION_COMPLETED: 'running'
1267         }
1268 
1269         expected_attrs = []
1270         if event_transition in migrate_finish_statuses:
1271             # Join on info_cache since that's needed in migrate_instance_start.
1272             expected_attrs.append('info_cache')
1273         instance = objects.Instance.get_by_uuid(context,
1274                                                 event.get_instance_uuid(),
1275                                                 expected_attrs=expected_attrs)
1276 
1277         # Note(lpetrut): The event may be delayed, thus not reflecting
1278         # the current instance power state. In that case, ignore the event.
1279         current_power_state = self._get_power_state(instance)
1280         if current_power_state == vm_power_state:
1281             LOG.debug('Synchronizing instance power state after lifecycle '
1282                       'event "%(event)s"; current vm_state: %(vm_state)s, '
1283                       'current task_state: %(task_state)s, current DB '
1284                       'power_state: %(db_power_state)s, VM power_state: '
1285                       '%(vm_power_state)s',
1286                       {'event': event.get_name(),
1287                        'vm_state': instance.vm_state,
1288                        'task_state': instance.task_state,
1289                        'db_power_state': instance.power_state,
1290                        'vm_power_state': vm_power_state},
1291                       instance_uuid=instance.uuid)
1292             self._sync_instance_power_state(context,
1293                                             instance,
1294                                             vm_power_state)
1295 
1296         # The following checks are for live migration. We want to activate
1297         # the port binding for the destination host before the live migration
1298         # is resumed on the destination host in order to reduce network
1299         # downtime. Otherwise the ports are bound to the destination host
1300         # in post_live_migration_at_destination.
1301         # TODO(danms): Explore options for using a different live migration
1302         # specific callback for this instead of piggy-backing on the
1303         # handle_lifecycle_event callback.
1304         if (instance.task_state == task_states.MIGRATING and
1305                 event_transition in migrate_finish_statuses):
1306             status = migrate_finish_statuses[event_transition]
1307             try:
1308                 migration = objects.Migration.get_by_instance_and_status(
1309                             context, instance.uuid, status)
1310                 LOG.debug('Binding ports to destination host: %s',
1311                           migration.dest_compute, instance=instance)
1312                 # For neutron, migrate_instance_start will activate the
1313                 # destination host port bindings, if there are any created by
1314                 # conductor before live migration started.
1315                 self.network_api.migrate_instance_start(
1316                     context, instance, migration)
1317             except exception.MigrationNotFoundByStatus:
1318                 LOG.warning("Unable to find migration record with status "
1319                             "'%s' for instance. Port binding will happen in "
1320                             "post live migration.", status, instance=instance)
1321 
1322     def handle_events(self, event):
1323         if isinstance(event, virtevent.LifecycleEvent):
1324             try:
1325                 self.handle_lifecycle_event(event)
1326             except exception.InstanceNotFound:
1327                 LOG.debug("Event %s arrived for non-existent instance. The "
1328                           "instance was probably deleted.", event)
1329         else:
1330             LOG.debug("Ignoring event %s", event)
1331 
1332     def init_virt_events(self):
1333         if CONF.workarounds.handle_virt_lifecycle_events:
1334             self.driver.register_event_listener(self.handle_events)
1335         else:
1336             # NOTE(mriedem): If the _sync_power_states periodic task is
1337             # disabled we should emit a warning in the logs.
1338             if CONF.sync_power_state_interval < 0:
1339                 LOG.warning('Instance lifecycle events from the compute '
1340                             'driver have been disabled. Note that lifecycle '
1341                             'changes to an instance outside of the compute '
1342                             'service will not be synchronized '
1343                             'automatically since the _sync_power_states '
1344                             'periodic task is also disabled.')
1345             else:
1346                 LOG.info('Instance lifecycle events from the compute '
1347                          'driver have been disabled. Note that lifecycle '
1348                          'changes to an instance outside of the compute '
1349                          'service will only be synchronized by the '
1350                          '_sync_power_states periodic task.')
1351 
1352     def _get_nodes(self, context):
1353         """Queried the ComputeNode objects from the DB that are reported by the
1354         hypervisor.
1355 
1356         :param context: the request context
1357         :return: a dict of ComputeNode objects keyed by the UUID of the given
1358             node.
1359         """
1360         nodes_by_uuid = {}
1361         try:
1362             node_names = self.driver.get_available_nodes()
1363         except exception.VirtDriverNotReady:
1364             LOG.warning(
1365                 "Virt driver is not ready. If this is the first time this "
1366                 "service is starting on this host, then you can ignore this "
1367                 "warning.")
1368             return {}
1369 
1370         for node_name in node_names:
1371             try:
1372                 node = objects.ComputeNode.get_by_host_and_nodename(
1373                     context, self.host, node_name)
1374                 nodes_by_uuid[node.uuid] = node
1375             except exception.ComputeHostNotFound:
1376                 LOG.warning(
1377                     "Compute node %s not found in the database. If this is "
1378                     "the first time this service is starting on this host, "
1379                     "then you can ignore this warning.", node_name)
1380         return nodes_by_uuid
1381 
1382     def init_host(self):
1383         """Initialization for a standalone compute service."""
1384 
1385         if CONF.pci.passthrough_whitelist:
1386             # Simply loading the PCI passthrough whitelist will do a bunch of
1387             # validation that would otherwise wait until the PciDevTracker is
1388             # constructed when updating available resources for the compute
1389             # node(s) in the resource tracker, effectively killing that task.
1390             # So load up the whitelist when starting the compute service to
1391             # flush any invalid configuration early so we can kill the service
1392             # if the configuration is wrong.
1393             whitelist.Whitelist(CONF.pci.passthrough_whitelist)
1394 
1395         nova.conf.neutron.register_dynamic_opts(CONF)
1396         # Even if only libvirt uses them, make it available for all drivers
1397         nova.conf.devices.register_dynamic_opts(CONF)
1398 
1399         # Override the number of concurrent disk operations allowed if the
1400         # user has specified a limit.
1401         if CONF.compute.max_concurrent_disk_ops != 0:
1402             compute_utils.disk_ops_semaphore = \
1403                 eventlet.semaphore.BoundedSemaphore(
1404                     CONF.compute.max_concurrent_disk_ops)
1405 
1406         if CONF.compute.max_disk_devices_to_attach == 0:
1407             msg = _('[compute]max_disk_devices_to_attach has been set to 0, '
1408                     'which will prevent instances from being able to boot. '
1409                     'Set -1 for unlimited or set >= 1 to limit the maximum '
1410                     'number of disk devices.')
1411             raise exception.InvalidConfiguration(msg)
1412 
1413         self.driver.init_host(host=self.host)
1414         context = nova.context.get_admin_context()
1415         instances = objects.InstanceList.get_by_host(
1416             context, self.host,
1417             expected_attrs=['info_cache', 'metadata', 'numa_topology'])
1418 
1419         self.init_virt_events()
1420 
1421         self._validate_pinning_configuration(instances)
1422         self._validate_vtpm_configuration(instances)
1423 
1424         # NOTE(gibi): At this point the compute_nodes of the resource tracker
1425         # has not been populated yet so we cannot rely on the resource tracker
1426         # here.
1427         # NOTE(gibi): If ironic and vcenter virt driver slow start time
1428         # becomes problematic here then we should consider adding a config
1429         # option or a driver flag to tell us if we should thread
1430         # _destroy_evacuated_instances and
1431         # _error_out_instances_whose_build_was_interrupted out in the
1432         # background on startup
1433         nodes_by_uuid = self._get_nodes(context)
1434 
1435         try:
1436             # checking that instance was not already evacuated to other host
1437             evacuated_instances = self._destroy_evacuated_instances(
1438                 context, nodes_by_uuid)
1439 
1440             # Initialise instances on the host that are not evacuating
1441             for instance in instances:
1442                 if instance.uuid not in evacuated_instances:
1443                     self._init_instance(context, instance)
1444 
1445             # NOTE(gibi): collect all the instance uuids that is in some way
1446             # was already handled above. Either by init_instance or by
1447             # _destroy_evacuated_instances. This way we can limit the scope of
1448             # the _error_out_instances_whose_build_was_interrupted call to look
1449             # only for instances that have allocations on this node and not
1450             # handled by the above calls.
1451             already_handled = {instance.uuid for instance in instances}.union(
1452                 evacuated_instances)
1453             self._error_out_instances_whose_build_was_interrupted(
1454                 context, already_handled, nodes_by_uuid.keys())
1455 
1456         finally:
1457             if instances:
1458                 # We only send the instance info to the scheduler on startup
1459                 # if there is anything to send, otherwise this host might
1460                 # not be mapped yet in a cell and the scheduler may have
1461                 # issues dealing with the information. Later changes to
1462                 # instances on this host will update the scheduler, or the
1463                 # _sync_scheduler_instance_info periodic task will.
1464                 self._update_scheduler_instance_info(context, instances)
1465 
1466     def _error_out_instances_whose_build_was_interrupted(
1467             self, context, already_handled_instances, node_uuids):
1468         """If there are instances in BUILDING state that are not
1469         assigned to this host but have allocations in placement towards
1470         this compute that means the nova-compute service was
1471         restarted while those instances waited for the resource claim
1472         to finish and the _set_instance_host_and_node() to update the
1473         instance.host field. We need to push them to ERROR state here to
1474         prevent keeping them in BUILDING state forever.
1475 
1476         :param context: The request context
1477         :param already_handled_instances: The set of instance UUIDs that the
1478             host initialization process already handled in some way.
1479         :param node_uuids: The list of compute node uuids handled by this
1480             service
1481         """
1482 
1483         # Strategy:
1484         # 1) Get the allocations from placement for our compute node(s)
1485         # 2) Remove the already handled instances from the consumer list;
1486         #    they are either already initialized or need to be skipped.
1487         # 3) Check which remaining consumer is an instance in BUILDING state
1488         #    and push it to ERROR state.
1489 
1490         LOG.info(
1491             "Looking for unclaimed instances stuck in BUILDING status for "
1492             "nodes managed by this host")
1493         for cn_uuid in node_uuids:
1494             try:
1495                 f = self.reportclient.get_allocations_for_resource_provider
1496                 allocations = f(context, cn_uuid).allocations
1497             except (exception.ResourceProviderAllocationRetrievalFailed,
1498                     keystone_exception.ClientException) as e:
1499                 LOG.error(
1500                     "Could not retrieve compute node resource provider %s and "
1501                     "therefore unable to error out any instances stuck in "
1502                     "BUILDING state. Error: %s", cn_uuid, str(e))
1503                 continue
1504 
1505             not_handled_consumers = (set(allocations) -
1506                                      already_handled_instances)
1507 
1508             if not not_handled_consumers:
1509                 continue
1510 
1511             filters = {
1512                 'vm_state': vm_states.BUILDING,
1513                 'uuid': not_handled_consumers
1514             }
1515 
1516             instances = objects.InstanceList.get_by_filters(
1517                 context, filters, expected_attrs=[])
1518 
1519             for instance in instances:
1520                 LOG.debug(
1521                     "Instance spawn was interrupted before instance_claim, "
1522                     "setting instance to ERROR state", instance=instance)
1523                 self._set_instance_obj_error_state(
1524                     instance, clean_task_state=True)
1525 
1526     def cleanup_host(self):
1527         self.driver.register_event_listener(None)
1528         self.instance_events.cancel_all_events()
1529         self.driver.cleanup_host(host=self.host)
1530         self._cleanup_live_migrations_in_pool()
1531 
1532     def _cleanup_live_migrations_in_pool(self):
1533         # Shutdown the pool so we don't get new requests.
1534         self._live_migration_executor.shutdown(wait=False)
1535         # For any queued migrations, cancel the migration and update
1536         # its status.
1537         for migration, future in self._waiting_live_migrations.values():
1538             # If we got here before the Future was submitted then we need
1539             # to move on since there isn't anything we can do.
1540             if future is None:
1541                 continue
1542             if future.cancel():
1543                 self._set_migration_status(migration, 'cancelled')
1544                 LOG.info('Successfully cancelled queued live migration.',
1545                          instance_uuid=migration.instance_uuid)
1546             else:
1547                 LOG.warning('Unable to cancel live migration.',
1548                             instance_uuid=migration.instance_uuid)
1549         self._waiting_live_migrations.clear()
1550 
1551     def pre_start_hook(self):
1552         """After the service is initialized, but before we fully bring
1553         the service up by listening on RPC queues, make sure to update
1554         our available resources (and indirectly our available nodes).
1555         """
1556         self.update_available_resource(nova.context.get_admin_context(),
1557                                        startup=True)
1558 
1559     def _get_power_state(self, instance):
1560         """Retrieve the power state for the given instance."""
1561         LOG.debug('Checking state', instance=instance)
1562         try:
1563             return self.driver.get_info(instance, use_cache=False).state
1564         except exception.InstanceNotFound:
1565             return power_state.NOSTATE
1566 
1567     def _await_block_device_map_created(self, context, vol_id):
1568         # TODO(yamahata): creating volume simultaneously
1569         #                 reduces creation time?
1570         # TODO(yamahata): eliminate dumb polling
1571         start = time.time()
1572         retries = CONF.block_device_allocate_retries
1573         # (1) if the configured value is 0, one attempt should be made
1574         # (2) if the configured value is > 0, then the total number attempts
1575         #      is (retries + 1)
1576         attempts = 1
1577         if retries >= 1:
1578             attempts = retries + 1
1579         for attempt in range(1, attempts + 1):
1580             volume = self.volume_api.get(context, vol_id)
1581             volume_status = volume['status']
1582             if volume_status not in ['creating', 'downloading']:
1583                 if volume_status == 'available':
1584                     return attempt
1585                 LOG.warning("Volume id: %(vol_id)s finished being "
1586                             "created but its status is %(vol_status)s.",
1587                             {'vol_id': vol_id,
1588                              'vol_status': volume_status})
1589                 break
1590             greenthread.sleep(CONF.block_device_allocate_retries_interval)
1591         raise exception.VolumeNotCreated(volume_id=vol_id,
1592                                          seconds=int(time.time() - start),
1593                                          attempts=attempt,
1594                                          volume_status=volume_status)
1595 
1596     def _decode_files(self, injected_files):
1597         """Base64 decode the list of files to inject."""
1598         if not injected_files:
1599             return []
1600 
1601         def _decode(f):
1602             path, contents = f
1603             # Py3 raises binascii.Error instead of TypeError as in Py27
1604             try:
1605                 decoded = base64.b64decode(contents)
1606                 return path, decoded
1607             except (TypeError, binascii.Error):
1608                 raise exception.Base64Exception(path=path)
1609 
1610         return [_decode(f) for f in injected_files]
1611 
1612     def _validate_instance_group_policy(self, context, instance,
1613                                         scheduler_hints=None):
1614 
1615         if CONF.workarounds.disable_group_policy_check_upcall:
1616             return
1617 
1618         # NOTE(russellb) Instance group policy is enforced by the scheduler.
1619         # However, there is a race condition with the enforcement of
1620         # the policy.  Since more than one instance may be scheduled at the
1621         # same time, it's possible that more than one instance with an
1622         # anti-affinity policy may end up here.  It's also possible that
1623         # multiple instances with an affinity policy could end up on different
1624         # hosts.  This is a validation step to make sure that starting the
1625         # instance here doesn't violate the policy.
1626         if scheduler_hints is not None:
1627             # only go through here if scheduler_hints is provided, even if it
1628             # is empty.
1629             group_hint = scheduler_hints.get('group')
1630             if not group_hint:
1631                 return
1632             else:
1633                 # The RequestSpec stores scheduler_hints as key=list pairs so
1634                 # we need to check the type on the value and pull the single
1635                 # entry out. The API request schema validates that
1636                 # the 'group' hint is a single value.
1637                 if isinstance(group_hint, list):
1638                     group_hint = group_hint[0]
1639 
1640                 group = objects.InstanceGroup.get_by_hint(context, group_hint)
1641         else:
1642             # TODO(ganso): a call to DB can be saved by adding request_spec
1643             # to rpcapi payload of live_migration, pre_live_migration and
1644             # check_can_live_migrate_destination
1645             try:
1646                 group = objects.InstanceGroup.get_by_instance_uuid(
1647                     context, instance.uuid)
1648             except exception.InstanceGroupNotFound:
1649                 return
1650 
1651         @utils.synchronized(group['uuid'])
1652         def _do_validation(context, instance, group):
1653             if group.policy and 'anti-affinity' == group.policy:
1654 
1655                 # instances on host
1656                 instances_uuids = objects.InstanceList.get_uuids_by_host(
1657                     context, self.host)
1658                 ins_on_host = set(instances_uuids)
1659 
1660                 # instance param is just for logging, the nodename obtained is
1661                 # not actually related to the instance at all
1662                 nodename = self._get_nodename(instance)
1663 
1664                 # instances being migrated to host
1665                 migrations = (
1666                     objects.MigrationList.get_in_progress_by_host_and_node(
1667                         context, self.host, nodename))
1668                 migration_vm_uuids = set([mig['instance_uuid']
1669                                           for mig in migrations])
1670 
1671                 total_instances = migration_vm_uuids | ins_on_host
1672                 members = set(group.members)
1673                 # Determine the set of instance group members on this host
1674                 # which are not the instance in question. This is used to
1675                 # determine how many other members from the same anti-affinity
1676                 # group can be on this host.
1677                 members_on_host = (total_instances & members -
1678                                    set([instance.uuid]))
1679                 rules = group.rules
1680                 if rules and 'max_server_per_host' in rules:
1681                     max_server = rules['max_server_per_host']
1682                 else:
1683                     max_server = 1
1684                 if len(members_on_host) >= max_server:
1685                     msg = _("Anti-affinity instance group policy "
1686                             "was violated.")
1687                     raise exception.RescheduledException(
1688                             instance_uuid=instance.uuid,
1689                             reason=msg)
1690 
1691             # NOTE(ganso): The check for affinity below does not work and it
1692             # can easily be violated because the lock happens in different
1693             # compute hosts.
1694             # The only fix seems to be a DB lock to perform the check whenever
1695             # setting the host field to an instance.
1696             elif group.policy and 'affinity' == group.policy:
1697                 group_hosts = group.get_hosts(exclude=[instance.uuid])
1698                 if group_hosts and self.host not in group_hosts:
1699                     msg = _("Affinity instance group policy was violated.")
1700                     raise exception.RescheduledException(
1701                             instance_uuid=instance.uuid,
1702                             reason=msg)
1703 
1704         _do_validation(context, instance, group)
1705 
1706     def _log_original_error(self, exc_info, instance_uuid):
1707         LOG.error('Error: %s', exc_info[1], instance_uuid=instance_uuid,
1708                   exc_info=exc_info)
1709 
1710     @periodic_task.periodic_task
1711     def _check_instance_build_time(self, context):
1712         """Ensure that instances are not stuck in build."""
1713         timeout = CONF.instance_build_timeout
1714         if timeout == 0:
1715             return
1716 
1717         filters = {'vm_state': vm_states.BUILDING,
1718                    'host': self.host}
1719 
1720         building_insts = objects.InstanceList.get_by_filters(context,
1721                            filters, expected_attrs=[], use_slave=True)
1722 
1723         for instance in building_insts:
1724             if timeutils.is_older_than(instance.created_at, timeout):
1725                 self._set_instance_obj_error_state(instance)
1726                 LOG.warning("Instance build timed out. Set to error "
1727                             "state.", instance=instance)
1728 
1729     def _check_instance_exists(self, instance):
1730         """Ensure an instance with the same name is not already present."""
1731         if self.driver.instance_exists(instance):
1732             raise exception.InstanceExists(name=instance.name)
1733 
1734     def _allocate_network_async(self, context, instance, requested_networks,
1735                                 security_groups, resource_provider_mapping):
1736         """Method used to allocate networks in the background.
1737 
1738         Broken out for testing.
1739         """
1740         # First check to see if we're specifically not supposed to allocate
1741         # networks because if so, we can exit early.
1742         if requested_networks and requested_networks.no_allocate:
1743             LOG.debug("Not allocating networking since 'none' was specified.",
1744                       instance=instance)
1745             return network_model.NetworkInfo([])
1746 
1747         LOG.debug("Allocating IP information in the background.",
1748                   instance=instance)
1749         retries = CONF.network_allocate_retries
1750         attempts = retries + 1
1751         retry_time = 1
1752         bind_host_id = self.driver.network_binding_host_id(context, instance)
1753         for attempt in range(1, attempts + 1):
1754             try:
1755                 nwinfo = self.network_api.allocate_for_instance(
1756                         context, instance,
1757                         requested_networks=requested_networks,
1758                         security_groups=security_groups,
1759                         bind_host_id=bind_host_id,
1760                         resource_provider_mapping=resource_provider_mapping)
1761                 LOG.debug('Instance network_info: |%s|', nwinfo,
1762                           instance=instance)
1763                 instance.system_metadata['network_allocated'] = 'True'
1764                 # NOTE(JoshNang) do not save the instance here, as it can cause
1765                 # races. The caller shares a reference to instance and waits
1766                 # for this async greenthread to finish before calling
1767                 # instance.save().
1768                 return nwinfo
1769             except Exception as e:
1770                 log_info = {'attempt': attempt,
1771                             'attempts': attempts}
1772                 if attempt == attempts:
1773                     LOG.exception('Instance failed network setup '
1774                                   'after %(attempts)d attempt(s)',
1775                                   log_info)
1776                     raise e
1777                 LOG.warning('Instance failed network setup '
1778                             '(attempt %(attempt)d of %(attempts)d)',
1779                             log_info, instance=instance)
1780                 time.sleep(retry_time)
1781                 retry_time *= 2
1782                 if retry_time > 30:
1783                     retry_time = 30
1784         # Not reached.
1785 
1786     def _build_networks_for_instance(self, context, instance,
1787             requested_networks, security_groups, resource_provider_mapping):
1788 
1789         # If we're here from a reschedule the network may already be allocated.
1790         if strutils.bool_from_string(
1791                 instance.system_metadata.get('network_allocated', 'False')):
1792             # NOTE(alex_xu): The network_allocated is True means the network
1793             # resource already allocated at previous scheduling, and the
1794             # network setup is cleanup at previous. After rescheduling, the
1795             # network resource need setup on the new host.
1796             self.network_api.setup_instance_network_on_host(
1797                 context, instance, instance.host)
1798             return self.network_api.get_instance_nw_info(context, instance)
1799 
1800         network_info = self._allocate_network(context, instance,
1801                 requested_networks, security_groups,
1802                 resource_provider_mapping)
1803 
1804         return network_info
1805 
1806     def _allocate_network(self, context, instance, requested_networks,
1807                           security_groups, resource_provider_mapping):
1808         """Start network allocation asynchronously.  Return an instance
1809         of NetworkInfoAsyncWrapper that can be used to retrieve the
1810         allocated networks when the operation has finished.
1811         """
1812         # NOTE(comstud): Since we're allocating networks asynchronously,
1813         # this task state has little meaning, as we won't be in this
1814         # state for very long.
1815         instance.vm_state = vm_states.BUILDING
1816         instance.task_state = task_states.NETWORKING
1817         instance.save(expected_task_state=[None])
1818 
1819         return network_model.NetworkInfoAsyncWrapper(
1820                 self._allocate_network_async, context, instance,
1821                 requested_networks, security_groups, resource_provider_mapping)
1822 
1823     def _default_root_device_name(self, instance, image_meta, root_bdm):
1824         """Gets a default root device name from the driver.
1825 
1826         :param nova.objects.Instance instance:
1827             The instance for which to get the root device name.
1828         :param nova.objects.ImageMeta image_meta:
1829             The metadata of the image of the instance.
1830         :param nova.objects.BlockDeviceMapping root_bdm:
1831             The description of the root device.
1832         :returns: str -- The default root device name.
1833         :raises: InternalError, TooManyDiskDevices
1834         """
1835         try:
1836             return self.driver.default_root_device_name(instance,
1837                                                         image_meta,
1838                                                         root_bdm)
1839         except NotImplementedError:
1840             return compute_utils.get_next_device_name(instance, [])
1841 
1842     def _default_device_names_for_instance(self, instance,
1843                                            root_device_name,
1844                                            *block_device_lists):
1845         """Default the missing device names in the BDM from the driver.
1846 
1847         :param nova.objects.Instance instance:
1848             The instance for which to get default device names.
1849         :param str root_device_name: The root device name.
1850         :param list block_device_lists: List of block device mappings.
1851         :returns: None
1852         :raises: InternalError, TooManyDiskDevices
1853         """
1854         try:
1855             self.driver.default_device_names_for_instance(instance,
1856                                                           root_device_name,
1857                                                           *block_device_lists)
1858         except NotImplementedError:
1859             compute_utils.default_device_names_for_instance(
1860                 instance, root_device_name, *block_device_lists)
1861 
1862     def _get_device_name_for_instance(self, instance, bdms, block_device_obj):
1863         """Get the next device name from the driver, based on the BDM.
1864 
1865         :param nova.objects.Instance instance:
1866             The instance whose volume is requesting a device name.
1867         :param nova.objects.BlockDeviceMappingList bdms:
1868             The block device mappings for the instance.
1869         :param nova.objects.BlockDeviceMapping block_device_obj:
1870             A block device mapping containing info about the requested block
1871             device.
1872         :returns: The next device name.
1873         :raises: InternalError, TooManyDiskDevices
1874         """
1875         # NOTE(ndipanov): Copy obj to avoid changing the original
1876         block_device_obj = block_device_obj.obj_clone()
1877         try:
1878             return self.driver.get_device_name_for_instance(
1879                 instance, bdms, block_device_obj)
1880         except NotImplementedError:
1881             return compute_utils.get_device_name_for_instance(
1882                 instance, bdms, block_device_obj.get("device_name"))
1883 
1884     def _default_block_device_names(self, instance, image_meta, block_devices):
1885         """Verify that all the devices have the device_name set. If not,
1886         provide a default name.
1887 
1888         It also ensures that there is a root_device_name and is set to the
1889         first block device in the boot sequence (boot_index=0).
1890         """
1891         root_bdm = block_device.get_root_bdm(block_devices)
1892         if not root_bdm:
1893             return
1894 
1895         # Get the root_device_name from the root BDM or the instance
1896         root_device_name = None
1897         update_root_bdm = False
1898 
1899         if root_bdm.device_name:
1900             root_device_name = root_bdm.device_name
1901             instance.root_device_name = root_device_name
1902         elif instance.root_device_name:
1903             root_device_name = instance.root_device_name
1904             root_bdm.device_name = root_device_name
1905             update_root_bdm = True
1906         else:
1907             root_device_name = self._default_root_device_name(instance,
1908                                                               image_meta,
1909                                                               root_bdm)
1910 
1911             instance.root_device_name = root_device_name
1912             root_bdm.device_name = root_device_name
1913             update_root_bdm = True
1914 
1915         if update_root_bdm:
1916             root_bdm.save()
1917 
1918         ephemerals = []
1919         swap = []
1920         block_device_mapping = []
1921 
1922         for device in block_devices:
1923             if block_device.new_format_is_ephemeral(device):
1924                 ephemerals.append(device)
1925 
1926             if block_device.new_format_is_swap(device):
1927                 swap.append(device)
1928 
1929             if driver_block_device.is_block_device_mapping(device):
1930                 block_device_mapping.append(device)
1931 
1932         self._default_device_names_for_instance(instance,
1933                                                 root_device_name,
1934                                                 ephemerals,
1935                                                 swap,
1936                                                 block_device_mapping)
1937 
1938     def _block_device_info_to_legacy(self, block_device_info):
1939         """Convert BDI to the old format for drivers that need it."""
1940 
1941         if self.use_legacy_block_device_info:
1942             ephemerals = driver_block_device.legacy_block_devices(
1943                 driver.block_device_info_get_ephemerals(block_device_info))
1944             mapping = driver_block_device.legacy_block_devices(
1945                 driver.block_device_info_get_mapping(block_device_info))
1946             swap = block_device_info['swap']
1947             if swap:
1948                 swap = swap.legacy()
1949 
1950             block_device_info.update({
1951                 'ephemerals': ephemerals,
1952                 'swap': swap,
1953                 'block_device_mapping': mapping})
1954 
1955     def _add_missing_dev_names(self, bdms, instance):
1956         for bdm in bdms:
1957             if bdm.device_name is not None:
1958                 continue
1959 
1960             device_name = self._get_device_name_for_instance(instance,
1961                                                              bdms, bdm)
1962             values = {'device_name': device_name}
1963             bdm.update(values)
1964             bdm.save()
1965 
1966     def _prep_block_device(self, context, instance, bdms):
1967         """Set up the block device for an instance with error logging."""
1968         try:
1969             self._add_missing_dev_names(bdms, instance)
1970             block_device_info = driver.get_block_device_info(instance, bdms)
1971             mapping = driver.block_device_info_get_mapping(block_device_info)
1972             driver_block_device.attach_block_devices(
1973                 mapping, context, instance, self.volume_api, self.driver,
1974                 wait_func=self._await_block_device_map_created)
1975 
1976             self._block_device_info_to_legacy(block_device_info)
1977             return block_device_info
1978 
1979         except exception.OverQuota as e:
1980             LOG.warning('Failed to create block device for instance due'
1981                         ' to exceeding volume related resource quota.'
1982                         ' Error: %s', e.message, instance=instance)
1983             raise
1984 
1985         except Exception as ex:
1986             LOG.exception('Instance failed block device setup',
1987                           instance=instance)
1988             # InvalidBDM will eventually result in a BuildAbortException when
1989             # booting from volume, and will be recorded as an instance fault.
1990             # Maintain the original exception message which most likely has
1991             # useful details which the standard InvalidBDM error message lacks.
1992             raise exception.InvalidBDM(str(ex))
1993 
1994     def _update_instance_after_spawn(self, instance,
1995                                      vm_state=vm_states.ACTIVE):
1996         instance.power_state = self._get_power_state(instance)
1997         instance.vm_state = vm_state
1998         instance.task_state = None
1999         # NOTE(sean-k-mooney): configdrive.update_instance checks
2000         # instance.launched_at to determine if it is the first or
2001         # subsequent spawn of an instance. We need to call update_instance
2002         # first before setting instance.launched_at or instance.config_drive
2003         # will never be set to true based on the value of force_config_drive.
2004         # As a result the config drive will be lost on a hard reboot of the
2005         # instance even when force_config_drive=true. see bug #1835822.
2006         configdrive.update_instance(instance)
2007         instance.launched_at = timeutils.utcnow()
2008 
2009     def _update_scheduler_instance_info(self, context, instance):
2010         """Sends an InstanceList with created or updated Instance objects to
2011         the Scheduler client.
2012 
2013         In the case of init_host, the value passed will already be an
2014         InstanceList. Other calls will send individual Instance objects that
2015         have been created or resized. In this case, we create an InstanceList
2016         object containing that Instance.
2017         """
2018         if not self.send_instance_updates:
2019             return
2020         if isinstance(instance, obj_instance.Instance):
2021             instance = objects.InstanceList(objects=[instance])
2022         context = context.elevated()
2023         self.query_client.update_instance_info(context, self.host,
2024                                                instance)
2025 
2026     def _delete_scheduler_instance_info(self, context, instance_uuid):
2027         """Sends the uuid of the deleted Instance to the Scheduler client."""
2028         if not self.send_instance_updates:
2029             return
2030         context = context.elevated()
2031         self.query_client.delete_instance_info(context, self.host,
2032                                                instance_uuid)
2033 
2034     @periodic_task.periodic_task(spacing=CONF.scheduler_instance_sync_interval)
2035     def _sync_scheduler_instance_info(self, context):
2036         if not self.send_instance_updates:
2037             return
2038         context = context.elevated()
2039         instances = objects.InstanceList.get_by_host(context, self.host,
2040                                                      expected_attrs=[],
2041                                                      use_slave=True)
2042         uuids = [instance.uuid for instance in instances]
2043         self.query_client.sync_instance_info(context, self.host, uuids)
2044 
2045     def _notify_about_instance_usage(self, context, instance, event_suffix,
2046                                      network_info=None, extra_usage_info=None,
2047                                      fault=None):
2048         compute_utils.notify_about_instance_usage(
2049             self.notifier, context, instance, event_suffix,
2050             network_info=network_info,
2051             extra_usage_info=extra_usage_info, fault=fault)
2052 
2053     def _deallocate_network(self, context, instance,
2054                             requested_networks=None):
2055         # If we were told not to allocate networks let's save ourselves
2056         # the trouble of calling the network API.
2057         if requested_networks and requested_networks.no_allocate:
2058             LOG.debug("Skipping network deallocation for instance since "
2059                       "networking was not requested.", instance=instance)
2060             return
2061 
2062         LOG.debug('Deallocating network for instance', instance=instance)
2063         with timeutils.StopWatch() as timer:
2064             self.network_api.deallocate_for_instance(
2065                 context, instance, requested_networks=requested_networks)
2066         # nova-network does an rpc call so we're OK tracking time spent here
2067         LOG.info('Took %0.2f seconds to deallocate network for instance.',
2068                  timer.elapsed(), instance=instance)
2069 
2070     def _get_instance_block_device_info(self, context, instance,
2071                                         refresh_conn_info=False,
2072                                         bdms=None):
2073         """Transform block devices to the driver block_device format."""
2074 
2075         if bdms is None:
2076             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
2077                     context, instance.uuid)
2078         block_device_info = driver.get_block_device_info(instance, bdms)
2079 
2080         if not refresh_conn_info:
2081             # if the block_device_mapping has no value in connection_info
2082             # (returned as None), don't include in the mapping
2083             block_device_info['block_device_mapping'] = [
2084                 bdm for bdm in driver.block_device_info_get_mapping(
2085                                     block_device_info)
2086                 if bdm.get('connection_info')]
2087         else:
2088             driver_block_device.refresh_conn_infos(
2089                 driver.block_device_info_get_mapping(block_device_info),
2090                 context, instance, self.volume_api, self.driver)
2091 
2092         self._block_device_info_to_legacy(block_device_info)
2093 
2094         return block_device_info
2095 
2096     def _build_failed(self, node):
2097         if CONF.compute.consecutive_build_service_disable_threshold:
2098             # NOTE(danms): Update our counter, but wait for the next
2099             # update_available_resource() periodic to flush it to the DB
2100             self.rt.build_failed(node)
2101 
2102     def _build_succeeded(self, node):
2103         self.rt.build_succeeded(node)
2104 
2105     @wrap_exception()
2106     @reverts_task_state
2107     @wrap_instance_fault
2108     def build_and_run_instance(self, context, instance, image, request_spec,
2109                      filter_properties, accel_uuids, admin_password=None,
2110                      injected_files=None, requested_networks=None,
2111                      security_groups=None, block_device_mapping=None,
2112                      node=None, limits=None, host_list=None):
2113 
2114         @utils.synchronized(instance.uuid)
2115         def _locked_do_build_and_run_instance(*args, **kwargs):
2116             # NOTE(danms): We grab the semaphore with the instance uuid
2117             # locked because we could wait in line to build this instance
2118             # for a while and we want to make sure that nothing else tries
2119             # to do anything with this instance while we wait.
2120             with self._build_semaphore:
2121                 try:
2122                     result = self._do_build_and_run_instance(*args, **kwargs)
2123                 except Exception:
2124                     # NOTE(mriedem): This should really only happen if
2125                     # _decode_files in _do_build_and_run_instance fails, and
2126                     # that's before a guest is spawned so it's OK to remove
2127                     # allocations for the instance for this node from Placement
2128                     # below as there is no guest consuming resources anyway.
2129                     # The _decode_files case could be handled more specifically
2130                     # but that's left for another day.
2131                     result = build_results.FAILED
2132                     raise
2133                 finally:
2134                     if result == build_results.FAILED:
2135                         # Remove the allocation records from Placement for the
2136                         # instance if the build failed. The instance.host is
2137                         # likely set to None in _do_build_and_run_instance
2138                         # which means if the user deletes the instance, it
2139                         # will be deleted in the API, not the compute service.
2140                         # Setting the instance.host to None in
2141                         # _do_build_and_run_instance means that the
2142                         # ResourceTracker will no longer consider this instance
2143                         # to be claiming resources against it, so we want to
2144                         # reflect that same thing in Placement.  No need to
2145                         # call this for a reschedule, as the allocations will
2146                         # have already been removed in
2147                         # self._do_build_and_run_instance().
2148                         self.reportclient.delete_allocation_for_instance(
2149                             context, instance.uuid)
2150 
2151                     if result in (build_results.FAILED,
2152                                   build_results.RESCHEDULED):
2153                         self._build_failed(node)
2154                     else:
2155                         self._build_succeeded(node)
2156 
2157         # NOTE(danms): We spawn here to return the RPC worker thread back to
2158         # the pool. Since what follows could take a really long time, we don't
2159         # want to tie up RPC workers.
2160         utils.spawn_n(_locked_do_build_and_run_instance,
2161                       context, instance, image, request_spec,
2162                       filter_properties, admin_password, injected_files,
2163                       requested_networks, security_groups,
2164                       block_device_mapping, node, limits, host_list,
2165                       accel_uuids)
2166 
2167     def _check_device_tagging(self, requested_networks, block_device_mapping):
2168         tagging_requested = False
2169         if requested_networks:
2170             for net in requested_networks:
2171                 if 'tag' in net and net.tag is not None:
2172                     tagging_requested = True
2173                     break
2174         if block_device_mapping and not tagging_requested:
2175             for bdm in block_device_mapping:
2176                 if 'tag' in bdm and bdm.tag is not None:
2177                     tagging_requested = True
2178                     break
2179         if (tagging_requested and
2180                 not self.driver.capabilities.get('supports_device_tagging',
2181                                                  False)):
2182             raise exception.BuildAbortException('Attempt to boot guest with '
2183                                                 'tagged devices on host that '
2184                                                 'does not support tagging.')
2185 
2186     def _check_trusted_certs(self, instance):
2187         if (instance.trusted_certs and
2188                 not self.driver.capabilities.get('supports_trusted_certs',
2189                                                  False)):
2190             raise exception.BuildAbortException(
2191                 'Trusted image certificates provided on host that does not '
2192                 'support certificate validation.')
2193 
2194     @wrap_exception()
2195     @reverts_task_state
2196     @wrap_instance_event(prefix='compute')
2197     @wrap_instance_fault
2198     def _do_build_and_run_instance(self, context, instance, image,
2199             request_spec, filter_properties, admin_password, injected_files,
2200             requested_networks, security_groups, block_device_mapping,
2201             node=None, limits=None, host_list=None, accel_uuids=None):
2202 
2203         try:
2204             LOG.debug('Starting instance...', instance=instance)
2205             instance.vm_state = vm_states.BUILDING
2206             instance.task_state = None
2207             instance.save(expected_task_state=
2208                     (task_states.SCHEDULING, None))
2209         except exception.InstanceNotFound:
2210             msg = 'Instance disappeared before build.'
2211             LOG.debug(msg, instance=instance)
2212             return build_results.FAILED
2213         except exception.UnexpectedTaskStateError as e:
2214             LOG.debug(e.format_message(), instance=instance)
2215             return build_results.FAILED
2216 
2217         # b64 decode the files to inject:
2218         decoded_files = self._decode_files(injected_files)
2219 
2220         if limits is None:
2221             limits = {}
2222 
2223         if node is None:
2224             node = self._get_nodename(instance, refresh=True)
2225 
2226         try:
2227             with timeutils.StopWatch() as timer:
2228                 self._build_and_run_instance(context, instance, image,
2229                         decoded_files, admin_password, requested_networks,
2230                         security_groups, block_device_mapping, node, limits,
2231                         filter_properties, request_spec, accel_uuids)
2232             LOG.info('Took %0.2f seconds to build instance.',
2233                      timer.elapsed(), instance=instance)
2234             return build_results.ACTIVE
2235         except exception.RescheduledException as e:
2236             retry = filter_properties.get('retry')
2237             if not retry:
2238                 # no retry information, do not reschedule.
2239                 LOG.debug("Retry info not present, will not reschedule",
2240                     instance=instance)
2241                 self._cleanup_allocated_networks(context, instance,
2242                     requested_networks)
2243                 compute_utils.add_instance_fault_from_exc(context,
2244                         instance, e, sys.exc_info(),
2245                         fault_message=e.kwargs['reason'])
2246                 self._nil_out_instance_obj_host_and_node(instance)
2247                 self._set_instance_obj_error_state(instance,
2248                                                    clean_task_state=True)
2249                 return build_results.FAILED
2250             LOG.debug(e.format_message(), instance=instance)
2251             # This will be used for logging the exception
2252             retry['exc'] = traceback.format_exception(*sys.exc_info())
2253             # This will be used for setting the instance fault message
2254             retry['exc_reason'] = e.kwargs['reason']
2255 
2256             self._cleanup_allocated_networks(context, instance,
2257                                              requested_networks)
2258 
2259             self._nil_out_instance_obj_host_and_node(instance)
2260             instance.task_state = task_states.SCHEDULING
2261             instance.save()
2262             # The instance will have already claimed resources from this host
2263             # before this build was attempted. Now that it has failed, we need
2264             # to unclaim those resources before casting to the conductor, so
2265             # that if there are alternate hosts available for a retry, it can
2266             # claim resources on that new host for the instance.
2267             self.reportclient.delete_allocation_for_instance(context,
2268                                                              instance.uuid)
2269 
2270             self.compute_task_api.build_instances(context, [instance],
2271                     image, filter_properties, admin_password,
2272                     injected_files, requested_networks, security_groups,
2273                     block_device_mapping, request_spec=request_spec,
2274                     host_lists=[host_list])
2275             return build_results.RESCHEDULED
2276         except (exception.InstanceNotFound,
2277                 exception.UnexpectedDeletingTaskStateError):
2278             msg = 'Instance disappeared during build.'
2279             LOG.debug(msg, instance=instance)
2280             self._cleanup_allocated_networks(context, instance,
2281                     requested_networks)
2282             return build_results.FAILED
2283         except Exception as e:
2284             if isinstance(e, exception.BuildAbortException):
2285                 LOG.error(e.format_message(), instance=instance)
2286             else:
2287                 # Should not reach here.
2288                 LOG.exception('Unexpected build failure, not rescheduling '
2289                               'build.', instance=instance)
2290             self._cleanup_allocated_networks(context, instance,
2291                     requested_networks)
2292             self._cleanup_volumes(context, instance,
2293                     block_device_mapping, raise_exc=False)
2294             compute_utils.add_instance_fault_from_exc(context, instance,
2295                     e, sys.exc_info())
2296             self._nil_out_instance_obj_host_and_node(instance)
2297             self._set_instance_obj_error_state(instance, clean_task_state=True)
2298             return build_results.FAILED
2299 
2300     @staticmethod
2301     def _get_scheduler_hints(filter_properties, request_spec=None):
2302         """Helper method to get scheduler hints.
2303 
2304         This method prefers to get the hints out of the request spec, but that
2305         might not be provided. Conductor will pass request_spec down to the
2306         first compute chosen for a build but older computes will not pass
2307         the request_spec to conductor's build_instances method for a
2308         a reschedule, so if we're on a host via a retry, request_spec may not
2309         be provided so we need to fallback to use the filter_properties
2310         to get scheduler hints.
2311         """
2312         hints = {}
2313         if request_spec is not None and 'scheduler_hints' in request_spec:
2314             hints = request_spec.scheduler_hints
2315         if not hints:
2316             hints = filter_properties.get('scheduler_hints') or {}
2317         return hints
2318 
2319     @staticmethod
2320     def _get_request_group_mapping(request_spec):
2321         """Return request group resource - provider mapping. This is currently
2322         used for Neutron ports that have resource request due to the port
2323         having QoS minimum bandwidth policy rule attached.
2324 
2325         :param request_spec: A RequestSpec object or None
2326         :returns: A dict keyed by RequestGroup requester_id, currently Neutron
2327         port_id, to resource provider UUID that provides resource for that
2328         RequestGroup. Or None if the request_spec was None.
2329         """
2330         # TODO(sbauza): Remove this conditional once we only support
2331         # RPC API 6.0
2332         if request_spec:
2333             return request_spec.get_request_group_mapping()
2334         else:
2335             return None
2336 
2337     def _build_and_run_instance(self, context, instance, image, injected_files,
2338             admin_password, requested_networks, security_groups,
2339             block_device_mapping, node, limits, filter_properties,
2340             request_spec=None, accel_uuids=None):
2341 
2342         image_name = image.get('name')
2343         self._notify_about_instance_usage(context, instance, 'create.start',
2344                 extra_usage_info={'image_name': image_name})
2345         compute_utils.notify_about_instance_create(
2346             context, instance, self.host,
2347             phase=fields.NotificationPhase.START,
2348             bdms=block_device_mapping)
2349 
2350         # NOTE(mikal): cache the keystone roles associated with the instance
2351         # at boot time for later reference
2352         instance.system_metadata.update(
2353             {'boot_roles': ','.join(context.roles)})
2354 
2355         self._check_device_tagging(requested_networks, block_device_mapping)
2356         self._check_trusted_certs(instance)
2357 
2358         provider_mapping = self._get_request_group_mapping(request_spec)
2359 
2360         if provider_mapping:
2361             try:
2362                 compute_utils\
2363                     .update_pci_request_spec_with_allocated_interface_name(
2364                         context, self.reportclient,
2365                         instance.pci_requests.requests, provider_mapping)
2366             except (exception.AmbiguousResourceProviderForPCIRequest,
2367                     exception.UnexpectedResourceProviderNameForPCIRequest
2368                     ) as e:
2369                 raise exception.BuildAbortException(
2370                     reason=str(e), instance_uuid=instance.uuid)
2371 
2372         # TODO(Luyao) cut over to get_allocs_for_consumer
2373         allocs = self.reportclient.get_allocations_for_consumer(
2374                 context, instance.uuid)
2375 
2376         try:
2377             scheduler_hints = self._get_scheduler_hints(filter_properties,
2378                                                         request_spec)
2379             with self.rt.instance_claim(context, instance, node, allocs,
2380                                         limits):
2381                 # NOTE(russellb) It's important that this validation be done
2382                 # *after* the resource tracker instance claim, as that is where
2383                 # the host is set on the instance.
2384                 self._validate_instance_group_policy(context, instance,
2385                                                      scheduler_hints)
2386                 image_meta = objects.ImageMeta.from_dict(image)
2387 
2388                 with self._build_resources(context, instance,
2389                         requested_networks, security_groups, image_meta,
2390                         block_device_mapping, provider_mapping,
2391                         accel_uuids) as resources:
2392                     instance.vm_state = vm_states.BUILDING
2393                     instance.task_state = task_states.SPAWNING
2394                     # NOTE(JoshNang) This also saves the changes to the
2395                     # instance from _allocate_network_async, as they aren't
2396                     # saved in that function to prevent races.
2397                     instance.save(expected_task_state=
2398                             task_states.BLOCK_DEVICE_MAPPING)
2399                     block_device_info = resources['block_device_info']
2400                     network_info = resources['network_info']
2401                     accel_info = resources['accel_info']
2402                     LOG.debug('Start spawning the instance on the hypervisor.',
2403                               instance=instance)
2404                     with timeutils.StopWatch() as timer:
2405                         self.driver.spawn(context, instance, image_meta,
2406                                           injected_files, admin_password,
2407                                           allocs, network_info=network_info,
2408                                           block_device_info=block_device_info,
2409                                           accel_info=accel_info)
2410                     LOG.info('Took %0.2f seconds to spawn the instance on '
2411                              'the hypervisor.', timer.elapsed(),
2412                              instance=instance)
2413         except (exception.InstanceNotFound,
2414                 exception.UnexpectedDeletingTaskStateError) as e:
2415             with excutils.save_and_reraise_exception():
2416                 self._notify_about_instance_usage(context, instance,
2417                     'create.error', fault=e)
2418                 compute_utils.notify_about_instance_create(
2419                     context, instance, self.host,
2420                     phase=fields.NotificationPhase.ERROR, exception=e,
2421                     bdms=block_device_mapping)
2422         except exception.ComputeResourcesUnavailable as e:
2423             LOG.debug(e.format_message(), instance=instance)
2424             self._notify_about_instance_usage(context, instance,
2425                     'create.error', fault=e)
2426             compute_utils.notify_about_instance_create(
2427                     context, instance, self.host,
2428                     phase=fields.NotificationPhase.ERROR, exception=e,
2429                     bdms=block_device_mapping)
2430             raise exception.RescheduledException(
2431                     instance_uuid=instance.uuid, reason=e.format_message())
2432         except exception.BuildAbortException as e:
2433             with excutils.save_and_reraise_exception():
2434                 LOG.debug(e.format_message(), instance=instance)
2435                 self._notify_about_instance_usage(context, instance,
2436                     'create.error', fault=e)
2437                 compute_utils.notify_about_instance_create(
2438                     context, instance, self.host,
2439                     phase=fields.NotificationPhase.ERROR, exception=e,
2440                     bdms=block_device_mapping)
2441         except exception.NoMoreFixedIps as e:
2442             LOG.warning('No more fixed IP to be allocated',
2443                         instance=instance)
2444             self._notify_about_instance_usage(context, instance,
2445                     'create.error', fault=e)
2446             compute_utils.notify_about_instance_create(
2447                     context, instance, self.host,
2448                     phase=fields.NotificationPhase.ERROR, exception=e,
2449                     bdms=block_device_mapping)
2450             msg = _('Failed to allocate the network(s) with error %s, '
2451                     'not rescheduling.') % e.format_message()
2452             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2453                     reason=msg)
2454         except (exception.ExternalNetworkAttachForbidden,
2455                 exception.VirtualInterfaceCreateException,
2456                 exception.VirtualInterfaceMacAddressException,
2457                 exception.FixedIpInvalidOnHost,
2458                 exception.UnableToAutoAllocateNetwork,
2459                 exception.NetworksWithQoSPolicyNotSupported) as e:
2460             LOG.exception('Failed to allocate network(s)',
2461                           instance=instance)
2462             self._notify_about_instance_usage(context, instance,
2463                     'create.error', fault=e)
2464             compute_utils.notify_about_instance_create(
2465                     context, instance, self.host,
2466                     phase=fields.NotificationPhase.ERROR, exception=e,
2467                     bdms=block_device_mapping)
2468             msg = _('Failed to allocate the network(s), not rescheduling.')
2469             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2470                     reason=msg)
2471         except (exception.FlavorDiskTooSmall,
2472                 exception.FlavorMemoryTooSmall,
2473                 exception.ImageNotActive,
2474                 exception.ImageUnacceptable,
2475                 exception.InvalidDiskInfo,
2476                 exception.InvalidDiskFormat,
2477                 cursive_exception.SignatureVerificationError,
2478                 exception.CertificateValidationFailed,
2479                 exception.VolumeEncryptionNotSupported,
2480                 exception.InvalidInput,
2481                 # TODO(mriedem): We should be validating RequestedVRamTooHigh
2482                 # in the API during server create and rebuild.
2483                 exception.RequestedVRamTooHigh) as e:
2484             self._notify_about_instance_usage(context, instance,
2485                     'create.error', fault=e)
2486             compute_utils.notify_about_instance_create(
2487                     context, instance, self.host,
2488                     phase=fields.NotificationPhase.ERROR, exception=e,
2489                     bdms=block_device_mapping)
2490             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2491                     reason=e.format_message())
2492         except Exception as e:
2493             LOG.exception('Failed to build and run instance',
2494                           instance=instance)
2495             self._notify_about_instance_usage(context, instance,
2496                     'create.error', fault=e)
2497             compute_utils.notify_about_instance_create(
2498                     context, instance, self.host,
2499                     phase=fields.NotificationPhase.ERROR, exception=e,
2500                     bdms=block_device_mapping)
2501             raise exception.RescheduledException(
2502                     instance_uuid=instance.uuid, reason=str(e))
2503 
2504         # NOTE(alaski): This is only useful during reschedules, remove it now.
2505         instance.system_metadata.pop('network_allocated', None)
2506 
2507         # If CONF.default_access_ip_network_name is set, grab the
2508         # corresponding network and set the access ip values accordingly.
2509         network_name = CONF.default_access_ip_network_name
2510         if (network_name and not instance.access_ip_v4 and
2511                 not instance.access_ip_v6):
2512             # Note that when there are multiple ips to choose from, an
2513             # arbitrary one will be chosen.
2514             for vif in network_info:
2515                 if vif['network']['label'] == network_name:
2516                     for ip in vif.fixed_ips():
2517                         if not instance.access_ip_v4 and ip['version'] == 4:
2518                             instance.access_ip_v4 = ip['address']
2519                         if not instance.access_ip_v6 and ip['version'] == 6:
2520                             instance.access_ip_v6 = ip['address']
2521                     break
2522 
2523         self._update_instance_after_spawn(instance)
2524 
2525         try:
2526             instance.save(expected_task_state=task_states.SPAWNING)
2527         except (exception.InstanceNotFound,
2528                 exception.UnexpectedDeletingTaskStateError) as e:
2529             with excutils.save_and_reraise_exception():
2530                 self._notify_about_instance_usage(context, instance,
2531                     'create.error', fault=e)
2532                 compute_utils.notify_about_instance_create(
2533                     context, instance, self.host,
2534                     phase=fields.NotificationPhase.ERROR, exception=e,
2535                     bdms=block_device_mapping)
2536 
2537         self._update_scheduler_instance_info(context, instance)
2538         self._notify_about_instance_usage(context, instance, 'create.end',
2539                 extra_usage_info={'message': _('Success')},
2540                 network_info=network_info)
2541         compute_utils.notify_about_instance_create(context, instance,
2542                 self.host, phase=fields.NotificationPhase.END,
2543                 bdms=block_device_mapping)
2544 
2545     def _build_resources_cleanup(self, instance, network_info):
2546         # Make sure the async call finishes
2547         if network_info is not None:
2548             network_info.wait(do_raise=False)
2549             self.driver.clean_networks_preparation(instance,
2550                                                    network_info)
2551         self.driver.failed_spawn_cleanup(instance)
2552 
2553     @contextlib.contextmanager
2554     def _build_resources(self, context, instance, requested_networks,
2555                          security_groups, image_meta, block_device_mapping,
2556                          resource_provider_mapping, accel_uuids):
2557         resources = {}
2558         network_info = None
2559         try:
2560             LOG.debug('Start building networks asynchronously for instance.',
2561                       instance=instance)
2562             network_info = self._build_networks_for_instance(context, instance,
2563                     requested_networks, security_groups,
2564                     resource_provider_mapping)
2565             resources['network_info'] = network_info
2566         except (exception.InstanceNotFound,
2567                 exception.UnexpectedDeletingTaskStateError):
2568             raise
2569         except exception.UnexpectedTaskStateError as e:
2570             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2571                     reason=e.format_message())
2572         except Exception:
2573             # Because this allocation is async any failures are likely to occur
2574             # when the driver accesses network_info during spawn().
2575             LOG.exception('Failed to allocate network(s)',
2576                           instance=instance)
2577             msg = _('Failed to allocate the network(s), not rescheduling.')
2578             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2579                     reason=msg)
2580 
2581         try:
2582             # Perform any driver preparation work for the driver.
2583             self.driver.prepare_for_spawn(instance)
2584 
2585             # Depending on a virt driver, some network configuration is
2586             # necessary before preparing block devices.
2587             self.driver.prepare_networks_before_block_device_mapping(
2588                 instance, network_info)
2589 
2590             # Verify that all the BDMs have a device_name set and assign a
2591             # default to the ones missing it with the help of the driver.
2592             self._default_block_device_names(instance, image_meta,
2593                                              block_device_mapping)
2594 
2595             LOG.debug('Start building block device mappings for instance.',
2596                       instance=instance)
2597             instance.vm_state = vm_states.BUILDING
2598             instance.task_state = task_states.BLOCK_DEVICE_MAPPING
2599             instance.save()
2600 
2601             block_device_info = self._prep_block_device(context, instance,
2602                     block_device_mapping)
2603             resources['block_device_info'] = block_device_info
2604         except (exception.InstanceNotFound,
2605                 exception.UnexpectedDeletingTaskStateError):
2606             with excutils.save_and_reraise_exception():
2607                 self._build_resources_cleanup(instance, network_info)
2608         except (exception.UnexpectedTaskStateError,
2609                 exception.OverQuota, exception.InvalidBDM) as e:
2610             self._build_resources_cleanup(instance, network_info)
2611             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2612                     reason=e.format_message())
2613         except Exception:
2614             LOG.exception('Failure prepping block device',
2615                           instance=instance)
2616             self._build_resources_cleanup(instance, network_info)
2617             msg = _('Failure prepping block device.')
2618             raise exception.BuildAbortException(instance_uuid=instance.uuid,
2619                     reason=msg)
2620 
2621         arqs = []
2622         if instance.flavor.extra_specs.get('accel:device_profile'):
2623             try:
2624                 arqs = self._get_bound_arq_resources(
2625                     context, instance, accel_uuids)
2626             except (Exception, eventlet.timeout.Timeout) as exc:
2627                 LOG.exception(exc)
2628                 self._build_resources_cleanup(instance, network_info)
2629                 compute_utils.delete_arqs_if_needed(context, instance)
2630                 msg = _('Failure getting accelerator requests.')
2631                 raise exception.BuildAbortException(
2632                     reason=msg, instance_uuid=instance.uuid)
2633 
2634         resources['accel_info'] = arqs
2635         try:
2636             yield resources
2637         except Exception as exc:
2638             with excutils.save_and_reraise_exception() as ctxt:
2639                 if not isinstance(exc, (
2640                         exception.InstanceNotFound,
2641                         exception.UnexpectedDeletingTaskStateError)):
2642                     LOG.exception('Instance failed to spawn',
2643                                   instance=instance)
2644                 # Make sure the async call finishes
2645                 if network_info is not None:
2646                     network_info.wait(do_raise=False)
2647                 # if network_info is empty we're likely here because of
2648                 # network allocation failure. Since nothing can be reused on
2649                 # rescheduling it's better to deallocate network to eliminate
2650                 # the chance of orphaned ports in neutron
2651                 deallocate_networks = False if network_info else True
2652                 try:
2653                     self._shutdown_instance(context, instance,
2654                             block_device_mapping, requested_networks,
2655                             try_deallocate_networks=deallocate_networks)
2656                 except Exception as exc2:
2657                     ctxt.reraise = False
2658                     LOG.warning('Could not clean up failed build,'
2659                                 ' not rescheduling. Error: %s',
2660                                 str(exc2))
2661                     raise exception.BuildAbortException(
2662                             instance_uuid=instance.uuid,
2663                             reason=str(exc))
2664                 finally:
2665                     # Call Cyborg to delete accelerator requests
2666                     compute_utils.delete_arqs_if_needed(context, instance)
2667 
2668     def _get_bound_arq_resources(self, context, instance, arq_uuids):
2669         """Get bound accelerator requests.
2670 
2671         The ARQ binding was kicked off in the conductor as an async
2672         operation. Here we wait for the notification from Cyborg.
2673 
2674         If the notification arrived before this point, which can happen
2675         in many/most cases (see [1]), it will be lost. To handle that,
2676         we use exit_wait_early.
2677         [1] https://review.opendev.org/#/c/631244/46/nova/compute/
2678             manager.py@2627
2679 
2680         :param instance: instance object
2681         :param arq_uuids: List of accelerator request (ARQ) UUIDs.
2682         :returns: List of ARQs for which bindings have completed,
2683                   successfully or otherwise
2684         """
2685 
2686         cyclient = cyborg.get_client(context)
2687         if arq_uuids is None:
2688             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2689             arq_uuids = [arq['uuid'] for arq in arqs]
2690         events = [('accelerator-request-bound', arq_uuid)
2691                   for arq_uuid in arq_uuids]
2692 
2693         timeout = CONF.arq_binding_timeout
2694         with self.virtapi.wait_for_instance_event(
2695                 instance, events, deadline=timeout):
2696             resolved_arqs = cyclient.get_arqs_for_instance(
2697                     instance.uuid, only_resolved=True)
2698             # Events for these resolved ARQs may have already arrived.
2699             # Such 'early' events need to be ignored.
2700             early_events = [('accelerator-request-bound', arq['uuid'])
2701                              for arq in resolved_arqs]
2702             if early_events:
2703                 self.virtapi.exit_wait_early(early_events)
2704 
2705         # Since a timeout in wait_for_instance_event will raise, we get
2706         # here only if all binding events have been received.
2707         resolved_uuids = [arq['uuid'] for arq in resolved_arqs]
2708         if sorted(resolved_uuids) != sorted(arq_uuids):
2709             # Query Cyborg to get all.
2710             arqs = cyclient.get_arqs_for_instance(instance.uuid)
2711         else:
2712             arqs = resolved_arqs
2713         return arqs
2714 
2715     def _cleanup_allocated_networks(self, context, instance,
2716             requested_networks):
2717         """Cleanup networks allocated for instance.
2718 
2719         :param context: nova request context
2720         :param instance: nova.objects.instance.Instance object
2721         :param requested_networks: nova.objects.NetworkRequestList
2722         """
2723         LOG.debug('Unplugging VIFs for instance', instance=instance)
2724 
2725         network_info = instance.get_network_info()
2726 
2727         # NOTE(stephenfin) to avoid nova destroying the instance without
2728         # unplugging the interface, refresh network_info if it is empty.
2729         if not network_info:
2730             try:
2731                 network_info = self.network_api.get_instance_nw_info(
2732                     context, instance,
2733                 )
2734             except Exception as exc:
2735                 LOG.warning(
2736                     'Failed to update network info cache when cleaning up '
2737                     'allocated networks. Stale VIFs may be left on this host.'
2738                     'Error: %s', str(exc)
2739                 )
2740                 return
2741 
2742         try:
2743             self.driver.unplug_vifs(instance, network_info)
2744         except NotImplementedError:
2745             # This is an optional method so ignore things if it doesn't exist
2746             LOG.debug(
2747                 'Virt driver does not provide unplug_vifs method, so it '
2748                 'is not possible determine if VIFs should be unplugged.'
2749             )
2750         except exception.NovaException as exc:
2751             # It's possible that the instance never got as far as plugging
2752             # VIFs, in which case we would see an exception which can be
2753             # mostly ignored
2754             LOG.warning(
2755                 'Cleaning up VIFs failed for instance. Error: %s',
2756                 str(exc), instance=instance,
2757             )
2758         else:
2759             LOG.debug('Unplugged VIFs for instance', instance=instance)
2760 
2761         try:
2762             self._deallocate_network(context, instance, requested_networks)
2763         except Exception:
2764             LOG.exception('Failed to deallocate networks', instance=instance)
2765             return
2766 
2767         instance.system_metadata['network_allocated'] = 'False'
2768         try:
2769             instance.save()
2770         except exception.InstanceNotFound:
2771             # NOTE(alaski): It's possible that we're cleaning up the networks
2772             # because the instance was deleted.  If that's the case then this
2773             # exception will be raised by instance.save()
2774             pass
2775 
2776     def _try_deallocate_network(self, context, instance,
2777                                 requested_networks=None):
2778 
2779         # During auto-scale cleanup, we could be deleting a large number
2780         # of servers at the same time and overloading parts of the system,
2781         # so we retry a few times in case of connection failures to the
2782         # networking service.
2783         @loopingcall.RetryDecorator(
2784             max_retry_count=3, inc_sleep_time=2, max_sleep_time=12,
2785             exceptions=(keystone_exception.connection.ConnectFailure,))
2786         def _deallocate_network_with_retries():
2787             try:
2788                 self._deallocate_network(
2789                     context, instance, requested_networks)
2790             except keystone_exception.connection.ConnectFailure as e:
2791                 # Provide a warning that something is amiss.
2792                 with excutils.save_and_reraise_exception():
2793                     LOG.warning('Failed to deallocate network for instance; '
2794                                 'retrying. Error: %s', str(e),
2795                                 instance=instance)
2796 
2797         try:
2798             # tear down allocated network structure
2799             _deallocate_network_with_retries()
2800         except Exception as ex:
2801             with excutils.save_and_reraise_exception():
2802                 LOG.error('Failed to deallocate network for instance. '
2803                           'Error: %s', ex, instance=instance)
2804                 self._set_instance_obj_error_state(instance)
2805 
2806     def _get_power_off_values(self, instance, clean_shutdown):
2807         """Get the timing configuration for powering down this instance."""
2808         if clean_shutdown:
2809             timeout = compute_utils.get_value_from_system_metadata(instance,
2810                           key='image_os_shutdown_timeout', type=int,
2811                           default=CONF.shutdown_timeout)
2812             retry_interval = CONF.compute.shutdown_retry_interval
2813         else:
2814             timeout = 0
2815             retry_interval = 0
2816 
2817         return timeout, retry_interval
2818 
2819     def _power_off_instance(self, instance, clean_shutdown=True):
2820         """Power off an instance on this host."""
2821         timeout, retry_interval = self._get_power_off_values(
2822             instance, clean_shutdown)
2823         self.driver.power_off(instance, timeout, retry_interval)
2824 
2825     def _shutdown_instance(self, context, instance,
2826                            bdms, requested_networks=None, notify=True,
2827                            try_deallocate_networks=True):
2828         """Shutdown an instance on this host.
2829 
2830         :param:context: security context
2831         :param:instance: a nova.objects.Instance object
2832         :param:bdms: the block devices for the instance to be torn
2833                      down
2834         :param:requested_networks: the networks on which the instance
2835                                    has ports
2836         :param:notify: true if a final usage notification should be
2837                        emitted
2838         :param:try_deallocate_networks: false if we should avoid
2839                                         trying to teardown networking
2840         """
2841         context = context.elevated()
2842         LOG.info('Terminating instance', instance=instance)
2843 
2844         if notify:
2845             self._notify_about_instance_usage(context, instance,
2846                                               "shutdown.start")
2847             compute_utils.notify_about_instance_action(context, instance,
2848                     self.host, action=fields.NotificationAction.SHUTDOWN,
2849                     phase=fields.NotificationPhase.START, bdms=bdms)
2850 
2851         network_info = instance.get_network_info()
2852 
2853         # NOTE(arnaudmorin) to avoid nova destroying the instance without
2854         # unplugging the interface, refresh network_info if it is empty.
2855         if not network_info:
2856             network_info = self.network_api.get_instance_nw_info(
2857                 context, instance)
2858 
2859         # NOTE(vish) get bdms before destroying the instance
2860         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
2861         block_device_info = self._get_instance_block_device_info(
2862             context, instance, bdms=bdms)
2863 
2864         # NOTE(melwitt): attempt driver destroy before releasing ip, may
2865         #                want to keep ip allocated for certain failures
2866         try:
2867             LOG.debug('Start destroying the instance on the hypervisor.',
2868                       instance=instance)
2869             with timeutils.StopWatch() as timer:
2870                 self.driver.destroy(context, instance, network_info,
2871                                     block_device_info)
2872             LOG.info('Took %0.2f seconds to destroy the instance on the '
2873                      'hypervisor.', timer.elapsed(), instance=instance)
2874         except exception.InstancePowerOffFailure:
2875             # if the instance can't power off, don't release the ip
2876             with excutils.save_and_reraise_exception():
2877                 pass
2878         except Exception:
2879             with excutils.save_and_reraise_exception():
2880                 # deallocate ip and fail without proceeding to
2881                 # volume api calls, preserving current behavior
2882                 if try_deallocate_networks:
2883                     self._try_deallocate_network(context, instance,
2884                                                  requested_networks)
2885 
2886         if try_deallocate_networks:
2887             self._try_deallocate_network(context, instance, requested_networks)
2888 
2889         timer.restart()
2890         for bdm in vol_bdms:
2891             try:
2892                 if bdm.attachment_id:
2893                     self.volume_api.attachment_delete(context,
2894                                                       bdm.attachment_id)
2895                 else:
2896                     # NOTE(vish): actual driver detach done in driver.destroy,
2897                     #             so just tell cinder that we are done with it.
2898                     connector = self.driver.get_volume_connector(instance)
2899                     self.volume_api.terminate_connection(context,
2900                                                          bdm.volume_id,
2901                                                          connector)
2902                     self.volume_api.detach(context, bdm.volume_id,
2903                                            instance.uuid)
2904 
2905             except exception.VolumeAttachmentNotFound as exc:
2906                 LOG.debug('Ignoring VolumeAttachmentNotFound: %s', exc,
2907                           instance=instance)
2908             except exception.DiskNotFound as exc:
2909                 LOG.debug('Ignoring DiskNotFound: %s', exc,
2910                           instance=instance)
2911             except exception.VolumeNotFound as exc:
2912                 LOG.debug('Ignoring VolumeNotFound: %s', exc,
2913                           instance=instance)
2914             except (cinder_exception.EndpointNotFound,
2915                     keystone_exception.EndpointNotFound) as exc:
2916                 LOG.warning('Ignoring EndpointNotFound for '
2917                             'volume %(volume_id)s: %(exc)s',
2918                             {'exc': exc, 'volume_id': bdm.volume_id},
2919                             instance=instance)
2920             except cinder_exception.ClientException as exc:
2921                 LOG.warning('Ignoring unknown cinder exception for '
2922                             'volume %(volume_id)s: %(exc)s',
2923                             {'exc': exc, 'volume_id': bdm.volume_id},
2924                             instance=instance)
2925             except Exception as exc:
2926                 LOG.warning('Ignoring unknown exception for '
2927                             'volume %(volume_id)s: %(exc)s',
2928                             {'exc': exc, 'volume_id': bdm.volume_id},
2929                             instance=instance)
2930         if vol_bdms:
2931             LOG.info('Took %(time).2f seconds to detach %(num)s volumes '
2932                      'for instance.',
2933                      {'time': timer.elapsed(), 'num': len(vol_bdms)},
2934                      instance=instance)
2935 
2936         if notify:
2937             self._notify_about_instance_usage(context, instance,
2938                                               "shutdown.end")
2939             compute_utils.notify_about_instance_action(context, instance,
2940                     self.host, action=fields.NotificationAction.SHUTDOWN,
2941                     phase=fields.NotificationPhase.END, bdms=bdms)
2942 
2943     def _cleanup_volumes(self, context, instance, bdms, raise_exc=True,
2944                          detach=True):
2945         original_exception = None
2946         for bdm in bdms:
2947             if detach and bdm.volume_id:
2948                 try:
2949                     LOG.debug("Detaching volume: %s", bdm.volume_id,
2950                               instance_uuid=instance.uuid)
2951                     destroy = bdm.delete_on_termination
2952                     self._detach_volume(context, bdm, instance,
2953                                         destroy_bdm=destroy)
2954                 except Exception as exc:
2955                     original_exception = exc
2956                     LOG.warning('Failed to detach volume: %(volume_id)s '
2957                                 'due to %(exc)s',
2958                                 {'volume_id': bdm.volume_id, 'exc': exc})
2959 
2960             if bdm.volume_id and bdm.delete_on_termination:
2961                 try:
2962                     LOG.debug("Deleting volume: %s", bdm.volume_id,
2963                               instance_uuid=instance.uuid)
2964                     self.volume_api.delete(context, bdm.volume_id)
2965                 except Exception as exc:
2966                     original_exception = exc
2967                     LOG.warning('Failed to delete volume: %(volume_id)s '
2968                                 'due to %(exc)s',
2969                                 {'volume_id': bdm.volume_id, 'exc': exc})
2970         if original_exception is not None and raise_exc:
2971             raise original_exception
2972 
2973     def _delete_instance(self, context, instance, bdms):
2974         """Delete an instance on this host.
2975 
2976         :param context: nova request context
2977         :param instance: nova.objects.instance.Instance object
2978         :param bdms: nova.objects.block_device.BlockDeviceMappingList object
2979         """
2980         events = self.instance_events.clear_events_for_instance(instance)
2981         if events:
2982             LOG.debug('Events pending at deletion: %(events)s',
2983                       {'events': ','.join(events.keys())},
2984                       instance=instance)
2985         self._notify_about_instance_usage(context, instance,
2986                                           "delete.start")
2987         compute_utils.notify_about_instance_action(context, instance,
2988                 self.host, action=fields.NotificationAction.DELETE,
2989                 phase=fields.NotificationPhase.START, bdms=bdms)
2990 
2991         self._shutdown_instance(context, instance, bdms)
2992 
2993         # NOTE(vish): We have already deleted the instance, so we have
2994         #             to ignore problems cleaning up the volumes. It
2995         #             would be nice to let the user know somehow that
2996         #             the volume deletion failed, but it is not
2997         #             acceptable to have an instance that can not be
2998         #             deleted. Perhaps this could be reworked in the
2999         #             future to set an instance fault the first time
3000         #             and to only ignore the failure if the instance
3001         #             is already in ERROR.
3002 
3003         # NOTE(ameeda): The volumes have already been detached during
3004         #               the above _shutdown_instance() call and this is
3005         #               why detach is not requested from
3006         #               _cleanup_volumes() in this case
3007 
3008         self._cleanup_volumes(context, instance, bdms,
3009                 raise_exc=False, detach=False)
3010         # Delete Cyborg ARQs if the instance has a device profile.
3011         compute_utils.delete_arqs_if_needed(context, instance)
3012         # if a delete task succeeded, always update vm state and task
3013         # state without expecting task state to be DELETING
3014         instance.vm_state = vm_states.DELETED
3015         instance.task_state = None
3016         instance.power_state = power_state.NOSTATE
3017         instance.terminated_at = timeutils.utcnow()
3018         instance.save()
3019 
3020         self._complete_deletion(context, instance)
3021         # only destroy the instance in the db if the _complete_deletion
3022         # doesn't raise and therefore allocation is successfully
3023         # deleted in placement
3024         instance.destroy()
3025 
3026         self._notify_about_instance_usage(context, instance, "delete.end")
3027         compute_utils.notify_about_instance_action(context, instance,
3028                 self.host, action=fields.NotificationAction.DELETE,
3029                 phase=fields.NotificationPhase.END, bdms=bdms)
3030 
3031     @wrap_exception()
3032     @reverts_task_state
3033     @wrap_instance_event(prefix='compute')
3034     @wrap_instance_fault
3035     def terminate_instance(self, context, instance, bdms):
3036         """Terminate an instance on this host."""
3037         @utils.synchronized(instance.uuid)
3038         def do_terminate_instance(instance, bdms):
3039             # NOTE(mriedem): If we are deleting the instance while it was
3040             # booting from volume, we could be racing with a database update of
3041             # the BDM volume_id. Since the compute API passes the BDMs over RPC
3042             # to compute here, the BDMs may be stale at this point. So check
3043             # for any volume BDMs that don't have volume_id set and if we
3044             # detect that, we need to refresh the BDM list before proceeding.
3045             # TODO(mriedem): Move this into _delete_instance and make the bdms
3046             # parameter optional.
3047             for bdm in list(bdms):
3048                 if bdm.is_volume and not bdm.volume_id:
3049                     LOG.debug('There are potentially stale BDMs during '
3050                               'delete, refreshing the BlockDeviceMappingList.',
3051                               instance=instance)
3052                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3053                         context, instance.uuid)
3054                     break
3055             try:
3056                 self._delete_instance(context, instance, bdms)
3057             except exception.InstanceNotFound:
3058                 LOG.info("Instance disappeared during terminate",
3059                          instance=instance)
3060             except Exception:
3061                 # As we're trying to delete always go to Error if something
3062                 # goes wrong that _delete_instance can't handle.
3063                 with excutils.save_and_reraise_exception():
3064                     LOG.exception('Setting instance vm_state to ERROR',
3065                                   instance=instance)
3066                     self._set_instance_obj_error_state(instance)
3067 
3068         do_terminate_instance(instance, bdms)
3069 
3070     # NOTE(johannes): This is probably better named power_off_instance
3071     # so it matches the driver method, but because of other issues, we
3072     # can't use that name in grizzly.
3073     @wrap_exception()
3074     @reverts_task_state
3075     @wrap_instance_event(prefix='compute')
3076     @wrap_instance_fault
3077     def stop_instance(self, context, instance, clean_shutdown):
3078         """Stopping an instance on this host."""
3079 
3080         @utils.synchronized(instance.uuid)
3081         def do_stop_instance():
3082             current_power_state = self._get_power_state(instance)
3083             LOG.debug('Stopping instance; current vm_state: %(vm_state)s, '
3084                       'current task_state: %(task_state)s, current DB '
3085                       'power_state: %(db_power_state)s, current VM '
3086                       'power_state: %(current_power_state)s',
3087                       {'vm_state': instance.vm_state,
3088                        'task_state': instance.task_state,
3089                        'db_power_state': instance.power_state,
3090                        'current_power_state': current_power_state},
3091                       instance_uuid=instance.uuid)
3092 
3093             # NOTE(mriedem): If the instance is already powered off, we are
3094             # possibly tearing down and racing with other operations, so we can
3095             # expect the task_state to be None if something else updates the
3096             # instance and we're not locking it.
3097             expected_task_state = [task_states.POWERING_OFF]
3098             # The list of power states is from _sync_instance_power_state.
3099             if current_power_state in (power_state.NOSTATE,
3100                                        power_state.SHUTDOWN,
3101                                        power_state.CRASHED):
3102                 LOG.info('Instance is already powered off in the '
3103                          'hypervisor when stop is called.',
3104                          instance=instance)
3105                 expected_task_state.append(None)
3106 
3107             self._notify_about_instance_usage(context, instance,
3108                                               "power_off.start")
3109 
3110             compute_utils.notify_about_instance_action(context, instance,
3111                         self.host, action=fields.NotificationAction.POWER_OFF,
3112                         phase=fields.NotificationPhase.START)
3113 
3114             self._power_off_instance(instance, clean_shutdown)
3115             instance.power_state = self._get_power_state(instance)
3116             instance.vm_state = vm_states.STOPPED
3117             instance.task_state = None
3118             instance.save(expected_task_state=expected_task_state)
3119             self._notify_about_instance_usage(context, instance,
3120                                               "power_off.end")
3121 
3122             compute_utils.notify_about_instance_action(context, instance,
3123                         self.host, action=fields.NotificationAction.POWER_OFF,
3124                         phase=fields.NotificationPhase.END)
3125 
3126         do_stop_instance()
3127 
3128     def _power_on(self, context, instance):
3129         network_info = self.network_api.get_instance_nw_info(context, instance)
3130         block_device_info = self._get_instance_block_device_info(context,
3131                                                                  instance)
3132         accel_info = self._get_accel_info(context, instance)
3133         self.driver.power_on(context, instance,
3134                              network_info,
3135                              block_device_info, accel_info)
3136 
3137     def _delete_snapshot_of_shelved_instance(self, context, instance,
3138                                              snapshot_id):
3139         """Delete snapshot of shelved instance."""
3140         try:
3141             self.image_api.delete(context, snapshot_id)
3142         except (exception.ImageNotFound,
3143                 exception.ImageNotAuthorized) as exc:
3144             LOG.warning("Failed to delete snapshot "
3145                         "from shelved instance (%s).",
3146                         exc.format_message(), instance=instance)
3147         except Exception:
3148             LOG.exception("Something wrong happened when trying to "
3149                           "delete snapshot from shelved instance.",
3150                           instance=instance)
3151 
3152     # NOTE(johannes): This is probably better named power_on_instance
3153     # so it matches the driver method, but because of other issues, we
3154     # can't use that name in grizzly.
3155     @wrap_exception()
3156     @reverts_task_state
3157     @wrap_instance_event(prefix='compute')
3158     @wrap_instance_fault
3159     def start_instance(self, context, instance):
3160         """Starting an instance on this host."""
3161         self._notify_about_instance_usage(context, instance, "power_on.start")
3162         compute_utils.notify_about_instance_action(context, instance,
3163             self.host, action=fields.NotificationAction.POWER_ON,
3164             phase=fields.NotificationPhase.START)
3165         self._power_on(context, instance)
3166         instance.power_state = self._get_power_state(instance)
3167         instance.vm_state = vm_states.ACTIVE
3168         instance.task_state = None
3169 
3170         # Delete an image(VM snapshot) for a shelved instance
3171         snapshot_id = instance.system_metadata.get('shelved_image_id')
3172         if snapshot_id:
3173             self._delete_snapshot_of_shelved_instance(context, instance,
3174                                                       snapshot_id)
3175 
3176         # Delete system_metadata for a shelved instance
3177         compute_utils.remove_shelved_keys_from_system_metadata(instance)
3178 
3179         instance.save(expected_task_state=task_states.POWERING_ON)
3180         self._notify_about_instance_usage(context, instance, "power_on.end")
3181         compute_utils.notify_about_instance_action(context, instance,
3182             self.host, action=fields.NotificationAction.POWER_ON,
3183             phase=fields.NotificationPhase.END)
3184 
3185     @messaging.expected_exceptions(NotImplementedError,
3186                                    exception.TriggerCrashDumpNotSupported,
3187                                    exception.InstanceNotRunning)
3188     @wrap_exception()
3189     @wrap_instance_event(prefix='compute')
3190     @wrap_instance_fault
3191     def trigger_crash_dump(self, context, instance):
3192         """Trigger crash dump in an instance."""
3193 
3194         self._notify_about_instance_usage(context, instance,
3195                                           "trigger_crash_dump.start")
3196         compute_utils.notify_about_instance_action(context, instance,
3197                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3198                 phase=fields.NotificationPhase.START)
3199 
3200         # This method does not change task_state and power_state because the
3201         # effect of a trigger depends on user's configuration.
3202         self.driver.trigger_crash_dump(instance)
3203 
3204         self._notify_about_instance_usage(context, instance,
3205                                           "trigger_crash_dump.end")
3206         compute_utils.notify_about_instance_action(context, instance,
3207                 self.host, action=fields.NotificationAction.TRIGGER_CRASH_DUMP,
3208                 phase=fields.NotificationPhase.END)
3209 
3210     @wrap_exception()
3211     @reverts_task_state
3212     @wrap_instance_event(prefix='compute')
3213     @wrap_instance_fault
3214     def soft_delete_instance(self, context, instance):
3215         """Soft delete an instance on this host."""
3216         with compute_utils.notify_about_instance_delete(
3217                 self.notifier, context, instance, 'soft_delete',
3218                 source=fields.NotificationSource.COMPUTE):
3219             try:
3220                 self.driver.soft_delete(instance)
3221             except NotImplementedError:
3222                 # Fallback to just powering off the instance if the
3223                 # hypervisor doesn't implement the soft_delete method
3224                 self.driver.power_off(instance)
3225             instance.power_state = self._get_power_state(instance)
3226             instance.vm_state = vm_states.SOFT_DELETED
3227             instance.task_state = None
3228             instance.save(expected_task_state=[task_states.SOFT_DELETING])
3229 
3230     @wrap_exception()
3231     @reverts_task_state
3232     @wrap_instance_event(prefix='compute')
3233     @wrap_instance_fault
3234     def restore_instance(self, context, instance):
3235         """Restore a soft-deleted instance on this host."""
3236         self._notify_about_instance_usage(context, instance, "restore.start")
3237         compute_utils.notify_about_instance_action(context, instance,
3238             self.host, action=fields.NotificationAction.RESTORE,
3239             phase=fields.NotificationPhase.START)
3240         try:
3241             self.driver.restore(instance)
3242         except NotImplementedError:
3243             # Fallback to just powering on the instance if the hypervisor
3244             # doesn't implement the restore method
3245             self._power_on(context, instance)
3246         instance.power_state = self._get_power_state(instance)
3247         instance.vm_state = vm_states.ACTIVE
3248         instance.task_state = None
3249         instance.save(expected_task_state=task_states.RESTORING)
3250         self._notify_about_instance_usage(context, instance, "restore.end")
3251         compute_utils.notify_about_instance_action(context, instance,
3252             self.host, action=fields.NotificationAction.RESTORE,
3253             phase=fields.NotificationPhase.END)
3254 
3255     @staticmethod
3256     def _set_migration_status(migration, status):
3257         """Set the status, and guard against a None being passed in.
3258 
3259         This is useful as some of the compute RPC calls will not pass
3260         a migration object in older versions. The check can be removed when
3261         we move past 4.x major version of the RPC API.
3262         """
3263         if migration:
3264             migration.status = status
3265             migration.save()
3266 
3267     def _rebuild_default_impl(
3268             self, context, instance, image_meta, injected_files,
3269             admin_password, allocations, bdms, detach_block_devices,
3270             attach_block_devices, network_info=None, evacuate=False,
3271             block_device_info=None, preserve_ephemeral=False,
3272             accel_uuids=None):
3273         if preserve_ephemeral:
3274             # The default code path does not support preserving ephemeral
3275             # partitions.
3276             raise exception.PreserveEphemeralNotSupported()
3277 
3278         accel_info = []
3279         if evacuate:
3280             if instance.flavor.extra_specs.get('accel:device_profile'):
3281                 try:
3282                     accel_info = self._get_bound_arq_resources(
3283                         context, instance, accel_uuids or [])
3284                 except (Exception, eventlet.timeout.Timeout) as exc:
3285                     LOG.exception(exc)
3286                     self._build_resources_cleanup(instance, network_info)
3287                     msg = _('Failure getting accelerator resources.')
3288                     raise exception.BuildAbortException(
3289                         instance_uuid=instance.uuid, reason=msg)
3290             detach_block_devices(context, bdms)
3291         else:
3292             self._power_off_instance(instance, clean_shutdown=True)
3293             detach_block_devices(context, bdms)
3294             self.driver.destroy(context, instance,
3295                                 network_info=network_info,
3296                                 block_device_info=block_device_info)
3297             try:
3298                 accel_info = self._get_accel_info(context, instance)
3299             except Exception as exc:
3300                 LOG.exception(exc)
3301                 self._build_resources_cleanup(instance, network_info)
3302                 msg = _('Failure getting accelerator resources.')
3303                 raise exception.BuildAbortException(
3304                     instance_uuid=instance.uuid, reason=msg)
3305 
3306         instance.task_state = task_states.REBUILD_BLOCK_DEVICE_MAPPING
3307         instance.save(expected_task_state=[task_states.REBUILDING])
3308 
3309         new_block_device_info = attach_block_devices(context, instance, bdms)
3310 
3311         instance.task_state = task_states.REBUILD_SPAWNING
3312         instance.save(
3313             expected_task_state=[task_states.REBUILD_BLOCK_DEVICE_MAPPING])
3314 
3315         with instance.mutated_migration_context():
3316             self.driver.spawn(context, instance, image_meta, injected_files,
3317                               admin_password, allocations,
3318                               network_info=network_info,
3319                               block_device_info=new_block_device_info,
3320                               accel_info=accel_info)
3321 
3322     def _notify_instance_rebuild_error(self, context, instance, error, bdms):
3323         self._notify_about_instance_usage(context, instance,
3324                                           'rebuild.error', fault=error)
3325         compute_utils.notify_about_instance_rebuild(
3326             context, instance, self.host,
3327             phase=fields.NotificationPhase.ERROR, exception=error, bdms=bdms)
3328 
3329     @messaging.expected_exceptions(exception.PreserveEphemeralNotSupported,
3330                                    exception.BuildAbortException)
3331     @wrap_exception()
3332     @reverts_task_state
3333     @wrap_instance_event(prefix='compute')
3334     @wrap_instance_fault
3335     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
3336                          injected_files, new_pass, orig_sys_metadata,
3337                          bdms, recreate, on_shared_storage,
3338                          preserve_ephemeral, migration,
3339                          scheduled_node, limits, request_spec, accel_uuids):
3340         """Destroy and re-make this instance.
3341 
3342         A 'rebuild' effectively purges all existing data from the system and
3343         remakes the VM with given 'metadata' and 'personalities'.
3344 
3345         :param context: `nova.RequestContext` object
3346         :param instance: Instance object
3347         :param orig_image_ref: Original image_ref before rebuild
3348         :param image_ref: New image_ref for rebuild
3349         :param injected_files: Files to inject
3350         :param new_pass: password to set on rebuilt instance
3351         :param orig_sys_metadata: instance system metadata from pre-rebuild
3352         :param bdms: block-device-mappings to use for rebuild
3353         :param recreate: True if the instance is being evacuated (e.g. the
3354             hypervisor it was on failed) - cleanup of old state will be
3355             skipped.
3356         :param on_shared_storage: True if instance files on shared storage.
3357                                   If not provided then information from the
3358                                   driver will be used to decide if the instance
3359                                   files are available or not on the target host
3360         :param preserve_ephemeral: True if the default ephemeral storage
3361                                    partition must be preserved on rebuild
3362         :param migration: a Migration object if one was created for this
3363                           rebuild operation (if it's a part of evacuate)
3364         :param scheduled_node: A node of the host chosen by the scheduler. If a
3365                                host was specified by the user, this will be
3366                                None
3367         :param limits: Overcommit limits set by the scheduler. If a host was
3368                        specified by the user, this will be None
3369         :param request_spec: a RequestSpec object used to schedule the instance
3370         :param accel_uuids: a list of cyborg ARQ uuids
3371 
3372         """
3373         # recreate=True means the instance is being evacuated from a failed
3374         # host to a new destination host (this host). The 'recreate' variable
3375         # name is confusing, so rename it to evacuate here at the top, which
3376         # is simpler than renaming a parameter in an RPC versioned method.
3377         evacuate = recreate
3378         context = context.elevated()
3379 
3380         if evacuate:
3381             LOG.info("Evacuating instance", instance=instance)
3382         else:
3383             LOG.info("Rebuilding instance", instance=instance)
3384 
3385         if evacuate:
3386             # This is an evacuation to a new host, so we need to perform a
3387             # resource claim.
3388             rebuild_claim = self.rt.rebuild_claim
3389         else:
3390             # This is a rebuild to the same host, so we don't need to make
3391             # a claim since the instance is already on this host.
3392             rebuild_claim = claims.NopClaim
3393 
3394         if image_ref:
3395             image_meta = objects.ImageMeta.from_image_ref(
3396                 context, self.image_api, image_ref)
3397         elif evacuate:
3398             # For evacuate the API does not send down the image_ref since the
3399             # image does not change so just get it from what was stashed in
3400             # the instance system_metadata when the instance was created (or
3401             # last rebuilt). This also works for volume-backed instances.
3402             image_meta = instance.image_meta
3403         else:
3404             image_meta = objects.ImageMeta()
3405 
3406         # NOTE(mriedem): On an evacuate, we need to update
3407         # the instance's host and node properties to reflect it's
3408         # destination node for the evacuate.
3409         if not scheduled_node:
3410             if evacuate:
3411                 try:
3412                     compute_node = self._get_compute_info(context, self.host)
3413                     scheduled_node = compute_node.hypervisor_hostname
3414                 except exception.ComputeHostNotFound:
3415                     LOG.exception('Failed to get compute_info for %s',
3416                                   self.host)
3417             else:
3418                 scheduled_node = instance.node
3419 
3420         allocs = self.reportclient.get_allocations_for_consumer(
3421                     context, instance.uuid)
3422 
3423         # If the resource claim or group policy validation fails before we
3424         # do anything to the guest or its networking/volumes we want to keep
3425         # the current status rather than put the instance into ERROR status.
3426         instance_state = instance.vm_state
3427         with self._error_out_instance_on_exception(
3428                 context, instance, instance_state=instance_state):
3429             try:
3430                 self._do_rebuild_instance_with_claim(
3431                     context, instance, orig_image_ref,
3432                     image_meta, injected_files, new_pass, orig_sys_metadata,
3433                     bdms, evacuate, on_shared_storage, preserve_ephemeral,
3434                     migration, request_spec, allocs, rebuild_claim,
3435                     scheduled_node, limits, accel_uuids)
3436             except (exception.ComputeResourcesUnavailable,
3437                     exception.RescheduledException) as e:
3438                 if isinstance(e, exception.ComputeResourcesUnavailable):
3439                     LOG.debug("Could not rebuild instance on this host, not "
3440                               "enough resources available.", instance=instance)
3441                 else:
3442                     # RescheduledException is raised by the late server group
3443                     # policy check during evacuation if a parallel scheduling
3444                     # violated the policy.
3445                     # We catch the RescheduledException here but we don't have
3446                     # the plumbing to do an actual reschedule so we abort the
3447                     # operation.
3448                     LOG.debug("Could not rebuild instance on this host, "
3449                               "late server group check failed.",
3450                               instance=instance)
3451                 # NOTE(ndipanov): We just abort the build for now and leave a
3452                 # migration record for potential cleanup later
3453                 self._set_migration_status(migration, 'failed')
3454                 # Since the claim failed, we need to remove the allocation
3455                 # created against the destination node. Note that we can only
3456                 # get here when evacuating to a destination node. Rebuilding
3457                 # on the same host (not evacuate) uses the NopClaim which will
3458                 # not raise ComputeResourcesUnavailable.
3459                 self.rt.delete_allocation_for_evacuated_instance(
3460                     context, instance, scheduled_node, node_type='destination')
3461                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3462                 # Wrap this in InstanceFaultRollback so that the
3463                 # _error_out_instance_on_exception context manager keeps the
3464                 # vm_state unchanged.
3465                 raise exception.InstanceFaultRollback(
3466                     inner_exception=exception.BuildAbortException(
3467                         instance_uuid=instance.uuid,
3468                         reason=e.format_message()))
3469             except (exception.InstanceNotFound,
3470                     exception.UnexpectedDeletingTaskStateError) as e:
3471                 LOG.debug('Instance was deleted while rebuilding',
3472                           instance=instance)
3473                 self._set_migration_status(migration, 'failed')
3474                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3475             except Exception as e:
3476                 self._set_migration_status(migration, 'failed')
3477                 if evacuate or scheduled_node is not None:
3478                     self.rt.delete_allocation_for_evacuated_instance(
3479                         context, instance, scheduled_node,
3480                         node_type='destination')
3481                 self._notify_instance_rebuild_error(context, instance, e, bdms)
3482                 raise
3483             else:
3484                 # NOTE(gibi): Let the resource tracker set the instance
3485                 # host and drop the migration context as we need to hold the
3486                 # COMPUTE_RESOURCE_SEMAPHORE to avoid the race with
3487                 # _update_available_resources. See bug 1896463.
3488                 self.rt.finish_evacuation(instance, scheduled_node, migration)
3489 
3490     def _do_rebuild_instance_with_claim(
3491             self, context, instance, orig_image_ref, image_meta,
3492             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3493             on_shared_storage, preserve_ephemeral, migration, request_spec,
3494             allocations, rebuild_claim, scheduled_node, limits, accel_uuids):
3495         """Helper to avoid deep nesting in the top-level method."""
3496 
3497         provider_mapping = None
3498         if evacuate:
3499             provider_mapping = self._get_request_group_mapping(request_spec)
3500 
3501             if provider_mapping:
3502                 compute_utils.\
3503                     update_pci_request_spec_with_allocated_interface_name(
3504                         context, self.reportclient,
3505                         instance.pci_requests.requests, provider_mapping)
3506 
3507         claim_context = rebuild_claim(
3508             context, instance, scheduled_node, allocations,
3509             limits=limits, image_meta=image_meta, migration=migration)
3510 
3511         with claim_context:
3512             self._do_rebuild_instance(
3513                 context, instance, orig_image_ref, image_meta, injected_files,
3514                 new_pass, orig_sys_metadata, bdms, evacuate, on_shared_storage,
3515                 preserve_ephemeral, migration, request_spec, allocations,
3516                 provider_mapping, accel_uuids)
3517 
3518     @staticmethod
3519     def _get_image_name(image_meta):
3520         if image_meta.obj_attr_is_set("name"):
3521             return image_meta.name
3522         else:
3523             return ''
3524 
3525     def _do_rebuild_instance(
3526             self, context, instance, orig_image_ref, image_meta,
3527             injected_files, new_pass, orig_sys_metadata, bdms, evacuate,
3528             on_shared_storage, preserve_ephemeral, migration, request_spec,
3529             allocations, request_group_resource_providers_mapping,
3530             accel_uuids):
3531         orig_vm_state = instance.vm_state
3532 
3533         if evacuate:
3534             if request_spec:
3535                 # NOTE(gibi): Do a late check of server group policy as
3536                 # parallel scheduling could violate such policy. This will
3537                 # cause the evacuate to fail as rebuild does not implement
3538                 # reschedule.
3539                 hints = self._get_scheduler_hints({}, request_spec)
3540                 self._validate_instance_group_policy(context, instance, hints)
3541 
3542             if not self.driver.capabilities.get("supports_evacuate", False):
3543                 raise exception.InstanceEvacuateNotSupported
3544 
3545             self._check_instance_exists(instance)
3546 
3547             if on_shared_storage is None:
3548                 LOG.debug('on_shared_storage is not provided, using driver '
3549                           'information to decide if the instance needs to '
3550                           'be evacuated')
3551                 on_shared_storage = self.driver.instance_on_disk(instance)
3552 
3553             elif (on_shared_storage !=
3554                     self.driver.instance_on_disk(instance)):
3555                 # To cover case when admin expects that instance files are
3556                 # on shared storage, but not accessible and vice versa
3557                 raise exception.InvalidSharedStorage(
3558                         _("Invalid state of instance files on shared"
3559                             " storage"))
3560 
3561             if on_shared_storage:
3562                 LOG.info('disk on shared storage, evacuating using'
3563                          ' existing disk')
3564             elif instance.image_ref:
3565                 orig_image_ref = instance.image_ref
3566                 LOG.info("disk not on shared storage, evacuating from "
3567                          "image: '%s'", str(orig_image_ref))
3568             else:
3569                 LOG.info('disk on volume, evacuating using existing '
3570                          'volume')
3571 
3572         # We check trusted certs capabilities for both evacuate (rebuild on
3573         # another host) and rebuild (rebuild on the same host) because for
3574         # evacuate we need to make sure an instance with trusted certs can
3575         # have the image verified with those certs during rebuild, and for
3576         # rebuild we could be rebuilding a server that started out with no
3577         # trusted certs on this host, and then was rebuilt with trusted certs
3578         # for a new image, in which case we need to validate that new image
3579         # with the trusted certs during the rebuild.
3580         self._check_trusted_certs(instance)
3581 
3582         # This instance.exists message should contain the original
3583         # image_ref, not the new one.  Since the DB has been updated
3584         # to point to the new one... we have to override it.
3585         orig_image_ref_url = self.image_api.generate_image_url(orig_image_ref,
3586                                                                context)
3587         extra_usage_info = {'image_ref_url': orig_image_ref_url}
3588         compute_utils.notify_usage_exists(
3589                 self.notifier, context, instance, self.host,
3590                 current_period=True, system_metadata=orig_sys_metadata,
3591                 extra_usage_info=extra_usage_info)
3592 
3593         # This message should contain the new image_ref
3594         extra_usage_info = {'image_name': self._get_image_name(image_meta)}
3595         self._notify_about_instance_usage(context, instance,
3596                 "rebuild.start", extra_usage_info=extra_usage_info)
3597         # NOTE: image_name is not included in the versioned notification
3598         # because we already provide the image_uuid in the notification
3599         # payload and the image details can be looked up via the uuid.
3600         compute_utils.notify_about_instance_rebuild(
3601             context, instance, self.host,
3602             phase=fields.NotificationPhase.START,
3603             bdms=bdms)
3604 
3605         instance.power_state = self._get_power_state(instance)
3606         instance.task_state = task_states.REBUILDING
3607         instance.save(expected_task_state=[task_states.REBUILDING])
3608 
3609         if evacuate:
3610             self.network_api.setup_networks_on_host(
3611                     context, instance, self.host)
3612             # For nova-network this is needed to move floating IPs
3613             # For neutron this updates the host in the port binding
3614             # TODO(cfriesen): this network_api call and the one above
3615             # are so similar, we should really try to unify them.
3616             self.network_api.setup_instance_network_on_host(
3617                 context, instance, self.host, migration,
3618                 provider_mappings=request_group_resource_providers_mapping)
3619             # TODO(mriedem): Consider decorating setup_instance_network_on_host
3620             # with @api.refresh_cache and then we wouldn't need this explicit
3621             # call to get_instance_nw_info.
3622             network_info = self.network_api.get_instance_nw_info(context,
3623                                                                  instance)
3624         else:
3625             network_info = instance.get_network_info()
3626 
3627         if bdms is None:
3628             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3629                     context, instance.uuid)
3630 
3631         block_device_info = \
3632             self._get_instance_block_device_info(
3633                     context, instance, bdms=bdms)
3634 
3635         def detach_block_devices(context, bdms):
3636             for bdm in bdms:
3637                 if bdm.is_volume:
3638                     # NOTE (ildikov): Having the attachment_id set in the BDM
3639                     # means that it's the new Cinder attach/detach flow
3640                     # (available from v3.44). In that case we explicitly
3641                     # attach and detach the volumes through attachment level
3642                     # operations. In this scenario _detach_volume will delete
3643                     # the existing attachment which would make the volume
3644                     # status change to 'available' if we don't pre-create
3645                     # another empty attachment before deleting the old one.
3646                     attachment_id = None
3647                     if bdm.attachment_id:
3648                         attachment_id = self.volume_api.attachment_create(
3649                             context, bdm['volume_id'], instance.uuid)['id']
3650                     self._detach_volume(context, bdm, instance,
3651                                         destroy_bdm=False)
3652                     if attachment_id:
3653                         bdm.attachment_id = attachment_id
3654                         bdm.save()
3655 
3656         files = self._decode_files(injected_files)
3657 
3658         kwargs = dict(
3659             context=context,
3660             instance=instance,
3661             image_meta=image_meta,
3662             injected_files=files,
3663             admin_password=new_pass,
3664             allocations=allocations,
3665             bdms=bdms,
3666             detach_block_devices=detach_block_devices,
3667             attach_block_devices=self._prep_block_device,
3668             block_device_info=block_device_info,
3669             network_info=network_info,
3670             preserve_ephemeral=preserve_ephemeral,
3671             evacuate=evacuate,
3672             accel_uuids=accel_uuids)
3673         try:
3674             with instance.mutated_migration_context():
3675                 self.driver.rebuild(**kwargs)
3676         except NotImplementedError:
3677             # NOTE(rpodolyaka): driver doesn't provide specialized version
3678             # of rebuild, fall back to the default implementation
3679             self._rebuild_default_impl(**kwargs)
3680         self._update_instance_after_spawn(instance)
3681         instance.save(expected_task_state=[task_states.REBUILD_SPAWNING])
3682 
3683         if orig_vm_state == vm_states.STOPPED:
3684             LOG.info("bringing vm to original state: '%s'",
3685                      orig_vm_state, instance=instance)
3686             instance.vm_state = vm_states.ACTIVE
3687             instance.task_state = task_states.POWERING_OFF
3688             instance.progress = 0
3689             instance.save()
3690             self.stop_instance(context, instance, False)
3691         # TODO(melwitt): We should clean up instance console tokens here in the
3692         # case of evacuate. The instance is on a new host and will need to
3693         # establish a new console connection.
3694         self._update_scheduler_instance_info(context, instance)
3695         self._notify_about_instance_usage(
3696                 context, instance, "rebuild.end",
3697                 network_info=network_info,
3698                 extra_usage_info=extra_usage_info)
3699         compute_utils.notify_about_instance_rebuild(
3700             context, instance, self.host,
3701             phase=fields.NotificationPhase.END,
3702             bdms=bdms)
3703 
3704     def _handle_bad_volumes_detached(self, context, instance, bad_devices,
3705                                      block_device_info):
3706         """Handle cases where the virt-layer had to detach non-working volumes
3707         in order to complete an operation.
3708         """
3709         for bdm in block_device_info['block_device_mapping']:
3710             if bdm.get('mount_device') in bad_devices:
3711                 try:
3712                     volume_id = bdm['connection_info']['data']['volume_id']
3713                 except KeyError:
3714                     continue
3715 
3716                 # NOTE(sirp): ideally we'd just call
3717                 # `compute_api.detach_volume` here but since that hits the
3718                 # DB directly, that's off limits from within the
3719                 # compute-manager.
3720                 #
3721                 # API-detach
3722                 LOG.info("Detaching from volume api: %s", volume_id)
3723                 self.volume_api.begin_detaching(context, volume_id)
3724 
3725                 # Manager-detach
3726                 self.detach_volume(context, volume_id, instance)
3727 
3728     def _get_accel_info(self, context, instance):
3729         dp_name = instance.flavor.extra_specs.get('accel:device_profile')
3730         if dp_name:
3731             cyclient = cyborg.get_client(context)
3732             accel_info = cyclient.get_arqs_for_instance(instance.uuid)
3733         else:
3734             accel_info = []
3735         return accel_info
3736 
3737     @wrap_exception()
3738     @reverts_task_state
3739     @wrap_instance_event(prefix='compute')
3740     @wrap_instance_fault
3741     def reboot_instance(self, context, instance, block_device_info,
3742                         reboot_type):
3743         @utils.synchronized(instance.uuid)
3744         def do_reboot_instance(context, instance, block_device_info,
3745                                reboot_type):
3746             self._reboot_instance(context, instance, block_device_info,
3747                                   reboot_type)
3748         do_reboot_instance(context, instance, block_device_info, reboot_type)
3749 
3750     def _reboot_instance(self, context, instance, block_device_info,
3751                          reboot_type):
3752         """Reboot an instance on this host."""
3753         # acknowledge the request made it to the manager
3754         if reboot_type == "SOFT":
3755             instance.task_state = task_states.REBOOT_PENDING
3756             expected_states = task_states.soft_reboot_states
3757         else:
3758             instance.task_state = task_states.REBOOT_PENDING_HARD
3759             expected_states = task_states.hard_reboot_states
3760 
3761         context = context.elevated()
3762         LOG.info("Rebooting instance", instance=instance)
3763 
3764         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
3765             context, instance.uuid)
3766         block_device_info = self._get_instance_block_device_info(
3767             context, instance, bdms=bdms)
3768 
3769         network_info = self.network_api.get_instance_nw_info(context, instance)
3770 
3771         accel_info = self._get_accel_info(context, instance)
3772 
3773         self._notify_about_instance_usage(context, instance, "reboot.start")
3774         compute_utils.notify_about_instance_action(
3775             context, instance, self.host,
3776             action=fields.NotificationAction.REBOOT,
3777             phase=fields.NotificationPhase.START,
3778             bdms=bdms
3779         )
3780 
3781         instance.power_state = self._get_power_state(instance)
3782         instance.save(expected_task_state=expected_states)
3783 
3784         if instance.power_state != power_state.RUNNING:
3785             state = instance.power_state
3786             running = power_state.RUNNING
3787             LOG.warning('trying to reboot a non-running instance:'
3788                         ' (state: %(state)s expected: %(running)s)',
3789                         {'state': state, 'running': running},
3790                         instance=instance)
3791 
3792         def bad_volumes_callback(bad_devices):
3793             self._handle_bad_volumes_detached(
3794                     context, instance, bad_devices, block_device_info)
3795 
3796         try:
3797             # Don't change it out of rescue mode
3798             if instance.vm_state == vm_states.RESCUED:
3799                 new_vm_state = vm_states.RESCUED
3800             else:
3801                 new_vm_state = vm_states.ACTIVE
3802             new_power_state = None
3803             if reboot_type == "SOFT":
3804                 instance.task_state = task_states.REBOOT_STARTED
3805                 expected_state = task_states.REBOOT_PENDING
3806             else:
3807                 instance.task_state = task_states.REBOOT_STARTED_HARD
3808                 expected_state = task_states.REBOOT_PENDING_HARD
3809             instance.save(expected_task_state=expected_state)
3810             self.driver.reboot(context, instance,
3811                                network_info,
3812                                reboot_type,
3813                                block_device_info=block_device_info,
3814                                accel_info=accel_info,
3815                                bad_volumes_callback=bad_volumes_callback)
3816 
3817         except Exception as error:
3818             with excutils.save_and_reraise_exception() as ctxt:
3819                 exc_info = sys.exc_info()
3820                 # if the reboot failed but the VM is running don't
3821                 # put it into an error state
3822                 new_power_state = self._get_power_state(instance)
3823                 if new_power_state == power_state.RUNNING:
3824                     LOG.warning('Reboot failed but instance is running',
3825                                 instance=instance)
3826                     compute_utils.add_instance_fault_from_exc(context,
3827                             instance, error, exc_info)
3828                     self._notify_about_instance_usage(context, instance,
3829                             'reboot.error', fault=error)
3830                     compute_utils.notify_about_instance_action(
3831                         context, instance, self.host,
3832                         action=fields.NotificationAction.REBOOT,
3833                         phase=fields.NotificationPhase.ERROR,
3834                         exception=error, bdms=bdms
3835                     )
3836                     ctxt.reraise = False
3837                 else:
3838                     LOG.error('Cannot reboot instance: %s', error,
3839                               instance=instance)
3840                     self._set_instance_obj_error_state(instance)
3841 
3842         if not new_power_state:
3843             new_power_state = self._get_power_state(instance)
3844         try:
3845             instance.power_state = new_power_state
3846             instance.vm_state = new_vm_state
3847             instance.task_state = None
3848             instance.save()
3849         except exception.InstanceNotFound:
3850             LOG.warning("Instance disappeared during reboot",
3851                         instance=instance)
3852 
3853         self._notify_about_instance_usage(context, instance, "reboot.end")
3854         compute_utils.notify_about_instance_action(
3855             context, instance, self.host,
3856             action=fields.NotificationAction.REBOOT,
3857             phase=fields.NotificationPhase.END,
3858             bdms=bdms
3859         )
3860 
3861     @delete_image_on_error
3862     def _do_snapshot_instance(self, context, image_id, instance):
3863         self._snapshot_instance(context, image_id, instance,
3864                                 task_states.IMAGE_BACKUP)
3865 
3866     @wrap_exception()
3867     @reverts_task_state
3868     @wrap_instance_event(prefix='compute')
3869     @wrap_instance_fault
3870     def backup_instance(self, context, image_id, instance, backup_type,
3871                         rotation):
3872         """Backup an instance on this host.
3873 
3874         :param backup_type: daily | weekly
3875         :param rotation: int representing how many backups to keep around
3876         """
3877         self._do_snapshot_instance(context, image_id, instance)
3878         self._rotate_backups(context, instance, backup_type, rotation)
3879 
3880     @wrap_exception()
3881     @reverts_task_state
3882     @wrap_instance_event(prefix='compute')
3883     @wrap_instance_fault
3884     @delete_image_on_error
3885     def snapshot_instance(self, context, image_id, instance):
3886         """Snapshot an instance on this host.
3887 
3888         :param context: security context
3889         :param image_id: glance.db.sqlalchemy.models.Image.Id
3890         :param instance: a nova.objects.instance.Instance object
3891         """
3892         # NOTE(dave-mcnally) the task state will already be set by the api
3893         # but if the compute manager has crashed/been restarted prior to the
3894         # request getting here the task state may have been cleared so we set
3895         # it again and things continue normally
3896         try:
3897             instance.task_state = task_states.IMAGE_SNAPSHOT
3898             instance.save(
3899                         expected_task_state=task_states.IMAGE_SNAPSHOT_PENDING)
3900         except exception.InstanceNotFound:
3901             # possibility instance no longer exists, no point in continuing
3902             LOG.debug("Instance not found, could not set state %s "
3903                       "for instance.",
3904                       task_states.IMAGE_SNAPSHOT, instance=instance)
3905             return
3906 
3907         except exception.UnexpectedDeletingTaskStateError:
3908             LOG.debug("Instance being deleted, snapshot cannot continue",
3909                       instance=instance)
3910             return
3911 
3912         with self._snapshot_semaphore:
3913             self._snapshot_instance(context, image_id, instance,
3914                                     task_states.IMAGE_SNAPSHOT)
3915 
3916     def _snapshot_instance(self, context, image_id, instance,
3917                            expected_task_state):
3918         context = context.elevated()
3919 
3920         instance.power_state = self._get_power_state(instance)
3921         try:
3922             instance.save()
3923 
3924             LOG.info('instance snapshotting', instance=instance)
3925 
3926             if instance.power_state != power_state.RUNNING:
3927                 state = instance.power_state
3928                 running = power_state.RUNNING
3929                 LOG.warning('trying to snapshot a non-running instance: '
3930                             '(state: %(state)s expected: %(running)s)',
3931                             {'state': state, 'running': running},
3932                             instance=instance)
3933 
3934             self._notify_about_instance_usage(
3935                 context, instance, "snapshot.start")
3936             compute_utils.notify_about_instance_snapshot(context, instance,
3937                 self.host, phase=fields.NotificationPhase.START,
3938                 snapshot_image_id=image_id)
3939 
3940             def update_task_state(task_state,
3941                                   expected_state=expected_task_state):
3942                 instance.task_state = task_state
3943                 instance.save(expected_task_state=expected_state)
3944 
3945             with timeutils.StopWatch() as timer:
3946                 self.driver.snapshot(context, instance, image_id,
3947                                      update_task_state)
3948             LOG.info('Took %0.2f seconds to snapshot the instance on '
3949                      'the hypervisor.', timer.elapsed(), instance=instance)
3950 
3951             instance.task_state = None
3952             instance.save(expected_task_state=task_states.IMAGE_UPLOADING)
3953 
3954             self._notify_about_instance_usage(context, instance,
3955                                               "snapshot.end")
3956             compute_utils.notify_about_instance_snapshot(context, instance,
3957                 self.host, phase=fields.NotificationPhase.END,
3958                 snapshot_image_id=image_id)
3959         except (exception.InstanceNotFound,
3960                 exception.InstanceNotRunning,
3961                 exception.UnexpectedDeletingTaskStateError):
3962             # the instance got deleted during the snapshot
3963             # Quickly bail out of here
3964             msg = 'Instance disappeared during snapshot'
3965             LOG.debug(msg, instance=instance)
3966             try:
3967                 image = self.image_api.get(context, image_id)
3968                 if image['status'] != 'active':
3969                     self.image_api.delete(context, image_id)
3970             except exception.ImageNotFound:
3971                 LOG.debug('Image not found during clean up %s', image_id)
3972             except Exception:
3973                 LOG.warning("Error while trying to clean up image %s",
3974                             image_id, instance=instance)
3975         except exception.ImageNotFound:
3976             instance.task_state = None
3977             instance.save()
3978             LOG.warning("Image not found during snapshot", instance=instance)
3979 
3980     @messaging.expected_exceptions(NotImplementedError)
3981     @wrap_exception()
3982     def volume_snapshot_create(self, context, instance, volume_id,
3983                                create_info):
3984         try:
3985             self.driver.volume_snapshot_create(context, instance, volume_id,
3986                                                create_info)
3987         except exception.InstanceNotRunning:
3988             # Libvirt driver can raise this exception
3989             LOG.debug('Instance disappeared during volume snapshot create',
3990                       instance=instance)
3991 
3992     @messaging.expected_exceptions(NotImplementedError)
3993     @wrap_exception()
3994     def volume_snapshot_delete(self, context, instance, volume_id,
3995                                snapshot_id, delete_info):
3996         try:
3997             self.driver.volume_snapshot_delete(context, instance, volume_id,
3998                                                snapshot_id, delete_info)
3999         except exception.InstanceNotRunning:
4000             # Libvirt driver can raise this exception
4001             LOG.debug('Instance disappeared during volume snapshot delete',
4002                       instance=instance)
4003 
4004     @wrap_instance_fault
4005     def _rotate_backups(self, context, instance, backup_type, rotation):
4006         """Delete excess backups associated to an instance.
4007 
4008         Instances are allowed a fixed number of backups (the rotation number);
4009         this method deletes the oldest backups that exceed the rotation
4010         threshold.
4011 
4012         :param context: security context
4013         :param instance: Instance dict
4014         :param backup_type: a user-defined type, like "daily" or "weekly" etc.
4015         :param rotation: int representing how many backups to keep around;
4016             None if rotation shouldn't be used (as in the case of snapshots)
4017         """
4018         filters = {'property-image_type': 'backup',
4019                    'property-backup_type': backup_type,
4020                    'property-instance_uuid': instance.uuid}
4021 
4022         images = self.image_api.get_all(context, filters=filters,
4023                                         sort_key='created_at', sort_dir='desc')
4024         num_images = len(images)
4025         LOG.debug("Found %(num_images)d images (rotation: %(rotation)d)",
4026                   {'num_images': num_images, 'rotation': rotation},
4027                   instance=instance)
4028 
4029         if num_images > rotation:
4030             # NOTE(sirp): this deletes all backups that exceed the rotation
4031             # limit
4032             excess = len(images) - rotation
4033             LOG.debug("Rotating out %d backups", excess,
4034                       instance=instance)
4035             for i in range(excess):
4036                 image = images.pop()
4037                 image_id = image['id']
4038                 LOG.debug("Deleting image %s", image_id,
4039                           instance=instance)
4040                 try:
4041                     self.image_api.delete(context, image_id)
4042                 except exception.ImageNotFound:
4043                     LOG.info("Failed to find image %(image_id)s to "
4044                              "delete", {'image_id': image_id},
4045                              instance=instance)
4046                 except (exception.ImageDeleteConflict, Exception) as exc:
4047                     LOG.info("Failed to delete image %(image_id)s during "
4048                              "deleting excess backups. "
4049                              "Continuing for next image.. %(exc)s",
4050                              {'image_id': image_id, 'exc': exc},
4051                              instance=instance)
4052 
4053     @wrap_exception()
4054     @reverts_task_state
4055     @wrap_instance_event(prefix='compute')
4056     @wrap_instance_fault
4057     def set_admin_password(self, context, instance, new_pass):
4058         """Set the root/admin password for an instance on this host.
4059 
4060         This is generally only called by API password resets after an
4061         image has been built.
4062 
4063         @param context: Nova auth context.
4064         @param instance: Nova instance object.
4065         @param new_pass: The admin password for the instance.
4066         """
4067 
4068         context = context.elevated()
4069         current_power_state = self._get_power_state(instance)
4070         expected_state = power_state.RUNNING
4071 
4072         if current_power_state != expected_state:
4073             instance.task_state = None
4074             instance.save(expected_task_state=task_states.UPDATING_PASSWORD)
4075             _msg = _('instance %s is not running') % instance.uuid
4076             raise exception.InstancePasswordSetFailed(
4077                 instance=instance.uuid, reason=_msg)
4078 
4079         try:
4080             self.driver.set_admin_password(instance, new_pass)
4081             LOG.info("Admin password set", instance=instance)
4082             instance.task_state = None
4083             instance.save(
4084                 expected_task_state=task_states.UPDATING_PASSWORD)
4085         except exception.InstanceAgentNotEnabled:
4086             with excutils.save_and_reraise_exception():
4087                 LOG.debug('Guest agent is not enabled for the instance.',
4088                           instance=instance)
4089                 instance.task_state = None
4090                 instance.save(
4091                     expected_task_state=task_states.UPDATING_PASSWORD)
4092         except exception.SetAdminPasswdNotSupported:
4093             with excutils.save_and_reraise_exception():
4094                 LOG.info('set_admin_password is not supported '
4095                          'by this driver or guest instance.',
4096                          instance=instance)
4097                 instance.task_state = None
4098                 instance.save(
4099                     expected_task_state=task_states.UPDATING_PASSWORD)
4100         except NotImplementedError:
4101             LOG.warning('set_admin_password is not implemented '
4102                         'by this driver or guest instance.',
4103                         instance=instance)
4104             instance.task_state = None
4105             instance.save(
4106                 expected_task_state=task_states.UPDATING_PASSWORD)
4107             raise NotImplementedError(_('set_admin_password is not '
4108                                         'implemented by this driver or guest '
4109                                         'instance.'))
4110         except exception.UnexpectedTaskStateError:
4111             # interrupted by another (most likely delete) task
4112             # do not retry
4113             raise
4114         except Exception:
4115             # Catch all here because this could be anything.
4116             LOG.exception('set_admin_password failed', instance=instance)
4117             # We create a new exception here so that we won't
4118             # potentially reveal password information to the
4119             # API caller.  The real exception is logged above
4120             _msg = _('error setting admin password')
4121             raise exception.InstancePasswordSetFailed(
4122                 instance=instance.uuid, reason=_msg)
4123 
4124     def _get_rescue_image(self, context, instance, rescue_image_ref=None):
4125         """Determine what image should be used to boot the rescue VM."""
4126         # 1. If rescue_image_ref is passed in, use that for rescue.
4127         # 2. Else, use the base image associated with instance's current image.
4128         #       The idea here is to provide the customer with a rescue
4129         #       environment which they are familiar with.
4130         #       So, if they built their instance off of a Debian image,
4131         #       their rescue VM will also be Debian.
4132         # 3. As a last resort, use instance's current image.
4133         if not rescue_image_ref:
4134             system_meta = utils.instance_sys_meta(instance)
4135             rescue_image_ref = system_meta.get('image_base_image_ref')
4136 
4137         if not rescue_image_ref:
4138             LOG.warning('Unable to find a different image to use for '
4139                         'rescue VM, using instance\'s current image',
4140                         instance=instance)
4141             rescue_image_ref = instance.image_ref
4142 
4143         return objects.ImageMeta.from_image_ref(
4144             context, self.image_api, rescue_image_ref)
4145 
4146     @wrap_exception()
4147     @reverts_task_state
4148     @wrap_instance_event(prefix='compute')
4149     @wrap_instance_fault
4150     def rescue_instance(self, context, instance, rescue_password,
4151                         rescue_image_ref, clean_shutdown):
4152         context = context.elevated()
4153         LOG.info('Rescuing', instance=instance)
4154 
4155         admin_password = (rescue_password if rescue_password else
4156                       utils.generate_password())
4157 
4158         network_info = self.network_api.get_instance_nw_info(context, instance)
4159 
4160         rescue_image_meta = self._get_rescue_image(context, instance,
4161                                                    rescue_image_ref)
4162 
4163         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4164                                               context, instance.uuid)
4165         block_device_info = self._get_instance_block_device_info(
4166                                 context, instance, bdms=bdms)
4167 
4168         extra_usage_info = {'rescue_image_name':
4169                             self._get_image_name(rescue_image_meta)}
4170         self._notify_about_instance_usage(context, instance,
4171                 "rescue.start", extra_usage_info=extra_usage_info,
4172                 network_info=network_info)
4173         compute_utils.notify_about_instance_rescue_action(
4174             context, instance, self.host, rescue_image_ref,
4175             phase=fields.NotificationPhase.START)
4176 
4177         try:
4178             self._power_off_instance(instance, clean_shutdown)
4179 
4180             self.driver.rescue(context, instance, network_info,
4181                                rescue_image_meta, admin_password,
4182                                block_device_info)
4183         except Exception as e:
4184             LOG.exception("Error trying to Rescue Instance",
4185                           instance=instance)
4186             self._set_instance_obj_error_state(instance)
4187             raise exception.InstanceNotRescuable(
4188                 instance_id=instance.uuid,
4189                 reason=_("Driver Error: %s") % e)
4190 
4191         compute_utils.notify_usage_exists(self.notifier, context, instance,
4192                                           self.host, current_period=True)
4193 
4194         instance.vm_state = vm_states.RESCUED
4195         instance.task_state = None
4196         instance.power_state = self._get_power_state(instance)
4197         instance.launched_at = timeutils.utcnow()
4198         instance.save(expected_task_state=task_states.RESCUING)
4199 
4200         self._notify_about_instance_usage(context, instance,
4201                 "rescue.end", extra_usage_info=extra_usage_info,
4202                 network_info=network_info)
4203         compute_utils.notify_about_instance_rescue_action(
4204             context, instance, self.host, rescue_image_ref,
4205             phase=fields.NotificationPhase.END)
4206 
4207     @wrap_exception()
4208     @reverts_task_state
4209     @wrap_instance_event(prefix='compute')
4210     @wrap_instance_fault
4211     def unrescue_instance(self, context, instance):
4212         orig_context = context
4213         context = context.elevated()
4214         LOG.info('Unrescuing', instance=instance)
4215 
4216         network_info = self.network_api.get_instance_nw_info(context, instance)
4217         self._notify_about_instance_usage(context, instance,
4218                 "unrescue.start", network_info=network_info)
4219         compute_utils.notify_about_instance_action(context, instance,
4220             self.host, action=fields.NotificationAction.UNRESCUE,
4221             phase=fields.NotificationPhase.START)
4222 
4223         with self._error_out_instance_on_exception(context, instance):
4224             self.driver.unrescue(orig_context, instance)
4225 
4226         instance.vm_state = vm_states.ACTIVE
4227         instance.task_state = None
4228         instance.power_state = self._get_power_state(instance)
4229         instance.save(expected_task_state=task_states.UNRESCUING)
4230 
4231         self._notify_about_instance_usage(context,
4232                                           instance,
4233                                           "unrescue.end",
4234                                           network_info=network_info)
4235         compute_utils.notify_about_instance_action(context, instance,
4236             self.host, action=fields.NotificationAction.UNRESCUE,
4237             phase=fields.NotificationPhase.END)
4238 
4239     @wrap_exception()
4240     @wrap_instance_event(prefix='compute')
4241     @errors_out_migration
4242     @wrap_instance_fault
4243     def confirm_resize(self, context, instance, migration):
4244         """Confirms a migration/resize and deletes the 'old' instance.
4245 
4246         This is called from the API and runs on the source host.
4247 
4248         Nothing needs to happen on the destination host at this point since
4249         the instance is already running there. This routine just cleans up the
4250         source host.
4251         """
4252         @utils.synchronized(instance.uuid)
4253         def do_confirm_resize(context, instance, migration):
4254             LOG.debug("Going to confirm migration %s", migration.id,
4255                       instance=instance)
4256 
4257             if migration.status == 'confirmed':
4258                 LOG.info("Migration %s is already confirmed",
4259                          migration.id, instance=instance)
4260                 return
4261 
4262             if migration.status not in ('finished', 'confirming'):
4263                 LOG.warning("Unexpected confirmation status '%(status)s' "
4264                             "of migration %(id)s, exit confirmation process",
4265                             {"status": migration.status, "id": migration.id},
4266                             instance=instance)
4267                 return
4268 
4269             # NOTE(wangpan): Get the instance from db, if it has been
4270             #                deleted, we do nothing and return here
4271             expected_attrs = ['metadata', 'system_metadata', 'flavor']
4272             try:
4273                 instance = objects.Instance.get_by_uuid(
4274                         context, instance.uuid,
4275                         expected_attrs=expected_attrs)
4276             except exception.InstanceNotFound:
4277                 LOG.info("Instance is not found during confirmation",
4278                          instance=instance)
4279                 return
4280 
4281             with self._error_out_instance_on_exception(context, instance):
4282                 try:
4283                     self._confirm_resize(
4284                         context, instance, migration=migration)
4285                 except Exception:
4286                     # Something failed when cleaning up the source host so
4287                     # log a traceback and leave a hint about hard rebooting
4288                     # the server to correct its state in the DB.
4289                     with excutils.save_and_reraise_exception(logger=LOG):
4290                         LOG.exception(
4291                             'Confirm resize failed on source host %s. '
4292                             'Resource allocations in the placement service '
4293                             'will be removed regardless because the instance '
4294                             'is now on the destination host %s. You can try '
4295                             'hard rebooting the instance to correct its '
4296                             'state.', self.host, migration.dest_compute,
4297                             instance=instance)
4298                 finally:
4299                     # Whether an error occurred or not, at this point the
4300                     # instance is on the dest host. Avoid leaking allocations
4301                     # in placement by deleting them here...
4302                     self._delete_allocation_after_move(
4303                         context, instance, migration)
4304                     # ...inform the scheduler about the move...
4305                     self._delete_scheduler_instance_info(
4306                         context, instance.uuid)
4307                     # ...and unset the cached flavor information (this is done
4308                     # last since the resource tracker relies on it for its
4309                     # periodic tasks)
4310                     self._delete_stashed_flavor_info(instance)
4311 
4312         do_confirm_resize(context, instance, migration)
4313 
4314     def _get_updated_nw_info_with_pci_mapping(self, nw_info, pci_mapping):
4315         # NOTE(adrianc): This method returns a copy of nw_info if modifications
4316         # are made else it returns the original nw_info.
4317         updated_nw_info = nw_info
4318         if nw_info and pci_mapping:
4319             updated_nw_info = copy.deepcopy(nw_info)
4320             for vif in updated_nw_info:
4321                 if vif['vnic_type'] in network_model.VNIC_TYPES_SRIOV:
4322                     try:
4323                         vif_pci_addr = vif['profile']['pci_slot']
4324                         new_addr = pci_mapping[vif_pci_addr].address
4325                         vif['profile']['pci_slot'] = new_addr
4326                         LOG.debug("Updating VIF's PCI address for VIF %(id)s. "
4327                                   "Original value %(orig_val)s, "
4328                                   "new value %(new_val)s",
4329                                   {'id': vif['id'],
4330                                    'orig_val': vif_pci_addr,
4331                                    'new_val': new_addr})
4332                     except (KeyError, AttributeError):
4333                         with excutils.save_and_reraise_exception():
4334                             # NOTE(adrianc): This should never happen. If we
4335                             # get here it means there is some inconsistency
4336                             # with either 'nw_info' or 'pci_mapping'.
4337                             LOG.error("Unexpected error when updating network "
4338                                       "information with PCI mapping.")
4339         return updated_nw_info
4340 
4341     def _confirm_resize(self, context, instance, migration=None):
4342         """Destroys the source instance."""
4343         self._notify_about_instance_usage(context, instance,
4344                                           "resize.confirm.start")
4345         compute_utils.notify_about_instance_action(context, instance,
4346             self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4347             phase=fields.NotificationPhase.START)
4348 
4349         # NOTE(tr3buchet): tear down networks on source host
4350         self.network_api.setup_networks_on_host(context, instance,
4351                            migration.source_compute, teardown=True)
4352 
4353         # TODO(stephenfin): These next three calls should be bundled
4354         network_info = self.network_api.get_instance_nw_info(context,
4355                                                              instance)
4356 
4357         # NOTE(adrianc): Populate old PCI device in VIF profile
4358         # to allow virt driver to properly unplug it from Hypervisor.
4359         pci_mapping = (instance.migration_context.
4360                        get_pci_mapping_for_migration(True))
4361         network_info = self._get_updated_nw_info_with_pci_mapping(
4362             network_info, pci_mapping)
4363 
4364         self.driver.confirm_migration(context, migration, instance,
4365                                       network_info)
4366 
4367         # Free up the old_flavor usage from the resource tracker for this host.
4368         self.rt.drop_move_claim_at_source(context, instance, migration)
4369 
4370         # NOTE(mriedem): The old_vm_state could be STOPPED but the user
4371         # might have manually powered up the instance to confirm the
4372         # resize/migrate, so we need to check the current power state
4373         # on the instance and set the vm_state appropriately. We default
4374         # to ACTIVE because if the power state is not SHUTDOWN, we
4375         # assume _sync_instance_power_state will clean it up.
4376         p_state = instance.power_state
4377         vm_state = None
4378         if p_state == power_state.SHUTDOWN:
4379             vm_state = vm_states.STOPPED
4380             LOG.debug("Resized/migrated instance is powered off. "
4381                       "Setting vm_state to '%s'.", vm_state,
4382                       instance=instance)
4383         else:
4384             vm_state = vm_states.ACTIVE
4385 
4386         instance.vm_state = vm_state
4387         instance.task_state = None
4388         instance.save(expected_task_state=[None, task_states.DELETING,
4389                                            task_states.SOFT_DELETING])
4390 
4391         self._notify_about_instance_usage(
4392             context, instance, "resize.confirm.end",
4393             network_info=network_info)
4394         compute_utils.notify_about_instance_action(context, instance,
4395                self.host, action=fields.NotificationAction.RESIZE_CONFIRM,
4396                phase=fields.NotificationPhase.END)
4397 
4398     def _delete_allocation_after_move(self, context, instance, migration):
4399         """Deletes resource allocations held by the migration record against
4400         the source compute node resource provider after a confirmed cold /
4401         successful live migration.
4402         """
4403         try:
4404             # NOTE(danms): We're finishing on the source node, so try
4405             # to delete the allocation based on the migration uuid
4406             self.reportclient.delete_allocation_for_instance(
4407                 context, migration.uuid, consumer_type='migration')
4408         except exception.AllocationDeleteFailed:
4409             LOG.error('Deleting allocation in placement for migration '
4410                       '%(migration_uuid)s failed. The instance '
4411                       '%(instance_uuid)s will be put to ERROR state '
4412                       'but the allocation held by the migration is '
4413                       'leaked.',
4414                       {'instance_uuid': instance.uuid,
4415                        'migration_uuid': migration.uuid})
4416             raise
4417 
4418     def _delete_stashed_flavor_info(self, instance):
4419         """Remove information about the flavor change after a resize."""
4420         instance.old_flavor = None
4421         instance.new_flavor = None
4422         instance.system_metadata.pop('old_vm_state', None)
4423         instance.save()
4424 
4425     @wrap_exception()
4426     @wrap_instance_event(prefix='compute')
4427     @errors_out_migration
4428     @wrap_instance_fault
4429     def confirm_snapshot_based_resize_at_source(
4430             self, ctxt, instance, migration):
4431         """Confirms a snapshot-based resize on the source host.
4432 
4433         Cleans the guest from the source hypervisor including disks and drops
4434         the MoveClaim which will free up "old_flavor" usage from the
4435         ResourceTracker.
4436 
4437         Deletes the allocations held by the migration consumer against the
4438         source compute node resource provider.
4439 
4440         :param ctxt: nova auth request context targeted at the source cell
4441         :param instance: Instance object being resized which should have the
4442             "old_flavor" attribute set
4443         :param migration: Migration object for the resize operation
4444         """
4445 
4446         @utils.synchronized(instance.uuid)
4447         def do_confirm():
4448             LOG.info('Confirming resize on source host.', instance=instance)
4449             with self._error_out_instance_on_exception(ctxt, instance):
4450                 # TODO(mriedem): Could probably make this try/except/finally
4451                 # a context manager to share with confirm_resize().
4452                 try:
4453                     self._confirm_snapshot_based_resize_at_source(
4454                         ctxt, instance, migration)
4455                 except Exception:
4456                     # Something failed when cleaning up the source host so
4457                     # log a traceback and leave a hint about hard rebooting
4458                     # the server to correct its state in the DB.
4459                     with excutils.save_and_reraise_exception(logger=LOG):
4460                         LOG.exception(
4461                             'Confirm resize failed on source host %s. '
4462                             'Resource allocations in the placement service '
4463                             'will be removed regardless because the instance '
4464                             'is now on the destination host %s. You can try '
4465                             'hard rebooting the instance to correct its '
4466                             'state.', self.host, migration.dest_compute,
4467                             instance=instance)
4468                 finally:
4469                     # Whether an error occurred or not, at this point the
4470                     # instance is on the dest host so to avoid leaking
4471                     # allocations in placement, delete them here.
4472                     # TODO(mriedem): Should we catch and just log
4473                     # AllocationDeleteFailed? What is the user's recourse if
4474                     # we got this far but this fails? At this point the
4475                     # instance is on the target host and the allocations
4476                     # could just be manually cleaned up by the operator.
4477                     self._delete_allocation_after_move(ctxt, instance,
4478                                                        migration)
4479         do_confirm()
4480 
4481     def _confirm_snapshot_based_resize_at_source(
4482             self, ctxt, instance, migration):
4483         """Private version of confirm_snapshot_based_resize_at_source
4484 
4485         This allows the main method to be decorated with error handlers.
4486 
4487         :param ctxt: nova auth request context targeted at the source cell
4488         :param instance: Instance object being resized which should have the
4489             "old_flavor" attribute set
4490         :param migration: Migration object for the resize operation
4491         """
4492         # Cleanup the guest from the hypervisor including local disks.
4493         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4494         LOG.debug('Cleaning up guest from source hypervisor including disks.',
4495                   instance=instance)
4496 
4497         # FIXME(mriedem): Per bug 1809095, _confirm_resize calls
4498         # _get_updated_nw_info_with_pci_mapping here prior to unplugging
4499         # VIFs on the source, but in our case we have already unplugged
4500         # VIFs during prep_snapshot_based_resize_at_source, so what do we
4501         # need to do about those kinds of ports? Do we need to wait to unplug
4502         # VIFs until confirm like normal resize?
4503 
4504         # Note that prep_snapshot_based_resize_at_source already destroyed the
4505         # guest which disconnected volumes and unplugged VIFs but did not
4506         # destroy disks in case something failed during the resize and the
4507         # instance needed to be rebooted or rebuilt on the source host. Now
4508         # that we are confirming the resize we want to cleanup the disks left
4509         # on the source host. We call cleanup() instead of destroy() to avoid
4510         # any InstanceNotFound confusion from the driver since the guest was
4511         # already destroyed on this host. block_device_info=None and
4512         # destroy_vifs=False means cleanup() will not try to disconnect volumes
4513         # or unplug VIFs.
4514         self.driver.cleanup(
4515             ctxt, instance, network_info, block_device_info=None,
4516             destroy_disks=True, destroy_vifs=False)
4517 
4518         # Delete port bindings for the source host.
4519         self._confirm_snapshot_based_resize_delete_port_bindings(
4520             ctxt, instance)
4521 
4522         # Delete volume attachments for the source host.
4523         self._delete_volume_attachments(ctxt, instance.get_bdms())
4524 
4525         # Free up the old_flavor usage from the resource tracker for this host.
4526         self.rt.drop_move_claim_at_source(ctxt, instance, migration)
4527 
4528     def _confirm_snapshot_based_resize_delete_port_bindings(
4529             self, ctxt, instance):
4530         """Delete port bindings for the source host when confirming
4531         snapshot-based resize on the source host."
4532 
4533         :param ctxt: nova auth RequestContext
4534         :param instance: Instance object that was resized/cold migrated
4535         """
4536         LOG.debug('Deleting port bindings for source host.',
4537                   instance=instance)
4538         try:
4539             self.network_api.cleanup_instance_network_on_host(
4540                 ctxt, instance, self.host)
4541         except exception.PortBindingDeletionFailed as e:
4542             # Do not let this stop us from cleaning up since the guest
4543             # is already gone.
4544             LOG.error('Failed to delete port bindings from source host. '
4545                       'Error: %s', str(e), instance=instance)
4546 
4547     def _delete_volume_attachments(self, ctxt, bdms):
4548         """Deletes volume attachment records for the given bdms.
4549 
4550         This method will log but not re-raise any exceptions if the volume
4551         attachment delete fails.
4552 
4553         :param ctxt: nova auth request context used to make
4554             DELETE /attachments/{attachment_id} requests to cinder.
4555         :param bdms: objects.BlockDeviceMappingList representing volume
4556             attachments to delete based on BlockDeviceMapping.attachment_id.
4557         """
4558         for bdm in bdms:
4559             if bdm.attachment_id:
4560                 try:
4561                     self.volume_api.attachment_delete(ctxt, bdm.attachment_id)
4562                 except Exception as e:
4563                     LOG.error('Failed to delete volume attachment with ID %s. '
4564                               'Error: %s', bdm.attachment_id, str(e),
4565                               instance_uuid=bdm.instance_uuid)
4566 
4567     @wrap_exception()
4568     @reverts_task_state
4569     @wrap_instance_event(prefix='compute')
4570     @errors_out_migration
4571     @wrap_instance_fault
4572     def revert_snapshot_based_resize_at_dest(self, ctxt, instance, migration):
4573         """Reverts a snapshot-based resize at the destination host.
4574 
4575         Cleans the guest from the destination compute service host hypervisor
4576         and related resources (ports, volumes) and frees resource usage from
4577         the compute service on that host.
4578 
4579         :param ctxt: nova auth request context targeted at the target cell
4580         :param instance: Instance object whose vm_state is "resized" and
4581             task_state is "resize_reverting".
4582         :param migration: Migration object whose status is "reverting".
4583         """
4584         # A resize revert is essentially a resize back to the old size, so we
4585         # need to send a usage event here.
4586         compute_utils.notify_usage_exists(
4587             self.notifier, ctxt, instance, self.host, current_period=True)
4588 
4589         @utils.synchronized(instance.uuid)
4590         def do_revert():
4591             LOG.info('Reverting resize on destination host.',
4592                      instance=instance)
4593             with self._error_out_instance_on_exception(ctxt, instance):
4594                 self._revert_snapshot_based_resize_at_dest(
4595                     ctxt, instance, migration)
4596         do_revert()
4597 
4598         # Broadcast to all schedulers that the instance is no longer on
4599         # this host and clear any waiting callback events. This is best effort
4600         # so if anything fails just log it.
4601         try:
4602             self._delete_scheduler_instance_info(ctxt, instance.uuid)
4603             self.instance_events.clear_events_for_instance(instance)
4604         except Exception as e:
4605             LOG.warning('revert_snapshot_based_resize_at_dest failed during '
4606                         'post-processing. Error: %s', e, instance=instance)
4607 
4608     def _revert_snapshot_based_resize_at_dest(
4609             self, ctxt, instance, migration):
4610         """Private version of revert_snapshot_based_resize_at_dest.
4611 
4612         This allows the main method to be decorated with error handlers.
4613 
4614         :param ctxt: nova auth request context targeted at the target cell
4615         :param instance: Instance object whose vm_state is "resized" and
4616             task_state is "resize_reverting".
4617         :param migration: Migration object whose status is "reverting".
4618         """
4619         # Cleanup the guest from the hypervisor including local disks.
4620         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4621         bdms = instance.get_bdms()
4622         block_device_info = self._get_instance_block_device_info(
4623             ctxt, instance, bdms=bdms)
4624         LOG.debug('Destroying guest from destination hypervisor including '
4625                   'disks.', instance=instance)
4626         self.driver.destroy(
4627             ctxt, instance, network_info, block_device_info=block_device_info)
4628 
4629         # Activate source host port bindings. We need to do this before
4630         # deleting the (active) dest host port bindings in
4631         # setup_networks_on_host otherwise the ports will be unbound and
4632         # finish on the source will fail.
4633         # migrate_instance_start uses migration.dest_compute for the port
4634         # binding host and since we want to activate the source host port
4635         # bindings, we need to temporarily mutate the migration object.
4636         with utils.temporary_mutation(
4637                 migration, dest_compute=migration.source_compute):
4638             LOG.debug('Activating port bindings for source host %s.',
4639                       migration.source_compute, instance=instance)
4640             # TODO(mriedem): https://review.opendev.org/#/c/594139/ would allow
4641             # us to remove this and make setup_networks_on_host do it.
4642             # TODO(mriedem): Should we try/except/log any errors but continue?
4643             self.network_api.migrate_instance_start(
4644                 ctxt, instance, migration)
4645 
4646         # Delete port bindings for the target host.
4647         LOG.debug('Deleting port bindings for target host %s.',
4648                   self.host, instance=instance)
4649         try:
4650             # Note that deleting the destination host port bindings does
4651             # not automatically activate the source host port bindings.
4652             self.network_api.cleanup_instance_network_on_host(
4653                 ctxt, instance, self.host)
4654         except exception.PortBindingDeletionFailed as e:
4655             # Do not let this stop us from cleaning up since the guest
4656             # is already gone.
4657             LOG.error('Failed to delete port bindings from target host. '
4658                       'Error: %s', str(e), instance=instance)
4659 
4660         # Delete any volume attachments remaining for this target host.
4661         LOG.debug('Deleting volume attachments for target host.',
4662                   instance=instance)
4663         self._delete_volume_attachments(ctxt, bdms)
4664 
4665         # Free up the new_flavor usage from the resource tracker for this host.
4666         self.rt.drop_move_claim_at_dest(ctxt, instance, migration)
4667 
4668     def _revert_instance_flavor_host_node(self, instance, migration):
4669         """Revert host, node and flavor fields after a resize-revert."""
4670         self._set_instance_info(instance, instance.old_flavor)
4671         instance.host = migration.source_compute
4672         instance.node = migration.source_node
4673         instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4674 
4675     @wrap_exception()
4676     @reverts_task_state
4677     @wrap_instance_event(prefix='compute')
4678     @errors_out_migration
4679     @wrap_instance_fault
4680     def finish_revert_snapshot_based_resize_at_source(
4681             self, ctxt, instance, migration):
4682         """Reverts a snapshot-based resize at the source host.
4683 
4684         Spawn the guest and re-connect volumes/VIFs on the source host and
4685         revert the instance to use the old_flavor for resource usage reporting.
4686 
4687         Updates allocations in the placement service to move the source node
4688         allocations, held by the migration record, to the instance and drop
4689         the allocations held by the instance on the destination node.
4690 
4691         :param ctxt: nova auth request context targeted at the target cell
4692         :param instance: Instance object whose vm_state is "resized" and
4693             task_state is "resize_reverting".
4694         :param migration: Migration object whose status is "reverting".
4695         """
4696 
4697         @utils.synchronized(instance.uuid)
4698         def do_revert():
4699             LOG.info('Reverting resize on source host.', instance=instance)
4700             with self._error_out_instance_on_exception(ctxt, instance):
4701                 self._finish_revert_snapshot_based_resize_at_source(
4702                     ctxt, instance, migration)
4703 
4704         try:
4705             do_revert()
4706         finally:
4707             self._delete_stashed_flavor_info(instance)
4708 
4709         # Broadcast to all schedulers that the instance is on this host.
4710         # This is best effort so if anything fails just log it.
4711         try:
4712             self._update_scheduler_instance_info(ctxt, instance)
4713         except Exception as e:
4714             LOG.warning('finish_revert_snapshot_based_resize_at_source failed '
4715                         'during post-processing. Error: %s', e,
4716                         instance=instance)
4717 
4718     def _finish_revert_snapshot_based_resize_at_source(
4719             self, ctxt, instance, migration):
4720         """Private version of finish_revert_snapshot_based_resize_at_source.
4721 
4722         This allows the main method to be decorated with error handlers.
4723 
4724         :param ctxt: nova auth request context targeted at the source cell
4725         :param instance: Instance object whose vm_state is "resized" and
4726             task_state is "resize_reverting".
4727         :param migration: Migration object whose status is "reverting".
4728         """
4729         # Get stashed old_vm_state information to determine if guest should
4730         # be powered on after spawn; we default to ACTIVE for backwards
4731         # compatibility if old_vm_state is not set
4732         old_vm_state = instance.system_metadata.get(
4733             'old_vm_state', vm_states.ACTIVE)
4734 
4735         # Revert the flavor and host/node fields to their previous values
4736         self._revert_instance_flavor_host_node(instance, migration)
4737 
4738         # Move the allocations against the source compute node resource
4739         # provider, held by the migration, to the instance which will drop
4740         # the destination compute node resource provider allocations held by
4741         # the instance. This puts the allocations against the source node
4742         # back to the old_flavor and owned by the instance.
4743         try:
4744             self._revert_allocation(ctxt, instance, migration)
4745         except exception.AllocationMoveFailed:
4746             # Log the error but do not re-raise because we want to continue to
4747             # process ports and volumes below.
4748             LOG.error('Reverting allocation in placement for migration '
4749                       '%(migration_uuid)s failed. You may need to manually '
4750                       'remove the allocations for the migration consumer '
4751                       'against the source node resource provider '
4752                       '%(source_provider)s and the allocations for the '
4753                       'instance consumer against the destination node '
4754                       'resource provider %(dest_provider)s and then run the '
4755                       '"nova-manage placement heal_allocations" command.',
4756                       {'instance_uuid': instance.uuid,
4757                        'migration_uuid': migration.uuid,
4758                        'source_provider': migration.source_node,
4759                        'dest_provider': migration.dest_node},
4760                       instance=instance)
4761 
4762         bdms = instance.get_bdms()
4763         # prep_snapshot_based_resize_at_source created empty volume attachments
4764         # that we need to update here to get the connection_info before calling
4765         # driver.finish_revert_migration which will connect the volumes to this
4766         # host.
4767         LOG.debug('Updating volume attachments for target host %s.',
4768                   self.host, instance=instance)
4769         # TODO(mriedem): We should probably make _update_volume_attachments
4770         # (optionally) graceful to errors so we (1) try to process all
4771         # attachments and (2) continue to process networking below.
4772         self._update_volume_attachments(ctxt, instance, bdms)
4773 
4774         LOG.debug('Updating port bindings for source host %s.',
4775                   self.host, instance=instance)
4776         # TODO(mriedem): Calculate provider mappings when we support
4777         # cross-cell resize/migrate with ports having resource requests.
4778         self._finish_revert_resize_network_migrate_finish(
4779             ctxt, instance, migration, provider_mappings=None)
4780         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
4781 
4782         # Remember that prep_snapshot_based_resize_at_source destroyed the
4783         # guest but left the disks intact so we cannot call spawn() here but
4784         # finish_revert_migration should do the job.
4785         block_device_info = self._get_instance_block_device_info(
4786             ctxt, instance, bdms=bdms)
4787         power_on = old_vm_state == vm_states.ACTIVE
4788         driver_error = None
4789         try:
4790             self.driver.finish_revert_migration(
4791                 ctxt, instance, network_info, migration,
4792                 block_device_info=block_device_info, power_on=power_on)
4793         except Exception as e:
4794             driver_error = e
4795             # Leave a hint about hard rebooting the guest and reraise so the
4796             # instance is put into ERROR state.
4797             with excutils.save_and_reraise_exception(logger=LOG):
4798                 LOG.error('An error occurred during finish_revert_migration. '
4799                           'The instance may need to be hard rebooted. Error: '
4800                           '%s', driver_error, instance=instance)
4801         else:
4802             # Perform final cleanup of the instance in the database.
4803             instance.drop_migration_context()
4804             # If the original vm_state was STOPPED, set it back to STOPPED.
4805             vm_state = vm_states.ACTIVE if power_on else vm_states.STOPPED
4806             self._update_instance_after_spawn(instance, vm_state=vm_state)
4807             instance.save(expected_task_state=[task_states.RESIZE_REVERTING])
4808         finally:
4809             # Complete any volume attachments so the volumes are in-use. We
4810             # do this regardless of finish_revert_migration failing because
4811             # the instance is back on this host now and we do not want to leave
4812             # the volumes in a pending state in case the instance is hard
4813             # rebooted.
4814             LOG.debug('Completing volume attachments for instance on source '
4815                       'host.', instance=instance)
4816             with excutils.save_and_reraise_exception(
4817                     reraise=driver_error is not None, logger=LOG):
4818                 self._complete_volume_attachments(ctxt, bdms)
4819 
4820         migration.status = 'reverted'
4821         migration.save()
4822 
4823     @wrap_exception()
4824     @reverts_task_state
4825     @wrap_instance_event(prefix='compute')
4826     @errors_out_migration
4827     @wrap_instance_fault
4828     def revert_resize(self, context, instance, migration, request_spec):
4829         """Destroys the new instance on the destination machine.
4830 
4831         Reverts the model changes, and powers on the old instance on the
4832         source machine.
4833 
4834         """
4835         # NOTE(comstud): A revert_resize is essentially a resize back to
4836         # the old size, so we need to send a usage event here.
4837         compute_utils.notify_usage_exists(self.notifier, context, instance,
4838                                           self.host, current_period=True)
4839 
4840         with self._error_out_instance_on_exception(context, instance):
4841             # NOTE(tr3buchet): tear down networks on destination host
4842             self.network_api.setup_networks_on_host(context, instance,
4843                                                     teardown=True)
4844 
4845             self.network_api.migrate_instance_start(context,
4846                                                     instance,
4847                                                     migration)
4848 
4849             network_info = self.network_api.get_instance_nw_info(context,
4850                                                                  instance)
4851             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4852                     context, instance.uuid)
4853             block_device_info = self._get_instance_block_device_info(
4854                                 context, instance, bdms=bdms)
4855 
4856             destroy_disks = not self._is_instance_storage_shared(
4857                 context, instance, host=migration.source_compute)
4858             self.driver.destroy(context, instance, network_info,
4859                                 block_device_info, destroy_disks)
4860 
4861             self._terminate_volume_connections(context, instance, bdms)
4862 
4863             # Free up the new_flavor usage from the resource tracker for this
4864             # host.
4865             self.rt.drop_move_claim_at_dest(context, instance, migration)
4866 
4867             # RPC cast back to the source host to finish the revert there.
4868             self.compute_rpcapi.finish_revert_resize(context, instance,
4869                     migration, migration.source_compute, request_spec)
4870 
4871     def _finish_revert_resize_network_migrate_finish(
4872             self, context, instance, migration, provider_mappings):
4873         """Causes port binding to be updated. In some Neutron or port
4874         configurations - see NetworkModel.get_bind_time_events() - we
4875         expect the vif-plugged event from Neutron immediately and wait for it.
4876         The rest of the time, the event is expected further along in the
4877         virt driver, so we don't wait here.
4878 
4879         :param context: The request context.
4880         :param instance: The instance undergoing the revert resize.
4881         :param migration: The Migration object of the resize being reverted.
4882         :param provider_mappings: a dict of list of resource provider uuids
4883             keyed by port uuid
4884         :raises: eventlet.timeout.Timeout or
4885                  exception.VirtualInterfacePlugException.
4886         """
4887         network_info = instance.get_network_info()
4888         events = []
4889         deadline = CONF.vif_plugging_timeout
4890         if deadline and network_info:
4891             events = network_info.get_bind_time_events(migration)
4892             if events:
4893                 LOG.debug('Will wait for bind-time events: %s', events)
4894         error_cb = self._neutron_failed_migration_callback
4895         try:
4896             with self.virtapi.wait_for_instance_event(instance, events,
4897                                                       deadline=deadline,
4898                                                       error_callback=error_cb):
4899                 # NOTE(hanrong): we need to change migration.dest_compute to
4900                 # source host temporarily.
4901                 # "network_api.migrate_instance_finish" will setup the network
4902                 # for the instance on the destination host. For revert resize,
4903                 # the instance will back to the source host, the setup of the
4904                 # network for instance should be on the source host. So set
4905                 # the migration.dest_compute to source host at here.
4906                 with utils.temporary_mutation(
4907                         migration, dest_compute=migration.source_compute):
4908                     self.network_api.migrate_instance_finish(
4909                         context, instance, migration, provider_mappings)
4910         except eventlet.timeout.Timeout:
4911             with excutils.save_and_reraise_exception():
4912                 LOG.error('Timeout waiting for Neutron events: %s', events,
4913                           instance=instance)
4914 
4915     @wrap_exception()
4916     @reverts_task_state
4917     @wrap_instance_event(prefix='compute')
4918     @errors_out_migration
4919     @wrap_instance_fault
4920     def finish_revert_resize(self, context, instance, migration, request_spec):
4921         """Finishes the second half of reverting a resize on the source host.
4922 
4923         Bring the original source instance state back (active/shutoff) and
4924         revert the resized attributes in the database.
4925 
4926         """
4927         try:
4928             self._finish_revert_resize(
4929                 context, instance, migration, request_spec)
4930         finally:
4931             self._delete_stashed_flavor_info(instance)
4932 
4933     def _finish_revert_resize(
4934         self, context, instance, migration, request_spec=None,
4935     ):
4936         """Inner version of finish_revert_resize."""
4937         with self._error_out_instance_on_exception(context, instance):
4938             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
4939                 context, instance.uuid)
4940             self._notify_about_instance_usage(
4941                     context, instance, "resize.revert.start")
4942             compute_utils.notify_about_instance_action(context, instance,
4943                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
4944                     phase=fields.NotificationPhase.START, bdms=bdms)
4945 
4946             # Get stashed old_vm_state information to determine if guest should
4947             # be powered on after spawn; we default to ACTIVE for backwards
4948             # compatibility if old_vm_state is not set
4949             old_vm_state = instance.system_metadata.get(
4950                 'old_vm_state', vm_states.ACTIVE)
4951 
4952             # Revert the flavor and host/node fields to their previous values
4953             self._revert_instance_flavor_host_node(instance, migration)
4954 
4955             try:
4956                 source_allocations = self._revert_allocation(
4957                     context, instance, migration)
4958             except exception.AllocationMoveFailed:
4959                 LOG.error('Reverting allocation in placement for migration '
4960                           '%(migration_uuid)s failed. The instance '
4961                           '%(instance_uuid)s will be put into ERROR state but '
4962                           'the allocation held by the migration is leaked.',
4963                           {'instance_uuid': instance.uuid,
4964                            'migration_uuid': migration.uuid})
4965                 raise
4966 
4967             provider_mappings = self._fill_provider_mapping_based_on_allocs(
4968                 context, source_allocations, request_spec)
4969 
4970             self.network_api.setup_networks_on_host(context, instance,
4971                                                     migration.source_compute)
4972             self._finish_revert_resize_network_migrate_finish(
4973                 context, instance, migration, provider_mappings)
4974             network_info = self.network_api.get_instance_nw_info(context,
4975                                                                  instance)
4976 
4977             # revert_resize deleted any volume attachments for the instance
4978             # and created new ones to be used on this host, but we
4979             # have to update those attachments with the host connector so the
4980             # BDM.connection_info will get set in the call to
4981             # _get_instance_block_device_info below with refresh_conn_info=True
4982             # and then the volumes can be re-connected via the driver on this
4983             # host.
4984             self._update_volume_attachments(context, instance, bdms)
4985 
4986             block_device_info = self._get_instance_block_device_info(
4987                     context, instance, refresh_conn_info=True, bdms=bdms)
4988 
4989             power_on = old_vm_state != vm_states.STOPPED
4990             self.driver.finish_revert_migration(
4991                 context, instance, network_info, migration, block_device_info,
4992                 power_on)
4993 
4994             instance.drop_migration_context()
4995             instance.launched_at = timeutils.utcnow()
4996             instance.save(expected_task_state=task_states.RESIZE_REVERTING)
4997 
4998             # Complete any volume attachments so the volumes are in-use.
4999             self._complete_volume_attachments(context, bdms)
5000 
5001             # if the original vm state was STOPPED, set it back to STOPPED
5002             LOG.info("Updating instance to original state: '%s'",
5003                      old_vm_state, instance=instance)
5004             if power_on:
5005                 instance.vm_state = vm_states.ACTIVE
5006                 instance.task_state = None
5007                 instance.save()
5008             else:
5009                 instance.task_state = task_states.POWERING_OFF
5010                 instance.save()
5011                 self.stop_instance(context, instance=instance,
5012                                    clean_shutdown=True)
5013 
5014             self._notify_about_instance_usage(
5015                     context, instance, "resize.revert.end")
5016             compute_utils.notify_about_instance_action(context, instance,
5017                 self.host, action=fields.NotificationAction.RESIZE_REVERT,
5018                     phase=fields.NotificationPhase.END, bdms=bdms)
5019 
5020     def _fill_provider_mapping_based_on_allocs(
5021             self, context, allocations, request_spec):
5022         """Fills and returns the request group - resource provider mapping
5023         based on the allocation passed in.
5024 
5025         :param context: The security context
5026         :param allocation: allocation dict keyed by RP UUID.
5027         :param request_spec: The RequestSpec object associated with the
5028             operation
5029         :returns: None if the request_spec is None. Otherwise a mapping
5030             between RequestGroup requester_id, currently Neutron port_id,
5031             and a list of resource provider UUIDs providing resource for
5032             that RequestGroup.
5033         """
5034         if request_spec:
5035             # NOTE(gibi): We need to re-calculate the resource provider -
5036             # port mapping as we have to have the neutron ports allocate
5037             # from the source compute after revert.
5038             scheduler_utils.fill_provider_mapping_based_on_allocation(
5039                 context, self.reportclient, request_spec, allocations)
5040             provider_mappings = self._get_request_group_mapping(
5041                 request_spec)
5042         else:
5043             # TODO(sbauza): Remove this conditional once we only support RPC
5044             # API 6.0
5045             # NOTE(gibi): The compute RPC is pinned to be older than 5.2
5046             # and therefore request_spec is not sent. We cannot calculate
5047             # the provider mappings. If the instance has ports with
5048             # resource request then the port update will fail in
5049             # _update_port_binding_for_instance() called via
5050             # _finish_revert_resize_network_migrate_finish() in
5051             # finish_revert_resize.
5052             provider_mappings = None
5053         return provider_mappings
5054 
5055     def _revert_allocation(self, context, instance, migration):
5056         """Revert an allocation that is held by migration to our instance."""
5057 
5058         # Fetch the original allocation that the instance had on the source
5059         # node, which are now held by the migration
5060         orig_alloc = self.reportclient.get_allocations_for_consumer(
5061             context, migration.uuid)
5062         if not orig_alloc:
5063             LOG.error('Did not find resource allocations for migration '
5064                       '%s on source node %s. Unable to revert source node '
5065                       'allocations back to the instance.',
5066                       migration.uuid, migration.source_node, instance=instance)
5067             return False
5068 
5069         LOG.info('Swapping old allocation on %(rp_uuids)s held by migration '
5070                  '%(mig)s for instance',
5071                  {'rp_uuids': orig_alloc.keys(), 'mig': migration.uuid},
5072                  instance=instance)
5073         # FIXME(gibi): This method is flawed in that it does not handle
5074         # allocations against sharing providers in any special way. This leads
5075         # to duplicate allocations against the sharing provider during
5076         # migration.
5077         # TODO(cdent): Should we be doing anything with return values here?
5078         self.reportclient.move_allocations(context, migration.uuid,
5079                                            instance.uuid)
5080         return orig_alloc
5081 
5082     def _prep_resize(self, context, image, instance, instance_type,
5083                      filter_properties, node, migration, request_spec,
5084                      clean_shutdown=True):
5085 
5086         if not filter_properties:
5087             filter_properties = {}
5088 
5089         if not instance.host:
5090             self._set_instance_obj_error_state(instance)
5091             msg = _('Instance has no source host')
5092             raise exception.MigrationError(reason=msg)
5093 
5094         same_host = instance.host == self.host
5095         # if the flavor IDs match, it's migrate; otherwise resize
5096         if same_host and instance_type.id == instance['instance_type_id']:
5097             # check driver whether support migrate to same host
5098             if not self.driver.capabilities.get(
5099                     'supports_migrate_to_same_host', False):
5100                 # Raise InstanceFaultRollback so that the
5101                 # _error_out_instance_on_exception context manager in
5102                 # prep_resize will set the instance.vm_state properly.
5103                 raise exception.InstanceFaultRollback(
5104                     inner_exception=exception.UnableToMigrateToSelf(
5105                         instance_id=instance.uuid, host=self.host))
5106 
5107         # NOTE(danms): Stash the new instance_type to avoid having to
5108         # look it up in the database later
5109         instance.new_flavor = instance_type
5110         # NOTE(mriedem): Stash the old vm_state so we can set the
5111         # resized/reverted instance back to the same state later.
5112         vm_state = instance.vm_state
5113         LOG.debug('Stashing vm_state: %s', vm_state, instance=instance)
5114         instance.system_metadata['old_vm_state'] = vm_state
5115         instance.save()
5116 
5117         if not isinstance(request_spec, objects.RequestSpec):
5118             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
5119             # version of the request spec to compute and since Stein compute
5120             # could be sending that back to conductor on reschedule, so if we
5121             # got a dict convert it to an object.
5122             # TODO(mriedem): We can drop this compat code when we only support
5123             # compute RPC API >=6.0.
5124             request_spec = objects.RequestSpec.from_primitives(
5125                 context, request_spec, filter_properties)
5126             # We don't have to set the new flavor on the request spec because
5127             # if we got here it was due to a reschedule from the compute and
5128             # the request spec would already have the new flavor in it from the
5129             # else block below.
5130 
5131         provider_mapping = self._get_request_group_mapping(request_spec)
5132 
5133         if provider_mapping:
5134             try:
5135                 compute_utils.\
5136                     update_pci_request_spec_with_allocated_interface_name(
5137                         context, self.reportclient,
5138                         instance.pci_requests.requests, provider_mapping)
5139             except (exception.AmbiguousResourceProviderForPCIRequest,
5140                     exception.UnexpectedResourceProviderNameForPCIRequest
5141                     ) as e:
5142                 raise exception.BuildAbortException(
5143                     reason=str(e), instance_uuid=instance.uuid)
5144 
5145         limits = filter_properties.get('limits', {})
5146         allocs = self.reportclient.get_allocations_for_consumer(
5147             context, instance.uuid)
5148         with self.rt.resize_claim(context, instance, instance_type, node,
5149                                   migration, allocs, image_meta=image,
5150                                   limits=limits) as claim:
5151             LOG.info('Migrating', instance=instance)
5152             # RPC cast to the source host to start the actual resize/migration.
5153             self.compute_rpcapi.resize_instance(
5154                     context, instance, claim.migration, image,
5155                     instance_type, request_spec, clean_shutdown)
5156 
5157     def _send_prep_resize_notifications(
5158             self, context, instance, phase, flavor):
5159         """Send "resize.prep.*" notifications.
5160 
5161         :param context: nova auth request context
5162         :param instance: The instance being resized
5163         :param phase: The phase of the action (NotificationPhase enum)
5164         :param flavor: The (new) flavor for the resize (same as existing
5165             instance.flavor for a cold migration)
5166         """
5167         # Only send notify_usage_exists if it's the "start" phase.
5168         if phase == fields.NotificationPhase.START:
5169             compute_utils.notify_usage_exists(
5170                 self.notifier, context, instance, self.host,
5171                 current_period=True)
5172 
5173         # Send extra usage info about the flavor if it's the "end" phase for
5174         # the legacy unversioned notification.
5175         extra_usage_info = None
5176         if phase == fields.NotificationPhase.END:
5177             extra_usage_info = dict(
5178                 new_instance_type=flavor.name,
5179                 new_instance_type_id=flavor.id)
5180         self._notify_about_instance_usage(
5181             context, instance, "resize.prep.%s" % phase,
5182             extra_usage_info=extra_usage_info)
5183 
5184         # Send the versioned notification.
5185         compute_utils.notify_about_resize_prep_instance(
5186             context, instance, self.host, phase, flavor)
5187 
5188     @wrap_exception()
5189     @reverts_task_state
5190     @wrap_instance_event(prefix='compute')
5191     @wrap_instance_fault
5192     def prep_resize(self, context, image, instance, flavor,
5193                     request_spec, filter_properties, node,
5194                     clean_shutdown, migration, host_list):
5195         """Initiates the process of moving a running instance to another host.
5196 
5197         Possibly changes the VCPU, RAM and disk size in the process.
5198 
5199         This is initiated from conductor and runs on the destination host.
5200 
5201         The main purpose of this method is performing some checks on the
5202         destination host and making a claim for resources. If the claim fails
5203         then a reschedule to another host may be attempted which involves
5204         calling back to conductor to start the process over again.
5205         """
5206         if node is None:
5207             node = self._get_nodename(instance, refresh=True)
5208 
5209         # Pass instance_state=instance.vm_state because we can resize
5210         # a STOPPED server and we don't want to set it back to ACTIVE
5211         # in case _prep_resize fails.
5212         instance_state = instance.vm_state
5213         with self._error_out_instance_on_exception(
5214                 context, instance, instance_state=instance_state),\
5215                 errors_out_migration_ctxt(migration):
5216 
5217             self._send_prep_resize_notifications(
5218                 context, instance, fields.NotificationPhase.START,
5219                 flavor)
5220             try:
5221                 scheduler_hints = self._get_scheduler_hints(filter_properties,
5222                                                             request_spec)
5223                 # Error out if this host cannot accept the new instance due
5224                 # to anti-affinity. At this point the migration is already
5225                 # in-progress, so this is the definitive moment to abort due to
5226                 # the policy violation. Also, exploding here is covered by the
5227                 # cleanup methods in except block.
5228                 try:
5229                     self._validate_instance_group_policy(context, instance,
5230                                                          scheduler_hints)
5231                 except exception.RescheduledException as e:
5232                     raise exception.InstanceFaultRollback(inner_exception=e)
5233 
5234                 self._prep_resize(context, image, instance,
5235                                   flavor, filter_properties,
5236                                   node, migration, request_spec,
5237                                   clean_shutdown)
5238             except exception.BuildAbortException:
5239                 # NOTE(gibi): We failed
5240                 # update_pci_request_spec_with_allocated_interface_name so
5241                 # there is no reason to re-schedule. Just revert the allocation
5242                 # and fail the migration.
5243                 with excutils.save_and_reraise_exception():
5244                     self._revert_allocation(context, instance, migration)
5245             except Exception:
5246                 # Since we hit a failure, we're either rescheduling or dead
5247                 # and either way we need to cleanup any allocations created
5248                 # by the scheduler for the destination node.
5249                 self._revert_allocation(context, instance, migration)
5250                 # try to re-schedule the resize elsewhere:
5251                 exc_info = sys.exc_info()
5252                 self._reschedule_resize_or_reraise(context, instance,
5253                         exc_info, flavor, request_spec,
5254                         filter_properties, host_list)
5255             finally:
5256                 self._send_prep_resize_notifications(
5257                     context, instance, fields.NotificationPhase.END,
5258                     flavor)
5259 
5260     def _reschedule_resize_or_reraise(self, context, instance, exc_info,
5261             instance_type, request_spec, filter_properties, host_list):
5262         """Try to re-schedule the resize or re-raise the original error to
5263         error out the instance.
5264         """
5265         if not filter_properties:
5266             filter_properties = {}
5267 
5268         rescheduled = False
5269         instance_uuid = instance.uuid
5270 
5271         try:
5272             retry = filter_properties.get('retry')
5273             if retry:
5274                 LOG.debug('Rescheduling, attempt %d', retry['num_attempts'],
5275                           instance_uuid=instance_uuid)
5276 
5277                 # reset the task state
5278                 task_state = task_states.RESIZE_PREP
5279                 self._instance_update(context, instance, task_state=task_state)
5280 
5281                 if exc_info:
5282                     # stringify to avoid circular ref problem in json
5283                     # serialization
5284                     retry['exc'] = traceback.format_exception_only(
5285                         exc_info[0], exc_info[1])
5286 
5287                 scheduler_hint = {'filter_properties': filter_properties}
5288 
5289                 self.compute_task_api.resize_instance(
5290                     context, instance, scheduler_hint, instance_type,
5291                     request_spec=request_spec, host_list=host_list)
5292 
5293                 rescheduled = True
5294             else:
5295                 # no retry information, do not reschedule.
5296                 LOG.debug('Retry info not present, will not reschedule',
5297                           instance_uuid=instance_uuid)
5298                 rescheduled = False
5299         except Exception as error:
5300             rescheduled = False
5301             LOG.exception("Error trying to reschedule",
5302                           instance_uuid=instance_uuid)
5303             compute_utils.add_instance_fault_from_exc(context,
5304                     instance, error,
5305                     exc_info=sys.exc_info())
5306             self._notify_about_instance_usage(context, instance,
5307                     'resize.error', fault=error)
5308             compute_utils.notify_about_instance_action(
5309                 context, instance, self.host,
5310                 action=fields.NotificationAction.RESIZE,
5311                 phase=fields.NotificationPhase.ERROR,
5312                 exception=error,
5313             )
5314 
5315         if rescheduled:
5316             self._log_original_error(exc_info, instance_uuid)
5317             compute_utils.add_instance_fault_from_exc(context,
5318                     instance, exc_info[1], exc_info=exc_info)
5319             self._notify_about_instance_usage(context, instance,
5320                     'resize.error', fault=exc_info[1])
5321             compute_utils.notify_about_instance_action(
5322                 context, instance, self.host,
5323                 action=fields.NotificationAction.RESIZE,
5324                 phase=fields.NotificationPhase.ERROR,
5325                 exception=exc_info[1],
5326             )
5327         else:
5328             # not re-scheduling
5329             exc = exc_info[1] or exc_info[0]()
5330             if exc.__traceback__ is not exc_info[2]:
5331                 raise exc.with_traceback(exc_info[2])
5332             raise exc
5333 
5334     @messaging.expected_exceptions(exception.MigrationPreCheckError)
5335     @wrap_exception()
5336     @wrap_instance_event(prefix='compute')
5337     @wrap_instance_fault
5338     def prep_snapshot_based_resize_at_dest(
5339             self, ctxt, instance, flavor, nodename, migration, limits):
5340         """Performs pre-cross-cell resize resource claim on the dest host.
5341 
5342         This runs on the destination host in a cross-cell resize operation
5343         before the resize is actually started.
5344 
5345         Performs a resize_claim for resources that are not claimed in placement
5346         like PCI devices and NUMA topology.
5347 
5348         Note that this is different from same-cell prep_resize in that this:
5349 
5350         * Does not RPC cast to the source compute, that is orchestrated from
5351           conductor.
5352         * This does not reschedule on failure, conductor handles that since
5353           conductor is synchronously RPC calling this method. As such, the
5354           reverts_task_state decorator is not used on this method.
5355 
5356         :param ctxt: user auth request context
5357         :param instance: the instance being resized
5358         :param flavor: the flavor being resized to (unchanged for cold migrate)
5359         :param nodename: Name of the target compute node
5360         :param migration: nova.objects.Migration object for the operation
5361         :param limits: nova.objects.SchedulerLimits object of resource limits
5362         :returns: nova.objects.MigrationContext; the migration context created
5363             on the destination host during the resize_claim.
5364         :raises: nova.exception.MigrationPreCheckError if the pre-check
5365             validation fails for the given host selection
5366         """
5367         LOG.debug('Checking if we can cross-cell migrate instance to this '
5368                   'host (%s).', self.host, instance=instance)
5369         self._send_prep_resize_notifications(
5370             ctxt, instance, fields.NotificationPhase.START, flavor)
5371         # TODO(mriedem): update_pci_request_spec_with_allocated_interface_name
5372         # should be called here if the request spec has request group mappings,
5373         # e.g. for things like QoS ports with resource requests. Do it outside
5374         # the try/except so if it raises BuildAbortException we do not attempt
5375         # to reschedule.
5376         try:
5377             # Get the allocations within the try/except block in case we get
5378             # an error so MigrationPreCheckError is raised up.
5379             allocations = self.reportclient.get_allocs_for_consumer(
5380                 ctxt, instance.uuid)['allocations']
5381             # Claim resources on this target host using the new flavor which
5382             # will create the MigrationContext object. Note that in the future
5383             # if we want to do other validation here we should do it within
5384             # the MoveClaim context so we can drop the claim if anything fails.
5385             self.rt.resize_claim(
5386                 ctxt, instance, flavor, nodename, migration, allocations,
5387                 image_meta=instance.image_meta, limits=limits)
5388         except Exception as ex:
5389             err = str(ex)
5390             LOG.warning(
5391                 'Cross-cell resize pre-checks failed for this host (%s). '
5392                 'Cleaning up. Failure: %s', self.host, err,
5393                 instance=instance, exc_info=True)
5394             raise exception.MigrationPreCheckError(
5395                 reason=(_("Pre-checks failed on host '%(host)s'. "
5396                           "Error: %(error)s") %
5397                         {'host': self.host, 'error': err}))
5398         finally:
5399             self._send_prep_resize_notifications(
5400                 ctxt, instance, fields.NotificationPhase.END, flavor)
5401 
5402         # ResourceTracker.resize_claim() sets instance.migration_context.
5403         return instance.migration_context
5404 
5405     @messaging.expected_exceptions(exception.InstancePowerOffFailure)
5406     @wrap_exception()
5407     @reverts_task_state
5408     @wrap_instance_event(prefix='compute')
5409     @errors_out_migration
5410     @wrap_instance_fault
5411     def prep_snapshot_based_resize_at_source(
5412             self, ctxt, instance, migration, snapshot_id=None):
5413         """Prepares the instance at the source host for cross-cell resize
5414 
5415         Performs actions like powering off the guest, upload snapshot data if
5416         the instance is not volume-backed, disconnecting volumes, unplugging
5417         VIFs and activating the destination host port bindings.
5418 
5419         :param ctxt: user auth request context targeted at source cell
5420         :param instance: nova.objects.Instance; the instance being resized.
5421             The expected instance.task_state is "resize_migrating" when calling
5422             this method, and the expected task_state upon successful completion
5423             is "resize_migrated".
5424         :param migration: nova.objects.Migration object for the operation.
5425             The expected migration.status is "pre-migrating" when calling this
5426             method and the expected status upon successful completion is
5427             "post-migrating".
5428         :param snapshot_id: ID of the image snapshot to upload if not a
5429             volume-backed instance
5430         :raises: nova.exception.InstancePowerOffFailure if stopping the
5431             instance fails
5432         """
5433         LOG.info('Preparing for snapshot based resize on source host %s.',
5434                  self.host, instance=instance)
5435         # Note that if anything fails here, the migration-based allocations
5436         # created in conductor should be reverted by conductor as well,
5437         # see MigrationTask.rollback.
5438         self._prep_snapshot_based_resize_at_source(
5439             ctxt, instance, migration, snapshot_id=snapshot_id)
5440 
5441     @delete_image_on_error
5442     def _snapshot_for_resize(self, ctxt, image_id, instance):
5443         """Uploads snapshot for the instance during a snapshot-based resize
5444 
5445         If the snapshot operation fails the image will be deleted.
5446 
5447         :param ctxt: the nova auth request context for the resize operation
5448         :param image_id: the snapshot image ID
5449         :param instance: the instance to snapshot/resize
5450         """
5451         LOG.debug('Uploading snapshot data for image %s', image_id,
5452                   instance=instance)
5453         # Note that we do not track the snapshot phase task states
5454         # during resize since we do not want to reflect those into the
5455         # actual instance.task_state.
5456         update_task_state = lambda *args, **kwargs: None
5457         with timeutils.StopWatch() as timer:
5458             self.driver.snapshot(ctxt, instance, image_id, update_task_state)
5459             LOG.debug('Took %0.2f seconds to snapshot the instance on '
5460                       'the hypervisor.', timer.elapsed(), instance=instance)
5461 
5462     def _prep_snapshot_based_resize_at_source(
5463             self, ctxt, instance, migration, snapshot_id=None):
5464         """Private method for prep_snapshot_based_resize_at_source so calling
5465         code can handle errors and perform rollbacks as necessary.
5466         """
5467         # Fetch and update the instance.info_cache.
5468         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
5469         # Get the BDMs attached to this instance on this source host.
5470         bdms = instance.get_bdms()
5471         # Send the resize.start notification.
5472         self._send_resize_instance_notifications(
5473             ctxt, instance, bdms, network_info, fields.NotificationPhase.START)
5474         # Update the migration status from "pre-migrating" to "migrating".
5475         migration.status = 'migrating'
5476         migration.save()
5477 
5478         # Since the instance is going to be left on the source host during the
5479         # resize, we need to power it off so we do not have the instance
5480         # potentially running in two places.
5481         LOG.debug('Stopping instance', instance=instance)
5482         try:
5483             self._power_off_instance(instance)
5484         except Exception as e:
5485             LOG.exception('Failed to power off instance.', instance=instance)
5486             raise exception.InstancePowerOffFailure(reason=str(e))
5487         instance.power_state = self._get_power_state(instance)
5488 
5489         # If a snapshot image ID was provided, we need to snapshot the guest
5490         # disk image and upload it to the image service.
5491         if snapshot_id:
5492             self._snapshot_for_resize(ctxt, snapshot_id, instance)
5493 
5494         block_device_info = self._get_instance_block_device_info(
5495             ctxt, instance, bdms=bdms)
5496 
5497         # If something fails at this point the instance must go to ERROR
5498         # status for operator intervention or to reboot/rebuild the instance.
5499         with self._error_out_instance_on_exception(
5500                 ctxt, instance, instance_state=vm_states.ERROR):
5501 
5502             # Destroy the guest on the source host which will disconnect
5503             # volumes and unplug VIFs. Note that we DO NOT destroy disks since
5504             # we want to leave those on the source host in case of a later
5505             # failure and disks are needed to recover the guest or in case the
5506             # resize is reverted.
5507             LOG.debug('Destroying guest on source host but retaining disks.',
5508                       instance=instance)
5509             self.driver.destroy(
5510                 ctxt, instance, network_info,
5511                 block_device_info=block_device_info, destroy_disks=False)
5512 
5513             # At this point the volumes are disconnected from this source host.
5514             # Delete the old volume attachment records and create new empty
5515             # ones which will be used later if the resize is reverted.
5516             LOG.debug('Deleting volume attachments for the source host.',
5517                       instance=instance)
5518             self._terminate_volume_connections(ctxt, instance, bdms)
5519 
5520             # At this point the VIFs are unplugged from this source host.
5521             # Activate the dest host port bindings created by conductor.
5522             self.network_api.migrate_instance_start(ctxt, instance, migration)
5523 
5524             # Update the migration status from "migrating" to "post-migrating".
5525             migration.status = 'post-migrating'
5526             migration.save()
5527 
5528             # At this point, the traditional resize_instance would update the
5529             # instance host/node values to point at the dest host/node because
5530             # that is where the disk is transferred during resize_instance, but
5531             # with cross-cell resize the instance is not yet at the dest host
5532             # so we do not make that update here.
5533             instance.task_state = task_states.RESIZE_MIGRATED
5534             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5535 
5536         self._send_resize_instance_notifications(
5537             ctxt, instance, bdms, network_info,
5538             fields.NotificationPhase.END)
5539         self.instance_events.clear_events_for_instance(instance)
5540 
5541     @wrap_exception()
5542     @reverts_task_state
5543     @wrap_instance_event(prefix='compute')
5544     @wrap_instance_fault
5545     def resize_instance(self, context, instance, image,
5546                         migration, flavor, clean_shutdown,
5547                         request_spec):
5548         """Starts the migration of a running instance to another host.
5549 
5550         This is initiated from the destination host's ``prep_resize`` routine
5551         and runs on the source host.
5552         """
5553         try:
5554             self._resize_instance(context, instance, image, migration,
5555                                   flavor, clean_shutdown, request_spec)
5556         except Exception:
5557             with excutils.save_and_reraise_exception():
5558                 self._revert_allocation(context, instance, migration)
5559 
5560     def _resize_instance(self, context, instance, image,
5561                          migration, instance_type, clean_shutdown,
5562                          request_spec):
5563         # Pass instance_state=instance.vm_state because we can resize
5564         # a STOPPED server and we don't want to set it back to ACTIVE
5565         # in case migrate_disk_and_power_off raises InstanceFaultRollback.
5566         instance_state = instance.vm_state
5567         with self._error_out_instance_on_exception(
5568                 context, instance, instance_state=instance_state), \
5569              errors_out_migration_ctxt(migration):
5570             network_info = self.network_api.get_instance_nw_info(context,
5571                                                                  instance)
5572 
5573             migration.status = 'migrating'
5574             migration.save()
5575 
5576             instance.task_state = task_states.RESIZE_MIGRATING
5577             instance.save(expected_task_state=task_states.RESIZE_PREP)
5578 
5579             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5580                     context, instance.uuid)
5581             self._send_resize_instance_notifications(
5582                 context, instance, bdms, network_info,
5583                 fields.NotificationPhase.START)
5584 
5585             block_device_info = self._get_instance_block_device_info(
5586                                 context, instance, bdms=bdms)
5587 
5588             timeout, retry_interval = self._get_power_off_values(
5589                 instance, clean_shutdown)
5590             disk_info = self.driver.migrate_disk_and_power_off(
5591                     context, instance, migration.dest_host,
5592                     instance_type, network_info,
5593                     block_device_info,
5594                     timeout, retry_interval)
5595 
5596             self._terminate_volume_connections(context, instance, bdms)
5597 
5598             self.network_api.migrate_instance_start(context,
5599                                                     instance,
5600                                                     migration)
5601 
5602             migration.status = 'post-migrating'
5603             migration.save()
5604 
5605             instance.host = migration.dest_compute
5606             instance.node = migration.dest_node
5607             instance.task_state = task_states.RESIZE_MIGRATED
5608             instance.save(expected_task_state=task_states.RESIZE_MIGRATING)
5609 
5610             # RPC cast to the destination host to finish the resize/migration.
5611             self.compute_rpcapi.finish_resize(context, instance,
5612                 migration, image, disk_info, migration.dest_compute,
5613                 request_spec)
5614 
5615         self._send_resize_instance_notifications(
5616             context, instance, bdms, network_info,
5617             fields.NotificationPhase.END)
5618         self.instance_events.clear_events_for_instance(instance)
5619 
5620     def _send_resize_instance_notifications(
5621             self, context, instance, bdms, network_info, phase):
5622         """Send "resize.(start|end)" notifications.
5623 
5624         :param context: nova auth request context
5625         :param instance: The instance being resized
5626         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5627             instance
5628         :param network_info: NetworkInfo for the instance info cache of ports
5629         :param phase: The phase of the action (NotificationPhase enum, either
5630             ``start`` or ``end``)
5631         """
5632         action = fields.NotificationAction.RESIZE
5633         # Send the legacy unversioned notification.
5634         self._notify_about_instance_usage(
5635             context, instance, "%s.%s" % (action, phase),
5636             network_info=network_info)
5637         # Send the versioned notification.
5638         compute_utils.notify_about_instance_action(
5639             context, instance, self.host, action=action, phase=phase,
5640             bdms=bdms)
5641 
5642     def _terminate_volume_connections(self, context, instance, bdms):
5643         connector = None
5644         for bdm in bdms:
5645             if bdm.is_volume:
5646                 if bdm.attachment_id:
5647                     # NOTE(jdg): So here's the thing, the idea behind the new
5648                     # attach API's was to have a new code fork/path that we
5649                     # followed, we're not going to do that so we have to do
5650                     # some extra work in here to make it *behave* just like the
5651                     # old code. Cinder doesn't allow disconnect/reconnect (you
5652                     # just delete the attachment and get a new one)
5653                     # attachments in the new attach code so we have to do
5654                     # a delete and create without a connector (reserve),
5655                     # in other words, beware
5656                     attachment_id = self.volume_api.attachment_create(
5657                         context, bdm.volume_id, instance.uuid)['id']
5658                     self.volume_api.attachment_delete(context,
5659                                                       bdm.attachment_id)
5660                     bdm.attachment_id = attachment_id
5661                     bdm.save()
5662 
5663                 else:
5664                     if connector is None:
5665                         connector = self.driver.get_volume_connector(instance)
5666                     self.volume_api.terminate_connection(context,
5667                                                          bdm.volume_id,
5668                                                          connector)
5669 
5670     @staticmethod
5671     def _set_instance_info(instance, instance_type):
5672         instance.instance_type_id = instance_type.id
5673         instance.memory_mb = instance_type.memory_mb
5674         instance.vcpus = instance_type.vcpus
5675         instance.root_gb = instance_type.root_gb
5676         instance.ephemeral_gb = instance_type.ephemeral_gb
5677         instance.flavor = instance_type
5678 
5679     def _update_volume_attachments(self, context, instance, bdms):
5680         """Updates volume attachments using the virt driver host connector.
5681 
5682         :param context: nova.context.RequestContext - user request context
5683         :param instance: nova.objects.Instance
5684         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5685                      device mappings for the given instance
5686         """
5687         if bdms:
5688             connector = None
5689             for bdm in bdms:
5690                 if bdm.is_volume and bdm.attachment_id:
5691                     if connector is None:
5692                         connector = self.driver.get_volume_connector(instance)
5693                     self.volume_api.attachment_update(
5694                         context, bdm.attachment_id, connector, bdm.device_name)
5695 
5696     def _complete_volume_attachments(self, context, bdms):
5697         """Completes volume attachments for the instance
5698 
5699         :param context: nova.context.RequestContext - user request context
5700         :param bdms: nova.objects.BlockDeviceMappingList - the list of block
5701                      device mappings for the given instance
5702         """
5703         if bdms:
5704             for bdm in bdms:
5705                 if bdm.is_volume and bdm.attachment_id:
5706                     self.volume_api.attachment_complete(
5707                         context, bdm.attachment_id)
5708 
5709     def _finish_resize(self, context, instance, migration, disk_info,
5710                        image_meta, bdms, request_spec):
5711         resize_instance = False  # indicates disks have been resized
5712         old_instance_type_id = migration['old_instance_type_id']
5713         new_instance_type_id = migration['new_instance_type_id']
5714         old_flavor = instance.flavor  # the current flavor is now old
5715         # NOTE(mriedem): Get the old_vm_state so we know if we should
5716         # power on the instance. If old_vm_state is not set we need to default
5717         # to ACTIVE for backwards compatibility
5718         old_vm_state = instance.system_metadata.get('old_vm_state',
5719                                                     vm_states.ACTIVE)
5720         instance.old_flavor = old_flavor
5721 
5722         if old_instance_type_id != new_instance_type_id:
5723             new_flavor = instance.new_flavor  # this is set in _prep_resize
5724             # Set the flavor-related fields on the instance object including
5725             # making instance.flavor = new_flavor.
5726             self._set_instance_info(instance, new_flavor)
5727             for key in ('root_gb', 'swap', 'ephemeral_gb'):
5728                 if old_flavor[key] != new_flavor[key]:
5729                     resize_instance = True
5730                     break
5731         instance.apply_migration_context()
5732 
5733         # NOTE(tr3buchet): setup networks on destination host
5734         self.network_api.setup_networks_on_host(context, instance,
5735                                                 migration.dest_compute)
5736         provider_mappings = self._get_request_group_mapping(request_spec)
5737 
5738         # For neutron, migrate_instance_finish updates port bindings for this
5739         # host including any PCI devices claimed for SR-IOV ports.
5740         self.network_api.migrate_instance_finish(
5741             context, instance, migration, provider_mappings)
5742 
5743         network_info = self.network_api.get_instance_nw_info(context, instance)
5744 
5745         instance.task_state = task_states.RESIZE_FINISH
5746         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5747 
5748         self._send_finish_resize_notifications(
5749             context, instance, bdms, network_info,
5750             fields.NotificationPhase.START)
5751 
5752         # We need to update any volume attachments using the destination
5753         # host connector so that we can update the BDM.connection_info
5754         # before calling driver.finish_migration otherwise the driver
5755         # won't know how to connect the volumes to this host.
5756         # Note that _get_instance_block_device_info with
5757         # refresh_conn_info=True will update the BDM.connection_info value
5758         # in the database so we must do this before calling that method.
5759         self._update_volume_attachments(context, instance, bdms)
5760 
5761         block_device_info = self._get_instance_block_device_info(
5762             context, instance, refresh_conn_info=True, bdms=bdms)
5763 
5764         # NOTE(mriedem): If the original vm_state was STOPPED, we don't
5765         # automatically power on the instance after it's migrated
5766         power_on = old_vm_state != vm_states.STOPPED
5767 
5768         # NOTE(sbauza): During a migration, the original allocation is against
5769         # the migration UUID while the target allocation (for the destination
5770         # node) is related to the instance UUID, so here we need to pass the
5771         # new ones.
5772         allocations = self.reportclient.get_allocs_for_consumer(
5773             context, instance.uuid)['allocations']
5774 
5775         try:
5776             self.driver.finish_migration(context, migration, instance,
5777                                          disk_info,
5778                                          network_info,
5779                                          image_meta, resize_instance,
5780                                          allocations,
5781                                          block_device_info, power_on)
5782         except Exception:
5783             # Note that we do not rollback port bindings to the source host
5784             # because resize_instance (on the source host) updated the
5785             # instance.host to point to *this* host (the destination host)
5786             # so the port bindings pointing at this host are correct even
5787             # though we failed to create the guest.
5788             with excutils.save_and_reraise_exception():
5789                 # If we failed to create the guest on this host, reset the
5790                 # instance flavor-related fields to the old flavor. An
5791                 # error handler like reverts_task_state will save the changes.
5792                 if old_instance_type_id != new_instance_type_id:
5793                     self._set_instance_info(instance, old_flavor)
5794 
5795         # Now complete any volume attachments that were previously updated.
5796         self._complete_volume_attachments(context, bdms)
5797 
5798         migration.status = 'finished'
5799         migration.save()
5800 
5801         instance.vm_state = vm_states.RESIZED
5802         instance.task_state = None
5803         instance.launched_at = timeutils.utcnow()
5804         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5805 
5806         return network_info
5807 
5808     @wrap_exception()
5809     @reverts_task_state
5810     @wrap_instance_event(prefix='compute')
5811     @errors_out_migration
5812     @wrap_instance_fault
5813     def finish_resize(self, context, disk_info, image, instance,
5814                       migration, request_spec):
5815         """Completes the migration process.
5816 
5817         Sets up the newly transferred disk and turns on the instance at its
5818         new host machine.
5819 
5820         """
5821         try:
5822             self._finish_resize_helper(context, disk_info, image, instance,
5823                                        migration, request_spec)
5824         except Exception:
5825             with excutils.save_and_reraise_exception():
5826                 # At this point, resize_instance (which runs on the source) has
5827                 # already updated the instance host/node values to point to
5828                 # this (the dest) compute, so we need to leave the allocations
5829                 # against the dest node resource provider intact and drop the
5830                 # allocations against the source node resource provider. If the
5831                 # user tries to recover the server by hard rebooting it, it
5832                 # will happen on this host so that's where the allocations
5833                 # should go. Note that this is the same method called from
5834                 # confirm_resize to cleanup the source node allocations held
5835                 # by the migration record.
5836                 LOG.info('Deleting allocations for old flavor on source node '
5837                          '%s after finish_resize failure. You may be able to '
5838                          'recover the instance by hard rebooting it.',
5839                          migration.source_compute, instance=instance)
5840                 self._delete_allocation_after_move(
5841                     context, instance, migration)
5842 
5843     def _finish_resize_helper(self, context, disk_info, image, instance,
5844                               migration, request_spec):
5845         """Completes the migration process.
5846 
5847         The caller must revert the instance's allocations if the migration
5848         process failed.
5849         """
5850         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
5851             context, instance.uuid)
5852 
5853         with self._error_out_instance_on_exception(context, instance):
5854             image_meta = objects.ImageMeta.from_dict(image)
5855             network_info = self._finish_resize(context, instance, migration,
5856                                                disk_info, image_meta, bdms,
5857                                                request_spec)
5858 
5859         # TODO(melwitt): We should clean up instance console tokens here. The
5860         # instance is on a new host and will need to establish a new console
5861         # connection.
5862         self._update_scheduler_instance_info(context, instance)
5863         self._send_finish_resize_notifications(
5864             context, instance, bdms, network_info,
5865             fields.NotificationPhase.END)
5866 
5867     def _send_finish_resize_notifications(
5868             self, context, instance, bdms, network_info, phase):
5869         """Send notifications for the finish_resize flow.
5870 
5871         :param context: nova auth request context
5872         :param instance: The instance being resized
5873         :param bdms: BlockDeviceMappingList for the BDMs associated with the
5874             instance
5875         :param network_info: NetworkInfo for the instance info cache of ports
5876         :param phase: The phase of the action (NotificationPhase enum, either
5877             ``start`` or ``end``)
5878         """
5879         # Send the legacy unversioned notification.
5880         self._notify_about_instance_usage(
5881             context, instance, "finish_resize.%s" % phase,
5882             network_info=network_info)
5883         # Send the versioned notification.
5884         compute_utils.notify_about_instance_action(
5885             context, instance, self.host,
5886             action=fields.NotificationAction.RESIZE_FINISH, phase=phase,
5887             bdms=bdms)
5888 
5889     @wrap_exception()
5890     @reverts_task_state
5891     @wrap_instance_event(prefix='compute')
5892     @errors_out_migration
5893     @wrap_instance_fault
5894     def finish_snapshot_based_resize_at_dest(
5895             self, ctxt, instance, migration, snapshot_id):
5896         """Finishes the snapshot-based resize at the destination compute.
5897 
5898         Sets up block devices and networking on the destination compute and
5899         spawns the guest.
5900 
5901         :param ctxt: nova auth request context targeted at the target cell DB
5902         :param instance: The Instance object being resized with the
5903             ``migration_context`` field set. Upon successful completion of this
5904             method the vm_state should be "resized", the task_state should be
5905             None, and migration context, host/node and flavor-related fields
5906             should be set on the instance.
5907         :param migration: The Migration object for this resize operation. Upon
5908             successful completion of this method the migration status should
5909             be "finished".
5910         :param snapshot_id: ID of the image snapshot created for a
5911             non-volume-backed instance, else None.
5912         """
5913         LOG.info('Finishing snapshot based resize on destination host %s.',
5914                  self.host, instance=instance)
5915         with self._error_out_instance_on_exception(ctxt, instance):
5916             # Note that if anything fails here, the migration-based allocations
5917             # created in conductor should be reverted by conductor as well,
5918             # see MigrationTask.rollback.
5919             self._finish_snapshot_based_resize_at_dest(
5920                 ctxt, instance, migration, snapshot_id)
5921 
5922     def _finish_snapshot_based_resize_at_dest(
5923             self, ctxt, instance, migration, snapshot_id):
5924         """Private variant of finish_snapshot_based_resize_at_dest so the
5925         caller can handle reverting resource allocations on failure and perform
5926         other generic error handling.
5927         """
5928         # Figure out the image metadata to use when spawning the guest.
5929         origin_image_ref = instance.image_ref
5930         if snapshot_id:
5931             instance.image_ref = snapshot_id
5932             image_meta = objects.ImageMeta.from_image_ref(
5933                 ctxt, self.image_api, snapshot_id)
5934         else:
5935             # Just use what is already on the volume-backed instance.
5936             image_meta = instance.image_meta
5937 
5938         resize = migration.migration_type == 'resize'
5939         instance.old_flavor = instance.flavor
5940         if resize:
5941             flavor = instance.new_flavor
5942             # If we are resizing to a new flavor we need to set the
5943             # flavor-related fields on the instance.
5944             # NOTE(mriedem): This is likely where storing old/new_flavor on
5945             # the MigrationContext would make this cleaner.
5946             self._set_instance_info(instance, flavor)
5947 
5948         instance.apply_migration_context()
5949         instance.task_state = task_states.RESIZE_FINISH
5950         instance.save(expected_task_state=task_states.RESIZE_MIGRATED)
5951 
5952         # This seems a bit late to be sending the start notification but
5953         # it is what traditional resize has always done as well and it does
5954         # contain the changes to the instance with the new_flavor and
5955         # task_state.
5956         bdms = instance.get_bdms()
5957         network_info = instance.get_network_info()
5958         self._send_finish_resize_notifications(
5959             ctxt, instance, bdms, network_info,
5960             fields.NotificationPhase.START)
5961 
5962         # Setup volumes and networking and spawn the guest in the hypervisor.
5963         self._finish_snapshot_based_resize_at_dest_spawn(
5964             ctxt, instance, migration, image_meta, bdms)
5965 
5966         # If we spawned from a temporary snapshot image we can delete that now,
5967         # similar to how unshelve works.
5968         if snapshot_id:
5969             instance.image_ref = origin_image_ref
5970             compute_utils.delete_image(
5971                 ctxt, instance, self.image_api, snapshot_id)
5972 
5973         migration.status = 'finished'
5974         migration.save()
5975 
5976         self._update_instance_after_spawn(instance, vm_state=vm_states.RESIZED)
5977         # Setting the host/node values will make the ResourceTracker continue
5978         # to track usage for this instance on this host.
5979         instance.host = migration.dest_compute
5980         instance.node = migration.dest_node
5981         instance.save(expected_task_state=task_states.RESIZE_FINISH)
5982 
5983         # Broadcast to all schedulers that the instance is on this host.
5984         self._update_scheduler_instance_info(ctxt, instance)
5985         self._send_finish_resize_notifications(
5986             ctxt, instance, bdms, network_info,
5987             fields.NotificationPhase.END)
5988 
5989     def _finish_snapshot_based_resize_at_dest_spawn(
5990             self, ctxt, instance, migration, image_meta, bdms):
5991         """Sets up volumes and networking and spawns the guest on the dest host
5992 
5993         If the instance was stopped when the resize was initiated the guest
5994         will be created but remain in a shutdown power state.
5995 
5996         If the spawn fails, port bindings are rolled back to the source host
5997         and volume connections are terminated for this dest host.
5998 
5999         :param ctxt: nova auth request context
6000         :param instance: Instance object being migrated
6001         :param migration: Migration object for the operation
6002         :param image_meta: ImageMeta object used during driver.spawn
6003         :param bdms: BlockDeviceMappingList of BDMs for the instance
6004         """
6005         # Update the volume attachments using this host's connector.
6006         # That will update the BlockDeviceMapping.connection_info which
6007         # will be used to connect the volumes on this host during spawn().
6008         block_device_info = self._prep_block_device(ctxt, instance, bdms)
6009 
6010         allocations = self.reportclient.get_allocations_for_consumer(
6011             ctxt, instance.uuid)
6012 
6013         # We do not call self.network_api.setup_networks_on_host here because
6014         # for neutron that sets up the port migration profile which is only
6015         # used during live migration with DVR. Yes it is gross knowing what
6016         # that method does internally. We could change this when bug 1814837
6017         # is fixed if setup_networks_on_host is made smarter by passing the
6018         # migration record and the method checks the migration_type.
6019 
6020         # Activate the port bindings for this host.
6021         # FIXME(mriedem): We're going to have the same issue as bug 1813789
6022         # here because this will update the port bindings and send the
6023         # network-vif-plugged event and that means when driver.spawn waits for
6024         # it we might have already gotten the event and neutron won't send
6025         # another one so we could timeout.
6026         # TODO(mriedem): Calculate provider mappings when we support cross-cell
6027         # resize/migrate with ports having resource requests.
6028         self.network_api.migrate_instance_finish(
6029             ctxt, instance, migration, provider_mappings=None)
6030         network_info = self.network_api.get_instance_nw_info(ctxt, instance)
6031 
6032         # If the original vm_state was STOPPED, we do not automatically
6033         # power on the instance after it is migrated.
6034         power_on = instance.system_metadata['old_vm_state'] == vm_states.ACTIVE
6035         try:
6036             # NOTE(mriedem): If this instance uses a config drive, it will get
6037             # rebuilt here which means any personality files will be lost,
6038             # similar to unshelve. If the instance is not using a config drive
6039             # and getting metadata from the metadata API service, personality
6040             # files would be lost regardless of the move operation.
6041             self.driver.spawn(
6042                 ctxt, instance, image_meta, injected_files=[],
6043                 admin_password=None, allocations=allocations,
6044                 network_info=network_info, block_device_info=block_device_info,
6045                 power_on=power_on)
6046         except Exception:
6047             with excutils.save_and_reraise_exception(logger=LOG):
6048                 # Rollback port bindings to the source host.
6049                 try:
6050                     # This is gross but migrate_instance_start looks at the
6051                     # migration.dest_compute to determine where to activate the
6052                     # port bindings and we want the source compute port
6053                     # bindings to be re-activated. Remember at this point the
6054                     # instance.host is still pointing at the source compute.
6055                     # TODO(mriedem): Maybe we should be calling
6056                     # setup_instance_network_on_host here to deal with pci
6057                     # devices?
6058                     with utils.temporary_mutation(
6059                             migration, dest_compute=migration.source_compute):
6060                         self.network_api.migrate_instance_start(
6061                             ctxt, instance, migration)
6062                 except Exception:
6063                     LOG.exception(
6064                         'Failed to activate port bindings on the source '
6065                         'host: %s', migration.source_compute,
6066                         instance=instance)
6067 
6068                 # Rollback volume connections on this host.
6069                 for bdm in bdms:
6070                     if bdm.is_volume:
6071                         try:
6072                             self._remove_volume_connection(
6073                                 ctxt, bdm, instance, delete_attachment=True)
6074                         except Exception:
6075                             LOG.exception('Failed to remove volume connection '
6076                                           'on this host %s for volume %s.',
6077                                           self.host, bdm.volume_id,
6078                                           instance=instance)
6079 
6080     @wrap_exception()
6081     @wrap_instance_fault
6082     def add_fixed_ip_to_instance(self, context, network_id, instance):
6083         """Calls network_api to add new fixed_ip to instance
6084         then injects the new network info and resets instance networking.
6085 
6086         """
6087         self._notify_about_instance_usage(
6088                 context, instance, "create_ip.start")
6089 
6090         network_info = self.network_api.add_fixed_ip_to_instance(context,
6091                                                                  instance,
6092                                                                  network_id)
6093         self._inject_network_info(instance, network_info)
6094 
6095         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6096         instance.updated_at = timeutils.utcnow()
6097         instance.save()
6098 
6099         self._notify_about_instance_usage(
6100             context, instance, "create_ip.end", network_info=network_info)
6101 
6102     @wrap_exception()
6103     @wrap_instance_fault
6104     def remove_fixed_ip_from_instance(self, context, address, instance):
6105         """Calls network_api to remove existing fixed_ip from instance
6106         by injecting the altered network info and resetting
6107         instance networking.
6108         """
6109         self._notify_about_instance_usage(
6110                 context, instance, "delete_ip.start")
6111 
6112         network_info = self.network_api.remove_fixed_ip_from_instance(context,
6113                                                                       instance,
6114                                                                       address)
6115         self._inject_network_info(instance, network_info)
6116 
6117         # NOTE(russellb) We just want to bump updated_at.  See bug 1143466.
6118         instance.updated_at = timeutils.utcnow()
6119         instance.save()
6120 
6121         self._notify_about_instance_usage(
6122             context, instance, "delete_ip.end", network_info=network_info)
6123 
6124     @wrap_exception()
6125     @reverts_task_state
6126     @wrap_instance_event(prefix='compute')
6127     @wrap_instance_fault
6128     def pause_instance(self, context, instance):
6129         """Pause an instance on this host."""
6130         context = context.elevated()
6131         LOG.info('Pausing', instance=instance)
6132         self._notify_about_instance_usage(context, instance, 'pause.start')
6133         compute_utils.notify_about_instance_action(context, instance,
6134                self.host, action=fields.NotificationAction.PAUSE,
6135                phase=fields.NotificationPhase.START)
6136         self.driver.pause(instance)
6137         instance.power_state = self._get_power_state(instance)
6138         instance.vm_state = vm_states.PAUSED
6139         instance.task_state = None
6140         instance.save(expected_task_state=task_states.PAUSING)
6141         self._notify_about_instance_usage(context, instance, 'pause.end')
6142         compute_utils.notify_about_instance_action(context, instance,
6143                self.host, action=fields.NotificationAction.PAUSE,
6144                phase=fields.NotificationPhase.END)
6145 
6146     @wrap_exception()
6147     @reverts_task_state
6148     @wrap_instance_event(prefix='compute')
6149     @wrap_instance_fault
6150     def unpause_instance(self, context, instance):
6151         """Unpause a paused instance on this host."""
6152         context = context.elevated()
6153         LOG.info('Unpausing', instance=instance)
6154         self._notify_about_instance_usage(context, instance, 'unpause.start')
6155         compute_utils.notify_about_instance_action(context, instance,
6156             self.host, action=fields.NotificationAction.UNPAUSE,
6157             phase=fields.NotificationPhase.START)
6158         self.driver.unpause(instance)
6159         instance.power_state = self._get_power_state(instance)
6160         instance.vm_state = vm_states.ACTIVE
6161         instance.task_state = None
6162         instance.save(expected_task_state=task_states.UNPAUSING)
6163         self._notify_about_instance_usage(context, instance, 'unpause.end')
6164         compute_utils.notify_about_instance_action(context, instance,
6165             self.host, action=fields.NotificationAction.UNPAUSE,
6166             phase=fields.NotificationPhase.END)
6167 
6168     @wrap_exception()
6169     def host_power_action(self, context, action):
6170         """Reboots, shuts down or powers up the host."""
6171         return self.driver.host_power_action(action)
6172 
6173     @wrap_exception()
6174     def host_maintenance_mode(self, context, host, mode):
6175         """Start/Stop host maintenance window. On start, it triggers
6176         guest VMs evacuation.
6177         """
6178         return self.driver.host_maintenance_mode(host, mode)
6179 
6180     def _update_compute_provider_status(self, context, enabled):
6181         """Adds or removes the COMPUTE_STATUS_DISABLED trait for this host.
6182 
6183         For each ComputeNode managed by this service, adds or removes the
6184         COMPUTE_STATUS_DISABLED traits to/from the associated resource provider
6185         in Placement.
6186 
6187         :param context: nova auth RequestContext
6188         :param enabled: True if the node is enabled in which case the trait
6189             would be removed, False if the node is disabled in which case
6190             the trait would be added.
6191         :raises: ComputeHostNotFound if there are no compute nodes found in
6192             the ResourceTracker for this service.
6193         """
6194         # Get the compute node(s) on this host. Remember that ironic can be
6195         # managing more than one compute node.
6196         nodes = self.rt.compute_nodes.values()
6197         if not nodes:
6198             raise exception.ComputeHostNotFound(host=self.host)
6199         # For each node, we want to add (or remove) the COMPUTE_STATUS_DISABLED
6200         # trait on the related resource provider in placement so the scheduler
6201         # (pre-)filters the provider based on its status.
6202         for node in nodes:
6203             try:
6204                 self.virtapi.update_compute_provider_status(
6205                     context, node.uuid, enabled)
6206             except (exception.ResourceProviderTraitRetrievalFailed,
6207                     exception.ResourceProviderUpdateConflict,
6208                     exception.ResourceProviderUpdateFailed,
6209                     exception.TraitRetrievalFailed) as e:
6210                 # This is best effort so just log a warning and continue.
6211                 LOG.warning('An error occurred while updating '
6212                             'COMPUTE_STATUS_DISABLED trait on compute node '
6213                             'resource provider %s. The trait will be '
6214                             'synchronized when the update_available_resource '
6215                             'periodic task runs. Error: %s',
6216                             node.uuid, e.format_message())
6217             except Exception:
6218                 LOG.exception('An error occurred while updating '
6219                               'COMPUTE_STATUS_DISABLED trait on compute node '
6220                               'resource provider %s. The trait will be '
6221                               'synchronized when the '
6222                               'update_available_resource periodic task runs.',
6223                               node.uuid)
6224 
6225     @wrap_exception()
6226     def set_host_enabled(self, context, enabled):
6227         """Sets the specified host's ability to accept new instances.
6228 
6229         This method will add or remove the COMPUTE_STATUS_DISABLED trait
6230         to/from the associated compute node resource provider(s) for this
6231         compute service.
6232         """
6233         try:
6234             self._update_compute_provider_status(context, enabled)
6235         except exception.ComputeHostNotFound:
6236             LOG.warning('Unable to add/remove trait COMPUTE_STATUS_DISABLED. '
6237                         'No ComputeNode(s) found for host: %s', self.host)
6238 
6239         try:
6240             return self.driver.set_host_enabled(enabled)
6241         except NotImplementedError:
6242             # Only the xenapi driver implements set_host_enabled but we don't
6243             # want NotImplementedError to get raised back to the API. We still
6244             # need to honor the compute RPC API contract and return 'enabled'
6245             # or 'disabled' though.
6246             return 'enabled' if enabled else 'disabled'
6247 
6248     @wrap_exception()
6249     def get_host_uptime(self, context):
6250         """Returns the result of calling "uptime" on the target host."""
6251         return self.driver.get_host_uptime()
6252 
6253     @wrap_exception()
6254     @wrap_instance_fault
6255     def get_diagnostics(self, context, instance):
6256         """Retrieve diagnostics for an instance on this host."""
6257         current_power_state = self._get_power_state(instance)
6258         if current_power_state == power_state.RUNNING:
6259             LOG.info("Retrieving diagnostics", instance=instance)
6260             return self.driver.get_diagnostics(instance)
6261         else:
6262             raise exception.InstanceInvalidState(
6263                 attr='power state',
6264                 instance_uuid=instance.uuid,
6265                 state=power_state.STATE_MAP[instance.power_state],
6266                 method='get_diagnostics')
6267 
6268     @wrap_exception()
6269     @wrap_instance_fault
6270     def get_instance_diagnostics(self, context, instance):
6271         """Retrieve diagnostics for an instance on this host."""
6272         current_power_state = self._get_power_state(instance)
6273         if current_power_state == power_state.RUNNING:
6274             LOG.info("Retrieving diagnostics", instance=instance)
6275             return self.driver.get_instance_diagnostics(instance)
6276         else:
6277             raise exception.InstanceInvalidState(
6278                 attr='power state',
6279                 instance_uuid=instance.uuid,
6280                 state=power_state.STATE_MAP[instance.power_state],
6281                 method='get_diagnostics')
6282 
6283     @wrap_exception()
6284     @reverts_task_state
6285     @wrap_instance_event(prefix='compute')
6286     @wrap_instance_fault
6287     def suspend_instance(self, context, instance):
6288         """Suspend the given instance."""
6289         context = context.elevated()
6290 
6291         # Store the old state
6292         instance.system_metadata['old_vm_state'] = instance.vm_state
6293         self._notify_about_instance_usage(context, instance, 'suspend.start')
6294         compute_utils.notify_about_instance_action(context, instance,
6295                 self.host, action=fields.NotificationAction.SUSPEND,
6296                 phase=fields.NotificationPhase.START)
6297         with self._error_out_instance_on_exception(context, instance,
6298              instance_state=instance.vm_state):
6299             self.driver.suspend(context, instance)
6300         instance.power_state = self._get_power_state(instance)
6301         instance.vm_state = vm_states.SUSPENDED
6302         instance.task_state = None
6303         instance.save(expected_task_state=task_states.SUSPENDING)
6304         self._notify_about_instance_usage(context, instance, 'suspend.end')
6305         compute_utils.notify_about_instance_action(context, instance,
6306                 self.host, action=fields.NotificationAction.SUSPEND,
6307                 phase=fields.NotificationPhase.END)
6308 
6309     @wrap_exception()
6310     @reverts_task_state
6311     @wrap_instance_event(prefix='compute')
6312     @wrap_instance_fault
6313     def resume_instance(self, context, instance):
6314         """Resume the given suspended instance."""
6315         context = context.elevated()
6316         LOG.info('Resuming', instance=instance)
6317 
6318         self._notify_about_instance_usage(context, instance, 'resume.start')
6319 
6320         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6321             context, instance.uuid)
6322         block_device_info = self._get_instance_block_device_info(
6323             context, instance, bdms=bdms)
6324 
6325         compute_utils.notify_about_instance_action(context, instance,
6326             self.host, action=fields.NotificationAction.RESUME,
6327             phase=fields.NotificationPhase.START, bdms=bdms)
6328 
6329         network_info = self.network_api.get_instance_nw_info(context, instance)
6330 
6331         with self._error_out_instance_on_exception(context, instance,
6332              instance_state=instance.vm_state):
6333             self.driver.resume(context, instance, network_info,
6334                                block_device_info)
6335 
6336         instance.power_state = self._get_power_state(instance)
6337 
6338         # We default to the ACTIVE state for backwards compatibility
6339         instance.vm_state = instance.system_metadata.pop('old_vm_state',
6340                                                          vm_states.ACTIVE)
6341 
6342         instance.task_state = None
6343         instance.save(expected_task_state=task_states.RESUMING)
6344         self._notify_about_instance_usage(context, instance, 'resume.end')
6345         compute_utils.notify_about_instance_action(context, instance,
6346             self.host, action=fields.NotificationAction.RESUME,
6347             phase=fields.NotificationPhase.END, bdms=bdms)
6348 
6349     @wrap_exception()
6350     @reverts_task_state
6351     @wrap_instance_event(prefix='compute')
6352     @wrap_instance_fault
6353     def shelve_instance(self, context, instance, image_id,
6354                         clean_shutdown, accel_uuids):
6355         """Shelve an instance.
6356 
6357         This should be used when you want to take a snapshot of the instance.
6358         It also adds system_metadata that can be used by a periodic task to
6359         offload the shelved instance after a period of time.
6360 
6361         :param context: request context
6362         :param instance: an Instance object
6363         :param image_id: an image id to snapshot to.
6364         :param clean_shutdown: give the GuestOS a chance to stop
6365         :param accel_uuids: the accelerators uuids for the instance
6366         """
6367 
6368         @utils.synchronized(instance.uuid)
6369         def do_shelve_instance():
6370             self._shelve_instance(context, instance, image_id, clean_shutdown,
6371                                   accel_uuids)
6372         do_shelve_instance()
6373 
6374     def _shelve_instance(self, context, instance, image_id,
6375                          clean_shutdown, accel_uuids=None):
6376         LOG.info('Shelving', instance=instance)
6377         offload = CONF.shelved_offload_time == 0
6378         if offload:
6379             # Get the BDMs early so we can pass them into versioned
6380             # notifications since _shelve_offload_instance needs the
6381             # BDMs anyway.
6382             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6383                 context, instance.uuid)
6384         else:
6385             bdms = None
6386         compute_utils.notify_usage_exists(self.notifier, context, instance,
6387                                           self.host, current_period=True)
6388         self._notify_about_instance_usage(context, instance, 'shelve.start')
6389         compute_utils.notify_about_instance_action(context, instance,
6390                 self.host, action=fields.NotificationAction.SHELVE,
6391                 phase=fields.NotificationPhase.START, bdms=bdms)
6392 
6393         def update_task_state(task_state, expected_state=task_states.SHELVING):
6394             shelving_state_map = {
6395                     task_states.IMAGE_PENDING_UPLOAD:
6396                         task_states.SHELVING_IMAGE_PENDING_UPLOAD,
6397                     task_states.IMAGE_UPLOADING:
6398                         task_states.SHELVING_IMAGE_UPLOADING,
6399                     task_states.SHELVING: task_states.SHELVING}
6400             task_state = shelving_state_map[task_state]
6401             expected_state = shelving_state_map[expected_state]
6402             instance.task_state = task_state
6403             instance.save(expected_task_state=expected_state)
6404         # Do not attempt a clean shutdown of a paused guest since some
6405         # hypervisors will fail the clean shutdown if the guest is not
6406         # running.
6407         if instance.power_state == power_state.PAUSED:
6408             clean_shutdown = False
6409         self._power_off_instance(instance, clean_shutdown)
6410         self.driver.snapshot(context, instance, image_id, update_task_state)
6411 
6412         instance.system_metadata['shelved_at'] = timeutils.utcnow().isoformat()
6413         instance.system_metadata['shelved_image_id'] = image_id
6414         instance.system_metadata['shelved_host'] = self.host
6415         instance.vm_state = vm_states.SHELVED
6416         instance.task_state = None
6417         if offload:
6418             instance.task_state = task_states.SHELVING_OFFLOADING
6419         instance.power_state = self._get_power_state(instance)
6420         instance.save(expected_task_state=[
6421                 task_states.SHELVING,
6422                 task_states.SHELVING_IMAGE_UPLOADING])
6423 
6424         self._notify_about_instance_usage(context, instance, 'shelve.end')
6425         compute_utils.notify_about_instance_action(context, instance,
6426                 self.host, action=fields.NotificationAction.SHELVE,
6427                 phase=fields.NotificationPhase.END, bdms=bdms)
6428 
6429         if offload:
6430             self._shelve_offload_instance(
6431                 context, instance, clean_shutdown=False, bdms=bdms,
6432                 accel_uuids=accel_uuids)
6433 
6434     @wrap_exception()
6435     @reverts_task_state
6436     @wrap_instance_event(prefix='compute')
6437     @wrap_instance_fault
6438     def shelve_offload_instance(self, context, instance, clean_shutdown,
6439             accel_uuids):
6440         """Remove a shelved instance from the hypervisor.
6441 
6442         This frees up those resources for use by other instances, but may lead
6443         to slower unshelve times for this instance.  This method is used by
6444         volume backed instances since restoring them doesn't involve the
6445         potentially large download of an image.
6446 
6447         :param context: request context
6448         :param instance: nova.objects.instance.Instance
6449         :param clean_shutdown: give the GuestOS a chance to stop
6450         :param accel_uuids: the accelerators uuids for the instance
6451         """
6452 
6453         @utils.synchronized(instance.uuid)
6454         def do_shelve_offload_instance():
6455             self._shelve_offload_instance(context, instance, clean_shutdown,
6456                                           accel_uuids=accel_uuids)
6457         do_shelve_offload_instance()
6458 
6459     def _shelve_offload_instance(self, context, instance, clean_shutdown,
6460                                  bdms=None, accel_uuids=None):
6461         LOG.info('Shelve offloading', instance=instance)
6462         if bdms is None:
6463             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6464                 context, instance.uuid)
6465         self._notify_about_instance_usage(context, instance,
6466                 'shelve_offload.start')
6467         compute_utils.notify_about_instance_action(context, instance,
6468                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6469                 phase=fields.NotificationPhase.START, bdms=bdms)
6470 
6471         self._power_off_instance(instance, clean_shutdown)
6472         current_power_state = self._get_power_state(instance)
6473         network_info = self.network_api.get_instance_nw_info(context, instance)
6474 
6475         block_device_info = self._get_instance_block_device_info(context,
6476                                                                  instance,
6477                                                                  bdms=bdms)
6478         self.driver.destroy(context, instance, network_info,
6479                 block_device_info)
6480 
6481         # the instance is going to be removed from the host so we want to
6482         # terminate all the connections with the volume server and the host
6483         self._terminate_volume_connections(context, instance, bdms)
6484 
6485         # NOTE(brinzhang): Free up the accelerator resource occupied
6486         # in the cyborg service.
6487         if accel_uuids:
6488             cyclient = cyborg.get_client(context)
6489             cyclient.delete_arqs_for_instance(instance.uuid)
6490 
6491         # Free up the resource allocations in the placement service.
6492         # This should happen *before* the vm_state is changed to
6493         # SHELVED_OFFLOADED in case client-side code is polling the API to
6494         # schedule more instances (or unshelve) once this server is offloaded.
6495         self.rt.delete_allocation_for_shelve_offloaded_instance(context,
6496                                                                 instance)
6497 
6498         instance.power_state = current_power_state
6499         # NOTE(mriedem): The vm_state has to be set before updating the
6500         # resource tracker, see vm_states.ALLOW_RESOURCE_REMOVAL. The host/node
6501         # values cannot be nulled out until after updating the resource tracker
6502         # though.
6503         instance.vm_state = vm_states.SHELVED_OFFLOADED
6504         instance.task_state = None
6505         instance.save(expected_task_state=[task_states.SHELVING,
6506                                            task_states.SHELVING_OFFLOADING])
6507 
6508         # NOTE(ndipanov): Free resources from the resource tracker
6509         self._update_resource_tracker(context, instance)
6510 
6511         # NOTE(sfinucan): RPC calls should no longer be attempted against this
6512         # instance, so ensure any calls result in errors
6513         self._nil_out_instance_obj_host_and_node(instance)
6514         instance.save(expected_task_state=None)
6515 
6516         # TODO(melwitt): We should clean up instance console tokens here. The
6517         # instance has no host at this point and will need to establish a new
6518         # console connection in the future after it is unshelved.
6519         self._delete_scheduler_instance_info(context, instance.uuid)
6520         self._notify_about_instance_usage(context, instance,
6521                 'shelve_offload.end')
6522         compute_utils.notify_about_instance_action(context, instance,
6523                 self.host, action=fields.NotificationAction.SHELVE_OFFLOAD,
6524                 phase=fields.NotificationPhase.END, bdms=bdms)
6525 
6526     @wrap_exception()
6527     @reverts_task_state
6528     @wrap_instance_event(prefix='compute')
6529     @wrap_instance_fault
6530     def unshelve_instance(self, context, instance, image,
6531                           filter_properties, node, request_spec, accel_uuids):
6532         """Unshelve the instance.
6533 
6534         :param context: request context
6535         :param instance: a nova.objects.instance.Instance object
6536         :param image: an image to build from.  If None we assume a
6537             volume backed instance.
6538         :param filter_properties: dict containing limits, retry info etc.
6539         :param node: target compute node
6540         :param request_spec: the RequestSpec object used to schedule the
6541             instance
6542         :param accel_uuids: the accelerators uuids for the instance
6543         """
6544         if filter_properties is None:
6545             filter_properties = {}
6546 
6547         @utils.synchronized(instance.uuid)
6548         def do_unshelve_instance():
6549             self._unshelve_instance(
6550                 context, instance, image, filter_properties, node,
6551                 request_spec, accel_uuids)
6552         do_unshelve_instance()
6553 
6554     def _unshelve_instance_key_scrub(self, instance):
6555         """Remove data from the instance that may cause side effects."""
6556         cleaned_keys = dict(
6557                 key_data=instance.key_data,
6558                 auto_disk_config=instance.auto_disk_config)
6559         instance.key_data = None
6560         instance.auto_disk_config = False
6561         return cleaned_keys
6562 
6563     def _unshelve_instance_key_restore(self, instance, keys):
6564         """Restore previously scrubbed keys before saving the instance."""
6565         instance.update(keys)
6566 
6567     def _unshelve_instance(self, context, instance, image, filter_properties,
6568                            node, request_spec, accel_uuids):
6569         LOG.info('Unshelving', instance=instance)
6570         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
6571                 context, instance.uuid)
6572 
6573         self._notify_about_instance_usage(context, instance, 'unshelve.start')
6574         compute_utils.notify_about_instance_action(context, instance,
6575                 self.host, action=fields.NotificationAction.UNSHELVE,
6576                 phase=fields.NotificationPhase.START, bdms=bdms)
6577 
6578         instance.task_state = task_states.SPAWNING
6579         instance.save()
6580 
6581         block_device_info = self._prep_block_device(context, instance, bdms)
6582         scrubbed_keys = self._unshelve_instance_key_scrub(instance)
6583 
6584         if node is None:
6585             node = self._get_nodename(instance)
6586 
6587         limits = filter_properties.get('limits', {})
6588 
6589         allocations = self.reportclient.get_allocations_for_consumer(
6590             context, instance.uuid)
6591 
6592         shelved_image_ref = instance.image_ref
6593         if image:
6594             instance.image_ref = image['id']
6595             image_meta = objects.ImageMeta.from_dict(image)
6596         else:
6597             image_meta = objects.ImageMeta.from_dict(
6598                 utils.get_image_from_system_metadata(
6599                     instance.system_metadata))
6600 
6601         provider_mappings = self._get_request_group_mapping(request_spec)
6602 
6603         try:
6604             if provider_mappings:
6605                 update = (
6606                     compute_utils.
6607                         update_pci_request_spec_with_allocated_interface_name)
6608                 update(
6609                     context, self.reportclient, instance.pci_requests.requests,
6610                     provider_mappings)
6611 
6612             self.network_api.setup_instance_network_on_host(
6613                 context, instance, self.host,
6614                 provider_mappings=provider_mappings)
6615             network_info = self.network_api.get_instance_nw_info(
6616                 context, instance)
6617 
6618             accel_info = []
6619             if accel_uuids:
6620                 try:
6621                     accel_info = self._get_bound_arq_resources(
6622                         context, instance, accel_uuids)
6623                 except (Exception, eventlet.timeout.Timeout) as exc:
6624                     LOG.exception('Failure getting accelerator requests '
6625                                   'with the exception: %s', exc,
6626                                   instance=instance)
6627                     self._build_resources_cleanup(instance, network_info)
6628                     raise
6629 
6630             with self.rt.instance_claim(context, instance, node, allocations,
6631                                         limits):
6632                 self.driver.spawn(context, instance, image_meta,
6633                                   injected_files=[],
6634                                   admin_password=None,
6635                                   allocations=allocations,
6636                                   network_info=network_info,
6637                                   block_device_info=block_device_info,
6638                                   accel_info=accel_info)
6639         except Exception:
6640             with excutils.save_and_reraise_exception(logger=LOG):
6641                 LOG.exception('Instance failed to spawn',
6642                               instance=instance)
6643                 # Cleanup allocations created by the scheduler on this host
6644                 # since we failed to spawn the instance. We do this both if
6645                 # the instance claim failed with ComputeResourcesUnavailable
6646                 # or if we did claim but the spawn failed, because aborting the
6647                 # instance claim will not remove the allocations.
6648                 self.reportclient.delete_allocation_for_instance(context,
6649                                                                  instance.uuid)
6650                 # FIXME: Umm, shouldn't we be rolling back port bindings too?
6651                 self._terminate_volume_connections(context, instance, bdms)
6652                 # The reverts_task_state decorator on unshelve_instance will
6653                 # eventually save these updates.
6654                 self._nil_out_instance_obj_host_and_node(instance)
6655 
6656         if image:
6657             instance.image_ref = shelved_image_ref
6658             self._delete_snapshot_of_shelved_instance(context, instance,
6659                                                       image['id'])
6660 
6661         self._unshelve_instance_key_restore(instance, scrubbed_keys)
6662         self._update_instance_after_spawn(instance)
6663         # Delete system_metadata for a shelved instance
6664         compute_utils.remove_shelved_keys_from_system_metadata(instance)
6665 
6666         instance.save(expected_task_state=task_states.SPAWNING)
6667         self._update_scheduler_instance_info(context, instance)
6668         self._notify_about_instance_usage(context, instance, 'unshelve.end')
6669         compute_utils.notify_about_instance_action(context, instance,
6670                 self.host, action=fields.NotificationAction.UNSHELVE,
6671                 phase=fields.NotificationPhase.END, bdms=bdms)
6672 
6673     def _inject_network_info(self, instance, network_info):
6674         """Inject network info for the given instance."""
6675         LOG.debug('Inject network info', instance=instance)
6676         LOG.debug('network_info to inject: |%s|', network_info,
6677                   instance=instance)
6678 
6679         self.driver.inject_network_info(instance, network_info)
6680 
6681     @wrap_instance_fault
6682     def inject_network_info(self, context, instance):
6683         """Inject network info, but don't return the info."""
6684         network_info = self.network_api.get_instance_nw_info(context, instance)
6685         self._inject_network_info(instance, network_info)
6686 
6687     @messaging.expected_exceptions(NotImplementedError,
6688                                    exception.ConsoleNotAvailable,
6689                                    exception.InstanceNotFound)
6690     @wrap_exception()
6691     @wrap_instance_fault
6692     def get_console_output(self, context, instance, tail_length):
6693         """Send the console output for the given instance."""
6694         context = context.elevated()
6695         LOG.info("Get console output", instance=instance)
6696         output = self.driver.get_console_output(context, instance)
6697 
6698         if type(output) is str:
6699             output = output.encode("latin-1")
6700 
6701         if tail_length is not None:
6702             output = self._tail_log(output, tail_length)
6703 
6704         return output.decode('ascii', 'replace')
6705 
6706     def _tail_log(self, log, length):
6707         try:
6708             length = int(length)
6709         except ValueError:
6710             length = 0
6711 
6712         if length == 0:
6713             return b''
6714         else:
6715             return b'\n'.join(log.split(b'\n')[-int(length):])
6716 
6717     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6718                                    exception.InstanceNotReady,
6719                                    exception.InstanceNotFound,
6720                                    exception.ConsoleTypeUnavailable,
6721                                    NotImplementedError)
6722     @wrap_exception()
6723     @wrap_instance_fault
6724     def get_vnc_console(self, context, console_type, instance):
6725         """Return connection information for a vnc console."""
6726         context = context.elevated()
6727         LOG.debug("Getting vnc console", instance=instance)
6728 
6729         if not CONF.vnc.enabled:
6730             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6731 
6732         if console_type == 'novnc':
6733             # For essex, novncproxy_base_url must include the full path
6734             # including the html file (like http://myhost/vnc_auto.html)
6735             access_url_base = CONF.vnc.novncproxy_base_url
6736         else:
6737             raise exception.ConsoleTypeInvalid(console_type=console_type)
6738 
6739         try:
6740             # Retrieve connect info from driver, and then decorate with our
6741             # access info token
6742             console = self.driver.get_vnc_console(context, instance)
6743             console_auth = objects.ConsoleAuthToken(
6744                 context=context,
6745                 console_type=console_type,
6746                 host=console.host,
6747                 port=console.port,
6748                 internal_access_path=console.internal_access_path,
6749                 instance_uuid=instance.uuid,
6750                 access_url_base=access_url_base,
6751             )
6752             console_auth.authorize(CONF.consoleauth.token_ttl)
6753             connect_info = console.get_connection_info(
6754                 console_auth.token, console_auth.access_url)
6755 
6756         except exception.InstanceNotFound:
6757             if instance.vm_state != vm_states.BUILDING:
6758                 raise
6759             raise exception.InstanceNotReady(instance_id=instance.uuid)
6760 
6761         return connect_info
6762 
6763     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6764                                    exception.InstanceNotReady,
6765                                    exception.InstanceNotFound,
6766                                    exception.ConsoleTypeUnavailable,
6767                                    NotImplementedError)
6768     @wrap_exception()
6769     @wrap_instance_fault
6770     def get_spice_console(self, context, console_type, instance):
6771         """Return connection information for a spice console."""
6772         context = context.elevated()
6773         LOG.debug("Getting spice console", instance=instance)
6774 
6775         if not CONF.spice.enabled:
6776             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6777 
6778         if console_type != 'spice-html5':
6779             raise exception.ConsoleTypeInvalid(console_type=console_type)
6780 
6781         try:
6782             # Retrieve connect info from driver, and then decorate with our
6783             # access info token
6784             console = self.driver.get_spice_console(context, instance)
6785             console_auth = objects.ConsoleAuthToken(
6786                 context=context,
6787                 console_type=console_type,
6788                 host=console.host,
6789                 port=console.port,
6790                 internal_access_path=console.internal_access_path,
6791                 instance_uuid=instance.uuid,
6792                 access_url_base=CONF.spice.html5proxy_base_url,
6793             )
6794             console_auth.authorize(CONF.consoleauth.token_ttl)
6795             connect_info = console.get_connection_info(
6796                 console_auth.token, console_auth.access_url)
6797 
6798         except exception.InstanceNotFound:
6799             if instance.vm_state != vm_states.BUILDING:
6800                 raise
6801             raise exception.InstanceNotReady(instance_id=instance.uuid)
6802 
6803         return connect_info
6804 
6805     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6806                                    exception.InstanceNotReady,
6807                                    exception.InstanceNotFound,
6808                                    exception.ConsoleTypeUnavailable,
6809                                    NotImplementedError)
6810     @wrap_exception()
6811     @wrap_instance_fault
6812     def get_rdp_console(self, context, console_type, instance):
6813         """Return connection information for a RDP console."""
6814         context = context.elevated()
6815         LOG.debug("Getting RDP console", instance=instance)
6816 
6817         if not CONF.rdp.enabled:
6818             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6819 
6820         if console_type != 'rdp-html5':
6821             raise exception.ConsoleTypeInvalid(console_type=console_type)
6822 
6823         try:
6824             # Retrieve connect info from driver, and then decorate with our
6825             # access info token
6826             console = self.driver.get_rdp_console(context, instance)
6827             console_auth = objects.ConsoleAuthToken(
6828                 context=context,
6829                 console_type=console_type,
6830                 host=console.host,
6831                 port=console.port,
6832                 internal_access_path=console.internal_access_path,
6833                 instance_uuid=instance.uuid,
6834                 access_url_base=CONF.rdp.html5_proxy_base_url,
6835             )
6836             console_auth.authorize(CONF.consoleauth.token_ttl)
6837             connect_info = console.get_connection_info(
6838                 console_auth.token, console_auth.access_url)
6839 
6840         except exception.InstanceNotFound:
6841             if instance.vm_state != vm_states.BUILDING:
6842                 raise
6843             raise exception.InstanceNotReady(instance_id=instance.uuid)
6844 
6845         return connect_info
6846 
6847     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6848                                    exception.InstanceNotReady,
6849                                    exception.InstanceNotFound,
6850                                    exception.ConsoleTypeUnavailable,
6851                                    NotImplementedError)
6852     @wrap_exception()
6853     @wrap_instance_fault
6854     def get_mks_console(self, context, console_type, instance):
6855         """Return connection information for a MKS console."""
6856         context = context.elevated()
6857         LOG.debug("Getting MKS console", instance=instance)
6858 
6859         if not CONF.mks.enabled:
6860             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6861 
6862         if console_type != 'webmks':
6863             raise exception.ConsoleTypeInvalid(console_type=console_type)
6864 
6865         try:
6866             # Retrieve connect info from driver, and then decorate with our
6867             # access info token
6868             console = self.driver.get_mks_console(context, instance)
6869             console_auth = objects.ConsoleAuthToken(
6870                 context=context,
6871                 console_type=console_type,
6872                 host=console.host,
6873                 port=console.port,
6874                 internal_access_path=console.internal_access_path,
6875                 instance_uuid=instance.uuid,
6876                 access_url_base=CONF.mks.mksproxy_base_url,
6877             )
6878             console_auth.authorize(CONF.consoleauth.token_ttl)
6879             connect_info = console.get_connection_info(
6880                 console_auth.token, console_auth.access_url)
6881 
6882         except exception.InstanceNotFound:
6883             if instance.vm_state != vm_states.BUILDING:
6884                 raise
6885             raise exception.InstanceNotReady(instance_id=instance.uuid)
6886 
6887         return connect_info
6888 
6889     @messaging.expected_exceptions(
6890         exception.ConsoleTypeInvalid,
6891         exception.InstanceNotReady,
6892         exception.InstanceNotFound,
6893         exception.ConsoleTypeUnavailable,
6894         exception.SocketPortRangeExhaustedException,
6895         exception.ImageSerialPortNumberInvalid,
6896         exception.ImageSerialPortNumberExceedFlavorValue,
6897         NotImplementedError)
6898     @wrap_exception()
6899     @wrap_instance_fault
6900     def get_serial_console(self, context, console_type, instance):
6901         """Returns connection information for a serial console."""
6902 
6903         LOG.debug("Getting serial console", instance=instance)
6904 
6905         if not CONF.serial_console.enabled:
6906             raise exception.ConsoleTypeUnavailable(console_type=console_type)
6907 
6908         context = context.elevated()
6909 
6910         try:
6911             # Retrieve connect info from driver, and then decorate with our
6912             # access info token
6913             console = self.driver.get_serial_console(context, instance)
6914             console_auth = objects.ConsoleAuthToken(
6915                 context=context,
6916                 console_type=console_type,
6917                 host=console.host,
6918                 port=console.port,
6919                 internal_access_path=console.internal_access_path,
6920                 instance_uuid=instance.uuid,
6921                 access_url_base=CONF.serial_console.base_url,
6922             )
6923             console_auth.authorize(CONF.consoleauth.token_ttl)
6924             connect_info = console.get_connection_info(
6925                 console_auth.token, console_auth.access_url)
6926 
6927         except exception.InstanceNotFound:
6928             if instance.vm_state != vm_states.BUILDING:
6929                 raise
6930             raise exception.InstanceNotReady(instance_id=instance.uuid)
6931 
6932         return connect_info
6933 
6934     @messaging.expected_exceptions(exception.ConsoleTypeInvalid,
6935                                    exception.InstanceNotReady,
6936                                    exception.InstanceNotFound)
6937     @wrap_exception()
6938     @wrap_instance_fault
6939     def validate_console_port(self, ctxt, instance, port, console_type):
6940         if console_type == "spice-html5":
6941             console_info = self.driver.get_spice_console(ctxt, instance)
6942         elif console_type == "rdp-html5":
6943             console_info = self.driver.get_rdp_console(ctxt, instance)
6944         elif console_type == "serial":
6945             console_info = self.driver.get_serial_console(ctxt, instance)
6946         elif console_type == "webmks":
6947             console_info = self.driver.get_mks_console(ctxt, instance)
6948         else:
6949             console_info = self.driver.get_vnc_console(ctxt, instance)
6950 
6951         # Some drivers may return an int on console_info.port but the port
6952         # variable in this method is a string, so cast to be sure we are
6953         # comparing the correct types.
6954         return str(console_info.port) == port
6955 
6956     @wrap_exception()
6957     @reverts_task_state
6958     @wrap_instance_fault
6959     def reserve_block_device_name(self, context, instance, device,
6960                                   volume_id, disk_bus, device_type, tag,
6961                                   multiattach):
6962         if (tag and not
6963                 self.driver.capabilities.get('supports_tagged_attach_volume',
6964                                              False)):
6965             raise exception.VolumeTaggedAttachNotSupported()
6966 
6967         if (multiattach and not
6968                 self.driver.capabilities.get('supports_multiattach', False)):
6969             raise exception.MultiattachNotSupportedByVirtDriver(
6970                 volume_id=volume_id)
6971 
6972         @utils.synchronized(instance.uuid)
6973         def do_reserve():
6974             bdms = (
6975                 objects.BlockDeviceMappingList.get_by_instance_uuid(
6976                     context, instance.uuid))
6977 
6978             # NOTE(ndipanov): We need to explicitly set all the fields on the
6979             #                 object so that obj_load_attr does not fail
6980             new_bdm = objects.BlockDeviceMapping(
6981                     context=context,
6982                     source_type='volume', destination_type='volume',
6983                     instance_uuid=instance.uuid, boot_index=None,
6984                     volume_id=volume_id,
6985                     device_name=device, guest_format=None,
6986                     disk_bus=disk_bus, device_type=device_type, tag=tag)
6987 
6988             new_bdm.device_name = self._get_device_name_for_instance(
6989                     instance, bdms, new_bdm)
6990 
6991             # NOTE(vish): create bdm here to avoid race condition
6992             new_bdm.create()
6993             return new_bdm
6994 
6995         return do_reserve()
6996 
6997     @wrap_exception()
6998     @wrap_instance_event(prefix='compute')
6999     @wrap_instance_fault
7000     def attach_volume(self, context, instance, bdm):
7001         """Attach a volume to an instance."""
7002         driver_bdm = driver_block_device.convert_volume(bdm)
7003 
7004         @utils.synchronized(instance.uuid)
7005         def do_attach_volume(context, instance, driver_bdm):
7006             try:
7007                 return self._attach_volume(context, instance, driver_bdm)
7008             except Exception:
7009                 with excutils.save_and_reraise_exception():
7010                     bdm.destroy()
7011 
7012         do_attach_volume(context, instance, driver_bdm)
7013 
7014     def _attach_volume(self, context, instance, bdm):
7015         context = context.elevated()
7016         LOG.info('Attaching volume %(volume_id)s to %(mountpoint)s',
7017                  {'volume_id': bdm.volume_id,
7018                   'mountpoint': bdm['mount_device']},
7019                  instance=instance)
7020         compute_utils.notify_about_volume_attach_detach(
7021             context, instance, self.host,
7022             action=fields.NotificationAction.VOLUME_ATTACH,
7023             phase=fields.NotificationPhase.START,
7024             volume_id=bdm.volume_id)
7025         try:
7026             bdm.attach(context, instance, self.volume_api, self.driver,
7027                        do_driver_attach=True)
7028         except Exception as e:
7029             with excutils.save_and_reraise_exception():
7030                 LOG.exception("Failed to attach %(volume_id)s "
7031                               "at %(mountpoint)s",
7032                               {'volume_id': bdm.volume_id,
7033                                'mountpoint': bdm['mount_device']},
7034                               instance=instance)
7035                 if bdm['attachment_id']:
7036                     # Try to delete the attachment to make the volume
7037                     # available again. Note that DriverVolumeBlockDevice
7038                     # may have already deleted the attachment so ignore
7039                     # VolumeAttachmentNotFound.
7040                     try:
7041                         self.volume_api.attachment_delete(
7042                             context, bdm['attachment_id'])
7043                     except exception.VolumeAttachmentNotFound as exc:
7044                         LOG.debug('Ignoring VolumeAttachmentNotFound: %s',
7045                                   exc, instance=instance)
7046                 else:
7047                     self.volume_api.unreserve_volume(context, bdm.volume_id)
7048                 compute_utils.notify_about_volume_attach_detach(
7049                     context, instance, self.host,
7050                     action=fields.NotificationAction.VOLUME_ATTACH,
7051                     phase=fields.NotificationPhase.ERROR,
7052                     exception=e,
7053                     volume_id=bdm.volume_id)
7054 
7055         info = {'volume_id': bdm.volume_id}
7056         self._notify_about_instance_usage(
7057             context, instance, "volume.attach", extra_usage_info=info)
7058         compute_utils.notify_about_volume_attach_detach(
7059             context, instance, self.host,
7060             action=fields.NotificationAction.VOLUME_ATTACH,
7061             phase=fields.NotificationPhase.END,
7062             volume_id=bdm.volume_id)
7063 
7064     def _notify_volume_usage_detach(self, context, instance, bdm):
7065         if CONF.volume_usage_poll_interval <= 0:
7066             return
7067 
7068         mp = bdm.device_name
7069         # Handle bootable volumes which will not contain /dev/
7070         if '/dev/' in mp:
7071             mp = mp[5:]
7072         try:
7073             vol_stats = self.driver.block_stats(instance, mp)
7074             if vol_stats is None:
7075                 return
7076         except NotImplementedError:
7077             return
7078 
7079         LOG.debug("Updating volume usage cache with totals", instance=instance)
7080         rd_req, rd_bytes, wr_req, wr_bytes, flush_ops = vol_stats
7081         vol_usage = objects.VolumeUsage(context)
7082         vol_usage.volume_id = bdm.volume_id
7083         vol_usage.instance_uuid = instance.uuid
7084         vol_usage.project_id = instance.project_id
7085         vol_usage.user_id = instance.user_id
7086         vol_usage.availability_zone = instance.availability_zone
7087         vol_usage.curr_reads = rd_req
7088         vol_usage.curr_read_bytes = rd_bytes
7089         vol_usage.curr_writes = wr_req
7090         vol_usage.curr_write_bytes = wr_bytes
7091         vol_usage.save(update_totals=True)
7092         self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
7093         compute_utils.notify_about_volume_usage(context, vol_usage, self.host)
7094 
7095     def _detach_volume(self, context, bdm, instance, destroy_bdm=True,
7096                        attachment_id=None):
7097         """Detach a volume from an instance.
7098 
7099         :param context: security context
7100         :param bdm: nova.objects.BlockDeviceMapping volume bdm to detach
7101         :param instance: the Instance object to detach the volume from
7102         :param destroy_bdm: if True, the corresponding BDM entry will be marked
7103                             as deleted. Disabling this is useful for operations
7104                             like rebuild, when we don't want to destroy BDM
7105         :param attachment_id: The volume attachment_id for the given instance
7106                               and volume.
7107         """
7108         volume_id = bdm.volume_id
7109         compute_utils.notify_about_volume_attach_detach(
7110             context, instance, self.host,
7111             action=fields.NotificationAction.VOLUME_DETACH,
7112             phase=fields.NotificationPhase.START,
7113             volume_id=volume_id)
7114 
7115         self._notify_volume_usage_detach(context, instance, bdm)
7116 
7117         LOG.info('Detaching volume %(volume_id)s',
7118                  {'volume_id': volume_id}, instance=instance)
7119 
7120         driver_bdm = driver_block_device.convert_volume(bdm)
7121         driver_bdm.detach(context, instance, self.volume_api, self.driver,
7122                           attachment_id=attachment_id, destroy_bdm=destroy_bdm)
7123 
7124         info = dict(volume_id=volume_id)
7125         self._notify_about_instance_usage(
7126             context, instance, "volume.detach", extra_usage_info=info)
7127         compute_utils.notify_about_volume_attach_detach(
7128             context, instance, self.host,
7129             action=fields.NotificationAction.VOLUME_DETACH,
7130             phase=fields.NotificationPhase.END,
7131             volume_id=volume_id)
7132 
7133         if 'tag' in bdm and bdm.tag:
7134             self._delete_disk_metadata(instance, bdm)
7135         if destroy_bdm:
7136             bdm.destroy()
7137 
7138     def _delete_disk_metadata(self, instance, bdm):
7139         for device in instance.device_metadata.devices:
7140             if isinstance(device, objects.DiskMetadata):
7141                 if 'serial' in device:
7142                     if device.serial == bdm.volume_id:
7143                         instance.device_metadata.devices.remove(device)
7144                         instance.save()
7145                         break
7146                 else:
7147                     # NOTE(artom) We log the entire device object because all
7148                     # fields are nullable and may not be set
7149                     LOG.warning('Unable to determine whether to clean up '
7150                                 'device metadata for disk %s', device,
7151                                 instance=instance)
7152 
7153     @wrap_exception()
7154     @wrap_instance_event(prefix='compute')
7155     @wrap_instance_fault
7156     def detach_volume(self, context, volume_id, instance, attachment_id):
7157         """Detach a volume from an instance.
7158 
7159         :param context: security context
7160         :param volume_id: the volume id
7161         :param instance: the Instance object to detach the volume from
7162         :param attachment_id: The volume attachment_id for the given instance
7163                               and volume.
7164 
7165         """
7166         @utils.synchronized(instance.uuid)
7167         def do_detach_volume(context, volume_id, instance, attachment_id):
7168             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7169                     context, volume_id, instance.uuid)
7170             self._detach_volume(context, bdm, instance,
7171                                 attachment_id=attachment_id)
7172 
7173         do_detach_volume(context, volume_id, instance, attachment_id)
7174 
7175     def _init_volume_connection(self, context, new_volume,
7176                                 old_volume_id, connector, bdm,
7177                                 new_attachment_id, mountpoint):
7178         new_volume_id = new_volume['id']
7179         if new_attachment_id is None:
7180             # We're dealing with an old-style attachment so initialize the
7181             # connection so we can get the connection_info.
7182             new_cinfo = self.volume_api.initialize_connection(context,
7183                                                               new_volume_id,
7184                                                               connector)
7185         else:
7186             # Check for multiattach on the new volume and if True, check to
7187             # see if the virt driver supports multiattach.
7188             # TODO(mriedem): This is copied from DriverVolumeBlockDevice
7189             # and should be consolidated into some common code at some point.
7190             vol_multiattach = new_volume.get('multiattach', False)
7191             virt_multiattach = self.driver.capabilities.get(
7192                 'supports_multiattach', False)
7193             if vol_multiattach and not virt_multiattach:
7194                 raise exception.MultiattachNotSupportedByVirtDriver(
7195                     volume_id=new_volume_id)
7196 
7197             # This is a new style attachment and the API created the new
7198             # volume attachment and passed the id to the compute over RPC.
7199             # At this point we need to update the new volume attachment with
7200             # the host connector, which will give us back the new attachment
7201             # connection_info.
7202             new_cinfo = self.volume_api.attachment_update(
7203                 context, new_attachment_id, connector,
7204                 mountpoint)['connection_info']
7205 
7206             if vol_multiattach:
7207                 # This will be used by the volume driver to determine the
7208                 # proper disk configuration.
7209                 new_cinfo['multiattach'] = True
7210 
7211         old_cinfo = jsonutils.loads(bdm['connection_info'])
7212         if old_cinfo and 'serial' not in old_cinfo:
7213             old_cinfo['serial'] = old_volume_id
7214         # NOTE(lyarwood): serial is not always present in the returned
7215         # connection_info so set it if it is missing as we do in
7216         # DriverVolumeBlockDevice.attach().
7217         if 'serial' not in new_cinfo:
7218             new_cinfo['serial'] = new_volume_id
7219         return (old_cinfo, new_cinfo)
7220 
7221     def _swap_volume(self, context, instance, bdm, connector,
7222                      old_volume_id, new_volume, resize_to,
7223                      new_attachment_id, is_cinder_migration):
7224         new_volume_id = new_volume['id']
7225         mountpoint = bdm['device_name']
7226         failed = False
7227         new_cinfo = None
7228         try:
7229             old_cinfo, new_cinfo = self._init_volume_connection(
7230                 context, new_volume, old_volume_id, connector,
7231                 bdm, new_attachment_id, mountpoint)
7232             # NOTE(lyarwood): The Libvirt driver, the only virt driver
7233             # currently implementing swap_volume, will modify the contents of
7234             # new_cinfo when connect_volume is called. This is then saved to
7235             # the BDM in swap_volume for future use outside of this flow.
7236             msg = ("swap_volume: Calling driver volume swap with "
7237                    "connection infos: new: %(new_cinfo)s; "
7238                    "old: %(old_cinfo)s" %
7239                    {'new_cinfo': new_cinfo, 'old_cinfo': old_cinfo})
7240             # Both new and old info might contain password
7241             LOG.debug(strutils.mask_password(msg), instance=instance)
7242 
7243             self.driver.swap_volume(context, old_cinfo, new_cinfo, instance,
7244                                     mountpoint, resize_to)
7245             if new_attachment_id:
7246                 self.volume_api.attachment_complete(context, new_attachment_id)
7247             msg = ("swap_volume: Driver volume swap returned, new "
7248                    "connection_info is now : %(new_cinfo)s" %
7249                    {'new_cinfo': new_cinfo})
7250             LOG.debug(strutils.mask_password(msg))
7251         except Exception as ex:
7252             failed = True
7253             with excutils.save_and_reraise_exception():
7254                 compute_utils.notify_about_volume_swap(
7255                     context, instance, self.host,
7256                     fields.NotificationPhase.ERROR,
7257                     old_volume_id, new_volume_id, ex)
7258                 if new_cinfo:
7259                     msg = ("Failed to swap volume %(old_volume_id)s "
7260                            "for %(new_volume_id)s")
7261                     LOG.exception(msg, {'old_volume_id': old_volume_id,
7262                                         'new_volume_id': new_volume_id},
7263                                   instance=instance)
7264                 else:
7265                     msg = ("Failed to connect to volume %(volume_id)s "
7266                            "with volume at %(mountpoint)s")
7267                     LOG.exception(msg, {'volume_id': new_volume_id,
7268                                         'mountpoint': bdm['device_name']},
7269                                   instance=instance)
7270 
7271                 # The API marked the volume as 'detaching' for the old volume
7272                 # so we need to roll that back so the volume goes back to
7273                 # 'in-use' state.
7274                 self.volume_api.roll_detaching(context, old_volume_id)
7275 
7276                 if new_attachment_id is None:
7277                     # The API reserved the new volume so it would be in
7278                     # 'attaching' status, so we need to unreserve it so it
7279                     # goes back to 'available' status.
7280                     self.volume_api.unreserve_volume(context, new_volume_id)
7281                 else:
7282                     # This is a new style attachment for the new volume, which
7283                     # was created in the API. We just need to delete it here
7284                     # to put the new volume back into 'available' status.
7285                     self.volume_api.attachment_delete(
7286                         context, new_attachment_id)
7287         finally:
7288             # TODO(mriedem): This finally block is terribly confusing and is
7289             # trying to do too much. We should consider removing the finally
7290             # block and move whatever needs to happen on success and failure
7291             # into the blocks above for clarity, even if it means a bit of
7292             # redundant code.
7293             conn_volume = new_volume_id if failed else old_volume_id
7294             if new_cinfo:
7295                 LOG.debug("swap_volume: removing Cinder connection "
7296                           "for volume %(volume)s", {'volume': conn_volume},
7297                           instance=instance)
7298                 if bdm.attachment_id is None:
7299                     # This is the pre-3.44 flow for new-style volume
7300                     # attachments so just terminate the connection.
7301                     self.volume_api.terminate_connection(context,
7302                                                          conn_volume,
7303                                                          connector)
7304                 else:
7305                     # This is a new style volume attachment. If we failed, then
7306                     # the new attachment was already deleted above in the
7307                     # exception block and we have nothing more to do here. If
7308                     # swap_volume was successful in the driver, then we need to
7309                     # "detach" the original attachment by deleting it.
7310                     if not failed:
7311                         self.volume_api.attachment_delete(
7312                             context, bdm.attachment_id)
7313 
7314             # Need to make some decisions based on whether this was
7315             # a Cinder initiated migration or not. The callback to
7316             # migration completion isn't needed in the case of a
7317             # nova initiated simple swap of two volume
7318             # "volume-update" call so skip that. The new attachment
7319             # scenarios will give us a new attachment record and
7320             # that's what we want.
7321             if bdm.attachment_id and not is_cinder_migration:
7322                 # we don't callback to cinder
7323                 comp_ret = {'save_volume_id': new_volume_id}
7324             else:
7325                 # NOTE(lyarwood): The following call to
7326                 # os-migrate-volume-completion returns a dict containing
7327                 # save_volume_id, this volume id has two possible values :
7328                 # 1. old_volume_id if we are migrating (retyping) volumes
7329                 # 2. new_volume_id if we are swapping between two existing
7330                 #    volumes
7331                 # This volume id is later used to update the volume_id and
7332                 # connection_info['serial'] of the BDM.
7333                 comp_ret = self.volume_api.migrate_volume_completion(
7334                                                           context,
7335                                                           old_volume_id,
7336                                                           new_volume_id,
7337                                                           error=failed)
7338                 LOG.debug("swap_volume: Cinder migrate_volume_completion "
7339                           "returned: %(comp_ret)s", {'comp_ret': comp_ret},
7340                           instance=instance)
7341 
7342         return (comp_ret, new_cinfo)
7343 
7344     @wrap_exception()
7345     @wrap_instance_event(prefix='compute')
7346     @wrap_instance_fault
7347     def swap_volume(self, context, old_volume_id, new_volume_id, instance,
7348                     new_attachment_id):
7349         """Replace the old volume with the new volume within the active server
7350 
7351         :param context: User request context
7352         :param old_volume_id: Original volume id
7353         :param new_volume_id: New volume id being swapped to
7354         :param instance: Instance with original_volume_id attached
7355         :param new_attachment_id: ID of the new attachment for new_volume_id
7356         """
7357         @utils.synchronized(instance.uuid)
7358         def _do_locked_swap_volume(context, old_volume_id, new_volume_id,
7359                                    instance, new_attachment_id):
7360             self._do_swap_volume(context, old_volume_id, new_volume_id,
7361                                  instance, new_attachment_id)
7362         _do_locked_swap_volume(context, old_volume_id, new_volume_id, instance,
7363                                new_attachment_id)
7364 
7365     def _do_swap_volume(self, context, old_volume_id, new_volume_id,
7366                         instance, new_attachment_id):
7367         """Replace the old volume with the new volume within the active server
7368 
7369         :param context: User request context
7370         :param old_volume_id: Original volume id
7371         :param new_volume_id: New volume id being swapped to
7372         :param instance: Instance with original_volume_id attached
7373         :param new_attachment_id: ID of the new attachment for new_volume_id
7374         """
7375         context = context.elevated()
7376         compute_utils.notify_about_volume_swap(
7377             context, instance, self.host,
7378             fields.NotificationPhase.START,
7379             old_volume_id, new_volume_id)
7380 
7381         bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7382                 context, old_volume_id, instance.uuid)
7383         connector = self.driver.get_volume_connector(instance)
7384 
7385         resize_to = 0
7386         old_volume = self.volume_api.get(context, old_volume_id)
7387         # Yes this is a tightly-coupled state check of what's going on inside
7388         # cinder, but we need this while we still support old (v1/v2) and
7389         # new style attachments (v3.44). Once we drop support for old style
7390         # attachments we could think about cleaning up the cinder-initiated
7391         # swap volume API flows.
7392         is_cinder_migration = False
7393         if 'migration_status' in old_volume:
7394             is_cinder_migration = old_volume['migration_status'] == 'migrating'
7395         old_vol_size = old_volume['size']
7396         new_volume = self.volume_api.get(context, new_volume_id)
7397         new_vol_size = new_volume['size']
7398         if new_vol_size > old_vol_size:
7399             resize_to = new_vol_size
7400 
7401         LOG.info('Swapping volume %(old_volume)s for %(new_volume)s',
7402                  {'old_volume': old_volume_id, 'new_volume': new_volume_id},
7403                  instance=instance)
7404         comp_ret, new_cinfo = self._swap_volume(context,
7405                                                 instance,
7406                                                 bdm,
7407                                                 connector,
7408                                                 old_volume_id,
7409                                                 new_volume,
7410                                                 resize_to,
7411                                                 new_attachment_id,
7412                                                 is_cinder_migration)
7413 
7414         # NOTE(lyarwood): Update the BDM with the modified new_cinfo and
7415         # correct volume_id returned by Cinder.
7416         save_volume_id = comp_ret['save_volume_id']
7417         new_cinfo['serial'] = save_volume_id
7418         values = {
7419             'connection_info': jsonutils.dumps(new_cinfo),
7420             'source_type': 'volume',
7421             'destination_type': 'volume',
7422             'snapshot_id': None,
7423             'volume_id': save_volume_id,
7424             'no_device': None}
7425 
7426         if resize_to:
7427             values['volume_size'] = resize_to
7428 
7429         if new_attachment_id is not None:
7430             # This was a volume swap for a new-style attachment so we
7431             # need to update the BDM attachment_id for the new attachment.
7432             values['attachment_id'] = new_attachment_id
7433 
7434         LOG.debug("swap_volume: Updating volume %(volume_id)s BDM record with "
7435                   "%(updates)s", {'volume_id': bdm.volume_id,
7436                                   'updates': values},
7437                   instance=instance)
7438         bdm.update(values)
7439         bdm.save()
7440 
7441         compute_utils.notify_about_volume_swap(
7442             context, instance, self.host,
7443             fields.NotificationPhase.END,
7444             old_volume_id, new_volume_id)
7445 
7446     @wrap_exception()
7447     def remove_volume_connection(self, context, volume_id, instance):
7448         """Remove the volume connection on this host
7449 
7450         Detach the volume from this instance on this host, and if this is
7451         the cinder v2 flow, call cinder to terminate the connection.
7452         """
7453         try:
7454             # NOTE(mriedem): If the BDM was just passed directly we would not
7455             # need to do this DB query, but this is an RPC interface so
7456             # changing that requires some care.
7457             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
7458                     context, volume_id, instance.uuid)
7459             # NOTE(mriedem): Normally we would pass delete_attachment=True to
7460             # _remove_volume_connection to delete a v3 style volume attachment,
7461             # but this method is RPC called from _rollback_live_migration which
7462             # already deletes the attachment, so because of that tight coupling
7463             # we cannot simply delete a v3 style attachment here without
7464             # needing to do some behavior modification of that
7465             # _rollback_live_migration flow which gets messy.
7466             self._remove_volume_connection(context, bdm, instance)
7467         except exception.NotFound:
7468             pass
7469 
7470     def _remove_volume_connection(self, context, bdm, instance,
7471                                   delete_attachment=False):
7472         """Remove the volume connection on this host
7473 
7474         Detach the volume from this instance on this host.
7475 
7476         :param context: nova auth request context
7477         :param bdm: BlockDeviceMapping object for a volume attached to the
7478             instance
7479         :param instance: Instance object with a volume attached represented
7480             by ``bdm``
7481         :param delete_attachment: If ``bdm.attachment_id`` is not None the
7482             attachment was made as a cinder v3 style attachment and if True,
7483             then deletes the volume attachment, otherwise just terminates
7484             the connection for a cinder legacy style connection.
7485         """
7486         driver_bdm = driver_block_device.convert_volume(bdm)
7487         driver_bdm.driver_detach(context, instance,
7488                                  self.volume_api, self.driver)
7489         if bdm.attachment_id is None:
7490             # cinder v2 api flow
7491             connector = self.driver.get_volume_connector(instance)
7492             self.volume_api.terminate_connection(context, bdm.volume_id,
7493                                                  connector)
7494         elif delete_attachment:
7495             # cinder v3 api flow
7496             self.volume_api.attachment_delete(context, bdm.attachment_id)
7497 
7498     def _deallocate_port_resource_for_instance(
7499         self,
7500         context: nova.context.RequestContext,
7501         instance: 'objects.Instance',
7502         port_id: str,
7503         port_allocation: ty.Dict[str, ty.Dict[str, ty.Dict[str, int]]],
7504     ) -> None:
7505 
7506         if not port_allocation:
7507             return
7508 
7509         try:
7510             client = self.reportclient
7511             client.remove_resources_from_instance_allocation(
7512                 context, instance.uuid, port_allocation)
7513         except Exception as ex:
7514             # We always raise here as it is not a race condition where
7515             # somebody has already deleted the port we want to cleanup.
7516             # Here we see that the port exists, the allocation exists,
7517             # but we cannot clean it up so we will actually leak
7518             # allocations.
7519             with excutils.save_and_reraise_exception():
7520                 LOG.warning(
7521                     'Failed to remove resource allocation of port %(port_id)s '
7522                     'for instance. Error: %(error)s',
7523                     {'port_id': port_id, 'error': ex},
7524                     instance=instance)
7525 
7526     def _deallocate_port_for_instance(
7527             self, context, instance, port_id, raise_on_failure=False,
7528             pci_device=None):
7529         try:
7530             result = self.network_api.deallocate_port_for_instance(
7531                 context, instance, port_id)
7532             __, port_allocation = result
7533         except Exception as ex:
7534             with excutils.save_and_reraise_exception(
7535                     reraise=raise_on_failure):
7536                 LOG.warning('Failed to deallocate port %(port_id)s '
7537                             'for instance. Error: %(error)s',
7538                             {'port_id': port_id, 'error': ex},
7539                             instance=instance)
7540         else:
7541             if pci_device:
7542                 self.rt.unclaim_pci_devices(context, pci_device, instance)
7543                 instance.remove_pci_device_and_request(pci_device)
7544 
7545             # Deallocate the resources in placement that were used by the
7546             # detached port.
7547             self._deallocate_port_resource_for_instance(
7548                 context, instance, port_id, port_allocation)
7549 
7550     def _claim_pci_device_for_interface_attach(
7551         self,
7552         context: nova.context.RequestContext,
7553         instance: 'objects.Instance',
7554         pci_reqs: 'objects.InstancePCIRequests',
7555     ) -> ty.Optional['objects.PciDevice']:
7556         """Claim PCI devices if there are PCI requests
7557 
7558         :param context: nova.context.RequestContext
7559         :param instance: the objects.Instance to where the interface is being
7560             attached
7561         :param pci_reqs: A InstancePCIRequests object describing the
7562             needed PCI devices
7563         :raises InterfaceAttachPciClaimFailed: if the PCI device claim fails
7564         :returns: An objects.PciDevice describing the claimed PCI device for
7565             the interface or None if no device is requested
7566         """
7567 
7568         if not pci_reqs.requests:
7569             return None
7570 
7571         devices = self.rt.claim_pci_devices(
7572             context, pci_reqs, instance.numa_topology)
7573 
7574         if not devices:
7575             LOG.info('Failed to claim PCI devices during interface attach '
7576                      'for PCI request %s', pci_reqs, instance=instance)
7577             raise exception.InterfaceAttachPciClaimFailed(
7578                 instance_uuid=instance.uuid)
7579 
7580         # NOTE(gibi): We assume that maximum one PCI devices is attached per
7581         # interface attach request.
7582         device = devices[0]
7583         instance.pci_devices.objects.append(device)
7584 
7585         return device
7586 
7587     def _allocate_port_resource_for_instance(
7588         self,
7589         context: nova.context.RequestContext,
7590         instance: 'objects.Instance',
7591         pci_reqs: 'objects.InstancePCIRequests',
7592         request_groups: ty.List['objects.RequestGroup'],
7593     ) -> ty.Tuple[ty.Optional[ty.Dict[str, ty.List[str]]],
7594                   ty.Optional[ty.Dict[str, ty.Dict[str, ty.Dict[str, int]]]]]:
7595         """Allocate resources for the request in placement
7596 
7597         :param context: nova.context.RequestContext
7598         :param instance: the objects.Instance to where the interface is being
7599             attached
7600         :param pci_reqs: A list of InstancePCIRequest objects describing the
7601             needed PCI devices
7602         :param request_groups: A list of RequestGroup objects describing the
7603             resources the port requests from placement
7604         :raises InterfaceAttachResourceAllocationFailed: if we failed to
7605             allocate resource in placement for the request
7606         :returns: A tuple of provider mappings and allocated resources or
7607             (None, None) if no resource allocation was needed for the request
7608         """
7609 
7610         if not request_groups:
7611             return None, None
7612 
7613         # NOTE(gibi): we assume a single RequestGroup here as:
7614         # 1) there can only be a single port per interface attach request
7615         # 2) a single port can only request resources in a single RequestGroup
7616         #    as per the current neutron API.
7617         # #2) might change in the future so both
7618         # nova.network.neutron.API.create_resource_requests() and this function
7619         # takes a list of groups
7620         request_group = request_groups[0]
7621 
7622         # restrict the resource request to the current compute node. The
7623         # compute node uuid is the uuid of the root provider of the node in
7624         # placement
7625         compute_node_uuid = objects.ComputeNode.get_by_nodename(
7626             context, instance.node).uuid
7627         request_group.in_tree = compute_node_uuid
7628 
7629         # NOTE(gibi): when support is added for attaching a cyborg based
7630         # smart NIC the ResourceRequest could be extended to handle multiple
7631         # request groups.
7632         rr = scheduler_utils.ResourceRequest.from_request_group(request_group)
7633         res = self.reportclient.get_allocation_candidates(context, rr)
7634         alloc_reqs, provider_sums, version = res
7635 
7636         if not alloc_reqs:
7637             # no allocation candidates available, we run out of free resources
7638             raise exception.InterfaceAttachResourceAllocationFailed(
7639                 instance_uuid=instance.uuid)
7640 
7641         # select one of the candidates and update the instance
7642         # allocation
7643         # TODO(gibi): We could loop over all possible candidates
7644         # if the first one selected here does not work due to race or due
7645         # to not having free PCI devices. However the latter is only
7646         # detected later in the interface attach code path.
7647         alloc_req = alloc_reqs[0]
7648         resources = alloc_req['allocations']
7649         provider_mappings = alloc_req['mappings']
7650         try:
7651             self.reportclient.add_resources_to_instance_allocation(
7652                 context, instance.uuid, resources)
7653         except exception.AllocationUpdateFailed as e:
7654             # We lost a race. We could retry another candidate
7655             raise exception.InterfaceAttachResourceAllocationFailed(
7656                 instance_uuid=instance.uuid) from e
7657         except (
7658             exception.ConsumerAllocationRetrievalFailed,
7659             keystone_exception.ClientException,
7660         ) as e:
7661             # These are non-recoverable errors so we should not retry
7662             raise exception.InterfaceAttachResourceAllocationFailed(
7663                 instance_uuid=instance.uuid) from e
7664 
7665         try:
7666             update = (
7667                 compute_utils.
7668                     update_pci_request_spec_with_allocated_interface_name)
7669             update(
7670                 context, self.reportclient, pci_reqs.requests,
7671                 provider_mappings)
7672         except (
7673             exception.AmbiguousResourceProviderForPCIRequest,
7674             exception.UnexpectedResourceProviderNameForPCIRequest
7675         ):
7676             # These are programing errors. So we clean up an re-raise to let
7677             # the request fail
7678             with excutils.save_and_reraise_exception():
7679                 self.reportclient.remove_resources_from_instance_allocation(
7680                     context, instance.uuid, resources)
7681 
7682         return provider_mappings, resources
7683 
7684     # TODO(mriedem): There are likely race failures which can result in
7685     # NotFound and QuotaError exceptions getting traced as well.
7686     @messaging.expected_exceptions(
7687         # Do not log a traceback for user errors. We use Invalid generically
7688         # since this method can raise lots of different exceptions:
7689         # AttachInterfaceNotSupported
7690         # NetworkInterfaceTaggedAttachNotSupported
7691         # NetworkAmbiguous
7692         # PortNotUsable
7693         # PortInUse
7694         # PortNotUsableDNS
7695         # AttachSRIOVPortNotSupported
7696         # NetworksWithQoSPolicyNotSupported
7697         # InterfaceAttachResourceAllocationFailed
7698         exception.Invalid)
7699     @wrap_exception()
7700     @wrap_instance_event(prefix='compute')
7701     @wrap_instance_fault
7702     def attach_interface(self, context, instance, network_id, port_id,
7703                          requested_ip, tag):
7704         """Use hotplug to add an network adapter to an instance."""
7705         lockname = 'interface-%s-%s' % (instance.uuid, port_id)
7706 
7707         @utils.synchronized(lockname)
7708         def do_attach_interface(context, instance, network_id, port_id,
7709                                 requested_ip, tag):
7710             return self._attach_interface(context, instance, network_id,
7711                                 port_id, requested_ip, tag)
7712 
7713         return do_attach_interface(context, instance, network_id, port_id,
7714                                    requested_ip, tag)
7715 
7716     def _attach_interface(self, context, instance, network_id, port_id,
7717                           requested_ip, tag):
7718         if not self.driver.capabilities.get('supports_attach_interface',
7719                                             False):
7720             raise exception.AttachInterfaceNotSupported(
7721                 instance_uuid=instance.uuid)
7722         if (tag and not
7723             self.driver.capabilities.get('supports_tagged_attach_interface',
7724                                          False)):
7725             raise exception.NetworkInterfaceTaggedAttachNotSupported()
7726 
7727         compute_utils.notify_about_instance_action(
7728             context, instance, self.host,
7729             action=fields.NotificationAction.INTERFACE_ATTACH,
7730             phase=fields.NotificationPhase.START)
7731 
7732         bind_host_id = self.driver.network_binding_host_id(context, instance)
7733 
7734         requested_networks = objects.NetworkRequestList(
7735             objects=[
7736                 objects.NetworkRequest(
7737                     network_id=network_id,
7738                     port_id=port_id,
7739                     address=requested_ip,
7740                     tag=tag,
7741                 )
7742             ]
7743         )
7744 
7745         if len(requested_networks) != 1:
7746             LOG.warning(
7747                 "Interface attach only supports one interface per attach "
7748                 "request", instance=instance)
7749             raise exception.InterfaceAttachFailed(instance_uuid=instance.uuid)
7750 
7751         pci_numa_affinity_policy = hardware.get_pci_numa_policy_constraint(
7752             instance.flavor, instance.image_meta)
7753         pci_reqs = objects.InstancePCIRequests(
7754             requests=[], instance_uuid=instance.uuid)
7755         _, request_groups = self.network_api.create_resource_requests(
7756             context, requested_networks, pci_reqs,
7757             affinity_policy=pci_numa_affinity_policy)
7758 
7759         # We only support one port per attach request so we at most have one
7760         # pci request
7761         if pci_reqs.requests:
7762             pci_req = pci_reqs.requests[0]
7763             requested_networks[0].pci_request_id = pci_req.request_id
7764 
7765         result = self._allocate_port_resource_for_instance(
7766             context, instance, pci_reqs, request_groups)
7767         provider_mappings, resources = result
7768 
7769         try:
7770             pci_device = self._claim_pci_device_for_interface_attach(
7771                 context, instance, pci_reqs)
7772         except exception.InterfaceAttachPciClaimFailed:
7773             with excutils.save_and_reraise_exception():
7774                 if resources:
7775                     # TODO(gibi): Instead of giving up we could try another
7776                     # allocation candidate from _allocate_resources() if any
7777                     self._deallocate_port_resource_for_instance(
7778                         context, instance, port_id, resources)
7779 
7780         instance.pci_requests.requests.extend(pci_reqs.requests)
7781 
7782         network_info = self.network_api.allocate_for_instance(
7783             context,
7784             instance,
7785             requested_networks,
7786             bind_host_id=bind_host_id,
7787             resource_provider_mapping=provider_mappings,
7788         )
7789 
7790         if len(network_info) != 1:
7791             LOG.error('allocate_for_instance returned %(ports)s '
7792                       'ports', {'ports': len(network_info)})
7793             # TODO(elod.illes): an instance.interface_attach.error notification
7794             # should be sent here
7795             raise exception.InterfaceAttachFailed(
7796                     instance_uuid=instance.uuid)
7797         image_meta = objects.ImageMeta.from_instance(instance)
7798 
7799         try:
7800             self.driver.attach_interface(context, instance, image_meta,
7801                                          network_info[0])
7802         except exception.NovaException as ex:
7803             port_id = network_info[0].get('id')
7804             LOG.warning("attach interface failed , try to deallocate "
7805                         "port %(port_id)s, reason: %(msg)s",
7806                         {'port_id': port_id, 'msg': ex},
7807                         instance=instance)
7808             self._deallocate_port_for_instance(
7809                 context, instance, port_id, pci_device=pci_device)
7810 
7811             compute_utils.notify_about_instance_action(
7812                 context, instance, self.host,
7813                 action=fields.NotificationAction.INTERFACE_ATTACH,
7814                 phase=fields.NotificationPhase.ERROR,
7815                 exception=ex)
7816 
7817             raise exception.InterfaceAttachFailed(
7818                 instance_uuid=instance.uuid)
7819 
7820         if pci_device:
7821             # NOTE(gibi): The _claim_pci_device_for_interface_attach() call
7822             # found a pci device but it only marked the device as claimed. The
7823             # periodic update_available_resource would move the device to
7824             # allocated state. But as driver.attach_interface() has been
7825             # succeeded we now know that the interface is also allocated
7826             # (used by) to the instance. So make sure the pci tracker also
7827             # tracks this device as allocated. This way we can avoid a possible
7828             # race condition when a detach arrives for a device that is only
7829             # in claimed state.
7830             self.rt.allocate_pci_devices_for_instance(context, instance)
7831 
7832         instance.save()
7833 
7834         compute_utils.notify_about_instance_action(
7835             context, instance, self.host,
7836             action=fields.NotificationAction.INTERFACE_ATTACH,
7837             phase=fields.NotificationPhase.END)
7838 
7839         return network_info[0]
7840 
7841     @wrap_exception()
7842     @wrap_instance_event(prefix='compute')
7843     @wrap_instance_fault
7844     def detach_interface(self, context, instance, port_id):
7845         """Detach a network adapter from an instance."""
7846         lockname = 'interface-%s-%s' % (instance.uuid, port_id)
7847 
7848         @utils.synchronized(lockname)
7849         def do_detach_interface(context, instance, port_id):
7850             self._detach_interface(context, instance, port_id)
7851 
7852         do_detach_interface(context, instance, port_id)
7853 
7854     def _detach_interface(self, context, instance, port_id):
7855         # NOTE(aarents): we need to refresh info cache from DB here,
7856         # as previous detach/attach lock holder just updated it.
7857         compute_utils.refresh_info_cache_for_instance(context, instance)
7858         network_info = instance.info_cache.network_info
7859         condemned = None
7860         for vif in network_info:
7861             if vif['id'] == port_id:
7862                 condemned = vif
7863                 break
7864         if condemned is None:
7865             raise exception.PortNotFound(_("Port %s is not "
7866                                            "attached") % port_id)
7867 
7868         pci_req = pci_req_module.get_instance_pci_request_from_vif(
7869             context, instance, condemned)
7870 
7871         pci_device = None
7872         if pci_req:
7873             pci_devices = [pci_device
7874                            for pci_device in instance.pci_devices.objects
7875                            if pci_device.request_id == pci_req.request_id]
7876 
7877             if not pci_devices:
7878                 LOG.warning(
7879                     "Detach interface failed, port_id=%(port_id)s, "
7880                     "reason: PCI device not found for PCI request %(pci_req)s",
7881                     {'port_id': port_id, 'pci_req': pci_req})
7882                 raise exception.InterfaceDetachFailed(
7883                     instance_uuid=instance.uuid)
7884 
7885             pci_device = pci_devices[0]
7886 
7887         compute_utils.notify_about_instance_action(
7888             context, instance, self.host,
7889             action=fields.NotificationAction.INTERFACE_DETACH,
7890             phase=fields.NotificationPhase.START)
7891 
7892         try:
7893             self.driver.detach_interface(context, instance, condemned)
7894         except exception.NovaException as ex:
7895             # If the instance was deleted before the interface was detached,
7896             # just log it at debug.
7897             log_level = (logging.DEBUG
7898                          if isinstance(ex, exception.InstanceNotFound)
7899                          else logging.WARNING)
7900             LOG.log(log_level,
7901                     "Detach interface failed, port_id=%(port_id)s, reason: "
7902                     "%(msg)s", {'port_id': port_id, 'msg': ex},
7903                     instance=instance)
7904             raise exception.InterfaceDetachFailed(instance_uuid=instance.uuid)
7905         else:
7906             self._deallocate_port_for_instance(
7907                 context, instance, port_id, raise_on_failure=True,
7908                 pci_device=pci_device)
7909 
7910         instance.save()
7911 
7912         compute_utils.notify_about_instance_action(
7913             context, instance, self.host,
7914             action=fields.NotificationAction.INTERFACE_DETACH,
7915             phase=fields.NotificationPhase.END)
7916 
7917     def _get_compute_info(self, context, host):
7918         return objects.ComputeNode.get_first_node_by_host_for_old_compat(
7919             context, host)
7920 
7921     @wrap_exception()
7922     def check_instance_shared_storage(self, ctxt, data):
7923         """Check if the instance files are shared
7924 
7925         :param ctxt: security context
7926         :param data: result of driver.check_instance_shared_storage_local
7927 
7928         Returns True if instance disks located on shared storage and
7929         False otherwise.
7930         """
7931         return self.driver.check_instance_shared_storage_remote(ctxt, data)
7932 
7933     def _dest_can_numa_live_migrate(self, dest_check_data, migration):
7934         # TODO(artom) If we have a libvirt driver we expect it to set
7935         # dst_supports_numa_live_migration, but we have to remove it if we
7936         # did not get a migration from the conductor, indicating that it
7937         # cannot send RPC 5.3. This check can be removed in RPC 6.0.
7938         if ('dst_supports_numa_live_migration' in dest_check_data and
7939                 dest_check_data.dst_supports_numa_live_migration and
7940                 not migration):
7941             delattr(dest_check_data, 'dst_supports_numa_live_migration')
7942         return dest_check_data
7943 
7944     @wrap_exception()
7945     @wrap_instance_event(prefix='compute')
7946     @wrap_instance_fault
7947     def check_can_live_migrate_destination(self, ctxt, instance,
7948                                            block_migration, disk_over_commit,
7949                                            migration, limits):
7950         """Check if it is possible to execute live migration.
7951 
7952         This runs checks on the destination host, and then calls
7953         back to the source host to check the results.
7954 
7955         :param context: security context
7956         :param instance: dict of instance data
7957         :param block_migration: if true, prepare for block migration
7958                                 if None, calculate it in driver
7959         :param disk_over_commit: if true, allow disk over commit
7960                                  if None, ignore disk usage checking
7961         :param migration: objects.Migration object for this live migration.
7962         :param limits: objects.SchedulerLimits object for this live migration.
7963         :returns: a LiveMigrateData object (hypervisor-dependent)
7964         """
7965 
7966         # Error out if this host cannot accept the new instance due
7967         # to anti-affinity. This check at this moment is not very accurate, as
7968         # multiple requests may be happening concurrently and miss the lock,
7969         # but when it works it provides a better user experience by failing
7970         # earlier. Also, it should be safe to explode here, error becomes
7971         # NoValidHost and instance status remains ACTIVE.
7972         try:
7973             self._validate_instance_group_policy(ctxt, instance)
7974         except exception.RescheduledException as e:
7975             msg = ("Failed to validate instance group policy "
7976                    "due to: {}".format(e))
7977             raise exception.MigrationPreCheckError(reason=msg)
7978 
7979         src_compute_info = obj_base.obj_to_primitive(
7980             self._get_compute_info(ctxt, instance.host))
7981         dst_compute_info = obj_base.obj_to_primitive(
7982             self._get_compute_info(ctxt, self.host))
7983         dest_check_data = self.driver.check_can_live_migrate_destination(ctxt,
7984             instance, src_compute_info, dst_compute_info,
7985             block_migration, disk_over_commit)
7986         dest_check_data = self._dest_can_numa_live_migrate(dest_check_data,
7987                                                            migration)
7988         LOG.debug('destination check data is %s', dest_check_data)
7989         try:
7990             allocs = self.reportclient.get_allocations_for_consumer(
7991                 ctxt, instance.uuid)
7992             migrate_data = self.compute_rpcapi.check_can_live_migrate_source(
7993                 ctxt, instance, dest_check_data)
7994             if ('src_supports_numa_live_migration' in migrate_data and
7995                     migrate_data.src_supports_numa_live_migration):
7996                 migrate_data = self._live_migration_claim(
7997                     ctxt, instance, migrate_data, migration, limits, allocs)
7998             elif 'dst_supports_numa_live_migration' in dest_check_data:
7999                 LOG.info('Destination was ready for NUMA live migration, '
8000                          'but source is either too old, or is set to an '
8001                          'older upgrade level.', instance=instance)
8002             if self.network_api.supports_port_binding_extension(ctxt):
8003                 # Create migrate_data vifs
8004                 migrate_data.vifs = \
8005                     migrate_data_obj.\
8006                     VIFMigrateData.create_skeleton_migrate_vifs(
8007                         instance.get_network_info())
8008                 # Claim PCI devices for VIFs on destination (if needed)
8009                 port_id_to_pci = self._claim_pci_for_instance_vifs(
8010                     ctxt, instance)
8011                 # Update migrate VIFs with the newly claimed PCI devices
8012                 self._update_migrate_vifs_profile_with_pci(
8013                     migrate_data.vifs, port_id_to_pci)
8014         finally:
8015             self.driver.cleanup_live_migration_destination_check(ctxt,
8016                     dest_check_data)
8017         return migrate_data
8018 
8019     def _live_migration_claim(self, ctxt, instance, migrate_data,
8020                               migration, limits, allocs):
8021         """Runs on the destination and does a resources claim, if necessary.
8022         Currently, only NUMA live migrations require it.
8023 
8024         :param ctxt: Request context
8025         :param instance: The Instance being live migrated
8026         :param migrate_data: The MigrateData object for this live migration
8027         :param migration: The Migration object for this live migration
8028         :param limits: The SchedulerLimits object for this live migration
8029         :returns: migrate_data with dst_numa_info set if necessary
8030         """
8031         try:
8032             # NOTE(artom) We might have gotten here from _find_destination() in
8033             # the conductor live migrate task. At that point,
8034             # migration.dest_node is not set yet (nor should it be, we're still
8035             # looking for a destination, after all). Therefore, we cannot use
8036             # migration.dest_node here and must use self._get_nodename().
8037             claim = self.rt.live_migration_claim(
8038                 ctxt, instance, self._get_nodename(instance), migration,
8039                 limits, allocs)
8040             LOG.debug('Created live migration claim.', instance=instance)
8041         except exception.ComputeResourcesUnavailable as e:
8042             raise exception.MigrationPreCheckError(
8043                 reason=e.format_message())
8044         return self.driver.post_claim_migrate_data(ctxt, instance,
8045                                                    migrate_data, claim)
8046 
8047     def _source_can_numa_live_migrate(self, ctxt, dest_check_data,
8048                                       source_check_data):
8049         # TODO(artom) Our virt driver may have told us that it supports NUMA
8050         # live migration. However, the following other conditions must be met
8051         # for a NUMA live migration to happen:
8052         # 1. We got a True dst_supports_numa_live_migration in
8053         #    dest_check_data, indicating that the dest virt driver supports
8054         #    NUMA live migration and that the conductor can send RPC 5.3 and
8055         #    that the destination compute manager can receive it.
8056         # 2. Ourselves, the source, can send RPC 5.3. There's no
8057         #    sentinel/parameter for this, so we just ask our rpcapi directly.
8058         # If any of these are not met, we need to remove the
8059         # src_supports_numa_live_migration flag from source_check_data to avoid
8060         # incorrectly initiating a NUMA live migration.
8061         # All of this can be removed in RPC 6.0/objects 2.0.
8062         can_numa_live_migrate = (
8063             'dst_supports_numa_live_migration' in dest_check_data and
8064             dest_check_data.dst_supports_numa_live_migration and
8065             self.compute_rpcapi.supports_numa_live_migration(ctxt))
8066         if ('src_supports_numa_live_migration' in source_check_data and
8067                 source_check_data.src_supports_numa_live_migration and
8068                 not can_numa_live_migrate):
8069             delattr(source_check_data, 'src_supports_numa_live_migration')
8070         return source_check_data
8071 
8072     @wrap_exception()
8073     @wrap_instance_event(prefix='compute')
8074     @wrap_instance_fault
8075     def check_can_live_migrate_source(self, ctxt, instance, dest_check_data):
8076         """Check if it is possible to execute live migration.
8077 
8078         This checks if the live migration can succeed, based on the
8079         results from check_can_live_migrate_destination.
8080 
8081         :param ctxt: security context
8082         :param instance: dict of instance data
8083         :param dest_check_data: result of check_can_live_migrate_destination
8084         :returns: a LiveMigrateData object
8085         """
8086         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8087             ctxt, instance.uuid)
8088         is_volume_backed = compute_utils.is_volume_backed_instance(
8089             ctxt, instance, bdms)
8090         dest_check_data.is_volume_backed = is_volume_backed
8091         block_device_info = self._get_instance_block_device_info(
8092                             ctxt, instance, refresh_conn_info=False, bdms=bdms)
8093         result = self.driver.check_can_live_migrate_source(ctxt, instance,
8094                                                            dest_check_data,
8095                                                            block_device_info)
8096         result = self._source_can_numa_live_migrate(ctxt, dest_check_data,
8097                                                     result)
8098         LOG.debug('source check data is %s', result)
8099         return result
8100 
8101     @wrap_exception()
8102     @wrap_instance_event(prefix='compute')
8103     @wrap_instance_fault
8104     def pre_live_migration(self, context, instance, disk, migrate_data):
8105         """Preparations for live migration at dest host.
8106 
8107         :param context: security context
8108         :param instance: dict of instance data
8109         :param disk: disk info of instance
8110         :param migrate_data: A dict or LiveMigrateData object holding data
8111                              required for live migration without shared
8112                              storage.
8113         :returns: migrate_data containing additional migration info
8114         """
8115         LOG.debug('pre_live_migration data is %s', migrate_data)
8116 
8117         # Error out if this host cannot accept the new instance due
8118         # to anti-affinity. At this point the migration is already in-progress,
8119         # so this is the definitive moment to abort due to the policy
8120         # violation. Also, it should be safe to explode here. The instance
8121         # status remains ACTIVE, migration status failed.
8122         self._validate_instance_group_policy(context, instance)
8123 
8124         migrate_data.old_vol_attachment_ids = {}
8125         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8126             context, instance.uuid)
8127         network_info = self.network_api.get_instance_nw_info(context, instance)
8128         self._notify_about_instance_usage(
8129             context, instance, "live_migration.pre.start",
8130             network_info=network_info)
8131         compute_utils.notify_about_instance_action(
8132             context, instance, self.host,
8133             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
8134             phase=fields.NotificationPhase.START, bdms=bdms)
8135 
8136         connector = self.driver.get_volume_connector(instance)
8137         try:
8138             for bdm in bdms:
8139                 if bdm.is_volume and bdm.attachment_id is not None:
8140                     # This bdm uses the new cinder v3.44 API.
8141                     # We will create a new attachment for this
8142                     # volume on this migration destination host. The old
8143                     # attachment will be deleted on the source host
8144                     # when the migration succeeds. The old attachment_id
8145                     # is stored in dict with the key being the bdm.volume_id
8146                     # so it can be restored on rollback.
8147                     #
8148                     # Also note that attachment_update is not needed as we
8149                     # are providing the connector in the create call.
8150                     attach_ref = self.volume_api.attachment_create(
8151                         context, bdm.volume_id, bdm.instance_uuid,
8152                         connector=connector, mountpoint=bdm.device_name)
8153 
8154                     # save current attachment so we can detach it on success,
8155                     # or restore it on a rollback.
8156                     # NOTE(mdbooth): This data is no longer used by the source
8157                     # host since change Ibe9215c0. We can't remove it until we
8158                     # are sure the source host has been upgraded.
8159                     migrate_data.old_vol_attachment_ids[bdm.volume_id] = \
8160                         bdm.attachment_id
8161 
8162                     # update the bdm with the new attachment_id.
8163                     bdm.attachment_id = attach_ref['id']
8164                     bdm.save()
8165 
8166             block_device_info = self._get_instance_block_device_info(
8167                                 context, instance, refresh_conn_info=True,
8168                                 bdms=bdms)
8169 
8170             # The driver pre_live_migration will plug vifs on the host
8171             migrate_data = self.driver.pre_live_migration(context,
8172                                            instance,
8173                                            block_device_info,
8174                                            network_info,
8175                                            disk,
8176                                            migrate_data)
8177             LOG.debug('driver pre_live_migration data is %s', migrate_data)
8178             # driver.pre_live_migration is what plugs vifs on the destination
8179             # host so now we can set the wait_for_vif_plugged flag in the
8180             # migrate_data object which the source compute will use to
8181             # determine if it should wait for a 'network-vif-plugged' event
8182             # from neutron before starting the actual guest transfer in the
8183             # hypervisor
8184             using_multiple_port_bindings = (
8185                 'vifs' in migrate_data and migrate_data.vifs)
8186             migrate_data.wait_for_vif_plugged = (
8187                 CONF.compute.live_migration_wait_for_vif_plug and
8188                 using_multiple_port_bindings
8189             )
8190 
8191             # NOTE(tr3buchet): setup networks on destination host
8192             self.network_api.setup_networks_on_host(context, instance,
8193                                                              self.host)
8194 
8195         except Exception:
8196             # If we raise, migrate_data with the updated attachment ids
8197             # will not be returned to the source host for rollback.
8198             # So we need to rollback new attachments here.
8199             with excutils.save_and_reraise_exception():
8200                 old_attachments = migrate_data.old_vol_attachment_ids
8201                 for bdm in bdms:
8202                     if (bdm.is_volume and bdm.attachment_id is not None and
8203                             bdm.volume_id in old_attachments):
8204                         self.volume_api.attachment_delete(context,
8205                                                           bdm.attachment_id)
8206                         bdm.attachment_id = old_attachments[bdm.volume_id]
8207                         bdm.save()
8208 
8209         # Volume connections are complete, tell cinder that all the
8210         # attachments have completed.
8211         for bdm in bdms:
8212             if bdm.is_volume and bdm.attachment_id is not None:
8213                 self.volume_api.attachment_complete(context,
8214                                                     bdm.attachment_id)
8215 
8216         self._notify_about_instance_usage(
8217                      context, instance, "live_migration.pre.end",
8218                      network_info=network_info)
8219         compute_utils.notify_about_instance_action(
8220             context, instance, self.host,
8221             action=fields.NotificationAction.LIVE_MIGRATION_PRE,
8222             phase=fields.NotificationPhase.END, bdms=bdms)
8223 
8224         LOG.debug('pre_live_migration result data is %s', migrate_data)
8225         return migrate_data
8226 
8227     @staticmethod
8228     def _neutron_failed_migration_callback(event_name, instance):
8229         msg = ('Neutron reported failure during migration '
8230                'with %(event)s for instance %(uuid)s')
8231         msg_args = {'event': event_name, 'uuid': instance.uuid}
8232         if CONF.vif_plugging_is_fatal:
8233             raise exception.VirtualInterfacePlugException(msg % msg_args)
8234         LOG.error(msg, msg_args)
8235 
8236     @staticmethod
8237     def _get_neutron_events_for_live_migration(instance):
8238         # We don't generate events if CONF.vif_plugging_timeout=0
8239         # meaning that the operator disabled using them.
8240         if CONF.vif_plugging_timeout:
8241             return (instance.get_network_info()
8242                     .get_live_migration_plug_time_events())
8243         else:
8244             return []
8245 
8246     def _cleanup_pre_live_migration(self, context, dest, instance,
8247                                     migration, migrate_data, source_bdms):
8248         """Helper method for when pre_live_migration fails
8249 
8250         Sets the migration status to "error" and rolls back the live migration
8251         setup on the destination host.
8252 
8253         :param context: The user request context.
8254         :type context: nova.context.RequestContext
8255         :param dest: The live migration destination hostname.
8256         :type dest: str
8257         :param instance: The instance being live migrated.
8258         :type instance: nova.objects.Instance
8259         :param migration: The migration record tracking this live migration.
8260         :type migration: nova.objects.Migration
8261         :param migrate_data: Data about the live migration, populated from
8262                              the destination host.
8263         :type migrate_data: Subclass of nova.objects.LiveMigrateData
8264         :param source_bdms: BDMs prior to modification by the destination
8265                             compute host. Set by _do_live_migration and not
8266                             part of the callback interface, so this is never
8267                             None
8268         """
8269         self._set_migration_status(migration, 'error')
8270         # Make sure we set this for _rollback_live_migration()
8271         # so it can find it, as expected if it was called later
8272         migrate_data.migration = migration
8273         self._rollback_live_migration(context, instance, dest,
8274                                       migrate_data=migrate_data,
8275                                       source_bdms=source_bdms)
8276 
8277     def _do_pre_live_migration_from_source(self, context, dest, instance,
8278                                            block_migration, migration,
8279                                            migrate_data, source_bdms):
8280         """Prepares for pre-live-migration on the source host and calls dest
8281 
8282         Will setup a callback networking event handler (if configured) and
8283         then call the dest host's pre_live_migration method to prepare the
8284         dest host for live migration (plugs vifs, connect volumes, etc).
8285 
8286         _rollback_live_migration (on the source) will be called if
8287         pre_live_migration (on the dest) fails.
8288 
8289         :param context: nova auth request context for this operation
8290         :param dest: name of the destination compute service host
8291         :param instance: Instance object being live migrated
8292         :param block_migration: If true, prepare for block migration.
8293         :param migration: Migration object tracking this operation
8294         :param migrate_data: MigrateData object for this operation populated
8295             by the destination host compute driver as part of the
8296             check_can_live_migrate_destination call.
8297         :param source_bdms: BlockDeviceMappingList of BDMs currently attached
8298             to the instance from the source host.
8299         :returns: MigrateData object which is a modified version of the
8300             ``migrate_data`` argument from the compute driver on the dest
8301             host during the ``pre_live_migration`` call.
8302         :raises: MigrationError if waiting for the network-vif-plugged event
8303             timed out and is fatal.
8304         """
8305         class _BreakWaitForInstanceEvent(Exception):
8306             """Used as a signal to stop waiting for the network-vif-plugged
8307             event when we discover that
8308             [compute]/live_migration_wait_for_vif_plug is not set on the
8309             destination.
8310             """
8311             pass
8312 
8313         events = self._get_neutron_events_for_live_migration(instance)
8314         try:
8315             if ('block_migration' in migrate_data and
8316                     migrate_data.block_migration):
8317                 block_device_info = self._get_instance_block_device_info(
8318                     context, instance, bdms=source_bdms)
8319                 disk = self.driver.get_instance_disk_info(
8320                     instance, block_device_info=block_device_info)
8321             else:
8322                 disk = None
8323 
8324             deadline = CONF.vif_plugging_timeout
8325             error_cb = self._neutron_failed_migration_callback
8326             # In order to avoid a race with the vif plugging that the virt
8327             # driver does on the destination host, we register our events
8328             # to wait for before calling pre_live_migration. Then if the
8329             # dest host reports back that we shouldn't wait, we can break
8330             # out of the context manager using _BreakWaitForInstanceEvent.
8331             with self.virtapi.wait_for_instance_event(
8332                     instance, events, deadline=deadline,
8333                     error_callback=error_cb):
8334                 with timeutils.StopWatch() as timer:
8335                     # TODO(mriedem): The "block_migration" parameter passed
8336                     # here is not actually used in pre_live_migration but it
8337                     # is not optional in the RPC interface either.
8338                     migrate_data = self.compute_rpcapi.pre_live_migration(
8339                         context, instance,
8340                         block_migration, disk, dest, migrate_data)
8341                 LOG.info('Took %0.2f seconds for pre_live_migration on '
8342                          'destination host %s.',
8343                          timer.elapsed(), dest, instance=instance)
8344                 wait_for_vif_plugged = (
8345                     'wait_for_vif_plugged' in migrate_data and
8346                     migrate_data.wait_for_vif_plugged)
8347                 if events and not wait_for_vif_plugged:
8348                     raise _BreakWaitForInstanceEvent
8349         except _BreakWaitForInstanceEvent:
8350             if events:
8351                 LOG.debug('Not waiting for events after pre_live_migration: '
8352                           '%s. ', events, instance=instance)
8353         except exception.VirtualInterfacePlugException:
8354             with excutils.save_and_reraise_exception():
8355                 LOG.exception('Failed waiting for network virtual interfaces '
8356                               'to be plugged on the destination host %s.',
8357                               dest, instance=instance)
8358                 self._cleanup_pre_live_migration(
8359                     context, dest, instance, migration, migrate_data,
8360                     source_bdms)
8361         except eventlet.timeout.Timeout:
8362             # We only get here if wait_for_vif_plugged is True which means
8363             # live_migration_wait_for_vif_plug=True on the destination host.
8364             msg = (
8365                 'Timed out waiting for events: %(events)s. If these timeouts '
8366                 'are a persistent issue it could mean the networking backend '
8367                 'on host %(dest)s does not support sending these events '
8368                 'unless there are port binding host changes which does not '
8369                 'happen at this point in the live migration process. You may '
8370                 'need to disable the live_migration_wait_for_vif_plug option '
8371                 'on host %(dest)s.')
8372             subs = {'events': events, 'dest': dest}
8373             LOG.warning(msg, subs, instance=instance)
8374             if CONF.vif_plugging_is_fatal:
8375                 self._cleanup_pre_live_migration(
8376                     context, dest, instance, migration, migrate_data,
8377                     source_bdms)
8378                 raise exception.MigrationError(reason=msg % subs)
8379         except Exception:
8380             with excutils.save_and_reraise_exception():
8381                 LOG.exception('Pre live migration failed at %s',
8382                               dest, instance=instance)
8383                 self._cleanup_pre_live_migration(
8384                     context, dest, instance, migration, migrate_data,
8385                     source_bdms)
8386         return migrate_data
8387 
8388     def _do_live_migration(self, context, dest, instance, block_migration,
8389                            migration, migrate_data):
8390         # NOTE(danms): We should enhance the RT to account for migrations
8391         # and use the status field to denote when the accounting has been
8392         # done on source/destination. For now, this is just here for status
8393         # reporting
8394         self._set_migration_status(migration, 'preparing')
8395         source_bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
8396                 context, instance.uuid)
8397 
8398         migrate_data = self._do_pre_live_migration_from_source(
8399             context, dest, instance, block_migration, migration, migrate_data,
8400             source_bdms)
8401 
8402         # Set migrate_data.migration because that is how _post_live_migration
8403         # and _rollback_live_migration get the migration object for cleanup.
8404         # Yes this is gross but changing the _post_live_migration and
8405         # _rollback_live_migration interfaces would also mean changing how the
8406         # virt drivers call them from the driver.live_migration method, i.e.
8407         # we would have to pass the migration object through the driver (or
8408         # consider using a partial but some do not like that pattern).
8409         migrate_data.migration = migration
8410 
8411         # NOTE(Kevin_Zheng): Pop the migration from the waiting queue
8412         # if it exist in the queue, then we are good to moving on, if
8413         # not, some other process must have aborted it, then we should
8414         # rollback.
8415         try:
8416             self._waiting_live_migrations.pop(instance.uuid)
8417         except KeyError:
8418             LOG.debug('Migration %s aborted by another process, rollback.',
8419                       migration.uuid, instance=instance)
8420             self._rollback_live_migration(context, instance, dest,
8421                                           migrate_data, 'cancelled',
8422                                           source_bdms=source_bdms)
8423             self._notify_live_migrate_abort_end(context, instance)
8424             return
8425 
8426         self._set_migration_status(migration, 'running')
8427 
8428         # NOTE(mdbooth): pre_live_migration will update connection_info and
8429         # attachment_id on all volume BDMS to reflect the new destination
8430         # host attachment. We fetch BDMs before that to retain connection_info
8431         # and attachment_id relating to the source host for post migration
8432         # cleanup.
8433         post_live_migration = functools.partial(self._post_live_migration,
8434                                                 source_bdms=source_bdms)
8435         rollback_live_migration = functools.partial(
8436             self._rollback_live_migration, source_bdms=source_bdms)
8437 
8438         LOG.debug('live_migration data is %s', migrate_data)
8439         try:
8440             self.driver.live_migration(context, instance, dest,
8441                                        post_live_migration,
8442                                        rollback_live_migration,
8443                                        block_migration, migrate_data)
8444         except Exception:
8445             LOG.exception('Live migration failed.', instance=instance)
8446             with excutils.save_and_reraise_exception():
8447                 # Put instance and migration into error state,
8448                 # as its almost certainly too late to rollback
8449                 self._set_migration_status(migration, 'error')
8450                 # first refresh instance as it may have got updated by
8451                 # post_live_migration_at_destination
8452                 instance.refresh()
8453                 self._set_instance_obj_error_state(instance,
8454                                                    clean_task_state=True)
8455 
8456     @wrap_exception()
8457     @wrap_instance_event(prefix='compute')
8458     @errors_out_migration
8459     @wrap_instance_fault
8460     def live_migration(self, context, dest, instance, block_migration,
8461                        migration, migrate_data):
8462         """Executing live migration.
8463 
8464         :param context: security context
8465         :param dest: destination host
8466         :param instance: a nova.objects.instance.Instance object
8467         :param block_migration: if true, prepare for block migration
8468         :param migration: an nova.objects.Migration object
8469         :param migrate_data: implementation specific params
8470 
8471         """
8472         self._set_migration_status(migration, 'queued')
8473         # NOTE(Kevin_Zheng): Submit the live_migration job to the pool and
8474         # put the returned Future object into dict mapped with migration.uuid
8475         # in order to be able to track and abort it in the future.
8476         self._waiting_live_migrations[instance.uuid] = (None, None)
8477         try:
8478             future = self._live_migration_executor.submit(
8479                 self._do_live_migration, context, dest, instance,
8480                 block_migration, migration, migrate_data)
8481             self._waiting_live_migrations[instance.uuid] = (migration, future)
8482         except RuntimeError:
8483             # GreenThreadPoolExecutor.submit will raise RuntimeError if the
8484             # pool is shutdown, which happens in
8485             # _cleanup_live_migrations_in_pool.
8486             LOG.info('Migration %s failed to submit as the compute service '
8487                      'is shutting down.', migration.uuid, instance=instance)
8488             raise exception.LiveMigrationNotSubmitted(
8489                 migration_uuid=migration.uuid, instance_uuid=instance.uuid)
8490 
8491     @wrap_exception()
8492     @wrap_instance_event(prefix='compute')
8493     @wrap_instance_fault
8494     def live_migration_force_complete(self, context, instance):
8495         """Force live migration to complete.
8496 
8497         :param context: Security context
8498         :param instance: The instance that is being migrated
8499         """
8500 
8501         self._notify_about_instance_usage(
8502             context, instance, 'live.migration.force.complete.start')
8503         compute_utils.notify_about_instance_action(
8504             context, instance, self.host,
8505             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8506             phase=fields.NotificationPhase.START)
8507         self.driver.live_migration_force_complete(instance)
8508         self._notify_about_instance_usage(
8509             context, instance, 'live.migration.force.complete.end')
8510         compute_utils.notify_about_instance_action(
8511             context, instance, self.host,
8512             action=fields.NotificationAction.LIVE_MIGRATION_FORCE_COMPLETE,
8513             phase=fields.NotificationPhase.END)
8514 
8515     def _notify_live_migrate_abort_end(self, context, instance):
8516         self._notify_about_instance_usage(
8517             context, instance, 'live.migration.abort.end')
8518         compute_utils.notify_about_instance_action(
8519             context, instance, self.host,
8520             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8521             phase=fields.NotificationPhase.END)
8522 
8523     @wrap_exception()
8524     @wrap_instance_event(prefix='compute')
8525     @wrap_instance_fault
8526     def live_migration_abort(self, context, instance, migration_id):
8527         """Abort an in-progress live migration.
8528 
8529         :param context: Security context
8530         :param instance: The instance that is being migrated
8531         :param migration_id: ID of in-progress live migration
8532 
8533         """
8534         self._notify_about_instance_usage(
8535             context, instance, 'live.migration.abort.start')
8536         compute_utils.notify_about_instance_action(
8537             context, instance, self.host,
8538             action=fields.NotificationAction.LIVE_MIGRATION_ABORT,
8539             phase=fields.NotificationPhase.START)
8540         # NOTE(Kevin_Zheng): Pop the migration out from the queue, this might
8541         # lead to 3 scenarios:
8542         # 1. The selected migration is still in queue, and the future.cancel()
8543         #    succeed, then the abort action is succeed, mark the migration
8544         #    status to 'cancelled'.
8545         # 2. The selected migration is still in queue, but the future.cancel()
8546         #    failed, then the _do_live_migration() has started executing, and
8547         #    the migration status is 'preparing', then we just pop it from the
8548         #    queue, and the migration process will handle it later. And the
8549         #    migration status couldn't be 'running' in this scenario because
8550         #    if _do_live_migration has started executing and we've already
8551         #    popped it from the queue and set the migration status to
8552         #    'running' at this point, popping it here will raise KeyError at
8553         #    which point we check if it's running and if so, we abort the old
8554         #    way.
8555         # 3. The selected migration is not in the queue, then the migration
8556         #    status is 'running', let the driver handle it.
8557         try:
8558             migration, future = (
8559                 self._waiting_live_migrations.pop(instance.uuid))
8560             if future and future.cancel():
8561                 # If we got here, we've successfully aborted the queued
8562                 # migration and _do_live_migration won't run so we need
8563                 # to set the migration status to cancelled and send the
8564                 # notification. If Future.cancel() fails, it means
8565                 # _do_live_migration is running and the migration status
8566                 # is preparing, and _do_live_migration() itself will attempt
8567                 # to pop the queued migration, hit a KeyError, and rollback,
8568                 # set the migration to cancelled and send the
8569                 # live.migration.abort.end notification.
8570                 self._set_migration_status(migration, 'cancelled')
8571         except KeyError:
8572             migration = objects.Migration.get_by_id(context, migration_id)
8573             if migration.status != 'running':
8574                 raise exception.InvalidMigrationState(
8575                     migration_id=migration_id, instance_uuid=instance.uuid,
8576                     state=migration.status, method='abort live migration')
8577             self.driver.live_migration_abort(instance)
8578         self._notify_live_migrate_abort_end(context, instance)
8579 
8580     def _live_migration_cleanup_flags(self, migrate_data, migr_ctxt=None):
8581         """Determine whether disks, instance path or other resources
8582         need to be cleaned up after live migration (at source on success,
8583         at destination on rollback)
8584 
8585         Block migration needs empty image at destination host before migration
8586         starts, so if any failure occurs, any empty images has to be deleted.
8587 
8588         Also Volume backed live migration w/o shared storage needs to delete
8589         newly created instance-xxx dir on the destination as a part of its
8590         rollback process
8591 
8592         There may be other resources which need cleanup; currently this is
8593         limited to vPMEM devices with the libvirt driver.
8594 
8595         :param migrate_data: implementation specific data
8596         :param migr_ctxt: specific resources stored in migration_context
8597         :returns: (bool, bool) -- do_cleanup, destroy_disks
8598         """
8599         # NOTE(pkoniszewski): block migration specific params are set inside
8600         # migrate_data objects for drivers that expose block live migration
8601         # information (i.e. Libvirt, HyperV). For other drivers cleanup is not
8602         # needed.
8603         do_cleanup = False
8604         destroy_disks = False
8605         if isinstance(migrate_data, migrate_data_obj.LibvirtLiveMigrateData):
8606             has_vpmem = False
8607             if migr_ctxt and migr_ctxt.old_resources:
8608                 for resource in migr_ctxt.old_resources:
8609                     if ('metadata' in resource and
8610                         isinstance(resource.metadata,
8611                                    objects.LibvirtVPMEMDevice)):
8612                         has_vpmem = True
8613                         break
8614             # No instance booting at source host, but instance dir
8615             # must be deleted for preparing next block migration
8616             # must be deleted for preparing next live migration w/o shared
8617             # storage
8618             # vpmem must be cleanped
8619             do_cleanup = not migrate_data.is_shared_instance_path or has_vpmem
8620             destroy_disks = not migrate_data.is_shared_block_storage
8621         elif isinstance(migrate_data, migrate_data_obj.HyperVLiveMigrateData):
8622             # NOTE(claudiub): We need to cleanup any zombie Planned VM.
8623             do_cleanup = True
8624             destroy_disks = not migrate_data.is_shared_instance_path
8625 
8626         return (do_cleanup, destroy_disks)
8627 
8628     def _post_live_migration_remove_source_vol_connections(
8629             self, context, instance, source_bdms):
8630         """Disconnect volume connections from the source host during
8631         _post_live_migration.
8632 
8633         :param context: nova auth RequestContext
8634         :param instance: Instance object being live migrated
8635         :param source_bdms: BlockDeviceMappingList representing the attached
8636             volumes with connection_info set for the source host
8637         """
8638         # Detaching volumes.
8639         connector = self.driver.get_volume_connector(instance)
8640         for bdm in source_bdms:
8641             if bdm.is_volume:
8642                 # Detaching volumes is a call to an external API that can fail.
8643                 # If it does, we need to handle it gracefully so that the call
8644                 # to post_live_migration_at_destination - where we set instance
8645                 # host and task state - still happens. We need to rethink the
8646                 # current approach of setting instance host and task state
8647                 # AFTER a whole bunch of things that could fail in unhandled
8648                 # ways, but that is left as a TODO(artom).
8649                 try:
8650                     if bdm.attachment_id is None:
8651                         # Prior to cinder v3.44:
8652                         # We don't want to actually mark the volume detached,
8653                         # or delete the bdm, just remove the connection from
8654                         # this host.
8655                         #
8656                         # remove the volume connection without detaching from
8657                         # hypervisor because the instance is not running
8658                         # anymore on the current host
8659                         self.volume_api.terminate_connection(context,
8660                                                              bdm.volume_id,
8661                                                              connector)
8662                     else:
8663                         # cinder v3.44 api flow - delete the old attachment
8664                         # for the source host
8665                         self.volume_api.attachment_delete(context,
8666                                                           bdm.attachment_id)
8667 
8668                 except Exception as e:
8669                     if bdm.attachment_id is None:
8670                         LOG.error('Connection for volume %s not terminated on '
8671                                   'source host %s during post_live_migration: '
8672                                   '%s', bdm.volume_id, self.host,
8673                                   str(e), instance=instance)
8674                     else:
8675                         LOG.error('Volume attachment %s not deleted on source '
8676                                   'host %s during post_live_migration: %s',
8677                                   bdm.attachment_id, self.host,
8678                                   str(e), instance=instance)
8679 
8680     @wrap_exception()
8681     @wrap_instance_fault
8682     def _post_live_migration(self, ctxt, instance, dest,
8683                              block_migration=False, migrate_data=None,
8684                              source_bdms=None):
8685         """Post operations for live migration.
8686 
8687         This method is called from live_migration
8688         and mainly updating database record.
8689 
8690         :param ctxt: security context
8691         :param instance: instance dict
8692         :param dest: destination host
8693         :param block_migration: if true, prepare for block migration
8694         :param migrate_data: if not None, it is a dict which has data
8695         :param source_bdms: BDMs prior to modification by the destination
8696                             compute host. Set by _do_live_migration and not
8697                             part of the callback interface, so this is never
8698                             None
8699         required for live migration without shared storage
8700 
8701         """
8702         LOG.info('_post_live_migration() is started..',
8703                  instance=instance)
8704 
8705         # Cleanup source host post live-migration
8706         block_device_info = self._get_instance_block_device_info(
8707                             ctxt, instance, bdms=source_bdms)
8708         self.driver.post_live_migration(ctxt, instance, block_device_info,
8709                                         migrate_data)
8710 
8711         # Disconnect volumes from this (the source) host.
8712         self._post_live_migration_remove_source_vol_connections(
8713             ctxt, instance, source_bdms)
8714 
8715         # NOTE(artom) At this point in time we have not bound the ports to the
8716         # destination host yet (this happens in migrate_instance_start()
8717         # below). Therefore, the "old" source network info that's still in the
8718         # instance info cache is safe to use here, since it'll be used below
8719         # during driver.post_live_migration_at_source() to unplug the VIFs on
8720         # the source.
8721         network_info = instance.get_network_info()
8722 
8723         self._notify_about_instance_usage(ctxt, instance,
8724                                           "live_migration._post.start",
8725                                           network_info=network_info)
8726         compute_utils.notify_about_instance_action(
8727             ctxt, instance, self.host,
8728             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8729             phase=fields.NotificationPhase.START)
8730 
8731         migration = {'source_compute': self.host,
8732                      'dest_compute': dest, }
8733         # For neutron, migrate_instance_start will activate the destination
8734         # host port bindings, if there are any created by conductor before live
8735         # migration started.
8736         self.network_api.migrate_instance_start(ctxt,
8737                                                 instance,
8738                                                 migration)
8739 
8740         destroy_vifs = False
8741         try:
8742             # It's possible that the vif type changed on the destination
8743             # host and is already bound and active, so we need to use the
8744             # stashed source vifs in migrate_data.vifs (if present) to unplug
8745             # on the source host.
8746             unplug_nw_info = network_info
8747             if migrate_data and 'vifs' in migrate_data:
8748                 nw_info = []
8749                 for migrate_vif in migrate_data.vifs:
8750                     nw_info.append(migrate_vif.source_vif)
8751                 unplug_nw_info = network_model.NetworkInfo.hydrate(nw_info)
8752                 LOG.debug('Calling driver.post_live_migration_at_source '
8753                           'with original source VIFs from migrate_data: %s',
8754                           unplug_nw_info, instance=instance)
8755             self.driver.post_live_migration_at_source(ctxt, instance,
8756                                                       unplug_nw_info)
8757         except NotImplementedError as ex:
8758             LOG.debug(ex, instance=instance)
8759             # For all hypervisors other than libvirt, there is a possibility
8760             # they are unplugging networks from source node in the cleanup
8761             # method
8762             destroy_vifs = True
8763 
8764         # Free instance allocations on source before claims are allocated on
8765         # destination node
8766         self.rt.free_pci_device_allocations_for_instance(ctxt, instance)
8767         # NOTE(danms): Save source node before calling post method on
8768         # destination, which will update it
8769         source_node = instance.node
8770 
8771         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
8772             migrate_data, migr_ctxt=instance.migration_context)
8773 
8774         if do_cleanup:
8775             LOG.debug('Calling driver.cleanup from _post_live_migration',
8776                       instance=instance)
8777             self.driver.cleanup(ctxt, instance, unplug_nw_info,
8778                                 destroy_disks=destroy_disks,
8779                                 migrate_data=migrate_data,
8780                                 destroy_vifs=destroy_vifs)
8781 
8782         # Define domain at destination host, without doing it,
8783         # pause/suspend/terminate do not work.
8784         post_at_dest_success = True
8785         try:
8786             self.compute_rpcapi.post_live_migration_at_destination(ctxt,
8787                     instance, block_migration, dest)
8788         except Exception as error:
8789             post_at_dest_success = False
8790             # We don't want to break _post_live_migration() if
8791             # post_live_migration_at_destination() fails as it should never
8792             # affect cleaning up source node.
8793             LOG.exception("Post live migration at destination %s failed",
8794                           dest, instance=instance, error=error)
8795 
8796         self.instance_events.clear_events_for_instance(instance)
8797 
8798         # NOTE(timello): make sure we update available resources on source
8799         # host even before next periodic task.
8800         self.update_available_resource(ctxt)
8801 
8802         self._update_scheduler_instance_info(ctxt, instance)
8803         self._notify_about_instance_usage(ctxt, instance,
8804                                           "live_migration._post.end",
8805                                           network_info=network_info)
8806         compute_utils.notify_about_instance_action(
8807             ctxt, instance, self.host,
8808             action=fields.NotificationAction.LIVE_MIGRATION_POST,
8809             phase=fields.NotificationPhase.END)
8810         if post_at_dest_success:
8811             LOG.info('Migrating instance to %s finished successfully.',
8812                      dest, instance=instance)
8813 
8814         self._clean_instance_console_tokens(ctxt, instance)
8815         if migrate_data and migrate_data.obj_attr_is_set('migration'):
8816             migrate_data.migration.status = 'completed'
8817             migrate_data.migration.save()
8818             self._delete_allocation_after_move(ctxt,
8819                                                instance,
8820                                                migrate_data.migration)
8821         else:
8822             # We didn't have data on a migration, which means we can't
8823             # look up to see if we had new-style migration-based
8824             # allocations. This should really only happen in cases of
8825             # a buggy virt driver. Log a warning so we know it happened.
8826             LOG.warning('Live migration ended with no migrate_data '
8827                         'record. Unable to clean up migration-based '
8828                         'allocations for node %s which is almost certainly '
8829                         'not an expected situation.', source_node,
8830                         instance=instance)
8831 
8832     def _consoles_enabled(self):
8833         """Returns whether a console is enable."""
8834         return (CONF.vnc.enabled or CONF.spice.enabled or
8835                 CONF.rdp.enabled or CONF.serial_console.enabled or
8836                 CONF.mks.enabled)
8837 
8838     def _clean_instance_console_tokens(self, ctxt, instance):
8839         """Clean console tokens stored for an instance."""
8840         # If the database backend isn't in use, don't bother trying to clean
8841         # tokens.
8842         if self._consoles_enabled():
8843             objects.ConsoleAuthToken.\
8844                 clean_console_auths_for_instance(ctxt, instance.uuid)
8845 
8846     @wrap_exception()
8847     @wrap_instance_event(prefix='compute')
8848     @wrap_instance_fault
8849     def post_live_migration_at_destination(self, context, instance,
8850                                            block_migration):
8851         """Post operations for live migration .
8852 
8853         :param context: security context
8854         :param instance: Instance dict
8855         :param block_migration: if true, prepare for block migration
8856 
8857         """
8858         LOG.info('Post operation of migration started',
8859                  instance=instance)
8860 
8861         # NOTE(tr3buchet): setup networks on destination host
8862         #                  this is called a second time because
8863         #                  multi_host does not create the bridge in
8864         #                  plug_vifs
8865         # NOTE(mriedem): This is a no-op for neutron.
8866         self.network_api.setup_networks_on_host(context, instance,
8867                                                          self.host)
8868         migration = objects.Migration(
8869             source_compute=instance.host,
8870             dest_compute=self.host,
8871             migration_type=fields.MigrationType.LIVE_MIGRATION)
8872         self.network_api.migrate_instance_finish(
8873             context, instance, migration, provider_mappings=None)
8874 
8875         network_info = self.network_api.get_instance_nw_info(context, instance)
8876         self._notify_about_instance_usage(
8877                      context, instance, "live_migration.post.dest.start",
8878                      network_info=network_info)
8879         compute_utils.notify_about_instance_action(context, instance,
8880                 self.host,
8881                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8882                 phase=fields.NotificationPhase.START)
8883         block_device_info = self._get_instance_block_device_info(context,
8884                                                                  instance)
8885         # Allocate the claimed PCI resources at destination.
8886         self.rt.allocate_pci_devices_for_instance(context, instance)
8887 
8888         try:
8889             self.driver.post_live_migration_at_destination(
8890                 context, instance, network_info, block_migration,
8891                 block_device_info)
8892         except Exception:
8893             with excutils.save_and_reraise_exception():
8894                 instance.vm_state = vm_states.ERROR
8895                 LOG.error('Unexpected error during post live migration at '
8896                           'destination host.', instance=instance)
8897         finally:
8898             # Restore instance state and update host
8899             current_power_state = self._get_power_state(instance)
8900             node_name = None
8901             prev_host = instance.host
8902             try:
8903                 compute_node = self._get_compute_info(context, self.host)
8904                 node_name = compute_node.hypervisor_hostname
8905             except exception.ComputeHostNotFound:
8906                 LOG.exception('Failed to get compute_info for %s', self.host)
8907             finally:
8908                 # NOTE(artom) We need to apply the migration context here
8909                 # regardless of whether the driver's
8910                 # post_live_migration_at_destination succeeded or not: the
8911                 # instance is on the destination, potentially with a new NUMA
8912                 # topology and resource usage. We need to persist that.
8913                 # NOTE(artom) Apply followed by drop looks weird, but apply
8914                 # just saves the new fields while drop actually removes the
8915                 # migration context from the instance.
8916                 instance.apply_migration_context()
8917                 instance.drop_migration_context()
8918                 instance.host = self.host
8919                 instance.power_state = current_power_state
8920                 instance.task_state = None
8921                 instance.node = node_name
8922                 instance.progress = 0
8923                 instance.save(expected_task_state=task_states.MIGRATING)
8924 
8925         # NOTE(tr3buchet): tear down networks on source host (nova-net)
8926         # NOTE(mriedem): For neutron, this will delete any inactive source
8927         # host port bindings.
8928         try:
8929             self.network_api.setup_networks_on_host(context, instance,
8930                                                     prev_host, teardown=True)
8931         except exception.PortBindingDeletionFailed as e:
8932             # Removing the inactive port bindings from the source host is not
8933             # critical so just log an error but don't fail.
8934             LOG.error('Network cleanup failed for source host %s during post '
8935                       'live migration. You may need to manually clean up '
8936                       'resources in the network service. Error: %s',
8937                       prev_host, str(e))
8938         # NOTE(vish): this is necessary to update dhcp for nova-network
8939         # NOTE(mriedem): This is a no-op for neutron.
8940         self.network_api.setup_networks_on_host(context, instance, self.host)
8941         self._notify_about_instance_usage(
8942                      context, instance, "live_migration.post.dest.end",
8943                      network_info=network_info)
8944         compute_utils.notify_about_instance_action(context, instance,
8945                 self.host,
8946                 action=fields.NotificationAction.LIVE_MIGRATION_POST_DEST,
8947                 phase=fields.NotificationPhase.END)
8948 
8949     def _remove_remote_volume_connections(self, context, dest, bdms, instance):
8950         """Rollback remote volume connections on the dest"""
8951         for bdm in bdms:
8952             try:
8953                 # remove the connection on the destination host
8954                 # NOTE(lyarwood): This actually calls the cinderv2
8955                 # os-terminate_connection API if required.
8956                 self.compute_rpcapi.remove_volume_connection(
8957                         context, instance, bdm.volume_id, dest)
8958             except Exception:
8959                 LOG.warning("Ignoring exception while attempting "
8960                             "to rollback volume connections for "
8961                             "volume %s on host %s.", bdm.volume_id,
8962                             dest, instance=instance)
8963 
8964     def _rollback_volume_bdms(self, context, bdms, original_bdms, instance):
8965         """Rollback the connection_info and attachment_id for each bdm"""
8966         original_bdms_by_volid = {bdm.volume_id: bdm for bdm in original_bdms
8967                                   if bdm.is_volume}
8968         for bdm in bdms:
8969             try:
8970                 original_bdm = original_bdms_by_volid[bdm.volume_id]
8971                 # NOTE(lyarwood): Only delete the referenced attachment if it
8972                 # is different to the original in order to avoid accidentally
8973                 # removing the source host volume attachment after it has
8974                 # already been rolled back by a failure in pre_live_migration.
8975                 if (bdm.attachment_id and original_bdm.attachment_id and
8976                     bdm.attachment_id != original_bdm.attachment_id):
8977                     # NOTE(lyarwood): 3.44 cinder api flow. Delete the
8978                     # attachment used by the bdm and reset it to that of
8979                     # the original bdm.
8980                     self.volume_api.attachment_delete(context,
8981                                                       bdm.attachment_id)
8982                     bdm.attachment_id = original_bdm.attachment_id
8983                 # NOTE(lyarwood): Reset the connection_info to the original
8984                 bdm.connection_info = original_bdm.connection_info
8985                 bdm.save()
8986             except cinder_exception.ClientException:
8987                 LOG.warning("Ignoring cinderclient exception when "
8988                             "attempting to delete attachment %s for volume "
8989                             "%s while rolling back volume bdms.",
8990                             bdm.attachment_id, bdm.volume_id,
8991                             instance=instance)
8992             except Exception:
8993                 with excutils.save_and_reraise_exception():
8994                     LOG.exception("Exception while attempting to rollback "
8995                                   "BDM for volume %s.", bdm.volume_id,
8996                                   instance=instance)
8997 
8998     @wrap_exception()
8999     @wrap_instance_fault
9000     def _rollback_live_migration(self, context, instance,
9001                                  dest, migrate_data=None,
9002                                  migration_status='failed',
9003                                  source_bdms=None):
9004         """Recovers Instance/volume state from migrating -> running.
9005 
9006         :param context: security context
9007         :param instance: nova.objects.instance.Instance object
9008         :param dest:
9009             This method is called from live migration src host.
9010             This param specifies destination host.
9011         :param migrate_data:
9012             if not none, contains implementation specific data.
9013         :param migration_status:
9014             Contains the status we want to set for the migration object
9015         :param source_bdms: BDMs prior to modification by the destination
9016                             compute host. Set by _do_live_migration and not
9017                             part of the callback interface, so this is never
9018                             None
9019 
9020         """
9021         # NOTE(gibi): We need to refresh pci_requests of the instance as it
9022         # might be changed by the conductor during scheduling based on the
9023         # selected destination host. If the instance has SRIOV ports with
9024         # resource request then the LiveMigrationTask._find_destination call
9025         # updated the instance.pci_requests.requests[].spec with the SRIOV PF
9026         # device name to be used on the destination host. As the migration is
9027         # rolling back to the source host now we don't want to persist the
9028         # destination host related changes in the DB.
9029         instance.pci_requests = \
9030             objects.InstancePCIRequests.get_by_instance_uuid(
9031                 context, instance.uuid)
9032 
9033         if (isinstance(migrate_data, migrate_data_obj.LiveMigrateData) and
9034               migrate_data.obj_attr_is_set('migration')):
9035             migration = migrate_data.migration
9036         else:
9037             migration = None
9038 
9039         if migration:
9040             # Remove allocations created in Placement for the dest node.
9041             # If migration is None, the virt driver didn't pass it which is
9042             # a bug.
9043             self._revert_allocation(context, instance, migration)
9044         else:
9045             LOG.error('Unable to revert allocations during live migration '
9046                       'rollback; compute driver did not provide migrate_data',
9047                       instance=instance)
9048 
9049         # NOTE(tr3buchet): setup networks on source host (really it's re-setup
9050         #                  for nova-network)
9051         # NOTE(mriedem): This is a no-op for neutron.
9052         self.network_api.setup_networks_on_host(context, instance, self.host)
9053         self.driver.rollback_live_migration_at_source(context, instance,
9054                                                       migrate_data)
9055 
9056         # NOTE(lyarwood): Fetch the current list of BDMs, disconnect any
9057         # connected volumes from the dest and delete any volume attachments
9058         # used by the destination host before rolling back to the original
9059         # still valid source host volume attachments.
9060         bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9061                 context, instance.uuid)
9062         # TODO(lyarwood): Turn the following into a lookup method within
9063         # BlockDeviceMappingList.
9064         vol_bdms = [bdm for bdm in bdms if bdm.is_volume]
9065         self._remove_remote_volume_connections(context, dest, vol_bdms,
9066                                                instance)
9067         self._rollback_volume_bdms(context, vol_bdms, source_bdms, instance)
9068 
9069         self._notify_about_instance_usage(context, instance,
9070                                           "live_migration._rollback.start")
9071         compute_utils.notify_about_instance_action(context, instance,
9072                 self.host,
9073                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
9074                 phase=fields.NotificationPhase.START,
9075                 bdms=bdms)
9076 
9077         do_cleanup, destroy_disks = self._live_migration_cleanup_flags(
9078                 migrate_data, migr_ctxt=instance.migration_context)
9079 
9080         if do_cleanup:
9081             self.compute_rpcapi.rollback_live_migration_at_destination(
9082                     context, instance, dest, destroy_disks=destroy_disks,
9083                     migrate_data=migrate_data)
9084         else:
9085             # The port binding profiles need to be cleaned up.
9086             with errors_out_migration_ctxt(migration):
9087                 try:
9088                     # This call will delete any inactive destination host
9089                     # port bindings.
9090                     self.network_api.setup_networks_on_host(
9091                         context, instance, host=dest, teardown=True)
9092                 except exception.PortBindingDeletionFailed as e:
9093                     # Removing the inactive port bindings from the destination
9094                     # host is not critical so just log an error but don't fail.
9095                     LOG.error(
9096                         'Network cleanup failed for destination host %s '
9097                         'during live migration rollback. You may need to '
9098                         'manually clean up resources in the network service. '
9099                         'Error: %s', dest, str(e))
9100                 except Exception:
9101                     with excutils.save_and_reraise_exception():
9102                         LOG.exception(
9103                             'An error occurred while cleaning up networking '
9104                             'during live migration rollback.',
9105                             instance=instance)
9106 
9107         # NOTE(luyao): We drop move_claim and migration_context after cleanup
9108         # is complete, to ensure the specific resources claimed on destination
9109         # are released safely.
9110         # TODO(artom) drop_move_claim_at_destination() is new in RPC 5.3, only
9111         # call it if we performed a NUMA-aware live migration (which implies us
9112         # being able to send RPC 5.3). To check this, we can use the
9113         # src_supports_numa_live_migration flag, as it will be set if and only
9114         # if:
9115         # - dst_supports_numa_live_migration made its way to the source
9116         #   (meaning both dest and source are new and conductor can speak
9117         #   RPC 5.3)
9118         # - src_supports_numa_live_migration was set by the source driver and
9119         #   passed the send-RPC-5.3 check.
9120         # This check can be removed in RPC 6.0.
9121         if ('src_supports_numa_live_migration' in migrate_data and
9122                 migrate_data.src_supports_numa_live_migration):
9123             LOG.debug('Calling destination to drop move claim.',
9124                       instance=instance)
9125             self.compute_rpcapi.drop_move_claim_at_destination(context,
9126                                                                instance, dest)
9127 
9128         # NOTE(luyao): We only update instance info after rollback operations
9129         # are complete
9130         instance.task_state = None
9131         instance.progress = 0
9132         instance.drop_migration_context()
9133         instance.save(expected_task_state=[task_states.MIGRATING])
9134 
9135         self._notify_about_instance_usage(context, instance,
9136                                           "live_migration._rollback.end")
9137         compute_utils.notify_about_instance_action(context, instance,
9138                 self.host,
9139                 action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK,
9140                 phase=fields.NotificationPhase.END,
9141                 bdms=bdms)
9142 
9143         # NOTE(luyao): we have cleanup everything and get instance
9144         # back to normal status, now set migration status to 'failed'
9145         self._set_migration_status(migration, migration_status)
9146 
9147     @wrap_exception()
9148     @wrap_instance_fault
9149     def drop_move_claim_at_destination(self, context, instance):
9150         """Called by the source of a live migration during rollback to ask the
9151         destination to drop the MoveClaim object that was created for the live
9152         migration on the destination.
9153         """
9154         nodename = self._get_nodename(instance)
9155         LOG.debug('Dropping live migration resource claim on destination '
9156                   'node %s', nodename, instance=instance)
9157         self.rt.drop_move_claim(
9158             context, instance, nodename, instance_type=instance.flavor)
9159 
9160     @wrap_exception()
9161     @wrap_instance_event(prefix='compute')
9162     @wrap_instance_fault
9163     def rollback_live_migration_at_destination(self, context, instance,
9164                                                destroy_disks,
9165                                                migrate_data):
9166         """Cleaning up image directory that is created pre_live_migration.
9167 
9168         :param context: security context
9169         :param instance: a nova.objects.instance.Instance object sent over rpc
9170         :param destroy_disks: whether to destroy volumes or not
9171         :param migrate_data: contains migration info
9172         """
9173         network_info = self.network_api.get_instance_nw_info(context, instance)
9174         self._notify_about_instance_usage(
9175                       context, instance, "live_migration.rollback.dest.start",
9176                       network_info=network_info)
9177         compute_utils.notify_about_instance_action(
9178             context, instance, self.host,
9179             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
9180             phase=fields.NotificationPhase.START)
9181         try:
9182             # NOTE(tr3buchet): tear down networks on dest host (nova-net)
9183             # NOTE(mriedem): For neutron, this call will delete any
9184             # destination host port bindings.
9185             # TODO(mriedem): We should eventually remove this call from
9186             # this method (rollback_live_migration_at_destination) since this
9187             # method is only called conditionally based on whether or not the
9188             # instance is running on shared storage. _rollback_live_migration
9189             # already calls this method for neutron if we are running on
9190             # shared storage.
9191             self.network_api.setup_networks_on_host(context, instance,
9192                                                     self.host, teardown=True)
9193         except exception.PortBindingDeletionFailed as e:
9194             # Removing the inactive port bindings from the destination
9195             # host is not critical so just log an error but don't fail.
9196             LOG.error(
9197                 'Network cleanup failed for destination host %s '
9198                 'during live migration rollback. You may need to '
9199                 'manually clean up resources in the network service. '
9200                 'Error: %s', self.host, str(e))
9201         except Exception:
9202             with excutils.save_and_reraise_exception():
9203                 # NOTE(tdurakov): even if teardown networks fails driver
9204                 # should try to rollback live migration on destination.
9205                 LOG.exception('An error occurred while deallocating network.',
9206                               instance=instance)
9207         finally:
9208             # always run this even if setup_networks_on_host fails
9209             # NOTE(vish): The mapping is passed in so the driver can disconnect
9210             #             from remote volumes if necessary
9211             block_device_info = self._get_instance_block_device_info(context,
9212                                                                      instance)
9213             # free any instance PCI claims done on destination during
9214             # check_can_live_migrate_destination()
9215             self.rt.free_pci_device_claims_for_instance(context, instance)
9216 
9217             # NOTE(luyao): Apply migration_context temporarily since it's
9218             # on destination host, we rely on instance object to cleanup
9219             # specific resources like vpmem
9220             with instance.mutated_migration_context():
9221                 self.driver.rollback_live_migration_at_destination(
9222                     context, instance, network_info, block_device_info,
9223                     destroy_disks=destroy_disks, migrate_data=migrate_data)
9224 
9225         self._notify_about_instance_usage(
9226                         context, instance, "live_migration.rollback.dest.end",
9227                         network_info=network_info)
9228         compute_utils.notify_about_instance_action(
9229             context, instance, self.host,
9230             action=fields.NotificationAction.LIVE_MIGRATION_ROLLBACK_DEST,
9231             phase=fields.NotificationPhase.END)
9232 
9233     def _require_nw_info_update(self, context, instance):
9234         """Detect whether there is a mismatch in binding:host_id, or
9235         binding_failed or unbound binding:vif_type for any of the instances
9236         ports.
9237         """
9238         # Only update port bindings if compute manager does manage port
9239         # bindings instead of the compute driver. For example IronicDriver
9240         # manages the port binding for baremetal instance ports, hence,
9241         # external intervention with the binding is not desired.
9242         if self.driver.manages_network_binding_host_id():
9243             return False
9244 
9245         search_opts = {'device_id': instance.uuid,
9246                        'fields': ['binding:host_id', 'binding:vif_type']}
9247         ports = self.network_api.list_ports(context, **search_opts)
9248         for p in ports['ports']:
9249             if p.get('binding:host_id') != self.host:
9250                 return True
9251             vif_type = p.get('binding:vif_type')
9252             if (vif_type == network_model.VIF_TYPE_UNBOUND or
9253                     vif_type == network_model.VIF_TYPE_BINDING_FAILED):
9254                 return True
9255         return False
9256 
9257     @periodic_task.periodic_task(
9258         spacing=CONF.heal_instance_info_cache_interval)
9259     def _heal_instance_info_cache(self, context):
9260         """Called periodically.  On every call, try to update the
9261         info_cache's network information for another instance by
9262         calling to the network manager.
9263 
9264         This is implemented by keeping a cache of uuids of instances
9265         that live on this host.  On each call, we pop one off of a
9266         list, pull the DB record, and try the call to the network API.
9267         If anything errors don't fail, as it's possible the instance
9268         has been deleted, etc.
9269         """
9270         heal_interval = CONF.heal_instance_info_cache_interval
9271         if not heal_interval:
9272             return
9273 
9274         instance_uuids = getattr(self, '_instance_uuids_to_heal', [])
9275         instance = None
9276 
9277         LOG.debug('Starting heal instance info cache')
9278 
9279         if not instance_uuids:
9280             # The list of instances to heal is empty so rebuild it
9281             LOG.debug('Rebuilding the list of instances to heal')
9282             db_instances = objects.InstanceList.get_by_host(
9283                 context, self.host, expected_attrs=[], use_slave=True)
9284             for inst in db_instances:
9285                 # We don't want to refresh the cache for instances
9286                 # which are building or deleting so don't put them
9287                 # in the list. If they are building they will get
9288                 # added to the list next time we build it.
9289                 if (inst.vm_state == vm_states.BUILDING):
9290                     LOG.debug('Skipping network cache update for instance '
9291                               'because it is Building.', instance=inst)
9292                     continue
9293                 if (inst.task_state == task_states.DELETING):
9294                     LOG.debug('Skipping network cache update for instance '
9295                               'because it is being deleted.', instance=inst)
9296                     continue
9297 
9298                 if not instance:
9299                     # Save the first one we find so we don't
9300                     # have to get it again
9301                     instance = inst
9302                 else:
9303                     instance_uuids.append(inst['uuid'])
9304 
9305             self._instance_uuids_to_heal = instance_uuids
9306         else:
9307             # Find the next valid instance on the list
9308             while instance_uuids:
9309                 try:
9310                     inst = objects.Instance.get_by_uuid(
9311                             context, instance_uuids.pop(0),
9312                             expected_attrs=['system_metadata', 'info_cache',
9313                                             'flavor'],
9314                             use_slave=True)
9315                 except exception.InstanceNotFound:
9316                     # Instance is gone.  Try to grab another.
9317                     continue
9318 
9319                 # Check the instance hasn't been migrated
9320                 if inst.host != self.host:
9321                     LOG.debug('Skipping network cache update for instance '
9322                               'because it has been migrated to another '
9323                               'host.', instance=inst)
9324                 # Check the instance isn't being deleting
9325                 elif inst.task_state == task_states.DELETING:
9326                     LOG.debug('Skipping network cache update for instance '
9327                               'because it is being deleted.', instance=inst)
9328                 else:
9329                     instance = inst
9330                     break
9331 
9332         if instance:
9333             # We have an instance now to refresh
9334             try:
9335                 # Fix potential mismatch in port binding if evacuation failed
9336                 # after reassigning the port binding to the dest host but
9337                 # before the instance host is changed.
9338                 # Do this only when instance has no pending task.
9339                 if instance.task_state is None and \
9340                         self._require_nw_info_update(context, instance):
9341                     LOG.info("Updating ports in neutron", instance=instance)
9342                     self.network_api.setup_instance_network_on_host(
9343                         context, instance, self.host)
9344                 # Call to network API to get instance info.. this will
9345                 # force an update to the instance's info_cache
9346                 self.network_api.get_instance_nw_info(
9347                     context, instance, force_refresh=True)
9348                 LOG.debug('Updated the network info_cache for instance',
9349                           instance=instance)
9350             except exception.InstanceNotFound:
9351                 # Instance is gone.
9352                 LOG.debug('Instance no longer exists. Unable to refresh',
9353                           instance=instance)
9354                 return
9355             except exception.InstanceInfoCacheNotFound:
9356                 # InstanceInfoCache is gone.
9357                 LOG.debug('InstanceInfoCache no longer exists. '
9358                           'Unable to refresh', instance=instance)
9359             except Exception:
9360                 LOG.error('An error occurred while refreshing the network '
9361                           'cache.', instance=instance, exc_info=True)
9362         else:
9363             LOG.debug("Didn't find any instances for network info cache "
9364                       "update.")
9365 
9366     @periodic_task.periodic_task
9367     def _poll_rebooting_instances(self, context):
9368         if CONF.reboot_timeout > 0:
9369             filters = {'task_state':
9370                        [task_states.REBOOTING,
9371                         task_states.REBOOT_STARTED,
9372                         task_states.REBOOT_PENDING],
9373                        'host': self.host}
9374             rebooting = objects.InstanceList.get_by_filters(
9375                 context, filters, expected_attrs=[], use_slave=True)
9376 
9377             to_poll = []
9378             for instance in rebooting:
9379                 if timeutils.is_older_than(instance.updated_at,
9380                                            CONF.reboot_timeout):
9381                     to_poll.append(instance)
9382 
9383             self.driver.poll_rebooting_instances(CONF.reboot_timeout, to_poll)
9384 
9385     @periodic_task.periodic_task
9386     def _poll_rescued_instances(self, context):
9387         if CONF.rescue_timeout > 0:
9388             filters = {'vm_state': vm_states.RESCUED,
9389                        'host': self.host}
9390             rescued_instances = objects.InstanceList.get_by_filters(
9391                 context, filters, expected_attrs=["system_metadata"],
9392                 use_slave=True)
9393 
9394             to_unrescue = []
9395             for instance in rescued_instances:
9396                 if timeutils.is_older_than(instance.launched_at,
9397                                            CONF.rescue_timeout):
9398                     to_unrescue.append(instance)
9399 
9400             for instance in to_unrescue:
9401                 self.compute_api.unrescue(context, instance)
9402 
9403     @periodic_task.periodic_task
9404     def _poll_unconfirmed_resizes(self, context):
9405         if CONF.resize_confirm_window == 0:
9406             return
9407 
9408         migrations = objects.MigrationList.get_unconfirmed_by_dest_compute(
9409                 context, CONF.resize_confirm_window, self.host,
9410                 use_slave=True)
9411 
9412         migrations_info = dict(migration_count=len(migrations),
9413                 confirm_window=CONF.resize_confirm_window)
9414 
9415         if migrations_info["migration_count"] > 0:
9416             LOG.info("Found %(migration_count)d unconfirmed migrations "
9417                      "older than %(confirm_window)d seconds",
9418                      migrations_info)
9419 
9420         def _set_migration_to_error(migration, reason, **kwargs):
9421             LOG.warning("Setting migration %(migration_id)s to error: "
9422                         "%(reason)s",
9423                         {'migration_id': migration['id'], 'reason': reason},
9424                         **kwargs)
9425             migration.status = 'error'
9426             migration.save()
9427 
9428         for migration in migrations:
9429             instance_uuid = migration.instance_uuid
9430             LOG.info("Automatically confirming migration "
9431                      "%(migration_id)s for instance %(instance_uuid)s",
9432                      {'migration_id': migration.id,
9433                       'instance_uuid': instance_uuid})
9434             expected_attrs = ['metadata', 'system_metadata']
9435             try:
9436                 instance = objects.Instance.get_by_uuid(context,
9437                             instance_uuid, expected_attrs=expected_attrs,
9438                             use_slave=True)
9439             except exception.InstanceNotFound:
9440                 reason = (_("Instance %s not found") %
9441                           instance_uuid)
9442                 _set_migration_to_error(migration, reason)
9443                 continue
9444             if instance.vm_state == vm_states.ERROR:
9445                 reason = _("In ERROR state")
9446                 _set_migration_to_error(migration, reason,
9447                                         instance=instance)
9448                 continue
9449             # race condition: The instance in DELETING state should not be
9450             # set the migration state to error, otherwise the instance in
9451             # to be deleted which is in RESIZED state
9452             # will not be able to confirm resize
9453             if instance.task_state in [task_states.DELETING,
9454                                        task_states.SOFT_DELETING]:
9455                 msg = ("Instance being deleted or soft deleted during resize "
9456                        "confirmation. Skipping.")
9457                 LOG.debug(msg, instance=instance)
9458                 continue
9459 
9460             # race condition: This condition is hit when this method is
9461             # called between the save of the migration record with a status of
9462             # finished and the save of the instance object with a state of
9463             # RESIZED. The migration record should not be set to error.
9464             if instance.task_state == task_states.RESIZE_FINISH:
9465                 msg = ("Instance still resizing during resize "
9466                        "confirmation. Skipping.")
9467                 LOG.debug(msg, instance=instance)
9468                 continue
9469 
9470             vm_state = instance.vm_state
9471             task_state = instance.task_state
9472             if vm_state != vm_states.RESIZED or task_state is not None:
9473                 reason = (_("In states %(vm_state)s/%(task_state)s, not "
9474                            "RESIZED/None") %
9475                           {'vm_state': vm_state,
9476                            'task_state': task_state})
9477                 _set_migration_to_error(migration, reason,
9478                                         instance=instance)
9479                 continue
9480             try:
9481                 self.compute_api.confirm_resize(context, instance,
9482                                                 migration=migration)
9483             except Exception as e:
9484                 LOG.info("Error auto-confirming resize: %s. "
9485                          "Will retry later.", e, instance=instance)
9486 
9487     @periodic_task.periodic_task(spacing=CONF.shelved_poll_interval)
9488     def _poll_shelved_instances(self, context):
9489 
9490         if CONF.shelved_offload_time <= 0:
9491             return
9492 
9493         filters = {'vm_state': vm_states.SHELVED,
9494                    'task_state': None,
9495                    'host': self.host}
9496         shelved_instances = objects.InstanceList.get_by_filters(
9497             context, filters=filters, expected_attrs=['system_metadata'],
9498             use_slave=True)
9499 
9500         to_gc = []
9501         for instance in shelved_instances:
9502             sys_meta = instance.system_metadata
9503             shelved_at = timeutils.parse_strtime(sys_meta['shelved_at'])
9504             if timeutils.is_older_than(shelved_at, CONF.shelved_offload_time):
9505                 to_gc.append(instance)
9506 
9507         cyclient = cyborg.get_client(context)
9508         for instance in to_gc:
9509             try:
9510                 instance.task_state = task_states.SHELVING_OFFLOADING
9511                 instance.save(expected_task_state=(None,))
9512                 accel_uuids = []
9513                 if instance.flavor.extra_specs.get('accel:device_profile'):
9514                     # TODO(brinzhang): After cyborg support batch query ARQs
9515                     # for more than one instances, we will improve efficiency
9516                     # with this implemention.
9517                     accel_uuids = cyclient.get_arq_uuids_for_instance(instance)
9518                 self.shelve_offload_instance(
9519                     context, instance, clean_shutdown=False,
9520                     accel_uuids=accel_uuids)
9521             except Exception:
9522                 LOG.exception('Periodic task failed to offload instance.',
9523                               instance=instance)
9524 
9525     @periodic_task.periodic_task
9526     def _instance_usage_audit(self, context):
9527         if not CONF.instance_usage_audit:
9528             return
9529 
9530         begin, end = utils.last_completed_audit_period()
9531         if objects.TaskLog.get(context, 'instance_usage_audit', begin, end,
9532                                self.host):
9533             return
9534 
9535         instances = objects.InstanceList.get_active_by_window_joined(
9536             context, begin, end, host=self.host,
9537             expected_attrs=['system_metadata', 'info_cache', 'metadata',
9538                             'flavor'],
9539             use_slave=True)
9540         num_instances = len(instances)
9541         errors = 0
9542         successes = 0
9543         LOG.info("Running instance usage audit for host %(host)s "
9544                  "from %(begin_time)s to %(end_time)s. "
9545                  "%(number_instances)s instances.",
9546                  {'host': self.host,
9547                   'begin_time': begin,
9548                   'end_time': end,
9549                   'number_instances': num_instances})
9550         start_time = time.time()
9551         task_log = objects.TaskLog(context)
9552         task_log.task_name = 'instance_usage_audit'
9553         task_log.period_beginning = begin
9554         task_log.period_ending = end
9555         task_log.host = self.host
9556         task_log.task_items = num_instances
9557         task_log.message = 'Instance usage audit started...'
9558         task_log.begin_task()
9559         for instance in instances:
9560             try:
9561                 compute_utils.notify_usage_exists(
9562                     self.notifier, context, instance, self.host,
9563                     ignore_missing_network_data=False)
9564                 successes += 1
9565             except Exception:
9566                 LOG.exception('Failed to generate usage '
9567                               'audit for instance '
9568                               'on host %s', self.host,
9569                               instance=instance)
9570                 errors += 1
9571         task_log.errors = errors
9572         task_log.message = (
9573             'Instance usage audit ran for host %s, %s instances in %s seconds.'
9574             % (self.host, num_instances, time.time() - start_time))
9575         task_log.end_task()
9576 
9577     def _get_host_volume_bdms(self, context, use_slave=False):
9578         """Return all block device mappings on a compute host."""
9579         compute_host_bdms = []
9580         instances = objects.InstanceList.get_by_host(context, self.host,
9581             use_slave=use_slave)
9582         for instance in instances:
9583             bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9584                     context, instance.uuid, use_slave=use_slave)
9585             instance_bdms = [bdm for bdm in bdms if bdm.is_volume]
9586             compute_host_bdms.append(dict(instance=instance,
9587                                           instance_bdms=instance_bdms))
9588 
9589         return compute_host_bdms
9590 
9591     def _update_volume_usage_cache(self, context, vol_usages):
9592         """Updates the volume usage cache table with a list of stats."""
9593         for usage in vol_usages:
9594             # Allow switching of greenthreads between queries.
9595             greenthread.sleep(0)
9596             vol_usage = objects.VolumeUsage(context)
9597             vol_usage.volume_id = usage['volume']
9598             vol_usage.instance_uuid = usage['instance'].uuid
9599             vol_usage.project_id = usage['instance'].project_id
9600             vol_usage.user_id = usage['instance'].user_id
9601             vol_usage.availability_zone = usage['instance'].availability_zone
9602             vol_usage.curr_reads = usage['rd_req']
9603             vol_usage.curr_read_bytes = usage['rd_bytes']
9604             vol_usage.curr_writes = usage['wr_req']
9605             vol_usage.curr_write_bytes = usage['wr_bytes']
9606             vol_usage.save()
9607             self.notifier.info(context, 'volume.usage', vol_usage.to_dict())
9608             compute_utils.notify_about_volume_usage(context, vol_usage,
9609                                                     self.host)
9610 
9611     @periodic_task.periodic_task(spacing=CONF.volume_usage_poll_interval)
9612     def _poll_volume_usage(self, context):
9613         if CONF.volume_usage_poll_interval == 0:
9614             return
9615 
9616         compute_host_bdms = self._get_host_volume_bdms(context,
9617                                                        use_slave=True)
9618         if not compute_host_bdms:
9619             return
9620 
9621         LOG.debug("Updating volume usage cache")
9622         try:
9623             vol_usages = self.driver.get_all_volume_usage(context,
9624                                                           compute_host_bdms)
9625         except NotImplementedError:
9626             return
9627 
9628         self._update_volume_usage_cache(context, vol_usages)
9629 
9630     @periodic_task.periodic_task(spacing=CONF.sync_power_state_interval,
9631                                  run_immediately=True)
9632     def _sync_power_states(self, context):
9633         """Align power states between the database and the hypervisor.
9634 
9635         To sync power state data we make a DB call to get the number of
9636         virtual machines known by the hypervisor and if the number matches the
9637         number of virtual machines known by the database, we proceed in a lazy
9638         loop, one database record at a time, checking if the hypervisor has the
9639         same power state as is in the database.
9640         """
9641         db_instances = objects.InstanceList.get_by_host(context, self.host,
9642                                                         expected_attrs=[],
9643                                                         use_slave=True)
9644 
9645         try:
9646             num_vm_instances = self.driver.get_num_instances()
9647         except exception.VirtDriverNotReady as e:
9648             # If the virt driver is not ready, like ironic-api not being up
9649             # yet in the case of ironic, just log it and exit.
9650             LOG.info('Skipping _sync_power_states periodic task due to: %s', e)
9651             return
9652 
9653         num_db_instances = len(db_instances)
9654 
9655         if num_vm_instances != num_db_instances:
9656             LOG.warning("While synchronizing instance power states, found "
9657                         "%(num_db_instances)s instances in the database "
9658                         "and %(num_vm_instances)s instances on the "
9659                         "hypervisor.",
9660                         {'num_db_instances': num_db_instances,
9661                          'num_vm_instances': num_vm_instances})
9662 
9663         def _sync(db_instance):
9664             # NOTE(melwitt): This must be synchronized as we query state from
9665             #                two separate sources, the driver and the database.
9666             #                They are set (in stop_instance) and read, in sync.
9667             @utils.synchronized(db_instance.uuid)
9668             def query_driver_power_state_and_sync():
9669                 self._query_driver_power_state_and_sync(context, db_instance)
9670 
9671             try:
9672                 query_driver_power_state_and_sync()
9673             except Exception:
9674                 LOG.exception("Periodic sync_power_state task had an "
9675                               "error while processing an instance.",
9676                               instance=db_instance)
9677 
9678             self._syncs_in_progress.pop(db_instance.uuid)
9679 
9680         for db_instance in db_instances:
9681             # process syncs asynchronously - don't want instance locking to
9682             # block entire periodic task thread
9683             uuid = db_instance.uuid
9684             if uuid in self._syncs_in_progress:
9685                 LOG.debug('Sync already in progress for %s', uuid)
9686             else:
9687                 LOG.debug('Triggering sync for uuid %s', uuid)
9688                 self._syncs_in_progress[uuid] = True
9689                 self._sync_power_pool.spawn_n(_sync, db_instance)
9690 
9691     def _query_driver_power_state_and_sync(self, context, db_instance):
9692         if db_instance.task_state is not None:
9693             LOG.info("During sync_power_state the instance has a "
9694                      "pending task (%(task)s). Skip.",
9695                      {'task': db_instance.task_state}, instance=db_instance)
9696             return
9697         # No pending tasks. Now try to figure out the real vm_power_state.
9698         try:
9699             vm_instance = self.driver.get_info(db_instance)
9700             vm_power_state = vm_instance.state
9701         except exception.InstanceNotFound:
9702             vm_power_state = power_state.NOSTATE
9703         # Note(maoy): the above get_info call might take a long time,
9704         # for example, because of a broken libvirt driver.
9705         try:
9706             self._sync_instance_power_state(context,
9707                                             db_instance,
9708                                             vm_power_state,
9709                                             use_slave=True)
9710         except exception.InstanceNotFound:
9711             # NOTE(hanlind): If the instance gets deleted during sync,
9712             # silently ignore.
9713             pass
9714 
9715     def _stop_unexpected_shutdown_instance(self, context, vm_state,
9716                                            db_instance, orig_db_power_state):
9717         # this is an exceptional case; make sure our data is up
9718         # to date before slamming through a power off
9719         vm_instance = self.driver.get_info(db_instance,
9720                                            use_cache=False)
9721         vm_power_state = vm_instance.state
9722 
9723         # if it still looks off, go ahead and call stop()
9724         if vm_power_state in (power_state.SHUTDOWN,
9725                               power_state.CRASHED):
9726 
9727             LOG.warning("Instance shutdown by itself. Calling the "
9728                         "stop API. Current vm_state: %(vm_state)s, "
9729                         "current task_state: %(task_state)s, "
9730                         "original DB power_state: %(db_power_state)s, "
9731                         "current VM power_state: %(vm_power_state)s",
9732                         {'vm_state': vm_state,
9733                          'task_state': db_instance.task_state,
9734                          'db_power_state': orig_db_power_state,
9735                          'vm_power_state': vm_power_state},
9736                         instance=db_instance)
9737             try:
9738                 # Note(maoy): here we call the API instead of
9739                 # brutally updating the vm_state in the database
9740                 # to allow all the hooks and checks to be performed.
9741                 if db_instance.shutdown_terminate:
9742                     self.compute_api.delete(context, db_instance)
9743                 else:
9744                     self.compute_api.stop(context, db_instance)
9745             except Exception:
9746                 # Note(maoy): there is no need to propagate the error
9747                 # because the same power_state will be retrieved next
9748                 # time and retried.
9749                 # For example, there might be another task scheduled.
9750                 LOG.exception("error during stop() in sync_power_state.",
9751                               instance=db_instance)
9752 
9753     def _sync_instance_power_state(self, context, db_instance, vm_power_state,
9754                                    use_slave=False):
9755         """Align instance power state between the database and hypervisor.
9756 
9757         If the instance is not found on the hypervisor, but is in the database,
9758         then a stop() API will be called on the instance.
9759         """
9760 
9761         # We re-query the DB to get the latest instance info to minimize
9762         # (not eliminate) race condition.
9763         db_instance.refresh(use_slave=use_slave)
9764         db_power_state = db_instance.power_state
9765         vm_state = db_instance.vm_state
9766 
9767         if self.host != db_instance.host:
9768             # on the sending end of nova-compute _sync_power_state
9769             # may have yielded to the greenthread performing a live
9770             # migration; this in turn has changed the resident-host
9771             # for the VM; However, the instance is still active, it
9772             # is just in the process of migrating to another host.
9773             # This implies that the compute source must relinquish
9774             # control to the compute destination.
9775             LOG.info("During the sync_power process the "
9776                      "instance has moved from "
9777                      "host %(src)s to host %(dst)s",
9778                      {'src': db_instance.host,
9779                       'dst': self.host},
9780                      instance=db_instance)
9781             return
9782         elif db_instance.task_state is not None:
9783             # on the receiving end of nova-compute, it could happen
9784             # that the DB instance already report the new resident
9785             # but the actual VM has not showed up on the hypervisor
9786             # yet. In this case, let's allow the loop to continue
9787             # and run the state sync in a later round
9788             LOG.info("During sync_power_state the instance has a "
9789                      "pending task (%(task)s). Skip.",
9790                      {'task': db_instance.task_state},
9791                      instance=db_instance)
9792             return
9793 
9794         orig_db_power_state = db_power_state
9795         if vm_power_state != db_power_state:
9796             LOG.info('During _sync_instance_power_state the DB '
9797                      'power_state (%(db_power_state)s) does not match '
9798                      'the vm_power_state from the hypervisor '
9799                      '(%(vm_power_state)s). Updating power_state in the '
9800                      'DB to match the hypervisor.',
9801                      {'db_power_state': db_power_state,
9802                       'vm_power_state': vm_power_state},
9803                      instance=db_instance)
9804             # power_state is always updated from hypervisor to db
9805             db_instance.power_state = vm_power_state
9806             db_instance.save()
9807             db_power_state = vm_power_state
9808 
9809         # Note(maoy): Now resolve the discrepancy between vm_state and
9810         # vm_power_state. We go through all possible vm_states.
9811         if vm_state in (vm_states.BUILDING,
9812                         vm_states.RESCUED,
9813                         vm_states.RESIZED,
9814                         vm_states.SUSPENDED,
9815                         vm_states.ERROR):
9816             # TODO(maoy): we ignore these vm_state for now.
9817             pass
9818         elif vm_state == vm_states.ACTIVE:
9819             # The only rational power state should be RUNNING
9820             if vm_power_state in (power_state.SHUTDOWN,
9821                                   power_state.CRASHED):
9822                 self._stop_unexpected_shutdown_instance(
9823                     context, vm_state, db_instance, orig_db_power_state)
9824             elif vm_power_state == power_state.SUSPENDED:
9825                 LOG.warning("Instance is suspended unexpectedly. Calling "
9826                             "the stop API.", instance=db_instance)
9827                 try:
9828                     self.compute_api.stop(context, db_instance)
9829                 except Exception:
9830                     LOG.exception("error during stop() in sync_power_state.",
9831                                   instance=db_instance)
9832             elif vm_power_state == power_state.PAUSED:
9833                 # Note(maoy): a VM may get into the paused state not only
9834                 # because the user request via API calls, but also
9835                 # due to (temporary) external instrumentations.
9836                 # Before the virt layer can reliably report the reason,
9837                 # we simply ignore the state discrepancy. In many cases,
9838                 # the VM state will go back to running after the external
9839                 # instrumentation is done. See bug 1097806 for details.
9840                 LOG.warning("Instance is paused unexpectedly. Ignore.",
9841                             instance=db_instance)
9842             elif vm_power_state == power_state.NOSTATE:
9843                 # Occasionally, depending on the status of the hypervisor,
9844                 # which could be restarting for example, an instance may
9845                 # not be found.  Therefore just log the condition.
9846                 LOG.warning("Instance is unexpectedly not found. Ignore.",
9847                             instance=db_instance)
9848         elif vm_state == vm_states.STOPPED:
9849             if vm_power_state not in (power_state.NOSTATE,
9850                                       power_state.SHUTDOWN,
9851                                       power_state.CRASHED):
9852                 LOG.warning("Instance is not stopped. Calling "
9853                             "the stop API. Current vm_state: %(vm_state)s,"
9854                             " current task_state: %(task_state)s, "
9855                             "original DB power_state: %(db_power_state)s, "
9856                             "current VM power_state: %(vm_power_state)s",
9857                             {'vm_state': vm_state,
9858                              'task_state': db_instance.task_state,
9859                              'db_power_state': orig_db_power_state,
9860                              'vm_power_state': vm_power_state},
9861                             instance=db_instance)
9862                 try:
9863                     # NOTE(russellb) Force the stop, because normally the
9864                     # compute API would not allow an attempt to stop a stopped
9865                     # instance.
9866                     self.compute_api.force_stop(context, db_instance)
9867                 except Exception:
9868                     LOG.exception("error during stop() in sync_power_state.",
9869                                   instance=db_instance)
9870         elif vm_state == vm_states.PAUSED:
9871             if vm_power_state in (power_state.SHUTDOWN,
9872                                   power_state.CRASHED):
9873                 LOG.warning("Paused instance shutdown by itself. Calling "
9874                             "the stop API.", instance=db_instance)
9875                 try:
9876                     self.compute_api.force_stop(context, db_instance)
9877                 except Exception:
9878                     LOG.exception("error during stop() in sync_power_state.",
9879                                   instance=db_instance)
9880         elif vm_state in (vm_states.SOFT_DELETED,
9881                           vm_states.DELETED):
9882             if vm_power_state not in (power_state.NOSTATE,
9883                                       power_state.SHUTDOWN):
9884                 # Note(maoy): this should be taken care of periodically in
9885                 # _cleanup_running_deleted_instances().
9886                 LOG.warning("Instance is not (soft-)deleted.",
9887                             instance=db_instance)
9888 
9889     @periodic_task.periodic_task
9890     def _reclaim_queued_deletes(self, context):
9891         """Reclaim instances that are queued for deletion."""
9892         interval = CONF.reclaim_instance_interval
9893         if interval <= 0:
9894             LOG.debug("CONF.reclaim_instance_interval <= 0, skipping...")
9895             return
9896 
9897         filters = {'vm_state': vm_states.SOFT_DELETED,
9898                    'task_state': None,
9899                    'host': self.host}
9900         instances = objects.InstanceList.get_by_filters(
9901             context, filters,
9902             expected_attrs=objects.instance.INSTANCE_DEFAULT_FIELDS,
9903             use_slave=True)
9904         for instance in instances:
9905             if self._deleted_old_enough(instance, interval):
9906                 bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
9907                         context, instance.uuid)
9908                 LOG.info('Reclaiming deleted instance', instance=instance)
9909                 try:
9910                     self._delete_instance(context, instance, bdms)
9911                 except Exception as e:
9912                     LOG.warning("Periodic reclaim failed to delete "
9913                                 "instance: %s",
9914                                 e, instance=instance)
9915 
9916     def _get_nodename(self, instance, refresh=False):
9917         """Helper method to get the name of the first available node
9918         on this host. This method should not be used with any operations
9919         on ironic instances since it does not handle multiple nodes.
9920         """
9921         node = self.driver.get_available_nodes(refresh=refresh)[0]
9922         LOG.debug("No node specified, defaulting to %s", node,
9923                   instance=instance)
9924         return node
9925 
9926     def _update_available_resource_for_node(self, context, nodename,
9927                                             startup=False):
9928 
9929         try:
9930             self.rt.update_available_resource(context, nodename,
9931                                               startup=startup)
9932         except exception.ComputeHostNotFound:
9933             LOG.warning("Compute node '%s' not found in "
9934                         "update_available_resource.", nodename)
9935         except exception.ReshapeFailed:
9936             # We're only supposed to get here on startup, if a reshape was
9937             # needed, was attempted, and failed. We want to kill the service.
9938             with excutils.save_and_reraise_exception():
9939                 LOG.critical("Resource provider data migration failed "
9940                              "fatally during startup for node %s.", nodename)
9941         except exception.ReshapeNeeded:
9942             # This exception should only find its way here if the virt driver's
9943             # update_provider_tree raised it incorrectly: either
9944             # a) After the resource tracker already caught it once and
9945             # reinvoked update_provider_tree with allocations. At this point
9946             # the driver is just supposed to *do* the reshape, so if it raises
9947             # ReshapeNeeded, it's a bug, and we want to kill the compute
9948             # service.
9949             # b) On periodic rather than startup (we only allow reshapes to
9950             # happen on startup). In this case we'll just make the logs red and
9951             # go again at the next periodic interval, where the same thing may
9952             # or may not happen again. Depending on the previous and intended
9953             # shape of the providers/inventories, this may not actually cause
9954             # any immediately visible symptoms (in terms of scheduling, etc.)
9955             # If this becomes a problem, we may wish to make it pop immediately
9956             # (e.g. disable the service).
9957             with excutils.save_and_reraise_exception():
9958                 LOG.exception("ReshapeNeeded exception is unexpected here!")
9959         except Exception:
9960             LOG.exception("Error updating resources for node %(node)s.",
9961                           {'node': nodename})
9962 
9963     @periodic_task.periodic_task(spacing=CONF.update_resources_interval)
9964     def update_available_resource(self, context, startup=False):
9965         """See driver.get_available_resource()
9966 
9967         Periodic process that keeps that the compute host's understanding of
9968         resource availability and usage in sync with the underlying hypervisor.
9969 
9970         :param context: security context
9971         :param startup: True if this is being called when the nova-compute
9972             service is starting, False otherwise.
9973         """
9974         try:
9975             nodenames = set(self.driver.get_available_nodes())
9976         except exception.VirtDriverNotReady:
9977             LOG.warning("Virt driver is not ready.")
9978             return
9979 
9980         compute_nodes_in_db = self._get_compute_nodes_in_db(context,
9981                                                             nodenames,
9982                                                             use_slave=True,
9983                                                             startup=startup)
9984 
9985         # Delete orphan compute node not reported by driver but still in db
9986         for cn in compute_nodes_in_db:
9987             if cn.hypervisor_hostname not in nodenames:
9988                 LOG.info("Deleting orphan compute node %(id)s "
9989                          "hypervisor host is %(hh)s, "
9990                          "nodes are %(nodes)s",
9991                          {'id': cn.id, 'hh': cn.hypervisor_hostname,
9992                           'nodes': nodenames})
9993                 cn.destroy()
9994                 self.rt.remove_node(cn.hypervisor_hostname)
9995                 # Delete the corresponding resource provider in placement,
9996                 # along with any associated allocations.
9997                 try:
9998                     self.reportclient.delete_resource_provider(context, cn,
9999                                                                cascade=True)
10000                 except keystone_exception.ClientException as e:
10001                     LOG.error(
10002                         "Failed to delete compute node resource provider "
10003                         "for compute node %s: %s", cn.uuid, str(e))
10004 
10005         for nodename in nodenames:
10006             self._update_available_resource_for_node(context, nodename,
10007                                                      startup=startup)
10008 
10009     def _get_compute_nodes_in_db(self, context, nodenames, use_slave=False,
10010                                  startup=False):
10011         try:
10012             return objects.ComputeNodeList.get_all_by_host(context, self.host,
10013                                                            use_slave=use_slave)
10014         except exception.NotFound:
10015             # If the driver is not reporting any nodenames we should not
10016             # expect there to be compute nodes so we just return in that case.
10017             # For example, this could be an ironic compute and it is not
10018             # managing any nodes yet.
10019             if nodenames:
10020                 if startup:
10021                     LOG.warning(
10022                         "No compute node record found for host %s. If this is "
10023                         "the first time this service is starting on this "
10024                         "host, then you can ignore this warning.", self.host)
10025                 else:
10026                     LOG.error("No compute node record for host %s", self.host)
10027             return []
10028 
10029     @periodic_task.periodic_task(
10030         spacing=CONF.running_deleted_instance_poll_interval,
10031         run_immediately=True)
10032     def _cleanup_running_deleted_instances(self, context):
10033         """Cleanup any instances which are erroneously still running after
10034         having been deleted.
10035 
10036         Valid actions to take are:
10037 
10038             1. noop - do nothing
10039             2. log - log which instances are erroneously running
10040             3. reap - shutdown and cleanup any erroneously running instances
10041             4. shutdown - power off *and disable* any erroneously running
10042                           instances
10043 
10044         The use-case for this cleanup task is: for various reasons, it may be
10045         possible for the database to show an instance as deleted but for that
10046         instance to still be running on a host machine (see bug
10047         https://bugs.launchpad.net/nova/+bug/911366).
10048 
10049         This cleanup task is a cross-hypervisor utility for finding these
10050         zombied instances and either logging the discrepancy (likely what you
10051         should do in production), or automatically reaping the instances (more
10052         appropriate for dev environments).
10053         """
10054         action = CONF.running_deleted_instance_action
10055 
10056         if action == "noop":
10057             return
10058 
10059         # NOTE(sirp): admin contexts don't ordinarily return deleted records
10060         with utils.temporary_mutation(context, read_deleted="yes"):
10061 
10062             try:
10063                 instances = self._running_deleted_instances(context)
10064             except exception.VirtDriverNotReady:
10065                 # Since this task runs immediately on startup, if the
10066                 # hypervisor is not yet ready handle it gracefully.
10067                 LOG.debug('Unable to check for running deleted instances '
10068                           'at this time since the hypervisor is not ready.')
10069                 return
10070 
10071             for instance in instances:
10072                 if action == "log":
10073                     LOG.warning("Detected instance with name label "
10074                                 "'%s' which is marked as "
10075                                 "DELETED but still present on host.",
10076                                 instance.name, instance=instance)
10077 
10078                 elif action == 'shutdown':
10079                     LOG.info("Powering off instance with name label "
10080                              "'%s' which is marked as "
10081                              "DELETED but still present on host.",
10082                              instance.name, instance=instance)
10083                     try:
10084                         self.driver.power_off(instance)
10085                     except Exception:
10086                         LOG.warning("Failed to power off instance",
10087                                     instance=instance, exc_info=True)
10088 
10089                 elif action == 'reap':
10090                     LOG.info("Destroying instance with name label "
10091                              "'%s' which is marked as "
10092                              "DELETED but still present on host.",
10093                              instance.name, instance=instance)
10094                     bdms = objects.BlockDeviceMappingList.get_by_instance_uuid(
10095                         context, instance.uuid, use_slave=True)
10096                     self.instance_events.clear_events_for_instance(instance)
10097                     try:
10098                         self._shutdown_instance(context, instance, bdms,
10099                                                 notify=False)
10100                         self._cleanup_volumes(context, instance, bdms,
10101                                               detach=False)
10102                     except Exception as e:
10103                         LOG.warning("Periodic cleanup failed to delete "
10104                                     "instance: %s",
10105                                     e, instance=instance)
10106                 else:
10107                     raise Exception(_("Unrecognized value '%s'"
10108                                       " for CONF.running_deleted_"
10109                                       "instance_action") % action)
10110 
10111     def _running_deleted_instances(self, context):
10112         """Returns a list of instances nova thinks is deleted,
10113         but the hypervisor thinks is still running.
10114         """
10115         timeout = CONF.running_deleted_instance_timeout
10116         filters = {'deleted': True,
10117                    'soft_deleted': False}
10118         instances = self._get_instances_on_driver(context, filters)
10119         return [i for i in instances if self._deleted_old_enough(i, timeout)]
10120 
10121     def _deleted_old_enough(self, instance, timeout):
10122         deleted_at = instance.deleted_at
10123         if deleted_at:
10124             deleted_at = deleted_at.replace(tzinfo=None)
10125         return (not deleted_at or timeutils.is_older_than(deleted_at, timeout))
10126 
10127     @contextlib.contextmanager
10128     def _error_out_instance_on_exception(self, context, instance,
10129                                          instance_state=vm_states.ACTIVE):
10130         """Context manager to set instance.vm_state after some operation raises
10131 
10132         Used to handle NotImplementedError and InstanceFaultRollback errors
10133         and reset the instance vm_state and task_state. The vm_state is set
10134         to the $instance_state parameter and task_state is set to None.
10135         For all other types of exceptions, the vm_state is set to ERROR and
10136         the task_state is left unchanged (although most callers will have the
10137         @reverts_task_state decorator which will set the task_state to None).
10138 
10139         Re-raises the original exception *except* in the case of
10140         InstanceFaultRollback in which case the wrapped `inner_exception` is
10141         re-raised.
10142 
10143         :param context: The nova auth request context for the operation.
10144         :param instance: The instance to update. The vm_state will be set by
10145             this context manager when an exception is raised.
10146         :param instance_state: For NotImplementedError and
10147             InstanceFaultRollback this is the vm_state to set the instance to
10148             when handling one of those types of exceptions. By default the
10149             instance will be set to ACTIVE, but the caller should control this
10150             in case there have been no changes to the running state of the
10151             instance. For example, resizing a stopped server where prep_resize
10152             fails early and does not change the power state of the guest should
10153             not set the instance status to ACTIVE but remain STOPPED.
10154             This parameter is ignored for all other types of exceptions and the
10155             instance vm_state is set to ERROR.
10156         """
10157         # NOTE(mriedem): Why doesn't this method just save off the
10158         # original instance.vm_state here rather than use a parameter? Or use
10159         # instance_state=None as an override but default to the current
10160         # vm_state when rolling back.
10161         instance_uuid = instance.uuid
10162         try:
10163             yield
10164         except (NotImplementedError, exception.InstanceFaultRollback) as error:
10165             # Use reraise=False to determine if we want to raise the original
10166             # exception or something else.
10167             with excutils.save_and_reraise_exception(reraise=False) as ctxt:
10168                 LOG.info("Setting instance back to %(state)s after: %(error)s",
10169                          {'state': instance_state, 'error': error},
10170                          instance_uuid=instance_uuid)
10171                 self._instance_update(context, instance,
10172                                       vm_state=instance_state,
10173                                       task_state=None)
10174                 if isinstance(error, exception.InstanceFaultRollback):
10175                     # Raise the wrapped exception.
10176                     raise error.inner_exception
10177                 # Else re-raise the NotImplementedError.
10178                 ctxt.reraise = True
10179         except Exception:
10180             LOG.exception('Setting instance vm_state to ERROR',
10181                           instance_uuid=instance_uuid)
10182             with excutils.save_and_reraise_exception():
10183                 # NOTE(mriedem): Why don't we pass clean_task_state=True here?
10184                 self._set_instance_obj_error_state(instance)
10185 
10186     def _process_instance_event(self, instance, event):
10187         _event = self.instance_events.pop_instance_event(instance, event)
10188         if _event:
10189             LOG.debug('Processing event %(event)s',
10190                       {'event': event.key}, instance=instance)
10191             _event.send(event)
10192         else:
10193             # If it's a network-vif-unplugged event and the instance is being
10194             # deleted or live migrated then we don't need to make this a
10195             # warning as it's expected. There are other expected things which
10196             # could trigger this event like detaching an interface, but we
10197             # don't have a task state for that.
10198             # TODO(mriedem): We have other move operations and things like
10199             # hard reboot (probably rebuild as well) which trigger this event
10200             # but nothing listens for network-vif-unplugged. We should either
10201             # handle those other known cases or consider just not logging a
10202             # warning if we get this event and the instance is undergoing some
10203             # task state transition.
10204             if (event.name == 'network-vif-unplugged' and
10205                     instance.task_state in (
10206                         task_states.DELETING, task_states.MIGRATING)):
10207                 LOG.debug('Received event %s for instance with task_state %s.',
10208                           event.key, instance.task_state, instance=instance)
10209             else:
10210                 LOG.warning('Received unexpected event %(event)s for '
10211                             'instance with vm_state %(vm_state)s and '
10212                             'task_state %(task_state)s.',
10213                             {'event': event.key,
10214                              'vm_state': instance.vm_state,
10215                              'task_state': instance.task_state},
10216                             instance=instance)
10217 
10218     def _process_instance_vif_deleted_event(self, context, instance,
10219                                             deleted_vif_id):
10220         # If an attached port is deleted by neutron, it needs to
10221         # be detached from the instance.
10222         # And info cache needs to be updated.
10223         network_info = instance.info_cache.network_info
10224         for index, vif in enumerate(network_info):
10225             if vif['id'] == deleted_vif_id:
10226                 LOG.info('Neutron deleted interface %(intf)s; '
10227                          'detaching it from the instance and '
10228                          'deleting it from the info cache',
10229                          {'intf': vif['id']},
10230                          instance=instance)
10231                 profile = vif.get('profile', {}) or {}  # profile can be None
10232                 if profile.get('allocation'):
10233                     LOG.error(
10234                         'The bound port %(port_id)s is deleted in Neutron but '
10235                         'the resource allocation on the resource provider '
10236                         '%(rp_uuid)s is leaked until the server '
10237                         '%(server_uuid)s is deleted.',
10238                         {'port_id': vif['id'],
10239                          'rp_uuid': vif['profile']['allocation'],
10240                          'server_uuid': instance.uuid})
10241 
10242                 del network_info[index]
10243                 neutron.update_instance_cache_with_nw_info(
10244                     self.network_api, context, instance, nw_info=network_info)
10245                 try:
10246                     self.driver.detach_interface(context, instance, vif)
10247                 except NotImplementedError:
10248                     # Not all virt drivers support attach/detach of interfaces
10249                     # yet (like Ironic), so just ignore this.
10250                     pass
10251                 except exception.NovaException as ex:
10252                     # If the instance was deleted before the interface was
10253                     # detached, just log it at debug.
10254                     log_level = (logging.DEBUG
10255                                  if isinstance(ex, exception.InstanceNotFound)
10256                                  else logging.WARNING)
10257                     LOG.log(log_level,
10258                             "Detach interface failed, "
10259                             "port_id=%(port_id)s, reason: %(msg)s",
10260                             {'port_id': deleted_vif_id, 'msg': ex},
10261                             instance=instance)
10262                 break
10263 
10264     @wrap_instance_event(prefix='compute')
10265     @wrap_instance_fault
10266     def extend_volume(self, context, instance, extended_volume_id):
10267 
10268         # If an attached volume is extended by cinder, it needs to
10269         # be extended by virt driver so host can detect its new size.
10270         # And bdm needs to be updated.
10271         LOG.debug('Handling volume-extended event for volume %(vol)s',
10272                   {'vol': extended_volume_id}, instance=instance)
10273 
10274         try:
10275             bdm = objects.BlockDeviceMapping.get_by_volume_and_instance(
10276                    context, extended_volume_id, instance.uuid)
10277         except exception.NotFound:
10278             LOG.warning('Extend volume failed, '
10279                         'volume %(vol)s is not attached to instance.',
10280                         {'vol': extended_volume_id},
10281                         instance=instance)
10282             return
10283 
10284         LOG.info('Cinder extended volume %(vol)s; '
10285                  'extending it to detect new size',
10286                  {'vol': extended_volume_id},
10287                  instance=instance)
10288         volume = self.volume_api.get(context, bdm.volume_id)
10289 
10290         if bdm.connection_info is None:
10291             LOG.warning('Extend volume failed, '
10292                         'attached volume %(vol)s has no connection_info',
10293                         {'vol': extended_volume_id},
10294                         instance=instance)
10295             return
10296 
10297         connection_info = jsonutils.loads(bdm.connection_info)
10298         bdm.volume_size = volume['size']
10299         bdm.save()
10300 
10301         if not self.driver.capabilities.get('supports_extend_volume', False):
10302             raise exception.ExtendVolumeNotSupported()
10303 
10304         try:
10305             self.driver.extend_volume(context, connection_info, instance,
10306                                       bdm.volume_size * units.Gi)
10307         except Exception as ex:
10308             LOG.warning('Extend volume failed, '
10309                         'volume_id=%(volume_id)s, reason: %(msg)s',
10310                         {'volume_id': extended_volume_id, 'msg': ex},
10311                         instance=instance)
10312             raise
10313 
10314     @staticmethod
10315     def _is_state_valid_for_power_update_event(instance, target_power_state):
10316         """Check if the current state of the instance allows it to be
10317         a candidate for the power-update event.
10318 
10319         :param instance: The nova instance object.
10320         :param target_power_state: The desired target power state; this should
10321                                    either be "POWER_ON" or "POWER_OFF".
10322         :returns Boolean: True if the instance can be subjected to the
10323                           power-update event.
10324         """
10325         if ((target_power_state == external_event_obj.POWER_ON and
10326                 instance.task_state is None and
10327                 instance.vm_state == vm_states.STOPPED and
10328                 instance.power_state == power_state.SHUTDOWN) or
10329             (target_power_state == external_event_obj.POWER_OFF and
10330                 instance.task_state is None and
10331                 instance.vm_state == vm_states.ACTIVE and
10332                 instance.power_state == power_state.RUNNING)):
10333             return True
10334         return False
10335 
10336     @wrap_exception()
10337     @reverts_task_state
10338     @wrap_instance_event(prefix='compute')
10339     @wrap_instance_fault
10340     def power_update(self, context, instance, target_power_state):
10341         """Power update of an instance prompted by an external event.
10342         :param context: The API request context.
10343         :param instance: The nova instance object.
10344         :param target_power_state: The desired target power state;
10345                                    this should either be "POWER_ON" or
10346                                    "POWER_OFF".
10347         """
10348 
10349         @utils.synchronized(instance.uuid)
10350         def do_power_update():
10351             LOG.debug('Handling power-update event with target_power_state %s '
10352                       'for instance', target_power_state, instance=instance)
10353             if not self._is_state_valid_for_power_update_event(
10354                     instance, target_power_state):
10355                 pow_state = fields.InstancePowerState.from_index(
10356                     instance.power_state)
10357                 LOG.info('The power-update %(tag)s event for instance '
10358                          '%(uuid)s is a no-op since the instance is in '
10359                          'vm_state %(vm_state)s, task_state '
10360                          '%(task_state)s and power_state '
10361                          '%(power_state)s.',
10362                          {'tag': target_power_state, 'uuid': instance.uuid,
10363                          'vm_state': instance.vm_state,
10364                          'task_state': instance.task_state,
10365                          'power_state': pow_state})
10366                 return
10367             LOG.debug("Trying to %s instance",
10368                       target_power_state, instance=instance)
10369             if target_power_state == external_event_obj.POWER_ON:
10370                 action = fields.NotificationAction.POWER_ON
10371                 notification_name = "power_on."
10372                 instance.task_state = task_states.POWERING_ON
10373             else:
10374                 # It's POWER_OFF
10375                 action = fields.NotificationAction.POWER_OFF
10376                 notification_name = "power_off."
10377                 instance.task_state = task_states.POWERING_OFF
10378                 instance.progress = 0
10379 
10380             try:
10381                 # Note that the task_state is set here rather than the API
10382                 # because this is a best effort operation and deferring
10383                 # updating the task_state until we get to the compute service
10384                 # avoids error handling in the API and needing to account for
10385                 # older compute services during rolling upgrades from Stein.
10386                 # If we lose a race, UnexpectedTaskStateError is handled
10387                 # below.
10388                 instance.save(expected_task_state=[None])
10389                 self._notify_about_instance_usage(context, instance,
10390                                                   notification_name + "start")
10391                 compute_utils.notify_about_instance_action(context, instance,
10392                     self.host, action=action,
10393                     phase=fields.NotificationPhase.START)
10394                 # UnexpectedTaskStateError raised from the driver will be
10395                 # handled below and not result in a fault, error notification
10396                 # or failure of the instance action. Other driver errors like
10397                 # NotImplementedError will be record a fault, send an error
10398                 # notification and mark the instance action as failed.
10399                 self.driver.power_update_event(instance, target_power_state)
10400                 self._notify_about_instance_usage(context, instance,
10401                                                   notification_name + "end")
10402                 compute_utils.notify_about_instance_action(context, instance,
10403                     self.host, action=action,
10404                     phase=fields.NotificationPhase.END)
10405             except exception.UnexpectedTaskStateError as e:
10406                 # Handling the power-update event is best effort and if we lost
10407                 # a race with some other action happening to the instance we
10408                 # just log it and return rather than fail the action.
10409                 LOG.info("The power-update event was possibly preempted: %s ",
10410                          e.format_message(), instance=instance)
10411                 return
10412         do_power_update()
10413 
10414     @wrap_exception()
10415     def external_instance_event(self, context, instances, events):
10416         # NOTE(danms): Some event types are handled by the manager, such
10417         # as when we're asked to update the instance's info_cache. If it's
10418         # not one of those, look for some thread(s) waiting for the event and
10419         # unblock them if so.
10420         for event in events:
10421             instance = [inst for inst in instances
10422                         if inst.uuid == event.instance_uuid][0]
10423             LOG.debug('Received event %(event)s',
10424                       {'event': event.key},
10425                       instance=instance)
10426             if event.name == 'network-changed':
10427                 try:
10428                     LOG.debug('Refreshing instance network info cache due to '
10429                               'event %s.', event.key, instance=instance)
10430                     self.network_api.get_instance_nw_info(
10431                         context, instance, refresh_vif_id=event.tag)
10432                 except exception.NotFound as e:
10433                     LOG.info('Failed to process external instance event '
10434                              '%(event)s due to: %(error)s',
10435                              {'event': event.key, 'error': str(e)},
10436                              instance=instance)
10437             elif event.name == 'network-vif-deleted':
10438                 try:
10439                     self._process_instance_vif_deleted_event(context,
10440                                                              instance,
10441                                                              event.tag)
10442                 except exception.NotFound as e:
10443                     LOG.info('Failed to process external instance event '
10444                              '%(event)s due to: %(error)s',
10445                              {'event': event.key, 'error': str(e)},
10446                              instance=instance)
10447             elif event.name == 'volume-extended':
10448                 self.extend_volume(context, instance, event.tag)
10449             elif event.name == 'power-update':
10450                 self.power_update(context, instance, event.tag)
10451             else:
10452                 self._process_instance_event(instance, event)
10453 
10454     @periodic_task.periodic_task(spacing=CONF.image_cache.manager_interval,
10455                                  external_process_ok=True)
10456     def _run_image_cache_manager_pass(self, context):
10457         """Run a single pass of the image cache manager."""
10458 
10459         if not self.driver.capabilities.get("has_imagecache", False):
10460             return
10461 
10462         # Determine what other nodes use this storage
10463         storage_users.register_storage_use(CONF.instances_path, CONF.host)
10464         nodes = storage_users.get_storage_users(CONF.instances_path)
10465 
10466         # Filter all_instances to only include those nodes which share this
10467         # storage path.
10468         # TODO(mikal): this should be further refactored so that the cache
10469         # cleanup code doesn't know what those instances are, just a remote
10470         # count, and then this logic should be pushed up the stack.
10471         filters = {'deleted': False,
10472                    'soft_deleted': True,
10473                    'host': nodes}
10474         filtered_instances = objects.InstanceList.get_by_filters(context,
10475                                  filters, expected_attrs=[], use_slave=True)
10476 
10477         self.driver.manage_image_cache(context, filtered_instances)
10478 
10479     def cache_images(self, context, image_ids):
10480         """Ask the virt driver to pre-cache a set of base images.
10481 
10482         :param context: The RequestContext
10483         :param image_ids: The image IDs to be cached
10484         :return: A dict, keyed by image-id where the values are one of:
10485                  'cached' if the image was downloaded,
10486                  'existing' if the image was already in the cache,
10487                  'unsupported' if the virt driver does not support caching,
10488                  'error' if the virt driver raised an exception.
10489         """
10490 
10491         results = {}
10492 
10493         LOG.info('Caching %i image(s) by request', len(image_ids))
10494         for image_id in image_ids:
10495             try:
10496                 cached = self.driver.cache_image(context, image_id)
10497                 if cached:
10498                     results[image_id] = 'cached'
10499                 else:
10500                     results[image_id] = 'existing'
10501             except NotImplementedError:
10502                 LOG.warning('Virt driver does not support image pre-caching;'
10503                             ' ignoring request')
10504                 # NOTE(danms): Yes, technically we could short-circuit here to
10505                 # avoid trying the rest of the images, but it's very cheap to
10506                 # just keep hitting the NotImplementedError to keep the logic
10507                 # clean.
10508                 results[image_id] = 'unsupported'
10509             except Exception as e:
10510                 results[image_id] = 'error'
10511                 LOG.error('Failed to cache image %(image_id)s: %(err)s',
10512                           {'image_id': image_id,
10513                            'err': e})
10514 
10515         return results
10516 
10517     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10518     def _run_pending_deletes(self, context):
10519         """Retry any pending instance file deletes."""
10520         LOG.debug('Cleaning up deleted instances')
10521         filters = {'deleted': True,
10522                    'soft_deleted': False,
10523                    'host': CONF.host,
10524                    'cleaned': False}
10525         attrs = ['system_metadata']
10526         with utils.temporary_mutation(context, read_deleted='yes'):
10527             instances = objects.InstanceList.get_by_filters(
10528                 context, filters, expected_attrs=attrs, use_slave=True)
10529         LOG.debug('There are %d instances to clean', len(instances))
10530 
10531         for instance in instances:
10532             attempts = int(instance.system_metadata.get('clean_attempts', '0'))
10533             LOG.debug('Instance has had %(attempts)s of %(max)s '
10534                       'cleanup attempts',
10535                       {'attempts': attempts,
10536                        'max': CONF.maximum_instance_delete_attempts},
10537                       instance=instance)
10538             if attempts < CONF.maximum_instance_delete_attempts:
10539                 success = self.driver.delete_instance_files(instance)
10540 
10541                 instance.system_metadata['clean_attempts'] = str(attempts + 1)
10542                 if success:
10543                     instance.cleaned = True
10544                 with utils.temporary_mutation(context, read_deleted='yes'):
10545                     instance.save()
10546 
10547     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10548     def _cleanup_incomplete_migrations(self, context):
10549         """Cleanup on failed resize/revert-resize operation and
10550         failed rollback live migration operation.
10551 
10552         During resize/revert-resize operation, or after a failed rollback
10553         live migration operation, if that instance gets deleted then instance
10554         files might remain either on source or destination compute node and
10555         other specific resources might not be cleaned up because of the race
10556         condition.
10557         """
10558         LOG.debug('Cleaning up deleted instances with incomplete migration ')
10559         migration_filters = {'host': CONF.host,
10560                              'status': 'error'}
10561         migrations = objects.MigrationList.get_by_filters(context,
10562                                                           migration_filters)
10563 
10564         if not migrations:
10565             return
10566 
10567         inst_uuid_from_migrations = set([migration.instance_uuid for migration
10568                                          in migrations])
10569 
10570         inst_filters = {'deleted': True, 'soft_deleted': False,
10571                         'uuid': inst_uuid_from_migrations}
10572         attrs = ['info_cache', 'security_groups', 'system_metadata']
10573         with utils.temporary_mutation(context, read_deleted='yes'):
10574             instances = objects.InstanceList.get_by_filters(
10575                 context, inst_filters, expected_attrs=attrs, use_slave=True)
10576 
10577         for instance in instances:
10578             if instance.host == CONF.host:
10579                 continue
10580             for migration in migrations:
10581                 if instance.uuid != migration.instance_uuid:
10582                     continue
10583                 self.driver.delete_instance_files(instance)
10584                 # we are not sure whether the migration_context is applied
10585                 # during incompleted migrating, we need to apply/revert
10586                 # migration_context to get instance object content matching
10587                 # current host.
10588                 revert = (True if migration.source_compute == CONF.host
10589                           else False)
10590                 with instance.mutated_migration_context(revert=revert):
10591                     self.driver.cleanup_lingering_instance_resources(instance)
10592 
10593                 try:
10594                     migration.status = 'failed'
10595                     migration.save()
10596                 except exception.MigrationNotFound:
10597                     LOG.warning("Migration %s is not found.",
10598                                 migration.id,
10599                                 instance=instance)
10600                 break
10601 
10602     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10603                                    exception.QemuGuestAgentNotEnabled,
10604                                    exception.NovaException,
10605                                    NotImplementedError)
10606     @wrap_exception()
10607     def quiesce_instance(self, context, instance):
10608         """Quiesce an instance on this host."""
10609         context = context.elevated()
10610         image_meta = objects.ImageMeta.from_instance(instance)
10611         self.driver.quiesce(context, instance, image_meta)
10612 
10613     def _wait_for_snapshots_completion(self, context, mapping):
10614         for mapping_dict in mapping:
10615             if mapping_dict.get('source_type') == 'snapshot':
10616 
10617                 def _wait_snapshot():
10618                     snapshot = self.volume_api.get_snapshot(
10619                         context, mapping_dict['snapshot_id'])
10620                     if snapshot.get('status') != 'creating':
10621                         raise loopingcall.LoopingCallDone()
10622 
10623                 timer = loopingcall.FixedIntervalLoopingCall(_wait_snapshot)
10624                 timer.start(interval=0.5).wait()
10625 
10626     @messaging.expected_exceptions(exception.InstanceQuiesceNotSupported,
10627                                    exception.QemuGuestAgentNotEnabled,
10628                                    exception.NovaException,
10629                                    NotImplementedError)
10630     @wrap_exception()
10631     def unquiesce_instance(self, context, instance, mapping=None):
10632         """Unquiesce an instance on this host.
10633 
10634         If snapshots' image mapping is provided, it waits until snapshots are
10635         completed before unqueiscing.
10636         """
10637         context = context.elevated()
10638         if mapping:
10639             try:
10640                 self._wait_for_snapshots_completion(context, mapping)
10641             except Exception as error:
10642                 LOG.exception("Exception while waiting completion of "
10643                               "volume snapshots: %s",
10644                               error, instance=instance)
10645         image_meta = objects.ImageMeta.from_instance(instance)
10646         self.driver.unquiesce(context, instance, image_meta)
10647 
10648     @periodic_task.periodic_task(spacing=CONF.instance_delete_interval)
10649     def _cleanup_expired_console_auth_tokens(self, context):
10650         """Remove all expired console auth tokens.
10651 
10652         Console authorization tokens and their connection data are stored
10653         in the database when a user asks for a console connection to an
10654         instance. After a time they expire. We periodically remove any expired
10655         tokens from the database.
10656         """
10657         objects.ConsoleAuthToken.clean_expired_console_auths(context)
10658 
10659     def _claim_pci_for_instance_vifs(self, ctxt, instance):
10660         """Claim PCI devices for the instance's VIFs on the compute node
10661 
10662         :param ctxt: Context
10663         :param instance: Instance object
10664         :return: <port ID: PciDevice> mapping for the VIFs that yielded a
10665                 PCI claim on the compute node
10666         """
10667         pci_req_id_to_port_id = {}
10668         pci_reqs = []
10669         port_id_to_pci_dev = {}
10670 
10671         for vif in instance.get_network_info():
10672             pci_req = pci_req_module.get_instance_pci_request_from_vif(
10673                 ctxt,
10674                 instance,
10675                 vif)
10676             if pci_req:
10677                 pci_req_id_to_port_id[pci_req.request_id] = vif['id']
10678                 pci_reqs.append(pci_req)
10679 
10680         if pci_reqs:
10681             # Create PCI requests and claim against PCI resource tracker
10682             # NOTE(adrianc): We claim against the same requests as on the
10683             # source node.
10684             vif_pci_requests = objects.InstancePCIRequests(
10685                 requests=pci_reqs,
10686                 instance_uuid=instance.uuid)
10687 
10688             # if we are called during the live migration with NUMA topology
10689             # support the PCI claim needs to consider the destination NUMA
10690             # topology that is then stored in the migration_context
10691             dest_topo = None
10692             if instance.migration_context:
10693                 dest_topo = instance.migration_context.new_numa_topology
10694 
10695             claimed_pci_devices_objs = self.rt.claim_pci_devices(
10696                 ctxt, vif_pci_requests, dest_topo)
10697 
10698             # Update VIFMigrateData profile with the newly claimed PCI
10699             # device
10700             for pci_dev in claimed_pci_devices_objs:
10701                 LOG.debug("PCI device: %s Claimed on destination node",
10702                           pci_dev.address)
10703                 port_id = pci_req_id_to_port_id[pci_dev.request_id]
10704                 port_id_to_pci_dev[port_id] = pci_dev
10705 
10706         return port_id_to_pci_dev
10707 
10708     def _update_migrate_vifs_profile_with_pci(self,
10709                                               migrate_vifs,
10710                                               port_id_to_pci_dev):
10711         """Update migrate vifs profile with the claimed PCI devices
10712 
10713         :param migrate_vifs: list of VIFMigrateData objects
10714         :param port_id_to_pci_dev: a <port_id: PciDevice> mapping
10715         :return: None.
10716         """
10717         for mig_vif in migrate_vifs:
10718             port_id = mig_vif.port_id
10719             if port_id not in port_id_to_pci_dev:
10720                 continue
10721 
10722             pci_dev = port_id_to_pci_dev[port_id]
10723             profile = copy.deepcopy(mig_vif.source_vif['profile'])
10724             profile['pci_slot'] = pci_dev.address
10725             profile['pci_vendor_info'] = ':'.join([pci_dev.vendor_id,
10726                                                    pci_dev.product_id])
10727             mig_vif.profile = profile
10728             LOG.debug("Updating migrate VIF profile for port %(port_id)s:"
10729                       "%(profile)s", {'port_id': port_id,
10730                                       'profile': profile})
10731 
10732 
10733 # TODO(sbauza): Remove this proxy class in the X release once we drop the 5.x
10734 # support.
10735 # NOTE(sbauza): This proxy class will support the existing <=5.13 RPC calls
10736 # from any RPC client but will also make sure that the new 6.0 RPC calls will
10737 # be supported.
10738 class _ComputeV5Proxy(object):
10739 
10740     target = messaging.Target(version='5.13')
10741 
10742     def __init__(self, manager):
10743         self.manager = manager
10744 
10745     def __getattr__(self, name):
10746         # NOTE(sbauza): Proxying all the other methods but the V5 ones.
10747         return getattr(self.manager, name)
10748 
10749     # 5.0 support for block_migration argument
10750     def pre_live_migration(self, context, instance, block_migration, disk,
10751                            migrate_data):
10752         return self.manager.pre_live_migration(context, instance, disk,
10753                                                migrate_data)
10754 
10755     # 5.1 support for legacy request_spec argument
10756     def prep_resize(self, context, image, instance, instance_type,
10757                     request_spec, filter_properties, node,
10758                     clean_shutdown, migration, host_list):
10759         if not isinstance(request_spec, objects.RequestSpec):
10760             # Prior to compute RPC API 5.1 conductor would pass a legacy dict
10761             # version of the request spec to compute and since Stein compute
10762             # could be sending that back to conductor on reschedule, so if we
10763             # got a dict convert it to an object.
10764             # TODO(mriedem): We can drop this compat code when we only support
10765             # compute RPC API >=6.0.
10766             request_spec = objects.RequestSpec.from_primitives(
10767                 context, request_spec, filter_properties)
10768             # We don't have to set the new flavor on the request spec because
10769             # if we got here it was due to a reschedule from the compute and
10770             # the request spec would already have the new flavor in it from the
10771             # else block below.
10772         self.manager.prep_resize(context, image, instance, instance_type,
10773                                  request_spec, filter_properties, node,
10774                                  clean_shutdown, migration, host_list)
10775 
10776     # 5.2 support for optional request_spec argument
10777     def resize_instance(self, context, instance, image,
10778                         migration, instance_type, clean_shutdown,
10779                         request_spec=None):
10780         self.manager.resize_instance(context, instance, image,
10781                                     migration, instance_type, clean_shutdown,
10782                                     request_spec)
10783 
10784     # 5.2 support for optional request_spec argument
10785     def finish_resize(self, context, disk_info, image, instance,
10786                       migration, request_spec=None):
10787         self.manager.finish_resize(context, disk_info, image, instance,
10788                                    migration, request_spec)
10789 
10790     # 5.2 support for optional request_spec argument
10791     def revert_resize(self, context, instance, migration, request_spec=None):
10792         self.manager.revert_resize(context, instance, migration, request_spec)
10793 
10794     # 5.2 support for optional request_spec argument
10795     def finish_revert_resize(
10796             self, context, instance, migration, request_spec=None):
10797         self.manager.finish_revert_resize(context, instance, migration,
10798                                           request_spec)
10799 
10800     # 5.2 support for optional request_spec argument
10801     # 5.13 support for optional accel_uuids argument
10802     def unshelve_instance(self, context, instance, image, filter_properties,
10803                           node, request_spec=None, accel_uuids=None):
10804         self.manager.unshelve_instance(context, instance, image,
10805                                        filter_properties, node, request_spec,
10806                                        accel_uuids or [])
10807 
10808     # 5.3 support for optional migration and limits arguments
10809     def check_can_live_migrate_destination(self, ctxt, instance,
10810                                            block_migration, disk_over_commit,
10811                                            migration=None, limits=None):
10812         return self.manager.check_can_live_migrate_destination(
10813             ctxt, instance, block_migration, disk_over_commit,
10814             migration, limits)
10815 
10816     # 5.11 support for optional accel_uuids argument
10817     def build_and_run_instance(self, context, instance, image, request_spec,
10818                      filter_properties, admin_password=None,
10819                      injected_files=None, requested_networks=None,
10820                      security_groups=None, block_device_mapping=None,
10821                      node=None, limits=None, host_list=None, accel_uuids=None):
10822         self.manager.build_and_run_instance(
10823             context, instance, image, request_spec,
10824             filter_properties, accel_uuids, admin_password,
10825             injected_files, requested_networks,
10826             security_groups, block_device_mapping,
10827             node, limits, host_list)
10828 
10829     # 5.12 support for optional accel_uuids argument
10830     def rebuild_instance(self, context, instance, orig_image_ref, image_ref,
10831                          injected_files, new_pass, orig_sys_metadata,
10832                          bdms, recreate, on_shared_storage,
10833                          preserve_ephemeral, migration,
10834                          scheduled_node, limits, request_spec,
10835                          accel_uuids=None):
10836         self.manager.rebuild_instance(
10837             context, instance, orig_image_ref, image_ref,
10838             injected_files, new_pass, orig_sys_metadata,
10839             bdms, recreate, on_shared_storage,
10840             preserve_ephemeral, migration,
10841             scheduled_node, limits, request_spec,
10842             accel_uuids)
10843 
10844     # 5.13 support for optional accel_uuids argument
10845     def shelve_instance(self, context, instance, image_id,
10846                         clean_shutdown, accel_uuids=None):
10847         self.manager.shelve_instance(context, instance, image_id,
10848                                      clean_shutdown, accel_uuids)
10849 
10850     # 5.13 support for optional accel_uuids argument
10851     def shelve_offload_instance(self, context, instance, clean_shutdown,
10852             accel_uuids=None):
10853         self.manager.shelve_offload_instance(
10854             context, instance, clean_shutdown, accel_uuids)
10855 
10856     # 6.0 drop unused request_spec argument
10857     def prep_snapshot_based_resize_at_dest(
10858             self, ctxt, instance, flavor, nodename, migration, limits,
10859             request_spec):
10860         return self.manager.prep_snapshot_based_resize_at_dest(
10861             ctxt, instance, flavor, nodename, migration, limits)
10862 
10863     # 6.0 drop unused request_spec argument
10864     def finish_snapshot_based_resize_at_dest(
10865             self, ctxt, instance, migration, snapshot_id, request_spec):
10866         self.manager.finish_snapshot_based_resize_at_dest(
10867             ctxt, instance, migration, snapshot_id)
10868 
10869     # 6.0 drop unused instance argument
10870     def check_instance_shared_storage(self, ctxt, instance, data):
10871         return self.manager.check_instance_shared_storage(ctxt, data)
